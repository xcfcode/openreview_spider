{"notes": [{"id": "NUCZeoVlAe", "original": "cTsKA5T2Sv9", "number": 1111, "cdate": 1601308124919, "ddate": null, "tcdate": 1601308124919, "tmdate": 1614985762629, "tddate": null, "forum": "NUCZeoVlAe", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Empirical Studies on the Convergence of Feature Spaces in Deep Learning", "authorids": ["~Haoran_Liu2", "~Haoyi_Xiong1", "~Yaqing_Wang2", "~Haozhe_An1", "~Dongrui_Wu1", "~Dejing_Dou1"], "authors": ["Haoran Liu", "Haoyi Xiong", "Yaqing Wang", "Haozhe An", "Dongrui Wu", "Dejing Dou"], "keywords": [], "abstract": "While deep learning is effective to learn features/representations from data, the distributions of samples in feature spaces learned by various architectures for different training tasks (e.g., latent layers of AEs and feature vectors in CNN classifiers) have not been well-studied or compared. We hypothesize that the feature spaces of networks trained by various architectures (AEs or CNNs) and tasks (supervised, unsupervised, or self-supervised learning) share some common subspaces, no matter what types of DNN architectures or whether the labels have been used in feature learning. To test our hypothesis, through Singular Value Decomposition (SVD) of feature vectors, we demonstrate that one could linearly project the feature vectors of the same group of samples to a similar distribution, where the distribution is represented as the top left singular vector (i.e., principal subspace of feature vectors), namely $\\mathcal{P}$-vectors. We further assess the convergence of feature space learning using angles between $\\mathcal{P}$-vectors obtained from the well-trained model and its checkpoint per epoch during the learning procedure, where a quasi-monotonic trend of convergence to small angles has been observed. Finally, we carry out case studies to connect $\\mathcal{P}$-vectors to the data distribution, and generalization performance. Extensive experiments with practically-used MLP, AE and CNN architectures for classification, image reconstruction, and self-supervised learning tasks on MNIST, CIFAR-10 and CIFAR-100 datasets have been done to support our claims with solid evidences.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "liu|empirical_studies_on_the_convergence_of_feature_spaces_in_deep_learning", "supplementary_material": "", "pdf": "/pdf/2b195c6847605c50ea00784757198719861afaf3.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=iphj4u5v3", "_bibtex": "@misc{\nliu2021empirical,\ntitle={Empirical Studies on the Convergence of Feature Spaces in Deep Learning},\nauthor={Haoran Liu and Haoyi Xiong and Yaqing Wang and Haozhe An and Dongrui Wu and Dejing Dou},\nyear={2021},\nurl={https://openreview.net/forum?id=NUCZeoVlAe}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 13, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "GUEenoB_NIL", "original": null, "number": 1, "cdate": 1610040371218, "ddate": null, "tcdate": 1610040371218, "tmdate": 1610473962710, "tddate": null, "forum": "NUCZeoVlAe", "replyto": "NUCZeoVlAe", "invitation": "ICLR.cc/2021/Conference/Paper1111/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "This paper presents an intriguing empirical phenomenon in deep learning. They train a variety of architectures for different tasks using different datasets and study the relationship between the learned representations. In particular they collect the representations into a large matrix and take the top left singular vector and measure the cosine of the angle. They show that it is much smaller than one might expect, about 10 degrees or so, and has an approximate monotonicity property as the network is being trained although it does not seem to converge to zero. Moreover this measure also correlates with performance. \n\nThe reviewers had divided opinions on this paper. On the one hand, the range of experiments is impressive and truly demonstrates that this is a pervasive phenomenon. On the other hand, it is not so clear what it means. In particular, suppose we have a collection of graphs which have close to the same degree distributions. If we take the top left singular vectors of all the adjacency matrices, they would also have low angles between them. While this is a very different setting and there is no analogy between the experiments in this paper and this toy model, it does raise philosophical questions about whether the phenomenon is meaningful or is a byproduct of something else about the data. This may be a challenging question to answer, but one reviewer brought up a natural next step: One could measure the principal angle between the subspace of the top k left singular vectors across experiments for larger values of k. The authors do bring up the point that the spectrum decays very quickly, so it could be that beyond a certain point the singular vectors behave somewhat randomly. "}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Empirical Studies on the Convergence of Feature Spaces in Deep Learning", "authorids": ["~Haoran_Liu2", "~Haoyi_Xiong1", "~Yaqing_Wang2", "~Haozhe_An1", "~Dongrui_Wu1", "~Dejing_Dou1"], "authors": ["Haoran Liu", "Haoyi Xiong", "Yaqing Wang", "Haozhe An", "Dongrui Wu", "Dejing Dou"], "keywords": [], "abstract": "While deep learning is effective to learn features/representations from data, the distributions of samples in feature spaces learned by various architectures for different training tasks (e.g., latent layers of AEs and feature vectors in CNN classifiers) have not been well-studied or compared. We hypothesize that the feature spaces of networks trained by various architectures (AEs or CNNs) and tasks (supervised, unsupervised, or self-supervised learning) share some common subspaces, no matter what types of DNN architectures or whether the labels have been used in feature learning. To test our hypothesis, through Singular Value Decomposition (SVD) of feature vectors, we demonstrate that one could linearly project the feature vectors of the same group of samples to a similar distribution, where the distribution is represented as the top left singular vector (i.e., principal subspace of feature vectors), namely $\\mathcal{P}$-vectors. We further assess the convergence of feature space learning using angles between $\\mathcal{P}$-vectors obtained from the well-trained model and its checkpoint per epoch during the learning procedure, where a quasi-monotonic trend of convergence to small angles has been observed. Finally, we carry out case studies to connect $\\mathcal{P}$-vectors to the data distribution, and generalization performance. Extensive experiments with practically-used MLP, AE and CNN architectures for classification, image reconstruction, and self-supervised learning tasks on MNIST, CIFAR-10 and CIFAR-100 datasets have been done to support our claims with solid evidences.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "liu|empirical_studies_on_the_convergence_of_feature_spaces_in_deep_learning", "supplementary_material": "", "pdf": "/pdf/2b195c6847605c50ea00784757198719861afaf3.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=iphj4u5v3", "_bibtex": "@misc{\nliu2021empirical,\ntitle={Empirical Studies on the Convergence of Feature Spaces in Deep Learning},\nauthor={Haoran Liu and Haoyi Xiong and Yaqing Wang and Haozhe An and Dongrui Wu and Dejing Dou},\nyear={2021},\nurl={https://openreview.net/forum?id=NUCZeoVlAe}\n}"}, "tags": [], "invitation": {"reply": {"forum": "NUCZeoVlAe", "replyto": "NUCZeoVlAe", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040371204, "tmdate": 1610473962692, "id": "ICLR.cc/2021/Conference/Paper1111/-/Decision"}}}, {"id": "Ep8Vw-RxhJa", "original": null, "number": 11, "cdate": 1606298867757, "ddate": null, "tcdate": 1606298867757, "tmdate": 1606298867757, "tddate": null, "forum": "NUCZeoVlAe", "replyto": "NUCZeoVlAe", "invitation": "ICLR.cc/2021/Conference/Paper1111/-/Official_Comment", "content": {"title": "Summary of Changes (Part 1)", "comment": "We thank AC\u2019s efforts in organizing the reviews. Many thanks for reviewers. We do appreciate your efforts in helping us improve the manuscript. Reviewers raised major concerns from the following perspectives.\n\n1.\t**Visualization of P-vector.** In the revised manuscript, we include two examples to visualize P-vectors using CIFAR-10 and CIFAR-100 datasets. Specifically, P-vector is an extremely high-dimensional unit vector, and every sample refers to a dimension in the vector. We are interested in the value on every dimension in the vector. We present the distribution of values in the P-vector in Figure 4 of Section 4 in Page 5, where we plot the smoothed probability density of the distributions obtained in various training epochs. In Appendix (A.6 in Page 14), we present the raw frequency of values in the P-vector. From these visualizations, we can clearly understand the shape that values in a P-vector distribute and how they change throughout the training procedure.\n\n2.\t**Experiment Settings and SupCon.** Actually, for the experiment results reported in Figures 1, 5, 6, 7 and 8, we carried out experiments using 5 independent trials with different random seeds and averaged the results. The SupCon\u2019s behavior is significantly different from another contrastive learning approach SimCLR, as it incorporates both self-supervised (contrastive) and supervised learning loss. To address your comments, we have included the introduction to the experiment settings in the caption of Figure 1.\n\n3.\t**Definition of Convergence.** In our experiments, no model\u2019s P-vector angles could strictly converge to zero. In this way, we lower tone to claim the converging trend of angles to smaller ones. Further, we define the phenomena of \u201cconvergence\u201d in the footnote of Page 3 in the revised manuscript, where we clearly state that \u201cIn our research, we name convergence as the decreasing trend of P-vector angles from a larger one to a smaller one (e.g., $10^\\circ$ for supervised CNN classifiers and SimCLR, $<10^\\circ$ for ConvAEs/DenoiseAEs, and $30^\\circ$ for SupCon) over training epochs. For reference, $Cosine(10^\\circ)=0.985$ is close to 1.0.\n\n4.\t**Why P-vector could be used to predict performance.** All reviewers seem interested in the motivation that we correlate the angles between model and data P-vector and the performance of models. Especially, the angles are estimated using the training dataset without validation information, but they correlate to the testing accuracy. We are motivated by the earlier observations that well-trained DNN models are locally or piecewise linear (Zhang & Wu, 2020; Arora et al, 2018). Considering the DNN model as a mixture of linear and nonlinear transform, when the linear part is significant in the output of feature extractor, the subspace of feature matrix should be close to the subspace of data matrix. In the original submission, we introduce our intuitions in paragraph of \u201cHypothesis\u201d in Page 1. To further address your concerns, we elaborate our intuition in Section 5 of Page 8 in the revised manuscript and discuss this issue again in the Conclusion.\n\n5.\t**Triviality/significance of P-vectors.** We appreciate reviewers\u2019 comments to help us rethink more about the capacity of P-vectors. Under the guidance, we carried out the Singular Value Decomposition (SVD) Analysis on the feature matrices obtained from ResNet-50 using both CIFAR-10 and CIFAR-100 datasets. The results are presented in Figure 3 of Page 5 in the revised manuscript, where we present the distributions of singular values and how the distribution changes throughout the training procedure. Cliff patterns have been observed in the singular value distributions. Only few (less than 10) singular values are significant, while the rest are all close to zero. We also carried out explained variance analysis. It shows that P-vector and the corresponding top right singular take 50% of total variances, while the second top singular vectors take less than 10%. These evidences suggest that the P-vector could represent the subspace of features learned. Furthermore, in Appendix (A.7), we also include the analysis results using top-2, 3, 4, 5, and 6 left singular vectors, no converging trends could be observed when comparing these vectors in every training epoch and well-trained ones. Many thanks for the comments. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1111/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1111/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Empirical Studies on the Convergence of Feature Spaces in Deep Learning", "authorids": ["~Haoran_Liu2", "~Haoyi_Xiong1", "~Yaqing_Wang2", "~Haozhe_An1", "~Dongrui_Wu1", "~Dejing_Dou1"], "authors": ["Haoran Liu", "Haoyi Xiong", "Yaqing Wang", "Haozhe An", "Dongrui Wu", "Dejing Dou"], "keywords": [], "abstract": "While deep learning is effective to learn features/representations from data, the distributions of samples in feature spaces learned by various architectures for different training tasks (e.g., latent layers of AEs and feature vectors in CNN classifiers) have not been well-studied or compared. We hypothesize that the feature spaces of networks trained by various architectures (AEs or CNNs) and tasks (supervised, unsupervised, or self-supervised learning) share some common subspaces, no matter what types of DNN architectures or whether the labels have been used in feature learning. To test our hypothesis, through Singular Value Decomposition (SVD) of feature vectors, we demonstrate that one could linearly project the feature vectors of the same group of samples to a similar distribution, where the distribution is represented as the top left singular vector (i.e., principal subspace of feature vectors), namely $\\mathcal{P}$-vectors. We further assess the convergence of feature space learning using angles between $\\mathcal{P}$-vectors obtained from the well-trained model and its checkpoint per epoch during the learning procedure, where a quasi-monotonic trend of convergence to small angles has been observed. Finally, we carry out case studies to connect $\\mathcal{P}$-vectors to the data distribution, and generalization performance. Extensive experiments with practically-used MLP, AE and CNN architectures for classification, image reconstruction, and self-supervised learning tasks on MNIST, CIFAR-10 and CIFAR-100 datasets have been done to support our claims with solid evidences.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "liu|empirical_studies_on_the_convergence_of_feature_spaces_in_deep_learning", "supplementary_material": "", "pdf": "/pdf/2b195c6847605c50ea00784757198719861afaf3.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=iphj4u5v3", "_bibtex": "@misc{\nliu2021empirical,\ntitle={Empirical Studies on the Convergence of Feature Spaces in Deep Learning},\nauthor={Haoran Liu and Haoyi Xiong and Yaqing Wang and Haozhe An and Dongrui Wu and Dejing Dou},\nyear={2021},\nurl={https://openreview.net/forum?id=NUCZeoVlAe}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "NUCZeoVlAe", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1111/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1111/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1111/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1111/Authors|ICLR.cc/2021/Conference/Paper1111/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1111/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923863553, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1111/-/Official_Comment"}}}, {"id": "aLS5n9wxNWU", "original": null, "number": 10, "cdate": 1606298767908, "ddate": null, "tcdate": 1606298767908, "tmdate": 1606298767908, "tddate": null, "forum": "NUCZeoVlAe", "replyto": "NUCZeoVlAe", "invitation": "ICLR.cc/2021/Conference/Paper1111/-/Official_Comment", "content": {"title": "Summary of Changes (Part 2)", "comment": "6.\t**Correlation and Significance Test.** In fact, to avoid the dominance of some outliers in the correlation study, we performed the significance test using Spearman\u2019s correlation coefficients, where we majorly correlate the rank of model performance and the rank of P-vector angle among a large number of trained models. To make it even clearer, we also analyze the correlation in logarithmic scale i.e., \u201caccuracy (log) versus P-vector angles (log)\u201d. As the Spearman\u2019s correlations (i.e., rank information) are used, the results of significance tests are the same. We truly understand that it is inappropriate the compare the exact values of two correlation coefficients or P-values here, as the numbers of models in comparisons here are still small. Thus, we revise Section 5 of Page 8 in the revised manuscript to address the concerns and include the log-log plots in Appendix (A.8 of Page 16 in the revised manuscript). \n\n7.\t**Language and figure issues.** Many thanks for pointing out the language issues, such as the use acronyms (such as \u201cAE\u201d or \u201cCNN\u201d) before defining them and inappropriate use of x-axis titles (e.g., epoch\uf0e8iteration). We have fixed them and will continue improving the writing of this manuscript.\n\nMany thanks for Reviewers\u2019 time and patience in reviewing this paper and shepherding us to improve the manuscript. It is quite an enjoyable period for us to discuss and receive instructions from reviewers in these days. We have uploaded a revision to the manuscript. Hopefully, this manuscript could receive your full consideration for acceptance. Any help from you is highly appreciated. Thank you!\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1111/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1111/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Empirical Studies on the Convergence of Feature Spaces in Deep Learning", "authorids": ["~Haoran_Liu2", "~Haoyi_Xiong1", "~Yaqing_Wang2", "~Haozhe_An1", "~Dongrui_Wu1", "~Dejing_Dou1"], "authors": ["Haoran Liu", "Haoyi Xiong", "Yaqing Wang", "Haozhe An", "Dongrui Wu", "Dejing Dou"], "keywords": [], "abstract": "While deep learning is effective to learn features/representations from data, the distributions of samples in feature spaces learned by various architectures for different training tasks (e.g., latent layers of AEs and feature vectors in CNN classifiers) have not been well-studied or compared. We hypothesize that the feature spaces of networks trained by various architectures (AEs or CNNs) and tasks (supervised, unsupervised, or self-supervised learning) share some common subspaces, no matter what types of DNN architectures or whether the labels have been used in feature learning. To test our hypothesis, through Singular Value Decomposition (SVD) of feature vectors, we demonstrate that one could linearly project the feature vectors of the same group of samples to a similar distribution, where the distribution is represented as the top left singular vector (i.e., principal subspace of feature vectors), namely $\\mathcal{P}$-vectors. We further assess the convergence of feature space learning using angles between $\\mathcal{P}$-vectors obtained from the well-trained model and its checkpoint per epoch during the learning procedure, where a quasi-monotonic trend of convergence to small angles has been observed. Finally, we carry out case studies to connect $\\mathcal{P}$-vectors to the data distribution, and generalization performance. Extensive experiments with practically-used MLP, AE and CNN architectures for classification, image reconstruction, and self-supervised learning tasks on MNIST, CIFAR-10 and CIFAR-100 datasets have been done to support our claims with solid evidences.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "liu|empirical_studies_on_the_convergence_of_feature_spaces_in_deep_learning", "supplementary_material": "", "pdf": "/pdf/2b195c6847605c50ea00784757198719861afaf3.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=iphj4u5v3", "_bibtex": "@misc{\nliu2021empirical,\ntitle={Empirical Studies on the Convergence of Feature Spaces in Deep Learning},\nauthor={Haoran Liu and Haoyi Xiong and Yaqing Wang and Haozhe An and Dongrui Wu and Dejing Dou},\nyear={2021},\nurl={https://openreview.net/forum?id=NUCZeoVlAe}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "NUCZeoVlAe", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1111/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1111/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1111/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1111/Authors|ICLR.cc/2021/Conference/Paper1111/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1111/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923863553, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1111/-/Official_Comment"}}}, {"id": "OViIOgNJccn", "original": null, "number": 9, "cdate": 1605849378395, "ddate": null, "tcdate": 1605849378395, "tmdate": 1606287659052, "tddate": null, "forum": "NUCZeoVlAe", "replyto": "zR-sEWsqjQm", "invitation": "ICLR.cc/2021/Conference/Paper1111/-/Official_Comment", "content": {"title": "Additional Response to ICLR 2021 Conference Paper1111 AnonReviewer4", "comment": "Many thanks for your further comments. Sorry for the late reply.\n\n**Only P-vector converges in our empirical study.** You mentioned that when the reconstruction error is not good enough according to different criterion, adding more singular vectors to the subspace will be necessary. Actually, we have conducted experiments using top-k left singular vectors (k=1 is P-vector). However, in contrast to P-vector, the angles between other top singular vectors do not show a trend of convergence in the training process. The angles between the top-k left singular vectors (>1) of the training and well-trained models are close to be orthogonal. Besides, the angles jitter intensively over the number of epochs in the training process, as shown in the revised appendix Fig.17.\n\nA possible explanation for the inconsistency of other top singular vectors is the \u201cmismatching of indices\u201d in the SVD of feature matrices. While the top-1 singular vectors always dominant the feature space, the 2nd, 3rd, \u2026 top singular vectors may have different permutations as the singular value distribution shifts and alters in the training process. Thus, adding more singular vectors to the measurement would be difficult (maybe using optimal transport is a solution to compare two sets of top singular vectors between models, however it is another topic of research). Therefore, only the P-vector is principal and definitely matched cross feature spaces and that is the reason why only P-vector is used in our empirical studies.\n\nThanks again for the comments. To address your concerns, we will include a discussion on the angles using different dimensions of top left singular vectors, in the formal revised manuscript. Again, please feel free to ask any more questions or make any more comments. It\u2019s our pleasure to have further discussions.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1111/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1111/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Empirical Studies on the Convergence of Feature Spaces in Deep Learning", "authorids": ["~Haoran_Liu2", "~Haoyi_Xiong1", "~Yaqing_Wang2", "~Haozhe_An1", "~Dongrui_Wu1", "~Dejing_Dou1"], "authors": ["Haoran Liu", "Haoyi Xiong", "Yaqing Wang", "Haozhe An", "Dongrui Wu", "Dejing Dou"], "keywords": [], "abstract": "While deep learning is effective to learn features/representations from data, the distributions of samples in feature spaces learned by various architectures for different training tasks (e.g., latent layers of AEs and feature vectors in CNN classifiers) have not been well-studied or compared. We hypothesize that the feature spaces of networks trained by various architectures (AEs or CNNs) and tasks (supervised, unsupervised, or self-supervised learning) share some common subspaces, no matter what types of DNN architectures or whether the labels have been used in feature learning. To test our hypothesis, through Singular Value Decomposition (SVD) of feature vectors, we demonstrate that one could linearly project the feature vectors of the same group of samples to a similar distribution, where the distribution is represented as the top left singular vector (i.e., principal subspace of feature vectors), namely $\\mathcal{P}$-vectors. We further assess the convergence of feature space learning using angles between $\\mathcal{P}$-vectors obtained from the well-trained model and its checkpoint per epoch during the learning procedure, where a quasi-monotonic trend of convergence to small angles has been observed. Finally, we carry out case studies to connect $\\mathcal{P}$-vectors to the data distribution, and generalization performance. Extensive experiments with practically-used MLP, AE and CNN architectures for classification, image reconstruction, and self-supervised learning tasks on MNIST, CIFAR-10 and CIFAR-100 datasets have been done to support our claims with solid evidences.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "liu|empirical_studies_on_the_convergence_of_feature_spaces_in_deep_learning", "supplementary_material": "", "pdf": "/pdf/2b195c6847605c50ea00784757198719861afaf3.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=iphj4u5v3", "_bibtex": "@misc{\nliu2021empirical,\ntitle={Empirical Studies on the Convergence of Feature Spaces in Deep Learning},\nauthor={Haoran Liu and Haoyi Xiong and Yaqing Wang and Haozhe An and Dongrui Wu and Dejing Dou},\nyear={2021},\nurl={https://openreview.net/forum?id=NUCZeoVlAe}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "NUCZeoVlAe", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1111/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1111/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1111/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1111/Authors|ICLR.cc/2021/Conference/Paper1111/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1111/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923863553, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1111/-/Official_Comment"}}}, {"id": "zR-sEWsqjQm", "original": null, "number": 8, "cdate": 1605749178215, "ddate": null, "tcdate": 1605749178215, "tmdate": 1605779710650, "tddate": null, "forum": "NUCZeoVlAe", "replyto": "YUkxmtV0YRB", "invitation": "ICLR.cc/2021/Conference/Paper1111/-/Official_Comment", "content": {"title": "Thanks for the additional analysis", "comment": "Thanks for the additional analysis on the reconstruction error. From Fig. 16 in the appendix (ignoring the data point at epoch 0), it seems that as the epoch increases, the weight of the matrix is spread to more dimensions, because the reconstruction error using the top-1 singular vector slightly increases. I agree it's not easy to say the top-1 singular vector is trivial or not. As mentioned in my original comment, you have the choice to define which level of reconstruction error works for you (i.e. how much variance you want to capture in the principal subspace, which in general has more than 1 dimensions). For example, maybe you believe $E < 0.6 * \\|X\\|_F^2$ is good enough, then the top 1-dimensional principal subspace is good to use; if you prefer to have $E < 0.2 * \\|X\\|_F^2$, adding more singular vectors to the subspace will be necessary. Essentially it will be good to quantify how much variance the principal subspace captures, and the principal subspaces capturing the same percent of variance of different feature matrices will be made comparable. Then the scope of this work can be accurate. Looking forward to the discussion of measuring the angle between principal subspaces of general dimensions.\n\nBy the way, for Fig. 15 it may give more insight by normalizing the singular values: $\\sigma_i / \\sqrt{\\sum_j{\\sigma_j^2}} = \\sigma_i / \\|X\\|_F$, because $\\|X\\|_F$ of feature matrix $X$ increases during training, empirically.\n\nThanks for the explanation of local linearity. It makes sense to me now."}, "signatures": ["ICLR.cc/2021/Conference/Paper1111/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1111/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Empirical Studies on the Convergence of Feature Spaces in Deep Learning", "authorids": ["~Haoran_Liu2", "~Haoyi_Xiong1", "~Yaqing_Wang2", "~Haozhe_An1", "~Dongrui_Wu1", "~Dejing_Dou1"], "authors": ["Haoran Liu", "Haoyi Xiong", "Yaqing Wang", "Haozhe An", "Dongrui Wu", "Dejing Dou"], "keywords": [], "abstract": "While deep learning is effective to learn features/representations from data, the distributions of samples in feature spaces learned by various architectures for different training tasks (e.g., latent layers of AEs and feature vectors in CNN classifiers) have not been well-studied or compared. We hypothesize that the feature spaces of networks trained by various architectures (AEs or CNNs) and tasks (supervised, unsupervised, or self-supervised learning) share some common subspaces, no matter what types of DNN architectures or whether the labels have been used in feature learning. To test our hypothesis, through Singular Value Decomposition (SVD) of feature vectors, we demonstrate that one could linearly project the feature vectors of the same group of samples to a similar distribution, where the distribution is represented as the top left singular vector (i.e., principal subspace of feature vectors), namely $\\mathcal{P}$-vectors. We further assess the convergence of feature space learning using angles between $\\mathcal{P}$-vectors obtained from the well-trained model and its checkpoint per epoch during the learning procedure, where a quasi-monotonic trend of convergence to small angles has been observed. Finally, we carry out case studies to connect $\\mathcal{P}$-vectors to the data distribution, and generalization performance. Extensive experiments with practically-used MLP, AE and CNN architectures for classification, image reconstruction, and self-supervised learning tasks on MNIST, CIFAR-10 and CIFAR-100 datasets have been done to support our claims with solid evidences.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "liu|empirical_studies_on_the_convergence_of_feature_spaces_in_deep_learning", "supplementary_material": "", "pdf": "/pdf/2b195c6847605c50ea00784757198719861afaf3.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=iphj4u5v3", "_bibtex": "@misc{\nliu2021empirical,\ntitle={Empirical Studies on the Convergence of Feature Spaces in Deep Learning},\nauthor={Haoran Liu and Haoyi Xiong and Yaqing Wang and Haozhe An and Dongrui Wu and Dejing Dou},\nyear={2021},\nurl={https://openreview.net/forum?id=NUCZeoVlAe}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "NUCZeoVlAe", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1111/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1111/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1111/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1111/Authors|ICLR.cc/2021/Conference/Paper1111/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1111/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923863553, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1111/-/Official_Comment"}}}, {"id": "q2P2UHNEiXH", "original": null, "number": 4, "cdate": 1605696198551, "ddate": null, "tcdate": 1605696198551, "tmdate": 1605699254134, "tddate": null, "forum": "NUCZeoVlAe", "replyto": "-aZ-tFnQdRp", "invitation": "ICLR.cc/2021/Conference/Paper1111/-/Official_Comment", "content": {"title": "Response to ICLR 2021 Conference Paper1111 AnonReviewer3 Continued", "comment": "Many thanks for your comments on the definition of \u201cconvergence\u201d. We agree with you that the angles between P-vectors here usually cannot converge to zero after 200 epochs training, and the angles remaining are around 10$^\\circ$ degrees.  Actually, we use top left singular vector of the feature matrix (i.e., #samples $\\times$ #features) as the P-vector. Thus, the number of dimensions of a P-vector is equivalent to the number of samples in a dataset. When we use CIFAR-10 or CIFAR-100 for experiments, the P-vectors should be with 50,000 dimensions. Please note that the high-dimensional random vectors are tending to orthogonal when they are not correlated. In our opinion, 10$^\\circ$ degree is indeed quite small in such case, compared to the 80$^\\circ$ to 90$^\\circ$ in the begin of training procedure. Furthermore, cosine(10$^\\circ$) =0.985, which is quite significant in similarity comparison. Thus, we believe the remaining angles would not hurt our claims. To address your comments, we will include a discussion on the definition of \u201cconvergence\u201d in the formal revised manuscript. \n\nMany thanks for your comments on the relation between P-vector and \u201cgeneralization\u201d. Indeed, we estimate the top left singular vector of the raw data matrix as the data P-vector and use the angle between the data P-vector and the P-vector (obtained from the feature matrix of a model) to predict performance of the model. A small angle between the data P-vector and the P-vector well demonstrates the correlation or divergence between the distribution of samples in the raw dataset and the distribution of samples in the feature space, through comparing their principal subspaces. Our intuition is that when the principal subspace of the raw dataset is close to the feature one, the CNN model would preserve more information about the data distribution (even though the models are intensively parameterized and trained), demonstrate higher linearity (as the principal subspaces of features should equivalent to the principal subspace of raw data when only linear transform applied to the data), are supposed to be with better generalization performance. We are inspired by some work on the local linearity of CNN, such as [1] and [2]. After all, we only hope to report our observations as a potential application of P-vector, while we don\u2019t intend to over-claim the effectiveness of P-vector for model selection and/or generalization prediction. To address your comments, we will discuss these issues in the formal revised manuscript. \n\nPlease check the appendix of current modified manuscript for additional examples, observations, and evidences. Again, many thanks for your review and encouraging comments. We will address all your concerns and fix language issues in the revised version. Please feel free to comment on the thread of discussion timely and shepherd us for improving the manuscript. \n\n[1] Zhang X, Wu D. Empirical Studies on the Properties of Linear Regions in Deep Neural Networks[C]//International Conference on Learning Representations. 2019.\n\n[2] Arora R, Basu A, Mianjy P, et al. Understanding Deep Neural Networks with Rectified Linear Units[C]//International Conference on Learning Representations. 2018."}, "signatures": ["ICLR.cc/2021/Conference/Paper1111/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1111/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Empirical Studies on the Convergence of Feature Spaces in Deep Learning", "authorids": ["~Haoran_Liu2", "~Haoyi_Xiong1", "~Yaqing_Wang2", "~Haozhe_An1", "~Dongrui_Wu1", "~Dejing_Dou1"], "authors": ["Haoran Liu", "Haoyi Xiong", "Yaqing Wang", "Haozhe An", "Dongrui Wu", "Dejing Dou"], "keywords": [], "abstract": "While deep learning is effective to learn features/representations from data, the distributions of samples in feature spaces learned by various architectures for different training tasks (e.g., latent layers of AEs and feature vectors in CNN classifiers) have not been well-studied or compared. We hypothesize that the feature spaces of networks trained by various architectures (AEs or CNNs) and tasks (supervised, unsupervised, or self-supervised learning) share some common subspaces, no matter what types of DNN architectures or whether the labels have been used in feature learning. To test our hypothesis, through Singular Value Decomposition (SVD) of feature vectors, we demonstrate that one could linearly project the feature vectors of the same group of samples to a similar distribution, where the distribution is represented as the top left singular vector (i.e., principal subspace of feature vectors), namely $\\mathcal{P}$-vectors. We further assess the convergence of feature space learning using angles between $\\mathcal{P}$-vectors obtained from the well-trained model and its checkpoint per epoch during the learning procedure, where a quasi-monotonic trend of convergence to small angles has been observed. Finally, we carry out case studies to connect $\\mathcal{P}$-vectors to the data distribution, and generalization performance. Extensive experiments with practically-used MLP, AE and CNN architectures for classification, image reconstruction, and self-supervised learning tasks on MNIST, CIFAR-10 and CIFAR-100 datasets have been done to support our claims with solid evidences.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "liu|empirical_studies_on_the_convergence_of_feature_spaces_in_deep_learning", "supplementary_material": "", "pdf": "/pdf/2b195c6847605c50ea00784757198719861afaf3.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=iphj4u5v3", "_bibtex": "@misc{\nliu2021empirical,\ntitle={Empirical Studies on the Convergence of Feature Spaces in Deep Learning},\nauthor={Haoran Liu and Haoyi Xiong and Yaqing Wang and Haozhe An and Dongrui Wu and Dejing Dou},\nyear={2021},\nurl={https://openreview.net/forum?id=NUCZeoVlAe}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "NUCZeoVlAe", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1111/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1111/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1111/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1111/Authors|ICLR.cc/2021/Conference/Paper1111/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1111/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923863553, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1111/-/Official_Comment"}}}, {"id": "Hot7SNgnjw", "original": null, "number": 3, "cdate": 1605695305714, "ddate": null, "tcdate": 1605695305714, "tmdate": 1605698870148, "tddate": null, "forum": "NUCZeoVlAe", "replyto": "3qH-tbIEh4N", "invitation": "ICLR.cc/2021/Conference/Paper1111/-/Official_Comment", "content": {"title": "Response to ICLR 2021 Conference Paper1111 AnonReviewer1", "comment": "Many thanks for the review and encouraging comments. Actually, we have upload a modified version of manuscript, with a new appendix including additional figures/tables/evidences to address some of your concerns. We are working hard to revise the manuscript accordingly to address all your concerns and fix all language and presentation issues. For the formal revised manuscript, we will upload it before the end of rebuttal period. \n\nActually, in P-vector computation, we use the index of every sample in the training/test datasets as the indices of dimensions in the P-vector. In appendix of the current modified manuscript, we have provided the visualization of the P-vector, including a figure on the P-vector values versus the indices of samples, figures on the frequency of the P-vector values (counts versus the P-vector values), and a figure on the smoothed density of the P-vector values (probability density versus the -vector values). The feature matrices are obtained through training a ResNet50 model using CIFAR-10 datasets, the feature matrices of 0th epoch (obtained by random initial weights), 60th epoch, 120th epoch, and 200th epoch (well-trained) have been used for plots. In the formal revised manuscript, we will include all results based on the three datasets. \n\nSupCon refers to the supervised contrastive learning [1]. We include SupCon together with SimCLR [2] as two typical algorithms for self-supervised training. Generally, self-supervised learning, especially SimCLR [2], aims at improving the training procedure of deep neural networks with self-supervised contrastive loss.  While SimCLR [2] trains CNN feature extractors without the use of label information, SupCon [1] extended the self-supervised paradigm using supervised contrastive learning (which is based on the labels). To make it clear, in the revised manuscript, we will address this issue when we introduce the self-supervised learning paradigms.\n\nWe didn\u2019t incorporate any algorithms, regularizers, or any treatments to make the angles between P-vectors smaller. All algorithms used for DNN training here were based on the open source implementations that are available online. The goal of our research is to investigate the similarity between P-vectors for DNN trained using the same dataset with various architectures/tasks. All experiments were carried out to follow the standard implementation and operations.\n\nAgain, many thanks for your review and encouraging comments. We will address all your concerns and fix language issues in the revised version. Please feel free to comment on the thread of discussion and timely shepherd us for improving the manuscript. \n\n[1] Khosla P, Teterwak P, Wang C, et al. Supervised contrastive learning[J]. arXiv preprint arXiv:2004.11362, 2020.\n\n[2] Chen T, Kornblith S, Norouzi M, et al. A simple framework for contrastive learning of visual representations[J]. arXiv preprint arXiv:2002.05709, 2020. MLA"}, "signatures": ["ICLR.cc/2021/Conference/Paper1111/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1111/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Empirical Studies on the Convergence of Feature Spaces in Deep Learning", "authorids": ["~Haoran_Liu2", "~Haoyi_Xiong1", "~Yaqing_Wang2", "~Haozhe_An1", "~Dongrui_Wu1", "~Dejing_Dou1"], "authors": ["Haoran Liu", "Haoyi Xiong", "Yaqing Wang", "Haozhe An", "Dongrui Wu", "Dejing Dou"], "keywords": [], "abstract": "While deep learning is effective to learn features/representations from data, the distributions of samples in feature spaces learned by various architectures for different training tasks (e.g., latent layers of AEs and feature vectors in CNN classifiers) have not been well-studied or compared. We hypothesize that the feature spaces of networks trained by various architectures (AEs or CNNs) and tasks (supervised, unsupervised, or self-supervised learning) share some common subspaces, no matter what types of DNN architectures or whether the labels have been used in feature learning. To test our hypothesis, through Singular Value Decomposition (SVD) of feature vectors, we demonstrate that one could linearly project the feature vectors of the same group of samples to a similar distribution, where the distribution is represented as the top left singular vector (i.e., principal subspace of feature vectors), namely $\\mathcal{P}$-vectors. We further assess the convergence of feature space learning using angles between $\\mathcal{P}$-vectors obtained from the well-trained model and its checkpoint per epoch during the learning procedure, where a quasi-monotonic trend of convergence to small angles has been observed. Finally, we carry out case studies to connect $\\mathcal{P}$-vectors to the data distribution, and generalization performance. Extensive experiments with practically-used MLP, AE and CNN architectures for classification, image reconstruction, and self-supervised learning tasks on MNIST, CIFAR-10 and CIFAR-100 datasets have been done to support our claims with solid evidences.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "liu|empirical_studies_on_the_convergence_of_feature_spaces_in_deep_learning", "supplementary_material": "", "pdf": "/pdf/2b195c6847605c50ea00784757198719861afaf3.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=iphj4u5v3", "_bibtex": "@misc{\nliu2021empirical,\ntitle={Empirical Studies on the Convergence of Feature Spaces in Deep Learning},\nauthor={Haoran Liu and Haoyi Xiong and Yaqing Wang and Haozhe An and Dongrui Wu and Dejing Dou},\nyear={2021},\nurl={https://openreview.net/forum?id=NUCZeoVlAe}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "NUCZeoVlAe", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1111/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1111/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1111/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1111/Authors|ICLR.cc/2021/Conference/Paper1111/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1111/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923863553, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1111/-/Official_Comment"}}}, {"id": "YUkxmtV0YRB", "original": null, "number": 6, "cdate": 1605696968663, "ddate": null, "tcdate": 1605696968663, "tmdate": 1605698664078, "tddate": null, "forum": "NUCZeoVlAe", "replyto": "lRdMrcaXwmR", "invitation": "ICLR.cc/2021/Conference/Paper1111/-/Official_Comment", "content": {"title": "Response to ICLR 2021 Conference Paper1111 AnonReviewer4 Continued", "comment": "Even you conclude the P-vector is trivial upon the additional results above and figures in the appendix, we still make contribution in this work\u2013it might be the first evidence on the triviality of principal subspace of features learned by DNN or well-trained DNNs would converge to have a trivial principal subspace in their feature learning. Many thanks for mention the existing work in measuring the angle between principal subspaces. We will include these work in the discussion. \n\nWe really appreciate your guidance here. We will significantly extend the methodology section to first analyze the singular value distribution of feature matrices, explained variances, and the reconstruction error of approximation using the top-k singular vectors, prior to introducing our observations! Many thanks!\n\nExplained Variances of top-k singular vectors:\n\n|  k   | Ratio |\n|  ----  | ----  |\n| 1  | 0.5674805 |\n| 2  | 0.6489089 |\n|3 | 0.7067079|\n|4 | 0.75878125|\n|5 | 0.8063708|\n|6 | 0.8489084|\n|7 | 0.8891225|\n|8 | 0.92527133|\n|9 | 0.9577149|\n|10 | 0.98870885|\n|11 | 0.9896392|\n|12 | 0.9902976|\n\nMany thanks for your advices. We will lower our tone in the formal revised manuscript. Indeed, cosin(10$^\\circ$)=98.5\\% and cosin(20$^\\circ$)= 94.0\\%, are quite small when we treat the cosine measure in a correlation sense. All in all, in our formal revised manuscript, we will conclude the \u201cconverging trends\u201d of the angle between P-vectors, and emphasized that it would not converge to zero, but to a much smaller angle.\n\nMany thanks for your question on the correlation between features and raw data in their linear principal subspace. First of all, this is just our empirical observation and we report it. Latter, to avoid possible affects due to the outliers, we tested the hypothesis using Spearman\u2019s correlation to correlate the angles and training/testing accuracy, which considers the order of the data point in the samples rather than the value of them. In this way, few outliers would not dominate the correlation analysis. In the current modified manuscript, we take the log-log plots, where we can see the same phenomena. Thus, we don\u2019t believe a few outliers would affect our conclusion here. Though the overall DNN is nonlinear, many previous work [1],[2] also demonstrates the local linearity or piecewise linearity of DNN with certain activations. We believe it is the linear subcomponents of the DNN (e.g., a significant first-order term in the Taylor expansion) that \u201cleaks\u201d certain information about the raw data through a weak linear transform. In this way, the correlation between features and raw data in the linear principal subspace characterizes \u201chow linear is the feature extractor of a DNN model\u201d which should relates to the (generalization) performance.  Of-course, it is all of our intuition. We didn\u2019t hope to claim it in the manuscript. To address your comment, we will discuss this issue in the formal revised manuscript to elaborate our intuition well.\n\nWe really appreciate your comments. We hope to could get a chance to be shepherded and improve the manuscript. Please feel free to comment on the discussion thread, especially when we made anything wrong. Many thanks!\n\n[1] Zhang X, Wu D. Empirical Studies on the Properties of Linear Regions in Deep Neural Networks[C]//International Conference on Learning Representations. 2019.\n\n[2] Arora R, Basu A, Mianjy P, et al. Understanding Deep Neural Networks with Rectified Linear Units[C]//International Conference on Learning Representations. 2018.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1111/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1111/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Empirical Studies on the Convergence of Feature Spaces in Deep Learning", "authorids": ["~Haoran_Liu2", "~Haoyi_Xiong1", "~Yaqing_Wang2", "~Haozhe_An1", "~Dongrui_Wu1", "~Dejing_Dou1"], "authors": ["Haoran Liu", "Haoyi Xiong", "Yaqing Wang", "Haozhe An", "Dongrui Wu", "Dejing Dou"], "keywords": [], "abstract": "While deep learning is effective to learn features/representations from data, the distributions of samples in feature spaces learned by various architectures for different training tasks (e.g., latent layers of AEs and feature vectors in CNN classifiers) have not been well-studied or compared. We hypothesize that the feature spaces of networks trained by various architectures (AEs or CNNs) and tasks (supervised, unsupervised, or self-supervised learning) share some common subspaces, no matter what types of DNN architectures or whether the labels have been used in feature learning. To test our hypothesis, through Singular Value Decomposition (SVD) of feature vectors, we demonstrate that one could linearly project the feature vectors of the same group of samples to a similar distribution, where the distribution is represented as the top left singular vector (i.e., principal subspace of feature vectors), namely $\\mathcal{P}$-vectors. We further assess the convergence of feature space learning using angles between $\\mathcal{P}$-vectors obtained from the well-trained model and its checkpoint per epoch during the learning procedure, where a quasi-monotonic trend of convergence to small angles has been observed. Finally, we carry out case studies to connect $\\mathcal{P}$-vectors to the data distribution, and generalization performance. Extensive experiments with practically-used MLP, AE and CNN architectures for classification, image reconstruction, and self-supervised learning tasks on MNIST, CIFAR-10 and CIFAR-100 datasets have been done to support our claims with solid evidences.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "liu|empirical_studies_on_the_convergence_of_feature_spaces_in_deep_learning", "supplementary_material": "", "pdf": "/pdf/2b195c6847605c50ea00784757198719861afaf3.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=iphj4u5v3", "_bibtex": "@misc{\nliu2021empirical,\ntitle={Empirical Studies on the Convergence of Feature Spaces in Deep Learning},\nauthor={Haoran Liu and Haoyi Xiong and Yaqing Wang and Haozhe An and Dongrui Wu and Dejing Dou},\nyear={2021},\nurl={https://openreview.net/forum?id=NUCZeoVlAe}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "NUCZeoVlAe", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1111/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1111/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1111/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1111/Authors|ICLR.cc/2021/Conference/Paper1111/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1111/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923863553, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1111/-/Official_Comment"}}}, {"id": "KSs4PivryB1", "original": null, "number": 5, "cdate": 1605696268387, "ddate": null, "tcdate": 1605696268387, "tmdate": 1605697262625, "tddate": null, "forum": "NUCZeoVlAe", "replyto": "-aZ-tFnQdRp", "invitation": "ICLR.cc/2021/Conference/Paper1111/-/Official_Comment", "content": {"title": "Response to ICLR 2021 Conference Paper1111 AnonReviewer3", "comment": "Many thanks for the review and encouraging comments. Actually, we have upload a modified version of the manuscript, with a new appendix including additional figures/tables/evidences to address some of your concerns. We are working hard to revise the manuscript accordingly to address all your concerns and fix all language and presentation issues. For the formal revised manuscript, we will upload it before the end of rebuttal period. \n\nThe P-vector, as the top left singular vector of the feature matrix (#samples $\\times$ #features), indicates the principal subspace of the samples encoded by CNN features (namely feature vectors of the samples). According to the definition of SVD and PCA, in our settings, the principal component (top-1) projects the feature vector of every sample into a 1-dimesnional principal subspace that the variance of these data points is maximized. The value of every sample in the P-vector here refers to the position of the sample in such 1-dimensional space. In this way, we believe the principal subspace characterizes the distribution of the samples in the feature space, and the P-vector represents such distribution (at least partially). Thus, the comparison between the P-vectors is meaningful.  In addition, though P-vector only takes the top-1 left singular vector of the feature matrix, comparisons between the P-vectors avoid a lot of technical problems, such as dimensionality mismatching issues for CNN features from different architectures. To address your concerns, we will discuss this part with running examples in the formal revised manuscript. \n\nIn Figure 1(a)\u2014(b), to alleviate the influence of random initialization, we carry out and repeat the experiments using 5 random seeds independently. In every experiment, we train the DNN models of various architectures, and compare the P-vectors between any two architectures using cosine measure. Finally, we average the cosine measures obtained in the 5 independent trials. Many thanks for your suggestion, we will include the statistics on mean and variance. Furthermore, our experiments half dozens of architectures, three datasets of various sizes, three learning tasks of various losses, and 5 random seeds, all based on default hyper-parameters/training settings, we have not found \u201cparameters such as the number of samples and dimensions of features affect Hypothesis I\u201d. To address your concerns, we will include a discussion on the internal and external threats to validity in the formal revised manuscript. \n\nMany thanks for your note on the jitters in the curve of Wide-ResNet28. In Figure 4 (a) -(c), we compare the P-vectors of various models obtained during the training procedure to the P-vector of the well-trained Wide-ResNet28 (200 epoch). We can see the curve of Wide-ResNet28 (P-vectors between the training Wide-ResNet28 versus well-trained Wide-ResNet28) with small jitters around 10$^\\circ$, while rest lines look smooth. Actually, similar observations could be obtained in the P-vector comparisons between the training Wide-ResNet28 and well-trained ConvAE (Figure 4 (d)) and the P-vector comparisons between the training Wide-ResNet28 and well-trained SimCLR (Figure 4 (g)). We indeed carried out the experiments using 5 random seeds independently. In every experiment, we obtain the curve of cosine measures between P-vectors of any two models and average the cosine measures to plot Figure 4. We can see the P-vectors of Wide-ResNet28 over the training epochs are with higher perturbations than others. We conclude it is a characteristic of Wide-ResNet28, as such architectures is wide, over-parameterized, and easy to be over-fitted. Actually, the CNN feature extractor of Wide-ResNet28 outputs feature vectors with 1280 dimensions, while other networks usually generate features with extremely less dimensions (e.g., 256 for ResNet). In this way, the feature vectors obtained by Wide-ResNet28 are easier to incorporate noise or redundant information during the training procedure. They are much more stochastic than other modes. Thus, even we compare the P-vector between the training and well-trained models of Wide-ResNet28, the difference might be higher than the rest models. To address your comments, we will discuss these issues in the formal revised manuscript. "}, "signatures": ["ICLR.cc/2021/Conference/Paper1111/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1111/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Empirical Studies on the Convergence of Feature Spaces in Deep Learning", "authorids": ["~Haoran_Liu2", "~Haoyi_Xiong1", "~Yaqing_Wang2", "~Haozhe_An1", "~Dongrui_Wu1", "~Dejing_Dou1"], "authors": ["Haoran Liu", "Haoyi Xiong", "Yaqing Wang", "Haozhe An", "Dongrui Wu", "Dejing Dou"], "keywords": [], "abstract": "While deep learning is effective to learn features/representations from data, the distributions of samples in feature spaces learned by various architectures for different training tasks (e.g., latent layers of AEs and feature vectors in CNN classifiers) have not been well-studied or compared. We hypothesize that the feature spaces of networks trained by various architectures (AEs or CNNs) and tasks (supervised, unsupervised, or self-supervised learning) share some common subspaces, no matter what types of DNN architectures or whether the labels have been used in feature learning. To test our hypothesis, through Singular Value Decomposition (SVD) of feature vectors, we demonstrate that one could linearly project the feature vectors of the same group of samples to a similar distribution, where the distribution is represented as the top left singular vector (i.e., principal subspace of feature vectors), namely $\\mathcal{P}$-vectors. We further assess the convergence of feature space learning using angles between $\\mathcal{P}$-vectors obtained from the well-trained model and its checkpoint per epoch during the learning procedure, where a quasi-monotonic trend of convergence to small angles has been observed. Finally, we carry out case studies to connect $\\mathcal{P}$-vectors to the data distribution, and generalization performance. Extensive experiments with practically-used MLP, AE and CNN architectures for classification, image reconstruction, and self-supervised learning tasks on MNIST, CIFAR-10 and CIFAR-100 datasets have been done to support our claims with solid evidences.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "liu|empirical_studies_on_the_convergence_of_feature_spaces_in_deep_learning", "supplementary_material": "", "pdf": "/pdf/2b195c6847605c50ea00784757198719861afaf3.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=iphj4u5v3", "_bibtex": "@misc{\nliu2021empirical,\ntitle={Empirical Studies on the Convergence of Feature Spaces in Deep Learning},\nauthor={Haoran Liu and Haoyi Xiong and Yaqing Wang and Haozhe An and Dongrui Wu and Dejing Dou},\nyear={2021},\nurl={https://openreview.net/forum?id=NUCZeoVlAe}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "NUCZeoVlAe", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1111/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1111/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1111/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1111/Authors|ICLR.cc/2021/Conference/Paper1111/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1111/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923863553, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1111/-/Official_Comment"}}}, {"id": "Dmuxy2kwrWP", "original": null, "number": 7, "cdate": 1605697018140, "ddate": null, "tcdate": 1605697018140, "tmdate": 1605697018140, "tddate": null, "forum": "NUCZeoVlAe", "replyto": "lRdMrcaXwmR", "invitation": "ICLR.cc/2021/Conference/Paper1111/-/Official_Comment", "content": {"title": "Response to ICLR 2021 Conference Paper1111 AnonReviewer4 ", "comment": "Many thanks for the review and constructive comments. We must say your comments are quite helpful to shepherd us for approaching a better work. Actually, we have upload a modified version of the manuscript, with a new appendix including additional figures/tables/evidences to address some of your concerns. We are working hard to revise the manuscript accordingly to address all your concerns and fix all language and presentation issues. For the formal revised manuscript, we will upload it before the end of rebuttal period. \n\nOn the triviality of the P-vector. Many thanks for your comments. We also have concerns on the validity of our observations. With your comments, we preformed SVD on the feature matrices (#samples $\\times$ #features = 50,000 $\\times$ 256) obtained using ResNet50 and CIFAR-10 data. We attached the result in appendix of current modified manuscript. The result shows the feature matrix is extremely spiked (with use log x-axis in the figure). Through comparing the singular value distributions obtained in different training epochs, we can observe the progression of the feature matrices during the training procedure. For both initial model and well-trained model, the most of energy is distributed in the first 10 singular vectors for the well-trained model. In the explained variance analysis, the variance ratio of top-10 singular vectors could achieve 98.8\\%, while the variance ratio of top-1 singular vectors (i.e., P-vector and the corresponding right singular vector) could achieve 56.8\\%. \n\nFurther, we follow your insightful instruction to analyze the low-rank approximation to the feature matrix using the principal singular vectors. We first estimate the reconstruction error of feature matrix using the P-vector and the corresponding top right singular vector with the singular value. We have included a new figure on reconstruction errors using P-vectors in the appendix of current modified manuscript. The reconstruction error grows from 40\\% to 60\\%, i.e., $E_1=0.4*|X|_F^2$ in the begin of training procedure while $E_1=0.6*|X|_F^2$ for well-trained one, using the single P-vector. It coincides our observation in the singular value distributions. After all, we are not an expert on the determining the triviality of singular vectors. However, no matter whether the P-vector is trivial or not, we believe (1) the P-vector well represents the feature matrix in a data-driven fashion, (2) the P-vector contains information about the distribution of samples in the feature space (see the explained variance analysis), and (3) the singular value distribution of the feature matrix would vary over the training epochs while the distribution would become flatten among the top-10 singular vectors in our experiments. Please advise us if we make anything wrong here. "}, "signatures": ["ICLR.cc/2021/Conference/Paper1111/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1111/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Empirical Studies on the Convergence of Feature Spaces in Deep Learning", "authorids": ["~Haoran_Liu2", "~Haoyi_Xiong1", "~Yaqing_Wang2", "~Haozhe_An1", "~Dongrui_Wu1", "~Dejing_Dou1"], "authors": ["Haoran Liu", "Haoyi Xiong", "Yaqing Wang", "Haozhe An", "Dongrui Wu", "Dejing Dou"], "keywords": [], "abstract": "While deep learning is effective to learn features/representations from data, the distributions of samples in feature spaces learned by various architectures for different training tasks (e.g., latent layers of AEs and feature vectors in CNN classifiers) have not been well-studied or compared. We hypothesize that the feature spaces of networks trained by various architectures (AEs or CNNs) and tasks (supervised, unsupervised, or self-supervised learning) share some common subspaces, no matter what types of DNN architectures or whether the labels have been used in feature learning. To test our hypothesis, through Singular Value Decomposition (SVD) of feature vectors, we demonstrate that one could linearly project the feature vectors of the same group of samples to a similar distribution, where the distribution is represented as the top left singular vector (i.e., principal subspace of feature vectors), namely $\\mathcal{P}$-vectors. We further assess the convergence of feature space learning using angles between $\\mathcal{P}$-vectors obtained from the well-trained model and its checkpoint per epoch during the learning procedure, where a quasi-monotonic trend of convergence to small angles has been observed. Finally, we carry out case studies to connect $\\mathcal{P}$-vectors to the data distribution, and generalization performance. Extensive experiments with practically-used MLP, AE and CNN architectures for classification, image reconstruction, and self-supervised learning tasks on MNIST, CIFAR-10 and CIFAR-100 datasets have been done to support our claims with solid evidences.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "liu|empirical_studies_on_the_convergence_of_feature_spaces_in_deep_learning", "supplementary_material": "", "pdf": "/pdf/2b195c6847605c50ea00784757198719861afaf3.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=iphj4u5v3", "_bibtex": "@misc{\nliu2021empirical,\ntitle={Empirical Studies on the Convergence of Feature Spaces in Deep Learning},\nauthor={Haoran Liu and Haoyi Xiong and Yaqing Wang and Haozhe An and Dongrui Wu and Dejing Dou},\nyear={2021},\nurl={https://openreview.net/forum?id=NUCZeoVlAe}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "NUCZeoVlAe", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1111/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1111/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1111/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1111/Authors|ICLR.cc/2021/Conference/Paper1111/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1111/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923863553, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1111/-/Official_Comment"}}}, {"id": "lRdMrcaXwmR", "original": null, "number": 1, "cdate": 1603672286587, "ddate": null, "tcdate": 1603672286587, "tmdate": 1605024527978, "tddate": null, "forum": "NUCZeoVlAe", "replyto": "NUCZeoVlAe", "invitation": "ICLR.cc/2021/Conference/Paper1111/-/Official_Review", "content": {"title": "Interesting hypothesis but results are not convincing enough", "review": "This paper studies the top singular vector of the feature space learned by supervised and unsupervised deep learning models on CIFAR datasets. The hypothesis of converging feature spaces is interesting (converging both in terms of different models, and in terms of training epochs), but the conclusion from the current experiment results is overstretching.\n\n1. While the authors emphasize the convergence of subspaces, the P-vector defined in the paper is actually the top singular vector of the feature space, so it's actually about the convergence of the 1-dimensional principal subspace. A subspace refers to an arbitrary dimensional space in general. In the context of SVD, the literature often studies the top-$k$ dimensional subspace, which is represented by the $k$ top singular vectors, and the approximation error of the top-$k$ dimensional subspace: $E=\\|X - U_k \\Sigma_k V_k^T\\|_F^2$, where $X$ would be the feature matrix in this paper, and $U_k, V_k$ are the first $k$ columns in the result of SVD. The authors didn't measure $E$, so the readers won't know how well the top-1 dimensional subspace represents the feature matrix. I recommend looking at $E$ as a function of $k$, and use some criteria to determine how closely you want the subspace to approximate the feature matrix. For example, we can say we want to keep the top-$k$ dimensional subspace such that $E < 0.1 \\|X\\|_F^2$. This way, you can rule out the possibility that the P-vector is a trivial vector that every model will converge to.\n(As an analogy for a trivial vector, we can consider the top-1 eigenvector of the similarity matrix defined in the classical spectral clustering method called Normalized Cut. No matter how the edge weights in a graph is defined, the similarity matrix used in Normalized Cut always has an all-one vector as the top-1 eigenvector.)\nAnd to measure the angle between general subspaces, many methods are available including classical ones (e.g. \u00c5ke Bj\u00f6rck and Gene H. Golub, Numerical Methods for Computing Angles Between Linear Subspaces, 1973).\n\n2. This paper tries to emphasize the P-vectors found in the features from different deep learning models are very close (for example, \"no matter what type of DNN architectures or whether the labels have been used to train the models, the P-vectors of different models would converge to the same one\"). Actually it seems the angle typically converges to 10 to 20 degrees. It may be better to lower the tone, or quantify better (compared to the angles obtained by ..., the angles between P-vectors are smaller).\n\n3. The data in Fig. 7 looks quite noisy, though p-value shows statistical significance of the correlation. p-value can guide our findings but is not always meaningful. For example, comparing Fig.7(e) and Fig.7(l), we may argue the latter has a better correlation but the former has a much smaller p-value. It seems the very small p-value in Fig.7(e) results from some outliers. Intuitively I don't quite understand why the raw data and the features should have a correlated linear principal subspace, given that the neural network layers that generate the feature from the data are highly nonlinear.\n\nThe only convincing data I found is in Table 1, which shows P-vectors can serve as an indicator of the model performance. But overall the readers would need more evidence as explained in #1 above.", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1111/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1111/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Empirical Studies on the Convergence of Feature Spaces in Deep Learning", "authorids": ["~Haoran_Liu2", "~Haoyi_Xiong1", "~Yaqing_Wang2", "~Haozhe_An1", "~Dongrui_Wu1", "~Dejing_Dou1"], "authors": ["Haoran Liu", "Haoyi Xiong", "Yaqing Wang", "Haozhe An", "Dongrui Wu", "Dejing Dou"], "keywords": [], "abstract": "While deep learning is effective to learn features/representations from data, the distributions of samples in feature spaces learned by various architectures for different training tasks (e.g., latent layers of AEs and feature vectors in CNN classifiers) have not been well-studied or compared. We hypothesize that the feature spaces of networks trained by various architectures (AEs or CNNs) and tasks (supervised, unsupervised, or self-supervised learning) share some common subspaces, no matter what types of DNN architectures or whether the labels have been used in feature learning. To test our hypothesis, through Singular Value Decomposition (SVD) of feature vectors, we demonstrate that one could linearly project the feature vectors of the same group of samples to a similar distribution, where the distribution is represented as the top left singular vector (i.e., principal subspace of feature vectors), namely $\\mathcal{P}$-vectors. We further assess the convergence of feature space learning using angles between $\\mathcal{P}$-vectors obtained from the well-trained model and its checkpoint per epoch during the learning procedure, where a quasi-monotonic trend of convergence to small angles has been observed. Finally, we carry out case studies to connect $\\mathcal{P}$-vectors to the data distribution, and generalization performance. Extensive experiments with practically-used MLP, AE and CNN architectures for classification, image reconstruction, and self-supervised learning tasks on MNIST, CIFAR-10 and CIFAR-100 datasets have been done to support our claims with solid evidences.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "liu|empirical_studies_on_the_convergence_of_feature_spaces_in_deep_learning", "supplementary_material": "", "pdf": "/pdf/2b195c6847605c50ea00784757198719861afaf3.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=iphj4u5v3", "_bibtex": "@misc{\nliu2021empirical,\ntitle={Empirical Studies on the Convergence of Feature Spaces in Deep Learning},\nauthor={Haoran Liu and Haoyi Xiong and Yaqing Wang and Haozhe An and Dongrui Wu and Dejing Dou},\nyear={2021},\nurl={https://openreview.net/forum?id=NUCZeoVlAe}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "NUCZeoVlAe", "replyto": "NUCZeoVlAe", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1111/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538126665, "tmdate": 1606915764425, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1111/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1111/-/Official_Review"}}}, {"id": "-aZ-tFnQdRp", "original": null, "number": 2, "cdate": 1604222903200, "ddate": null, "tcdate": 1604222903200, "tmdate": 1605024527914, "tddate": null, "forum": "NUCZeoVlAe", "replyto": "NUCZeoVlAe", "invitation": "ICLR.cc/2021/Conference/Paper1111/-/Official_Review", "content": {"title": "Marginally below acceptance threshold", "review": "Summary:\n\nThis paper has a closer look at the distributions of samples in the feature space by utilizing P-vector to analyze principal subspace. According to their empirical studies, the authors concluded that the feature spaces learned by different deep models with the same dataset would share common principal subspaces for the same dataset. It will not be affected by DNN architectures or the usage of labels in feature learning. Only the training procedure gradually shapes the feature subspace to the shared common subspace. \n\n-----------------------------------------------------------------------\n\nReasons for score: \n \nThe paper explores a new question and gives some interesting conclusions. But my major concern is its empirical studies cannot support the findings well. Besides, there are few discussions to provide the readers with some insights. I hope the authors carefully consider how to enhance this paper and make the conclusions more convincing.\n \n-----------------------------------------------------------------------\n\nPros: \n \n1. The paper explores a new question and gives some interesting conclusions.\n \n2. The proposed metric is simple and easy to follow. The authors also attached the source code for reference.\n\n3. The usage of the P-vector for predicting generalization achieves promising results.\n\n-----------------------------------------------------------------------\n\nCons: \n \n1. Why can the similarity of P-vectors be used to indicate the similarity of two distributions in the feature space?\n2. I would like to know how many trials it takes to plot similarity figures (e.g., Figure 1 (a)-(b)). It would be better to try many times and give the mean and variance to avoid coincidence. Besides, will other parameters such as the number of samples and dimensions of features affect Hypothesis I?\n3. The authors mentioned that the reference model used in Figure 4 (a)-(c) is Wide-ResNet28 trained with 200 epochs under suggest settings. But, the plot of Wide-ResNet28 in Figure 4 (a) is weird. It cannot converge to zero. \n4. I would like to know why most models (such as Figure 4) cannot converge to zero after about 200 epochs training, and the angles are approximately 10 degrees. In other words, is there exists a threshold after which we can think the compared two models have a common subspace? \n5. Why the P-vector can be used to predict the generalization?\n\n-----------------------------------------------------------------------\n\nQuestions during the rebuttal period: \n \nPlease address and clarify the cons above. \n \n-----------------------------------------------------------------------\n\nSome typos: \n(1) Figure 5, Figure 9, the xaxis's titile should be iterations rather than epochs.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1111/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1111/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Empirical Studies on the Convergence of Feature Spaces in Deep Learning", "authorids": ["~Haoran_Liu2", "~Haoyi_Xiong1", "~Yaqing_Wang2", "~Haozhe_An1", "~Dongrui_Wu1", "~Dejing_Dou1"], "authors": ["Haoran Liu", "Haoyi Xiong", "Yaqing Wang", "Haozhe An", "Dongrui Wu", "Dejing Dou"], "keywords": [], "abstract": "While deep learning is effective to learn features/representations from data, the distributions of samples in feature spaces learned by various architectures for different training tasks (e.g., latent layers of AEs and feature vectors in CNN classifiers) have not been well-studied or compared. We hypothesize that the feature spaces of networks trained by various architectures (AEs or CNNs) and tasks (supervised, unsupervised, or self-supervised learning) share some common subspaces, no matter what types of DNN architectures or whether the labels have been used in feature learning. To test our hypothesis, through Singular Value Decomposition (SVD) of feature vectors, we demonstrate that one could linearly project the feature vectors of the same group of samples to a similar distribution, where the distribution is represented as the top left singular vector (i.e., principal subspace of feature vectors), namely $\\mathcal{P}$-vectors. We further assess the convergence of feature space learning using angles between $\\mathcal{P}$-vectors obtained from the well-trained model and its checkpoint per epoch during the learning procedure, where a quasi-monotonic trend of convergence to small angles has been observed. Finally, we carry out case studies to connect $\\mathcal{P}$-vectors to the data distribution, and generalization performance. Extensive experiments with practically-used MLP, AE and CNN architectures for classification, image reconstruction, and self-supervised learning tasks on MNIST, CIFAR-10 and CIFAR-100 datasets have been done to support our claims with solid evidences.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "liu|empirical_studies_on_the_convergence_of_feature_spaces_in_deep_learning", "supplementary_material": "", "pdf": "/pdf/2b195c6847605c50ea00784757198719861afaf3.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=iphj4u5v3", "_bibtex": "@misc{\nliu2021empirical,\ntitle={Empirical Studies on the Convergence of Feature Spaces in Deep Learning},\nauthor={Haoran Liu and Haoyi Xiong and Yaqing Wang and Haozhe An and Dongrui Wu and Dejing Dou},\nyear={2021},\nurl={https://openreview.net/forum?id=NUCZeoVlAe}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "NUCZeoVlAe", "replyto": "NUCZeoVlAe", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1111/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538126665, "tmdate": 1606915764425, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1111/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1111/-/Official_Review"}}}, {"id": "3qH-tbIEh4N", "original": null, "number": 3, "cdate": 1604291817561, "ddate": null, "tcdate": 1604291817561, "tmdate": 1605024527845, "tddate": null, "forum": "NUCZeoVlAe", "replyto": "NUCZeoVlAe", "invitation": "ICLR.cc/2021/Conference/Paper1111/-/Official_Review", "content": {"title": "Interesting experiments into neural networks behaving similarly", "review": "The authors identify an interesting empirical phenomenon: across a range of network architectures and training approaches (supervised, unsupervised, auto-encoders), the feature spaces identified by these networks are similar. The authors introduce a specific way to summarize the feature space of a network as a vector (the top-left singular vector of the num_examples x num_features matrix) and show that these vectors are highly correlated across networks. In addition, the authors show that the features spaces become more similar throughout training and are predictive of the generalization performance of a neural network.\n\nThe paper presents purely experimental findings, but the experiments are sufficiently broad (e.g., covering different training approaches and datasets) so that this is not a shortcoming. Investigating potential theoretical models or more models and datasets could be fruitful directions for future work.\n\nAspects of the presentation in the paper could be improved (see the comments below). Overall I still recommend accepting the paper.\n\n\nAdditional comments:\n\n- Many plots have labels that are too small to read. I strongly encourage the authors to produce more readable plots.\n\n- Have the authors explored visualizations of the P-vectors? For instance, what is the ordering of training examples induced by the P-vectors?\n\n- What is the \"SupCon\" method? Do the authors have a hypothesis for why it behaves different from the other methods w.r.t. P-vector angles?\n\n- Have the authors experimented with training approaches that explicitly encourage small angles between model and data P-vectors?\n\n- The paper would benefit from a thorough editing pass to fix typos and improve clarity. The structure of the paper is well-organized, just some sentences are hard to parse.\n\n- When abbreviations like \"AE\" or \"CNN\" are used for the first time, it is generally good to write them out.\n\n- Why does the paper sometimes use angle and sometimes use cosine of the angle? It could be better to use one of the two consistently.\n\n- Should the x-axis labels in Figure 5 be \"training steps\" instead of \"epochs\"?\n\n- There is too little vertical space separating the caption of Figure 7 from the text below.\n\n- Typos:\n  * Introduction: \"To better euclid\"\n  * Section 3: \"various architectures amd different training paradigms\"\n  * Section 5: \"an data, We carry out\"  (capitalization)\n  * Section 5: \"expect\" -> \"except\"", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1111/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1111/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Empirical Studies on the Convergence of Feature Spaces in Deep Learning", "authorids": ["~Haoran_Liu2", "~Haoyi_Xiong1", "~Yaqing_Wang2", "~Haozhe_An1", "~Dongrui_Wu1", "~Dejing_Dou1"], "authors": ["Haoran Liu", "Haoyi Xiong", "Yaqing Wang", "Haozhe An", "Dongrui Wu", "Dejing Dou"], "keywords": [], "abstract": "While deep learning is effective to learn features/representations from data, the distributions of samples in feature spaces learned by various architectures for different training tasks (e.g., latent layers of AEs and feature vectors in CNN classifiers) have not been well-studied or compared. We hypothesize that the feature spaces of networks trained by various architectures (AEs or CNNs) and tasks (supervised, unsupervised, or self-supervised learning) share some common subspaces, no matter what types of DNN architectures or whether the labels have been used in feature learning. To test our hypothesis, through Singular Value Decomposition (SVD) of feature vectors, we demonstrate that one could linearly project the feature vectors of the same group of samples to a similar distribution, where the distribution is represented as the top left singular vector (i.e., principal subspace of feature vectors), namely $\\mathcal{P}$-vectors. We further assess the convergence of feature space learning using angles between $\\mathcal{P}$-vectors obtained from the well-trained model and its checkpoint per epoch during the learning procedure, where a quasi-monotonic trend of convergence to small angles has been observed. Finally, we carry out case studies to connect $\\mathcal{P}$-vectors to the data distribution, and generalization performance. Extensive experiments with practically-used MLP, AE and CNN architectures for classification, image reconstruction, and self-supervised learning tasks on MNIST, CIFAR-10 and CIFAR-100 datasets have been done to support our claims with solid evidences.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "liu|empirical_studies_on_the_convergence_of_feature_spaces_in_deep_learning", "supplementary_material": "", "pdf": "/pdf/2b195c6847605c50ea00784757198719861afaf3.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=iphj4u5v3", "_bibtex": "@misc{\nliu2021empirical,\ntitle={Empirical Studies on the Convergence of Feature Spaces in Deep Learning},\nauthor={Haoran Liu and Haoyi Xiong and Yaqing Wang and Haozhe An and Dongrui Wu and Dejing Dou},\nyear={2021},\nurl={https://openreview.net/forum?id=NUCZeoVlAe}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "NUCZeoVlAe", "replyto": "NUCZeoVlAe", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1111/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538126665, "tmdate": 1606915764425, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1111/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1111/-/Official_Review"}}}], "count": 14}