{"notes": [{"id": "BJlEEaEFDS", "original": "HJxpW2GPPH", "number": 482, "cdate": 1569439019888, "ddate": null, "tcdate": 1569439019888, "tmdate": 1577168274493, "tddate": null, "forum": "BJlEEaEFDS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["awais@khu.ac.kr", "fahad.shamshad@itu.edu.pk", "shbae@khu.ac.kr"], "title": "Towards an Adversarially Robust Normalization Approach", "authors": ["Muhammad Awais", "Fahad Shamshad", "Sung-Ho Bae"], "pdf": "/pdf/0efe8a4f4d061e6ba829ba602c56b88eb48b15b0.pdf", "TL;DR": "Investigation of how BatchNorm causes adversarial vulnerability and how to avoid it. ", "abstract": "Batch Normalization (BatchNorm) has shown to be effective for improving and accelerating the training of deep neural networks. However, recently it has been shown that it is also vulnerable to adversarial perturbations. In this work, we aim to investigate the cause of adversarial vulnerability of the BatchNorm. We hypothesize that the use of different normalization statistics during training and inference (mini-batch statistics for training and moving average of these values at inference) is the main cause of this adversarial vulnerability in the BatchNorm layer. We empirically proved this by experiments on various neural network architectures and datasets. Furthermore, we introduce Robust Normalization (RobustNorm) and experimentally show that it is not only resilient to adversarial perturbation but also inherit the benefits of BatchNorm.", "keywords": ["robustness", "BatchNorm", "adversarial"], "paperhash": "awais|towards_an_adversarially_robust_normalization_approach", "original_pdf": "/attachment/0efe8a4f4d061e6ba829ba602c56b88eb48b15b0.pdf", "_bibtex": "@misc{\nawais2020towards,\ntitle={Towards an Adversarially Robust Normalization Approach},\nauthor={Muhammad Awais and Fahad Shamshad and Sung-Ho Bae},\nyear={2020},\nurl={https://openreview.net/forum?id=BJlEEaEFDS}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "FXiuXBGXA2", "original": null, "number": 1, "cdate": 1576798697807, "ddate": null, "tcdate": 1576798697807, "tmdate": 1576800937960, "tddate": null, "forum": "BJlEEaEFDS", "replyto": "BJlEEaEFDS", "invitation": "ICLR.cc/2020/Conference/Paper482/-/Decision", "content": {"decision": "Reject", "comment": " This paper presents an empirical analysis of the reasons behind BatchNorm vulnerability to adversarial inputs, based on the hypothesis that such vulnerability may be caused by using different statistics during the inference stage as compared to the training stage. While the paper is interesting and clearly written, reviewers point out insufficient empirical evaluation in order to make the claim more convincing.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["awais@khu.ac.kr", "fahad.shamshad@itu.edu.pk", "shbae@khu.ac.kr"], "title": "Towards an Adversarially Robust Normalization Approach", "authors": ["Muhammad Awais", "Fahad Shamshad", "Sung-Ho Bae"], "pdf": "/pdf/0efe8a4f4d061e6ba829ba602c56b88eb48b15b0.pdf", "TL;DR": "Investigation of how BatchNorm causes adversarial vulnerability and how to avoid it. ", "abstract": "Batch Normalization (BatchNorm) has shown to be effective for improving and accelerating the training of deep neural networks. However, recently it has been shown that it is also vulnerable to adversarial perturbations. In this work, we aim to investigate the cause of adversarial vulnerability of the BatchNorm. We hypothesize that the use of different normalization statistics during training and inference (mini-batch statistics for training and moving average of these values at inference) is the main cause of this adversarial vulnerability in the BatchNorm layer. We empirically proved this by experiments on various neural network architectures and datasets. Furthermore, we introduce Robust Normalization (RobustNorm) and experimentally show that it is not only resilient to adversarial perturbation but also inherit the benefits of BatchNorm.", "keywords": ["robustness", "BatchNorm", "adversarial"], "paperhash": "awais|towards_an_adversarially_robust_normalization_approach", "original_pdf": "/attachment/0efe8a4f4d061e6ba829ba602c56b88eb48b15b0.pdf", "_bibtex": "@misc{\nawais2020towards,\ntitle={Towards an Adversarially Robust Normalization Approach},\nauthor={Muhammad Awais and Fahad Shamshad and Sung-Ho Bae},\nyear={2020},\nurl={https://openreview.net/forum?id=BJlEEaEFDS}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "BJlEEaEFDS", "replyto": "BJlEEaEFDS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795708721, "tmdate": 1576800257239, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper482/-/Decision"}}}, {"id": "SkgvVW1htH", "original": null, "number": 1, "cdate": 1571709230895, "ddate": null, "tcdate": 1571709230895, "tmdate": 1572972589908, "tddate": null, "forum": "BJlEEaEFDS", "replyto": "BJlEEaEFDS", "invitation": "ICLR.cc/2020/Conference/Paper482/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper addresses a limitation of BatchNorm: vulnerability to adversarial perturbations. The authors propose a possible explanation of this issue and correspondingly an alternative called RobustNorm to tackle this problem. Specifically, the authors observe that the statistics of BatchNorm for training and inference are different, resulting in different data distributions for training and inference. To solve this problem, the authors propose to use min-max rescaling instead of normalization. In addition, the running average is calculated with mean and the running mean of the denominator during inference. Experimental results show significant improvement of robustness and also comparable accuracy for clean data.\n\nThe paper is well-written and the contributions are stated clearly. The explanation of vulnerability is reasonable. The proposed solution is simple but effective.\n\nHowever, I have several concerns:\n*The authors verify that the running average is the main culprit of vulnerability to adversarial attack, but provide no further investigation of why this happens. A possible solution is the drift in input distributions, but the manuscript does not state clearly how is the distribution changed. Further experiments would have made this claim more convincing.\n*The proposed method involves a hyper-parameter \\rho, but it may result in problematic issues. The variance of input is of the same order of magnitude as (max(x)-min(x))^2. If \\rho is set to other value, the magnitude of gradient will change drastically during back-propagation. Although \\rho can be set to 0.2, it still seems ad-hoc. Experiments on more datasets and the sensitivity of the proposed method to \\rho would have validated the claims of the authors.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper482/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper482/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["awais@khu.ac.kr", "fahad.shamshad@itu.edu.pk", "shbae@khu.ac.kr"], "title": "Towards an Adversarially Robust Normalization Approach", "authors": ["Muhammad Awais", "Fahad Shamshad", "Sung-Ho Bae"], "pdf": "/pdf/0efe8a4f4d061e6ba829ba602c56b88eb48b15b0.pdf", "TL;DR": "Investigation of how BatchNorm causes adversarial vulnerability and how to avoid it. ", "abstract": "Batch Normalization (BatchNorm) has shown to be effective for improving and accelerating the training of deep neural networks. However, recently it has been shown that it is also vulnerable to adversarial perturbations. In this work, we aim to investigate the cause of adversarial vulnerability of the BatchNorm. We hypothesize that the use of different normalization statistics during training and inference (mini-batch statistics for training and moving average of these values at inference) is the main cause of this adversarial vulnerability in the BatchNorm layer. We empirically proved this by experiments on various neural network architectures and datasets. Furthermore, we introduce Robust Normalization (RobustNorm) and experimentally show that it is not only resilient to adversarial perturbation but also inherit the benefits of BatchNorm.", "keywords": ["robustness", "BatchNorm", "adversarial"], "paperhash": "awais|towards_an_adversarially_robust_normalization_approach", "original_pdf": "/attachment/0efe8a4f4d061e6ba829ba602c56b88eb48b15b0.pdf", "_bibtex": "@misc{\nawais2020towards,\ntitle={Towards an Adversarially Robust Normalization Approach},\nauthor={Muhammad Awais and Fahad Shamshad and Sung-Ho Bae},\nyear={2020},\nurl={https://openreview.net/forum?id=BJlEEaEFDS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BJlEEaEFDS", "replyto": "BJlEEaEFDS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper482/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper482/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576555021725, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper482/Reviewers"], "noninvitees": [], "tcdate": 1570237751491, "tmdate": 1576555021742, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper482/-/Official_Review"}}}, {"id": "SyxQ0cXatS", "original": null, "number": 2, "cdate": 1571793611516, "ddate": null, "tcdate": 1571793611516, "tmdate": 1572972589873, "tddate": null, "forum": "BJlEEaEFDS", "replyto": "BJlEEaEFDS", "invitation": "ICLR.cc/2020/Conference/Paper482/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Review: This paper investigates the reason behind the vulnerability of BatchNorm and proposes a Robust Normalization. They experimentally show that it is the moving averages of mini-batch means and variances (tracking) used in Normalization that cause the adversarial vulnerability. Based on this observation, they propose a new normalization method not only achieves significantly better results under a variety of attack methods but ensures a comparable test accuracy to that of BatchNorm on unperturbed datasets. The paper is clearly written, easy to read.\n \nStrengths:\n \nExplore the cause of adversarial vulnerability of the BatchNorm and assume that the tracking mechanism used in original BatchNorm leads to the vulnerability from experiment results.\nPropose a new and simple normalization method and perform extensive experiments to validate the efficacy of proposed method.\n \nWeaknesses:\nThough extensive experiments have been done by revealing what leads the vulnerability and the effectiveness of proposed method. The results seem unconvincing with respect to different datasets, since Cifar10 and Cifar100 are inherently connected. Would you mind performing some experiments on ImageNet? Since adversarial training on ImageNet is time-consuming, can you show us the result of Natural Training of different models with different norms on ImageNet and compare their robustness under different attack?\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper482/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper482/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["awais@khu.ac.kr", "fahad.shamshad@itu.edu.pk", "shbae@khu.ac.kr"], "title": "Towards an Adversarially Robust Normalization Approach", "authors": ["Muhammad Awais", "Fahad Shamshad", "Sung-Ho Bae"], "pdf": "/pdf/0efe8a4f4d061e6ba829ba602c56b88eb48b15b0.pdf", "TL;DR": "Investigation of how BatchNorm causes adversarial vulnerability and how to avoid it. ", "abstract": "Batch Normalization (BatchNorm) has shown to be effective for improving and accelerating the training of deep neural networks. However, recently it has been shown that it is also vulnerable to adversarial perturbations. In this work, we aim to investigate the cause of adversarial vulnerability of the BatchNorm. We hypothesize that the use of different normalization statistics during training and inference (mini-batch statistics for training and moving average of these values at inference) is the main cause of this adversarial vulnerability in the BatchNorm layer. We empirically proved this by experiments on various neural network architectures and datasets. Furthermore, we introduce Robust Normalization (RobustNorm) and experimentally show that it is not only resilient to adversarial perturbation but also inherit the benefits of BatchNorm.", "keywords": ["robustness", "BatchNorm", "adversarial"], "paperhash": "awais|towards_an_adversarially_robust_normalization_approach", "original_pdf": "/attachment/0efe8a4f4d061e6ba829ba602c56b88eb48b15b0.pdf", "_bibtex": "@misc{\nawais2020towards,\ntitle={Towards an Adversarially Robust Normalization Approach},\nauthor={Muhammad Awais and Fahad Shamshad and Sung-Ho Bae},\nyear={2020},\nurl={https://openreview.net/forum?id=BJlEEaEFDS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BJlEEaEFDS", "replyto": "BJlEEaEFDS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper482/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper482/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576555021725, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper482/Reviewers"], "noninvitees": [], "tcdate": 1570237751491, "tmdate": 1576555021742, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper482/-/Official_Review"}}}, {"id": "SyxnlNl0tB", "original": null, "number": 3, "cdate": 1571845108190, "ddate": null, "tcdate": 1571845108190, "tmdate": 1572972589824, "tddate": null, "forum": "BJlEEaEFDS", "replyto": "BJlEEaEFDS", "invitation": "ICLR.cc/2020/Conference/Paper482/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes an interesting perspective that BatchNorm may introduce the adversarial vulnerability, and probes why BatchNorm performs like that (the tracking part in BatchNorm). In experiment, the robustness of the networks increases by 20% when removing the tracking part, but the test accuracy on the clean images drops a lot. Afterwards, the authors propose RobustNorm, which performs better than BatchNorm for both natural and adversarial scenarios.\n\nDetailed Comments: \n+ The paper is well written. The paper structure is clear and figures are well illustrated.\n+ The paper understands and carefully investigates BatchNorm in a interesting and important direction. After the investigation, the improved version RobustNorm shows more potential.\n+ The experimental results seem good. The RobustNorm performs better than BatchNorm for both natural and adversarial scenarios.\n- More results on ImageNet would be better to verify the proposed RobustNorm method."}, "signatures": ["ICLR.cc/2020/Conference/Paper482/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper482/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["awais@khu.ac.kr", "fahad.shamshad@itu.edu.pk", "shbae@khu.ac.kr"], "title": "Towards an Adversarially Robust Normalization Approach", "authors": ["Muhammad Awais", "Fahad Shamshad", "Sung-Ho Bae"], "pdf": "/pdf/0efe8a4f4d061e6ba829ba602c56b88eb48b15b0.pdf", "TL;DR": "Investigation of how BatchNorm causes adversarial vulnerability and how to avoid it. ", "abstract": "Batch Normalization (BatchNorm) has shown to be effective for improving and accelerating the training of deep neural networks. However, recently it has been shown that it is also vulnerable to adversarial perturbations. In this work, we aim to investigate the cause of adversarial vulnerability of the BatchNorm. We hypothesize that the use of different normalization statistics during training and inference (mini-batch statistics for training and moving average of these values at inference) is the main cause of this adversarial vulnerability in the BatchNorm layer. We empirically proved this by experiments on various neural network architectures and datasets. Furthermore, we introduce Robust Normalization (RobustNorm) and experimentally show that it is not only resilient to adversarial perturbation but also inherit the benefits of BatchNorm.", "keywords": ["robustness", "BatchNorm", "adversarial"], "paperhash": "awais|towards_an_adversarially_robust_normalization_approach", "original_pdf": "/attachment/0efe8a4f4d061e6ba829ba602c56b88eb48b15b0.pdf", "_bibtex": "@misc{\nawais2020towards,\ntitle={Towards an Adversarially Robust Normalization Approach},\nauthor={Muhammad Awais and Fahad Shamshad and Sung-Ho Bae},\nyear={2020},\nurl={https://openreview.net/forum?id=BJlEEaEFDS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BJlEEaEFDS", "replyto": "BJlEEaEFDS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper482/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper482/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576555021725, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper482/Reviewers"], "noninvitees": [], "tcdate": 1570237751491, "tmdate": 1576555021742, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper482/-/Official_Review"}}}, {"id": "S1g8PfOk9B", "original": null, "number": 6, "cdate": 1571943005879, "ddate": null, "tcdate": 1571943005879, "tmdate": 1571945757842, "tddate": null, "forum": "BJlEEaEFDS", "replyto": "BJlEEaEFDS", "invitation": "ICLR.cc/2020/Conference/Paper482/-/Public_Comment", "content": {"title": "Small questions", "comment": "Hello! I think your work is interesting and has practical values for applying adversarial training in real applications. Can I ask several questions to better understand your work?\n\nFirst, for the model of BatchNorm w/o tracking, in the inference time, how do you normalize? Or, do you apply no batch normalization? Likewise, what about RobustNorm w/o tracking? I am assuming you applied some normalization that I did not realize because no normalization would again change the data distribution, which is one of main things that you want to preserve.\n\nSecond, in deriving RobustNorm with $\\mu$ and $p=0.2$, you mention that \"Equation 12 suppress activations much stronger than BatchNorm\". What do you mean by suppress? May I get a little more explanation about your observation on Equation (12) w.r.t. this sentence?\n\nThird, the results in Figure 4 for PGD-$\\ell_\\infty$ is strange. In my understanding, BIM in your (2) is equivalent to PGD-$\\ell_\\infty$ without initial random noise. However, in Figure 4, the performance of PGD-$\\ell_\\infty$ is significantly worse than BIM-$\\ell_\\infty$ about by 10% to 30%. Moreover, in your table 4 and 5, this significant difference is not observed. May I get explanation for this discrepancy between Figure 4 and table 4, 5? Also, can I get explanation why your model outperforms on PGD-$\\ell_\\infty$ unlike on BIM-$\\ell_\\infty$?\n\nFourth, in Figure 1, may I know the reason you compare tracked mean with mini-batch mean \"before training\"? I think a comparison with mini-batch mean at the \"convergence\" could be more meaningful because 1) the concern is the data distributional change caused when tracked mean is used 2) during the course of learning the mini-batch mean would change. \n\nFifth, in Madry et al (2016) https://arxiv.org/pdf/1706.06083.pdf, the reported robust accuracy of WideResnet on CIFAR10 is 45.8% agianst $\\ell_\\infty$ PGD attacks of $\\epsilon=8/255$. In your table 4, I can see your networks have much better results. Do you think this is because of BatchNorm w/o tracking and RobustNorm? Or because of the network difference? Or any other things?\n\nThank you!"}, "signatures": ["~Jungeum_Kim1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Jungeum_Kim1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["awais@khu.ac.kr", "fahad.shamshad@itu.edu.pk", "shbae@khu.ac.kr"], "title": "Towards an Adversarially Robust Normalization Approach", "authors": ["Muhammad Awais", "Fahad Shamshad", "Sung-Ho Bae"], "pdf": "/pdf/0efe8a4f4d061e6ba829ba602c56b88eb48b15b0.pdf", "TL;DR": "Investigation of how BatchNorm causes adversarial vulnerability and how to avoid it. ", "abstract": "Batch Normalization (BatchNorm) has shown to be effective for improving and accelerating the training of deep neural networks. However, recently it has been shown that it is also vulnerable to adversarial perturbations. In this work, we aim to investigate the cause of adversarial vulnerability of the BatchNorm. We hypothesize that the use of different normalization statistics during training and inference (mini-batch statistics for training and moving average of these values at inference) is the main cause of this adversarial vulnerability in the BatchNorm layer. We empirically proved this by experiments on various neural network architectures and datasets. Furthermore, we introduce Robust Normalization (RobustNorm) and experimentally show that it is not only resilient to adversarial perturbation but also inherit the benefits of BatchNorm.", "keywords": ["robustness", "BatchNorm", "adversarial"], "paperhash": "awais|towards_an_adversarially_robust_normalization_approach", "original_pdf": "/attachment/0efe8a4f4d061e6ba829ba602c56b88eb48b15b0.pdf", "_bibtex": "@misc{\nawais2020towards,\ntitle={Towards an Adversarially Robust Normalization Approach},\nauthor={Muhammad Awais and Fahad Shamshad and Sung-Ho Bae},\nyear={2020},\nurl={https://openreview.net/forum?id=BJlEEaEFDS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJlEEaEFDS", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504208486, "tmdate": 1576860571498, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper482/Authors", "ICLR.cc/2020/Conference/Paper482/Reviewers", "ICLR.cc/2020/Conference/Paper482/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper482/-/Public_Comment"}}}, {"id": "r1lxwX2TFS", "original": null, "number": 4, "cdate": 1571828567640, "ddate": null, "tcdate": 1571828567640, "tmdate": 1571889533361, "tddate": null, "forum": "BJlEEaEFDS", "replyto": "BJlEEaEFDS", "invitation": "ICLR.cc/2020/Conference/Paper482/-/Public_Comment", "content": {"title": "typos and the adversarial vulnerability of BN for TRADES ", "comment": "In Section 2: \n\n\"2.0.1 ADVERSARIAL TRAINING:\" => \"2.0 ADVERSARIAL TRAINING\"\n\"100\" => \"CIFAR100\"\n\"Table 2\" => \"Table 1\" . Table 1 is missing\n\nHave the authors tried to evaluate the adversarial vulnerability of BN for TRADES[1], which combines the clean data and the adversarial data to train the nerual networks. Unlike PGD adversarial training, for the training process of TRADES, the normalization statistics for BN involve the clean data and the adversarial data, which may decrease the distance of  the normalization statistics during training and inference phase.\n\nIs the performance of \"BatchNom w/o tracking\" or \"RN\"  related with the batch size in the inference phase, where the normalization statistics are related with the testing data. Sorry, I missing the results in Figure 5, where the result of BN is missing for Part a.\n\n\n\n[1] Theoretically Principled Trade-off between Robustness and Accuracy. ICML 2019"}, "signatures": ["~Anthony_Wittmer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Anthony_Wittmer1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["awais@khu.ac.kr", "fahad.shamshad@itu.edu.pk", "shbae@khu.ac.kr"], "title": "Towards an Adversarially Robust Normalization Approach", "authors": ["Muhammad Awais", "Fahad Shamshad", "Sung-Ho Bae"], "pdf": "/pdf/0efe8a4f4d061e6ba829ba602c56b88eb48b15b0.pdf", "TL;DR": "Investigation of how BatchNorm causes adversarial vulnerability and how to avoid it. ", "abstract": "Batch Normalization (BatchNorm) has shown to be effective for improving and accelerating the training of deep neural networks. However, recently it has been shown that it is also vulnerable to adversarial perturbations. In this work, we aim to investigate the cause of adversarial vulnerability of the BatchNorm. We hypothesize that the use of different normalization statistics during training and inference (mini-batch statistics for training and moving average of these values at inference) is the main cause of this adversarial vulnerability in the BatchNorm layer. We empirically proved this by experiments on various neural network architectures and datasets. Furthermore, we introduce Robust Normalization (RobustNorm) and experimentally show that it is not only resilient to adversarial perturbation but also inherit the benefits of BatchNorm.", "keywords": ["robustness", "BatchNorm", "adversarial"], "paperhash": "awais|towards_an_adversarially_robust_normalization_approach", "original_pdf": "/attachment/0efe8a4f4d061e6ba829ba602c56b88eb48b15b0.pdf", "_bibtex": "@misc{\nawais2020towards,\ntitle={Towards an Adversarially Robust Normalization Approach},\nauthor={Muhammad Awais and Fahad Shamshad and Sung-Ho Bae},\nyear={2020},\nurl={https://openreview.net/forum?id=BJlEEaEFDS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJlEEaEFDS", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504208486, "tmdate": 1576860571498, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper482/Authors", "ICLR.cc/2020/Conference/Paper482/Reviewers", "ICLR.cc/2020/Conference/Paper482/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper482/-/Public_Comment"}}}, {"id": "r1x9AqqAtH", "original": null, "number": 5, "cdate": 1571887826100, "ddate": null, "tcdate": 1571887826100, "tmdate": 1571889056628, "tddate": null, "forum": "BJlEEaEFDS", "replyto": "rklX-V2tOB", "invitation": "ICLR.cc/2020/Conference/Paper482/-/Public_Comment", "content": {"title": "Thanks", "comment": "Hi, thanks for the additional results.\n\nFrom the above results, it seems that both LayerNorm and GroupNorm show better robustness than BatchNorm (with or without tracking) . Have the authors tried to compare the RN with LayerNorm and GroupNorm? Does the result reveal that the normalizations on a single sample show better robustness than  the normalizations on the batch samples?\n\nFrom the result of Table 4, could the authors explain why natural training with RN  show worse performance on VGG11 and VGG16 for CIFAR10 and CIFAR100 ?"}, "signatures": ["~Anthony_Wittmer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Anthony_Wittmer1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["awais@khu.ac.kr", "fahad.shamshad@itu.edu.pk", "shbae@khu.ac.kr"], "title": "Towards an Adversarially Robust Normalization Approach", "authors": ["Muhammad Awais", "Fahad Shamshad", "Sung-Ho Bae"], "pdf": "/pdf/0efe8a4f4d061e6ba829ba602c56b88eb48b15b0.pdf", "TL;DR": "Investigation of how BatchNorm causes adversarial vulnerability and how to avoid it. ", "abstract": "Batch Normalization (BatchNorm) has shown to be effective for improving and accelerating the training of deep neural networks. However, recently it has been shown that it is also vulnerable to adversarial perturbations. In this work, we aim to investigate the cause of adversarial vulnerability of the BatchNorm. We hypothesize that the use of different normalization statistics during training and inference (mini-batch statistics for training and moving average of these values at inference) is the main cause of this adversarial vulnerability in the BatchNorm layer. We empirically proved this by experiments on various neural network architectures and datasets. Furthermore, we introduce Robust Normalization (RobustNorm) and experimentally show that it is not only resilient to adversarial perturbation but also inherit the benefits of BatchNorm.", "keywords": ["robustness", "BatchNorm", "adversarial"], "paperhash": "awais|towards_an_adversarially_robust_normalization_approach", "original_pdf": "/attachment/0efe8a4f4d061e6ba829ba602c56b88eb48b15b0.pdf", "_bibtex": "@misc{\nawais2020towards,\ntitle={Towards an Adversarially Robust Normalization Approach},\nauthor={Muhammad Awais and Fahad Shamshad and Sung-Ho Bae},\nyear={2020},\nurl={https://openreview.net/forum?id=BJlEEaEFDS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJlEEaEFDS", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504208486, "tmdate": 1576860571498, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper482/Authors", "ICLR.cc/2020/Conference/Paper482/Reviewers", "ICLR.cc/2020/Conference/Paper482/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper482/-/Public_Comment"}}}, {"id": "HylCJXVXYr", "original": null, "number": 3, "cdate": 1571140325551, "ddate": null, "tcdate": 1571140325551, "tmdate": 1571140325551, "tddate": null, "forum": "BJlEEaEFDS", "replyto": "BJgbQgVXYB", "invitation": "ICLR.cc/2020/Conference/Paper482/-/Official_Comment", "content": {"comment": "Thank you for pointing this out.\nWe have used standard ICLR latex template in overleaf. We will try to fix this in our final submission. \n\nWe would love to hear more feedback from you. ", "title": "Thanks"}, "signatures": ["ICLR.cc/2020/Conference/Paper482/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper482/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["awais@khu.ac.kr", "fahad.shamshad@itu.edu.pk", "shbae@khu.ac.kr"], "title": "Towards an Adversarially Robust Normalization Approach", "authors": ["Muhammad Awais", "Fahad Shamshad", "Sung-Ho Bae"], "pdf": "/pdf/0efe8a4f4d061e6ba829ba602c56b88eb48b15b0.pdf", "TL;DR": "Investigation of how BatchNorm causes adversarial vulnerability and how to avoid it. ", "abstract": "Batch Normalization (BatchNorm) has shown to be effective for improving and accelerating the training of deep neural networks. However, recently it has been shown that it is also vulnerable to adversarial perturbations. In this work, we aim to investigate the cause of adversarial vulnerability of the BatchNorm. We hypothesize that the use of different normalization statistics during training and inference (mini-batch statistics for training and moving average of these values at inference) is the main cause of this adversarial vulnerability in the BatchNorm layer. We empirically proved this by experiments on various neural network architectures and datasets. Furthermore, we introduce Robust Normalization (RobustNorm) and experimentally show that it is not only resilient to adversarial perturbation but also inherit the benefits of BatchNorm.", "keywords": ["robustness", "BatchNorm", "adversarial"], "paperhash": "awais|towards_an_adversarially_robust_normalization_approach", "original_pdf": "/attachment/0efe8a4f4d061e6ba829ba602c56b88eb48b15b0.pdf", "_bibtex": "@misc{\nawais2020towards,\ntitle={Towards an Adversarially Robust Normalization Approach},\nauthor={Muhammad Awais and Fahad Shamshad and Sung-Ho Bae},\nyear={2020},\nurl={https://openreview.net/forum?id=BJlEEaEFDS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJlEEaEFDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper482/Authors", "ICLR.cc/2020/Conference/Paper482/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper482/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper482/Reviewers", "ICLR.cc/2020/Conference/Paper482/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper482/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper482/Authors|ICLR.cc/2020/Conference/Paper482/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504170827, "tmdate": 1576860537769, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper482/Authors", "ICLR.cc/2020/Conference/Paper482/Reviewers", "ICLR.cc/2020/Conference/Paper482/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper482/-/Official_Comment"}}}, {"id": "BJgbQgVXYB", "original": null, "number": 3, "cdate": 1571139608805, "ddate": null, "tcdate": 1571139608805, "tmdate": 1571139608805, "tddate": null, "forum": "BJlEEaEFDS", "replyto": "BJlEEaEFDS", "invitation": "ICLR.cc/2020/Conference/Paper482/-/Public_Comment", "content": {"comment": "Hi,\n\nI find that the page length is shorter than the standard length, i.e., there are a lot of blank lines under the page number of each page.", "title": "Minor problem about the page format"}, "signatures": ["~Anthony_Wittmer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Anthony_Wittmer1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["awais@khu.ac.kr", "fahad.shamshad@itu.edu.pk", "shbae@khu.ac.kr"], "title": "Towards an Adversarially Robust Normalization Approach", "authors": ["Muhammad Awais", "Fahad Shamshad", "Sung-Ho Bae"], "pdf": "/pdf/0efe8a4f4d061e6ba829ba602c56b88eb48b15b0.pdf", "TL;DR": "Investigation of how BatchNorm causes adversarial vulnerability and how to avoid it. ", "abstract": "Batch Normalization (BatchNorm) has shown to be effective for improving and accelerating the training of deep neural networks. However, recently it has been shown that it is also vulnerable to adversarial perturbations. In this work, we aim to investigate the cause of adversarial vulnerability of the BatchNorm. We hypothesize that the use of different normalization statistics during training and inference (mini-batch statistics for training and moving average of these values at inference) is the main cause of this adversarial vulnerability in the BatchNorm layer. We empirically proved this by experiments on various neural network architectures and datasets. Furthermore, we introduce Robust Normalization (RobustNorm) and experimentally show that it is not only resilient to adversarial perturbation but also inherit the benefits of BatchNorm.", "keywords": ["robustness", "BatchNorm", "adversarial"], "paperhash": "awais|towards_an_adversarially_robust_normalization_approach", "original_pdf": "/attachment/0efe8a4f4d061e6ba829ba602c56b88eb48b15b0.pdf", "_bibtex": "@misc{\nawais2020towards,\ntitle={Towards an Adversarially Robust Normalization Approach},\nauthor={Muhammad Awais and Fahad Shamshad and Sung-Ho Bae},\nyear={2020},\nurl={https://openreview.net/forum?id=BJlEEaEFDS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJlEEaEFDS", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504208486, "tmdate": 1576860571498, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper482/Authors", "ICLR.cc/2020/Conference/Paper482/Reviewers", "ICLR.cc/2020/Conference/Paper482/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper482/-/Public_Comment"}}}, {"id": "rklX-V2tOB", "original": null, "number": 2, "cdate": 1570518011207, "ddate": null, "tcdate": 1570518011207, "tmdate": 1570518261957, "tddate": null, "forum": "BJlEEaEFDS", "replyto": "BJlEEaEFDS", "invitation": "ICLR.cc/2020/Conference/Paper482/-/Official_Comment", "content": {"comment": "As suggested by Anthony Wittmer in the comments section of the paper [1], \"BatchNorm is the cause of adversarial vulnerability\",  a litmus test for our hypothesis, \"tracking in BatchNorm is a cause of its adversarial vulnerability\", is to test it for normalizations that do not require tracking. We have tested this for Group and LayerNorm. Both of these normalizations don't require tracking for test time statistics.  The results are shown in the following two figures. From figures, it is clear that normalization without tracking is less vulnerable to adversarial attacks. We believe this can also help solve the problem mentioned in section 5 of the paper. \n\nhttps://ibb.co/B37LFFS\nThe figure shows the effect of different attacks on the accuracy of networks trained with different normalizations. All the normalizations that don't require have much better adversarial accuracy.\n\nhttps://ibb.co/JQvYjjB\nEffect of increasing $\\epsilon$ for different adversarial attacks on networks with different norms. The first sub-figure shows results for BatchNorm with tracking and the rest of the figure shows results for normalizations without tracking.\n\n\n\n[1] \\url{https://openreview.net/forum?id=H1x-3xSKDr&noteId=SklNaNu2vr}\n", "title": "Litmus Test of Paper's Hypothesis "}, "signatures": ["ICLR.cc/2020/Conference/Paper482/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper482/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["awais@khu.ac.kr", "fahad.shamshad@itu.edu.pk", "shbae@khu.ac.kr"], "title": "Towards an Adversarially Robust Normalization Approach", "authors": ["Muhammad Awais", "Fahad Shamshad", "Sung-Ho Bae"], "pdf": "/pdf/0efe8a4f4d061e6ba829ba602c56b88eb48b15b0.pdf", "TL;DR": "Investigation of how BatchNorm causes adversarial vulnerability and how to avoid it. ", "abstract": "Batch Normalization (BatchNorm) has shown to be effective for improving and accelerating the training of deep neural networks. However, recently it has been shown that it is also vulnerable to adversarial perturbations. In this work, we aim to investigate the cause of adversarial vulnerability of the BatchNorm. We hypothesize that the use of different normalization statistics during training and inference (mini-batch statistics for training and moving average of these values at inference) is the main cause of this adversarial vulnerability in the BatchNorm layer. We empirically proved this by experiments on various neural network architectures and datasets. Furthermore, we introduce Robust Normalization (RobustNorm) and experimentally show that it is not only resilient to adversarial perturbation but also inherit the benefits of BatchNorm.", "keywords": ["robustness", "BatchNorm", "adversarial"], "paperhash": "awais|towards_an_adversarially_robust_normalization_approach", "original_pdf": "/attachment/0efe8a4f4d061e6ba829ba602c56b88eb48b15b0.pdf", "_bibtex": "@misc{\nawais2020towards,\ntitle={Towards an Adversarially Robust Normalization Approach},\nauthor={Muhammad Awais and Fahad Shamshad and Sung-Ho Bae},\nyear={2020},\nurl={https://openreview.net/forum?id=BJlEEaEFDS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJlEEaEFDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper482/Authors", "ICLR.cc/2020/Conference/Paper482/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper482/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper482/Reviewers", "ICLR.cc/2020/Conference/Paper482/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper482/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper482/Authors|ICLR.cc/2020/Conference/Paper482/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504170827, "tmdate": 1576860537769, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper482/Authors", "ICLR.cc/2020/Conference/Paper482/Reviewers", "ICLR.cc/2020/Conference/Paper482/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper482/-/Official_Comment"}}}, {"id": "HklMcg3tuS", "original": null, "number": 1, "cdate": 1570517129760, "ddate": null, "tcdate": 1570517129760, "tmdate": 1570517129760, "tddate": null, "forum": "BJlEEaEFDS", "replyto": "BylDR1y3Pr", "invitation": "ICLR.cc/2020/Conference/Paper482/-/Official_Comment", "content": {"comment": "Thank you for your comment. Here we will answer your question one by one.\n\n- ``\"however, as an aside, that robustness also be assessed on unseen attacks and corruptions\". By unseen attacks and corruption, do you mean attacks other than PGD on which we have adversarially trained? If so, yes, we have included results for many attacks and Gaussian noise. For instance, table 2 shows the effect of Noise, GradientSign, BIM-$\\ell_{\\infty}$ and PGD-$\\ell_{\\infty}$. Similarly, figure 4 shows results for two additional attacks, Grad and PGD-$\\ell_{2}$ along with different $\\epsilon$ levels ranging from 0.003/1 to 0.9/1. Similarly, other results also show the effect of unseen attacks.\n\nIf by unseen attacks, you mean BlackBox settings, we also have tested one such setting based on transferability property [6]. For this experiment, we have crafted iterative PGD-$\\ell_{\\infty}$ noise for Renet20 with the BatchNorm layer and used it for ResNet38 with different normalizations. Please note that for the creation of adversarial samples, we have used only the BatchNorm layer to comply with BlackBox settings. As suggested by [7], we have only used a non-targeted attack. We will add more results in the final version of the paper. \n\n-----------------------------------------------------------------------------------\n                          BN                      BN w/o Tracking            RN\n-----------------------------------------------------------------------------------\nRenset38         43.96                   47.12                               52.71\n------------------------------------------------------------------------------------\n\n-   For a fair comparison, we used the same network with the same settings except for the normalization layer. This means, for the un-normalized network, we removed the BatchNorm layer. We did not use fixup initialization and used standard initialization used by most of the researchers. We agree that fixup initialization can increase accuracy in some cases and can eliminate the use of the BatchNorm layer. Although we have not experimented with fixup initialization, based on the experiments in the paper [5], it can be helpful for the reduction in adversarial vulnerability. But according to the Fixup paper, we also need to use multiplier and bias factors and some kind of regularization to match the accuracy of BatchNorm. \n    \nAlso, this, in a way, supports our hypothesis that tracking is a cause of adversarial vulnerability.  Because fixup initialization solves the problem (mysterious problem with many interpretations such as alleviation of internal Covariate  shift(ICS) [4], correction of activations to avoid their explosion [1], making loss landscape smoother [2] or regularization [3], etc.) in a way which do not make input distribution at train and test time different. \n\n- For a fair comparison, we have used uniform settings across our experiments. For data preprocessing, we have used random crops with padding of 4 and random horizontal flips. Similarly, we have used Pytorch default values for all variables which means eps = 1e-5 and tau = 0.1. Please note that we have not tuned these settings for our experiments to get better results. \n\n- We do agree with your observation. By switching from misclassification to targeted objective, adversarial accuracy improves. We have used the misclassification objective for all of our experiments. Thank you for highlighting it, We will add relevant experiments in the appendix. Yes, accuracy with BIM-$\\ell_{\\infty}$ attack decreases to zero for BatchNorm as well as BatchNorm w/o tracking as the value of $\\epsilon$ is increased to a very high level. For other attacks, this is not the case. Also, note that RobustNorm is still resistive to the attacks with such high levels of adversarial noise. \n\n\n[1] Bjorck, Nils, et al. \"Understanding batch normalization.\" Advances in Neural Information Processing Systems. 2018.\n\n[2] Santurkar, Shibani, et al. \"How does batch normalization help optimization?.\" Advances in Neural Information Processing Systems. 2018.\n\n[3] Luo, Ping, et al. \"Towards understanding regularization in batch normalization.\" (2018).\n\n[4] Ioffe, Sergey. \"Batch renormalization: Towards reducing minibatch dependence in batch-normalized models.\" Advances in neural information processing systems. 2017.\n\n[5] Galloway, Angus, et al. \"Batch Normalization is a Cause of Adversarial Vulnerability.\" arXiv preprint arXiv:1905.02161 (2019).\n\n[6] Papernot, Nicolas, Patrick McDaniel, and Ian Goodfellow. \"Transferability in machine learning: from phenomena to black-box attacks using adversarial samples.\" arXiv preprint arXiv:1605.07277 (2016).\n\n[7] Liu, Yanpei, et al. \"Delving into transferable adversarial examples and black-box attacks.\" arXiv preprint arXiv:1611.02770 (2016).\n    ", "title": "Re: A few questions "}, "signatures": ["ICLR.cc/2020/Conference/Paper482/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper482/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["awais@khu.ac.kr", "fahad.shamshad@itu.edu.pk", "shbae@khu.ac.kr"], "title": "Towards an Adversarially Robust Normalization Approach", "authors": ["Muhammad Awais", "Fahad Shamshad", "Sung-Ho Bae"], "pdf": "/pdf/0efe8a4f4d061e6ba829ba602c56b88eb48b15b0.pdf", "TL;DR": "Investigation of how BatchNorm causes adversarial vulnerability and how to avoid it. ", "abstract": "Batch Normalization (BatchNorm) has shown to be effective for improving and accelerating the training of deep neural networks. However, recently it has been shown that it is also vulnerable to adversarial perturbations. In this work, we aim to investigate the cause of adversarial vulnerability of the BatchNorm. We hypothesize that the use of different normalization statistics during training and inference (mini-batch statistics for training and moving average of these values at inference) is the main cause of this adversarial vulnerability in the BatchNorm layer. We empirically proved this by experiments on various neural network architectures and datasets. Furthermore, we introduce Robust Normalization (RobustNorm) and experimentally show that it is not only resilient to adversarial perturbation but also inherit the benefits of BatchNorm.", "keywords": ["robustness", "BatchNorm", "adversarial"], "paperhash": "awais|towards_an_adversarially_robust_normalization_approach", "original_pdf": "/attachment/0efe8a4f4d061e6ba829ba602c56b88eb48b15b0.pdf", "_bibtex": "@misc{\nawais2020towards,\ntitle={Towards an Adversarially Robust Normalization Approach},\nauthor={Muhammad Awais and Fahad Shamshad and Sung-Ho Bae},\nyear={2020},\nurl={https://openreview.net/forum?id=BJlEEaEFDS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJlEEaEFDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper482/Authors", "ICLR.cc/2020/Conference/Paper482/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper482/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper482/Reviewers", "ICLR.cc/2020/Conference/Paper482/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper482/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper482/Authors|ICLR.cc/2020/Conference/Paper482/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504170827, "tmdate": 1576860537769, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper482/Authors", "ICLR.cc/2020/Conference/Paper482/Reviewers", "ICLR.cc/2020/Conference/Paper482/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper482/-/Official_Comment"}}}, {"id": "BylDR1y3Pr", "original": null, "number": 1, "cdate": 1569611727388, "ddate": null, "tcdate": 1569611727388, "tmdate": 1569611727388, "tddate": null, "forum": "BJlEEaEFDS", "replyto": "BJlEEaEFDS", "invitation": "ICLR.cc/2020/Conference/Paper482/-/Public_Comment", "content": {"comment": "The results of Table 2 are very nice, showing that the degradation in robustness caused by BatchNorm persists for PGD adversarial training when evaluated on the same. We did not include this in the original work, but have since found similar results by a slimmer margin with the WideResNet architecture from Madry et al. I would maintain however as an aside that robustness also be assessed on unseen attacks and corruptions.\n\nOne concern I have here is that the clean test accuracy of the ResNet20 without BatchNorm is 9-10 points lower than with BatchNorm. Are you using Fixup initialization as the unnormalized baseline (https://openreview.net/forum?id=H1gsz30cKX)? I find that unnormalized ResNets can outperform their batch-normalized equivalent in terms of clean test accuracy, even when using training hyperparameters that were originally tuned specifically for batch-normalized models. For example, here are some ResNet110 checkpoints, the Fixup variant gets ~93.0 vs ~92.5 for BatchNorm: https://github.com/AngusG/bn-advex-zhang-fixup.\n\nMisc questions:\n- How is the data preprocessed for various experiments in this work?\n- What was the value of the normalization constant for BatchNorm (We have since realized that this is an important hyperparameter of BatchNorm, but seldom reported. Pytorch defaults to 1e-5, tf to 1e-3).\n- Apologies if I missed this, what value of \\tau is used in the experiments with tracking?\n- In Figure 4, accuracy plateaus rather than reaching zero as epsilon increases up to 0.9. This suggests vanishing gradients. What was the attack objective here? Sometimes switching from misclassification to a targeted objective can recover the efficacy of a white-box attack. I noticed exactly this for PGD max-norm trained ResNets with Fixup when initially evaluating PGD 2-norm robustness; the misclassification objective would plateau, but targeted attacks go to zero.", "title": "A few questions"}, "signatures": ["~Angus_Galloway1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Angus_Galloway1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["awais@khu.ac.kr", "fahad.shamshad@itu.edu.pk", "shbae@khu.ac.kr"], "title": "Towards an Adversarially Robust Normalization Approach", "authors": ["Muhammad Awais", "Fahad Shamshad", "Sung-Ho Bae"], "pdf": "/pdf/0efe8a4f4d061e6ba829ba602c56b88eb48b15b0.pdf", "TL;DR": "Investigation of how BatchNorm causes adversarial vulnerability and how to avoid it. ", "abstract": "Batch Normalization (BatchNorm) has shown to be effective for improving and accelerating the training of deep neural networks. However, recently it has been shown that it is also vulnerable to adversarial perturbations. In this work, we aim to investigate the cause of adversarial vulnerability of the BatchNorm. We hypothesize that the use of different normalization statistics during training and inference (mini-batch statistics for training and moving average of these values at inference) is the main cause of this adversarial vulnerability in the BatchNorm layer. We empirically proved this by experiments on various neural network architectures and datasets. Furthermore, we introduce Robust Normalization (RobustNorm) and experimentally show that it is not only resilient to adversarial perturbation but also inherit the benefits of BatchNorm.", "keywords": ["robustness", "BatchNorm", "adversarial"], "paperhash": "awais|towards_an_adversarially_robust_normalization_approach", "original_pdf": "/attachment/0efe8a4f4d061e6ba829ba602c56b88eb48b15b0.pdf", "_bibtex": "@misc{\nawais2020towards,\ntitle={Towards an Adversarially Robust Normalization Approach},\nauthor={Muhammad Awais and Fahad Shamshad and Sung-Ho Bae},\nyear={2020},\nurl={https://openreview.net/forum?id=BJlEEaEFDS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJlEEaEFDS", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504208486, "tmdate": 1576860571498, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper482/Authors", "ICLR.cc/2020/Conference/Paper482/Reviewers", "ICLR.cc/2020/Conference/Paper482/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper482/-/Public_Comment"}}}], "count": 13}