{"notes": [{"ddate": null, "legacy_migration": true, "tmdate": 1363761180000, "tcdate": 1363761180000, "number": 5, "id": "kk_CoX43Cfks-", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "2LzIDWSabfLe9", "replyto": "2LzIDWSabfLe9", "signatures": ["Maya Baya"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "The updated Herded Gibbs report is now available on arxiv at the following url:\r\n\r\nhttp://arxiv.org/abs/1301.4168v2\r\n\r\nThe herded Gibbs team."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Herded Gibbs Sampling", "decision": "conferenceOral-iclr2013-conference", "abstract": "The Gibbs sampler is one of the most popular algorithms for inference in statistical models. In this paper, we introduce a herding variant of this algorithm, called herded Gibbs, that is entirely deterministic. We prove that herded Gibbs has an $O(1/T)$ convergence rate for models with independent variables and for fully connected probabilistic graphical models. Herded Gibbs is shown to outperform Gibbs in the tasks of image denoising with MRFs and named entity recognition with CRFs. However, the convergence for herded Gibbs for sparsely connected probabilistic graphical models is still an open problem.", "pdf": "https://arxiv.org/abs/1301.4168", "paperhash": "bornn|herded_gibbs_sampling", "keywords": [], "conflicts": [], "authors": ["Luke Bornn", "Yutian Chen", "Nando de Freitas", "Maya Baya", "Jing Fang", "Max Welling"], "authorids": ["bornn@stat.harvard.edu", "yutian.chen@uci.edu", "nando@cs.ubc.ca", "mareijae@gmail.com", "welling@ics.uci.edu", "jingf@cs.ubc.ca"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1363408140000, "tcdate": 1363408140000, "number": 1, "id": "55Sf5h7-bs1wC", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "2LzIDWSabfLe9", "replyto": "2LzIDWSabfLe9", "signatures": ["Luke Bornn, Yutian Chen, Nando de Freitas, Mareija Eskelin, Jing Fang, Max Welling"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "Taking the reviewers' comments into consideration, and after many useful email exchanges with experts in the field including Prof Art Owen, we have prepared a newer version of the report. If it is not on Arxiv by the time you read this, you can find it at \r\n\r\nhttp://www.cs.ubc.ca/~nando/papers/herding_ICLR.pdf\r\n\r\nReviewer Anonymous 600b: We have made the code available and expanded our description of the CRF for NER section. An empirical comparison with the work of Art Owen and colleagues was not possible given the short time window of this week. However, we engaged in many discussions with Art and he not only added his comments here in open review, but also provided many useful comments via email. One difference between herding and his approach is that herding is greedy (that is, the random sequence does not need to be constructed beforehand). Art also pointed us out to the very interesting work of James Propp and colleagues on Rotor-Router models. Please see our comments in the last paragraph of the Conclusions and Future Work section of the new version of the paper. Prof Propp has also begun to look at the problem of establishing connections between herding and his work.\r\n\r\nReviewer Anonymous cf4e: For marginals, the convergence rate of herded Gibbs is also O(1/T) because marginal probabilities are linear functions of the joint distribution. However, in practice, we observe very rapid convergence results for the marginals, so we might be able to strengthen these results in the future.\r\n\r\nReviewer Anonymous 2d06: We have added more detail to the CRF section and made the code available so as to ensure that our results are reproducible.\r\n\r\nWe thank all reviewers for excellent comments. This openreview discussion has been extremely useful and engaging.\r\n\r\nMany thanks,\r\nThe herded Gibbs team"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Herded Gibbs Sampling", "decision": "conferenceOral-iclr2013-conference", "abstract": "The Gibbs sampler is one of the most popular algorithms for inference in statistical models. In this paper, we introduce a herding variant of this algorithm, called herded Gibbs, that is entirely deterministic. We prove that herded Gibbs has an $O(1/T)$ convergence rate for models with independent variables and for fully connected probabilistic graphical models. Herded Gibbs is shown to outperform Gibbs in the tasks of image denoising with MRFs and named entity recognition with CRFs. However, the convergence for herded Gibbs for sparsely connected probabilistic graphical models is still an open problem.", "pdf": "https://arxiv.org/abs/1301.4168", "paperhash": "bornn|herded_gibbs_sampling", "keywords": [], "conflicts": [], "authors": ["Luke Bornn", "Yutian Chen", "Nando de Freitas", "Maya Baya", "Jing Fang", "Max Welling"], "authorids": ["bornn@stat.harvard.edu", "yutian.chen@uci.edu", "nando@cs.ubc.ca", "mareijae@gmail.com", "welling@ics.uci.edu", "jingf@cs.ubc.ca"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1363212720000, "tcdate": 1363212720000, "number": 2, "id": "PHnWHNpf5bHUO", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "2LzIDWSabfLe9", "replyto": "2LzIDWSabfLe9", "signatures": ["Art Owen"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "Nando asked me for some comments and then he thought I should share them on openreview. So here they are.\r\n\r\nThere are a few other efforts at replacing the IID numbers which drive MCMC. It would be interesting to explore the connections among them.  Here is a sample:\r\n\r\nJim Propp and others have been working on rotor-routers for quite a while. Here is one link:\r\nhttp://front.math.ucdavis.edu/0904.4507\r\n\r\nI've been working with several people on replacing IID numbers by completely uniformly distributed (CUD) ones. This is like taking a small random number generator and using it all up.  See this thesis by Su Chen for the latest results, and lots of references:\r\nwww-stat.stanford.edu/~owen/students/SuChenThesis.pdf\r\nor for earlier work, this thesis by Seth Tribble:\r\nwww-stat.stanford.edu/~owen/students/SethTribbleThesis.pdf\r\n\r\nThe oldest papers in that line of work go back to the late 1960s and early 1970s by Chentsov and also by Sobol'.\r\n\r\nThere is some very recent work by Dick Rudolf and Zhu:\r\nhttp://arxiv.org/abs/1303.2423\r\nthat is similar to herding. The idea there is to make a followup sample of values that fill in holes left after a first sampling.\r\n\r\nNot quite as close to this work but still related is the array-RQMC work of Pierre L'Ecuyer and others. See for instance:\r\nwww.iro.umontreal.ca/~lecuyer/myftp/papers/mcqmc08-array.pdf"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Herded Gibbs Sampling", "decision": "conferenceOral-iclr2013-conference", "abstract": "The Gibbs sampler is one of the most popular algorithms for inference in statistical models. In this paper, we introduce a herding variant of this algorithm, called herded Gibbs, that is entirely deterministic. We prove that herded Gibbs has an $O(1/T)$ convergence rate for models with independent variables and for fully connected probabilistic graphical models. Herded Gibbs is shown to outperform Gibbs in the tasks of image denoising with MRFs and named entity recognition with CRFs. However, the convergence for herded Gibbs for sparsely connected probabilistic graphical models is still an open problem.", "pdf": "https://arxiv.org/abs/1301.4168", "paperhash": "bornn|herded_gibbs_sampling", "keywords": [], "conflicts": [], "authors": ["Luke Bornn", "Yutian Chen", "Nando de Freitas", "Maya Baya", "Jing Fang", "Max Welling"], "authorids": ["bornn@stat.harvard.edu", "yutian.chen@uci.edu", "nando@cs.ubc.ca", "mareijae@gmail.com", "welling@ics.uci.edu", "jingf@cs.ubc.ca"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362793920000, "tcdate": 1362793920000, "number": 9, "id": "OOw6hkBUq_fEr", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "2LzIDWSabfLe9", "replyto": "2LzIDWSabfLe9", "signatures": ["Maya Baya"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "Dear reviewers,\r\n\r\nThank you for the encouraging reviews and useful feedback. We will soon address your questions and comments.\r\n\r\nTo this end, we would like to begin by announcing that the code is available online in both matlab and python, at:\r\n\r\nhttp://www.mareija.ca/research/code/\r\n\r\nThis code contains both the image denoising experiments and the two node example, however, we have omitted the NER experiment because the code is highly dependent on the Stanford NER software. Nonetheless, upon request, we would be happy to share this more complex code too.\r\n\r\nA comprehensive reply and a newer version of the arxiv paper addressing your concerns will appear soon.\r\n\r\nIn the meantime, we look forward to further comments.\r\n\r\nThe herded Gibbs team."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Herded Gibbs Sampling", "decision": "conferenceOral-iclr2013-conference", "abstract": "The Gibbs sampler is one of the most popular algorithms for inference in statistical models. In this paper, we introduce a herding variant of this algorithm, called herded Gibbs, that is entirely deterministic. We prove that herded Gibbs has an $O(1/T)$ convergence rate for models with independent variables and for fully connected probabilistic graphical models. Herded Gibbs is shown to outperform Gibbs in the tasks of image denoising with MRFs and named entity recognition with CRFs. However, the convergence for herded Gibbs for sparsely connected probabilistic graphical models is still an open problem.", "pdf": "https://arxiv.org/abs/1301.4168", "paperhash": "bornn|herded_gibbs_sampling", "keywords": [], "conflicts": [], "authors": ["Luke Bornn", "Yutian Chen", "Nando de Freitas", "Maya Baya", "Jing Fang", "Max Welling"], "authorids": ["bornn@stat.harvard.edu", "yutian.chen@uci.edu", "nando@cs.ubc.ca", "mareijae@gmail.com", "welling@ics.uci.edu", "jingf@cs.ubc.ca"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362497280000, "tcdate": 1362497280000, "number": 6, "id": "rafTmpD60FrZR", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "2LzIDWSabfLe9", "replyto": "2LzIDWSabfLe9", "signatures": ["anonymous reviewer 2d06"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Herded Gibbs Sampling", "review": "The paper presents a deterministic 'sampling' algorithm for unnormalized distributions on discrete variables, similar to Gibbs sampling, which operates by  matching the statistics of the conditional distribution of each node given its Markov blanket. Proofs are provided for the independent and fully-connected cases, with an impressive improvement in asymptotic convergence rate to O(1/T) over O(1/sqrt(T)) available from Monte Carlo methods in the fully-connected case. Experimental results demonstrate herded Gibbs outperforming traditional Gibbs sampling in the sparsely connected case, a regime unfortunately not addressed by the provided proofs. The algorithm's Achilles heel is its prohibitive worst-case memory complexity, scaling exponentially with the maximal node degree of the network.\r\n\r\nThe paper is compelling for its demonstration that a conceptually simple deterministic procedure can (in some cases at least) greatly outperform Gibbs sampling, one of the traditional workhorses of Monte Carlo inference, both asymptotically and empirically. Though the procedure in its current form is of little use in large networks of even moderate edge density, the ubiquity of application domains involving very sparse interaction graphs makes this already an important contribution. The proofs appear to be reasonable upon cursory examination, but I have not as yet verified them in detail.\r\n\r\nPROS\r\n\r\n* A lucidly explained idea that gives rise to somewhat surprising theoretical results.\r\n* Proofs of convergence as well as experimental interrogations.\r\n* A step towards practical herding algorithms for dense unnormalized models, and an important milestone for the literature on herding in general.\r\n\r\nCONS\r\n\r\n* An (acknowledged) disconnect between theory and practice -- available proofs apply only in cases that are uninteresting or impractical.\r\n* Experiments in 4.3 make mention of NER with skip-chain CRFs, where Viterbi is not tractable, but resorts to experiments with chain CRFs instead. An additional experiment utilizing skip-chain CRFs (a more challenging inference task, not amenable to Viterbi) would have been more compelling,  though I realize space is at a premium.\r\n\r\nMinor concerns:\r\n- The precise dimensionality of the image denoising problem is, as far as I can tell, never specified. This would be nice to know. \r\n- More details as to how the herded Gibbs procedure maps onto the point estimate provided as output on the NER task would be helpful -- presumably the single highest-probability sample is used?"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Herded Gibbs Sampling", "decision": "conferenceOral-iclr2013-conference", "abstract": "The Gibbs sampler is one of the most popular algorithms for inference in statistical models. In this paper, we introduce a herding variant of this algorithm, called herded Gibbs, that is entirely deterministic. We prove that herded Gibbs has an $O(1/T)$ convergence rate for models with independent variables and for fully connected probabilistic graphical models. Herded Gibbs is shown to outperform Gibbs in the tasks of image denoising with MRFs and named entity recognition with CRFs. However, the convergence for herded Gibbs for sparsely connected probabilistic graphical models is still an open problem.", "pdf": "https://arxiv.org/abs/1301.4168", "paperhash": "bornn|herded_gibbs_sampling", "keywords": [], "conflicts": [], "authors": ["Luke Bornn", "Yutian Chen", "Nando de Freitas", "Maya Baya", "Jing Fang", "Max Welling"], "authorids": ["bornn@stat.harvard.edu", "yutian.chen@uci.edu", "nando@cs.ubc.ca", "mareijae@gmail.com", "welling@ics.uci.edu", "jingf@cs.ubc.ca"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362382860000, "tcdate": 1362382860000, "number": 3, "id": "-wDkwa3mkYwTa", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "2LzIDWSabfLe9", "replyto": "2LzIDWSabfLe9", "signatures": ["anonymous reviewer b2c5"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Herded Gibbs Sampling", "review": "This paper shows how Herding, a deterministic moment-matching algorithm, can be used to sample from un-normalized probabilities, by applying Herding to the full-conditional distributions. The paper presents (1) theoretical proof of O(1/T) convergence in the case of empty and fully-connected graphical models, as well as (2) empirical evidence, showing that Herded Gibbs sampling outperforms both Gibbs and mean-field for 2D structured MRFs and chain structured CRFs. This improved performance however comes at the price of memory, which is exponential in the maximum in-degree of the graph, thus making the method best suited to sparsely connected graphical models.\r\n\r\nWhile the application of Herding to sample from joint-distributions through its conditionals may not appear exciting at first glance, I believe this represents a novel research direction with potentially high impact. A 1/T convergence rate would be a boon in many domains of application, which tend to overly rely on Gibbs sampling, an old and often brittle sampling algorithm. The algorithm's exponential memory requirements are somewhat troubling. However, I believe this can be overlooked given the early state of research and the fact that sparse graphical models represent a realistic (and immediate) domain of application.\r\n\r\nThe paper is well written and clear. I unfortunately cannot comment on the correctness of the convergence proofs (which appear in the Appendix), as those proved to be too time-consuming for me to make a professional judgement on.  Hopefully the open review process of ICLR will help weed out any potential issues therein.\r\n\r\nPROS:\r\n* A novel sampling algorithm with faster convergence rate than MCMC methods.\r\n* Another milestone for Herding: sampling for un-normalized probabilities \r\n(with tractable conditionals).\r\n* Combination of theoretical proofs (when available) and empirical evidence.\r\n* Experiments are thorough and span common domains of application: image denoising through MRFs and Named Entity Recognition through chain-CRFs.\r\n\r\nCONS:\r\n* Convergence proofs hold for less than practicle graph structures. \r\n* Exponential memory requirements of the algorithm make Herded Gibbs sampling impractical for lage families of graphical models, including Boltzmann Machines."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Herded Gibbs Sampling", "decision": "conferenceOral-iclr2013-conference", "abstract": "The Gibbs sampler is one of the most popular algorithms for inference in statistical models. In this paper, we introduce a herding variant of this algorithm, called herded Gibbs, that is entirely deterministic. We prove that herded Gibbs has an $O(1/T)$ convergence rate for models with independent variables and for fully connected probabilistic graphical models. Herded Gibbs is shown to outperform Gibbs in the tasks of image denoising with MRFs and named entity recognition with CRFs. However, the convergence for herded Gibbs for sparsely connected probabilistic graphical models is still an open problem.", "pdf": "https://arxiv.org/abs/1301.4168", "paperhash": "bornn|herded_gibbs_sampling", "keywords": [], "conflicts": [], "authors": ["Luke Bornn", "Yutian Chen", "Nando de Freitas", "Maya Baya", "Jing Fang", "Max Welling"], "authorids": ["bornn@stat.harvard.edu", "yutian.chen@uci.edu", "nando@cs.ubc.ca", "mareijae@gmail.com", "welling@ics.uci.edu", "jingf@cs.ubc.ca"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362377040000, "tcdate": 1362377040000, "number": 8, "id": "wy2cwQ8QPVybX", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "2LzIDWSabfLe9", "replyto": "2LzIDWSabfLe9", "signatures": ["anonymous reviewer cf4e"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Herded Gibbs Sampling", "review": "Herding has an advantage over standard Monte Carlo method, in that\r\nit estimates some statistics quickly, while Monte Carlo methods\r\nestimate all statistics but more slowly.\r\n\r\nThe paper presents a very interesting but impractical attempt to\r\ngeneralize Herding to Gibbs sampling by having a 'herding chain' for\r\neach configuration of the Markov blanket of the variables.  In\r\naddition to the exponential memory complexity, it seems like the\r\nmethod should have an exponentially large constant hidden in the\r\nO(1/T) convergence rate: Given that there are many herding chains,\r\neach herding parameter would be updated extremely infrequently, which\r\nwould result in an exponential slowdown of the Herding effect and thus\r\nincrease the constant in O(1/T).  And indeed, lambda from theorem 2\r\nhas a 2^N factor.\r\n\r\nThe theorem is interesting in that it shows eventual O(1/T)\r\nconvergence in full distribution: that is, the empirical joint\r\ndistribution eventually converges to the full joint distribution.\r\nHowever, in practice we care about estimating marginals and not\r\njoints.  Is it possible to show fast convergence on every subset of\r\nthe marginals, or even on the singleton variables?  Can it be done\r\nwith a favourable constant?  Can such a result be derived from the\r\ntheorems presented in the paper?  Results about marginals would \r\nbe of more practical interest.\r\n\r\nThe experiments show that the idea works in principle, which is good.\r\n\r\nIn its current form, the paper presents a reasonable idea but is\r\nincomplete, since the idea is too impractical. It would be great if\r\nthe paper explored a practical implementation of Gibbs herding, even\r\nan is approximate one.  For example, would it be possible to represent\r\nw_{X_{Ni}} with a big linear function A X_{Ni} for all X and to herd\r\nA, instead of slowly herding the various W_{X_{Ni}}?  Would it work?\r\nWould it do something sensible on the experiments?  Can it be proved\r\nto work in a special case?\r\n\r\nIn conclusion, the paper is very interesting and should be accepted.  Its weakness is the general impracticality of the method."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Herded Gibbs Sampling", "decision": "conferenceOral-iclr2013-conference", "abstract": "The Gibbs sampler is one of the most popular algorithms for inference in statistical models. In this paper, we introduce a herding variant of this algorithm, called herded Gibbs, that is entirely deterministic. We prove that herded Gibbs has an $O(1/T)$ convergence rate for models with independent variables and for fully connected probabilistic graphical models. Herded Gibbs is shown to outperform Gibbs in the tasks of image denoising with MRFs and named entity recognition with CRFs. However, the convergence for herded Gibbs for sparsely connected probabilistic graphical models is still an open problem.", "pdf": "https://arxiv.org/abs/1301.4168", "paperhash": "bornn|herded_gibbs_sampling", "keywords": [], "conflicts": [], "authors": ["Luke Bornn", "Yutian Chen", "Nando de Freitas", "Maya Baya", "Jing Fang", "Max Welling"], "authorids": ["bornn@stat.harvard.edu", "yutian.chen@uci.edu", "nando@cs.ubc.ca", "mareijae@gmail.com", "welling@ics.uci.edu", "jingf@cs.ubc.ca"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362189120000, "tcdate": 1362189120000, "number": 7, "id": "cAhZAfXPZ6Sfw", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "2LzIDWSabfLe9", "replyto": "2LzIDWSabfLe9", "signatures": ["anonymous reviewer 600b"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Herded Gibbs Sampling", "review": "Herding is a relatively recent idea [23]: create a dynamical system that evolves a vector, which when time-averaged will match desired expectations. Originally it was designed as a novel means to generalize from observed data with measured moments. In this work, the conditional distributions of a Gibbs sampler are matched, with the hope of sampling from arbitrary target distributions.\r\n\r\nAs reviewed by the paper itself, this work joins only a small number of recent papers that try to simulate arbitrary target distributions using a deterministic dynamical system. Compared to [19] this work potentially works better in some situations: O(1/T) convergence can happen, whereas [19] seems to emulate a conventional Gibbs sampler with O(1/T^2) convergence. However, the current work seems to be more costly in memory and less-generally applicable than Gibbs sampling, because it needs to track weights for all possible conditional distributions (all possible neighbourhood settings for each variable) in some cases. The comparison to [7] is less clear, as that is motivated by O(1/T) QMC rates, but I don't know if/how it would compare to the current work. (No comparison is given.)\r\n\r\nOne of the features of Markov chain Monte Carlo methods, such as Gibbs sampling, is that represents _joint_ distributions, through examples. Unlike variational approximation methods, no simple form of the distribution is assumed, but Monte Carlo sampling may be a less efficient way to get marginal distributions. For example, Kuss and Rasmussen http://www.jmlr.org/papers/volume6/kuss05a/kuss05a.pdf demonstrated that EP gives exceedingly accurate posterior marginals with Gaussian process classifiers, even though its joint approximation, a Gaussian, is obviously wrong. The experiment in section 4.1 suggests that the herded Gibbs procedure is prepared to move through low probability joint settings more often than it 'should', but gets better marginals as a result. The experiment section 4.2 also depends only on low-dimensional marginals (as many applications do). The experiment in section 4.3 involves an optimization task, and I'm not sure how herded Gibbs was applied (also with annealing? The most probable sample chosen? ...).\r\n\r\nThis is an interesting, novel paper, that appears technically sound. The most time-consuming research contributions are the proofs in the appendices, which seem plausible, but I have not carefully checked them. As discussed in the conclusion, there is a gap between the applicability of this theory and the applicability of the methods. But there is plenty in this paper to suggest that herded sampling for generic target distributions is an interesting direction.\r\n\r\nAs requested, a list of pros and cons:\r\n\r\nPros:\r\n- a novel approach to sampling from high-dimensional distributions, an area of large interest.\r\n- Good combination of toy experiments, up to fairly realistic, but harder to understand, demonstration.\r\n- Raises many open questions: could have impact within community.\r\n- Has the potential to be both general and fast to converge: in long term could have impact outside community.\r\n\r\nCons:\r\n- Should possibly compare to Owen's work on QMC and MCMC. Although there may be no interesting comparison to be made.\r\n- The most interesting example (NER, section 4.3) is slightly hard to understand. An extra sentence or two could help greatly to state how the sampler's output is used.\r\n- Code could be provided.\r\n\r\n\r\nVery minor: paragraph 3 of section 5 should be rewritten. It's wordy: 'We should mention...We have indeed studied this', and uses jargon that's explained parenthetically in the final sentence but not in the first two."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Herded Gibbs Sampling", "decision": "conferenceOral-iclr2013-conference", "abstract": "The Gibbs sampler is one of the most popular algorithms for inference in statistical models. In this paper, we introduce a herding variant of this algorithm, called herded Gibbs, that is entirely deterministic. We prove that herded Gibbs has an $O(1/T)$ convergence rate for models with independent variables and for fully connected probabilistic graphical models. Herded Gibbs is shown to outperform Gibbs in the tasks of image denoising with MRFs and named entity recognition with CRFs. However, the convergence for herded Gibbs for sparsely connected probabilistic graphical models is still an open problem.", "pdf": "https://arxiv.org/abs/1301.4168", "paperhash": "bornn|herded_gibbs_sampling", "keywords": [], "conflicts": [], "authors": ["Luke Bornn", "Yutian Chen", "Nando de Freitas", "Maya Baya", "Jing Fang", "Max Welling"], "authorids": ["bornn@stat.harvard.edu", "yutian.chen@uci.edu", "nando@cs.ubc.ca", "mareijae@gmail.com", "welling@ics.uci.edu", "jingf@cs.ubc.ca"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362189120000, "tcdate": 1362189120000, "number": 4, "id": "_ia0VPOP0SVPj", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "2LzIDWSabfLe9", "replyto": "2LzIDWSabfLe9", "signatures": ["anonymous reviewer 600b"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Herded Gibbs Sampling", "review": "Herding is a relatively recent idea [23]: create a dynamical system that evolves a vector, which when time-averaged will match desired expectations. Originally it was designed as a novel means to generalize from observed data with measured moments. In this work, the conditional distributions of a Gibbs sampler are matched, with the hope of sampling from arbitrary target distributions.\r\n\r\nAs reviewed by the paper itself, this work joins only a small number of recent papers that try to simulate arbitrary target distributions using a deterministic dynamical system. Compared to [19] this work potentially works better in some situations: O(1/T) convergence can happen, whereas [19] seems to emulate a conventional Gibbs sampler with O(1/T^2) convergence. However, the current work seems to be more costly in memory and less-generally applicable than Gibbs sampling, because it needs to track weights for all possible conditional distributions (all possible neighbourhood settings for each variable) in some cases. The comparison to [7] is less clear, as that is motivated by O(1/T) QMC rates, but I don't know if/how it would compare to the current work. (No comparison is given.)\r\n\r\nOne of the features of Markov chain Monte Carlo methods, such as Gibbs sampling, is that represents _joint_ distributions, through examples. Unlike variational approximation methods, no simple form of the distribution is assumed, but Monte Carlo sampling may be a less efficient way to get marginal distributions. For example, Kuss and Rasmussen http://www.jmlr.org/papers/volume6/kuss05a/kuss05a.pdf demonstrated that EP gives exceedingly accurate posterior marginals with Gaussian process classifiers, even though its joint approximation, a Gaussian, is obviously wrong. The experiment in section 4.1 suggests that the herded Gibbs procedure is prepared to move through low probability joint settings more often than it 'should', but gets better marginals as a result. The experiment section 4.2 also depends only on low-dimensional marginals (as many applications do). The experiment in section 4.3 involves an optimization task, and I'm not sure how herded Gibbs was applied (also with annealing? The most probable sample chosen? ...).\r\n\r\nThis is an interesting, novel paper, that appears technically sound. The most time-consuming research contributions are the proofs in the appendices, which seem plausible, but I have not carefully checked them. As discussed in the conclusion, there is a gap between the applicability of this theory and the applicability of the methods. But there is plenty in this paper to suggest that herded sampling for generic target distributions is an interesting direction.\r\n\r\nAs requested, a list of pros and cons:\r\n\r\nPros:\r\n- a novel approach to sampling from high-dimensional distributions, an area of large interest.\r\n- Good combination of toy experiments, up to fairly realistic, but harder to understand, demonstration.\r\n- Raises many open questions: could have impact within community.\r\n- Has the potential to be both general and fast to converge: in long term could have impact outside community.\r\n\r\nCons:\r\n- Should possibly compare to Owen's work on QMC and MCMC. Although there may be no interesting comparison to be made.\r\n- The most interesting example (NER, section 4.3) is slightly hard to understand. An extra sentence or two could help greatly to state how the sampler's output is used.\r\n- Code could be provided.\r\n\r\n\r\nVery minor: paragraph 3 of section 5 should be rewritten. It's wordy: 'We should mention...We have indeed studied this', and uses jargon that's explained parenthetically in the final sentence but not in the first two."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Herded Gibbs Sampling", "decision": "conferenceOral-iclr2013-conference", "abstract": "The Gibbs sampler is one of the most popular algorithms for inference in statistical models. In this paper, we introduce a herding variant of this algorithm, called herded Gibbs, that is entirely deterministic. We prove that herded Gibbs has an $O(1/T)$ convergence rate for models with independent variables and for fully connected probabilistic graphical models. Herded Gibbs is shown to outperform Gibbs in the tasks of image denoising with MRFs and named entity recognition with CRFs. However, the convergence for herded Gibbs for sparsely connected probabilistic graphical models is still an open problem.", "pdf": "https://arxiv.org/abs/1301.4168", "paperhash": "bornn|herded_gibbs_sampling", "keywords": [], "conflicts": [], "authors": ["Luke Bornn", "Yutian Chen", "Nando de Freitas", "Maya Baya", "Jing Fang", "Max Welling"], "authorids": ["bornn@stat.harvard.edu", "yutian.chen@uci.edu", "nando@cs.ubc.ca", "mareijae@gmail.com", "welling@ics.uci.edu", "jingf@cs.ubc.ca"]}, "tags": [], "invitation": {}}}, {"replyto": null, "ddate": null, "legacy_migration": true, "tmdate": 1358550000000, "tcdate": 1358550000000, "number": 2, "id": "2LzIDWSabfLe9", "invitation": "ICLR.cc/2013/conference/-/submission", "forum": "2LzIDWSabfLe9", "signatures": ["bornn@stat.harvard.edu"], "readers": ["everyone"], "content": {"title": "Herded Gibbs Sampling", "decision": "conferenceOral-iclr2013-conference", "abstract": "The Gibbs sampler is one of the most popular algorithms for inference in statistical models. In this paper, we introduce a herding variant of this algorithm, called herded Gibbs, that is entirely deterministic. We prove that herded Gibbs has an $O(1/T)$ convergence rate for models with independent variables and for fully connected probabilistic graphical models. Herded Gibbs is shown to outperform Gibbs in the tasks of image denoising with MRFs and named entity recognition with CRFs. However, the convergence for herded Gibbs for sparsely connected probabilistic graphical models is still an open problem.", "pdf": "https://arxiv.org/abs/1301.4168", "paperhash": "bornn|herded_gibbs_sampling", "keywords": [], "conflicts": [], "authors": ["Luke Bornn", "Yutian Chen", "Nando de Freitas", "Maya Baya", "Jing Fang", "Max Welling"], "authorids": ["bornn@stat.harvard.edu", "yutian.chen@uci.edu", "nando@cs.ubc.ca", "mareijae@gmail.com", "welling@ics.uci.edu", "jingf@cs.ubc.ca"]}, "writers": [], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1369422751717, "tmdate": 1496673673639, "cdate": 1496673673639, "tcdate": 1496673673639, "id": "ICLR.cc/2013/conference/-/submission", "writers": ["ICLR.cc/2013"], "signatures": ["OpenReview.net"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": []}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1377198751717}}}], "count": 10}