{"notes": [{"id": "S1ej8o05tm", "original": "S1g4DFSqFX", "number": 204, "cdate": 1538087762916, "ddate": null, "tcdate": 1538087762916, "tmdate": 1545355403540, "tddate": null, "forum": "S1ej8o05tm", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Object detection deep learning networks for Optical Character Recognition", "abstract": "In this article, we show how we applied a simple approach coming from deep learning networks for object detection to the task of optical character recognition in order to build image features taylored for documents. In contrast to scene text reading in natural images using networks pretrained on ImageNet, our document reading is performed with small networks inspired by MNIST digit recognition challenge, at a small computational budget and a small stride. The object detection modern frameworks allow a direct end-to-end training, with no other algorithm than the deep learning and the non-max-suppression algorithm to filter the duplicate predictions. The trained weights can be used for higher level models, such as, for example, document classification, or document segmentation.\n", "keywords": ["OCR", "object detection", "RCNN", "Yolo"], "authorids": ["christopher.bourez@gmail.com", "acq@ivalua.com"], "authors": ["Christopher Bourez", "Aurelien Coquard"], "TL;DR": "Yolo / RCNN neural network for object detection adapted to the task of OCR", "pdf": "/pdf/057e2f2740d9db6538c7ebb238c3b2a48bf9a11a.pdf", "paperhash": "bourez|object_detection_deep_learning_networks_for_optical_character_recognition", "_bibtex": "@misc{\nbourez2019object,\ntitle={Object detection deep learning networks for Optical Character Recognition},\nauthor={Christopher Bourez and Aurelien Coquard},\nyear={2019},\nurl={https://openreview.net/forum?id=S1ej8o05tm},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "SylEPHfle4", "original": null, "number": 1, "cdate": 1544721755680, "ddate": null, "tcdate": 1544721755680, "tmdate": 1545354509459, "tddate": null, "forum": "S1ej8o05tm", "replyto": "S1ej8o05tm", "invitation": "ICLR.cc/2019/Conference/-/Paper204/Meta_Review", "content": {"metareview": "1. Describe the strengths of the paper.  As pointed out by the reviewers and based on your expert opinion.\n\nThe paper tackles an interesting and relevant problem for ICLR: optical character recognition in document images.\n \n2. Describe the weaknesses of the paper. As pointed out by the reviewers and based on your expert opinion. Be sure to indicate which weaknesses are seen as salient for the decision (i.e., potential critical flaws), as opposed to weaknesses that the authors can likely fix in a revision.\n \n- The authors propose to use small networks to localize text in document images, claiming that for document images smaller networks work better than standard SOTA networks for scene text. As pointed out in the reviews, the authors didn't make any comparisons to SOTA object detection networks (trained either on scene text or on document images) so their central claim has not been experimentally verified.\n- The reviewers were unanimous that the work lacks novelty as object detection pipelines have already been used for OCR so a contribution of considering smaller detection networks is minor.\n- There were serious issues with formatting and clarity.\nThese three issues all informed the final decision.\n\n3. Discuss any major points of contention. As raised by the authors or reviewers in the discussion, and how these might have influenced the decision. If the authors provide a rebuttal to a potential reviewer concern, it\u2019s a good idea to acknowledge this and note whether it influenced the final decision or not. This makes sure that author responses are addressed adequately.\n \nThere were no major points of contention and no author feedback.\n\n4. If consensus was reached, say so. Otherwise, explain what the source of reviewer disagreement was and why the decision on the paper aligns with one set of reviewers or another.\n\nThe reviewers reached a consensus that the paper should be rejected.\n", "confidence": "5: The area chair is absolutely certain", "recommendation": "Reject", "title": "lacks novelty and clarity"}, "signatures": ["ICLR.cc/2019/Conference/Paper204/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper204/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Object detection deep learning networks for Optical Character Recognition", "abstract": "In this article, we show how we applied a simple approach coming from deep learning networks for object detection to the task of optical character recognition in order to build image features taylored for documents. In contrast to scene text reading in natural images using networks pretrained on ImageNet, our document reading is performed with small networks inspired by MNIST digit recognition challenge, at a small computational budget and a small stride. The object detection modern frameworks allow a direct end-to-end training, with no other algorithm than the deep learning and the non-max-suppression algorithm to filter the duplicate predictions. The trained weights can be used for higher level models, such as, for example, document classification, or document segmentation.\n", "keywords": ["OCR", "object detection", "RCNN", "Yolo"], "authorids": ["christopher.bourez@gmail.com", "acq@ivalua.com"], "authors": ["Christopher Bourez", "Aurelien Coquard"], "TL;DR": "Yolo / RCNN neural network for object detection adapted to the task of OCR", "pdf": "/pdf/057e2f2740d9db6538c7ebb238c3b2a48bf9a11a.pdf", "paperhash": "bourez|object_detection_deep_learning_networks_for_optical_character_recognition", "_bibtex": "@misc{\nbourez2019object,\ntitle={Object detection deep learning networks for Optical Character Recognition},\nauthor={Christopher Bourez and Aurelien Coquard},\nyear={2019},\nurl={https://openreview.net/forum?id=S1ej8o05tm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper204/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353300267, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1ej8o05tm", "replyto": "S1ej8o05tm", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper204/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper204/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper204/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353300267}}}, {"id": "BkxgKFy5hQ", "original": null, "number": 1, "cdate": 1541171576415, "ddate": null, "tcdate": 1541171576415, "tmdate": 1542948056374, "tddate": null, "forum": "S1ej8o05tm", "replyto": "S1ej8o05tm", "invitation": "ICLR.cc/2019/Conference/-/Paper204/Official_Review", "content": {"title": "A definite incautious paper, no novelty and wrong formatting", "review": "This paper applied an object detection network, like SSD, for optical character detection and recognition. This paper doesn't give any new contributions and has no potential values.\n\nweakness:\n1. the paper is lack of novelty and the motivation is weak. I even can't find any contribution to OCR or object detection.\n\n2. the paper is written badly so that I can't follow easily. In addition, the figures and tables are not always explained in the main body, which makes the experimental results confusing.\n\n3. There are no titles in the figures and tables in this paper\n\n4. the authors don't confirm the superiority of the proposed method to others.\n\nminor comments\n1. what's the meaning of Target mAP in the table?\n2. It seems that Some figures are cropped from TensorBoard, with some extra shadows.", "rating": "2: Strong rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper204/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Object detection deep learning networks for Optical Character Recognition", "abstract": "In this article, we show how we applied a simple approach coming from deep learning networks for object detection to the task of optical character recognition in order to build image features taylored for documents. In contrast to scene text reading in natural images using networks pretrained on ImageNet, our document reading is performed with small networks inspired by MNIST digit recognition challenge, at a small computational budget and a small stride. The object detection modern frameworks allow a direct end-to-end training, with no other algorithm than the deep learning and the non-max-suppression algorithm to filter the duplicate predictions. The trained weights can be used for higher level models, such as, for example, document classification, or document segmentation.\n", "keywords": ["OCR", "object detection", "RCNN", "Yolo"], "authorids": ["christopher.bourez@gmail.com", "acq@ivalua.com"], "authors": ["Christopher Bourez", "Aurelien Coquard"], "TL;DR": "Yolo / RCNN neural network for object detection adapted to the task of OCR", "pdf": "/pdf/057e2f2740d9db6538c7ebb238c3b2a48bf9a11a.pdf", "paperhash": "bourez|object_detection_deep_learning_networks_for_optical_character_recognition", "_bibtex": "@misc{\nbourez2019object,\ntitle={Object detection deep learning networks for Optical Character Recognition},\nauthor={Christopher Bourez and Aurelien Coquard},\nyear={2019},\nurl={https://openreview.net/forum?id=S1ej8o05tm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper204/Official_Review", "cdate": 1542234515412, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "S1ej8o05tm", "replyto": "S1ej8o05tm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper204/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335671719, "tmdate": 1552335671719, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper204/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "rJgd4DB4TQ", "original": null, "number": 4, "cdate": 1541850928233, "ddate": null, "tcdate": 1541850928233, "tmdate": 1541850928233, "tddate": null, "forum": "S1ej8o05tm", "replyto": "S1ej8o05tm", "invitation": "ICLR.cc/2019/Conference/-/Paper204/Official_Review", "content": {"title": "A technical report about the application of well-known and standard deep learning techniques to character detection/recognition in document images", "review": "This paper lacks any novelty/contribution as it just applies well-known and standard architectures for object detection (SSD) and image classification (LeNet) trained with standard algorithms and losses.\n\nMoreover, I fail to see what is the purpose of the proposed pipeline and it is not clear at all how it may help improving existing OCR engines in any particular scenario (handwriting recognition, printed text, historical documents, etc.). No demonstration or comparison with state of the art is provided. \n\nThe authors claim \u201cThis work is the first to apply modern object detection deep learning approaches to document data\u201d but there are previously published works. For example:\n\nTuggener, Lukas, et al. \"DeepScores--A Dataset for Segmentation, Detection and Classification of Tiny Objects.\" ICPR 2018.\nPacha, Alexander, et al. \"Handwritten music object detection: Open issues and baseline results.\" DAS 2018.\n\nActually, in my opinion Music Object Detection in musical scores would be a much better test-bed/application for the proposed pipeline than any of the datasets used in this paper.  The datasets used in the experimental section seem to be created ad-hoc for the proposed pipeline and do not come from any real world application. \n\nFinally, the presentation of the paper is marginal. Data plots have very bad resolution, there are no captions in any table or figure and they are not correctly referenced within the text. There seem to be also missing content in the last sections which makes them impossible to read/understand.", "rating": "1: Trivial or wrong", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper204/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Object detection deep learning networks for Optical Character Recognition", "abstract": "In this article, we show how we applied a simple approach coming from deep learning networks for object detection to the task of optical character recognition in order to build image features taylored for documents. In contrast to scene text reading in natural images using networks pretrained on ImageNet, our document reading is performed with small networks inspired by MNIST digit recognition challenge, at a small computational budget and a small stride. The object detection modern frameworks allow a direct end-to-end training, with no other algorithm than the deep learning and the non-max-suppression algorithm to filter the duplicate predictions. The trained weights can be used for higher level models, such as, for example, document classification, or document segmentation.\n", "keywords": ["OCR", "object detection", "RCNN", "Yolo"], "authorids": ["christopher.bourez@gmail.com", "acq@ivalua.com"], "authors": ["Christopher Bourez", "Aurelien Coquard"], "TL;DR": "Yolo / RCNN neural network for object detection adapted to the task of OCR", "pdf": "/pdf/057e2f2740d9db6538c7ebb238c3b2a48bf9a11a.pdf", "paperhash": "bourez|object_detection_deep_learning_networks_for_optical_character_recognition", "_bibtex": "@misc{\nbourez2019object,\ntitle={Object detection deep learning networks for Optical Character Recognition},\nauthor={Christopher Bourez and Aurelien Coquard},\nyear={2019},\nurl={https://openreview.net/forum?id=S1ej8o05tm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper204/Official_Review", "cdate": 1542234515412, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "S1ej8o05tm", "replyto": "S1ej8o05tm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper204/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335671719, "tmdate": 1552335671719, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper204/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "Hkg0A3UJ67", "original": null, "number": 3, "cdate": 1541528790246, "ddate": null, "tcdate": 1541528790246, "tmdate": 1541534197515, "tddate": null, "forum": "S1ej8o05tm", "replyto": "S1ej8o05tm", "invitation": "ICLR.cc/2019/Conference/-/Paper204/Official_Review", "content": {"title": "Object Detection for OCR", "review": "Unfortunately, the work does not introduce new contributions, with the point of the paper provided in the introduction:\nIn our experiments, we show that best performing approaches currently available for object detection\non natural images can be used with success at OCR tasks.\n\nThe work is applying established object detection algorithms to OCR. While the work provides a thorough experimental section exploring trade offs in network hyper-parameters, the application of object detection to the OCR domain does not provide enough novelty to warrant publication.\n", "rating": "2: Strong rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper204/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Object detection deep learning networks for Optical Character Recognition", "abstract": "In this article, we show how we applied a simple approach coming from deep learning networks for object detection to the task of optical character recognition in order to build image features taylored for documents. In contrast to scene text reading in natural images using networks pretrained on ImageNet, our document reading is performed with small networks inspired by MNIST digit recognition challenge, at a small computational budget and a small stride. The object detection modern frameworks allow a direct end-to-end training, with no other algorithm than the deep learning and the non-max-suppression algorithm to filter the duplicate predictions. The trained weights can be used for higher level models, such as, for example, document classification, or document segmentation.\n", "keywords": ["OCR", "object detection", "RCNN", "Yolo"], "authorids": ["christopher.bourez@gmail.com", "acq@ivalua.com"], "authors": ["Christopher Bourez", "Aurelien Coquard"], "TL;DR": "Yolo / RCNN neural network for object detection adapted to the task of OCR", "pdf": "/pdf/057e2f2740d9db6538c7ebb238c3b2a48bf9a11a.pdf", "paperhash": "bourez|object_detection_deep_learning_networks_for_optical_character_recognition", "_bibtex": "@misc{\nbourez2019object,\ntitle={Object detection deep learning networks for Optical Character Recognition},\nauthor={Christopher Bourez and Aurelien Coquard},\nyear={2019},\nurl={https://openreview.net/forum?id=S1ej8o05tm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper204/Official_Review", "cdate": 1542234515412, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "S1ej8o05tm", "replyto": "S1ej8o05tm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper204/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335671719, "tmdate": 1552335671719, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper204/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "rkg9q-Wc3m", "original": null, "number": 2, "cdate": 1541177745523, "ddate": null, "tcdate": 1541177745523, "tmdate": 1541534197313, "tddate": null, "forum": "S1ej8o05tm", "replyto": "S1ej8o05tm", "invitation": "ICLR.cc/2019/Conference/-/Paper204/Official_Review", "content": {"title": "A pure tech report with no novel contribution and serious flaws in the formatting, technical exposition and experimental evaluation", "review": "The authors experiment with building an SSD-like object detection network for text detection in documents, by replacing the usual VGG or ResNet base architecture with a light weight model inspired by the original digits classification CNN from [LeCun et al 1999].\n\nThis paper is a pure technical report with no novel contribution: all the authors do is replace the \"body\" network in the well-known SSD architecture with a simpler model (taken from existing literature) and evaluate it on two synthetic benchmarks of their creation.\nThe idea of employing object detection CNNs for OCR is not novel either, as pointed out in the related works section.\n\nBeside the absence of novelty, the paper also suffers from several other serious flaws:\n\n1) One of the main motivations provided by the authors for this work is that existing \"classification [...] detection [...] or segmentation networks, cannot be applied directly, even with finetuning\".\nHowever, no experimental results are reported to justify this claim.\nIn fact, in the experimental section the proposed network is not compared against any existing baseline.\n\n2) The text has serious clarity and formatting issues, in particular:\n- most tables and figures have no caption, and the few that have one are not numbered\n- the text exceed both the 8 pages limit and the extended 10 pages limit allowed in the case of big figures\n- the experimental section is very confusing, in particular the way the authors refer to the various network variants using long code names makes it really hard for the reader to follow the ablation studies\n- given the absence of proper captions and numbering, it is quite hard to understand which table refers to which experiment\n- most of the graphs seem to be in the form of low-resolution bitmaps, which are quite hard to read even on screen\n- many entries in the References section are either missing the venue, or point to an arXiv link even when a proper conference / journal reference would be available\n\n3) Some important details about the network are missing, in particular the authors do not mention how labels are assigned to the network outputs, and only give a vague indication about the losses being used.\nSimilarly, there's no mention about the use of NMS, which is also an important component of the two architectures (SSD and YOLO) that inspire this work.\nAssigning labels and performing NMS are actually some of the most crucial components in the training of object proposal / object detection networks, often requiring numerous meta-parameters to be properly configured and tuned, as testified by the meticulous descriptions given in previous works (e.g. YOLO and Fast / Faster / Mask r-CNN).\n\n4) The experimental section is very poorly organized and formatted (as mentioned in (2) above), and completely lacks any comparison with other state of the art approaches.\nA lot of space is devoted to presenting a detailed ablation study which, in my opinion, doesn't contribute much to the overall paper and actually reads more like a report on meta-parameter tuning.\nFinally, starting from Section 5.3.1 the text seems to be copy-pasted without a second read from some differently formatted document, as entire phrases or possibly tables / figures seems to be missing.\n\nIn conclusion, in my opinion this paper does not meet the conference's minimum quality standards and should definitely be rejected.", "rating": "1: Trivial or wrong", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper204/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Object detection deep learning networks for Optical Character Recognition", "abstract": "In this article, we show how we applied a simple approach coming from deep learning networks for object detection to the task of optical character recognition in order to build image features taylored for documents. In contrast to scene text reading in natural images using networks pretrained on ImageNet, our document reading is performed with small networks inspired by MNIST digit recognition challenge, at a small computational budget and a small stride. The object detection modern frameworks allow a direct end-to-end training, with no other algorithm than the deep learning and the non-max-suppression algorithm to filter the duplicate predictions. The trained weights can be used for higher level models, such as, for example, document classification, or document segmentation.\n", "keywords": ["OCR", "object detection", "RCNN", "Yolo"], "authorids": ["christopher.bourez@gmail.com", "acq@ivalua.com"], "authors": ["Christopher Bourez", "Aurelien Coquard"], "TL;DR": "Yolo / RCNN neural network for object detection adapted to the task of OCR", "pdf": "/pdf/057e2f2740d9db6538c7ebb238c3b2a48bf9a11a.pdf", "paperhash": "bourez|object_detection_deep_learning_networks_for_optical_character_recognition", "_bibtex": "@misc{\nbourez2019object,\ntitle={Object detection deep learning networks for Optical Character Recognition},\nauthor={Christopher Bourez and Aurelien Coquard},\nyear={2019},\nurl={https://openreview.net/forum?id=S1ej8o05tm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper204/Official_Review", "cdate": 1542234515412, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "S1ej8o05tm", "replyto": "S1ej8o05tm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper204/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335671719, "tmdate": 1552335671719, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper204/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 6}