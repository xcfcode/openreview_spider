{"notes": [{"id": "LDSeViRs4-Q", "original": "kozD0hdJtaL", "number": 1309, "cdate": 1601308146223, "ddate": null, "tcdate": 1601308146223, "tmdate": 1614985767272, "tddate": null, "forum": "LDSeViRs4-Q", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Increasing-Margin Adversarial (IMA) training to Improve Adversarial Robustness of Neural Networks", "authorids": ["~Linhai_Ma1", "~Liang_Liang2"], "authors": ["Linhai Ma", "Liang Liang"], "keywords": ["Robustness", "CNN", "Medical image classification"], "abstract": "Deep neural networks (DNNs), including convolutional neural networks, are known to be vulnerable to adversarial attacks, which may lead to disastrous consequences in life-critical applications. Adversarial samples are usually generated by attack algorithms and can also be induced by white noises, and therefore the threats are real. In this study, we propose a novel training method, named Increasing Margin Adversarial (IMA) Training, to improve DNN robustness against adversarial noises. During training, the IMA method increases the margins of training samples by moving the decision boundaries of the DNN model far away from the training samples to improve robustness. The IMA method is evaluated on six publicly available datasets (including a COVID-19 CT image dataset) under strong 100-PGD white-box adversarial attacks, and the results show that the proposed method significantly improved classification accuracy on noisy data while keeping a relatively high accuracy on clean data. We hope our approach may facilitate the development of robust DNN applications, especially for COVID-19 diagnosis using CT images.", "one-sentence_summary": "A new adversarial training method with individualized margin estimation to improve robustness against adversarial noises.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ma|increasingmargin_adversarial_ima_training_to_improve_adversarial_robustness_of_neural_networks", "pdf": "/pdf/8884767e3a049fa5a9b870efd97785195a020d0b.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=a_ELrYBlEo", "_bibtex": "@misc{\nma2021increasingmargin,\ntitle={Increasing-Margin Adversarial ({\\{}IMA{\\}}) training to Improve Adversarial Robustness of Neural Networks},\nauthor={Linhai Ma and Liang Liang},\nyear={2021},\nurl={https://openreview.net/forum?id=LDSeViRs4-Q}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 13, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "CnnZnnN3yPT", "original": null, "number": 1, "cdate": 1610040365953, "ddate": null, "tcdate": 1610040365953, "tmdate": 1610473956568, "tddate": null, "forum": "LDSeViRs4-Q", "replyto": "LDSeViRs4-Q", "invitation": "ICLR.cc/2021/Conference/Paper1309/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "The paper proposes a margin-based adversarial training procedure. The paper is lacking in terms of proper dicussion of related literature e.g. similarity and differences to MMA, the \"theoretical\" discussion on page 5 is incomplete as there is no way how one can estimate the perturbed samples to do the analysis (the authors seem to implicitly already assume that the adversarial samples lie on the decision boundary) and the underlying assumptions are not clearly stated, the reported robust accuracies \n(see https://github.com/fra31/auto-attack for a leaderboard of adversarial defenses) on MNIST and CIFAR10 are worse than that of MMA which are in turn worse than SOTA. Thus this paper is below the bar for ICLR."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Increasing-Margin Adversarial (IMA) training to Improve Adversarial Robustness of Neural Networks", "authorids": ["~Linhai_Ma1", "~Liang_Liang2"], "authors": ["Linhai Ma", "Liang Liang"], "keywords": ["Robustness", "CNN", "Medical image classification"], "abstract": "Deep neural networks (DNNs), including convolutional neural networks, are known to be vulnerable to adversarial attacks, which may lead to disastrous consequences in life-critical applications. Adversarial samples are usually generated by attack algorithms and can also be induced by white noises, and therefore the threats are real. In this study, we propose a novel training method, named Increasing Margin Adversarial (IMA) Training, to improve DNN robustness against adversarial noises. During training, the IMA method increases the margins of training samples by moving the decision boundaries of the DNN model far away from the training samples to improve robustness. The IMA method is evaluated on six publicly available datasets (including a COVID-19 CT image dataset) under strong 100-PGD white-box adversarial attacks, and the results show that the proposed method significantly improved classification accuracy on noisy data while keeping a relatively high accuracy on clean data. We hope our approach may facilitate the development of robust DNN applications, especially for COVID-19 diagnosis using CT images.", "one-sentence_summary": "A new adversarial training method with individualized margin estimation to improve robustness against adversarial noises.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ma|increasingmargin_adversarial_ima_training_to_improve_adversarial_robustness_of_neural_networks", "pdf": "/pdf/8884767e3a049fa5a9b870efd97785195a020d0b.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=a_ELrYBlEo", "_bibtex": "@misc{\nma2021increasingmargin,\ntitle={Increasing-Margin Adversarial ({\\{}IMA{\\}}) training to Improve Adversarial Robustness of Neural Networks},\nauthor={Linhai Ma and Liang Liang},\nyear={2021},\nurl={https://openreview.net/forum?id=LDSeViRs4-Q}\n}"}, "tags": [], "invitation": {"reply": {"forum": "LDSeViRs4-Q", "replyto": "LDSeViRs4-Q", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040365938, "tmdate": 1610473956549, "id": "ICLR.cc/2021/Conference/Paper1309/-/Decision"}}}, {"id": "NvYKfhW2DZ_", "original": null, "number": 6, "cdate": 1606013604454, "ddate": null, "tcdate": 1606013604454, "tmdate": 1606291651601, "tddate": null, "forum": "LDSeViRs4-Q", "replyto": "GzRPnNapS52", "invitation": "ICLR.cc/2021/Conference/Paper1309/-/Official_Comment", "content": {"title": "continue...", "comment": "(3) \"The results in Fig 6 shows that IMA outperforms other methods but drops sharply at 0.3 noise level to almost match TRADES and adv\u2019s performance, what is its performance vs other methods at levels past 0.3?\"\n\nReply: Since it is a synthetic 2D dataset, it is easy to visually identify the optimal decision boundary: a curve almost in the middle between the two classes, and the minimum margin of the samples is about 0.15. When the noise level for the adversarial attack is larger than that, some noisy samples will go across the optimal decision boundary, resulting in miss-classification, which is why the accuracy curve of IMA starts to drop when noise level is larger than 0.15. Obviously, if we use a significantly larger noise level that exceeds the maximum  of the true margins of the samples, the accuracy of every method will approach zero.\n\nWe note that it is meaningless to defend against very large adversarial noise levels. Noisy samples with adversarial noises can be correctly-classified by humans but may be incorrectly classified by neural networks, which is basically the definition of adversarial noises. From this definition, the true class label of the noisy sample should be the same as the true class label of the clean sample.\n\nFor this dataset, if the adversarial noise \u03b4 is very large, then the noisy sample x+\u03b4 can penetrate the true decision boundary (roughly the middle curve), then the true label of the noisy sample will change (but for method evaluation, it is still assumed that the true label of the noisy sample x+\u03b4 is the same as the the true label of the clean sample x).  For this dataset, the minimum distance between the two classes is about 0.3. Thus, evaluating any method with adversarial noise level > 0.3 is meaningless on this dataset.\n\n(4) \"The statement \u201ca model robust to noises less than the level of 0.2 is good enough for this application\u201c is not substantiated by any previous work or experiments.\"\n\nReply: the signal to noise ratio of CT imaging for COVID-19 diagnosis should be large enough. For example, in this reference https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7609045/, SNR is 17.0 \u00b1 5.9. When the noise level is 0.2, then SNR is roughly 1/0.2=5 below the required level. When the noise level is 0.1, then SNR is roughly 1/0.1=10, which is reasonable. Basically, if a doctor sees a CT image with noise level larger than 0.2 (see Fig. 10), then the CT imaging machine must have malfunctioned. This kind of noisy images will not be used for diagnosis in clinical practice.\n\n(5) \"How is the IMA\u2019s performance against black-box attacks?\"\n\nReply: In Appendix D, we add additional two experiments on MNIST and CIFAR10, using PGD white-box attack and SPSA black-box attack. The results show that IMA does not do gradient obfuscation.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1309/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1309/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Increasing-Margin Adversarial (IMA) training to Improve Adversarial Robustness of Neural Networks", "authorids": ["~Linhai_Ma1", "~Liang_Liang2"], "authors": ["Linhai Ma", "Liang Liang"], "keywords": ["Robustness", "CNN", "Medical image classification"], "abstract": "Deep neural networks (DNNs), including convolutional neural networks, are known to be vulnerable to adversarial attacks, which may lead to disastrous consequences in life-critical applications. Adversarial samples are usually generated by attack algorithms and can also be induced by white noises, and therefore the threats are real. In this study, we propose a novel training method, named Increasing Margin Adversarial (IMA) Training, to improve DNN robustness against adversarial noises. During training, the IMA method increases the margins of training samples by moving the decision boundaries of the DNN model far away from the training samples to improve robustness. The IMA method is evaluated on six publicly available datasets (including a COVID-19 CT image dataset) under strong 100-PGD white-box adversarial attacks, and the results show that the proposed method significantly improved classification accuracy on noisy data while keeping a relatively high accuracy on clean data. We hope our approach may facilitate the development of robust DNN applications, especially for COVID-19 diagnosis using CT images.", "one-sentence_summary": "A new adversarial training method with individualized margin estimation to improve robustness against adversarial noises.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ma|increasingmargin_adversarial_ima_training_to_improve_adversarial_robustness_of_neural_networks", "pdf": "/pdf/8884767e3a049fa5a9b870efd97785195a020d0b.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=a_ELrYBlEo", "_bibtex": "@misc{\nma2021increasingmargin,\ntitle={Increasing-Margin Adversarial ({\\{}IMA{\\}}) training to Improve Adversarial Robustness of Neural Networks},\nauthor={Linhai Ma and Liang Liang},\nyear={2021},\nurl={https://openreview.net/forum?id=LDSeViRs4-Q}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "LDSeViRs4-Q", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1309/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1309/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1309/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1309/Authors|ICLR.cc/2021/Conference/Paper1309/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1309/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923861216, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1309/-/Official_Comment"}}}, {"id": "GzRPnNapS52", "original": null, "number": 5, "cdate": 1606013584608, "ddate": null, "tcdate": 1606013584608, "tmdate": 1606288993178, "tddate": null, "forum": "LDSeViRs4-Q", "replyto": "Grwz7IKylvx", "invitation": "ICLR.cc/2021/Conference/Paper1309/-/Official_Comment", "content": {"title": "Reply to AnonReviewer2", "comment": "(1) \"Evaluation on non-standard image datasets used to evaluate adversarial robustness. Lack of evaluation on datasets such as MNIST, CIFAR10/100 or imagenet\" \n\nReply: we add additional two experiments on MNIST and CIFAR10 in Appendix D. We do not understand why SVHN and Fashion-MNIST are considered \"non-standard\". \n\n(2) \"IMA\u2019s assumption that clean samples from different classes are equally spaced from the boundary might not be valid for images. Some classes might require more pixel perturbations to change their \u2018ground-truth\u2019 class than others\u2026.While the idea of improving models\u2019 robustness via increasing margins from clean samples is a refreshing direction to counter adversarial examples, the basis behind the idea of IMA might be flawed. IMA assumes that clean samples from different classes are equally spaced from decision boundaries when in an equilibrium state. However, some classes might require more pixel perturbations to change their \u2018ground-truth\u2019 class than others\"\n\nReply:  IMA does not have the assumption that \"clean samples from different classes are equally spaced from the boundary\". Given the maximum possible sample margin \u03b5_max in IMA , the decision boundary is determined by the clean samples within the distance of \u03b5_max from it . In the equilibrium state, the local densities of noisy samples in different classes are the same along the decision boundary, which does not necessarily mean that the clean samples in different classes are equally spaced from the decision boundary (see Eq.(3) to Eq.(8) in Section 2.3; Eq.(9) and Eq.(10) in Appendix G). We guess the reviewer might get misled by Figure 5 (left), which is only an illustration for a simple scenario: the \u03b5-balls of the samples in two different classes expand and then collide with each other, resulting a local decision boundary that is robust (i.e. far away from the clean samples of the two classes).\n\nIMA indeed will find a decision boundary somewhere in the middle between classes, as shown in the 2D moons dataset.\n\nIt is possible that \"some classes might require more pixel perturbations to change their \u2018ground-truth\u2019 class than others\". The problem is we do NOT know the right magnitudes of the pixel perturbations for the samples. Knowing the right magnitudes of the pixel perturbations (i.e. true margins) for the samples is equivalent to knowing the optimal decision boundary. In general, we do NOT have training samples enough to cover the high dimensional input space so that we can do Bayesian classification to get the optimal and robust decision boundary.\n\nWhat should we do when we do not have enough training samples? In this work, we resort to the basic idea of Support Vector Machine (SVM). For (linear) SVM, if there are only two classes and the data samples are linearly-sparable, then SVM will produce a linear decision boundary in the \"middle\" between the two classes, and it will have great generalization ability by its theory. The SVM decision boundary is robust: classification output will not change if a small amount of noise \u03b4 is added to x as long as the vector norm of \u03b4 is smaller than the margin of x. Here, the margin of x is the (minimum) distance between x and the decision boundary.\n\nIn general, we do not have enough training samples to do perfect Bayesian classification to find the optimal and robust decision boundary. Therefore, a decision boundary somewhere in the middle between classes is a reasonable and viable choice. (see Appendix H for more discussions)\n\nIn general, the data samples in multiple classes are nonlinearly-separable, and the robust decision boundary should be somewhere in the middle between classes, which is the goal that IMA pursues. Because of a nonlinear decision boundary, the margins of the samples are not the same, as shown in Fig. 14 in Appendix F.\n\nFrom IMA, when an equilibrium state is reached, the distributions (i.e. local densities) of noisy samples in different classes are the same along the decision boundary. This is somewhat analog to Bayesian classification: at the optimal decision boundary, the distributions (densities) of samples in two classes are the same, assuming the classes have equal prior probabilities. From this perspective, the noisy samples, which are generated by IMA, serve as the surrogates of the real samples. Obviously, we cannot claim it is Bayesian classification because noisy samples may not reveal the true distributions.\n\nIn some special applications/datasets, if the user knows that the samples in class-1 have significantly larger margins than the samples in class-2, then in the IMA method, the samples in class-1 can be allowed to have significantly larger margins, which can be implemented by using several options: (1) increase \u03b2 for the samples in class-1, (2) use a larger margin expansion step size for the samples in class-1, and/or (3) use a larger \u03b5_max for the samples in class-1. Clearly, the use case like this would be very rare."}, "signatures": ["ICLR.cc/2021/Conference/Paper1309/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1309/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Increasing-Margin Adversarial (IMA) training to Improve Adversarial Robustness of Neural Networks", "authorids": ["~Linhai_Ma1", "~Liang_Liang2"], "authors": ["Linhai Ma", "Liang Liang"], "keywords": ["Robustness", "CNN", "Medical image classification"], "abstract": "Deep neural networks (DNNs), including convolutional neural networks, are known to be vulnerable to adversarial attacks, which may lead to disastrous consequences in life-critical applications. Adversarial samples are usually generated by attack algorithms and can also be induced by white noises, and therefore the threats are real. In this study, we propose a novel training method, named Increasing Margin Adversarial (IMA) Training, to improve DNN robustness against adversarial noises. During training, the IMA method increases the margins of training samples by moving the decision boundaries of the DNN model far away from the training samples to improve robustness. The IMA method is evaluated on six publicly available datasets (including a COVID-19 CT image dataset) under strong 100-PGD white-box adversarial attacks, and the results show that the proposed method significantly improved classification accuracy on noisy data while keeping a relatively high accuracy on clean data. We hope our approach may facilitate the development of robust DNN applications, especially for COVID-19 diagnosis using CT images.", "one-sentence_summary": "A new adversarial training method with individualized margin estimation to improve robustness against adversarial noises.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ma|increasingmargin_adversarial_ima_training_to_improve_adversarial_robustness_of_neural_networks", "pdf": "/pdf/8884767e3a049fa5a9b870efd97785195a020d0b.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=a_ELrYBlEo", "_bibtex": "@misc{\nma2021increasingmargin,\ntitle={Increasing-Margin Adversarial ({\\{}IMA{\\}}) training to Improve Adversarial Robustness of Neural Networks},\nauthor={Linhai Ma and Liang Liang},\nyear={2021},\nurl={https://openreview.net/forum?id=LDSeViRs4-Q}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "LDSeViRs4-Q", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1309/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1309/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1309/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1309/Authors|ICLR.cc/2021/Conference/Paper1309/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1309/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923861216, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1309/-/Official_Comment"}}}, {"id": "rLLfAiKX7bZ", "original": null, "number": 7, "cdate": 1606013793130, "ddate": null, "tcdate": 1606013793130, "tmdate": 1606287483744, "tddate": null, "forum": "LDSeViRs4-Q", "replyto": "mePa70WJv40", "invitation": "ICLR.cc/2021/Conference/Paper1309/-/Official_Comment", "content": {"title": "Reply to AnonReviewer1", "comment": "(1) \"Why do large margins result in higher adversarial robustness? What happens if I change the attack type? \"\n\nReply: Since everyone is familiar with SVM (support vector machine), here, we use SVM to explain why larger margins result in higher adversarial robustness. \n\nIf there are only two classes and the data samples are linearly-separable, then (linear) SVM will produce a linear decision boundary in the \"middle\" between the two classes. The decision boundary of SVM is robust against noises: the classification output will not change if a small amount of noise \u03b4 is added to x as long as the vector norm of \u03b4 is smaller than the margin of x. The decision boundary is also robust to noises from any type of adversarial attacks, as long as the vector norm of \u03b4 is smaller than the margin of x. Here, the margin of x is the (minimum) distance between x and the decision boundary.\n\nIn general, the data samples in multiple classes are nonlinearly-separable, and the robust decision boundary should be somewhere in the middle between classes, which is the goal that IMA pursues. \n\n(2) \"Benefits compared over other adversarial training methods are not clear.\"\n\nReply: IMA outperforms vanilla adversarial training and TRADES, and it is on par with MMA in the experiments.\nThe benefits of the IMA are discussed in appendices (F, G, and H) of the revised manuscript. Here is a brief summary.\n\nFor algorithm designers/researchers: the IMA method and the experimental results explain the trade-off between robustness and accuracy from the perspective of sample margins (see Appendices F and G). Our work has demonstrated that \"common intuition that adversarial attacks are most influential to the points close to the decision boundary\" can be materialized into algorithms to improve adversarial robustness, which points out a promising direction for defense against adversarial attacks.\n\nFor the user of IMA to make robust applications (e.g. COVID-19 CT): the IMA method provides a convenient and efficient way to make a trade-off between robustness and accuracy (see Appendices F and G), which is difficult to do for other methods.\n\n(3) \"A more detailed discussion about the equilibrium state is necessary, as currently provided in Sec. 2.3. This is rather an example.\"\n\nReply: Equations (3) to (8) show the equilibrium state when there are three classes, and it is mathematically trivial to show it is also true for more than three classes: we only need to focus on a pair of classes at a time. In Appendix G, we provide further explanation of the IMA method.\n \n(3) \"Need to report average over multiple runs. Results are very close together and it is hard to favor one method.\"\n\nReply: it would be great to do multiple runs and get a p-value. However, few people in the field has done this because of high computation cost (experiments on a dataset may take weeks). In Appendix D, we add additional two experiments on MNIST and CIFAR10, and each one runs twice with different random seeds, just like what was done in the MMA paper.\n\n(4) \"Sec. 3.1: Since this is the toy-dataset, a discussion why the decision boundaries look as they do, would be interesting.\"\n\nReply: In general, the data samples in multiple classes are nonlinearly-separable, and the robust decision boundary should be somewhere in the middle between classes, similar to SVM. And the decision boundary of IMA is indeed roughly in the middle. Other methods are not trying to get such a decision boundary.\n\n(5) \"Sec. 3.3: What information is in Fig. 9 middle and right?\"\n\nReply: As explained by the captions, Fig. 9 middle shows the sample margin distribution estimated by IMA. Fig. 9 right shows the sample margin distribution estimated by MMA, which indicates significant overestimation because MMA-estimated margin distribution is significantly in contradiction with MMA accuracy scores on the noisy data.\n\n(6) \u201cusing cross-entropy loss and clean data for training\u201d\"\n\nReply: thank you for this advice. in the revised paper, we changed it to using standard training with cross-entropy loss and clean data\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1309/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1309/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Increasing-Margin Adversarial (IMA) training to Improve Adversarial Robustness of Neural Networks", "authorids": ["~Linhai_Ma1", "~Liang_Liang2"], "authors": ["Linhai Ma", "Liang Liang"], "keywords": ["Robustness", "CNN", "Medical image classification"], "abstract": "Deep neural networks (DNNs), including convolutional neural networks, are known to be vulnerable to adversarial attacks, which may lead to disastrous consequences in life-critical applications. Adversarial samples are usually generated by attack algorithms and can also be induced by white noises, and therefore the threats are real. In this study, we propose a novel training method, named Increasing Margin Adversarial (IMA) Training, to improve DNN robustness against adversarial noises. During training, the IMA method increases the margins of training samples by moving the decision boundaries of the DNN model far away from the training samples to improve robustness. The IMA method is evaluated on six publicly available datasets (including a COVID-19 CT image dataset) under strong 100-PGD white-box adversarial attacks, and the results show that the proposed method significantly improved classification accuracy on noisy data while keeping a relatively high accuracy on clean data. We hope our approach may facilitate the development of robust DNN applications, especially for COVID-19 diagnosis using CT images.", "one-sentence_summary": "A new adversarial training method with individualized margin estimation to improve robustness against adversarial noises.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ma|increasingmargin_adversarial_ima_training_to_improve_adversarial_robustness_of_neural_networks", "pdf": "/pdf/8884767e3a049fa5a9b870efd97785195a020d0b.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=a_ELrYBlEo", "_bibtex": "@misc{\nma2021increasingmargin,\ntitle={Increasing-Margin Adversarial ({\\{}IMA{\\}}) training to Improve Adversarial Robustness of Neural Networks},\nauthor={Linhai Ma and Liang Liang},\nyear={2021},\nurl={https://openreview.net/forum?id=LDSeViRs4-Q}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "LDSeViRs4-Q", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1309/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1309/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1309/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1309/Authors|ICLR.cc/2021/Conference/Paper1309/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1309/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923861216, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1309/-/Official_Comment"}}}, {"id": "-MKG4CiQUiM", "original": null, "number": 8, "cdate": 1606013922020, "ddate": null, "tcdate": 1606013922020, "tmdate": 1606287196134, "tddate": null, "forum": "LDSeViRs4-Q", "replyto": "rLLfAiKX7bZ", "invitation": "ICLR.cc/2021/Conference/Paper1309/-/Official_Comment", "content": {"title": "continue...", "comment": "\n(7) \" Some variables are used but not introduced.  e.g. x_n1, x_n2  in Sec. 2.3.\"\n\nReply: the two variables are defined in Algorithm 3.\n\n(8) Figures are too small and not properly labeled in experimental section.\n\nReply: due to the 8-page limit, we have to squeeze some figures together. The figures are embedded into pdf with high resolution, please zoom in on computer screen.\n\n(9) \" References to prior work are missing as e.g. \u201cVirtual Adversarial Training: A Regularization Method for Supervised and Semi-Supervised Learning\u201d\n\nReply: we cited this paper in the revised manuscript. Given the large number of papers published per year, it is infeasible to cite everyone. We cited the most relevant papers that we compared with, and we cited several reviewer papers in the introduction section.  Please let us know if any other paper should be cited.\n\n(9) \"Algorithms need rework, e.g. information of Alg. 1 can be written in 2,3 lines.\"\n\nReply: we thank the reviewer for this suggestion. We think that it is better to use more words and sentences to provide a clear presentation of the algorithms.\n\n(10) \"theoretical contribution is relatively minor\"\n\nReply: The IMA method is the main contribution, and it is a heuristic-based method that is not derived from any theory. We use the equilibrium state analysis to provide a theoretical explanation of the method. Our analysis does not need any complex mathematics to explain the trade-off between robustness and accuracy from the perspective of sample margins (see Appendices F, G, and H), which is not a minor contribution. The perspective of equilibrium between classes is new.\n\n\n(11) \"the paper does not present the material sufficiently clearly to the reader\"\n\nReply: the paper is clear to AnonReviewer3, \"The paper is written clearly. There is no difficulty in understanding the content\". For readers not familiar with the margin concept, we are sorry that some background was not introduced, e.g. larger margin leading to higher robustness. In Appendix G of the the revised manuscript, we provide the explanation of the basic idea of the IMA method, including the analogy to SVM. The newly added appendices provide more explanations of our method. \n\n(12) \"experimental evaluation is not sufficiently conclusive in favor of the paper's central hypothesis.\"\n\nReply: We have tested the IMA method on Moons, MNIST, CIFAR10, SVHN, Fashion-MNIST, and COVID-19 CT. As a comparison, the papers of the other three methods reported experiments on fewer datasets. The idea of increasing margin is shown intuitively on the Moons dataset, and the method performance is quantitatively evaluated on those datasets. With the new experimental results and discussions in Appendices (F, G, and H), we believe the evaluations have become sufficiently conclusive to support the central hypothesis: increasing margins of the samples leads to higher robustness of the model.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1309/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1309/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Increasing-Margin Adversarial (IMA) training to Improve Adversarial Robustness of Neural Networks", "authorids": ["~Linhai_Ma1", "~Liang_Liang2"], "authors": ["Linhai Ma", "Liang Liang"], "keywords": ["Robustness", "CNN", "Medical image classification"], "abstract": "Deep neural networks (DNNs), including convolutional neural networks, are known to be vulnerable to adversarial attacks, which may lead to disastrous consequences in life-critical applications. Adversarial samples are usually generated by attack algorithms and can also be induced by white noises, and therefore the threats are real. In this study, we propose a novel training method, named Increasing Margin Adversarial (IMA) Training, to improve DNN robustness against adversarial noises. During training, the IMA method increases the margins of training samples by moving the decision boundaries of the DNN model far away from the training samples to improve robustness. The IMA method is evaluated on six publicly available datasets (including a COVID-19 CT image dataset) under strong 100-PGD white-box adversarial attacks, and the results show that the proposed method significantly improved classification accuracy on noisy data while keeping a relatively high accuracy on clean data. We hope our approach may facilitate the development of robust DNN applications, especially for COVID-19 diagnosis using CT images.", "one-sentence_summary": "A new adversarial training method with individualized margin estimation to improve robustness against adversarial noises.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ma|increasingmargin_adversarial_ima_training_to_improve_adversarial_robustness_of_neural_networks", "pdf": "/pdf/8884767e3a049fa5a9b870efd97785195a020d0b.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=a_ELrYBlEo", "_bibtex": "@misc{\nma2021increasingmargin,\ntitle={Increasing-Margin Adversarial ({\\{}IMA{\\}}) training to Improve Adversarial Robustness of Neural Networks},\nauthor={Linhai Ma and Liang Liang},\nyear={2021},\nurl={https://openreview.net/forum?id=LDSeViRs4-Q}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "LDSeViRs4-Q", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1309/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1309/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1309/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1309/Authors|ICLR.cc/2021/Conference/Paper1309/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1309/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923861216, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1309/-/Official_Comment"}}}, {"id": "RsBB0JBYMd", "original": null, "number": 4, "cdate": 1606009070865, "ddate": null, "tcdate": 1606009070865, "tmdate": 1606285962779, "tddate": null, "forum": "LDSeViRs4-Q", "replyto": "8IxutXESc6q", "invitation": "ICLR.cc/2021/Conference/Paper1309/-/Official_Comment", "content": {"title": "Reply to AnonReviewer3", "comment": "(1) \"In general, the paper has a good quality. The idea is based on a common intuition that adversarial attacks are most influential to the points close to the decision boundary. The proposed algorithm IMA makes effective use of this intuition and adopts an alternating training process. As an experimental work, the experimental performance of IMA is on par with the state of the art in the experimental settings considered in the paper. This work is important to the ML community. It would be interesting to see further exploration of the algorithm in different testing settings. The paper is written clearly. There is no difficulty in understanding the content. Experimental details are provided. \"\n\nReply: we thank the reviewer for the comment and support for our work.\n\n(2) \"In (vanilla) adversarial training, the choice of max perturbation is usually crucial to the performance of the classifier on noisy and standard data. Is the performance of IMA also that sensitive to the choice of \u03b5_max? And it is briefly mentioned in section 3.3 that IMA might indicate a good for vanilla adversarial training. But this does not say anything about the choice of  \u03b5_max for IMA. And this could be very important to its performance (on clean and noisy data).\"\n\nReply:  please read appendix F for the choice of  \u03b5_max of IMA. We have added more experimental results and discussions.\n\n\n(3) \"What might happen to the performance of the method under different choices of \u03b2. It might be interesting to see how IMA deals with the well-known trade-off between robust and standard accuracy, which is currently one of the main concerns of adversarial training methods.\"\n\nReply:   please read appendix E for the choice of  \u03b2 of IMA. We have added more experimental results and discussions.\n\n(4) \"Figures are not readable when printed. \"\n\nReply: we are sorry about this: we have to shrink figures to meet the page limit. The figures are in high resolution on computer screen.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1309/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1309/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Increasing-Margin Adversarial (IMA) training to Improve Adversarial Robustness of Neural Networks", "authorids": ["~Linhai_Ma1", "~Liang_Liang2"], "authors": ["Linhai Ma", "Liang Liang"], "keywords": ["Robustness", "CNN", "Medical image classification"], "abstract": "Deep neural networks (DNNs), including convolutional neural networks, are known to be vulnerable to adversarial attacks, which may lead to disastrous consequences in life-critical applications. Adversarial samples are usually generated by attack algorithms and can also be induced by white noises, and therefore the threats are real. In this study, we propose a novel training method, named Increasing Margin Adversarial (IMA) Training, to improve DNN robustness against adversarial noises. During training, the IMA method increases the margins of training samples by moving the decision boundaries of the DNN model far away from the training samples to improve robustness. The IMA method is evaluated on six publicly available datasets (including a COVID-19 CT image dataset) under strong 100-PGD white-box adversarial attacks, and the results show that the proposed method significantly improved classification accuracy on noisy data while keeping a relatively high accuracy on clean data. We hope our approach may facilitate the development of robust DNN applications, especially for COVID-19 diagnosis using CT images.", "one-sentence_summary": "A new adversarial training method with individualized margin estimation to improve robustness against adversarial noises.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ma|increasingmargin_adversarial_ima_training_to_improve_adversarial_robustness_of_neural_networks", "pdf": "/pdf/8884767e3a049fa5a9b870efd97785195a020d0b.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=a_ELrYBlEo", "_bibtex": "@misc{\nma2021increasingmargin,\ntitle={Increasing-Margin Adversarial ({\\{}IMA{\\}}) training to Improve Adversarial Robustness of Neural Networks},\nauthor={Linhai Ma and Liang Liang},\nyear={2021},\nurl={https://openreview.net/forum?id=LDSeViRs4-Q}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "LDSeViRs4-Q", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1309/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1309/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1309/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1309/Authors|ICLR.cc/2021/Conference/Paper1309/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1309/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923861216, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1309/-/Official_Comment"}}}, {"id": "MYkxxiJudvt", "original": null, "number": 3, "cdate": 1606008637167, "ddate": null, "tcdate": 1606008637167, "tmdate": 1606285774180, "tddate": null, "forum": "LDSeViRs4-Q", "replyto": "GGVSbbFcynM", "invitation": "ICLR.cc/2021/Conference/Paper1309/-/Official_Comment", "content": {"title": "Reply to AnonReviewer4", "comment": "(1) \"The proposed strategy sounds reasonable and worked well with simple dataset, the Moons dataset. However, when it was applied to more complicated real dataset such as Fashion-MINST, SVHN, and COVID-19 CT image dataset; there was no significant achievement if compare to the MMA approaches. Thus further investigation is needed to convince benefit of the IMA on real datasets.\"\n\nReply: the benefit of the IMA is discussed in Appendices (F, G, and H) with more experimental results.\n\nWe note that the comment from AnonReviewer3 highlights our contribution, \"The idea is based on a common intuition that adversarial attacks are most influential to the points close to the decision boundary. The proposed algorithm IMA makes effective use of this intuition and adopts an alternating training process. As an experimental work, the experimental performance of IMA is on par with the state of the art in the experimental settings considered in the paper. This work is important to the ML community.\"\n\nAlso, please read \"Summary of the Revision\" posted on this forum. \n\n(2) \"In addition, the authors tested only one medical image dataset, COVID-19 CT image dataset. Since there are multiple modalities in the medical field and the diversity among datasets are quite large, it is too early to emphasize the advantage of the proposed method in the medical field in general like the last phrase in the conclusion \u201cWe hope our apporach may facilitate the development of robust DNNs, especially in the medical field.\u201d\n\nReply: we changed the sentence to \"We hope our approach may facilitate the development of robust DNN applications, especially for COVID-19 diagnosis using CT images.\", which is more specific. We feel that we need to do something for the COVID-19 situation. In many countries, CT imaging is used as the primary diagnostic tool(https://ieeexplore.ieee.org/document/9069255)\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1309/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1309/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Increasing-Margin Adversarial (IMA) training to Improve Adversarial Robustness of Neural Networks", "authorids": ["~Linhai_Ma1", "~Liang_Liang2"], "authors": ["Linhai Ma", "Liang Liang"], "keywords": ["Robustness", "CNN", "Medical image classification"], "abstract": "Deep neural networks (DNNs), including convolutional neural networks, are known to be vulnerable to adversarial attacks, which may lead to disastrous consequences in life-critical applications. Adversarial samples are usually generated by attack algorithms and can also be induced by white noises, and therefore the threats are real. In this study, we propose a novel training method, named Increasing Margin Adversarial (IMA) Training, to improve DNN robustness against adversarial noises. During training, the IMA method increases the margins of training samples by moving the decision boundaries of the DNN model far away from the training samples to improve robustness. The IMA method is evaluated on six publicly available datasets (including a COVID-19 CT image dataset) under strong 100-PGD white-box adversarial attacks, and the results show that the proposed method significantly improved classification accuracy on noisy data while keeping a relatively high accuracy on clean data. We hope our approach may facilitate the development of robust DNN applications, especially for COVID-19 diagnosis using CT images.", "one-sentence_summary": "A new adversarial training method with individualized margin estimation to improve robustness against adversarial noises.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ma|increasingmargin_adversarial_ima_training_to_improve_adversarial_robustness_of_neural_networks", "pdf": "/pdf/8884767e3a049fa5a9b870efd97785195a020d0b.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=a_ELrYBlEo", "_bibtex": "@misc{\nma2021increasingmargin,\ntitle={Increasing-Margin Adversarial ({\\{}IMA{\\}}) training to Improve Adversarial Robustness of Neural Networks},\nauthor={Linhai Ma and Liang Liang},\nyear={2021},\nurl={https://openreview.net/forum?id=LDSeViRs4-Q}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "LDSeViRs4-Q", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1309/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1309/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1309/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1309/Authors|ICLR.cc/2021/Conference/Paper1309/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1309/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923861216, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1309/-/Official_Comment"}}}, {"id": "fTlGwLqzwWo", "original": null, "number": 10, "cdate": 1606282681458, "ddate": null, "tcdate": 1606282681458, "tmdate": 1606283398927, "tddate": null, "forum": "LDSeViRs4-Q", "replyto": "3CY4w6Bpqq", "invitation": "ICLR.cc/2021/Conference/Paper1309/-/Official_Comment", "content": {"title": "update", "comment": "Dear reviewers,\n\nWe updated the paper again on the last day to include more experimental results and discussions. Please read the lasted version that has 15 Figures and 21 Tables showing results on 6 datasets. Thank you for your valuable comments.\n\nBest,\n\nAuthors\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1309/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1309/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Increasing-Margin Adversarial (IMA) training to Improve Adversarial Robustness of Neural Networks", "authorids": ["~Linhai_Ma1", "~Liang_Liang2"], "authors": ["Linhai Ma", "Liang Liang"], "keywords": ["Robustness", "CNN", "Medical image classification"], "abstract": "Deep neural networks (DNNs), including convolutional neural networks, are known to be vulnerable to adversarial attacks, which may lead to disastrous consequences in life-critical applications. Adversarial samples are usually generated by attack algorithms and can also be induced by white noises, and therefore the threats are real. In this study, we propose a novel training method, named Increasing Margin Adversarial (IMA) Training, to improve DNN robustness against adversarial noises. During training, the IMA method increases the margins of training samples by moving the decision boundaries of the DNN model far away from the training samples to improve robustness. The IMA method is evaluated on six publicly available datasets (including a COVID-19 CT image dataset) under strong 100-PGD white-box adversarial attacks, and the results show that the proposed method significantly improved classification accuracy on noisy data while keeping a relatively high accuracy on clean data. We hope our approach may facilitate the development of robust DNN applications, especially for COVID-19 diagnosis using CT images.", "one-sentence_summary": "A new adversarial training method with individualized margin estimation to improve robustness against adversarial noises.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ma|increasingmargin_adversarial_ima_training_to_improve_adversarial_robustness_of_neural_networks", "pdf": "/pdf/8884767e3a049fa5a9b870efd97785195a020d0b.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=a_ELrYBlEo", "_bibtex": "@misc{\nma2021increasingmargin,\ntitle={Increasing-Margin Adversarial ({\\{}IMA{\\}}) training to Improve Adversarial Robustness of Neural Networks},\nauthor={Linhai Ma and Liang Liang},\nyear={2021},\nurl={https://openreview.net/forum?id=LDSeViRs4-Q}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "LDSeViRs4-Q", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1309/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1309/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1309/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1309/Authors|ICLR.cc/2021/Conference/Paper1309/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1309/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923861216, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1309/-/Official_Comment"}}}, {"id": "3CY4w6Bpqq", "original": null, "number": 2, "cdate": 1606008513756, "ddate": null, "tcdate": 1606008513756, "tmdate": 1606280515606, "tddate": null, "forum": "LDSeViRs4-Q", "replyto": "LDSeViRs4-Q", "invitation": "ICLR.cc/2021/Conference/Paper1309/-/Official_Comment", "content": {"title": "Summary of the Revision", "comment": "We thank the four reviewers for spending your valuable time reading our manuscript. Since we received the comments, we have been working day and night to do new experiments and analyze the results in order to answer the questions from the reviewers. Here, we provide a summary of the revision, and please read the revised paper for the details.\n\nPlease note that \"a lack of state-of-the-art results does not by itself constitute grounds for rejection. Submissions bring value to the ICLR community when they convincingly demonstrate new, relevant, impactful knowledge. Submissions can achieve this without achieving state-of-the-art results.\" https://iclr.cc/Conferences/2021/ReviewerGuide#faq\n\nWhat is new in the paper?\n\nIMA algorithms, theoretical analysis and experiments are all new. \n\nWhat is the relevant and impactful knowledge in the paper?\n\nThe comment from AnonReviewer3 can well answer this question, \"The idea is based on a common intuition that adversarial attacks are most influential to the points close to the decision boundary. The proposed algorithm IMA makes effective use of this intuition and adopts an alternating training process. As an experimental work, the experimental performance of IMA is on par with the state of the art in the experimental settings considered in the paper. This work is important to the ML community.\"\n\nWhat is new in the revised paper?\n\n(1) In Appendix D, we add two additional experiments on MNIST and CIFAR10.\n\n(2) In Appendix E, we show that how \u03b2 can be used to make a trade-off between robustness and accuracy, and we also show that the trade-off is highly nonlinear: a small decrease in accuracy on clean data can result in a large increase in accuracy on noisy data. The nonlinear trade-off between robustness and accuracy makes it difficult to directly compare two methods (e.g. IMA and MMA), as different methods make different trade-off between robustness and accuracy. \n\n(3) In Appendix F, we discuss how the allowed maximum margin (i.e. \u03b5_max) may affect the performance and how to choose its value. The IMA method has a clear explanation of the trade-off between robustness and accuracy, and it enables an easy way for the user of our method to choose the parameter \u03b5_max, which is difficult to do for other methods.\n\n(4) In Appendix F, we also show that for the COVID-19 application, vanilla adversarial training with \u03b5 =0.15 achieved the best performance, compared to other methods. The advantage of vanilla adversarial training is that it is simple to implement and has relatively low computational cost, but it is not easy to find a good \u03b5. The margin distribution estimated by IMA can reveal a good range of the \ud835\udf00. \n\n(5) In Appendix G, we provide further explanation of the IMA method.\n\n(6) In Appendix H, we discuss the trade-off between robustness and accuracy\n\nWhat are the benefits of IMA?\n\n(1) For algorithm designers/researchers: the IMA method and the experimental results explain the trade-off between robustness and accuracy from the perspective of sample margins (see Appendices F and G). Our work has demonstrated that \"common intuition that adversarial attacks are most influential to the points close to the decision boundary\" can be materialized into algorithms to improve adversarial robustness, which points out a promising direction for defense against adversarial attacks.\n\n(2) For the user of IMA to make robust applications (e.g. COVID-19 CT): the IMA method provides a convenient and efficient way to make a trade-off between robustness and accuracy (see Appendices F and G).\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1309/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1309/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Increasing-Margin Adversarial (IMA) training to Improve Adversarial Robustness of Neural Networks", "authorids": ["~Linhai_Ma1", "~Liang_Liang2"], "authors": ["Linhai Ma", "Liang Liang"], "keywords": ["Robustness", "CNN", "Medical image classification"], "abstract": "Deep neural networks (DNNs), including convolutional neural networks, are known to be vulnerable to adversarial attacks, which may lead to disastrous consequences in life-critical applications. Adversarial samples are usually generated by attack algorithms and can also be induced by white noises, and therefore the threats are real. In this study, we propose a novel training method, named Increasing Margin Adversarial (IMA) Training, to improve DNN robustness against adversarial noises. During training, the IMA method increases the margins of training samples by moving the decision boundaries of the DNN model far away from the training samples to improve robustness. The IMA method is evaluated on six publicly available datasets (including a COVID-19 CT image dataset) under strong 100-PGD white-box adversarial attacks, and the results show that the proposed method significantly improved classification accuracy on noisy data while keeping a relatively high accuracy on clean data. We hope our approach may facilitate the development of robust DNN applications, especially for COVID-19 diagnosis using CT images.", "one-sentence_summary": "A new adversarial training method with individualized margin estimation to improve robustness against adversarial noises.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ma|increasingmargin_adversarial_ima_training_to_improve_adversarial_robustness_of_neural_networks", "pdf": "/pdf/8884767e3a049fa5a9b870efd97785195a020d0b.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=a_ELrYBlEo", "_bibtex": "@misc{\nma2021increasingmargin,\ntitle={Increasing-Margin Adversarial ({\\{}IMA{\\}}) training to Improve Adversarial Robustness of Neural Networks},\nauthor={Linhai Ma and Liang Liang},\nyear={2021},\nurl={https://openreview.net/forum?id=LDSeViRs4-Q}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "LDSeViRs4-Q", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1309/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1309/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1309/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1309/Authors|ICLR.cc/2021/Conference/Paper1309/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1309/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923861216, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1309/-/Official_Comment"}}}, {"id": "mePa70WJv40", "original": null, "number": 1, "cdate": 1603474184315, "ddate": null, "tcdate": 1603474184315, "tmdate": 1605024477225, "tddate": null, "forum": "LDSeViRs4-Q", "replyto": "LDSeViRs4-Q", "invitation": "ICLR.cc/2021/Conference/Paper1309/-/Official_Review", "content": {"title": "Minor novelty, not sufficiently clear presentation", "review": "The paper proposes to increase the adversarial robustness of a neural net by training the model on both clean and adversarial samples. An adaptive form of the projected gradient descent generates the adversarial samples. Therefore, the noise magnitude is estimated separately for each training sample, such that the decision boundary (suppose a classification problem) of the neural net has maximum distance to each training sample. \n\nStrengths: \n1.\tAppealing idea of having adaptive noise magnitudes.\n2.\tRelevant experimental section (Covid19).\n3.\tIllustrative figures, describing the model.\n\nWeaknesses, Suggestions, Questions:\n1.\tA theoretical discussion about following points will improve the contribution of the paper:\n        a.\tWhy do large margins result in higher adversarial robustness? What happens if I change the attack type? \n        b.\tBenefits compared over other adversarial training methods are not clear.\n        c.\tA more detailed discussion about the equilibrium state is necessary, as currently provided in Sec. 2.3. This is rather an example.\n2.\tExperimental section:\n        a.\tNeed to report average over multiple runs. Results are very close together and it is hard to favor one method. \n        b.\tSec. 3.1: Since this is the toy-dataset, a discussion why the decision boundaries look as they do, would be interesting. \n        c.\tSec. 3.3: What information is in Fig. 9 middle and right? \n3.\tFormatting and writing:\n        a.\tDetailed proofreading required.  e.g. on p. 3  \u201cusing cross-entropy loss and clean data for training\u201d\n        b.\tSome variables are used but not introduced.  e.g. x_n1, x_n2  in Sec. 2.3.\n        c.\tFigures are too small and not properly labeled in experimental section.\n        d.\tReferences to prior work are missing as e.g. \u201cVirtual Adversarial Training: A Regularization Method for Supervised and Semi-Supervised Learning\u201d\n        e.\tAlgorithms need rework, e.g. information of Alg. 1 can be written in 2,3 lines. \n\nThough the idea of adaptive adversarial noise magnitude is in general appealing, the paper has some weaknesses: (i) theoretical contribution is relatively minor, (ii) the paper does not present the material sufficiently clearly to the reader, and (iii) experimental evaluation is not sufficiently conclusive in favor of the paper's central hypothesis.", "rating": "4: Ok but not good enough - rejection", "confidence": "1: The reviewer's evaluation is an educated guess"}, "signatures": ["ICLR.cc/2021/Conference/Paper1309/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1309/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Increasing-Margin Adversarial (IMA) training to Improve Adversarial Robustness of Neural Networks", "authorids": ["~Linhai_Ma1", "~Liang_Liang2"], "authors": ["Linhai Ma", "Liang Liang"], "keywords": ["Robustness", "CNN", "Medical image classification"], "abstract": "Deep neural networks (DNNs), including convolutional neural networks, are known to be vulnerable to adversarial attacks, which may lead to disastrous consequences in life-critical applications. Adversarial samples are usually generated by attack algorithms and can also be induced by white noises, and therefore the threats are real. In this study, we propose a novel training method, named Increasing Margin Adversarial (IMA) Training, to improve DNN robustness against adversarial noises. During training, the IMA method increases the margins of training samples by moving the decision boundaries of the DNN model far away from the training samples to improve robustness. The IMA method is evaluated on six publicly available datasets (including a COVID-19 CT image dataset) under strong 100-PGD white-box adversarial attacks, and the results show that the proposed method significantly improved classification accuracy on noisy data while keeping a relatively high accuracy on clean data. We hope our approach may facilitate the development of robust DNN applications, especially for COVID-19 diagnosis using CT images.", "one-sentence_summary": "A new adversarial training method with individualized margin estimation to improve robustness against adversarial noises.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ma|increasingmargin_adversarial_ima_training_to_improve_adversarial_robustness_of_neural_networks", "pdf": "/pdf/8884767e3a049fa5a9b870efd97785195a020d0b.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=a_ELrYBlEo", "_bibtex": "@misc{\nma2021increasingmargin,\ntitle={Increasing-Margin Adversarial ({\\{}IMA{\\}}) training to Improve Adversarial Robustness of Neural Networks},\nauthor={Linhai Ma and Liang Liang},\nyear={2021},\nurl={https://openreview.net/forum?id=LDSeViRs4-Q}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "LDSeViRs4-Q", "replyto": "LDSeViRs4-Q", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1309/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538121652, "tmdate": 1606915762896, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1309/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1309/-/Official_Review"}}}, {"id": "Grwz7IKylvx", "original": null, "number": 3, "cdate": 1603617974552, "ddate": null, "tcdate": 1603617974552, "tmdate": 1605024477162, "tddate": null, "forum": "LDSeViRs4-Q", "replyto": "LDSeViRs4-Q", "invitation": "ICLR.cc/2021/Conference/Paper1309/-/Official_Review", "content": {"title": "Review #2", "review": "Summary:\nThe paper proposes increasing-margin adversarial training (IMA) to improve adversarial robustness of a classifier. IMA works by alternating between two algorithms: Algorithm 1 update the model parameters while Algorithm 2 updates the margin estimate. By iteratively increasing the margins from clean training samples, IMA seeks to make the classifier more robust to L-p adversarial perturbations. The authors conducted experiments on the Moons, Fashion-MNIST, SVHN and a CT image dataset to evaluate IMA\u2019s performances against other baselines and found IMA to outperform or be on par with them.\n\nPro:\n+Improving robustness through the margins from clean samples is an interesting approach.\n \nCons:\n-Evaluation on non-standard image datasets used to evaluate adversarial robustness. Lack of evaluation on datasets such as MNIST, CIFAR10/100 or imagenet\n-IMA\u2019s assumption that clean samples from different classes are equally spaced from the boundary might not be valid for images. Some classes might require more pixel perturbations to change their \u2018ground-truth\u2019 class than others.\n\nRecommendation:\nWhile the idea of improving models\u2019 robustness via increasing margins from clean samples is a refreshing direction to counter adversarial examples, the basis behind the idea of IMA might be flawed. IMA assumes that clean samples from different classes are equally spaced from decision boundaries when in an equilibrium state. However, some classes might require more pixel perturbations to change their \u2018ground-truth\u2019 class than others. More discussions and theoretical studies would make IMA more convincing. Another major concern I have is the lack of evaluation on standard image datasets such as MNIST, CIFAR10/100 or imagenet in the paper. Given its current state, I believe the paper is not yet fit for publication.\n\n\nComments and Questions:\n \nThe results in Fig 6 shows that IMA outperforms other methods but drops sharply at 0.3 noise level to almost match TRADES and adv\u2019s performance, what is its performance vs other methods at levels past 0.3?\n\nThe statement \u201ca model robust to noises less than the level of 0.2 is good enough for this application\u201c is not substantiated by any previous work or experiments.\n\nHow is the IMA\u2019s performance against black-box attacks?\n\n\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1309/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1309/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Increasing-Margin Adversarial (IMA) training to Improve Adversarial Robustness of Neural Networks", "authorids": ["~Linhai_Ma1", "~Liang_Liang2"], "authors": ["Linhai Ma", "Liang Liang"], "keywords": ["Robustness", "CNN", "Medical image classification"], "abstract": "Deep neural networks (DNNs), including convolutional neural networks, are known to be vulnerable to adversarial attacks, which may lead to disastrous consequences in life-critical applications. Adversarial samples are usually generated by attack algorithms and can also be induced by white noises, and therefore the threats are real. In this study, we propose a novel training method, named Increasing Margin Adversarial (IMA) Training, to improve DNN robustness against adversarial noises. During training, the IMA method increases the margins of training samples by moving the decision boundaries of the DNN model far away from the training samples to improve robustness. The IMA method is evaluated on six publicly available datasets (including a COVID-19 CT image dataset) under strong 100-PGD white-box adversarial attacks, and the results show that the proposed method significantly improved classification accuracy on noisy data while keeping a relatively high accuracy on clean data. We hope our approach may facilitate the development of robust DNN applications, especially for COVID-19 diagnosis using CT images.", "one-sentence_summary": "A new adversarial training method with individualized margin estimation to improve robustness against adversarial noises.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ma|increasingmargin_adversarial_ima_training_to_improve_adversarial_robustness_of_neural_networks", "pdf": "/pdf/8884767e3a049fa5a9b870efd97785195a020d0b.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=a_ELrYBlEo", "_bibtex": "@misc{\nma2021increasingmargin,\ntitle={Increasing-Margin Adversarial ({\\{}IMA{\\}}) training to Improve Adversarial Robustness of Neural Networks},\nauthor={Linhai Ma and Liang Liang},\nyear={2021},\nurl={https://openreview.net/forum?id=LDSeViRs4-Q}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "LDSeViRs4-Q", "replyto": "LDSeViRs4-Q", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1309/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538121652, "tmdate": 1606915762896, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1309/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1309/-/Official_Review"}}}, {"id": "8IxutXESc6q", "original": null, "number": 2, "cdate": 1603595268120, "ddate": null, "tcdate": 1603595268120, "tmdate": 1605024477084, "tddate": null, "forum": "LDSeViRs4-Q", "replyto": "LDSeViRs4-Q", "invitation": "ICLR.cc/2021/Conference/Paper1309/-/Official_Review", "content": {"title": "The paper proposed an approach to increase the robustness of neural networks for classification tasks. The intuition of the method is to increase the margin of the training samples. The experimental performance of the method is in general on par with the state of the art method.", "review": "In general, the paper has a good quality. The idea is based on a common intuition that adversarial attacks are most influential to the points close to the decision boundary. The proposed algorithm IMA makes effective use of this intuition and adopts an alternating training process. As an experimental work, the experimental performance of IMA is on par with the state of the art in the experimental settings considered in the paper. This work is important to the ML community. It would be interesting to see further exploration of the algorithm in different testing settings. \nThe paper is written clearly. There is no difficulty in understanding the content.\nExperimental details are provided. \n\nDetailed comments:\n1. In (vanilla) adversarial training, the choice of max perturbation $\\epsilon_\\max$ is usually crucial to the performance of the classifier on noisy and standard data. Is the performance of IMA also that sensitive to the choice of $\\epsilon_\\max$? \nAnd it is briefly mentioned in section 3.3 that IMA might indicate a good $\\epsilon$ for vanilla adversarial training. But this does not say anything about the choice of $\\epsilon_\\max$ for IMA. And this could be very important to its performance (on clean and noisy data).\n2. What might happen to the performance of the method under different choices of $\\beta$? It might be interesting to see how IMA deals with the well-known trade-off between robust and standard accuracy, which is currently one of the main concerns of adversarial training methods.\n\nOther cons:\n1. Figures are not readable when printed. \n \nGiven the above concerns, my initial rating is 6. This may change given further detail of the paper.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1309/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1309/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Increasing-Margin Adversarial (IMA) training to Improve Adversarial Robustness of Neural Networks", "authorids": ["~Linhai_Ma1", "~Liang_Liang2"], "authors": ["Linhai Ma", "Liang Liang"], "keywords": ["Robustness", "CNN", "Medical image classification"], "abstract": "Deep neural networks (DNNs), including convolutional neural networks, are known to be vulnerable to adversarial attacks, which may lead to disastrous consequences in life-critical applications. Adversarial samples are usually generated by attack algorithms and can also be induced by white noises, and therefore the threats are real. In this study, we propose a novel training method, named Increasing Margin Adversarial (IMA) Training, to improve DNN robustness against adversarial noises. During training, the IMA method increases the margins of training samples by moving the decision boundaries of the DNN model far away from the training samples to improve robustness. The IMA method is evaluated on six publicly available datasets (including a COVID-19 CT image dataset) under strong 100-PGD white-box adversarial attacks, and the results show that the proposed method significantly improved classification accuracy on noisy data while keeping a relatively high accuracy on clean data. We hope our approach may facilitate the development of robust DNN applications, especially for COVID-19 diagnosis using CT images.", "one-sentence_summary": "A new adversarial training method with individualized margin estimation to improve robustness against adversarial noises.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ma|increasingmargin_adversarial_ima_training_to_improve_adversarial_robustness_of_neural_networks", "pdf": "/pdf/8884767e3a049fa5a9b870efd97785195a020d0b.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=a_ELrYBlEo", "_bibtex": "@misc{\nma2021increasingmargin,\ntitle={Increasing-Margin Adversarial ({\\{}IMA{\\}}) training to Improve Adversarial Robustness of Neural Networks},\nauthor={Linhai Ma and Liang Liang},\nyear={2021},\nurl={https://openreview.net/forum?id=LDSeViRs4-Q}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "LDSeViRs4-Q", "replyto": "LDSeViRs4-Q", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1309/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538121652, "tmdate": 1606915762896, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1309/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1309/-/Official_Review"}}}, {"id": "GGVSbbFcynM", "original": null, "number": 4, "cdate": 1603843826038, "ddate": null, "tcdate": 1603843826038, "tmdate": 1605024477015, "tddate": null, "forum": "LDSeViRs4-Q", "replyto": "LDSeViRs4-Q", "invitation": "ICLR.cc/2021/Conference/Paper1309/-/Official_Review", "content": {"title": "Theoretically working but not significant with real data", "review": "The authors propose a new training method, named Increasing Margin Adversarial (IMA) training, to improve DNN robustness against adversarial noises. The IMA method increases the margins of training samples by moving the decision boundaries of the DNN model far away from the training samples to improve robustness. Under strong 100-PGD whitebox adversarial attacks, the authors evaluated the IMA method on four publicly available datasets.\n\nOverall, I vote for ok but not goor enough - rejection. The proposed strategy sounds reasonable and worked well with simple dataset, the Moons dataset. However, when it was applied to more complicated real dataset such as Fashion-MINST, SVHN, and COVID-19 CT image dataset; there was no significant achievement if compare to the MMA approaches. Thus further investigation is needed to convince benefit of the IMA on real datasets.\n\nIn addition, the authors tested only one medical image dataset, COVID-19 CT image dataset. Since there are multiple modalities in the medical field and the diversity among datasets are quite large, it is too early to emphasize the advantage of the proposed method in the medical field in general like the last phrase in the conclusion \u201cWe hope our apporach may facilitate the development of robust DNNs, especially in the medical field.\u201d\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1309/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1309/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Increasing-Margin Adversarial (IMA) training to Improve Adversarial Robustness of Neural Networks", "authorids": ["~Linhai_Ma1", "~Liang_Liang2"], "authors": ["Linhai Ma", "Liang Liang"], "keywords": ["Robustness", "CNN", "Medical image classification"], "abstract": "Deep neural networks (DNNs), including convolutional neural networks, are known to be vulnerable to adversarial attacks, which may lead to disastrous consequences in life-critical applications. Adversarial samples are usually generated by attack algorithms and can also be induced by white noises, and therefore the threats are real. In this study, we propose a novel training method, named Increasing Margin Adversarial (IMA) Training, to improve DNN robustness against adversarial noises. During training, the IMA method increases the margins of training samples by moving the decision boundaries of the DNN model far away from the training samples to improve robustness. The IMA method is evaluated on six publicly available datasets (including a COVID-19 CT image dataset) under strong 100-PGD white-box adversarial attacks, and the results show that the proposed method significantly improved classification accuracy on noisy data while keeping a relatively high accuracy on clean data. We hope our approach may facilitate the development of robust DNN applications, especially for COVID-19 diagnosis using CT images.", "one-sentence_summary": "A new adversarial training method with individualized margin estimation to improve robustness against adversarial noises.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ma|increasingmargin_adversarial_ima_training_to_improve_adversarial_robustness_of_neural_networks", "pdf": "/pdf/8884767e3a049fa5a9b870efd97785195a020d0b.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=a_ELrYBlEo", "_bibtex": "@misc{\nma2021increasingmargin,\ntitle={Increasing-Margin Adversarial ({\\{}IMA{\\}}) training to Improve Adversarial Robustness of Neural Networks},\nauthor={Linhai Ma and Liang Liang},\nyear={2021},\nurl={https://openreview.net/forum?id=LDSeViRs4-Q}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "LDSeViRs4-Q", "replyto": "LDSeViRs4-Q", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1309/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538121652, "tmdate": 1606915762896, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1309/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1309/-/Official_Review"}}}], "count": 14}