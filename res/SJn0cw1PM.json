{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1528124460999, "tcdate": 1518463700114, "number": 227, "cdate": 1518463700114, "id": "SJn0cw1PM", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "SJn0cw1PM", "signatures": ["~Jeremy_Bernstein1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "Compression by the signs: distributed learning is a two-way street", "abstract": "Training large neural networks requires distributing learning over multiple workers. The rate limiting step is often in sending gradients from workers to parameter server and back again. We present signSGD with majority vote: the first gradient compression scheme to achieve 1-bit compression of worker-server communication in both directions with non-vacuous theoretical guarantees. To achieve this, we build an extensive theory of sign-based optimisation, which is also relevant to understanding adaptive gradient methods like Adam and RMSprop. We prove that signSGD can get the best of both worlds: compressed gradients and SGD-level convergence rate. signSGD can exploit mismatches between L1 and L2 geometry: when noise and curvature are much sparser than the gradients, signSGD is expected to converge at the same rate or faster than full-precision SGD. Measurements of the L1 versus L2 geometry of real networks support our theoretical claims, and we find that the momentum counterpart of signSGD is able to match the accuracy and convergence speed of Adam on deep Imagenet models.", "paperhash": "bernstein|compression_by_the_signs_distributed_learning_is_a_twoway_street", "keywords": ["distributed", "non-convex", "optimization", "adaptive", "gradient", "adam", "signum", "signsgd", "majority vote", "democracy"], "_bibtex": "@misc{\n  bernstein2018compression,\n  title={Compression by the signs: distributed learning is a two-way street},\n  author={Jeremy Bernstein and Yu-Xiang Wang and Kamyar Azizzadenesheli and Anima Anandkumar},\n  year={2018},\n  url={https://openreview.net/forum?id=SJn0cw1PM}\n}", "authorids": ["bernstein@caltech.edu", "yuxiangw@amazon.com", "kazizzad@uci.edu", "anima@amazon.com"], "authors": ["Jeremy Bernstein", "Yu-Xiang Wang", "Kamyar Azizzadenesheli", "Anima Anandkumar"], "TL;DR": "The parameter server is holding a referendum. GPUs must vote with their belief about the true gradient sign. Majority rule allows 1-bit gradient compression---in both directions between workers and parameter server---whilst keeping an SGD-level convergence rate.", "pdf": "/pdf/4c06b31f9fd6d45cc7dedae2c064de3845e0e66d.pdf"}, "nonreaders": [], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}}, {"tddate": null, "ddate": null, "tmdate": 1521664751019, "tcdate": 1521664751019, "number": 1, "cdate": 1521664751019, "id": "r1vx7rlqM", "invitation": "ICLR.cc/2018/Workshop/-/Paper227/Public_Comment", "forum": "SJn0cw1PM", "replyto": "HydCvQftG", "signatures": ["~Jeremy_Bernstein1"], "readers": ["everyone"], "writers": ["~Jeremy_Bernstein1"], "content": {"title": "Thanks for reviewing the work", "comment": "Dear anonReviewer, thanks a lot for your feedback.\n\nThe main observation about our theoretical bounds is that they depend on L1 norms of gradient, noise and stochasticity. Depending on how \"well-distributed\" these quantities are across the dimensions of the problem (i.e. ratio of L1 norm to L2 norm), signSGD can either be expected to perform better or worse than SGD. For Resnet-20 we show (Fig. 1, top-left) that noise and gradient are similarly \"well-distributed\", consistent with the empirical observation that signSGD converges at approx the same speed as SGD for deep nets. \n\nIn Fig.1, bottom-left, we compare the algorithms on Imagenet.\n\nWe provide all proofs and a much more detailed discussion of the intuition here: https://arxiv.org/abs/1802.04434\n\nThanks again!"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Compression by the signs: distributed learning is a two-way street", "abstract": "Training large neural networks requires distributing learning over multiple workers. The rate limiting step is often in sending gradients from workers to parameter server and back again. We present signSGD with majority vote: the first gradient compression scheme to achieve 1-bit compression of worker-server communication in both directions with non-vacuous theoretical guarantees. To achieve this, we build an extensive theory of sign-based optimisation, which is also relevant to understanding adaptive gradient methods like Adam and RMSprop. We prove that signSGD can get the best of both worlds: compressed gradients and SGD-level convergence rate. signSGD can exploit mismatches between L1 and L2 geometry: when noise and curvature are much sparser than the gradients, signSGD is expected to converge at the same rate or faster than full-precision SGD. Measurements of the L1 versus L2 geometry of real networks support our theoretical claims, and we find that the momentum counterpart of signSGD is able to match the accuracy and convergence speed of Adam on deep Imagenet models.", "paperhash": "bernstein|compression_by_the_signs_distributed_learning_is_a_twoway_street", "keywords": ["distributed", "non-convex", "optimization", "adaptive", "gradient", "adam", "signum", "signsgd", "majority vote", "democracy"], "_bibtex": "@misc{\n  bernstein2018compression,\n  title={Compression by the signs: distributed learning is a two-way street},\n  author={Jeremy Bernstein and Yu-Xiang Wang and Kamyar Azizzadenesheli and Anima Anandkumar},\n  year={2018},\n  url={https://openreview.net/forum?id=SJn0cw1PM}\n}", "authorids": ["bernstein@caltech.edu", "yuxiangw@amazon.com", "kazizzad@uci.edu", "anima@amazon.com"], "authors": ["Jeremy Bernstein", "Yu-Xiang Wang", "Kamyar Azizzadenesheli", "Anima Anandkumar"], "TL;DR": "The parameter server is holding a referendum. GPUs must vote with their belief about the true gradient sign. Majority rule allows 1-bit gradient compression---in both directions between workers and parameter server---whilst keeping an SGD-level convergence rate.", "pdf": "/pdf/4c06b31f9fd6d45cc7dedae2c064de3845e0e66d.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518712624603, "id": "ICLR.cc/2018/Workshop/-/Paper227/Public_Comment", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper227/Reviewers"], "reply": {"replyto": null, "forum": "SJn0cw1PM", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1518712624603}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582931943, "tcdate": 1520325883508, "number": 1, "cdate": 1520325883508, "id": "ry7ZB0oOf", "invitation": "ICLR.cc/2018/Workshop/-/Paper227/Official_Review", "forum": "SJn0cw1PM", "replyto": "SJn0cw1PM", "signatures": ["ICLR.cc/2018/Workshop/Paper227/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper227/AnonReviewer3"], "content": {"title": "Nice paper", "rating": "7: Good paper, accept", "review": "This paper discusses SignGD, an alternation of GD that uses only the sign of the gradient components. Under several assumptions, the authors provide guarantees that ensure convergence to a stationary point. They also compare their method to SGD for a deep learning task.\n\nTheoretical part:\nThe theoretical part is nice and illustrates the convergence rate of SignGD to a stationary point.\nUnfortunately, it seems like in theory SignGD  is slow compared to GD. Nevertheless, SignGD has a benefit in the distributed setting since we only have to communicate signs rather then full gradients.  \n\n\nExperimental part:\n-The experiments are interesting and show that SignGD has comparable convergence to Adam and GD.\n\n\nI recommend to accept the paper to ICLR workshop.\n", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Compression by the signs: distributed learning is a two-way street", "abstract": "Training large neural networks requires distributing learning over multiple workers. The rate limiting step is often in sending gradients from workers to parameter server and back again. We present signSGD with majority vote: the first gradient compression scheme to achieve 1-bit compression of worker-server communication in both directions with non-vacuous theoretical guarantees. To achieve this, we build an extensive theory of sign-based optimisation, which is also relevant to understanding adaptive gradient methods like Adam and RMSprop. We prove that signSGD can get the best of both worlds: compressed gradients and SGD-level convergence rate. signSGD can exploit mismatches between L1 and L2 geometry: when noise and curvature are much sparser than the gradients, signSGD is expected to converge at the same rate or faster than full-precision SGD. Measurements of the L1 versus L2 geometry of real networks support our theoretical claims, and we find that the momentum counterpart of signSGD is able to match the accuracy and convergence speed of Adam on deep Imagenet models.", "paperhash": "bernstein|compression_by_the_signs_distributed_learning_is_a_twoway_street", "keywords": ["distributed", "non-convex", "optimization", "adaptive", "gradient", "adam", "signum", "signsgd", "majority vote", "democracy"], "_bibtex": "@misc{\n  bernstein2018compression,\n  title={Compression by the signs: distributed learning is a two-way street},\n  author={Jeremy Bernstein and Yu-Xiang Wang and Kamyar Azizzadenesheli and Anima Anandkumar},\n  year={2018},\n  url={https://openreview.net/forum?id=SJn0cw1PM}\n}", "authorids": ["bernstein@caltech.edu", "yuxiangw@amazon.com", "kazizzad@uci.edu", "anima@amazon.com"], "authors": ["Jeremy Bernstein", "Yu-Xiang Wang", "Kamyar Azizzadenesheli", "Anima Anandkumar"], "TL;DR": "The parameter server is holding a referendum. GPUs must vote with their belief about the true gradient sign. Majority rule allows 1-bit gradient compression---in both directions between workers and parameter server---whilst keeping an SGD-level convergence rate.", "pdf": "/pdf/4c06b31f9fd6d45cc7dedae2c064de3845e0e66d.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582931749, "id": "ICLR.cc/2018/Workshop/-/Paper227/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper227/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper227/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper227/AnonReviewer1"], "reply": {"forum": "SJn0cw1PM", "replyto": "SJn0cw1PM", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper227/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper227/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582931749}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582670892, "tcdate": 1520740303771, "number": 2, "cdate": 1520740303771, "id": "HydCvQftG", "invitation": "ICLR.cc/2018/Workshop/-/Paper227/Official_Review", "forum": "SJn0cw1PM", "replyto": "SJn0cw1PM", "signatures": ["ICLR.cc/2018/Workshop/Paper227/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper227/AnonReviewer1"], "content": {"title": "A solid contribution, but needs more explanation", "rating": "7: Good paper, accept", "review": "\nIn this paper, the authors provide a series of new statistical algorithms which are particular suitable for distributed learning. The key difference lies between the proposed algorithm and the traditional SGD is that the update only relies on the sign of the stochastic gradient. Due to such update rule, the algorithm only requires 1-bit information about the gradient estimation, therefore, the communication cost will be largely reduced. The authors also provide the convergence guarantee of the algorithm, as well as the empirical comparison on the ResNet-20 on CIFAR-10, where the proposed algorithm achieves better results.\n\nI vote for acceptance of this paper. My only concern is the explanation of the algorithm and the convergence theorem. Specifically, the intuition why only the sign of the gradient estimation is enough for the convergence of the algorithm is not clear. I understand the details of the technical proof of the convergence are omitted due to the space limitation. However, to convey a fully reasonable idea, at least the intuitation and the interpretation of the theorem should be added. \n\nSecondly, as one of the major benefits of such algorithm is its efficiency in terms of the communication cost, it will be great if the empirical communication cost is compared with SGD and its variants, e.g., RMSprop and ADAM.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Compression by the signs: distributed learning is a two-way street", "abstract": "Training large neural networks requires distributing learning over multiple workers. The rate limiting step is often in sending gradients from workers to parameter server and back again. We present signSGD with majority vote: the first gradient compression scheme to achieve 1-bit compression of worker-server communication in both directions with non-vacuous theoretical guarantees. To achieve this, we build an extensive theory of sign-based optimisation, which is also relevant to understanding adaptive gradient methods like Adam and RMSprop. We prove that signSGD can get the best of both worlds: compressed gradients and SGD-level convergence rate. signSGD can exploit mismatches between L1 and L2 geometry: when noise and curvature are much sparser than the gradients, signSGD is expected to converge at the same rate or faster than full-precision SGD. Measurements of the L1 versus L2 geometry of real networks support our theoretical claims, and we find that the momentum counterpart of signSGD is able to match the accuracy and convergence speed of Adam on deep Imagenet models.", "paperhash": "bernstein|compression_by_the_signs_distributed_learning_is_a_twoway_street", "keywords": ["distributed", "non-convex", "optimization", "adaptive", "gradient", "adam", "signum", "signsgd", "majority vote", "democracy"], "_bibtex": "@misc{\n  bernstein2018compression,\n  title={Compression by the signs: distributed learning is a two-way street},\n  author={Jeremy Bernstein and Yu-Xiang Wang and Kamyar Azizzadenesheli and Anima Anandkumar},\n  year={2018},\n  url={https://openreview.net/forum?id=SJn0cw1PM}\n}", "authorids": ["bernstein@caltech.edu", "yuxiangw@amazon.com", "kazizzad@uci.edu", "anima@amazon.com"], "authors": ["Jeremy Bernstein", "Yu-Xiang Wang", "Kamyar Azizzadenesheli", "Anima Anandkumar"], "TL;DR": "The parameter server is holding a referendum. GPUs must vote with their belief about the true gradient sign. Majority rule allows 1-bit gradient compression---in both directions between workers and parameter server---whilst keeping an SGD-level convergence rate.", "pdf": "/pdf/4c06b31f9fd6d45cc7dedae2c064de3845e0e66d.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582931749, "id": "ICLR.cc/2018/Workshop/-/Paper227/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper227/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper227/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper227/AnonReviewer1"], "reply": {"forum": "SJn0cw1PM", "replyto": "SJn0cw1PM", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper227/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper227/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582931749}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521573552103, "tcdate": 1521573552103, "number": 39, "cdate": 1521573551749, "id": "r1u30ACtM", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "SJn0cw1PM", "replyto": "SJn0cw1PM", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Accept", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "Congratulations, your paper was accepted to the ICLR workshop."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Compression by the signs: distributed learning is a two-way street", "abstract": "Training large neural networks requires distributing learning over multiple workers. The rate limiting step is often in sending gradients from workers to parameter server and back again. We present signSGD with majority vote: the first gradient compression scheme to achieve 1-bit compression of worker-server communication in both directions with non-vacuous theoretical guarantees. To achieve this, we build an extensive theory of sign-based optimisation, which is also relevant to understanding adaptive gradient methods like Adam and RMSprop. We prove that signSGD can get the best of both worlds: compressed gradients and SGD-level convergence rate. signSGD can exploit mismatches between L1 and L2 geometry: when noise and curvature are much sparser than the gradients, signSGD is expected to converge at the same rate or faster than full-precision SGD. Measurements of the L1 versus L2 geometry of real networks support our theoretical claims, and we find that the momentum counterpart of signSGD is able to match the accuracy and convergence speed of Adam on deep Imagenet models.", "paperhash": "bernstein|compression_by_the_signs_distributed_learning_is_a_twoway_street", "keywords": ["distributed", "non-convex", "optimization", "adaptive", "gradient", "adam", "signum", "signsgd", "majority vote", "democracy"], "_bibtex": "@misc{\n  bernstein2018compression,\n  title={Compression by the signs: distributed learning is a two-way street},\n  author={Jeremy Bernstein and Yu-Xiang Wang and Kamyar Azizzadenesheli and Anima Anandkumar},\n  year={2018},\n  url={https://openreview.net/forum?id=SJn0cw1PM}\n}", "authorids": ["bernstein@caltech.edu", "yuxiangw@amazon.com", "kazizzad@uci.edu", "anima@amazon.com"], "authors": ["Jeremy Bernstein", "Yu-Xiang Wang", "Kamyar Azizzadenesheli", "Anima Anandkumar"], "TL;DR": "The parameter server is holding a referendum. GPUs must vote with their belief about the true gradient sign. Majority rule allows 1-bit gradient compression---in both directions between workers and parameter server---whilst keeping an SGD-level convergence rate.", "pdf": "/pdf/4c06b31f9fd6d45cc7dedae2c064de3845e0e66d.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}], "count": 5}