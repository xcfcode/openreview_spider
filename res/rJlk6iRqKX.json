{"notes": [{"id": "rJlk6iRqKX", "original": "H1gG02Jctm", "number": 763, "cdate": 1538087862694, "ddate": null, "tcdate": 1538087862694, "tmdate": 1553415052634, "tddate": null, "forum": "rJlk6iRqKX", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Query-Efficient Hard-label Black-box Attack: An Optimization-based Approach", "abstract": "We study the problem of attacking machine learning models in the hard-label black-box setting, where no model information is revealed except that the attacker can make queries to probe the corresponding hard-label decisions. This is a very challenging problem since the direct extension of state-of-the-art white-box attacks (e.g., C&W or PGD) to the hard-label black-box setting will require minimizing a non-continuous step function, which is combinatorial and cannot be solved by a gradient-based optimizer. The only two current approaches are based on random walk on the boundary (Brendel et al., 2017) and random trials to evaluate the loss function (Ilyas et al., 2018), which require lots of queries and lacks convergence guarantees. \nWe propose a novel way to formulate the hard-label black-box attack as a real-valued optimization problem which is usually continuous and can be solved by any zeroth order optimization algorithm. For example, using the Randomized Gradient-Free method (Nesterov & Spokoiny, 2017), we are able to bound the number of iterations needed for our algorithm to achieve stationary points under mild assumptions. We demonstrate that our proposed method outperforms the previous stochastic approaches to attacking convolutional neural networks on MNIST, CIFAR, and ImageNet datasets. More interestingly, we show that the proposed algorithm can also be used to attack other discrete and non-continuous machine learning models, such as Gradient Boosting Decision Trees (GBDT).", "keywords": ["Adversarial example", "Hard-label", "Black-box attack", "Query-efficient"], "authorids": ["mhcheng@ucla.edu", "thmle@ucdavis.edu", "pin-yu.chen@ibm.com", "huan@huan-zhang.com", "yijinfeng@jd.com", "chohsieh@cs.ucla.edu"], "authors": ["Minhao Cheng", "Thong Le", "Pin-Yu Chen", "Huan Zhang", "JinFeng Yi", "Cho-Jui Hsieh"], "pdf": "/pdf/182080868c2b5739e95727e69ac2b0c58401dad8.pdf", "paperhash": "cheng|queryefficient_hardlabel_blackbox_attack_an_optimizationbased_approach", "_bibtex": "@inproceedings{\ncheng2018queryefficient,\ntitle={Query-Efficient Hard-label Black-box Attack: An Optimization-based Approach},\nauthor={Minhao Cheng and Thong Le and Pin-Yu Chen and Huan Zhang and JinFeng Yi and Cho-Jui Hsieh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJlk6iRqKX},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "B1xZLc5HgV", "original": null, "number": 1, "cdate": 1545083464674, "ddate": null, "tcdate": 1545083464674, "tmdate": 1545354480655, "tddate": null, "forum": "rJlk6iRqKX", "replyto": "rJlk6iRqKX", "invitation": "ICLR.cc/2019/Conference/-/Paper763/Meta_Review", "content": {"metareview": "The reviewers liked the clarity of the material and agreed the experimental study is convincing. Accept.", "confidence": "5: The area chair is absolutely certain", "recommendation": "Accept (Poster)", "title": "Good paper, accept"}, "signatures": ["ICLR.cc/2019/Conference/Paper763/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper763/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Query-Efficient Hard-label Black-box Attack: An Optimization-based Approach", "abstract": "We study the problem of attacking machine learning models in the hard-label black-box setting, where no model information is revealed except that the attacker can make queries to probe the corresponding hard-label decisions. This is a very challenging problem since the direct extension of state-of-the-art white-box attacks (e.g., C&W or PGD) to the hard-label black-box setting will require minimizing a non-continuous step function, which is combinatorial and cannot be solved by a gradient-based optimizer. The only two current approaches are based on random walk on the boundary (Brendel et al., 2017) and random trials to evaluate the loss function (Ilyas et al., 2018), which require lots of queries and lacks convergence guarantees. \nWe propose a novel way to formulate the hard-label black-box attack as a real-valued optimization problem which is usually continuous and can be solved by any zeroth order optimization algorithm. For example, using the Randomized Gradient-Free method (Nesterov & Spokoiny, 2017), we are able to bound the number of iterations needed for our algorithm to achieve stationary points under mild assumptions. We demonstrate that our proposed method outperforms the previous stochastic approaches to attacking convolutional neural networks on MNIST, CIFAR, and ImageNet datasets. More interestingly, we show that the proposed algorithm can also be used to attack other discrete and non-continuous machine learning models, such as Gradient Boosting Decision Trees (GBDT).", "keywords": ["Adversarial example", "Hard-label", "Black-box attack", "Query-efficient"], "authorids": ["mhcheng@ucla.edu", "thmle@ucdavis.edu", "pin-yu.chen@ibm.com", "huan@huan-zhang.com", "yijinfeng@jd.com", "chohsieh@cs.ucla.edu"], "authors": ["Minhao Cheng", "Thong Le", "Pin-Yu Chen", "Huan Zhang", "JinFeng Yi", "Cho-Jui Hsieh"], "pdf": "/pdf/182080868c2b5739e95727e69ac2b0c58401dad8.pdf", "paperhash": "cheng|queryefficient_hardlabel_blackbox_attack_an_optimizationbased_approach", "_bibtex": "@inproceedings{\ncheng2018queryefficient,\ntitle={Query-Efficient Hard-label Black-box Attack: An Optimization-based Approach},\nauthor={Minhao Cheng and Thong Le and Pin-Yu Chen and Huan Zhang and JinFeng Yi and Cho-Jui Hsieh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJlk6iRqKX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper763/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353094723, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJlk6iRqKX", "replyto": "rJlk6iRqKX", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper763/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper763/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper763/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353094723}}}, {"id": "rkltT1X9pX", "original": null, "number": 3, "cdate": 1542234049105, "ddate": null, "tcdate": 1542234049105, "tmdate": 1542234330753, "tddate": null, "forum": "rJlk6iRqKX", "replyto": "Hyxsb3Ha2m", "invitation": "ICLR.cc/2019/Conference/-/Paper763/Official_Comment", "content": {"title": "Thanks for the review and we have answered your questions as follows", "comment": "1. Without additional assumptions, we couldn\u2019t prove g(theta) is continuous for general deep neural networks. It\u2019s true that the g(theta) may not be continuous; for example, we think it might be possible to construct some counter-examples using ReLU activation. However, although the assumption may not hold for DNN or GBDT globally, our algorithm still performs well in practice. Moreover, we have illustrated that the decision boundaries of DNN and GBDT are mostly smooth (Fig2,3) in some examples. What we can assure is that if $g(\\cdot)$ has Lipschitz continuous gradient, our algorithm has such a theoretical guarantee. Moreover, based on the same analysis in section 7 of [21], the condition can be relaxed to Lipschitz continuous. This is indeed a sufficient but not necessary condition.\n\n2. We use random directions instead of any format of attack to generate adversarial examples. More specifically, we generate i.i.d. random directions \\theta_1, \u2026 \\theta_n from Gaussian, and  for each of them check whether it\u2019s successful or not (successful if $f(x_i+\\theta)\\neq y_i$). We have added more details in the revised paper.\n\n3.  We have provided the tree models description in 4.1.3.\n\nWe don\u2019t really know any tree-based model that can achieve similar performance with CNN on ImageNet, but GBDT is still useful for many real-world data science applications (e.g., it\u2019s the most common model for click-through rate predictions). We would like to stress that it\u2019s not our focus to discuss whether GBDT is useful or not. The aim of attacking GBDT is to prove that our algorithm can also be used to attack other discrete and non-continuous machine learning models, which couldn\u2019t be done by current gradient-based attack methods. \n\n4. About the approximation of L-inf norm: yes, we could directly apply opt-attack on L-inf norm without any modification. However, we find it harder to optimize in practice because of the additional max term. With the approximation, the function of g is more smooth than previous, which leads to faster convergence.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper763/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper763/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper763/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Query-Efficient Hard-label Black-box Attack: An Optimization-based Approach", "abstract": "We study the problem of attacking machine learning models in the hard-label black-box setting, where no model information is revealed except that the attacker can make queries to probe the corresponding hard-label decisions. This is a very challenging problem since the direct extension of state-of-the-art white-box attacks (e.g., C&W or PGD) to the hard-label black-box setting will require minimizing a non-continuous step function, which is combinatorial and cannot be solved by a gradient-based optimizer. The only two current approaches are based on random walk on the boundary (Brendel et al., 2017) and random trials to evaluate the loss function (Ilyas et al., 2018), which require lots of queries and lacks convergence guarantees. \nWe propose a novel way to formulate the hard-label black-box attack as a real-valued optimization problem which is usually continuous and can be solved by any zeroth order optimization algorithm. For example, using the Randomized Gradient-Free method (Nesterov & Spokoiny, 2017), we are able to bound the number of iterations needed for our algorithm to achieve stationary points under mild assumptions. We demonstrate that our proposed method outperforms the previous stochastic approaches to attacking convolutional neural networks on MNIST, CIFAR, and ImageNet datasets. More interestingly, we show that the proposed algorithm can also be used to attack other discrete and non-continuous machine learning models, such as Gradient Boosting Decision Trees (GBDT).", "keywords": ["Adversarial example", "Hard-label", "Black-box attack", "Query-efficient"], "authorids": ["mhcheng@ucla.edu", "thmle@ucdavis.edu", "pin-yu.chen@ibm.com", "huan@huan-zhang.com", "yijinfeng@jd.com", "chohsieh@cs.ucla.edu"], "authors": ["Minhao Cheng", "Thong Le", "Pin-Yu Chen", "Huan Zhang", "JinFeng Yi", "Cho-Jui Hsieh"], "pdf": "/pdf/182080868c2b5739e95727e69ac2b0c58401dad8.pdf", "paperhash": "cheng|queryefficient_hardlabel_blackbox_attack_an_optimizationbased_approach", "_bibtex": "@inproceedings{\ncheng2018queryefficient,\ntitle={Query-Efficient Hard-label Black-box Attack: An Optimization-based Approach},\nauthor={Minhao Cheng and Thong Le and Pin-Yu Chen and Huan Zhang and JinFeng Yi and Cho-Jui Hsieh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJlk6iRqKX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper763/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621620171, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJlk6iRqKX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper763/Authors", "ICLR.cc/2019/Conference/Paper763/Reviewers", "ICLR.cc/2019/Conference/Paper763/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper763/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper763/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper763/Authors|ICLR.cc/2019/Conference/Paper763/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper763/Reviewers", "ICLR.cc/2019/Conference/Paper763/Authors", "ICLR.cc/2019/Conference/Paper763/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621620171}}}, {"id": "BkebK1X967", "original": null, "number": 2, "cdate": 1542233977081, "ddate": null, "tcdate": 1542233977081, "tmdate": 1542233977081, "tddate": null, "forum": "rJlk6iRqKX", "replyto": "HJgnVJuGaQ", "invitation": "ICLR.cc/2019/Conference/-/Paper763/Official_Comment", "content": {"title": "Thanks for the positive review and we have some clarifications ", "comment": "Thanks for the positive reviews. We agree that white-box setting could be a better way to evaluate the model\u2019s robustness. However, if an attacker wants to attack a system in the real world, it\u2019s usually in the black-box setting. In practice, commercial systems like Google Cloud vision only output the top-1 or top-k predictions to users, which is the same with our hard-label black-box setting, where no model information is revealed except that the attacker can make queries to probe the corresponding hard-label decisions. We agree that 20k queries are still too much and how to reduce the number of queries is still an open and challenging problem. "}, "signatures": ["ICLR.cc/2019/Conference/Paper763/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper763/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper763/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Query-Efficient Hard-label Black-box Attack: An Optimization-based Approach", "abstract": "We study the problem of attacking machine learning models in the hard-label black-box setting, where no model information is revealed except that the attacker can make queries to probe the corresponding hard-label decisions. This is a very challenging problem since the direct extension of state-of-the-art white-box attacks (e.g., C&W or PGD) to the hard-label black-box setting will require minimizing a non-continuous step function, which is combinatorial and cannot be solved by a gradient-based optimizer. The only two current approaches are based on random walk on the boundary (Brendel et al., 2017) and random trials to evaluate the loss function (Ilyas et al., 2018), which require lots of queries and lacks convergence guarantees. \nWe propose a novel way to formulate the hard-label black-box attack as a real-valued optimization problem which is usually continuous and can be solved by any zeroth order optimization algorithm. For example, using the Randomized Gradient-Free method (Nesterov & Spokoiny, 2017), we are able to bound the number of iterations needed for our algorithm to achieve stationary points under mild assumptions. We demonstrate that our proposed method outperforms the previous stochastic approaches to attacking convolutional neural networks on MNIST, CIFAR, and ImageNet datasets. More interestingly, we show that the proposed algorithm can also be used to attack other discrete and non-continuous machine learning models, such as Gradient Boosting Decision Trees (GBDT).", "keywords": ["Adversarial example", "Hard-label", "Black-box attack", "Query-efficient"], "authorids": ["mhcheng@ucla.edu", "thmle@ucdavis.edu", "pin-yu.chen@ibm.com", "huan@huan-zhang.com", "yijinfeng@jd.com", "chohsieh@cs.ucla.edu"], "authors": ["Minhao Cheng", "Thong Le", "Pin-Yu Chen", "Huan Zhang", "JinFeng Yi", "Cho-Jui Hsieh"], "pdf": "/pdf/182080868c2b5739e95727e69ac2b0c58401dad8.pdf", "paperhash": "cheng|queryefficient_hardlabel_blackbox_attack_an_optimizationbased_approach", "_bibtex": "@inproceedings{\ncheng2018queryefficient,\ntitle={Query-Efficient Hard-label Black-box Attack: An Optimization-based Approach},\nauthor={Minhao Cheng and Thong Le and Pin-Yu Chen and Huan Zhang and JinFeng Yi and Cho-Jui Hsieh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJlk6iRqKX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper763/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621620171, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJlk6iRqKX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper763/Authors", "ICLR.cc/2019/Conference/Paper763/Reviewers", "ICLR.cc/2019/Conference/Paper763/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper763/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper763/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper763/Authors|ICLR.cc/2019/Conference/Paper763/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper763/Reviewers", "ICLR.cc/2019/Conference/Paper763/Authors", "ICLR.cc/2019/Conference/Paper763/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621620171}}}, {"id": "Skx3G1mqpQ", "original": null, "number": 1, "cdate": 1542233876201, "ddate": null, "tcdate": 1542233876201, "tmdate": 1542233876201, "tddate": null, "forum": "rJlk6iRqKX", "replyto": "SJefbO63hQ", "invitation": "ICLR.cc/2019/Conference/-/Paper763/Official_Comment", "content": {"title": "Thanks for the great suggestions and we have revised our paper as follows:", "comment": "Thanks for the positive reviews and valuable suggestions.\n\n1. We have modified equation (4-5) to min_{\\lambda>0} {\\lambda} s.t. f(x_0+\\lambda \\theta/||\\theta||) != y_0  as you suggested.\n\n2. Section 3.1: Yes, you are right. We have followed your suggestion by changing \u201cfine-grained\u201d to \u201ccoarse-grained\u201d in the revision. \n\n3. Algorithm 2: \nA. We have included all implementation details above section 4 following your suggestion in revision.\nB. We have added a new table to show how the performance varies with the number of sampled directions per step u_t in Appendix 6.2.\n\n4. The performance in CIFAR10: During the experiment, we found that CIFAR is more sensitive to the initial direction in our method. Although we find a relatively small distortion direction at first, the method sometimes converges to a worse point than Boundary-attack. It could be solved by selecting several directions as initialization and do Algorithm 2 several times. \n\n5. The big O notation in proof: We have followed your suggestion to replace $\\epsilon\\simO()$ to $\\epsilon=O()$ and delete the big O notation with \\beta.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper763/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper763/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper763/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Query-Efficient Hard-label Black-box Attack: An Optimization-based Approach", "abstract": "We study the problem of attacking machine learning models in the hard-label black-box setting, where no model information is revealed except that the attacker can make queries to probe the corresponding hard-label decisions. This is a very challenging problem since the direct extension of state-of-the-art white-box attacks (e.g., C&W or PGD) to the hard-label black-box setting will require minimizing a non-continuous step function, which is combinatorial and cannot be solved by a gradient-based optimizer. The only two current approaches are based on random walk on the boundary (Brendel et al., 2017) and random trials to evaluate the loss function (Ilyas et al., 2018), which require lots of queries and lacks convergence guarantees. \nWe propose a novel way to formulate the hard-label black-box attack as a real-valued optimization problem which is usually continuous and can be solved by any zeroth order optimization algorithm. For example, using the Randomized Gradient-Free method (Nesterov & Spokoiny, 2017), we are able to bound the number of iterations needed for our algorithm to achieve stationary points under mild assumptions. We demonstrate that our proposed method outperforms the previous stochastic approaches to attacking convolutional neural networks on MNIST, CIFAR, and ImageNet datasets. More interestingly, we show that the proposed algorithm can also be used to attack other discrete and non-continuous machine learning models, such as Gradient Boosting Decision Trees (GBDT).", "keywords": ["Adversarial example", "Hard-label", "Black-box attack", "Query-efficient"], "authorids": ["mhcheng@ucla.edu", "thmle@ucdavis.edu", "pin-yu.chen@ibm.com", "huan@huan-zhang.com", "yijinfeng@jd.com", "chohsieh@cs.ucla.edu"], "authors": ["Minhao Cheng", "Thong Le", "Pin-Yu Chen", "Huan Zhang", "JinFeng Yi", "Cho-Jui Hsieh"], "pdf": "/pdf/182080868c2b5739e95727e69ac2b0c58401dad8.pdf", "paperhash": "cheng|queryefficient_hardlabel_blackbox_attack_an_optimizationbased_approach", "_bibtex": "@inproceedings{\ncheng2018queryefficient,\ntitle={Query-Efficient Hard-label Black-box Attack: An Optimization-based Approach},\nauthor={Minhao Cheng and Thong Le and Pin-Yu Chen and Huan Zhang and JinFeng Yi and Cho-Jui Hsieh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJlk6iRqKX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper763/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621620171, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJlk6iRqKX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper763/Authors", "ICLR.cc/2019/Conference/Paper763/Reviewers", "ICLR.cc/2019/Conference/Paper763/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper763/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper763/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper763/Authors|ICLR.cc/2019/Conference/Paper763/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper763/Reviewers", "ICLR.cc/2019/Conference/Paper763/Authors", "ICLR.cc/2019/Conference/Paper763/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621620171}}}, {"id": "HJgnVJuGaQ", "original": null, "number": 3, "cdate": 1541730100337, "ddate": null, "tcdate": 1541730100337, "tmdate": 1541730100337, "tddate": null, "forum": "rJlk6iRqKX", "replyto": "rJlk6iRqKX", "invitation": "ICLR.cc/2019/Conference/-/Paper763/Official_Review", "content": {"title": "Nice idea with solid experiments", "review": "In this paper the authors propose optimizing for adversarial examples against black box models by considering minimizing the distance to the decision boundary.  They show that because this gives real valued feedback, the optimizer is able to find closer adversarial examples with fewer queries.  This would be heavily dependent on the model structure (with more complex decision boundaries likely being harder to optimize) but they show empirically in 4 models that this method works well.\n\nI am not convinced that the black box model setting is the most relevant (and 20k queries is still a fair bit), but this is important research nonetheless.  I generally found the writing to be clear and the idea to be elegant; I think readers will value this paper. \n", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper763/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Query-Efficient Hard-label Black-box Attack: An Optimization-based Approach", "abstract": "We study the problem of attacking machine learning models in the hard-label black-box setting, where no model information is revealed except that the attacker can make queries to probe the corresponding hard-label decisions. This is a very challenging problem since the direct extension of state-of-the-art white-box attacks (e.g., C&W or PGD) to the hard-label black-box setting will require minimizing a non-continuous step function, which is combinatorial and cannot be solved by a gradient-based optimizer. The only two current approaches are based on random walk on the boundary (Brendel et al., 2017) and random trials to evaluate the loss function (Ilyas et al., 2018), which require lots of queries and lacks convergence guarantees. \nWe propose a novel way to formulate the hard-label black-box attack as a real-valued optimization problem which is usually continuous and can be solved by any zeroth order optimization algorithm. For example, using the Randomized Gradient-Free method (Nesterov & Spokoiny, 2017), we are able to bound the number of iterations needed for our algorithm to achieve stationary points under mild assumptions. We demonstrate that our proposed method outperforms the previous stochastic approaches to attacking convolutional neural networks on MNIST, CIFAR, and ImageNet datasets. More interestingly, we show that the proposed algorithm can also be used to attack other discrete and non-continuous machine learning models, such as Gradient Boosting Decision Trees (GBDT).", "keywords": ["Adversarial example", "Hard-label", "Black-box attack", "Query-efficient"], "authorids": ["mhcheng@ucla.edu", "thmle@ucdavis.edu", "pin-yu.chen@ibm.com", "huan@huan-zhang.com", "yijinfeng@jd.com", "chohsieh@cs.ucla.edu"], "authors": ["Minhao Cheng", "Thong Le", "Pin-Yu Chen", "Huan Zhang", "JinFeng Yi", "Cho-Jui Hsieh"], "pdf": "/pdf/182080868c2b5739e95727e69ac2b0c58401dad8.pdf", "paperhash": "cheng|queryefficient_hardlabel_blackbox_attack_an_optimizationbased_approach", "_bibtex": "@inproceedings{\ncheng2018queryefficient,\ntitle={Query-Efficient Hard-label Black-box Attack: An Optimization-based Approach},\nauthor={Minhao Cheng and Thong Le and Pin-Yu Chen and Huan Zhang and JinFeng Yi and Cho-Jui Hsieh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJlk6iRqKX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper763/Official_Review", "cdate": 1542234381954, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rJlk6iRqKX", "replyto": "rJlk6iRqKX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper763/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335797584, "tmdate": 1552335797584, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper763/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "Hyxsb3Ha2m", "original": null, "number": 2, "cdate": 1541393410811, "ddate": null, "tcdate": 1541393410811, "tmdate": 1541533708942, "tddate": null, "forum": "rJlk6iRqKX", "replyto": "rJlk6iRqKX", "invitation": "ICLR.cc/2019/Conference/-/Paper763/Official_Review", "content": {"title": "Interesting paper", "review": "This paper proposed a reformulation of objective function to solve the hard-label black-box attack problem. The idea is interesting and the performance of the proposed method seem to be capable of finding adversarial examples with smaller distortions and less queries compared with other hard-label attack algorithms. \n\nThis paper is well-written and clear.\n\n==============================================================================================\nQuestions\n\nA. Can it be proved the g(theta) is continuous? Also, the theoretical analysis assume the property of Lipschitz-smooth and thus obtain the complexity of number of queries. Does this assumption truly hold for g(theta), when f is a neural network classifiers? If so, how to obtain the Lipschitz constant of g that is used in the analysis sec 6.3? \n\nB. What is the random distortion in Table 1? What initialization technique is used for the query direction in the experiments? \n\nC. The GBDT result on MNIST dataset is interesting. The authors should provide tree models description in 4.1.3. However, on larger dataset, say imagenet, are the tree models performance truly comparable to ImageNet? If the test accuracy is low, then it seems less meaningful to compare the adversarial distortion with that of imagenet neural network classifiers. Please explain. \n\nD. For sec 3.2, it is not clear why the approximation is needed. Because the gradient of g with respect to theta is using equation (7) and theta is already given (from sampling); thus the Linf norm of theta is a constant. Why do we need the approximation? Given that, will there be any problems on the L2 norm case? \n", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper763/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Query-Efficient Hard-label Black-box Attack: An Optimization-based Approach", "abstract": "We study the problem of attacking machine learning models in the hard-label black-box setting, where no model information is revealed except that the attacker can make queries to probe the corresponding hard-label decisions. This is a very challenging problem since the direct extension of state-of-the-art white-box attacks (e.g., C&W or PGD) to the hard-label black-box setting will require minimizing a non-continuous step function, which is combinatorial and cannot be solved by a gradient-based optimizer. The only two current approaches are based on random walk on the boundary (Brendel et al., 2017) and random trials to evaluate the loss function (Ilyas et al., 2018), which require lots of queries and lacks convergence guarantees. \nWe propose a novel way to formulate the hard-label black-box attack as a real-valued optimization problem which is usually continuous and can be solved by any zeroth order optimization algorithm. For example, using the Randomized Gradient-Free method (Nesterov & Spokoiny, 2017), we are able to bound the number of iterations needed for our algorithm to achieve stationary points under mild assumptions. We demonstrate that our proposed method outperforms the previous stochastic approaches to attacking convolutional neural networks on MNIST, CIFAR, and ImageNet datasets. More interestingly, we show that the proposed algorithm can also be used to attack other discrete and non-continuous machine learning models, such as Gradient Boosting Decision Trees (GBDT).", "keywords": ["Adversarial example", "Hard-label", "Black-box attack", "Query-efficient"], "authorids": ["mhcheng@ucla.edu", "thmle@ucdavis.edu", "pin-yu.chen@ibm.com", "huan@huan-zhang.com", "yijinfeng@jd.com", "chohsieh@cs.ucla.edu"], "authors": ["Minhao Cheng", "Thong Le", "Pin-Yu Chen", "Huan Zhang", "JinFeng Yi", "Cho-Jui Hsieh"], "pdf": "/pdf/182080868c2b5739e95727e69ac2b0c58401dad8.pdf", "paperhash": "cheng|queryefficient_hardlabel_blackbox_attack_an_optimizationbased_approach", "_bibtex": "@inproceedings{\ncheng2018queryefficient,\ntitle={Query-Efficient Hard-label Black-box Attack: An Optimization-based Approach},\nauthor={Minhao Cheng and Thong Le and Pin-Yu Chen and Huan Zhang and JinFeng Yi and Cho-Jui Hsieh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJlk6iRqKX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper763/Official_Review", "cdate": 1542234381954, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rJlk6iRqKX", "replyto": "rJlk6iRqKX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper763/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335797584, "tmdate": 1552335797584, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper763/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SJefbO63hQ", "original": null, "number": 1, "cdate": 1541359610270, "ddate": null, "tcdate": 1541359610270, "tmdate": 1541533708735, "tddate": null, "forum": "rJlk6iRqKX", "replyto": "rJlk6iRqKX", "invitation": "ICLR.cc/2019/Conference/-/Paper763/Official_Review", "content": {"title": "well-written paper with good empirical results", "review": "This paper addresses black-box classifier attacks in the \u201chard-label\u201d setting, meaning that the only information the attacker has access to is single top-1 label predictions. Relative to even the standard black-box setting where the attacker has access to the per-class logits or probabilities, this setting is difficult as it makes the optimization landscape non-smooth. The proposed approach reformulates the optimization problem such that the outer-loop optimizes the direction using approximate gradients, and the inner-loop estimates the distance to the nearest attack in a given direction. The results show that the proposed approach successfully finds both untargeted and targeted adversarial examples for classifiers of various image datasets (including ImageNet), usually with substantially better query-efficiency and better final results (lower distance and/or higher success rate) than competing methods.\n\n=====================================\n\nPros:\n\nVery well-written and readable paper with good background and context for those (like me) who don\u2019t closely follow the literature on adversarial attacks. Figs. 1-3 are nice visual aids for understanding the problem and optimization landscape.\n\nNovel formulation and approach that appears to be well-motivated from the literature on randomized gradient-free search methods. Novel theoretical analysis in Appendix that generalizes prior work to approximations (although, see notes below).\n\nGood empirical results showing that the method is capable of query-efficiently finding attacks of classifiers on real-world datasets including ImageNet. Also shows that the model needn\u2019t be differentiable to be subject to such attacks by demonstrating the approach on a decision-tree based classifier. Appears to compare to and usually outperform appropriate baselines from prior work (though I\u2019m not very familiar with the literature here).\n\n=====================================\n\nCons/questions/suggestions/nitpicks:\n\nEq 4-5: why \\texttt argmin? Inconsistent with other min/maxes.\n\nEq 4-5: Though I understood the intention, I think the equations are incorrect as written: argmin_{\\lambda} { F(\\lambda) } of a binary-valued function F would produce the set of all \\lambdas that make F=0, rather than the smallest \\lambda that makes F=1. I think it should be something like:\n\nmin_{\\lambda>0} {\\lambda}\ns.t. f(x_0+\\lambda \\theta/||\\theta||) != y_0\n\nSec 3.1: why call the first search \u201cfine-grained\u201d? Isn\u2019t the binary search more fine-grained? I\u2019d suggest changing it to \u201ccoarse-grained\u201d unless I\u2019m misunderstanding something.\n\nAlgorithm 2: it would be nice if this included all the tricks described as \u201cimplementation details\u201d in the paragraph right before Sec. 4 -- e.g. averaging over multiple sampled directions u_t and line search to choose the step size \\eta. These seem important and not necessarily obvious to me.\n\nAlgorithm 2: it could be interesting to show how performance varies with number of sampled directions per step u_t.\n\nSec: 4.1.2: why might your algorithm perform worse than boundary-attack on targeted attacks for CIFAR classifiers? Would like to have seen at least a hypothesis on this.\n\nSec 6.3 Theorem 1: I think the theorem statement is a bit imprecise. There is an abuse of big-O notation here -- O(f(n)) is a set, not a quantity, so statements such as \\epsilon ~ O(...) and \\beta <= O(...) and \u201cat most O(...)\u201d are not well-defined (though common in informal settings) and the latter two are redundant given the meaning of O as an upper bound. The original theorem from [Nesterov & Spokoiny 2017] that this Theorem 1 would generalize doesn\u2019t rely on big-O notation -- I think following the same conventions here might improve the theorem and proof.\n\n=====================================\n\nOverall, this is a good paper with nice exposition, addressing a challenging but practically useful problem setting and proposing a novel and well-motivated approach with strong empirical results.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper763/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Query-Efficient Hard-label Black-box Attack: An Optimization-based Approach", "abstract": "We study the problem of attacking machine learning models in the hard-label black-box setting, where no model information is revealed except that the attacker can make queries to probe the corresponding hard-label decisions. This is a very challenging problem since the direct extension of state-of-the-art white-box attacks (e.g., C&W or PGD) to the hard-label black-box setting will require minimizing a non-continuous step function, which is combinatorial and cannot be solved by a gradient-based optimizer. The only two current approaches are based on random walk on the boundary (Brendel et al., 2017) and random trials to evaluate the loss function (Ilyas et al., 2018), which require lots of queries and lacks convergence guarantees. \nWe propose a novel way to formulate the hard-label black-box attack as a real-valued optimization problem which is usually continuous and can be solved by any zeroth order optimization algorithm. For example, using the Randomized Gradient-Free method (Nesterov & Spokoiny, 2017), we are able to bound the number of iterations needed for our algorithm to achieve stationary points under mild assumptions. We demonstrate that our proposed method outperforms the previous stochastic approaches to attacking convolutional neural networks on MNIST, CIFAR, and ImageNet datasets. More interestingly, we show that the proposed algorithm can also be used to attack other discrete and non-continuous machine learning models, such as Gradient Boosting Decision Trees (GBDT).", "keywords": ["Adversarial example", "Hard-label", "Black-box attack", "Query-efficient"], "authorids": ["mhcheng@ucla.edu", "thmle@ucdavis.edu", "pin-yu.chen@ibm.com", "huan@huan-zhang.com", "yijinfeng@jd.com", "chohsieh@cs.ucla.edu"], "authors": ["Minhao Cheng", "Thong Le", "Pin-Yu Chen", "Huan Zhang", "JinFeng Yi", "Cho-Jui Hsieh"], "pdf": "/pdf/182080868c2b5739e95727e69ac2b0c58401dad8.pdf", "paperhash": "cheng|queryefficient_hardlabel_blackbox_attack_an_optimizationbased_approach", "_bibtex": "@inproceedings{\ncheng2018queryefficient,\ntitle={Query-Efficient Hard-label Black-box Attack: An Optimization-based Approach},\nauthor={Minhao Cheng and Thong Le and Pin-Yu Chen and Huan Zhang and JinFeng Yi and Cho-Jui Hsieh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJlk6iRqKX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper763/Official_Review", "cdate": 1542234381954, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rJlk6iRqKX", "replyto": "rJlk6iRqKX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper763/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335797584, "tmdate": 1552335797584, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper763/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 8}