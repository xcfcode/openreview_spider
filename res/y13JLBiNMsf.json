{"notes": [{"id": "y13JLBiNMsf", "original": "dLyeBkuXOO", "number": 796, "cdate": 1601308092472, "ddate": null, "tcdate": 1601308092472, "tmdate": 1614985701345, "tddate": null, "forum": "y13JLBiNMsf", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Learning Monotonic Alignments with Source-Aware GMM Attention", "authorids": ["~Tae_Gyoon_Kang1", "hogyeong.kim@samsung.com", "minjoong.lee@samsung.com", "jihyun.s.lee@samsung.com", "~Seongmin_Ok1", "hoshik.lee@samsung.com", "macho@samsung.com"], "authors": ["Tae Gyoon Kang", "Ho-Gyeong Kim", "Min-Joong Lee", "Jihyun Lee", "Seongmin Ok", "Hoshik Lee", "Young Sang Choi"], "keywords": ["Monotonic alignments", "sequence-to-sequence model", "aligned attention", "streaming speech recognition", "long-form speech recognition"], "abstract": "Transformers with soft attention have been widely adopted in various sequence-to-sequence (Seq2Seq) tasks. Whereas soft attention is effective for learning semantic similarities between queries and keys based on their contents, it does not explicitly model the order of elements in sequences which is crucial for monotonic Seq2Seq tasks. Learning monotonic alignments between input and output sequences may be beneficial for long-form and online inference applications that are still challenging for the conventional soft attention algorithm. Herein, we focus on monotonic Seq2Seq tasks and propose a source-aware Gaussian mixture model attention in which the attention scores are monotonically calculated considering both the content and order of the source sequence. We experimentally demonstrate that the proposed attention mechanism improved the performance on the online and long-form speech recognition problems without performance degradation in offline in-distribution speech recognition.", "one-sentence_summary": "We focus on monotonic sequence-to-sequence tasks and propose source-aware GMM attention which enables online inference and improves long-form sequence generation performance in speech recognition.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kang|learning_monotonic_alignments_with_sourceaware_gmm_attention", "pdf": "/pdf/40b2a952edd850d01b072503e4919dee464c895e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=6uMvA4Tt6_", "_bibtex": "@misc{\nkang2021learning,\ntitle={Learning Monotonic Alignments with Source-Aware {\\{}GMM{\\}} Attention},\nauthor={Tae Gyoon Kang and Ho-Gyeong Kim and Min-Joong Lee and Jihyun Lee and Seongmin Ok and Hoshik Lee and Young Sang Choi},\nyear={2021},\nurl={https://openreview.net/forum?id=y13JLBiNMsf}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "rwUAlDXvFEX", "original": null, "number": 1, "cdate": 1610040443792, "ddate": null, "tcdate": 1610040443792, "tmdate": 1610474045160, "tddate": null, "forum": "y13JLBiNMsf", "replyto": "y13JLBiNMsf", "invitation": "ICLR.cc/2021/Conference/Paper796/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "The paper proposed a useful incremental extension to the monotonic GMM attention by incorporating source content.\nIt has shown comparable performance for online and long-form speech recognition, but falls behind on the machine translation task. For online ASR, it would be more convincing to include latency comparisons across different streaming models besides WERs. \nThe presentation of the paper can be further improved although it already got better based on reviewers' comments.\nAs in the discussion, a more accurate description of the method would be \"multi-head Gaussian attention\" instead of GMM attention.\n\nThe main factor for the decision is limited novelty and clarity can be further improved."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Monotonic Alignments with Source-Aware GMM Attention", "authorids": ["~Tae_Gyoon_Kang1", "hogyeong.kim@samsung.com", "minjoong.lee@samsung.com", "jihyun.s.lee@samsung.com", "~Seongmin_Ok1", "hoshik.lee@samsung.com", "macho@samsung.com"], "authors": ["Tae Gyoon Kang", "Ho-Gyeong Kim", "Min-Joong Lee", "Jihyun Lee", "Seongmin Ok", "Hoshik Lee", "Young Sang Choi"], "keywords": ["Monotonic alignments", "sequence-to-sequence model", "aligned attention", "streaming speech recognition", "long-form speech recognition"], "abstract": "Transformers with soft attention have been widely adopted in various sequence-to-sequence (Seq2Seq) tasks. Whereas soft attention is effective for learning semantic similarities between queries and keys based on their contents, it does not explicitly model the order of elements in sequences which is crucial for monotonic Seq2Seq tasks. Learning monotonic alignments between input and output sequences may be beneficial for long-form and online inference applications that are still challenging for the conventional soft attention algorithm. Herein, we focus on monotonic Seq2Seq tasks and propose a source-aware Gaussian mixture model attention in which the attention scores are monotonically calculated considering both the content and order of the source sequence. We experimentally demonstrate that the proposed attention mechanism improved the performance on the online and long-form speech recognition problems without performance degradation in offline in-distribution speech recognition.", "one-sentence_summary": "We focus on monotonic sequence-to-sequence tasks and propose source-aware GMM attention which enables online inference and improves long-form sequence generation performance in speech recognition.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kang|learning_monotonic_alignments_with_sourceaware_gmm_attention", "pdf": "/pdf/40b2a952edd850d01b072503e4919dee464c895e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=6uMvA4Tt6_", "_bibtex": "@misc{\nkang2021learning,\ntitle={Learning Monotonic Alignments with Source-Aware {\\{}GMM{\\}} Attention},\nauthor={Tae Gyoon Kang and Ho-Gyeong Kim and Min-Joong Lee and Jihyun Lee and Seongmin Ok and Hoshik Lee and Young Sang Choi},\nyear={2021},\nurl={https://openreview.net/forum?id=y13JLBiNMsf}\n}"}, "tags": [], "invitation": {"reply": {"forum": "y13JLBiNMsf", "replyto": "y13JLBiNMsf", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040443778, "tmdate": 1610474045144, "id": "ICLR.cc/2021/Conference/Paper796/-/Decision"}}}, {"id": "T_mB6hAshhy", "original": null, "number": 3, "cdate": 1603946364418, "ddate": null, "tcdate": 1603946364418, "tmdate": 1607028458846, "tddate": null, "forum": "y13JLBiNMsf", "replyto": "y13JLBiNMsf", "invitation": "ICLR.cc/2021/Conference/Paper796/-/Official_Review", "content": {"title": "Cool tweak to GMM attention, but very specific to ASR. ", "review": "Summary:\nThis paper introduces \u201csource-aware\u201d GMM attention and applies it to offline, online, long-form ASR.  The value of source-aware GMM attention appears to be its ability to \u201cignore\u201d long segments of silence in the input audio, which could potentially be more difficult to do using other attention mechanisms.  Fairly competitive results are presented for offline ASR.  For online ASR, the results are state-of-the-art amongst sequence-to-sequence-based models. \n\nReasons for score: \nThe main contribution of this paper seems to be the addition of predicted weights for each encoder step that allow the monotonic GMM attention mechanism to ignore certain input frames.  This seems to be directly useful in ASR; however, I\u2019m not sure if it has any direct use outside of ASR.  The paper is also fairly hard to follow and not well motivated.  Because it is so ASR specific, I feel that it would be a better fit for a speech conference.  \n\n\nHigh-level Comments:\n* Despite being a relatively minor tweak, the use of source-aware weights for each encoder step is a cool idea that integrates nicely into existing GMM-based attention mechanisms. \n* As mentioned above, I felt that the paper was fairly hard to follow and not well motivated.  It took me until Figure 2 on page 6 to realize that the main benefit of using the source-aware mechanism was to ignore long segments of silence.  It would be good to provide this motivation earlier in the paper. \n\n\nDetailed Comments:\n* I can\u2019t figure out why there are 4 attention plots in Figure 2 (it says it\u2019s single-head).  It might be a good idea to clarify in the caption. \n* I would recommend having your submission proof-read for English style and grammar issues.  The issues are subtle but addressing them would help to improve readability. \n* Eq. (14) seems unnecessary (it follows from basic calculus and doesn\u2019t apply in actual usage).  Is there a reason it was included? \n* End of Section 2.2, \u201cMoreover, the GMM attention score is uni-modal and cannot discard the non-informative tokens in attention window\u201d: I'm not sure what is meant by \"uni-modal\" here.  A mixture of unimodal distributions (such as a GMM) is multi-modal. \n* The addition of section 4.3 felt a bit tacked-on.  To me, it doesn\u2019t seem worthwhile to attempt to apply a monotonic attention mechanism specifically designed for ASR to a non-monotonic task like MT.  If indeed there is an interesting contribution to be made via positive constraint relaxation, I would save all of Section 4.3 for the followup paper rather than inserting it at the end of this paper. \n\nUpdate (2020-12-03):  Increasing score from 4 to 5. ", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper796/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper796/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Monotonic Alignments with Source-Aware GMM Attention", "authorids": ["~Tae_Gyoon_Kang1", "hogyeong.kim@samsung.com", "minjoong.lee@samsung.com", "jihyun.s.lee@samsung.com", "~Seongmin_Ok1", "hoshik.lee@samsung.com", "macho@samsung.com"], "authors": ["Tae Gyoon Kang", "Ho-Gyeong Kim", "Min-Joong Lee", "Jihyun Lee", "Seongmin Ok", "Hoshik Lee", "Young Sang Choi"], "keywords": ["Monotonic alignments", "sequence-to-sequence model", "aligned attention", "streaming speech recognition", "long-form speech recognition"], "abstract": "Transformers with soft attention have been widely adopted in various sequence-to-sequence (Seq2Seq) tasks. Whereas soft attention is effective for learning semantic similarities between queries and keys based on their contents, it does not explicitly model the order of elements in sequences which is crucial for monotonic Seq2Seq tasks. Learning monotonic alignments between input and output sequences may be beneficial for long-form and online inference applications that are still challenging for the conventional soft attention algorithm. Herein, we focus on monotonic Seq2Seq tasks and propose a source-aware Gaussian mixture model attention in which the attention scores are monotonically calculated considering both the content and order of the source sequence. We experimentally demonstrate that the proposed attention mechanism improved the performance on the online and long-form speech recognition problems without performance degradation in offline in-distribution speech recognition.", "one-sentence_summary": "We focus on monotonic sequence-to-sequence tasks and propose source-aware GMM attention which enables online inference and improves long-form sequence generation performance in speech recognition.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kang|learning_monotonic_alignments_with_sourceaware_gmm_attention", "pdf": "/pdf/40b2a952edd850d01b072503e4919dee464c895e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=6uMvA4Tt6_", "_bibtex": "@misc{\nkang2021learning,\ntitle={Learning Monotonic Alignments with Source-Aware {\\{}GMM{\\}} Attention},\nauthor={Tae Gyoon Kang and Ho-Gyeong Kim and Min-Joong Lee and Jihyun Lee and Seongmin Ok and Hoshik Lee and Young Sang Choi},\nyear={2021},\nurl={https://openreview.net/forum?id=y13JLBiNMsf}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "y13JLBiNMsf", "replyto": "y13JLBiNMsf", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper796/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538134746, "tmdate": 1606915784856, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper796/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper796/-/Official_Review"}}}, {"id": "yuNvazPMq02", "original": null, "number": 6, "cdate": 1605664558388, "ddate": null, "tcdate": 1605664558388, "tmdate": 1606188049754, "tddate": null, "forum": "y13JLBiNMsf", "replyto": "J6VnSfEbZ60", "invitation": "ICLR.cc/2021/Conference/Paper796/-/Official_Comment", "content": {"title": "Response to Reviewer #2 (1/2)", "comment": "We would like to thank the anonymous reviewers for their valuable feedback, and for the time and effort spent on evaluating our submission. The reviewers raised some interesting points, which we have carefully addressed, and as a result, we believe that the revised draft is significantly improved compared to the first submission.\n\n**Comment** The description of the proposed mechanism is inconsistent with existing literature and is very unclear and confusing in parts.\n\n**Response** Thanks to the reviewers\u2019 comments, we have polished up confusing parts in the paper. We fixed wrong explanation about the multi-modality of conventional GMM. Section 2.4 was revised for readability.\n\n**Comment** Experiments are somewhat incomplete/missing important comparisons, e.g. comparing baseline GMM attention to the proposed \"source-aware\" variant in Tables 1,2,5, and comparing to other location-based attention mechanisms, even if non-monotonic, e.g. from http://papers.nips.cc/paper/5847-attention-based-models-for-speech-recognition.pdf\n\n**Response** We added mentions for location-aware attention in Section 3 and compared our model with the recent study on location-aware attention [Link]( https://ieeexplore.ieee.org/document/8683510). We fine-tuned our soft-attention model with location-aware term and obtained WERs of 3.08/7.92 for test-clean and test-other, respectively. We did not include the results of our location-aware model in the paper because it was not trained from scratch.\n\n**Comment** Overall the writing/language use could use improvement.\n\n**Response** We are polishing up the paper to improve readability. We will update the proofread in the rebuttal stage.\n\n**Comment** Sec 2.2. and throughout the paper: The described \"GMM\" and \"SA-GMM\" attention always use a single component, so don't really count as a Gaussian mixture. Using multiple components would explicitly allow for multimodal attention weights for each output step. Moreover, since the mixing weights are generally computed independently at each step, using multiple components makes it possible for the base GMM attention to \"discard the non-informative tokens\". This mechanism, which would be more precisely called \"Gaussian attention\", is strictly less flexible than the base GMM attention mechanism that was originally described in Graves, 2013.\n\n**Response** The GMM attention model in our paper corresponds to the GMM attention from Graves, 2013 and Battenburg et. al, 2020 [Link](https://arxiv.org/abs/1910.10288). We started from the \u2018v2\u2019 version model of Battenberg et al., 2020  that improved Graves, 2013. The difference between Battenberg's model and our model is that we adopt the multi-head value matrices. When the value matrices are shared for all heads, our model is similar to Battenberg's model. By introducing multi-head value matrices, our model attends to the different representation subspaces simultaneously. Strictly, our model in session 2.2 is \u2018multi-head Gaussian attention\u2019 which is generalized form of the GMM attention. In this paper, we used the same term \u2018GMM attention\u2019 since introducing multi-head value matrices is minor update from previous study. \n\n**Comment** This claim is repeated in paragraph 2 of Sec 3: \"uni-modal similar to conventional GMM attention\". When using multiple mixture components, GMM attention is not unimodal. \n\n**Response** Thanks to the reviewer\u2019s comment, we fixed the wrong description about GMM attention and rewrote as \u2018Moreover, the number of modes in GMM attention is limited by the number of mixture components\u2019. in Section 2.2 and \u2018Also, the update gate for online inference in Lee et al. (2020) is uni-modal.\u2019 In Section 3.\n\n**Comment** Eq 8: The notation here is unclear. Why is there a softmax over $\\phi_{i}^{h}$? Is the softmax computed over all attention heads? Why is this necessary? It seems to impair the training of some heads, at least for SAGMM-tr according to paragraph 4 of Sec. 4.2\n\n**Response** As the reviewer commented, the softmax on $\\phi_{i}^{h}$ is conducted over all attention heads. We revised the notation for softmax over attention heads as $softmax_{h}$. We adopt $\\phi_{i}^{h}$ to match the model with previous studies on GMM attention. Also,  the redundant heads can be easily pruned by $softmax_{h}$  which helps to prevent over-fitting.\n\n**Comment**  Figure 1 is difficult to interpret. The two plots have difference horizontal axes and therefore don't seem to be directly comparable... It's not clear what the \"key width\" in Figure 1b is trying to convey since there is always going to be a single weight per (discrete) encoder step j.\n\n**Response** In Figure 2, (figure 1 in the original paper), the difference between (a) and (b) is non-uniform axis spacing which encodes source contents. $\\delta_{j}$ represents the amount of information of $j$-th encoder token ands helps the SAGMM attention to attend to informative tokens only. "}, "signatures": ["ICLR.cc/2021/Conference/Paper796/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper796/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Monotonic Alignments with Source-Aware GMM Attention", "authorids": ["~Tae_Gyoon_Kang1", "hogyeong.kim@samsung.com", "minjoong.lee@samsung.com", "jihyun.s.lee@samsung.com", "~Seongmin_Ok1", "hoshik.lee@samsung.com", "macho@samsung.com"], "authors": ["Tae Gyoon Kang", "Ho-Gyeong Kim", "Min-Joong Lee", "Jihyun Lee", "Seongmin Ok", "Hoshik Lee", "Young Sang Choi"], "keywords": ["Monotonic alignments", "sequence-to-sequence model", "aligned attention", "streaming speech recognition", "long-form speech recognition"], "abstract": "Transformers with soft attention have been widely adopted in various sequence-to-sequence (Seq2Seq) tasks. Whereas soft attention is effective for learning semantic similarities between queries and keys based on their contents, it does not explicitly model the order of elements in sequences which is crucial for monotonic Seq2Seq tasks. Learning monotonic alignments between input and output sequences may be beneficial for long-form and online inference applications that are still challenging for the conventional soft attention algorithm. Herein, we focus on monotonic Seq2Seq tasks and propose a source-aware Gaussian mixture model attention in which the attention scores are monotonically calculated considering both the content and order of the source sequence. We experimentally demonstrate that the proposed attention mechanism improved the performance on the online and long-form speech recognition problems without performance degradation in offline in-distribution speech recognition.", "one-sentence_summary": "We focus on monotonic sequence-to-sequence tasks and propose source-aware GMM attention which enables online inference and improves long-form sequence generation performance in speech recognition.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kang|learning_monotonic_alignments_with_sourceaware_gmm_attention", "pdf": "/pdf/40b2a952edd850d01b072503e4919dee464c895e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=6uMvA4Tt6_", "_bibtex": "@misc{\nkang2021learning,\ntitle={Learning Monotonic Alignments with Source-Aware {\\{}GMM{\\}} Attention},\nauthor={Tae Gyoon Kang and Ho-Gyeong Kim and Min-Joong Lee and Jihyun Lee and Seongmin Ok and Hoshik Lee and Young Sang Choi},\nyear={2021},\nurl={https://openreview.net/forum?id=y13JLBiNMsf}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "y13JLBiNMsf", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper796/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper796/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper796/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper796/Authors|ICLR.cc/2021/Conference/Paper796/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper796/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923867074, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper796/-/Official_Comment"}}}, {"id": "GUpfkVvSprG", "original": null, "number": 7, "cdate": 1605666220541, "ddate": null, "tcdate": 1605666220541, "tmdate": 1606187005078, "tddate": null, "forum": "y13JLBiNMsf", "replyto": "yuNvazPMq02", "invitation": "ICLR.cc/2021/Conference/Paper796/-/Official_Comment", "content": {"title": "Response to Reviewer #2 (2/2) ", "comment": "**Comment** Sec 2.3. There is no particular motivation given for the proposed method for integrating source keys. Why not include $K_{j}$ in the computation for the standard deviation as well? And why is the same weight $\\delta_{j}$ used as a scaling factor (eq (12)) and the mean offset in eq (10)? These design choices deserve more explanation, and possibly empirical justification.\n\n**Response** Our motivation for SAGMM is to encode source with $\\delta_{j}$ and aggregate information by weighted summation which is analogous to integral of normal distribution on non-uniform axis spacing. The mean offset is directly derived from $\\delta_{j}$ to match the integral of normal distribution.  Matching SAGMM to integral of probability density helps the numerical stability of the SAGMM attention mechanism without softmax operation over encoder tokens. We could not include $K_{j}$ into the normal distribution parameters since the Gaussian distribution is shared for all encoder tokens.\n\n**Comment** Sec 2.4: Is it possible to train SAGMM-tr from scratch? Or does it need to be first trained using SAGMM and then fine-tuning with truncation enabled?\n\n**Response** Training SAGMM-tr from scratch slowed the convergence of model. We recommend to train SAGMM in early stage and fine-tune SAGMM-tr from it.\n\n**Experiments**\n**Comment** Are the different GMM attention variants used encoder self-attention layers as well? Or does the encoder use conventional \"soft attention\"?\n\n**Response** Our models adopt the same the encoder and decoder self-attention layers based on the soft-attention with relative positional encoding. We added the sentence ' The encoder and decoder self-attention employed soft-attention with relative positional encoding' in Sec 4.2 for readability. \n\n**Comment** As above, it seems unfair not to include any experiments using multiple components when comparing different variants of GMM attention.\n\n**Response** We compared our model with location-aware attention in Table 2. \n\n**Comment** Sec 4.2, Table 1: Please clarify the differences between the three models labeled (Ours). Is the difference only in the encoder-decoder attention layer?\n\n**Response** The difference between three models is the encoder-decoder attention layer. We added the explanation as \u2018 We trained and tested models which employed the soft attention, GMM attention, and SAGMM-tr attention mechanisms as an encoder-decoder attention.\u2019 in Section 4.2.\n\n**Comment** English usage. Just a few examples of grammar errors and unclear text, as there are too many to list.\n\n**Response** Thanks to the reviewer\u2019s comments, we fixed the grammar errors in the paper. We are revising the paper and plan to update proofread in the rebuttal stage.\n\n**Comment** page 1, \"mismatch between attention parameters from decoders and information distribution in encoder outputs\". This sentence is difficult to parse. What is the mismatch here? Why would the source encoding and decoder query vector need to \"match\", especially in a purely location-based attention scheme?\n\n**Response** We rewrote confusing sentence and included figure 1 (c) to have a better understanding of GMM attention.\n\n**Comment** page 4: \"fixed length with hyperparameters\" What are they hyperparameters being referred to here?\n\n**Response** We add explanations and equations for SAGMM-tr with fixed attention window width as \u201cThe number of tokens we needs to wait \u2026\u201d in page 5, Sec. 2.4. $c$ refers to the attention window width in equation (17)-(20).\n\n**Comment** page 5, Sec. 4: \"enables early inference\". What does \"early inference\" mean? \n\n**Response** we corrected misleading term \u201cearly inference\u201d to \u201conline inference\u201d.\n\n**Detailed comments**\n> page 1, \"attend subset of long sequences\" is missing a preposition, e.g., \"attend to a subset\". It seems that \"long sequences\" is meant to refer a single source sequence.\n\n>page 5, Sec. 3: \"for the inference\" -> \"for inference\" \n\n>page 5, Sec 4.1: \"1 second speeches\" -> \"1 second long utterances\"\n\n>page 5, Sec 4.1: \"from 30 vocabulary\" -> \"from a vocabulary of 30 words\"\n\n**Response** According to reviewer\u2019s comments, we fixed grammar error in this paper. We will check other grammar issues in the rest of rebuttal stage.\n\n**Comment** Sec 2.2: Sutskever et al., 2014 did not use content-based attention.\n\n**Response** According to the reviewer\u2019s comment, we removed Sutskever et al., 2014 from the content-based attention reference.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper796/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper796/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Monotonic Alignments with Source-Aware GMM Attention", "authorids": ["~Tae_Gyoon_Kang1", "hogyeong.kim@samsung.com", "minjoong.lee@samsung.com", "jihyun.s.lee@samsung.com", "~Seongmin_Ok1", "hoshik.lee@samsung.com", "macho@samsung.com"], "authors": ["Tae Gyoon Kang", "Ho-Gyeong Kim", "Min-Joong Lee", "Jihyun Lee", "Seongmin Ok", "Hoshik Lee", "Young Sang Choi"], "keywords": ["Monotonic alignments", "sequence-to-sequence model", "aligned attention", "streaming speech recognition", "long-form speech recognition"], "abstract": "Transformers with soft attention have been widely adopted in various sequence-to-sequence (Seq2Seq) tasks. Whereas soft attention is effective for learning semantic similarities between queries and keys based on their contents, it does not explicitly model the order of elements in sequences which is crucial for monotonic Seq2Seq tasks. Learning monotonic alignments between input and output sequences may be beneficial for long-form and online inference applications that are still challenging for the conventional soft attention algorithm. Herein, we focus on monotonic Seq2Seq tasks and propose a source-aware Gaussian mixture model attention in which the attention scores are monotonically calculated considering both the content and order of the source sequence. We experimentally demonstrate that the proposed attention mechanism improved the performance on the online and long-form speech recognition problems without performance degradation in offline in-distribution speech recognition.", "one-sentence_summary": "We focus on monotonic sequence-to-sequence tasks and propose source-aware GMM attention which enables online inference and improves long-form sequence generation performance in speech recognition.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kang|learning_monotonic_alignments_with_sourceaware_gmm_attention", "pdf": "/pdf/40b2a952edd850d01b072503e4919dee464c895e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=6uMvA4Tt6_", "_bibtex": "@misc{\nkang2021learning,\ntitle={Learning Monotonic Alignments with Source-Aware {\\{}GMM{\\}} Attention},\nauthor={Tae Gyoon Kang and Ho-Gyeong Kim and Min-Joong Lee and Jihyun Lee and Seongmin Ok and Hoshik Lee and Young Sang Choi},\nyear={2021},\nurl={https://openreview.net/forum?id=y13JLBiNMsf}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "y13JLBiNMsf", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper796/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper796/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper796/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper796/Authors|ICLR.cc/2021/Conference/Paper796/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper796/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923867074, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper796/-/Official_Comment"}}}, {"id": "1IqtyRAsNji", "original": null, "number": 8, "cdate": 1606096366589, "ddate": null, "tcdate": 1606096366589, "tmdate": 1606096366589, "tddate": null, "forum": "y13JLBiNMsf", "replyto": "y13JLBiNMsf", "invitation": "ICLR.cc/2021/Conference/Paper796/-/Official_Comment", "content": {"title": "Upload proofread", "comment": "We revised grammar errors on our paper for better readability. We would like to thank the anonymous reviewers for their valuable feedback."}, "signatures": ["ICLR.cc/2021/Conference/Paper796/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper796/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Monotonic Alignments with Source-Aware GMM Attention", "authorids": ["~Tae_Gyoon_Kang1", "hogyeong.kim@samsung.com", "minjoong.lee@samsung.com", "jihyun.s.lee@samsung.com", "~Seongmin_Ok1", "hoshik.lee@samsung.com", "macho@samsung.com"], "authors": ["Tae Gyoon Kang", "Ho-Gyeong Kim", "Min-Joong Lee", "Jihyun Lee", "Seongmin Ok", "Hoshik Lee", "Young Sang Choi"], "keywords": ["Monotonic alignments", "sequence-to-sequence model", "aligned attention", "streaming speech recognition", "long-form speech recognition"], "abstract": "Transformers with soft attention have been widely adopted in various sequence-to-sequence (Seq2Seq) tasks. Whereas soft attention is effective for learning semantic similarities between queries and keys based on their contents, it does not explicitly model the order of elements in sequences which is crucial for monotonic Seq2Seq tasks. Learning monotonic alignments between input and output sequences may be beneficial for long-form and online inference applications that are still challenging for the conventional soft attention algorithm. Herein, we focus on monotonic Seq2Seq tasks and propose a source-aware Gaussian mixture model attention in which the attention scores are monotonically calculated considering both the content and order of the source sequence. We experimentally demonstrate that the proposed attention mechanism improved the performance on the online and long-form speech recognition problems without performance degradation in offline in-distribution speech recognition.", "one-sentence_summary": "We focus on monotonic sequence-to-sequence tasks and propose source-aware GMM attention which enables online inference and improves long-form sequence generation performance in speech recognition.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kang|learning_monotonic_alignments_with_sourceaware_gmm_attention", "pdf": "/pdf/40b2a952edd850d01b072503e4919dee464c895e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=6uMvA4Tt6_", "_bibtex": "@misc{\nkang2021learning,\ntitle={Learning Monotonic Alignments with Source-Aware {\\{}GMM{\\}} Attention},\nauthor={Tae Gyoon Kang and Ho-Gyeong Kim and Min-Joong Lee and Jihyun Lee and Seongmin Ok and Hoshik Lee and Young Sang Choi},\nyear={2021},\nurl={https://openreview.net/forum?id=y13JLBiNMsf}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "y13JLBiNMsf", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper796/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper796/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper796/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper796/Authors|ICLR.cc/2021/Conference/Paper796/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper796/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923867074, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper796/-/Official_Comment"}}}, {"id": "w1AUpymr16E", "original": null, "number": 5, "cdate": 1605612634389, "ddate": null, "tcdate": 1605612634389, "tmdate": 1605612634389, "tddate": null, "forum": "y13JLBiNMsf", "replyto": "bu1M1cEdNr", "invitation": "ICLR.cc/2021/Conference/Paper796/-/Official_Comment", "content": {"title": "Response to Reviewer #1 ", "comment": "We would like to thank the anonymous reviewers for their valuable feedback, and for the time and effort spent on evaluating our submission. The reviewers raised some interesting points, which we have carefully addressed, and as a result, we believe that the revised draft is significantly improved compared to the first submission. \n\n**Comment** In the introduction, \"The GMM attention is a pure location-aware algorithm in which the model selects attention windows cumulatively without considering source contents.\": This is a little bit hard to understand. I understood it after I followed Section 2. It would be better to make this expression more understandable. \n\n**Response** We rewrote the sentence for readability as \u201cThe GMM attention is a pure location-aware algorithm in which the encoder contents are not considered during attention score calculation.\u201d \n\n**Comment** I'm curious about the GMM's behavior (how the Gaussian parameters would be changed for each layer and each head). These are described in the appendix, but the authors may consider discussing it in the main paper. It is well known that the general soft attentions' behaviors are very different in the lower and higher layers. The lower-layer attentions are blur and less monotonic. I would like to ask the authors to discuss how the GMM parameters can adapt to capture such behaviors in the main paper. \n\n**Response** We added the paragraph \u2018Figure 3 shows...\u2019 in the subsection 4.2. The lower layers of SAGMM-tr have their attentions more spread-out over a consecutive subsequence of the encoder tokens. The higher layers utilize the context vectors from the lower encoder-decoder attentions and relatively more focused, partly resembling the behavior of soft attentions.\n\n**Comment** Equation (4): It's better to explain why the softplus function is used for \\Delta and \\Sigma. (monotonicity and positive constraint of the variance, right?) \n\n**Response**\nYes, we applied the softplus function to keep the monotonicity and positive constraints. The softplus function has been also motivated by the previous study in speech synthesis [Link](https://arxiv.org/abs/1910.10288).\n\n**Comment**  Equation (9): It's better to have some explanation about why $\\delta$ is bounded by [0, 1]. \n\n**Response** As shown in Figure 1, our goal is to extend the GMM attention by allowing variability in the interval lengths. Thus, the lower bound set to zero is natural. On the other hand, an interval too wide would harm the approximation of Gaussian distribution and we need some upper bound, and sigmoid is used widely to smoothly bound an arbitrary real number to fixed interval. Furthermore, CIF [Link](https://arxiv.org/abs/1905.11235) had similar idea for example.\n\n**Comment** Equation (14): Is there proof? \n\n**Response** Thanks to the reviewers\u2019 comments, we removed Equation (14) from the original paper and revised the paragraph. The equation (14) was intended to show the similarity between attention score summation and integral of normal distribution. However, it was confusing and redundant in the paper.\n\nIn the paragraph \u2018The attention score for each encoder token\u2026\u2019 in page 4, we raised a point that since the GMM-based attentions are analogous to integral of normal distribution, the sum of attention score does not diverge without softmax operation over encoder tokens. Then, we can train model easily without numerical instability. The softmax-free property of SAGMM attention leads to online inference in SAGMM-tr by truncating long-tail of the normal distributions. \n\n**Comment** Section 2.4: Does the truncation make training faster? \n\n**Response** No, the truncation did not reduce the training time.\n\n**Comment** Section 4: The authors should clarify why they did not use a language model. Besides, it's better to clarify that the method does not use the language model in the table's caption (Is that correct? All the results listed in the table are without LMs, right?). The readers remember the Librispeech number with the LM, and they may confuse it. \n\n**Response** All results in our paper did not use an external language model (LM). We did not employ LM for easy comparison with several previous studies and our main interest is on-device applications in which the external LM is not popular yet. Though we cannot combine a language model in rebuttal stage, the performance with model with LM will be obtained in follow-up study.\n\n\n**Comment** Why does the table 4 result not include the GMM attention result? It's better to compare the normal GMM and the proposed method in the online mode.\n\n**Response**  As shown in Figure 1 and appendix, the truncated GMM model failed to learn the alignments and attended to future contexts during inference. We decided to not include the truncated GMM in table 4 since it could not run online inference in practice. The truncated GMM showed a WER of 4.03% in test-clean and 10.16% in test-other, respectively.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper796/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper796/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Monotonic Alignments with Source-Aware GMM Attention", "authorids": ["~Tae_Gyoon_Kang1", "hogyeong.kim@samsung.com", "minjoong.lee@samsung.com", "jihyun.s.lee@samsung.com", "~Seongmin_Ok1", "hoshik.lee@samsung.com", "macho@samsung.com"], "authors": ["Tae Gyoon Kang", "Ho-Gyeong Kim", "Min-Joong Lee", "Jihyun Lee", "Seongmin Ok", "Hoshik Lee", "Young Sang Choi"], "keywords": ["Monotonic alignments", "sequence-to-sequence model", "aligned attention", "streaming speech recognition", "long-form speech recognition"], "abstract": "Transformers with soft attention have been widely adopted in various sequence-to-sequence (Seq2Seq) tasks. Whereas soft attention is effective for learning semantic similarities between queries and keys based on their contents, it does not explicitly model the order of elements in sequences which is crucial for monotonic Seq2Seq tasks. Learning monotonic alignments between input and output sequences may be beneficial for long-form and online inference applications that are still challenging for the conventional soft attention algorithm. Herein, we focus on monotonic Seq2Seq tasks and propose a source-aware Gaussian mixture model attention in which the attention scores are monotonically calculated considering both the content and order of the source sequence. We experimentally demonstrate that the proposed attention mechanism improved the performance on the online and long-form speech recognition problems without performance degradation in offline in-distribution speech recognition.", "one-sentence_summary": "We focus on monotonic sequence-to-sequence tasks and propose source-aware GMM attention which enables online inference and improves long-form sequence generation performance in speech recognition.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kang|learning_monotonic_alignments_with_sourceaware_gmm_attention", "pdf": "/pdf/40b2a952edd850d01b072503e4919dee464c895e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=6uMvA4Tt6_", "_bibtex": "@misc{\nkang2021learning,\ntitle={Learning Monotonic Alignments with Source-Aware {\\{}GMM{\\}} Attention},\nauthor={Tae Gyoon Kang and Ho-Gyeong Kim and Min-Joong Lee and Jihyun Lee and Seongmin Ok and Hoshik Lee and Young Sang Choi},\nyear={2021},\nurl={https://openreview.net/forum?id=y13JLBiNMsf}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "y13JLBiNMsf", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper796/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper796/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper796/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper796/Authors|ICLR.cc/2021/Conference/Paper796/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper796/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923867074, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper796/-/Official_Comment"}}}, {"id": "r4rg3IS5dUb", "original": null, "number": 4, "cdate": 1605600360191, "ddate": null, "tcdate": 1605600360191, "tmdate": 1605607745357, "tddate": null, "forum": "y13JLBiNMsf", "replyto": "T_mB6hAshhy", "invitation": "ICLR.cc/2021/Conference/Paper796/-/Official_Comment", "content": {"title": "Response to Reviewer #4", "comment": "We would like to thank the anonymous reviewers for their valuable feedback, and for the time and effort spent on evaluating our submission. The reviewers raised some interesting points, which we have carefully addressed, and as a result, we believe that the revised draft is significantly improved compared to the first submission. \n\n**Comment** As mentioned above, I felt that the paper was fairly hard to follow and not well motivated. It took me until Figure 2 on page 6 to realize that the main benefit of using the source-aware mechanism was to ignore long segments of silence. It would be good to provide this motivation earlier in the paper. \n\n**Response** We added the attention plots from soft attention, truncated GMM attention, and SAGMM attention in figure 1. We also attached description on behavior of attention mechanisms in section 1.\n\n**Detailed Comments**\n\n**Comment** I can\u2019t figure out why there are 4 attention plots in Figure 2 (it says it\u2019s single-head). It might be a good idea to clarify in the caption. \n\n**Response** Each subplot in figure 3 (figure 2 in original paper) corresponds to the attention plot for each decoder layers. Four attention plots come from  4 decoder blocks by indicating the encoder-decoder attention.\n\n**Comment** I would recommend having your submission proof-read for English style and grammar issues. The issues are subtle but addressing them would help to improve readability. \n\n**Response** We are polishing up the paper to improve readability. We plan to update the proofread in the rebuttal stage.\n\n**Comment** Eq. (14) seems unnecessary (it follows from basic calculus and doesn\u2019t apply in actual usage). Is there a reason it was included? \n\n**Response** Eq. (14) was included to emphasize the numerical stability of SAGMM algorithm. However, eq. (14) was confusing and seemed redundant in the paper. We removed Eq. (14) and revised the paragraph. In the paragraph \u2018The attention score for each encoder token\u2026\u2019 in page 4, we raised a point that since the GMM-based attentions are analogous to integral of normal distribution, the sum of attention score does not diverge without softmax operation over encoder tokens. Then, we can train model easily without numerical instability. The softmax-free property of SAGMM attention leads to online inference in SAGMM-tr by truncating long-tail of the normal distributions.\n\n**Comment** End of Section 2.2, \u201cMoreover, the GMM attention score is uni-modal and cannot discard the non-informative tokens in attention window\u201d: I'm not sure what is meant by \"uni-modal\" here. A mixture of unimodal distributions (such as a GMM) is multi-modal. \n\n**Response** As the reviewer commented, GMM attention is multi-modal. Thanks to the reviewer\u2019s comment, we have corrected the sentence as \u2018Moreover, the number of modes in GMM attention is limited by the number of mixture components\u2019.\n\n**Comment** The addition of section 4.3 felt a bit tacked-on. To me, it doesn\u2019t seem worthwhile to attempt to apply a monotonic attention mechanism specifically designed for ASR to a non-monotonic task like MT. If indeed there is an interesting contribution to be made via positive constraint relaxation, I would save all of Section 4.3 for the followup paper rather than inserting it at the end of this paper. \n\n**Response** Previous studies on monotonic attention include experimental results in the non-monotonic natural language processing tasks. [Link](https://openreview.net/pdf?id=Hko85plCW) [Link](https://arxiv.org/abs/1906.05218) While our monotonic attention mechanism did not perform well with the non-monotonic tasks, we have experimented and attached the results for readers who would like to know the performance of the SAGMM-tr on non-monotonic tasks.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper796/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper796/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Monotonic Alignments with Source-Aware GMM Attention", "authorids": ["~Tae_Gyoon_Kang1", "hogyeong.kim@samsung.com", "minjoong.lee@samsung.com", "jihyun.s.lee@samsung.com", "~Seongmin_Ok1", "hoshik.lee@samsung.com", "macho@samsung.com"], "authors": ["Tae Gyoon Kang", "Ho-Gyeong Kim", "Min-Joong Lee", "Jihyun Lee", "Seongmin Ok", "Hoshik Lee", "Young Sang Choi"], "keywords": ["Monotonic alignments", "sequence-to-sequence model", "aligned attention", "streaming speech recognition", "long-form speech recognition"], "abstract": "Transformers with soft attention have been widely adopted in various sequence-to-sequence (Seq2Seq) tasks. Whereas soft attention is effective for learning semantic similarities between queries and keys based on their contents, it does not explicitly model the order of elements in sequences which is crucial for monotonic Seq2Seq tasks. Learning monotonic alignments between input and output sequences may be beneficial for long-form and online inference applications that are still challenging for the conventional soft attention algorithm. Herein, we focus on monotonic Seq2Seq tasks and propose a source-aware Gaussian mixture model attention in which the attention scores are monotonically calculated considering both the content and order of the source sequence. We experimentally demonstrate that the proposed attention mechanism improved the performance on the online and long-form speech recognition problems without performance degradation in offline in-distribution speech recognition.", "one-sentence_summary": "We focus on monotonic sequence-to-sequence tasks and propose source-aware GMM attention which enables online inference and improves long-form sequence generation performance in speech recognition.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kang|learning_monotonic_alignments_with_sourceaware_gmm_attention", "pdf": "/pdf/40b2a952edd850d01b072503e4919dee464c895e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=6uMvA4Tt6_", "_bibtex": "@misc{\nkang2021learning,\ntitle={Learning Monotonic Alignments with Source-Aware {\\{}GMM{\\}} Attention},\nauthor={Tae Gyoon Kang and Ho-Gyeong Kim and Min-Joong Lee and Jihyun Lee and Seongmin Ok and Hoshik Lee and Young Sang Choi},\nyear={2021},\nurl={https://openreview.net/forum?id=y13JLBiNMsf}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "y13JLBiNMsf", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper796/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper796/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper796/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper796/Authors|ICLR.cc/2021/Conference/Paper796/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper796/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923867074, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper796/-/Official_Comment"}}}, {"id": "gGam_LYy_rH", "original": null, "number": 3, "cdate": 1605595328467, "ddate": null, "tcdate": 1605595328467, "tmdate": 1605601460653, "tddate": null, "forum": "y13JLBiNMsf", "replyto": "tedUdenMktj", "invitation": "ICLR.cc/2021/Conference/Paper796/-/Official_Comment", "content": {"title": "Response to Reviewer #5 (2/2)", "comment": "**Responses for detailed Comment**\n\n**Comment** ... the proposed attention mechanism solves the online and long-form speech recognition problems ...\nThis is a strong claim. I don't think the paper presents enough evidence that the problem is solved.\n\n**Response** Thanks to the reviewer's comment, we revised the sentence as \u2018\u2026the proposed attention mechanism improves the performance on the online and long-form speech recognition problems\u2026\u2019\n\n**Comment** equation (1), (2), and (3)\n\n This is minor, but the variables soft_\\alpha, head^h, and Multihead can be better typeset.\n\n**Response** We revised typeset as : $head^{h}$ to $H^{h}$, $Multihead$ to  $M$, $Soft$ _ ${\\alpha}, GMM{\\alpha}, ...$ to $\\alpha_{Soft}, \\alpha_{GMM}, ... $\n\n**Comment** equation (4)\n\n This is again minor. \\Sigma_i is a scalar, and it might be better to use the lower case \\sigma_i.\n\n**Response** Thanks to the reviewer\u2019s comment, changed $\\Sigma$ to $\\sigma$ for better readability.\n\n**Comment** equation (13)\n\n What are I and J?\n\n**Response** We add explanation for I and J as \u2018where I and J denote the length of decoder and encoder sequences, respectively\u2019.\n\n**Comment**\\mu_i = \\mu_{i-1} + relu3(\\Delta_i)\n\nWhat is relu3?\n\n**Response** We clarify relu3 activation function as min( max($\\Delta_i$, 0), 3).\n\n**Comment** equation (14)\n\nThis equation is wrong in many ways. I assume j \\to \\infty means having infinitely many time steps, and \\delta_j \\to 0 applies to \\delta_j for all j.\n\n**Response** As response to the above comment, we removed (14) and revised confusing sentences.  The response to above comment also summarizes the revised paragraph. \n\n**Comment**  In streaming mode, while the frames are coming in, how does the decoder know when to produce the next token?\n We tested the window width c = 15 centered by \\mu_i ...\n\nWhat is c?\n\n**Response** We revised the section 2.4 and clarified when the condition to predict the next token. In the adaptive window algorithm, the model needs to wait until $\\nu_{j}$ becomes larger than $\\mu_{i}  + 2\\sqrt{\\sigma_{i}}$. In the fixed window algorithm, the model waits $\\frac{c}{2}$ frames after $\\nu_{j}>\\mu_{i}$. \n\n$c$ is attention window size which leads to maximum latency of the model in equation (20).\n\n**Comment** ... we randomly concatenate the 1-vector after the end of source sentence with probability p_{eos} ...\n This trick is not implemented in other approaches. Have the authors tried without using this trick? How do know if the improvement is purely due to this trick and not about the proposed SAGMM?\n\n**Response** As described in the above comment, the 1-vector trick had minor improvements on SAGMM models. \n\n**Comment** We adopt the CTC loss on the encoder output ...\n\nThe CTC loss is meant to help learn monotonic attention. Why bother using the CTC loss if the proposed attention is already monotonic? Does this suggest that the CTC loss has other added benefits?\n\n**Response** The CTC loss is adopted for fair comparison with previous studies. Without auxiliary CTC loss, the performance of model dropped 0.32% in test-clean set. CTC loss calculates marginalization over all possible alignments, and it may help to emphasize the \"correct\" sequences in the intermediate layers. \n\n**Comment** Streamable model, bi-directional enc. in Table 2\n\nHow are bidirectional encoders in the streaming case? Using a bidirectional encoder by definition cannot be used for streaming unless I missed something.\n\n**Response** We clarified the notation as \u2018Streamable decoders, bi-directional encoders\u2019. All models in table 2 are offline models.\n\n**Comment** Transformer (121M) (Ours) in Table 2\n\nI assume this uses a regular content-based attention. Please clarify. I would even group the Transformer, GMM, and SAGMM-tr rows together, since they provide the control experiments to support the usefulness of the proposed method. The other rows are less useful, since the architectures are different. The comparison of absolute numbers tell us little, except that the numbers provided for the proposed approach are in the right ballpark.\n\n**Response** We clarified the notation as Soft attention, GMM, and SAGMM-tr for readability. The difference between three models is encoder-decoder attention scheme. We also separate our models from previous works for better comparison.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper796/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper796/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Monotonic Alignments with Source-Aware GMM Attention", "authorids": ["~Tae_Gyoon_Kang1", "hogyeong.kim@samsung.com", "minjoong.lee@samsung.com", "jihyun.s.lee@samsung.com", "~Seongmin_Ok1", "hoshik.lee@samsung.com", "macho@samsung.com"], "authors": ["Tae Gyoon Kang", "Ho-Gyeong Kim", "Min-Joong Lee", "Jihyun Lee", "Seongmin Ok", "Hoshik Lee", "Young Sang Choi"], "keywords": ["Monotonic alignments", "sequence-to-sequence model", "aligned attention", "streaming speech recognition", "long-form speech recognition"], "abstract": "Transformers with soft attention have been widely adopted in various sequence-to-sequence (Seq2Seq) tasks. Whereas soft attention is effective for learning semantic similarities between queries and keys based on their contents, it does not explicitly model the order of elements in sequences which is crucial for monotonic Seq2Seq tasks. Learning monotonic alignments between input and output sequences may be beneficial for long-form and online inference applications that are still challenging for the conventional soft attention algorithm. Herein, we focus on monotonic Seq2Seq tasks and propose a source-aware Gaussian mixture model attention in which the attention scores are monotonically calculated considering both the content and order of the source sequence. We experimentally demonstrate that the proposed attention mechanism improved the performance on the online and long-form speech recognition problems without performance degradation in offline in-distribution speech recognition.", "one-sentence_summary": "We focus on monotonic sequence-to-sequence tasks and propose source-aware GMM attention which enables online inference and improves long-form sequence generation performance in speech recognition.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kang|learning_monotonic_alignments_with_sourceaware_gmm_attention", "pdf": "/pdf/40b2a952edd850d01b072503e4919dee464c895e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=6uMvA4Tt6_", "_bibtex": "@misc{\nkang2021learning,\ntitle={Learning Monotonic Alignments with Source-Aware {\\{}GMM{\\}} Attention},\nauthor={Tae Gyoon Kang and Ho-Gyeong Kim and Min-Joong Lee and Jihyun Lee and Seongmin Ok and Hoshik Lee and Young Sang Choi},\nyear={2021},\nurl={https://openreview.net/forum?id=y13JLBiNMsf}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "y13JLBiNMsf", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper796/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper796/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper796/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper796/Authors|ICLR.cc/2021/Conference/Paper796/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper796/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923867074, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper796/-/Official_Comment"}}}, {"id": "tedUdenMktj", "original": null, "number": 2, "cdate": 1605592812911, "ddate": null, "tcdate": 1605592812911, "tmdate": 1605601449027, "tddate": null, "forum": "y13JLBiNMsf", "replyto": "aVr1NevVjQ", "invitation": "ICLR.cc/2021/Conference/Paper796/-/Official_Comment", "content": {"title": "Response to Reviewer #5 (1/2)", "comment": "We would like to thank the anonymous reviewers for their valuable feedback, and for the time and effort spent on evaluating our submission. The reviewers raised some interesting points, which we have carefully addressed, and as a result, we believe that the revised draft is significantly improved compared to the first submission. \n\n**Comment** The first concern is simply that there are errors and ambiguous statements in the paper. For example, equation (14) is obviously wrong, and in section 2.4 it is unclear how many frames the encoder needs to see before the decoder can predict the next token. The latter is crucial because streaming is the major reason behind the use of monotonic attention. A detailed list is given at the end.\n\n**Response** We removed the equation (14) and added explanations about numerical stability. Since the GMM-based attentions are analogous to integral of normal distribution, the sum of attention score does not diverge without softmax operation. Then, we can train easily model without numerical instability. The softmax-free property of SAGMM attention leads to online inference in SAGMM-tr.\n\nWe added equations (16), (20) to show the number of encoder frames needed to predict the attention output. In the adaptive window model, the number of frames decoder waits depends on $\\sigma_{i}$. In the fixed window model, the model needs to wait $\\frac{c}{2}$ frames after $\\nu_{j}> \\mu_{i}$ is satisfied. In experiments, the model waits 7 more frames with $c=15$. \n\n**Comment**  The second concern is that many confounding factors are introduced in the experiments, presenting us from concluding that the improvement is solely coming from the use of SAGMM. For example, in the experiments, CTC is used as an additional loss to guide the learning of monotonic attention. During training, the paper also stochastically introduces an all one vector at the end of the input. It is unclear whether these two have an impact on learning monotonic attentions. It would be more convincing to have results without these additions, and would make the comparison to others more meaningful (assuming that others did not use the same additions).\n\n**Response** In this paper, we adopt the CTC loss as an auxiliary loss for fair comparison with previous studies. The model without CTC loss learned similar mappings with that with CTC loss and the WER of the test-clean/test-other set was 3.84%/9.86%, respectively. Therefore, the model still learns monotonic mapping without an auxiliary loss.\n\nThe 1-vector trick has been developed for our in-house datasets in which EOS is allowed only when the user closes session. In common environments like LibriSpeech, the 1-vector trick does not have crucial role. Due to lack of time, we could not train the model without 1-vector trick from scratch. Instead, we fine-tuned existing SAGMM-tr model. The results is given as:\n\n(w/ 1-vector trick) test-clean: 3.52%  test-other: 9.29% WER\n\n(w/o 1-vector trick) test-clean: 3.54%, test-other: 9.37% WER\n\nFrom the result, the 1-vector trick is not significant to SAGMM-tr in general environments.\n\n**Comment**  One minor concern is that the learned representation depends on the attention used. The paper did mention this in the translation experiments, but it is under developed and no further hypotheses or evidence are provided. This would require almost a separate paper, but I believe this is key to understanding why regular content-based attention fails and whether we actually solves the problem.\n\n **Response** We suppose that the failure of the content-based attention partly comes from the softmax operation over all encoder outputs, which makes the model unstable when the similar contents from the encoder contents appears multiple times. \n In SAGMM-tr, the attention scores follows truncated normal distribution that the model attends to a subset of sequences.  Discarding tokens in irrelevant positions would be helpful to learn stable alignments. We did not include this discussion in the main paper since this assumption should be verified via a variety of tasks in following works.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper796/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper796/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Monotonic Alignments with Source-Aware GMM Attention", "authorids": ["~Tae_Gyoon_Kang1", "hogyeong.kim@samsung.com", "minjoong.lee@samsung.com", "jihyun.s.lee@samsung.com", "~Seongmin_Ok1", "hoshik.lee@samsung.com", "macho@samsung.com"], "authors": ["Tae Gyoon Kang", "Ho-Gyeong Kim", "Min-Joong Lee", "Jihyun Lee", "Seongmin Ok", "Hoshik Lee", "Young Sang Choi"], "keywords": ["Monotonic alignments", "sequence-to-sequence model", "aligned attention", "streaming speech recognition", "long-form speech recognition"], "abstract": "Transformers with soft attention have been widely adopted in various sequence-to-sequence (Seq2Seq) tasks. Whereas soft attention is effective for learning semantic similarities between queries and keys based on their contents, it does not explicitly model the order of elements in sequences which is crucial for monotonic Seq2Seq tasks. Learning monotonic alignments between input and output sequences may be beneficial for long-form and online inference applications that are still challenging for the conventional soft attention algorithm. Herein, we focus on monotonic Seq2Seq tasks and propose a source-aware Gaussian mixture model attention in which the attention scores are monotonically calculated considering both the content and order of the source sequence. We experimentally demonstrate that the proposed attention mechanism improved the performance on the online and long-form speech recognition problems without performance degradation in offline in-distribution speech recognition.", "one-sentence_summary": "We focus on monotonic sequence-to-sequence tasks and propose source-aware GMM attention which enables online inference and improves long-form sequence generation performance in speech recognition.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kang|learning_monotonic_alignments_with_sourceaware_gmm_attention", "pdf": "/pdf/40b2a952edd850d01b072503e4919dee464c895e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=6uMvA4Tt6_", "_bibtex": "@misc{\nkang2021learning,\ntitle={Learning Monotonic Alignments with Source-Aware {\\{}GMM{\\}} Attention},\nauthor={Tae Gyoon Kang and Ho-Gyeong Kim and Min-Joong Lee and Jihyun Lee and Seongmin Ok and Hoshik Lee and Young Sang Choi},\nyear={2021},\nurl={https://openreview.net/forum?id=y13JLBiNMsf}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "y13JLBiNMsf", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper796/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper796/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper796/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper796/Authors|ICLR.cc/2021/Conference/Paper796/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper796/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923867074, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper796/-/Official_Comment"}}}, {"id": "J6VnSfEbZ60", "original": null, "number": 1, "cdate": 1603850908438, "ddate": null, "tcdate": 1603850908438, "tmdate": 1605024602946, "tddate": null, "forum": "y13JLBiNMsf", "replyto": "y13JLBiNMsf", "invitation": "ICLR.cc/2021/Conference/Paper796/-/Official_Review", "content": {"title": "Nice idea, but details unclear and presentation needs polish", "review": "The paper describes a simple extension to the location-only monotonic GMM attention mechanism from Graves (2013), which takes the source/key context into account when computing attention weights.  The proposed method improves ASR performance over model using the baseline GMM attention which does not take source-content into account,  generalizes better to input sequences much longer than those seen during training, while also obtaining competitive performance to other streaming seq2seq ASR models on \"matched\" test sets.\n\n## Pros:\n\n1. Incorporating source content is an obvious and useful extension to monotonic GMM attention, combining the strengths of content-based approaches such as additive or dot-product attention.\n\n2. It improves performance and generalization while being simpler than existing techniques in the literature (e.g. Mocha, CTC/transducer models which have more complex loss functions).\n\n## Cons:\n\n1. The description of the proposed mechanism is inconsistent with existing literature and is very unclear and confusing in parts.\n\n2. Experiments are somewhat incomplete/missing important comparisons, e.g. comparing baseline GMM attention to the proposed \"source-aware\" variant in Tables 1,2,5, and comparing to other location-based attention mechanisms, even if non-monotinic, e.g. from http://papers.nips.cc/paper/5847-attention-based-models-for-speech-recognition.pdf\n \n3. Overall the writing/language use could use improvement.\n\n## Detailed comments\n\nAt the high level, the idea of incorporating the source keys K into GMM attention is a good one.  The proposed method seems to work, and be  simpler to implement than alternative monotonic alignment mechanisms used in seq2seq ASR models.  However, given the current state of the text, with many confusing details, I feel that the paper is not yet ready for publication without significant revisions. \n\nMany details in the paper, especially Section 2, are unclear:\n\n- Sec 2.2. and throughout the paper:  The described \"GMM\" and \"SA-GMM\" attention always use a single component, so don't really count as a Gaussian *mixture*.  Using multiple components would explicitly allow for multimodal attention weights for each output step.  Moreover, since the mixing weights are generally computed independently at each step, using multiple components makes it possible for the base GMM attention to \"discard the non-informative tokens\".   This mechanism, which would be more precisely called \"Gaussian attention\", is strictly less flexible than the base GMM attention mechanism that was originally described in Graves, 2013.\n\n- This claim is repeated in paragraph 2 of Sec 3: \"uni-modal similar to conventional GMM attention\".  When using multiple mixture components, GMM attention is not unimodal.\n\n- Eq 8: The notation here is unclear.  Why is there a softmax over $\\psi_i^h$ (a scalar AFAICT)?  Is the softmax computed over all attention heads?   Why is this necessary?  It seems to impair the training of some heads, at least for SAGMM-tr according to paragraph 4 of Sec. 4.2\n\n- Figure 1 is difficult to interpret.  The two plots have difference horizontal axes and therefore don't seem to be directly comparable...  It's not clear what the \"key width\" in Figure 1b is trying to convey since there is always going to be a single weight per (discrete) encoder step j.\n\n- Sec 2.3.  There is no particular motivation given for the proposed method for integrating source keys.  Why not include $K_j$ in the computation for the standard deviation $\\Sigma_i$ as well?  And why is the same weight $\\delta_j$ used as a scaling factor (eq (12)) and the mean offset in eq (10)?  These design choices deserve more explanation, and possibly empirical justification.\n\n- Sec 2.4: Is it possible to train SAGMM-tr from scratch?  Or does it need to be first trained using SAGMM and then fine-tuning with truncation enabled?\n\nExperiments:\n\n- Are the different GMM attention variants used encoder self-attention layers as well?  Or does the encoder use conventional \"soft attention\"?\n\n- As above, it seems unfair not to include any experiments using multiple components when comparing different variants of GMM attention.\n\n- Sec 4.2, Table 1:  Please clarify the differences between the three models labeled (Ours).  Is the difference only in the encoder-decoder attention layer?\n\nEnglish usage.  Just a few examples of grammar errors and unclear text, as there are too many to list.\n\n- page 1, \"attend subset of long sequences\" is missing a preposition, e.g., \"attend to a subset\".  It seems that \"long sequences\" is meant  to refer a single source sequence.\n\n- page 1, \"mismatch between attention parameters from decoders and information distribution in encoder outputs\".  This sentence is difficult to parse.  What is the mismatch here?   Why would the source encoding and decoder query vector need to \"match\", especially in a purely location-based attention scheme?\n\n- page 4: \"fixed length with hyperparameters\"  What are they hyperparameters being referred to here?\n- page 5, Sec. 4: \"enables early inference\".  What does \"early inference\" mean?  \n- page 5, Sec. 3: \"for the inference\" -> \"for inference\" \n- page 5, Sec 4.1:  \"1 second speeches\" -> \"1 second long utterances\"\n- page 5, Sec 4.1: \"from 30 vocabulary\" -> \"from a vocabulary of 30 words\"\n\n\nOther comments:\n\n- Sec 2.2: Sutskever et al., 2014 did not use content-based attention.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper796/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper796/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Monotonic Alignments with Source-Aware GMM Attention", "authorids": ["~Tae_Gyoon_Kang1", "hogyeong.kim@samsung.com", "minjoong.lee@samsung.com", "jihyun.s.lee@samsung.com", "~Seongmin_Ok1", "hoshik.lee@samsung.com", "macho@samsung.com"], "authors": ["Tae Gyoon Kang", "Ho-Gyeong Kim", "Min-Joong Lee", "Jihyun Lee", "Seongmin Ok", "Hoshik Lee", "Young Sang Choi"], "keywords": ["Monotonic alignments", "sequence-to-sequence model", "aligned attention", "streaming speech recognition", "long-form speech recognition"], "abstract": "Transformers with soft attention have been widely adopted in various sequence-to-sequence (Seq2Seq) tasks. Whereas soft attention is effective for learning semantic similarities between queries and keys based on their contents, it does not explicitly model the order of elements in sequences which is crucial for monotonic Seq2Seq tasks. Learning monotonic alignments between input and output sequences may be beneficial for long-form and online inference applications that are still challenging for the conventional soft attention algorithm. Herein, we focus on monotonic Seq2Seq tasks and propose a source-aware Gaussian mixture model attention in which the attention scores are monotonically calculated considering both the content and order of the source sequence. We experimentally demonstrate that the proposed attention mechanism improved the performance on the online and long-form speech recognition problems without performance degradation in offline in-distribution speech recognition.", "one-sentence_summary": "We focus on monotonic sequence-to-sequence tasks and propose source-aware GMM attention which enables online inference and improves long-form sequence generation performance in speech recognition.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kang|learning_monotonic_alignments_with_sourceaware_gmm_attention", "pdf": "/pdf/40b2a952edd850d01b072503e4919dee464c895e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=6uMvA4Tt6_", "_bibtex": "@misc{\nkang2021learning,\ntitle={Learning Monotonic Alignments with Source-Aware {\\{}GMM{\\}} Attention},\nauthor={Tae Gyoon Kang and Ho-Gyeong Kim and Min-Joong Lee and Jihyun Lee and Seongmin Ok and Hoshik Lee and Young Sang Choi},\nyear={2021},\nurl={https://openreview.net/forum?id=y13JLBiNMsf}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "y13JLBiNMsf", "replyto": "y13JLBiNMsf", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper796/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538134746, "tmdate": 1606915784856, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper796/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper796/-/Official_Review"}}}, {"id": "bu1M1cEdNr", "original": null, "number": 2, "cdate": 1603939169300, "ddate": null, "tcdate": 1603939169300, "tmdate": 1605024602873, "tddate": null, "forum": "y13JLBiNMsf", "replyto": "y13JLBiNMsf", "invitation": "ICLR.cc/2021/Conference/Paper796/-/Official_Review", "content": {"title": "A new Gaussian-based attention mechanism by incorporating the source (key) information", "review": "This paper proposes a novel Gaussian mixture-based attention mechanism by incorporating the source (key) information, enabling a flexible representation of the Gaussian attention pattern with the well-described formulation. The effectiveness of the proposed method was validated by 1) artificially created long utterances, 2) Librispeech (segmented and concatenated utterances), and 3) machine translation. The paper also includes several attention visualizations to show the reasonable attention patterns obtained by the proposed method. The paper is well written. The precise monotonic attention gains a lot of attention for streaming ASR and simultaneous machine translation, and this paper would gein broad interests. My concerns about this paper are that 1) the novelty and the effectiveness are a little bit incremental, and 2) the paper requires more clarifications about the formulations and experimental descriptions (see my detailed comments).\n\nDetailed comments:\n- In the introduction, \"The GMM attention is a pure location-aware algorithm in which the model selects attention windows cumulatively without considering source contents.\": This is a little bit hard to understand. I understood it after I followed Section 2. It would be better to make this expression more understandable.\n- I'm curious about the GMM's behavior (how the Gaussian parameters would be changed for each layer and each head). These are described in the appendix, but the authors may consider discussing it in the main paper. It is well known that the general soft attentions' behaviors are very different in the lower and higher layers. The lower-layer attentions are blur and less monotonic. I would like to ask the authors to discuss how the GMM parameters can adapt to capture such behaviors in the main paper.\n- Equation (4): It's better to explain why the softplus function is used for \\Delta and \\Sigma. (monotonicity and positive constraint of the variance, right?)\n- Equation (9): It's better to have some explanation about why $\\delta$ is bounded by [0, 1].\n- Equation (14): Is there proof?\n- Section 2.4: Does the truncation make training faster?\n- Section 4: The authors should clarify why they did not use a language model. Besides, it's better to clarify that the method does not use the language model in the table's caption (Is that correct? All the results listed in the table are without LMs, right?). The readers remember the Librispeech number with the LM, and they may confuse it.\n- Why does the table 4 result not include the GMM attention result? It's better to compare the normal GMM and the proposed method in the online mode.", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper796/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper796/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Monotonic Alignments with Source-Aware GMM Attention", "authorids": ["~Tae_Gyoon_Kang1", "hogyeong.kim@samsung.com", "minjoong.lee@samsung.com", "jihyun.s.lee@samsung.com", "~Seongmin_Ok1", "hoshik.lee@samsung.com", "macho@samsung.com"], "authors": ["Tae Gyoon Kang", "Ho-Gyeong Kim", "Min-Joong Lee", "Jihyun Lee", "Seongmin Ok", "Hoshik Lee", "Young Sang Choi"], "keywords": ["Monotonic alignments", "sequence-to-sequence model", "aligned attention", "streaming speech recognition", "long-form speech recognition"], "abstract": "Transformers with soft attention have been widely adopted in various sequence-to-sequence (Seq2Seq) tasks. Whereas soft attention is effective for learning semantic similarities between queries and keys based on their contents, it does not explicitly model the order of elements in sequences which is crucial for monotonic Seq2Seq tasks. Learning monotonic alignments between input and output sequences may be beneficial for long-form and online inference applications that are still challenging for the conventional soft attention algorithm. Herein, we focus on monotonic Seq2Seq tasks and propose a source-aware Gaussian mixture model attention in which the attention scores are monotonically calculated considering both the content and order of the source sequence. We experimentally demonstrate that the proposed attention mechanism improved the performance on the online and long-form speech recognition problems without performance degradation in offline in-distribution speech recognition.", "one-sentence_summary": "We focus on monotonic sequence-to-sequence tasks and propose source-aware GMM attention which enables online inference and improves long-form sequence generation performance in speech recognition.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kang|learning_monotonic_alignments_with_sourceaware_gmm_attention", "pdf": "/pdf/40b2a952edd850d01b072503e4919dee464c895e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=6uMvA4Tt6_", "_bibtex": "@misc{\nkang2021learning,\ntitle={Learning Monotonic Alignments with Source-Aware {\\{}GMM{\\}} Attention},\nauthor={Tae Gyoon Kang and Ho-Gyeong Kim and Min-Joong Lee and Jihyun Lee and Seongmin Ok and Hoshik Lee and Young Sang Choi},\nyear={2021},\nurl={https://openreview.net/forum?id=y13JLBiNMsf}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "y13JLBiNMsf", "replyto": "y13JLBiNMsf", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper796/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538134746, "tmdate": 1606915784856, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper796/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper796/-/Official_Review"}}}, {"id": "aVr1NevVjQ", "original": null, "number": 4, "cdate": 1604582156525, "ddate": null, "tcdate": 1604582156525, "tmdate": 1605024602727, "tddate": null, "forum": "y13JLBiNMsf", "replyto": "y13JLBiNMsf", "invitation": "ICLR.cc/2021/Conference/Paper796/-/Official_Review", "content": {"title": "Good paper if errors and confounding factors are resolved", "review": "This paper proposes a monotonic attention to improve the latency of decoding. The attention is an improvement over the attention based on Gaussian mixture model, allowing the attention weights to depend on the encoder outputs. The experimental numbers are strong.\n\nI am leaning towards accepting the paper, but I do have a few concerns that the paper fails to address. Some of the concerns can be fixed, and some will need additional experiments. I will revise the score based on the authors' response.\n\nThe first concern is simply that there are errors and ambiguous statements in the paper. For example, equation (14) is obviously wrong, and in section 2.4 it is unclear how many frames the encoder needs to see before the decoder can predict the next token. The latter is crucial because streaming is the major reason behind the use of monotonic attention. A detailed list is given at the end.\n\nThe second concern is that many confounding factors are introduced in the experiments, presenting us from concluding that the improvement is solely coming from the use of SAGMM. For example, in the experiments, CTC is used as an additional loss to guide the learning of monotonic attention. During training, the paper also stochastically introduces an all one vector at the end of the input. It is unclear whether these two have an impact on learning monotonic attentions. It would be more convincing to have results without these additions, and would make the comparison to others more meaningful (assuming that others did not use the same additions).\n\nOne minor concern is that the learned representation depends on the attention used. The paper did mention this in the translation experiments, but it is under developed and no further hypotheses or evidence are provided. This would require almost a separate paper, but I believe this is key to understanding why regular content-based attention fails and whether we actually solves the problem.\n\nHere are the detailed comments.\n\n> ... the proposed attention mechanism solves the online and long-form speech recognition problems ...\n\nThis is a strong claim. I don't think the paper presents enough evidence that the problem is solved.\n\n> equation (1), (2), and (3)\n\nThis is minor, but the variables soft_\\alpha, head^h, and Multihead can be better typeset.\n\n> equation (4)\n\nThis is again minor. \\Sigma_i is a scalar, and it might be better to use the lower case \\sigma_i.\n\n> equation (13)\n\nWhat are I and J?\n\n> \\mu_i = \\mu_{i-1} + relu3(\\Delta_i)\n\nWhat is relu3?\n\n> equation (14)\n\nThis equation is wrong in many ways. I assume j \\to \\infty means having infinitely many time steps, and \\delta_j \\to 0 applies to \\delta_j for all j.\n\n> 2.4 SAGMM-tr for online inference\n\nIn streaming mode, while the frames are coming in, how does the decoder know when to produce the next token?\n\n> We tested the window width c = 15 centered by \\mu_i ...\n\nWhat is c?\n\n> ... we randomly concatenate the 1-vector after the end of source sentence with probability p_{eos} ...\n\nThis trick is not implemented in other approaches. Have the authors tried without using this trick? How do know if the improvement is purely due to this trick and not about the proposed SAGMM?\n\n> We adopt the CTC loss on the encoder output ...\n\nThe CTC loss is meant to help learn monotonic attention. Why bother using the CTC loss if the proposed attention is already monotonic? Does this suggest that the CTC loss has other added benefits?\n\n> Streamable model, bi-directional enc. in Table 2\n\nHow are bidirectional encoders in the streaming case? Using a bidirectional encoder by definition cannot be used for streaming unless I missed something.\n\n> Transformer (121M) (Ours) in Table 2\n\nI assume this uses a regular content-based attention. Please clarify. I would even group the Transformer, GMM, and SAGMM-tr rows together, since they provide the control experiments to support the usefulness of the proposed method. The other rows are less useful, since the architectures are different. The comparison of absolute numbers tell us little, except that the numbers provided for the proposed approach are in the right ballpark.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper796/AnonReviewer5"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper796/AnonReviewer5"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Monotonic Alignments with Source-Aware GMM Attention", "authorids": ["~Tae_Gyoon_Kang1", "hogyeong.kim@samsung.com", "minjoong.lee@samsung.com", "jihyun.s.lee@samsung.com", "~Seongmin_Ok1", "hoshik.lee@samsung.com", "macho@samsung.com"], "authors": ["Tae Gyoon Kang", "Ho-Gyeong Kim", "Min-Joong Lee", "Jihyun Lee", "Seongmin Ok", "Hoshik Lee", "Young Sang Choi"], "keywords": ["Monotonic alignments", "sequence-to-sequence model", "aligned attention", "streaming speech recognition", "long-form speech recognition"], "abstract": "Transformers with soft attention have been widely adopted in various sequence-to-sequence (Seq2Seq) tasks. Whereas soft attention is effective for learning semantic similarities between queries and keys based on their contents, it does not explicitly model the order of elements in sequences which is crucial for monotonic Seq2Seq tasks. Learning monotonic alignments between input and output sequences may be beneficial for long-form and online inference applications that are still challenging for the conventional soft attention algorithm. Herein, we focus on monotonic Seq2Seq tasks and propose a source-aware Gaussian mixture model attention in which the attention scores are monotonically calculated considering both the content and order of the source sequence. We experimentally demonstrate that the proposed attention mechanism improved the performance on the online and long-form speech recognition problems without performance degradation in offline in-distribution speech recognition.", "one-sentence_summary": "We focus on monotonic sequence-to-sequence tasks and propose source-aware GMM attention which enables online inference and improves long-form sequence generation performance in speech recognition.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kang|learning_monotonic_alignments_with_sourceaware_gmm_attention", "pdf": "/pdf/40b2a952edd850d01b072503e4919dee464c895e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=6uMvA4Tt6_", "_bibtex": "@misc{\nkang2021learning,\ntitle={Learning Monotonic Alignments with Source-Aware {\\{}GMM{\\}} Attention},\nauthor={Tae Gyoon Kang and Ho-Gyeong Kim and Min-Joong Lee and Jihyun Lee and Seongmin Ok and Hoshik Lee and Young Sang Choi},\nyear={2021},\nurl={https://openreview.net/forum?id=y13JLBiNMsf}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "y13JLBiNMsf", "replyto": "y13JLBiNMsf", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper796/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538134746, "tmdate": 1606915784856, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper796/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper796/-/Official_Review"}}}], "count": 13}