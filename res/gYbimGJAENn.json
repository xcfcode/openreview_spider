{"notes": [{"id": "gYbimGJAENn", "original": "xp2ai7QyPSD", "number": 2904, "cdate": 1601308322104, "ddate": null, "tcdate": 1601308322104, "tmdate": 1614985706858, "tddate": null, "forum": "gYbimGJAENn", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Powers of layers for image-to-image translation", "authorids": ["~Hugo_Touvron1", "~Matthijs_Douze1", "~Matthieu_Cord1", "~Herve_Jegou1"], "authors": ["Hugo Touvron", "Matthijs Douze", "Matthieu Cord", "Herve Jegou"], "keywords": [], "abstract": "We propose a simple architecture to address unpaired image-to-image  translation tasks: style or class transfer, denoising, deblurring, deblocking, etc. \nWe start from an image autoencoder architecture with fixed weights. \nFor each task we learn a residual block operating in the latent space, which is iteratively called until the target domain is reached. \nA specific training schedule is required to alleviate the exponentiation effect of the iterations. \nAt test time, it offers several advantages: the number of weight parameters is limited and the compositional design allows one to modulate the strength of the transformation with the number of iterations. \nThis is useful, for instance, when the type or amount of noise to suppress is not known in advance.  \nExperimentally, we show that the performance of our model is comparable or better than CycleGAN and Nice-GAN with fewer parameters.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "touvron|powers_of_layers_for_imagetoimage_translation", "supplementary_material": "/attachment/2265526517f4081e91df8dd3a3e4cca1af34cb49.zip", "pdf": "/pdf/055075c122a5fdbeafc12c7792f595ac27a45b1c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=sz01OdIzhO", "_bibtex": "@misc{\ntouvron2021powers,\ntitle={Powers of layers for image-to-image translation},\nauthor={Hugo Touvron and Matthijs Douze and Matthieu Cord and Herve Jegou},\nyear={2021},\nurl={https://openreview.net/forum?id=gYbimGJAENn}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "Il1NHE7L68", "original": null, "number": 1, "cdate": 1610040436714, "ddate": null, "tcdate": 1610040436714, "tmdate": 1610474037401, "tddate": null, "forum": "gYbimGJAENn", "replyto": "gYbimGJAENn", "invitation": "ICLR.cc/2021/Conference/Paper2904/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "All the reviewers shared the concerns about the novelty and the quality of the results. Comparisons with some SOTA results are missing, and the inclusion of deblurring/denosing tasks is not convincing. The authors carefully addressed these issues in the rebuttal but the reviewers didn\u2019t change their mind afterwards. After carefully examining the results in the paper, the AC agrees with the reviewers that the improvement on image quality, if any, seems to be too small to warrant a publication. "}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Powers of layers for image-to-image translation", "authorids": ["~Hugo_Touvron1", "~Matthijs_Douze1", "~Matthieu_Cord1", "~Herve_Jegou1"], "authors": ["Hugo Touvron", "Matthijs Douze", "Matthieu Cord", "Herve Jegou"], "keywords": [], "abstract": "We propose a simple architecture to address unpaired image-to-image  translation tasks: style or class transfer, denoising, deblurring, deblocking, etc. \nWe start from an image autoencoder architecture with fixed weights. \nFor each task we learn a residual block operating in the latent space, which is iteratively called until the target domain is reached. \nA specific training schedule is required to alleviate the exponentiation effect of the iterations. \nAt test time, it offers several advantages: the number of weight parameters is limited and the compositional design allows one to modulate the strength of the transformation with the number of iterations. \nThis is useful, for instance, when the type or amount of noise to suppress is not known in advance.  \nExperimentally, we show that the performance of our model is comparable or better than CycleGAN and Nice-GAN with fewer parameters.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "touvron|powers_of_layers_for_imagetoimage_translation", "supplementary_material": "/attachment/2265526517f4081e91df8dd3a3e4cca1af34cb49.zip", "pdf": "/pdf/055075c122a5fdbeafc12c7792f595ac27a45b1c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=sz01OdIzhO", "_bibtex": "@misc{\ntouvron2021powers,\ntitle={Powers of layers for image-to-image translation},\nauthor={Hugo Touvron and Matthijs Douze and Matthieu Cord and Herve Jegou},\nyear={2021},\nurl={https://openreview.net/forum?id=gYbimGJAENn}\n}"}, "tags": [], "invitation": {"reply": {"forum": "gYbimGJAENn", "replyto": "gYbimGJAENn", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040436699, "tmdate": 1610474037385, "id": "ICLR.cc/2021/Conference/Paper2904/-/Decision"}}}, {"id": "wpAW1jD5oux", "original": null, "number": 5, "cdate": 1606135100342, "ddate": null, "tcdate": 1606135100342, "tmdate": 1606135100342, "tddate": null, "forum": "gYbimGJAENn", "replyto": "5mAkQKsIMN", "invitation": "ICLR.cc/2021/Conference/Paper2904/-/Official_Comment", "content": {"title": "Response to R3", "comment": "1)  The review states that  \u201cThe paper includes as motivation the idea that applying that different levels of transformation can be achieved by choosing different numbers of iterations, but the application of this is shown only for denoising (Table 1).\u201d \nThis is not correct. \u201cIf the network is trained with a fixed number of compositions, the intermediate states do not correspond to modulations of the transformation that \u201clook right\u201d (see Appendix A)\u201d (paragraph \u201cTraining for modulation\u201d page 5) See Figure 8 Appendix A. \u201cIn Appendix C we also give a comparison with the Fader network for the capacity to modulate a transformation, and more visual examples in Appendix D.\u201d (Section 5 page 7)  see Appendix D Figure 13 we provide example with 6 different transformations.\n\n2) See appendix for additional visualization. Images in all our illustrations are not cherry-picked, but overall the objective assessment (PSNR, FID, NIQE) quantitatively show that our method provides some improvement . \n\n3)  The paper \u201c Contrastive Learning for Unpaired Image-to-Image Translation\u201d Taesung Park, Alexei Efros, Richard Zhang, Jun-Yan Zhu ECCV 2020 is no more the state of the art in unpaired image to image translation than the paper \u201cReusing Discriminators for Encoding: Towards Unsupervised Image-to-Image Translation\u201d Chen et al. CVPR 2020 with which we compare our method. Indeed, in Unpaired Image to Image translation, we have 6 tasks in common with  Chen et al. (on the 8 tasks presented in the paper ) and only 1 task in common  with Park et al. (on the 3 tasks presented in the paper). In addition, as you can see in Table 1 in Park et al. they do not provide a comparison with Chen et al. and are evaluated in different ways so make it impossible to judge which of the two methods is the best (Compare Table 1 in Chen et al. and Table 1 in Park et al.).\nGiven all these elements and having more tasks in common with Chen et al. , we found it more appropriate to use this method as our baseline to compare with the state of the art. We have also proposed a version of PoL applied to CycleGAN's architecture (Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks, Zhu et al.), as it is a very popular architecture.\n\n4) The objective of our analysis section is not to have state of the art results in denoising and deblurring, which one can not expect from a super generic method that can handle multiples tasks with a single architecture (therefore more compact and more easily extendable, and less prone to overfit to a specific amount of noise or blur). In contrast, this analysis shows an experimental context that is easier to evaluate than the experimental part in order to better understand the impact of the different components of the method.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2904/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2904/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Powers of layers for image-to-image translation", "authorids": ["~Hugo_Touvron1", "~Matthijs_Douze1", "~Matthieu_Cord1", "~Herve_Jegou1"], "authors": ["Hugo Touvron", "Matthijs Douze", "Matthieu Cord", "Herve Jegou"], "keywords": [], "abstract": "We propose a simple architecture to address unpaired image-to-image  translation tasks: style or class transfer, denoising, deblurring, deblocking, etc. \nWe start from an image autoencoder architecture with fixed weights. \nFor each task we learn a residual block operating in the latent space, which is iteratively called until the target domain is reached. \nA specific training schedule is required to alleviate the exponentiation effect of the iterations. \nAt test time, it offers several advantages: the number of weight parameters is limited and the compositional design allows one to modulate the strength of the transformation with the number of iterations. \nThis is useful, for instance, when the type or amount of noise to suppress is not known in advance.  \nExperimentally, we show that the performance of our model is comparable or better than CycleGAN and Nice-GAN with fewer parameters.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "touvron|powers_of_layers_for_imagetoimage_translation", "supplementary_material": "/attachment/2265526517f4081e91df8dd3a3e4cca1af34cb49.zip", "pdf": "/pdf/055075c122a5fdbeafc12c7792f595ac27a45b1c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=sz01OdIzhO", "_bibtex": "@misc{\ntouvron2021powers,\ntitle={Powers of layers for image-to-image translation},\nauthor={Hugo Touvron and Matthijs Douze and Matthieu Cord and Herve Jegou},\nyear={2021},\nurl={https://openreview.net/forum?id=gYbimGJAENn}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "gYbimGJAENn", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2904/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2904/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2904/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2904/Authors|ICLR.cc/2021/Conference/Paper2904/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2904/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923843275, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2904/-/Official_Comment"}}}, {"id": "ImtXwT8uJx6", "original": null, "number": 4, "cdate": 1606134980107, "ddate": null, "tcdate": 1606134980107, "tmdate": 1606134980107, "tddate": null, "forum": "gYbimGJAENn", "replyto": "G-sg4zguXo6", "invitation": "ICLR.cc/2021/Conference/Paper2904/-/Official_Comment", "content": {"title": "Response to R2", "comment": "1 ) The reviewer is correct that, if we use the stopping criterion at each iteration, then the overall computation time would be higher. In practice, as shown by Table 3 and Figure 10 in Appendix A, the interesting range for iterations is relatively limited and we can restrict to number of times we use the discriminator\nIt is possible to set the maximum number of compositions in POL to the number of residual blocks present in CycleGAN or NiceGAN. So in this case we have the same inference time but fewer parameters. In this case as we have fewer parameters we can be faster than the baseline by using larger batches. We do this with high resolution images. \n\n2 & 3) Thanks for the references and the suggestion to improve our submission \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2904/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2904/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Powers of layers for image-to-image translation", "authorids": ["~Hugo_Touvron1", "~Matthijs_Douze1", "~Matthieu_Cord1", "~Herve_Jegou1"], "authors": ["Hugo Touvron", "Matthijs Douze", "Matthieu Cord", "Herve Jegou"], "keywords": [], "abstract": "We propose a simple architecture to address unpaired image-to-image  translation tasks: style or class transfer, denoising, deblurring, deblocking, etc. \nWe start from an image autoencoder architecture with fixed weights. \nFor each task we learn a residual block operating in the latent space, which is iteratively called until the target domain is reached. \nA specific training schedule is required to alleviate the exponentiation effect of the iterations. \nAt test time, it offers several advantages: the number of weight parameters is limited and the compositional design allows one to modulate the strength of the transformation with the number of iterations. \nThis is useful, for instance, when the type or amount of noise to suppress is not known in advance.  \nExperimentally, we show that the performance of our model is comparable or better than CycleGAN and Nice-GAN with fewer parameters.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "touvron|powers_of_layers_for_imagetoimage_translation", "supplementary_material": "/attachment/2265526517f4081e91df8dd3a3e4cca1af34cb49.zip", "pdf": "/pdf/055075c122a5fdbeafc12c7792f595ac27a45b1c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=sz01OdIzhO", "_bibtex": "@misc{\ntouvron2021powers,\ntitle={Powers of layers for image-to-image translation},\nauthor={Hugo Touvron and Matthijs Douze and Matthieu Cord and Herve Jegou},\nyear={2021},\nurl={https://openreview.net/forum?id=gYbimGJAENn}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "gYbimGJAENn", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2904/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2904/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2904/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2904/Authors|ICLR.cc/2021/Conference/Paper2904/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2904/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923843275, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2904/-/Official_Comment"}}}, {"id": "m38A8MxRDhA", "original": null, "number": 3, "cdate": 1606134751445, "ddate": null, "tcdate": 1606134751445, "tmdate": 1606134852642, "tddate": null, "forum": "gYbimGJAENn", "replyto": "4BhKUFuipDC", "invitation": "ICLR.cc/2021/Conference/Paper2904/-/Official_Comment", "content": {"title": "Response to R4", "comment": "We thank the reviewer for her/his feedback. \n\n1) CycleGAN is the only competitor for these tasks because we do not target a single pretext tasks (denoising deblurring), as we mostly focus on comparing architectures designed for Unpaired Image translation  (e.g. CycleGAN and PoL), as discussed in the introduction. This allows the most fair possible comparison. \nRequiring our generic method (which has not seen any paired data and can address multiple tasks) to be better than specific architectures specifically optimized for a single task, with paired data reflecting a specific level of noise of blur (while our method can accommodate a range as shown in our paper), does not sound reasonable to us.  \n\n2) We explain this in Section 3.1 \n\n3) Nevertheless, the result is visually more accurate than the CycleGAN result. The FID measurements indicate that we have equivalent results with fewer parameters.\n\n4)  We are pleased to add these results in the updated  9 pages submission. \n\n5) The principle of using an expansion factor between two blocks is what is used in the MLP of the feedforward part of the transformer.\n\n6) There are several differences and hurdles that would need to be addressed with a traditional RNN, which we actually address in our paper. (i) RNN are not spatial, so one would need to adapt them to process properly the 2d+depth feature map. (ii) one must treat properly how many times you apply the residual blocks, and how you train it. \nOverall, the gating in modern RNNs aims at enlarging the context and avoids catastrophic forgetting effects, which is different from the unpaired domain transfer that we consider. While it may be possible to apply RNN to unpaired domain adaptation, it is not straightforward.  For instance, to learn translation between two languages without having parallel sentences, recent works (See works by Lample, Conneau et al.[1] for instance) don\u2019t use a direct application of RNNs. Therefore, this would require some substantial work to make it work, which we achieved in our paper with our simple PoL architecture. \n\n[1]  Guillaume Lample, Alexis Conneau, Marc'Aurelio Ranzato, Ludovic Denoyer, Herv\u00e9 J\u00e9gou ,Word translation without parallel data, ICLR 2018\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2904/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2904/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Powers of layers for image-to-image translation", "authorids": ["~Hugo_Touvron1", "~Matthijs_Douze1", "~Matthieu_Cord1", "~Herve_Jegou1"], "authors": ["Hugo Touvron", "Matthijs Douze", "Matthieu Cord", "Herve Jegou"], "keywords": [], "abstract": "We propose a simple architecture to address unpaired image-to-image  translation tasks: style or class transfer, denoising, deblurring, deblocking, etc. \nWe start from an image autoencoder architecture with fixed weights. \nFor each task we learn a residual block operating in the latent space, which is iteratively called until the target domain is reached. \nA specific training schedule is required to alleviate the exponentiation effect of the iterations. \nAt test time, it offers several advantages: the number of weight parameters is limited and the compositional design allows one to modulate the strength of the transformation with the number of iterations. \nThis is useful, for instance, when the type or amount of noise to suppress is not known in advance.  \nExperimentally, we show that the performance of our model is comparable or better than CycleGAN and Nice-GAN with fewer parameters.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "touvron|powers_of_layers_for_imagetoimage_translation", "supplementary_material": "/attachment/2265526517f4081e91df8dd3a3e4cca1af34cb49.zip", "pdf": "/pdf/055075c122a5fdbeafc12c7792f595ac27a45b1c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=sz01OdIzhO", "_bibtex": "@misc{\ntouvron2021powers,\ntitle={Powers of layers for image-to-image translation},\nauthor={Hugo Touvron and Matthijs Douze and Matthieu Cord and Herve Jegou},\nyear={2021},\nurl={https://openreview.net/forum?id=gYbimGJAENn}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "gYbimGJAENn", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2904/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2904/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2904/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2904/Authors|ICLR.cc/2021/Conference/Paper2904/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2904/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923843275, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2904/-/Official_Comment"}}}, {"id": "u7NYggADg5H", "original": null, "number": 2, "cdate": 1606134346871, "ddate": null, "tcdate": 1606134346871, "tmdate": 1606134346871, "tddate": null, "forum": "gYbimGJAENn", "replyto": "7kOHFIjWEfs", "invitation": "ICLR.cc/2021/Conference/Paper2904/-/Official_Comment", "content": {"title": "Response to R1", "comment": "We do not use extra layers for the generator, quite the opposite: we use only one residual block with PoL versus 9 residual blocks for the baseline. \nIn our PoL framework, we train once the encoder and the decoder with a reconstruction task; then they are freezed, and we learn the transformations with only 1 residual block. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2904/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2904/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Powers of layers for image-to-image translation", "authorids": ["~Hugo_Touvron1", "~Matthijs_Douze1", "~Matthieu_Cord1", "~Herve_Jegou1"], "authors": ["Hugo Touvron", "Matthijs Douze", "Matthieu Cord", "Herve Jegou"], "keywords": [], "abstract": "We propose a simple architecture to address unpaired image-to-image  translation tasks: style or class transfer, denoising, deblurring, deblocking, etc. \nWe start from an image autoencoder architecture with fixed weights. \nFor each task we learn a residual block operating in the latent space, which is iteratively called until the target domain is reached. \nA specific training schedule is required to alleviate the exponentiation effect of the iterations. \nAt test time, it offers several advantages: the number of weight parameters is limited and the compositional design allows one to modulate the strength of the transformation with the number of iterations. \nThis is useful, for instance, when the type or amount of noise to suppress is not known in advance.  \nExperimentally, we show that the performance of our model is comparable or better than CycleGAN and Nice-GAN with fewer parameters.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "touvron|powers_of_layers_for_imagetoimage_translation", "supplementary_material": "/attachment/2265526517f4081e91df8dd3a3e4cca1af34cb49.zip", "pdf": "/pdf/055075c122a5fdbeafc12c7792f595ac27a45b1c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=sz01OdIzhO", "_bibtex": "@misc{\ntouvron2021powers,\ntitle={Powers of layers for image-to-image translation},\nauthor={Hugo Touvron and Matthijs Douze and Matthieu Cord and Herve Jegou},\nyear={2021},\nurl={https://openreview.net/forum?id=gYbimGJAENn}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "gYbimGJAENn", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2904/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2904/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2904/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2904/Authors|ICLR.cc/2021/Conference/Paper2904/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2904/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923843275, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2904/-/Official_Comment"}}}, {"id": "5mAkQKsIMN", "original": null, "number": 1, "cdate": 1602792976070, "ddate": null, "tcdate": 1602792976070, "tmdate": 1605024107203, "tddate": null, "forum": "gYbimGJAENn", "replyto": "gYbimGJAENn", "invitation": "ICLR.cc/2021/Conference/Paper2904/-/Official_Review", "content": {"title": "Review", "review": "The paper proposes a method for unsupervised image translation between unpaired domains of images. The main idea is to develop an iterative transformation module that operates in the embedding space.\n\nOverall I have the following concerns about the paper:\n\nThe motivation for this architecture is unclear. The introduction motivates this model with fractals and iterated function spaces, but that seems to have nothing to do with the types of applications shown here. What do IFSs have to do with denoising and pictures of zebras? This iterated refinement strategy seems more similar to iterative refinement/projection algorithms like conjugate gradient and Richardson-Lucy deconvolution, which are relevant to low-level signal processing operations like denoising/deblurring, but not zebra synthesis.  The paper includes as motivation the idea that applying that different levels of transformation can be achieved by choosing different numbers of iterations, but the application of this is shown only for denoising (Table 1). \n\nNo comparison is provided to the state-of-the-art in unpaired image translation: \nContrastive Learning for Unpaired Image-to-Image Translation\nTaesung Park, Alexei Efros, Richard Zhang, Jun-Yan Zhu\nECCV 2020\n\nVisually, the results are not convincing. Not many results are shown, and most do not look better than those from CycleGAN.  The results may be cherry-picked, since there was no statement as to how these results were chosen.  There is simply not enough visual evidence that the method has evidence over previous work. Additionally, quantitatve comparisons do not give a compelling outcome, but I would put more weight on visual comparisons anyway.\n\nFor the task of denoising, it is unclear why one would want to use a general-purpose unpaired translation method; supervised methods ought to be much effective here, and there is an enormous literature of related work that is not cited or compared with here. If one is to use denoising as a motivation application (rather than a toy example), then much more rigor is required. \n", "rating": "3: Clear rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2904/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2904/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Powers of layers for image-to-image translation", "authorids": ["~Hugo_Touvron1", "~Matthijs_Douze1", "~Matthieu_Cord1", "~Herve_Jegou1"], "authors": ["Hugo Touvron", "Matthijs Douze", "Matthieu Cord", "Herve Jegou"], "keywords": [], "abstract": "We propose a simple architecture to address unpaired image-to-image  translation tasks: style or class transfer, denoising, deblurring, deblocking, etc. \nWe start from an image autoencoder architecture with fixed weights. \nFor each task we learn a residual block operating in the latent space, which is iteratively called until the target domain is reached. \nA specific training schedule is required to alleviate the exponentiation effect of the iterations. \nAt test time, it offers several advantages: the number of weight parameters is limited and the compositional design allows one to modulate the strength of the transformation with the number of iterations. \nThis is useful, for instance, when the type or amount of noise to suppress is not known in advance.  \nExperimentally, we show that the performance of our model is comparable or better than CycleGAN and Nice-GAN with fewer parameters.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "touvron|powers_of_layers_for_imagetoimage_translation", "supplementary_material": "/attachment/2265526517f4081e91df8dd3a3e4cca1af34cb49.zip", "pdf": "/pdf/055075c122a5fdbeafc12c7792f595ac27a45b1c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=sz01OdIzhO", "_bibtex": "@misc{\ntouvron2021powers,\ntitle={Powers of layers for image-to-image translation},\nauthor={Hugo Touvron and Matthijs Douze and Matthieu Cord and Herve Jegou},\nyear={2021},\nurl={https://openreview.net/forum?id=gYbimGJAENn}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "gYbimGJAENn", "replyto": "gYbimGJAENn", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2904/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538086296, "tmdate": 1606915782852, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2904/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2904/-/Official_Review"}}}, {"id": "G-sg4zguXo6", "original": null, "number": 2, "cdate": 1603622139584, "ddate": null, "tcdate": 1603622139584, "tmdate": 1605024107142, "tddate": null, "forum": "gYbimGJAENn", "replyto": "gYbimGJAENn", "invitation": "ICLR.cc/2021/Conference/Paper2904/-/Official_Review", "content": {"title": "A good point to reduce the number of weights but the computation time is questionable", "review": "1. Summary. The submitted paper proposes to use a recurrent residual block for the task of unpaired img2img translation. A number of strategies of how many times to apply this block are suggested. \n\n1. Decision. I do really like the direction of weight sharing in img2img models and this work is a pretty nice case. This approach helps to decrease the number of weights, and, if done properly, does not harm quality. The results in deblurring/denoising look rather interesting.\n\nCons. \n\nHowever, the downside of the presented recurrent block is the increased computation time at the inference step, as far as I can judge. This is especially crucial when the discriminator is involved as the stopping criterion. Could you provide a comparison of the inference speed (FPS or FLOPS or any other measure) between CycleGAN/NiceGAN and PoL?\n\nSecond, I believe this approach could be also put into the context of the adaptive computation time research field [1,2,3,4]. This may help to determine the number of layers to apply.\n\nThird, the proposed block may be straightforwardly generalized to multi-domain img2img translation and showcased on more interesting and recent datasets against stronger baselines like MUNIT [5], FUNIT[6], etc. This could make img2img part of your experiments more solid, I suppose.\n\nAll-in-all, to my mind there is great room for improvement for your submission to demonstrate the real power of PoL. Therefore, now I tend to rate the submission a bit below the threshold.\n\n[1] https://openreview.net/forum?id=r1W1OxAF\n[2] https://openreview.net/forum?id=SkZq3vyDf\n[3] https://openaccess.thecvf.com/content_cvpr_2017/html/Figurnov_Spatially_Adaptive_Computation_CVPR_2017_paper.html\n[4] https://openreview.net/forum?id=HyzdRiR9Y7\n[5] https://link.springer.com/chapter/10.1007/978-3-030-01219-9_11\n[6] https://openaccess.thecvf.com/content_ICCV_2019/html/Liu_Few-Shot_Unsupervised_Image-to-Image_Translation_ICCV_2019_paper.html", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2904/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2904/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Powers of layers for image-to-image translation", "authorids": ["~Hugo_Touvron1", "~Matthijs_Douze1", "~Matthieu_Cord1", "~Herve_Jegou1"], "authors": ["Hugo Touvron", "Matthijs Douze", "Matthieu Cord", "Herve Jegou"], "keywords": [], "abstract": "We propose a simple architecture to address unpaired image-to-image  translation tasks: style or class transfer, denoising, deblurring, deblocking, etc. \nWe start from an image autoencoder architecture with fixed weights. \nFor each task we learn a residual block operating in the latent space, which is iteratively called until the target domain is reached. \nA specific training schedule is required to alleviate the exponentiation effect of the iterations. \nAt test time, it offers several advantages: the number of weight parameters is limited and the compositional design allows one to modulate the strength of the transformation with the number of iterations. \nThis is useful, for instance, when the type or amount of noise to suppress is not known in advance.  \nExperimentally, we show that the performance of our model is comparable or better than CycleGAN and Nice-GAN with fewer parameters.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "touvron|powers_of_layers_for_imagetoimage_translation", "supplementary_material": "/attachment/2265526517f4081e91df8dd3a3e4cca1af34cb49.zip", "pdf": "/pdf/055075c122a5fdbeafc12c7792f595ac27a45b1c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=sz01OdIzhO", "_bibtex": "@misc{\ntouvron2021powers,\ntitle={Powers of layers for image-to-image translation},\nauthor={Hugo Touvron and Matthijs Douze and Matthieu Cord and Herve Jegou},\nyear={2021},\nurl={https://openreview.net/forum?id=gYbimGJAENn}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "gYbimGJAENn", "replyto": "gYbimGJAENn", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2904/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538086296, "tmdate": 1606915782852, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2904/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2904/-/Official_Review"}}}, {"id": "4BhKUFuipDC", "original": null, "number": 3, "cdate": 1603762298852, "ddate": null, "tcdate": 1603762298852, "tmdate": 1605024107082, "tddate": null, "forum": "gYbimGJAENn", "replyto": "gYbimGJAENn", "invitation": "ICLR.cc/2021/Conference/Paper2904/-/Official_Review", "content": {"title": "Insufficient comparison and experiment settings", "review": "This paper proposes an unpaired image-to-image translation method which applies a pre-trained auto-encoder and a latent feature transformer (single block) to perform iterative image transformation. A progressive training and warm-up strategy is used to settle the numerical exponentiation effects caused by powers of layers. In the testing phase, the discriminator is also used to adjust the inference time.\n\nPros: \n1) Compared with the vanilla CycleGAN, the proposed PoL has significantly fewer parameters and similar performance.\n2) The flexibility offered by the common embedding space allows the modulation of the transformation strength, or to compose several transformations.\n3) The discriminator is used to adjust the inference time and find the optimal number of iterations in the testing phase.\n\nCons:\n1) CycleGAN is the only competitor in most comparative experiments, which is not sufficient. Besides, CycleGAN is not a good competitor for image restoration tasks (debluring, denoising, etc.), so the potential of the proposed PoL is questionable. Additional comparison results generated by other general image restoration methods [1, 2] should be reported. \n2) Is CycleGAN also pre-trained on the same dataset as PoL for fair comparison?\n3) Although progressive training contributes to more natural intermediate outputs, the final output is not satisfactory, for example the unnatural patterns on zebras in Fig. 4.\n4) More effective transformation modulation is a major advantage of the proposed PoL, but the provided experiments did not demonstrate this well. I think it would be more appropriate to put the results obtained along the iterations of the recurrent block in the main text instead of in the appendix.\n5) Why the proposed embedding transformer \u201cis similar to the feed-forward network used in transformers [3]\u201d? It seems that it is just a simple residual convolution module with expansion factor K to adjust the model\u2019s capacity, which is not related to transformer or self-attention.\n6) What are the significant advantages of PoL compared with traditional RNNs (LSTM, GRU)? Is it possible to directly replace PoL with RNNs to achieve close results?\n\n[1] Neural Sparse Representation for Image Restoration. NeurIPS, 2020.\n[2] Learning Invariant Representation for Unsupervised Image Restoration. CVPR, 2020.\n[3] Attention is all you need. NeurIPS, 2017.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2904/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2904/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Powers of layers for image-to-image translation", "authorids": ["~Hugo_Touvron1", "~Matthijs_Douze1", "~Matthieu_Cord1", "~Herve_Jegou1"], "authors": ["Hugo Touvron", "Matthijs Douze", "Matthieu Cord", "Herve Jegou"], "keywords": [], "abstract": "We propose a simple architecture to address unpaired image-to-image  translation tasks: style or class transfer, denoising, deblurring, deblocking, etc. \nWe start from an image autoencoder architecture with fixed weights. \nFor each task we learn a residual block operating in the latent space, which is iteratively called until the target domain is reached. \nA specific training schedule is required to alleviate the exponentiation effect of the iterations. \nAt test time, it offers several advantages: the number of weight parameters is limited and the compositional design allows one to modulate the strength of the transformation with the number of iterations. \nThis is useful, for instance, when the type or amount of noise to suppress is not known in advance.  \nExperimentally, we show that the performance of our model is comparable or better than CycleGAN and Nice-GAN with fewer parameters.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "touvron|powers_of_layers_for_imagetoimage_translation", "supplementary_material": "/attachment/2265526517f4081e91df8dd3a3e4cca1af34cb49.zip", "pdf": "/pdf/055075c122a5fdbeafc12c7792f595ac27a45b1c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=sz01OdIzhO", "_bibtex": "@misc{\ntouvron2021powers,\ntitle={Powers of layers for image-to-image translation},\nauthor={Hugo Touvron and Matthijs Douze and Matthieu Cord and Herve Jegou},\nyear={2021},\nurl={https://openreview.net/forum?id=gYbimGJAENn}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "gYbimGJAENn", "replyto": "gYbimGJAENn", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2904/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538086296, "tmdate": 1606915782852, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2904/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2904/-/Official_Review"}}}, {"id": "7kOHFIjWEfs", "original": null, "number": 4, "cdate": 1604198044598, "ddate": null, "tcdate": 1604198044598, "tmdate": 1605024107006, "tddate": null, "forum": "gYbimGJAENn", "replyto": "gYbimGJAENn", "invitation": "ICLR.cc/2021/Conference/Paper2904/-/Official_Review", "content": {"title": "reviews for Powers of layers for image-to-image translation", "review": "This paper presents an approach for image to image translation by introducing extra layers into the generator, which can be trained in an unsupervised way. The paper is generally easy to follow. I have the following concerns: the novelty is quite marginal since the backbone network and the training process are well developed before and the technique of employing more layers to the generator seems like simply extending the network. Please consider to use different notations in Section 3 to denote image from different domains. Also, please use the same subscript for G_{AB} and G_{\\mathcal{BA}}.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2904/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2904/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Powers of layers for image-to-image translation", "authorids": ["~Hugo_Touvron1", "~Matthijs_Douze1", "~Matthieu_Cord1", "~Herve_Jegou1"], "authors": ["Hugo Touvron", "Matthijs Douze", "Matthieu Cord", "Herve Jegou"], "keywords": [], "abstract": "We propose a simple architecture to address unpaired image-to-image  translation tasks: style or class transfer, denoising, deblurring, deblocking, etc. \nWe start from an image autoencoder architecture with fixed weights. \nFor each task we learn a residual block operating in the latent space, which is iteratively called until the target domain is reached. \nA specific training schedule is required to alleviate the exponentiation effect of the iterations. \nAt test time, it offers several advantages: the number of weight parameters is limited and the compositional design allows one to modulate the strength of the transformation with the number of iterations. \nThis is useful, for instance, when the type or amount of noise to suppress is not known in advance.  \nExperimentally, we show that the performance of our model is comparable or better than CycleGAN and Nice-GAN with fewer parameters.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "touvron|powers_of_layers_for_imagetoimage_translation", "supplementary_material": "/attachment/2265526517f4081e91df8dd3a3e4cca1af34cb49.zip", "pdf": "/pdf/055075c122a5fdbeafc12c7792f595ac27a45b1c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=sz01OdIzhO", "_bibtex": "@misc{\ntouvron2021powers,\ntitle={Powers of layers for image-to-image translation},\nauthor={Hugo Touvron and Matthijs Douze and Matthieu Cord and Herve Jegou},\nyear={2021},\nurl={https://openreview.net/forum?id=gYbimGJAENn}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "gYbimGJAENn", "replyto": "gYbimGJAENn", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2904/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538086296, "tmdate": 1606915782852, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2904/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2904/-/Official_Review"}}}], "count": 10}