{"notes": [{"id": "gh0DiCjUiYt", "original": null, "number": 5, "cdate": 1586307902765, "ddate": null, "tcdate": 1586307902765, "tmdate": 1586307902765, "tddate": null, "forum": "Byl5NREFDr", "replyto": "Byl5NREFDr", "invitation": "ICLR.cc/2020/Conference/Paper1084/-/Official_Comment", "content": {"title": "Blogpost describing the paper", "comment": "We recently wrote a blogpost describing the work, you can find it here: http://www.cleverhans.io/2020/04/06/stealing-bert.html"}, "signatures": ["ICLR.cc/2020/Conference/Paper1084/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1084/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Thieves on Sesame Street! Model Extraction of BERT-based APIs", "authors": ["Kalpesh Krishna", "Gaurav Singh Tomar", "Ankur P. Parikh", "Nicolas Papernot", "Mohit Iyyer"], "authorids": ["kalpesh@cs.umass.edu", "gtomar@google.com", "aparikh@google.com", "papernot@google.com", "miyyer@cs.umass.edu"], "keywords": ["model extraction", "BERT", "natural language processing", "pretraining language models", "model stealing", "deep learning security"], "TL;DR": "Outputs of modern NLP APIs on nonsensical text provide strong signals about model internals, allowing adversaries to steal the APIs.", "abstract": "We study the problem of model extraction in natural language processing, in which an adversary with only query access to a victim model attempts to reconstruct a local copy of that model. Assuming that both the adversary and victim model fine-tune a large pretrained language model such as BERT (Devlin et al., 2019), we show that the adversary does not need any real training data to successfully mount the attack. In fact, the attacker need not even use grammatical or semantically meaningful queries: we show that random sequences of words coupled with task-specific heuristics form effective queries for model extraction on a diverse set of NLP tasks, including natural language inference and question answering. Our work thus highlights an exploit only made feasible by the shift towards transfer learning methods within the NLP community: for a query budget of a few hundred dollars, an attacker can extract a model that performs only slightly worse than the victim model. Finally, we study two defense strategies against model extraction\u2014membership classification and API watermarking\u2014which while successful against some adversaries can also be circumvented by more clever ones.", "pdf": "/pdf/0072594e40e42b1d603a9ab8dca04f79de577fcb.pdf", "paperhash": "krishna|thieves_on_sesame_street_model_extraction_of_bertbased_apis", "code": "https://github.com/google-research/language/tree/master/language/bert_extraction", "_bibtex": "@inproceedings{\nKrishna2020Thieves,\ntitle={Thieves on Sesame Street! Model Extraction of BERT-based APIs},\nauthor={Kalpesh Krishna and Gaurav Singh Tomar and Ankur P. Parikh and Nicolas Papernot and Mohit Iyyer},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Byl5NREFDr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e6b9a22fab94d189f972720cf7b1b164082652cb.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Byl5NREFDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1084/Authors", "ICLR.cc/2020/Conference/Paper1084/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1084/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1084/Reviewers", "ICLR.cc/2020/Conference/Paper1084/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1084/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1084/Authors|ICLR.cc/2020/Conference/Paper1084/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504161511, "tmdate": 1576860538235, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1084/Authors", "ICLR.cc/2020/Conference/Paper1084/Reviewers", "ICLR.cc/2020/Conference/Paper1084/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1084/-/Official_Comment"}}}, {"id": "Byl5NREFDr", "original": "Hylb6SL_vB", "number": 1084, "cdate": 1569439281560, "ddate": null, "tcdate": 1569439281560, "tmdate": 1583912023602, "tddate": null, "forum": "Byl5NREFDr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Thieves on Sesame Street! Model Extraction of BERT-based APIs", "authors": ["Kalpesh Krishna", "Gaurav Singh Tomar", "Ankur P. Parikh", "Nicolas Papernot", "Mohit Iyyer"], "authorids": ["kalpesh@cs.umass.edu", "gtomar@google.com", "aparikh@google.com", "papernot@google.com", "miyyer@cs.umass.edu"], "keywords": ["model extraction", "BERT", "natural language processing", "pretraining language models", "model stealing", "deep learning security"], "TL;DR": "Outputs of modern NLP APIs on nonsensical text provide strong signals about model internals, allowing adversaries to steal the APIs.", "abstract": "We study the problem of model extraction in natural language processing, in which an adversary with only query access to a victim model attempts to reconstruct a local copy of that model. Assuming that both the adversary and victim model fine-tune a large pretrained language model such as BERT (Devlin et al., 2019), we show that the adversary does not need any real training data to successfully mount the attack. In fact, the attacker need not even use grammatical or semantically meaningful queries: we show that random sequences of words coupled with task-specific heuristics form effective queries for model extraction on a diverse set of NLP tasks, including natural language inference and question answering. Our work thus highlights an exploit only made feasible by the shift towards transfer learning methods within the NLP community: for a query budget of a few hundred dollars, an attacker can extract a model that performs only slightly worse than the victim model. Finally, we study two defense strategies against model extraction\u2014membership classification and API watermarking\u2014which while successful against some adversaries can also be circumvented by more clever ones.", "pdf": "/pdf/0072594e40e42b1d603a9ab8dca04f79de577fcb.pdf", "paperhash": "krishna|thieves_on_sesame_street_model_extraction_of_bertbased_apis", "code": "https://github.com/google-research/language/tree/master/language/bert_extraction", "_bibtex": "@inproceedings{\nKrishna2020Thieves,\ntitle={Thieves on Sesame Street! Model Extraction of BERT-based APIs},\nauthor={Kalpesh Krishna and Gaurav Singh Tomar and Ankur P. Parikh and Nicolas Papernot and Mohit Iyyer},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Byl5NREFDr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e6b9a22fab94d189f972720cf7b1b164082652cb.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "bNjWM4VtYi", "original": null, "number": 1, "cdate": 1576798714130, "ddate": null, "tcdate": 1576798714130, "tmdate": 1576800922344, "tddate": null, "forum": "Byl5NREFDr", "replyto": "Byl5NREFDr", "invitation": "ICLR.cc/2020/Conference/Paper1084/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "Two knowledgable reviewers recommend accepting the paper, and the less familiar reviewer is also positive. The final decision is to accept the paper. It's an interesting and timely topic with insightful results.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Thieves on Sesame Street! Model Extraction of BERT-based APIs", "authors": ["Kalpesh Krishna", "Gaurav Singh Tomar", "Ankur P. Parikh", "Nicolas Papernot", "Mohit Iyyer"], "authorids": ["kalpesh@cs.umass.edu", "gtomar@google.com", "aparikh@google.com", "papernot@google.com", "miyyer@cs.umass.edu"], "keywords": ["model extraction", "BERT", "natural language processing", "pretraining language models", "model stealing", "deep learning security"], "TL;DR": "Outputs of modern NLP APIs on nonsensical text provide strong signals about model internals, allowing adversaries to steal the APIs.", "abstract": "We study the problem of model extraction in natural language processing, in which an adversary with only query access to a victim model attempts to reconstruct a local copy of that model. Assuming that both the adversary and victim model fine-tune a large pretrained language model such as BERT (Devlin et al., 2019), we show that the adversary does not need any real training data to successfully mount the attack. In fact, the attacker need not even use grammatical or semantically meaningful queries: we show that random sequences of words coupled with task-specific heuristics form effective queries for model extraction on a diverse set of NLP tasks, including natural language inference and question answering. Our work thus highlights an exploit only made feasible by the shift towards transfer learning methods within the NLP community: for a query budget of a few hundred dollars, an attacker can extract a model that performs only slightly worse than the victim model. Finally, we study two defense strategies against model extraction\u2014membership classification and API watermarking\u2014which while successful against some adversaries can also be circumvented by more clever ones.", "pdf": "/pdf/0072594e40e42b1d603a9ab8dca04f79de577fcb.pdf", "paperhash": "krishna|thieves_on_sesame_street_model_extraction_of_bertbased_apis", "code": "https://github.com/google-research/language/tree/master/language/bert_extraction", "_bibtex": "@inproceedings{\nKrishna2020Thieves,\ntitle={Thieves on Sesame Street! Model Extraction of BERT-based APIs},\nauthor={Kalpesh Krishna and Gaurav Singh Tomar and Ankur P. Parikh and Nicolas Papernot and Mohit Iyyer},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Byl5NREFDr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e6b9a22fab94d189f972720cf7b1b164082652cb.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "Byl5NREFDr", "replyto": "Byl5NREFDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795727740, "tmdate": 1576800280034, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1084/-/Decision"}}}, {"id": "rJxuOr0KsB", "original": null, "number": 2, "cdate": 1573672304057, "ddate": null, "tcdate": 1573672304057, "tmdate": 1573686088117, "tddate": null, "forum": "Byl5NREFDr", "replyto": "r1exx7fTKS", "invitation": "ICLR.cc/2020/Conference/Paper1084/-/Official_Comment", "content": {"title": "Thank you for the detailed summary and comments", "comment": "Thank you for the detailed summary and comments.\n\n>> the victim model is fine-tuned on the original data, therefore it has picked up some of the data heuristics used to generate the queries, the annotators are not trained on, or shown any of the original examples (there is a control run, but these are presumably a separate set of annotators)\n\nYes, the control run used a different set of annotators. We don't believe showing the annotators original SQuAD examples would have made a significant difference due to the following reasons --- 1) while all annotators were unfamiliar with the specifics of our research goals, we chose only NLP/ML graduate students as annotators who had a good understanding of reading comprehension tasks in NLP 2) despite not seeing any original examples, performance on the control experiment is close to the human performance reported in the original SQuAD paper [1]. We obtain an estimate of 85 F1 using the single ground truth answer in the SQuAD training set; the original estimates are 87 F1 on the test set and 91 F1 on the dev set, after using *two ground-truth answers* per example.\n\n>> \"attackers can maximize their accuracy by fine-tuning more powerful language models, and that matching architectures is a secondary concern\". I would suggest that the authors add another few rows of experiments comparing less similar model architectures.\n\nWe agree that more experiments are needed to verify this hypothesis. We run experiments on XLNET [2] to reinforce this claim. (Empirically it has been shown that XLNET is a strong language model compared to BERT)\n\nOur preliminary results (below) show that fine-tuning XLNET-large indeed does leads to better model extraction, even with a BERT-large victim model (controlling the number of queries). We will add this along with more experiments in the next version of the paper.\n\n|-------------------------------------------------------------------------------------------------------------\n| Extracted Model   Dataset                                                                        Performance \n|-------------------------------------------------------------------------------------------------------------\n| BERT-large             original SQuAD                                                            90.6 F1          \n| XLNET-large           original SQuAD                                                            92.8 F1          \n|-------------------------------------------------------------------------------------------------------------\n| BERT-large             WIKI queries, BERT-large victim's labels               86.1 F1          \n| XLNET-large           WIKI queries, BERT-large victim's labels               89.2 F1          \n|-------------------------------------------------------------------------------------------------------------\n| BERT-large             RANDOM queries, BERT-large victim's labels       79.1 F1          \n| XLNET-large           RANDOM queries, BERT-large victim's labels       80.9 F1          \n|-------------------------------------------------------------------------------------------------------------\n\n[1] - Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. \"Squad: 100,000+ questions for machine comprehension of text.\" (EMNLP, 2016)\n[2] - Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V. Le. \"XLNet: Generalized Autoregressive Pretraining for Language Understanding.\" (NeurIPS 2019)"}, "signatures": ["ICLR.cc/2020/Conference/Paper1084/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1084/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Thieves on Sesame Street! Model Extraction of BERT-based APIs", "authors": ["Kalpesh Krishna", "Gaurav Singh Tomar", "Ankur P. Parikh", "Nicolas Papernot", "Mohit Iyyer"], "authorids": ["kalpesh@cs.umass.edu", "gtomar@google.com", "aparikh@google.com", "papernot@google.com", "miyyer@cs.umass.edu"], "keywords": ["model extraction", "BERT", "natural language processing", "pretraining language models", "model stealing", "deep learning security"], "TL;DR": "Outputs of modern NLP APIs on nonsensical text provide strong signals about model internals, allowing adversaries to steal the APIs.", "abstract": "We study the problem of model extraction in natural language processing, in which an adversary with only query access to a victim model attempts to reconstruct a local copy of that model. Assuming that both the adversary and victim model fine-tune a large pretrained language model such as BERT (Devlin et al., 2019), we show that the adversary does not need any real training data to successfully mount the attack. In fact, the attacker need not even use grammatical or semantically meaningful queries: we show that random sequences of words coupled with task-specific heuristics form effective queries for model extraction on a diverse set of NLP tasks, including natural language inference and question answering. Our work thus highlights an exploit only made feasible by the shift towards transfer learning methods within the NLP community: for a query budget of a few hundred dollars, an attacker can extract a model that performs only slightly worse than the victim model. Finally, we study two defense strategies against model extraction\u2014membership classification and API watermarking\u2014which while successful against some adversaries can also be circumvented by more clever ones.", "pdf": "/pdf/0072594e40e42b1d603a9ab8dca04f79de577fcb.pdf", "paperhash": "krishna|thieves_on_sesame_street_model_extraction_of_bertbased_apis", "code": "https://github.com/google-research/language/tree/master/language/bert_extraction", "_bibtex": "@inproceedings{\nKrishna2020Thieves,\ntitle={Thieves on Sesame Street! Model Extraction of BERT-based APIs},\nauthor={Kalpesh Krishna and Gaurav Singh Tomar and Ankur P. Parikh and Nicolas Papernot and Mohit Iyyer},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Byl5NREFDr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e6b9a22fab94d189f972720cf7b1b164082652cb.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Byl5NREFDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1084/Authors", "ICLR.cc/2020/Conference/Paper1084/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1084/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1084/Reviewers", "ICLR.cc/2020/Conference/Paper1084/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1084/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1084/Authors|ICLR.cc/2020/Conference/Paper1084/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504161511, "tmdate": 1576860538235, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1084/Authors", "ICLR.cc/2020/Conference/Paper1084/Reviewers", "ICLR.cc/2020/Conference/Paper1084/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1084/-/Official_Comment"}}}, {"id": "HJeUtwCKor", "original": null, "number": 3, "cdate": 1573672829732, "ddate": null, "tcdate": 1573672829732, "tmdate": 1573673054190, "tddate": null, "forum": "Byl5NREFDr", "replyto": "SJlh2oSRKH", "invitation": "ICLR.cc/2020/Conference/Paper1084/-/Official_Comment", "content": {"title": "Thank you for the comments", "comment": "Thank you for the comments.\n\n>> how about random/smartly chosen training examples from the training/tuning set of the victim model?\n\nYou are correct that if the attacker had access to the victim\u2019s training or tuning set, this would help bridge the gap in performance observed in our experiments between the victim and extracted models. However, in our setting we assume the attacker has no access to the training or tuning set of the victim model, which is a more realistic threat model: this makes our attack more widely applicable. \n\n>> Would well formed queries help?\n\nIn our experiments (Table 2) we do see the WIKI setting is consistently outperforming the RANDOM setting. Our general observation is that proximity between the input query distribution and the victim's training set distribution is a critical factor in determining the success of model extraction attacks. However, in our setting, the attacker does not have access to the victim's training set distribution. Collecting queries close to the victim's distribution might be hard for tasks with complex input spaces (natural language inference, question answering).\nIn initial experiments (Footnote 2), we did try query-synthesis active learning setups motivated by prior work in model extraction. However, we observed only marginal improvements in extraction performance. A major technical hurdle is the discrete nature of the textual input space. A prior work using pool-based active learning [1] did not work well in settings with complex input spaces (natural language inference, question answering).\n\n>> how it is possible to systematically close the performance gap between the extracted model and the victim one\n\nFrom our analysis, the most promising direction to improve model extraction is based on the experiments in Section 5.1 (under \"Do different victim models agree on the answers to nonsensical queries?\" and \"Are high-agreement queries closer to the original data distribution?\"). Our experiments in Section 5.1 show that the agreement between an ensemble of victim models (trained on the same training set) is a good indicator of a query's closeness to the input distribution. Related to this observation, prior work [2] has shown that an ensemble of classifiers leads to more accurate uncertainty estimates compared to a single overconfident classifier. \n\nThe dominant paradigm in improving model extraction accuracy is active learning, where queries are selected based on the current state of the extracted model. Such model extraction setups typically use the uncertainty of a single extracted model as an objective for query construction. Our observations in 5.1 motivate an alternative objective --- construct queries which have high agreement among an ensemble of victim models and low agreement among an ensemble of extracted models. However, the first term in this objective will need to be approximated in practice since only a single victim model is available to an attacker.\n\n[1] - Soham Pal, Yash Gupta, Aditya Shukla, Aditya Kanade, Shirish Shevade, and Vinod Ganapathy. \"A framework for the extraction of deep neural networks by leveraging public data.\" (arXiv:1905.09165, 2019)\n[2] - Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. \"Simple and scalable predictive uncertainty estimation using deep ensembles.\" (NIPS 2017)"}, "signatures": ["ICLR.cc/2020/Conference/Paper1084/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1084/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Thieves on Sesame Street! Model Extraction of BERT-based APIs", "authors": ["Kalpesh Krishna", "Gaurav Singh Tomar", "Ankur P. Parikh", "Nicolas Papernot", "Mohit Iyyer"], "authorids": ["kalpesh@cs.umass.edu", "gtomar@google.com", "aparikh@google.com", "papernot@google.com", "miyyer@cs.umass.edu"], "keywords": ["model extraction", "BERT", "natural language processing", "pretraining language models", "model stealing", "deep learning security"], "TL;DR": "Outputs of modern NLP APIs on nonsensical text provide strong signals about model internals, allowing adversaries to steal the APIs.", "abstract": "We study the problem of model extraction in natural language processing, in which an adversary with only query access to a victim model attempts to reconstruct a local copy of that model. Assuming that both the adversary and victim model fine-tune a large pretrained language model such as BERT (Devlin et al., 2019), we show that the adversary does not need any real training data to successfully mount the attack. In fact, the attacker need not even use grammatical or semantically meaningful queries: we show that random sequences of words coupled with task-specific heuristics form effective queries for model extraction on a diverse set of NLP tasks, including natural language inference and question answering. Our work thus highlights an exploit only made feasible by the shift towards transfer learning methods within the NLP community: for a query budget of a few hundred dollars, an attacker can extract a model that performs only slightly worse than the victim model. Finally, we study two defense strategies against model extraction\u2014membership classification and API watermarking\u2014which while successful against some adversaries can also be circumvented by more clever ones.", "pdf": "/pdf/0072594e40e42b1d603a9ab8dca04f79de577fcb.pdf", "paperhash": "krishna|thieves_on_sesame_street_model_extraction_of_bertbased_apis", "code": "https://github.com/google-research/language/tree/master/language/bert_extraction", "_bibtex": "@inproceedings{\nKrishna2020Thieves,\ntitle={Thieves on Sesame Street! Model Extraction of BERT-based APIs},\nauthor={Kalpesh Krishna and Gaurav Singh Tomar and Ankur P. Parikh and Nicolas Papernot and Mohit Iyyer},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Byl5NREFDr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e6b9a22fab94d189f972720cf7b1b164082652cb.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Byl5NREFDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1084/Authors", "ICLR.cc/2020/Conference/Paper1084/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1084/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1084/Reviewers", "ICLR.cc/2020/Conference/Paper1084/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1084/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1084/Authors|ICLR.cc/2020/Conference/Paper1084/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504161511, "tmdate": 1576860538235, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1084/Authors", "ICLR.cc/2020/Conference/Paper1084/Reviewers", "ICLR.cc/2020/Conference/Paper1084/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1084/-/Official_Comment"}}}, {"id": "B1lMh7AYoS", "original": null, "number": 1, "cdate": 1573671850218, "ddate": null, "tcdate": 1573671850218, "tmdate": 1573671882183, "tddate": null, "forum": "Byl5NREFDr", "replyto": "HklZG620Fr", "invitation": "ICLR.cc/2020/Conference/Paper1084/-/Official_Comment", "content": {"title": "Thank you for your comments", "comment": "Thank you for the comments.\n\n>> This paper is technically not very novel, but asks interesting questions.\n\nWe would like to re-emphasize our novel contributions here.\n\n1. Our work shows (surprisingly) that model extraction is possible even with nonsensical text --- randomly sampled words concatenated to form sequences (with no human interpretability). Prior work [1] has used random i.i.d noise to extract SVMs and 1-layer neural networks, but we are the first to scale this idea to deep neural networks. More specifically, we extract models based on a large pre-trained language model (BERT), which is a critical component in modern state-of-the-art NLP systems.\n\n2. Our paper is among the first studies of model extraction on NLP models. The only prior work studying model extraction in NLP is Pal et al. 2019 [2]. Our work has several differences from Pal et al. 2019. First, we study large-scale state-of-the-art NLP models based on BERT. In contrast, Pal et al. 2019 limited their study to text classifiers based on 1-layer CNNs. Second, we study NLP tasks with complex pairwise input spaces (like question answering and natural language inference). Pal et al. 2019 limited their study to single input classification. Pairwise inputs are significantly more challenging since it\u2019s harder to find naturally occurring text pairs close to the input distribution. Finally, Pal et al. 2019 used natural text from Wikipedia as queries. We show extraction is possible even with nonsensical text.\n\n3. Finally, we present an extensive analysis of our model extraction setup. Our analysis shows (1) some queries are more representative than others and these can be identified with access to several victim models trained with different random seeds; (2) humans struggle to interpret the nonsensical queries which were used to train our models; (3) language model pre-training is critical for extraction --- publicly available language models are increasing the risk of model extraction; and (4) defense against model extraction is a hard and open problem --- the mitigation strategies we present will only work against a class of naive adversaries who do not adapt to the attacks. \n\n[1] - Florian Tramer, Fan Zhang, Ari Juels, Michael K Reiter, and Thomas Ristenpart. \u201cStealing machine learning models via prediction apis.\u201d (USENIX 2016)\n[2] - Soham Pal, Yash Gupta, Aditya Shukla, Aditya Kanade, Shirish Shevade, and Vinod Ganapathy. \"A framework for the extraction of deep neural networks by leveraging public data.\" (arXiv:1905.09165, 2019)"}, "signatures": ["ICLR.cc/2020/Conference/Paper1084/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1084/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Thieves on Sesame Street! Model Extraction of BERT-based APIs", "authors": ["Kalpesh Krishna", "Gaurav Singh Tomar", "Ankur P. Parikh", "Nicolas Papernot", "Mohit Iyyer"], "authorids": ["kalpesh@cs.umass.edu", "gtomar@google.com", "aparikh@google.com", "papernot@google.com", "miyyer@cs.umass.edu"], "keywords": ["model extraction", "BERT", "natural language processing", "pretraining language models", "model stealing", "deep learning security"], "TL;DR": "Outputs of modern NLP APIs on nonsensical text provide strong signals about model internals, allowing adversaries to steal the APIs.", "abstract": "We study the problem of model extraction in natural language processing, in which an adversary with only query access to a victim model attempts to reconstruct a local copy of that model. Assuming that both the adversary and victim model fine-tune a large pretrained language model such as BERT (Devlin et al., 2019), we show that the adversary does not need any real training data to successfully mount the attack. In fact, the attacker need not even use grammatical or semantically meaningful queries: we show that random sequences of words coupled with task-specific heuristics form effective queries for model extraction on a diverse set of NLP tasks, including natural language inference and question answering. Our work thus highlights an exploit only made feasible by the shift towards transfer learning methods within the NLP community: for a query budget of a few hundred dollars, an attacker can extract a model that performs only slightly worse than the victim model. Finally, we study two defense strategies against model extraction\u2014membership classification and API watermarking\u2014which while successful against some adversaries can also be circumvented by more clever ones.", "pdf": "/pdf/0072594e40e42b1d603a9ab8dca04f79de577fcb.pdf", "paperhash": "krishna|thieves_on_sesame_street_model_extraction_of_bertbased_apis", "code": "https://github.com/google-research/language/tree/master/language/bert_extraction", "_bibtex": "@inproceedings{\nKrishna2020Thieves,\ntitle={Thieves on Sesame Street! Model Extraction of BERT-based APIs},\nauthor={Kalpesh Krishna and Gaurav Singh Tomar and Ankur P. Parikh and Nicolas Papernot and Mohit Iyyer},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Byl5NREFDr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e6b9a22fab94d189f972720cf7b1b164082652cb.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Byl5NREFDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1084/Authors", "ICLR.cc/2020/Conference/Paper1084/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1084/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1084/Reviewers", "ICLR.cc/2020/Conference/Paper1084/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1084/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1084/Authors|ICLR.cc/2020/Conference/Paper1084/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504161511, "tmdate": 1576860538235, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1084/Authors", "ICLR.cc/2020/Conference/Paper1084/Reviewers", "ICLR.cc/2020/Conference/Paper1084/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1084/-/Official_Comment"}}}, {"id": "SJlh2oSRKH", "original": null, "number": 2, "cdate": 1571867571895, "ddate": null, "tcdate": 1571867571895, "tmdate": 1572972514783, "tddate": null, "forum": "Byl5NREFDr", "replyto": "Byl5NREFDr", "invitation": "ICLR.cc/2020/Conference/Paper1084/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This authors introduce a novel approach to successful modern extraction.  The paper is well written and easy to follow (the two exceptions/oddities are Figure 1 & Table 1, which appear one page before they are refered, which makes them initially hard to understand because they are out of context). The experimental evaluation is both well-thought and convincing.\n\nGiven the \"unreasonable effectiveness\" of the proposed approach, one is left to wonder whether/how it is possible to systematically close the performance gap between the extracted model and the victim one. Would well formed queries help? how about random/smartly chosen training examples from the training/tuning set of the victim model? or anything else?"}, "signatures": ["ICLR.cc/2020/Conference/Paper1084/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1084/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Thieves on Sesame Street! Model Extraction of BERT-based APIs", "authors": ["Kalpesh Krishna", "Gaurav Singh Tomar", "Ankur P. Parikh", "Nicolas Papernot", "Mohit Iyyer"], "authorids": ["kalpesh@cs.umass.edu", "gtomar@google.com", "aparikh@google.com", "papernot@google.com", "miyyer@cs.umass.edu"], "keywords": ["model extraction", "BERT", "natural language processing", "pretraining language models", "model stealing", "deep learning security"], "TL;DR": "Outputs of modern NLP APIs on nonsensical text provide strong signals about model internals, allowing adversaries to steal the APIs.", "abstract": "We study the problem of model extraction in natural language processing, in which an adversary with only query access to a victim model attempts to reconstruct a local copy of that model. Assuming that both the adversary and victim model fine-tune a large pretrained language model such as BERT (Devlin et al., 2019), we show that the adversary does not need any real training data to successfully mount the attack. In fact, the attacker need not even use grammatical or semantically meaningful queries: we show that random sequences of words coupled with task-specific heuristics form effective queries for model extraction on a diverse set of NLP tasks, including natural language inference and question answering. Our work thus highlights an exploit only made feasible by the shift towards transfer learning methods within the NLP community: for a query budget of a few hundred dollars, an attacker can extract a model that performs only slightly worse than the victim model. Finally, we study two defense strategies against model extraction\u2014membership classification and API watermarking\u2014which while successful against some adversaries can also be circumvented by more clever ones.", "pdf": "/pdf/0072594e40e42b1d603a9ab8dca04f79de577fcb.pdf", "paperhash": "krishna|thieves_on_sesame_street_model_extraction_of_bertbased_apis", "code": "https://github.com/google-research/language/tree/master/language/bert_extraction", "_bibtex": "@inproceedings{\nKrishna2020Thieves,\ntitle={Thieves on Sesame Street! Model Extraction of BERT-based APIs},\nauthor={Kalpesh Krishna and Gaurav Singh Tomar and Ankur P. Parikh and Nicolas Papernot and Mohit Iyyer},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Byl5NREFDr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e6b9a22fab94d189f972720cf7b1b164082652cb.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Byl5NREFDr", "replyto": "Byl5NREFDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1084/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1084/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575853034982, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1084/Reviewers"], "noninvitees": [], "tcdate": 1570237742593, "tmdate": 1575853034999, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1084/-/Official_Review"}}}, {"id": "HklZG620Fr", "original": null, "number": 3, "cdate": 1571896585447, "ddate": null, "tcdate": 1571896585447, "tmdate": 1572972514741, "tddate": null, "forum": "Byl5NREFDr", "replyto": "Byl5NREFDr", "invitation": "ICLR.cc/2020/Conference/Paper1084/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The authors explore how well model extraction works on recent BERT-based NLP models. The question is: how easy is it for an adversary model to learn to imitate the victim model, only from novel inputs and the corresponding outputs? Importantly, the adversary is supposed to not have access to the original training set. The authors state that this is problematic because such techniques could be used in order to gain information about (potentially private!) training data of the victim model.\n\nIn the experiments, two different settings are studied: one where the output probabilities are known and one where only predicted classes (by the victim model) are available. In either case, the adversary model achieves high agreement with the victim model. One interesting finding is that random queries (i.e., inputs to the victim model) work well, too. So, the main conclusion is that the possibility of such attacks is a problem for natural language processing.\n\nFinally, the authors study two methods to help against the problem of potential model extraction: one that helps avoiding it and one that detects model copies.\n\nThis paper is technically not very novel, but asks interesting questions. The methodology seems sound."}, "signatures": ["ICLR.cc/2020/Conference/Paper1084/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1084/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Thieves on Sesame Street! Model Extraction of BERT-based APIs", "authors": ["Kalpesh Krishna", "Gaurav Singh Tomar", "Ankur P. Parikh", "Nicolas Papernot", "Mohit Iyyer"], "authorids": ["kalpesh@cs.umass.edu", "gtomar@google.com", "aparikh@google.com", "papernot@google.com", "miyyer@cs.umass.edu"], "keywords": ["model extraction", "BERT", "natural language processing", "pretraining language models", "model stealing", "deep learning security"], "TL;DR": "Outputs of modern NLP APIs on nonsensical text provide strong signals about model internals, allowing adversaries to steal the APIs.", "abstract": "We study the problem of model extraction in natural language processing, in which an adversary with only query access to a victim model attempts to reconstruct a local copy of that model. Assuming that both the adversary and victim model fine-tune a large pretrained language model such as BERT (Devlin et al., 2019), we show that the adversary does not need any real training data to successfully mount the attack. In fact, the attacker need not even use grammatical or semantically meaningful queries: we show that random sequences of words coupled with task-specific heuristics form effective queries for model extraction on a diverse set of NLP tasks, including natural language inference and question answering. Our work thus highlights an exploit only made feasible by the shift towards transfer learning methods within the NLP community: for a query budget of a few hundred dollars, an attacker can extract a model that performs only slightly worse than the victim model. Finally, we study two defense strategies against model extraction\u2014membership classification and API watermarking\u2014which while successful against some adversaries can also be circumvented by more clever ones.", "pdf": "/pdf/0072594e40e42b1d603a9ab8dca04f79de577fcb.pdf", "paperhash": "krishna|thieves_on_sesame_street_model_extraction_of_bertbased_apis", "code": "https://github.com/google-research/language/tree/master/language/bert_extraction", "_bibtex": "@inproceedings{\nKrishna2020Thieves,\ntitle={Thieves on Sesame Street! Model Extraction of BERT-based APIs},\nauthor={Kalpesh Krishna and Gaurav Singh Tomar and Ankur P. Parikh and Nicolas Papernot and Mohit Iyyer},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Byl5NREFDr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e6b9a22fab94d189f972720cf7b1b164082652cb.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Byl5NREFDr", "replyto": "Byl5NREFDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1084/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1084/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575853034982, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1084/Reviewers"], "noninvitees": [], "tcdate": 1570237742593, "tmdate": 1575853034999, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1084/-/Official_Review"}}}, {"id": "r1exx7fTKS", "original": null, "number": 1, "cdate": 1571787496499, "ddate": null, "tcdate": 1571787496499, "tmdate": 1572972514697, "tddate": null, "forum": "Byl5NREFDr", "replyto": "Byl5NREFDr", "invitation": "ICLR.cc/2020/Conference/Paper1084/-/Official_Review", "content": {"rating": "8: Accept", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper studies the effectiveness of model extraction techniques on large pretrained language models like BERT. The core hypothesis of the paper is that using pretrained language models, and pretrained contextualized embeddings, has made it easier to reconstruct models using model stealing/extraction methods. Furthermore, this paper demonstrates that an attacker needn't have access to queries from the training set, and that using random sequences of words as a query to the \"victim\" model is an effective strategy. They authors also show that their model stealing strategies are very cost effective (based on Google Cloud compute cost). \n\nThe basic set up of their experiments has a fine-tuned BERT model as the victim model, and a pre-trained BERT model as a the attacker model. The attacker model is assumed to not have access to the training set distribution and the queries are randomly generated. There are 2 strategies for query generation (with additional task specific heuristics): 1) randomly selecting words from WikiText-103, and 2) randomly selecting sentences or paragraphs from WikiText-103. The victim model is passed a generated query and the attacker model is fine-tuned using the output from the victim model. Overall, this paper find that this simple strategy for query generation is effective on 4 different datasets: SST2, MNLI, SQuAD 1.1 and BooolQ. The method is also cost-effective, a few hundred dollars depending on the dataset and the number of queries used to train the attacker model. \n\nThe paper also present some analysis. They find that queries with higher agreement across victim models (5 BERTs with different random seeds) also leda to better results for the attacker model. The authors also run some experiments with humans to test the interpretability of the queries they generated. They collect annotations on SQuAD using questions that were generated with the WIKI and RANDOM strategies (they also compare highest agreement and lowest agreement queries), and also collect a control with the original SQuAD questions. While this is an interesting analysis to present, showing that most of the generated queries are nonsensical to humans and there is low inter-annotator agreement, I have an issue with the experimental procedure here: the victim model is fine-tuned on the original data, therefore it has picked up some of the data heuristics used to generate the queries, the annotators are not trained on, or shown any of the original examples (there is a control run, but these are presumably a separate set of annotators).  Through interviews, the authors learn that the annotators were using word overlap heuristics, but perhaps training the annotators on a small set of the original data would draw a closer example to the victim model. Either way, while this is an interesting result, it seems a bit misplaced in this paper. I'm not sure this human annotation experiment is contributing in any real way to the core thesis of the paper.\n\nThe authors also test the results of having a mismatch between the victim and attacker model. They consider the mismatch of BERT-base and BERT-large models. They conclude that \"attackers can maximize their accuracy by fine-tuning more powerful language models, and that matching architectures is a secondary concern.\" This conclusion feels like a bit of a stretch. I would suggest that the authors add another few rows of experiments comparing less similar model architectures.\n\nThe paper's finding that  a model trained from scratch, QANet on SQuAD, suffers significantly without access to the training set inputs is strong supportive evidence for their hypothesis that using pretrained language models has made model extraction easier. \n\nThe authors also present a few defense strategies, membership inference, implicit membership classification, and watermarking. They also discuss the limitations of these strategies and do not claim to have solved the problem at hand.\n\nOverall, I think this paper makes a useful  contribution to the field and I would accept this paper. While I have a couple of issues with some of the experiments (human evaluation and architecture mismatch), I think this paper is thorough and the experiments are well presented. This is the first paper, to the best of my knowledge, showing the efficacy of model extraction of large pretrained language models using rubbish/nonsensical inputs.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1084/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1084/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Thieves on Sesame Street! Model Extraction of BERT-based APIs", "authors": ["Kalpesh Krishna", "Gaurav Singh Tomar", "Ankur P. Parikh", "Nicolas Papernot", "Mohit Iyyer"], "authorids": ["kalpesh@cs.umass.edu", "gtomar@google.com", "aparikh@google.com", "papernot@google.com", "miyyer@cs.umass.edu"], "keywords": ["model extraction", "BERT", "natural language processing", "pretraining language models", "model stealing", "deep learning security"], "TL;DR": "Outputs of modern NLP APIs on nonsensical text provide strong signals about model internals, allowing adversaries to steal the APIs.", "abstract": "We study the problem of model extraction in natural language processing, in which an adversary with only query access to a victim model attempts to reconstruct a local copy of that model. Assuming that both the adversary and victim model fine-tune a large pretrained language model such as BERT (Devlin et al., 2019), we show that the adversary does not need any real training data to successfully mount the attack. In fact, the attacker need not even use grammatical or semantically meaningful queries: we show that random sequences of words coupled with task-specific heuristics form effective queries for model extraction on a diverse set of NLP tasks, including natural language inference and question answering. Our work thus highlights an exploit only made feasible by the shift towards transfer learning methods within the NLP community: for a query budget of a few hundred dollars, an attacker can extract a model that performs only slightly worse than the victim model. Finally, we study two defense strategies against model extraction\u2014membership classification and API watermarking\u2014which while successful against some adversaries can also be circumvented by more clever ones.", "pdf": "/pdf/0072594e40e42b1d603a9ab8dca04f79de577fcb.pdf", "paperhash": "krishna|thieves_on_sesame_street_model_extraction_of_bertbased_apis", "code": "https://github.com/google-research/language/tree/master/language/bert_extraction", "_bibtex": "@inproceedings{\nKrishna2020Thieves,\ntitle={Thieves on Sesame Street! Model Extraction of BERT-based APIs},\nauthor={Kalpesh Krishna and Gaurav Singh Tomar and Ankur P. Parikh and Nicolas Papernot and Mohit Iyyer},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Byl5NREFDr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e6b9a22fab94d189f972720cf7b1b164082652cb.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Byl5NREFDr", "replyto": "Byl5NREFDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1084/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1084/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575853034982, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1084/Reviewers"], "noninvitees": [], "tcdate": 1570237742593, "tmdate": 1575853034999, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1084/-/Official_Review"}}}], "count": 9}