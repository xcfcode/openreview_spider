{"notes": [{"ddate": null, "legacy_migration": true, "tmdate": 1392763320000, "tcdate": 1392763320000, "number": 5, "id": "DOzK-BQbKw-J1", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "mugzy2nI-Ayi1", "replyto": "mugzy2nI-Ayi1", "signatures": ["Dimitris Athanasakis"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "We would like to thank the reviewers for their positive feedback and insightful comments. \r\n\r\nRegarding the theory supporting the algorithm, something that both reviewers a39c and 89fd have brought up,  \r\nwe have updated the proof sketch to reflect that the supporting theory relies on Hoeffding's bound for U-Statistics and provides a high-confidence bound provided a sufficient sample size. The 12.5% stems from the fact that the sample size requirement is typically too large in practice. Thus a more conservative approach employing an iterative scheme that rejects a percentage of the features after each iteration was used. We try to interweave this better with the paper by providing an additional example where the technical conditions hold more strongly (in the updated figure 1), and contrasting that with the iterative scheme (figure 2). \r\n \r\nRegarding the comparison of our approach to the existing approach employed by SignalP, we agree with reviewer a39c \r\nthat further testing is required and added further emphasis to this statement in the updated text. The entries of Table 1 were renamed to reflect where they come from, in accordance to the suggestion of reviewer 89fd.  \r\n \r\nWe have proceeded to rename the paper 'Principled Non-Linear Feature Selection' in response to the comments of reviewer c360. \r\n \r\nRegarding the runtime of our approach, I believe that results could be misleading, owing to the fact that currently the method only employs matlab, whereas stability selection and svms are in c-code with a matlab interface. Informally speaking the method was fast enough to process a single fold in the kaggle contest in roughly an hour. For the signal peptide prediction, this time is vastly increased but is still not substantially worse than competing methods."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Non-Linear Feature Maps, With An Application To Representation Learning", "decision": "submitted, no decision", "abstract": "Recent non-linear feature selection approaches employing greedy optimisation of Centred Kernel Target Alignment(KTA) exhibit strong results in terms of generalisation accuracy and sparsity. However, they are computationally prohibitive for large datasets. We propose randSel, a randomised feature selection algorithm, with attractive scaling properties. Our theoretical analysis of randSel provides strong probabilistic guarantees for correct identification of relevant features. RandSel's characteristics make it an ideal candidate for identifying informative learned representations. We've conducted experimentation to establish the performance of this approach, and present encouraging results, including a 3rd position result in the recent ICML black box learning challenge as well as competitive results for signal peptide prediction, an important problem in bioinformatics.", "pdf": "https://arxiv.org/abs/1312.5869", "paperhash": "athanasakis|learning_nonlinear_feature_maps_with_an_application_to_representation_learning", "keywords": [], "conflicts": [], "authors": ["Dimitrios Athanasakis", "John Shawe-Taylor", "Delmiro Fernandez-Reyes"], "authorids": ["dathanasakis@gmail.com", "jst@cs.ucl.ac.uk", "dfernan@nimr.mrc.ac.uk"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391858640000, "tcdate": 1391858640000, "number": 4, "id": "YTZaYOARqre6g", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "mugzy2nI-Ayi1", "replyto": "mugzy2nI-Ayi1", "signatures": ["anonymous reviewer 89fd"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Learning Non-Linear Feature Maps, With An Application To Representation Learning", "review": "The paper proposes a new and fast approach for selecting features using centered kernel target alignment.\r\n\r\nAs far as I can tell, there seems to be two contributions: 1. use kernel alignment as a way of selecting features 2. random subsample data so that the method can be made scalable. However, I think the algorithmic innovation seems to be thin.\r\n\r\nIn the description of the algorithm (section 4), the paper keeps referring to bootstrap (size). I think that is a misnomer --- the algorithm listing (Algorithm 1) changes to subsampling, which I think what the paper is using.  If the paper is indeed using boostrap (ie, sampling with replacement), the computational complexity would not be reduced as the # of samples (including repeated ones) will be unchanged.  \r\n\r\nI do not follow the reasoning in Theorem 3.6 to come up with the 'bottom 12.5% ' features need to be thrown away.\r\n\r\nThe paper does not seem to explain what are the methods 'deep with XYZ' in Table 1. \r\n\r\nOverall, I think the writing of this paper could use some polishing."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Non-Linear Feature Maps, With An Application To Representation Learning", "decision": "submitted, no decision", "abstract": "Recent non-linear feature selection approaches employing greedy optimisation of Centred Kernel Target Alignment(KTA) exhibit strong results in terms of generalisation accuracy and sparsity. However, they are computationally prohibitive for large datasets. We propose randSel, a randomised feature selection algorithm, with attractive scaling properties. Our theoretical analysis of randSel provides strong probabilistic guarantees for correct identification of relevant features. RandSel's characteristics make it an ideal candidate for identifying informative learned representations. We've conducted experimentation to establish the performance of this approach, and present encouraging results, including a 3rd position result in the recent ICML black box learning challenge as well as competitive results for signal peptide prediction, an important problem in bioinformatics.", "pdf": "https://arxiv.org/abs/1312.5869", "paperhash": "athanasakis|learning_nonlinear_feature_maps_with_an_application_to_representation_learning", "keywords": [], "conflicts": [], "authors": ["Dimitrios Athanasakis", "John Shawe-Taylor", "Delmiro Fernandez-Reyes"], "authorids": ["dathanasakis@gmail.com", "jst@cs.ucl.ac.uk", "dfernan@nimr.mrc.ac.uk"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391820000000, "tcdate": 1391820000000, "number": 2, "id": "CoxYCghVSMD2Z", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "mugzy2nI-Ayi1", "replyto": "mugzy2nI-Ayi1", "signatures": ["anonymous reviewer c360"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Learning Non-Linear Feature Maps, With An Application To Representation Learning", "review": "I fully confess in advance that I am not a very good reviewer for this paper. I have no real research experience with kernel learning or feature selection. As such, I\u2019ll be reporting my scores with low confidence. I haven\u2019t made any attempt to evaluate the sensibility or novelty of the proposed method itself or the related proofs. If none of the other reviewers is able to provide a more confident review I can take some time to study the literature and improve my review.\r\n\r\nThe main thing I do feel able to review somewhat confidently is the empirical results. It\u2019s nice that the authors were able to get good accuracy on the Black Box Learning challenge. It looks like that was a fairly competitive challenge, with over 200 competitors, and they took 3rd place. It\u2019s also nice that they were able to improve over SignalP\u2019s state of the art result on the cleavage site prediction task.\r\n\r\nOne thing I\u2019m a bit concerned about is whether the proposed method improves over the state of the art for feature selection methods. As the authors state, it is somewhat difficult to compare performance on the cleavage site prediction task. Beyond these issues, it\u2019s also not clear to me that randSel improves over state of the art feature selection methods. It looks like it does improve over RFE, but it would be nice if there was a baseline run by other authors. It\u2019s not clear to me that RFE is a state of the art method to beat though. It would be nice to have more explanation of the significance of beating SignalP.\r\n\r\nDetailed comments:\r\n\r\nTitle: I consider 'Representation Learning' to be a superset of 'Learning Non-Linear Feature Maps'. So it doesn't make much sense for the former to be an application of the other. Also both are pretty generic. Pretty much any feature learning method except PCA learns non-linear feature maps. Seems like you really could come up with a more informative title.\r\nI'd argue your title should have the phrase 'Feature Selection' in it. And probably not 'Feature Learning.'\r\n\r\nAbstract: \r\nmissing a space before the paren (this actually happens throughout the paper, not just in the abstract)\r\n\r\nFig 1: You might want to put all the different rows on the same scale so that it\u2019s possible to visually compare the height of the bars across rows. \r\n\r\nPage 4: \u201cSo far have\u201d: there is a word missing here\r\n\r\nPage 6: \u201cThe original data where projected\u201d -> \u201cwere projected\u201d\r\n\t\u201cThe organizers did not reveal the source of the dataset\u201d: this sounds like the organizers did not reveal the source of the data was SVHN. You might want to specify that you didn\u2019t know it was created by multiplication by a random projection matrix either.\r\n\t\u201cwhere provided\u201d -> \u201cwere provided\u201d\r\n\t\u201cranking third in both cases\u201d: \r\n\t\tHere\u2019s the public leaderboard: http://www.kaggle.com/c/challenges-in-representation-learning-the-black-box-learning-challenge/leaderboard/public\r\n\t\tAccording to this, Bing Xu is 3rd. But according to the report put out by the organizers ( http://arxiv.org/pdf/1307.0414.pdf ) you guys actually were 3rd and it sounds like Bing Xu was ranked worse than 3rd originally. Did you delete your kaggle account and get taken off the leaderboard or something?\r\n\r\n\r\nSupplementary Material T:\r\nThe box around 'OVER-COMPLETE LEARNED REPRESENTATION' is too tight"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Non-Linear Feature Maps, With An Application To Representation Learning", "decision": "submitted, no decision", "abstract": "Recent non-linear feature selection approaches employing greedy optimisation of Centred Kernel Target Alignment(KTA) exhibit strong results in terms of generalisation accuracy and sparsity. However, they are computationally prohibitive for large datasets. We propose randSel, a randomised feature selection algorithm, with attractive scaling properties. Our theoretical analysis of randSel provides strong probabilistic guarantees for correct identification of relevant features. RandSel's characteristics make it an ideal candidate for identifying informative learned representations. We've conducted experimentation to establish the performance of this approach, and present encouraging results, including a 3rd position result in the recent ICML black box learning challenge as well as competitive results for signal peptide prediction, an important problem in bioinformatics.", "pdf": "https://arxiv.org/abs/1312.5869", "paperhash": "athanasakis|learning_nonlinear_feature_maps_with_an_application_to_representation_learning", "keywords": [], "conflicts": [], "authors": ["Dimitrios Athanasakis", "John Shawe-Taylor", "Delmiro Fernandez-Reyes"], "authorids": ["dathanasakis@gmail.com", "jst@cs.ucl.ac.uk", "dfernan@nimr.mrc.ac.uk"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391820000000, "tcdate": 1391820000000, "number": 3, "id": "JJOimESUine4P", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "mugzy2nI-Ayi1", "replyto": "mugzy2nI-Ayi1", "signatures": ["anonymous reviewer c360"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Learning Non-Linear Feature Maps, With An Application To Representation Learning", "review": "I fully confess in advance that I am not a very good reviewer for this paper. I have no real research experience with kernel learning or feature selection. As such, I\u2019ll be reporting my scores with low confidence. I haven\u2019t made any attempt to evaluate the sensibility or novelty of the proposed method itself or the related proofs. If none of the other reviewers is able to provide a more confident review I can take some time to study the literature and improve my review.\r\n\r\nThe main thing I do feel able to review somewhat confidently is the empirical results. It\u2019s nice that the authors were able to get good accuracy on the Black Box Learning challenge. It looks like that was a fairly competitive challenge, with over 200 competitors, and they took 3rd place. It\u2019s also nice that they were able to improve over SignalP\u2019s state of the art result on the cleavage site prediction task.\r\n\r\nOne thing I\u2019m a bit concerned about is whether the proposed method improves over the state of the art for feature selection methods. As the authors state, it is somewhat difficult to compare performance on the cleavage site prediction task. Beyond these issues, it\u2019s also not clear to me that randSel improves over state of the art feature selection methods. It looks like it does improve over RFE, but it would be nice if there was a baseline run by other authors. It\u2019s not clear to me that RFE is a state of the art method to beat though. It would be nice to have more explanation of the significance of beating SignalP.\r\n\r\nDetailed comments:\r\n\r\nTitle: I consider 'Representation Learning' to be a superset of 'Learning Non-Linear Feature Maps'. So it doesn't make much sense for the former to be an application of the other. Also both are pretty generic. Pretty much any feature learning method except PCA learns non-linear feature maps. Seems like you really could come up with a more informative title.\r\nI'd argue your title should have the phrase 'Feature Selection' in it. And probably not 'Feature Learning.'\r\n\r\nAbstract: \r\nmissing a space before the paren (this actually happens throughout the paper, not just in the abstract)\r\n\r\nFig 1: You might want to put all the different rows on the same scale so that it\u2019s possible to visually compare the height of the bars across rows. \r\n\r\nPage 4: \u201cSo far have\u201d: there is a word missing here\r\n\r\nPage 6: \u201cThe original data where projected\u201d -> \u201cwere projected\u201d\r\n\t\u201cThe organizers did not reveal the source of the dataset\u201d: this sounds like the organizers did not reveal the source of the data was SVHN. You might want to specify that you didn\u2019t know it was created by multiplication by a random projection matrix either.\r\n\t\u201cwhere provided\u201d -> \u201cwere provided\u201d\r\n\t\u201cranking third in both cases\u201d: \r\n\t\tHere\u2019s the public leaderboard: http://www.kaggle.com/c/challenges-in-representation-learning-the-black-box-learning-challenge/leaderboard/public\r\n\t\tAccording to this, Bing Xu is 3rd. But according to the report put out by the organizers ( http://arxiv.org/pdf/1307.0414.pdf ) you guys actually were 3rd and it sounds like Bing Xu was ranked worse than 3rd originally. Did you delete your kaggle account and get taken off the leaderboard or something?\r\n\r\n\r\nSupplementary Material T:\r\nThe box around 'OVER-COMPLETE LEARNED REPRESENTATION' is too tight"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Non-Linear Feature Maps, With An Application To Representation Learning", "decision": "submitted, no decision", "abstract": "Recent non-linear feature selection approaches employing greedy optimisation of Centred Kernel Target Alignment(KTA) exhibit strong results in terms of generalisation accuracy and sparsity. However, they are computationally prohibitive for large datasets. We propose randSel, a randomised feature selection algorithm, with attractive scaling properties. Our theoretical analysis of randSel provides strong probabilistic guarantees for correct identification of relevant features. RandSel's characteristics make it an ideal candidate for identifying informative learned representations. We've conducted experimentation to establish the performance of this approach, and present encouraging results, including a 3rd position result in the recent ICML black box learning challenge as well as competitive results for signal peptide prediction, an important problem in bioinformatics.", "pdf": "https://arxiv.org/abs/1312.5869", "paperhash": "athanasakis|learning_nonlinear_feature_maps_with_an_application_to_representation_learning", "keywords": [], "conflicts": [], "authors": ["Dimitrios Athanasakis", "John Shawe-Taylor", "Delmiro Fernandez-Reyes"], "authorids": ["dathanasakis@gmail.com", "jst@cs.ucl.ac.uk", "dfernan@nimr.mrc.ac.uk"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391431140000, "tcdate": 1391431140000, "number": 1, "id": "T9PA0NgIcNTUw", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "mugzy2nI-Ayi1", "replyto": "mugzy2nI-Ayi1", "signatures": ["anonymous reviewer a39c"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Learning Non-Linear Feature Maps, With An Application To Representation Learning", "review": "This interesting submission links prior work on KTA with the question\r\nof representation learning.  I then provides learning theory to\r\nunderpin this idea and suggests a nonlinear feature selection\r\nalgorithm.  Finally, it shows its success for an ICML 2013 benchmark,\r\nwhere the method finished with a bronze medal. In addition it shows\r\ninteresting/impressive results on bioinformatics, namely cleavage site\r\nprediction. This is an interesting paper which should be accepted. \r\n\r\nSome suggestions/criticism that may help improving the ms:\r\n\r\n1. it would be nice to briefly discuss runtimes\r\n\r\n2. so far the LT results seem like a paper within a paper, it would be\r\nnice to interweave this better, and maybe show a/the toy simulation\r\nwhat the derived bounds mean on concrete data.\r\n\r\n3. The algorithm seems slightly ad hoc, where do the 12.5% come from\r\n\r\n4. It would be nice to leave more room to the applications, already\r\none seems sufficiently convincing\r\n\r\n5. Finally, I strongly suggest to further tone down that statement\r\nthat the present approach is better than ref 14. I do not see this.\r\nFor this statement extensive testing with p-values and a thorough\r\nunderstanding of the whys would be required."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Non-Linear Feature Maps, With An Application To Representation Learning", "decision": "submitted, no decision", "abstract": "Recent non-linear feature selection approaches employing greedy optimisation of Centred Kernel Target Alignment(KTA) exhibit strong results in terms of generalisation accuracy and sparsity. However, they are computationally prohibitive for large datasets. We propose randSel, a randomised feature selection algorithm, with attractive scaling properties. Our theoretical analysis of randSel provides strong probabilistic guarantees for correct identification of relevant features. RandSel's characteristics make it an ideal candidate for identifying informative learned representations. We've conducted experimentation to establish the performance of this approach, and present encouraging results, including a 3rd position result in the recent ICML black box learning challenge as well as competitive results for signal peptide prediction, an important problem in bioinformatics.", "pdf": "https://arxiv.org/abs/1312.5869", "paperhash": "athanasakis|learning_nonlinear_feature_maps_with_an_application_to_representation_learning", "keywords": [], "conflicts": [], "authors": ["Dimitrios Athanasakis", "John Shawe-Taylor", "Delmiro Fernandez-Reyes"], "authorids": ["dathanasakis@gmail.com", "jst@cs.ucl.ac.uk", "dfernan@nimr.mrc.ac.uk"]}, "tags": [], "invitation": {}}}, {"replyto": null, "ddate": null, "legacy_migration": true, "tmdate": 1387782420000, "tcdate": 1387782420000, "number": 29, "id": "mugzy2nI-Ayi1", "invitation": "ICLR.cc/2014/conference/-/submission", "forum": "mugzy2nI-Ayi1", "signatures": ["dathanasakis@gmail.com"], "readers": ["everyone"], "content": {"title": "Learning Non-Linear Feature Maps, With An Application To Representation Learning", "decision": "submitted, no decision", "abstract": "Recent non-linear feature selection approaches employing greedy optimisation of Centred Kernel Target Alignment(KTA) exhibit strong results in terms of generalisation accuracy and sparsity. However, they are computationally prohibitive for large datasets. We propose randSel, a randomised feature selection algorithm, with attractive scaling properties. Our theoretical analysis of randSel provides strong probabilistic guarantees for correct identification of relevant features. RandSel's characteristics make it an ideal candidate for identifying informative learned representations. We've conducted experimentation to establish the performance of this approach, and present encouraging results, including a 3rd position result in the recent ICML black box learning challenge as well as competitive results for signal peptide prediction, an important problem in bioinformatics.", "pdf": "https://arxiv.org/abs/1312.5869", "paperhash": "athanasakis|learning_nonlinear_feature_maps_with_an_application_to_representation_learning", "keywords": [], "conflicts": [], "authors": ["Dimitrios Athanasakis", "John Shawe-Taylor", "Delmiro Fernandez-Reyes"], "authorids": ["dathanasakis@gmail.com", "jst@cs.ucl.ac.uk", "dfernan@nimr.mrc.ac.uk"]}, "writers": [], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1369422751717, "tmdate": 1496674357195, "id": "ICLR.cc/2014/conference/-/submission", "writers": ["ICLR.cc/2014"], "signatures": ["OpenReview.net"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": []}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1377198751717, "cdate": 1496674357195}}}], "count": 6}