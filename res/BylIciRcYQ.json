{"notes": [{"id": "BylIciRcYQ", "original": "SJxOkROqF7", "number": 535, "cdate": 1538087821746, "ddate": null, "tcdate": 1538087821746, "tmdate": 1551371434149, "tddate": null, "forum": "BylIciRcYQ", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "SGD Converges to Global Minimum in Deep Learning via Star-convex Path", "abstract": "Stochastic gradient descent (SGD) has been found to be surprisingly effective in training a variety of deep neural networks. However, there is still a lack of understanding on how and why SGD can train these complex networks towards a global minimum. In this study, we establish the convergence of SGD to a global minimum for nonconvex optimization problems that are commonly encountered in neural network training. Our argument exploits the following two important properties: 1) the training loss can achieve zero value (approximately), which has been widely observed in deep learning; 2) SGD follows a star-convex path, which is verified by various experiments in this paper.  In such a context, our analysis shows that SGD, although has long been considered as a randomized algorithm, converges in an intrinsically deterministic manner to a global minimum. ", "keywords": ["SGD", "deep learning", "global minimum", "convergence"], "authorids": ["yi.zhou610@duke.edu", "baymax@mail.ustc.edu.cn", "huishuai.zhang@microsoft.com", "liang.889@osu.edu", "vahid.tarokh@duke.edu"], "authors": ["Yi Zhou", "Junjie Yang", "Huishuai Zhang", "Yingbin Liang", "Vahid Tarokh"], "pdf": "/pdf/06cea9de669d2b7ce5b2e75a513d4032fec4063f.pdf", "paperhash": "zhou|sgd_converges_to_global_minimum_in_deep_learning_via_starconvex_path", "_bibtex": "@inproceedings{\nzhou2018sgd,\ntitle={{SGD} Converges to Global Minimum in Deep Learning via Star-convex Path},\nauthor={Yi Zhou and Junjie Yang and Huishuai Zhang and Yingbin Liang and Vahid Tarokh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BylIciRcYQ},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "r1lkamDVxN", "original": null, "number": 1, "cdate": 1545003959402, "ddate": null, "tcdate": 1545003959402, "tmdate": 1545354486996, "tddate": null, "forum": "BylIciRcYQ", "replyto": "BylIciRcYQ", "invitation": "ICLR.cc/2019/Conference/-/Paper535/Meta_Review", "content": {"metareview": "The proposed notion of star convexity is interesting and the empirical work done to provide evidence that it is indeed present in real-world neural network training is appreciated.  The reviewers raise a number of concerns. The authors were able to convince some of the reviewers with new experiments under MSE loss and experiments showing how robust the method was to the reference point. The most serious concerns relate to novelty and the assumptions that individual functions share a global minima with respect to which the path of iterates generated by SGD satisfies the star convexity property. I'm inclined to accept the authors rebuttal, although it would have been nicer had the reviewer re-engaged. Overall, the paper is on the borderline.", "confidence": "3: The area chair is somewhat confident", "recommendation": "Accept (Poster)", "title": "New notion of nonconvexity"}, "signatures": ["ICLR.cc/2019/Conference/Paper535/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper535/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SGD Converges to Global Minimum in Deep Learning via Star-convex Path", "abstract": "Stochastic gradient descent (SGD) has been found to be surprisingly effective in training a variety of deep neural networks. However, there is still a lack of understanding on how and why SGD can train these complex networks towards a global minimum. In this study, we establish the convergence of SGD to a global minimum for nonconvex optimization problems that are commonly encountered in neural network training. Our argument exploits the following two important properties: 1) the training loss can achieve zero value (approximately), which has been widely observed in deep learning; 2) SGD follows a star-convex path, which is verified by various experiments in this paper.  In such a context, our analysis shows that SGD, although has long been considered as a randomized algorithm, converges in an intrinsically deterministic manner to a global minimum. ", "keywords": ["SGD", "deep learning", "global minimum", "convergence"], "authorids": ["yi.zhou610@duke.edu", "baymax@mail.ustc.edu.cn", "huishuai.zhang@microsoft.com", "liang.889@osu.edu", "vahid.tarokh@duke.edu"], "authors": ["Yi Zhou", "Junjie Yang", "Huishuai Zhang", "Yingbin Liang", "Vahid Tarokh"], "pdf": "/pdf/06cea9de669d2b7ce5b2e75a513d4032fec4063f.pdf", "paperhash": "zhou|sgd_converges_to_global_minimum_in_deep_learning_via_starconvex_path", "_bibtex": "@inproceedings{\nzhou2018sgd,\ntitle={{SGD} Converges to Global Minimum in Deep Learning via Star-convex Path},\nauthor={Yi Zhou and Junjie Yang and Huishuai Zhang and Yingbin Liang and Vahid Tarokh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BylIciRcYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper535/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353182150, "tddate": null, "super": null, "final": null, "reply": {"forum": "BylIciRcYQ", "replyto": "BylIciRcYQ", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper535/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper535/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper535/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353182150}}}, {"id": "H1eq2Hdo07", "original": null, "number": 7, "cdate": 1543370161963, "ddate": null, "tcdate": 1543370161963, "tmdate": 1543370161963, "tddate": null, "forum": "BylIciRcYQ", "replyto": "BylIciRcYQ", "invitation": "ICLR.cc/2019/Conference/-/Paper535/Official_Comment", "content": {"title": "Authors\u2019 comment on very recent related studies", "comment": "While our submission is under review, a few related but very different studies [1,2,3] were posted on Arxiv very recently. We would like to briefly clarify our difference from these results here, which we will further include into the future version of this paper. \n\nThe studies [1,2,3] proved the optimization convergence results of gradient-based algorithms in over-parameterized deep learning based on various technical assumptions about overparameterization and algorithm parameters, which are still subject to validations in deep learning practice in the future. As a comparison, our proof of convergence is based on the star-convexity property, which we have verified in training a variety of practical neural networks on real datasets. The star-convexity property by itself is a new finding and can be of independent interest. Furthermore, our result is on the convergence of parameters, which in nature is different from the results in [1,2,3] on the convergence of the loss function value.\n\n[1]``Gradient Descent Finds Global Minima of Deep Neural Networks\u2019\u2019, Simon S. Du, Jason D. Lee, Haochuan Li, Liwei Wang, Xiyu Zhai\n\n[2] ``A Convergence Theory for Deep Learning via Over-Parameterization\u2019\u2019, Zeyuan Allen-Zhu, Yuanzhi Li, Zhao Song\n\n[3] ``Stochastic Gradient Descent Optimizes Over-parameterized Deep ReLU Networks\u2019\u2019,\nDifan Zou, Yuan Cao, Dongruo Zhou, Quanquan Gu.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper535/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper535/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper535/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SGD Converges to Global Minimum in Deep Learning via Star-convex Path", "abstract": "Stochastic gradient descent (SGD) has been found to be surprisingly effective in training a variety of deep neural networks. However, there is still a lack of understanding on how and why SGD can train these complex networks towards a global minimum. In this study, we establish the convergence of SGD to a global minimum for nonconvex optimization problems that are commonly encountered in neural network training. Our argument exploits the following two important properties: 1) the training loss can achieve zero value (approximately), which has been widely observed in deep learning; 2) SGD follows a star-convex path, which is verified by various experiments in this paper.  In such a context, our analysis shows that SGD, although has long been considered as a randomized algorithm, converges in an intrinsically deterministic manner to a global minimum. ", "keywords": ["SGD", "deep learning", "global minimum", "convergence"], "authorids": ["yi.zhou610@duke.edu", "baymax@mail.ustc.edu.cn", "huishuai.zhang@microsoft.com", "liang.889@osu.edu", "vahid.tarokh@duke.edu"], "authors": ["Yi Zhou", "Junjie Yang", "Huishuai Zhang", "Yingbin Liang", "Vahid Tarokh"], "pdf": "/pdf/06cea9de669d2b7ce5b2e75a513d4032fec4063f.pdf", "paperhash": "zhou|sgd_converges_to_global_minimum_in_deep_learning_via_starconvex_path", "_bibtex": "@inproceedings{\nzhou2018sgd,\ntitle={{SGD} Converges to Global Minimum in Deep Learning via Star-convex Path},\nauthor={Yi Zhou and Junjie Yang and Huishuai Zhang and Yingbin Liang and Vahid Tarokh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BylIciRcYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper535/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621617343, "tddate": null, "super": null, "final": null, "reply": {"forum": "BylIciRcYQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper535/Authors", "ICLR.cc/2019/Conference/Paper535/Reviewers", "ICLR.cc/2019/Conference/Paper535/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper535/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper535/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper535/Authors|ICLR.cc/2019/Conference/Paper535/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper535/Reviewers", "ICLR.cc/2019/Conference/Paper535/Authors", "ICLR.cc/2019/Conference/Paper535/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621617343}}}, {"id": "B1xsEN2d37", "original": null, "number": 1, "cdate": 1541092402992, "ddate": null, "tcdate": 1541092402992, "tmdate": 1543339834214, "tddate": null, "forum": "BylIciRcYQ", "replyto": "BylIciRcYQ", "invitation": "ICLR.cc/2019/Conference/-/Paper535/Official_Review", "content": {"title": "Good theoretical paper", "review": "This paper analyzed the global convergence property of SGD in deep learning based on the star-convexity assumption. The claims seem correct and validated empirically with some observations in deep learning. The writing is good and easy to follow.\n\nMy understanding of the analysis is that all the claims seem to be valid when the solution is in a wide valley of the loss surface where the star-convexity holds, in general. This has been observed empirically in previous work, and the experiments on cifar10 in Fig. 2 support my hypothesis. My questions are:\n\n1. How to guarantee the star-convexity will be valid in deep learning?\n2. What network or data properties can lead to such assumption?\n\nAlso, this is a missing related work from the algorithmic perspective to explore the global optimization in deep learning: \n\nZhang et. al. CVPR'18. \"BPGrad: Towards Global Optimality in Deep Learning via Branch and Pruning\".\n", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper535/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "SGD Converges to Global Minimum in Deep Learning via Star-convex Path", "abstract": "Stochastic gradient descent (SGD) has been found to be surprisingly effective in training a variety of deep neural networks. However, there is still a lack of understanding on how and why SGD can train these complex networks towards a global minimum. In this study, we establish the convergence of SGD to a global minimum for nonconvex optimization problems that are commonly encountered in neural network training. Our argument exploits the following two important properties: 1) the training loss can achieve zero value (approximately), which has been widely observed in deep learning; 2) SGD follows a star-convex path, which is verified by various experiments in this paper.  In such a context, our analysis shows that SGD, although has long been considered as a randomized algorithm, converges in an intrinsically deterministic manner to a global minimum. ", "keywords": ["SGD", "deep learning", "global minimum", "convergence"], "authorids": ["yi.zhou610@duke.edu", "baymax@mail.ustc.edu.cn", "huishuai.zhang@microsoft.com", "liang.889@osu.edu", "vahid.tarokh@duke.edu"], "authors": ["Yi Zhou", "Junjie Yang", "Huishuai Zhang", "Yingbin Liang", "Vahid Tarokh"], "pdf": "/pdf/06cea9de669d2b7ce5b2e75a513d4032fec4063f.pdf", "paperhash": "zhou|sgd_converges_to_global_minimum_in_deep_learning_via_starconvex_path", "_bibtex": "@inproceedings{\nzhou2018sgd,\ntitle={{SGD} Converges to Global Minimum in Deep Learning via Star-convex Path},\nauthor={Yi Zhou and Junjie Yang and Huishuai Zhang and Yingbin Liang and Vahid Tarokh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BylIciRcYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper535/Official_Review", "cdate": 1542234439133, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BylIciRcYQ", "replyto": "BylIciRcYQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper535/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335746392, "tmdate": 1552335746392, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper535/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "H1g6MIMIAm", "original": null, "number": 5, "cdate": 1543018005437, "ddate": null, "tcdate": 1543018005437, "tmdate": 1543018005437, "tddate": null, "forum": "BylIciRcYQ", "replyto": "SJxGnBGW07", "invitation": "ICLR.cc/2019/Conference/-/Paper535/Official_Comment", "content": {"title": "Reply", "comment": "I thank authors for carefully addressing my concerns and I raise my score accordingly. \n\nAll the additional experiments are very interesting. The experiments of different reference point in Fig.5 suggests that the phenomenon of star-convexity is quite stable as long as the iterate are in a region \"closed\" to the minimum. The experiments on the norm of the iterate somehow supports this observation, since the change in the norm becomes less significant after some iteration, i.e. stableness of the last iterates. \n\nMoreover, according to Fig.5, it seems like a phase transition occurs after some iterations. In particular, the behavior of the first iterations are quite different from the latest ones, hence it would be interesting to develop some further characterization of such phase transition. \n"}, "signatures": ["ICLR.cc/2019/Conference/Paper535/AnonReviewer1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper535/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper535/AnonReviewer1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SGD Converges to Global Minimum in Deep Learning via Star-convex Path", "abstract": "Stochastic gradient descent (SGD) has been found to be surprisingly effective in training a variety of deep neural networks. However, there is still a lack of understanding on how and why SGD can train these complex networks towards a global minimum. In this study, we establish the convergence of SGD to a global minimum for nonconvex optimization problems that are commonly encountered in neural network training. Our argument exploits the following two important properties: 1) the training loss can achieve zero value (approximately), which has been widely observed in deep learning; 2) SGD follows a star-convex path, which is verified by various experiments in this paper.  In such a context, our analysis shows that SGD, although has long been considered as a randomized algorithm, converges in an intrinsically deterministic manner to a global minimum. ", "keywords": ["SGD", "deep learning", "global minimum", "convergence"], "authorids": ["yi.zhou610@duke.edu", "baymax@mail.ustc.edu.cn", "huishuai.zhang@microsoft.com", "liang.889@osu.edu", "vahid.tarokh@duke.edu"], "authors": ["Yi Zhou", "Junjie Yang", "Huishuai Zhang", "Yingbin Liang", "Vahid Tarokh"], "pdf": "/pdf/06cea9de669d2b7ce5b2e75a513d4032fec4063f.pdf", "paperhash": "zhou|sgd_converges_to_global_minimum_in_deep_learning_via_starconvex_path", "_bibtex": "@inproceedings{\nzhou2018sgd,\ntitle={{SGD} Converges to Global Minimum in Deep Learning via Star-convex Path},\nauthor={Yi Zhou and Junjie Yang and Huishuai Zhang and Yingbin Liang and Vahid Tarokh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BylIciRcYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper535/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621617343, "tddate": null, "super": null, "final": null, "reply": {"forum": "BylIciRcYQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper535/Authors", "ICLR.cc/2019/Conference/Paper535/Reviewers", "ICLR.cc/2019/Conference/Paper535/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper535/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper535/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper535/Authors|ICLR.cc/2019/Conference/Paper535/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper535/Reviewers", "ICLR.cc/2019/Conference/Paper535/Authors", "ICLR.cc/2019/Conference/Paper535/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621617343}}}, {"id": "Hyg7evSKnQ", "original": null, "number": 3, "cdate": 1541129963187, "ddate": null, "tcdate": 1541129963187, "tmdate": 1543013708610, "tddate": null, "forum": "BylIciRcYQ", "replyto": "BylIciRcYQ", "invitation": "ICLR.cc/2019/Conference/-/Paper535/Official_Review", "content": {"title": "The paper provides interesting idea but the empirical results may be biased due to ill-posed problem ", "review": "The paper proposes a new approach to explain the effective behavior of SGD in training deep neural networks by introducing the notion of star-convexity. A function h is star-convex if its global minimum lies on or above any plane tangent to the function, namely h* >= h(x) + < h'(x), x*-x> for any x. Under such condition, the paper shows that the empirical loss goes to zero and the iterates generated by SGD converges to a global minimum. Extensive experiments has been conducted to empirically validate the assumption. \n\nThe paper is very well organized and is easy to follow. The star-convexity assumption is very interesting which provides new insights about the landscape of the loss function and the trajectory of SGD. It is in general difficult to theoretically check this condition so several empirical verifications has been proposed. My main concern is about these empirical verifications.\n\n1) The minimum of the cross entropy loss lies at infinity \nThe experiments are performed respect to the cross entropy loss. However, cross entropy loss violates Fact 1 since for any finite weight, cross entropy loss is always strictly positive. Thus the zero is never attained and the global minimum always lies at infinity. As a result, the star-convexity inequality h* >= h(x) + < h'(x), x*-x> hardly makes sense since x* is at infinity and neither does the theorem followed. \nIn this case, a plot of the norm of xk is highly suggested since it is a sanity check to see whether the iterates goes to infinity. \n\n2) The phenomenon may depend on the reference point, i.e last iterate\nSince the minimum is never attained, the empirical check of the star-convexity maybe biased. More precisely, it might be possible that the behavior of the observed phenomenon depends on the reference point, i.e. the last iterate. Therefore, it will be interesting to see if the observed phenomenon still holds when varying the stopping time, for instance plot the star convexity check using the iterates at 60, 80, 100, 120 epochs as reference point. \n\nIn fact, the experiments shown in Figure 4 implicitly supports that the behavior may change dramatically respect to different reference point. The reason is that the loss in these experiments are far away from 0, meaning that we are far from the minimum, thus checking the star-convexity does not make sense because the star-convexity is only defined respect to the minimum. \n\nOverall, the paper provides interesting idea but the empirical results may be biased due to ill-posed problem ", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper535/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "SGD Converges to Global Minimum in Deep Learning via Star-convex Path", "abstract": "Stochastic gradient descent (SGD) has been found to be surprisingly effective in training a variety of deep neural networks. However, there is still a lack of understanding on how and why SGD can train these complex networks towards a global minimum. In this study, we establish the convergence of SGD to a global minimum for nonconvex optimization problems that are commonly encountered in neural network training. Our argument exploits the following two important properties: 1) the training loss can achieve zero value (approximately), which has been widely observed in deep learning; 2) SGD follows a star-convex path, which is verified by various experiments in this paper.  In such a context, our analysis shows that SGD, although has long been considered as a randomized algorithm, converges in an intrinsically deterministic manner to a global minimum. ", "keywords": ["SGD", "deep learning", "global minimum", "convergence"], "authorids": ["yi.zhou610@duke.edu", "baymax@mail.ustc.edu.cn", "huishuai.zhang@microsoft.com", "liang.889@osu.edu", "vahid.tarokh@duke.edu"], "authors": ["Yi Zhou", "Junjie Yang", "Huishuai Zhang", "Yingbin Liang", "Vahid Tarokh"], "pdf": "/pdf/06cea9de669d2b7ce5b2e75a513d4032fec4063f.pdf", "paperhash": "zhou|sgd_converges_to_global_minimum_in_deep_learning_via_starconvex_path", "_bibtex": "@inproceedings{\nzhou2018sgd,\ntitle={{SGD} Converges to Global Minimum in Deep Learning via Star-convex Path},\nauthor={Yi Zhou and Junjie Yang and Huishuai Zhang and Yingbin Liang and Vahid Tarokh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BylIciRcYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper535/Official_Review", "cdate": 1542234439133, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BylIciRcYQ", "replyto": "BylIciRcYQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper535/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335746392, "tmdate": 1552335746392, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper535/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "HkgubIm-Am", "original": null, "number": 4, "cdate": 1542694400004, "ddate": null, "tcdate": 1542694400004, "tmdate": 1542694454111, "tddate": null, "forum": "BylIciRcYQ", "replyto": "HyxZqj0O27", "invitation": "ICLR.cc/2019/Conference/-/Paper535/Official_Comment", "content": {"title": "Response", "comment": "We thank the reviewer for the valuable feedbacks. \n\nThis paper aims at reporting an interesting star-convex property of the SGD optimization path that has been observed in training a variety of DL models, including MLP, CNN, residual networks and RNN (verified recently). Moreover, our theory is motivated by such a common observation and attempts to justify the role of this property plays in determining the convergence of the optimization in DL. \n\nOur response to the reviewer\u2019s comments are provided as follows.\n\nQuality:  In Fact 1: How can you conclude that the set of common global minimizers are bounded? \n\nA: We thank the reviewer for pointing out this. In fact, our theoretical results do not require that all the global minima be bounded. To be precise, we only need the star-convexity to hold for a bounded subset of the common global minimum, under which our theory guarantees that SGD converges to one of the elements in that set.  We clarify this in the revision.\n\nClarity: On page 3, \u201cfact\u201d is the right word here. On Page 3 the x^* here is the last iteration produced by SGD. On page 4, the statement in definition 1 is more like a theorem.\n\nA: We thank the reviewer for valuable suggestions. In the revision, we use ``observation\u2019\u2019 instead of \u201cfact\u201d. We add the non-negativity assumption. We now refer to x^* as the output of SGD. We restate definition 1&2.\n\nSignificance 1) The analysis of this paper is based on ... \n\nA: We have added new experiments to demonstrate that for the star-convexity of SGD holds for the MSE loss function (which can achieve zero loss).  We agree that cross-entropy achieves only near-zero loss, but such an approximation is not that unreasonable, as can be observed by the experiments that we added as Fig. 6 in Appendix D, which illustrates that the cross-entropy loss is nearly zero for certain finite norm of the weight parameters. Hence, we do expect that such approximation can convey useful information.  \nWe believe that we should not restrict ourselves only to theory that exactly matches what happens in practice. Near-zero loss is widely observed for training over-parameterized neural networks. Thus, the common minimizer assumption is motivated by this observation, and has led us to discover the star-convexity of SGD paths empirically, and further develop the convergence of SGD based on such a property. Hence, the approximate common global minimizer does yield consistent practical and theoretical results that explain what happens in deep learning.\n\n2) Secondly, \u2026. \n\nA: We first want to point out that the \u201cepoch-wise star-convexity\u201d is an accumulative effect of the residual error of every component loss over one epoch, which is a nontrivial and much weaker condition than that the entire loss function is star-convex over the points that SGD visits. Thus, assuming \u201cepoch-wise star-convexity\u201d, the proofs of theorems 1 and 2 do not follow the conventional convex analysis. One can of course argue that it is simple, but we think the focus here should be the information that it conveys in such a context.\nSecond, we report a star-convex path property over a wide range of DL training tasks, which have not been reported in the existing literature to our knowledge. We do think this is an informative discovery. Of course, understanding when and why such a property hold for DL training is definitely important and deserves exploration in the future work. \n\n3) In fact, it is well-known that SGD with constant step size \u2026 \n\nA: We want to point out the difference between our theory and the result mentioned by the reviewer. The star-convexity is much more relaxed than that the loss function F being strongly convex. Second, the bounded variance assumption that Var(g_k) <= M ||\u2207F(x_k)||^2 is hard to justify in general, and it is not clear to what extent can it be justified in DL tasks. Not to mention that it is clearly not true that the loss function is strong convex! In contrast, our star-convexity assumption is verified by various DL experiments as we report in the paper. Moreover, under strong convexity, traditional analysis only guarantees the convergence of the sequence in probability, which is much weaker than our deterministic convergence results in Theorem 3. \n\n4) With respect to the empirical evidence, ... \n\nA: We thank the reviewer for pointing out this, and we are aware of it. We use ReLU activation as it is commonly used in DL tasks. Of course, one can use a smoothed version of ReLU (softplus) and obtain nearly the same result. We clarified this and added more experiments on this in the revision.\nFor the variance, we do observe that the variance vanishes as SGD converges, and in fact report such a property as Corollary 1 in the originally submitted version. This is a necessary observation when SGD converges to a common global minimizer, and therefore also justifies the existence of common global minimum to some extent. We add experiments on this in the revision.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper535/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper535/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper535/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SGD Converges to Global Minimum in Deep Learning via Star-convex Path", "abstract": "Stochastic gradient descent (SGD) has been found to be surprisingly effective in training a variety of deep neural networks. However, there is still a lack of understanding on how and why SGD can train these complex networks towards a global minimum. In this study, we establish the convergence of SGD to a global minimum for nonconvex optimization problems that are commonly encountered in neural network training. Our argument exploits the following two important properties: 1) the training loss can achieve zero value (approximately), which has been widely observed in deep learning; 2) SGD follows a star-convex path, which is verified by various experiments in this paper.  In such a context, our analysis shows that SGD, although has long been considered as a randomized algorithm, converges in an intrinsically deterministic manner to a global minimum. ", "keywords": ["SGD", "deep learning", "global minimum", "convergence"], "authorids": ["yi.zhou610@duke.edu", "baymax@mail.ustc.edu.cn", "huishuai.zhang@microsoft.com", "liang.889@osu.edu", "vahid.tarokh@duke.edu"], "authors": ["Yi Zhou", "Junjie Yang", "Huishuai Zhang", "Yingbin Liang", "Vahid Tarokh"], "pdf": "/pdf/06cea9de669d2b7ce5b2e75a513d4032fec4063f.pdf", "paperhash": "zhou|sgd_converges_to_global_minimum_in_deep_learning_via_starconvex_path", "_bibtex": "@inproceedings{\nzhou2018sgd,\ntitle={{SGD} Converges to Global Minimum in Deep Learning via Star-convex Path},\nauthor={Yi Zhou and Junjie Yang and Huishuai Zhang and Yingbin Liang and Vahid Tarokh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BylIciRcYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper535/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621617343, "tddate": null, "super": null, "final": null, "reply": {"forum": "BylIciRcYQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper535/Authors", "ICLR.cc/2019/Conference/Paper535/Reviewers", "ICLR.cc/2019/Conference/Paper535/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper535/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper535/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper535/Authors|ICLR.cc/2019/Conference/Paper535/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper535/Reviewers", "ICLR.cc/2019/Conference/Paper535/Authors", "ICLR.cc/2019/Conference/Paper535/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621617343}}}, {"id": "B1equOGWCm", "original": null, "number": 2, "cdate": 1542690929746, "ddate": null, "tcdate": 1542690929746, "tmdate": 1542690929746, "tddate": null, "forum": "BylIciRcYQ", "replyto": "B1xsEN2d37", "invitation": "ICLR.cc/2019/Conference/-/Paper535/Official_Comment", "content": {"title": "Response", "comment": "We thank the reviewer for the valuable feedbacks. \n\nThis paper aims at reporting an interesting star-convex property of the SGD optimization path that has been observed in training a variety of DL models, including MLP, CNN, residual networks and RNN (verified recently). Moreover, our theory is motivated by such a common observation and attempts to justify the role of this property plays in determining the convergence of the optimization in DL. \n\nOur response to the reviewer\u2019s comments are provided as follows.\n\n1. How to guarantee the star-convexity will be valid in deep learning?\n\nResponse: We thank the reviewer for pointing out this question. It is definitely interesting to explore the underlying mechanism that leads to such a common observation.  We think that over-parameterization can be one of the important factors. We are currently investigating this issue theoretically on some simple networks, and our understanding so far favors such a direction.\n\n2. What network or data properties can lead to such assumption?\n\nResponse All our experiments are conducted on practical neural network training tasks with real datasets. From the experiments, we find that the property holds for a variety of network architectures (MLP, CNN, Inception, RNN) and different datasets (image, text, etc). We think that this can be an amenable property of over-parameterized network. \nIn fact, several recent works ([1,2]) show that the optimization trajectories of SGD is generally smooth despite the nonconvexity and depth of the networks, and our star-convexity property can be viewed as another aspect that further promotes theoretical justification to deep learning optimization. We will explore these two questions more in future work.\n\n3. There is a missing related work from the algorithmic perspective to explore the global optimization in deep learning: \nZhang et. al. CVPR'18. \"BPGrad: Towards Global Optimality in Deep Learning via Branch and Pruning\".\n\nResponse: We thank the reviewer for pointing out this interesting related work. We will cite this work in the upcoming revision. \n\n[1] Li et al. Visualizing the loss landscape of neural nets. To appear in NIPS 2018\n[2] Eliana Lorch. Visualizing deep network training trajectories with pca. In ICML Workshop on Visualization for Deep Learning, 2016.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper535/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper535/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper535/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SGD Converges to Global Minimum in Deep Learning via Star-convex Path", "abstract": "Stochastic gradient descent (SGD) has been found to be surprisingly effective in training a variety of deep neural networks. However, there is still a lack of understanding on how and why SGD can train these complex networks towards a global minimum. In this study, we establish the convergence of SGD to a global minimum for nonconvex optimization problems that are commonly encountered in neural network training. Our argument exploits the following two important properties: 1) the training loss can achieve zero value (approximately), which has been widely observed in deep learning; 2) SGD follows a star-convex path, which is verified by various experiments in this paper.  In such a context, our analysis shows that SGD, although has long been considered as a randomized algorithm, converges in an intrinsically deterministic manner to a global minimum. ", "keywords": ["SGD", "deep learning", "global minimum", "convergence"], "authorids": ["yi.zhou610@duke.edu", "baymax@mail.ustc.edu.cn", "huishuai.zhang@microsoft.com", "liang.889@osu.edu", "vahid.tarokh@duke.edu"], "authors": ["Yi Zhou", "Junjie Yang", "Huishuai Zhang", "Yingbin Liang", "Vahid Tarokh"], "pdf": "/pdf/06cea9de669d2b7ce5b2e75a513d4032fec4063f.pdf", "paperhash": "zhou|sgd_converges_to_global_minimum_in_deep_learning_via_starconvex_path", "_bibtex": "@inproceedings{\nzhou2018sgd,\ntitle={{SGD} Converges to Global Minimum in Deep Learning via Star-convex Path},\nauthor={Yi Zhou and Junjie Yang and Huishuai Zhang and Yingbin Liang and Vahid Tarokh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BylIciRcYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper535/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621617343, "tddate": null, "super": null, "final": null, "reply": {"forum": "BylIciRcYQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper535/Authors", "ICLR.cc/2019/Conference/Paper535/Reviewers", "ICLR.cc/2019/Conference/Paper535/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper535/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper535/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper535/Authors|ICLR.cc/2019/Conference/Paper535/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper535/Reviewers", "ICLR.cc/2019/Conference/Paper535/Authors", "ICLR.cc/2019/Conference/Paper535/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621617343}}}, {"id": "SJxGnBGW07", "original": null, "number": 1, "cdate": 1542690218065, "ddate": null, "tcdate": 1542690218065, "tmdate": 1542690218065, "tddate": null, "forum": "BylIciRcYQ", "replyto": "Hyg7evSKnQ", "invitation": "ICLR.cc/2019/Conference/-/Paper535/Official_Comment", "content": {"title": "Response", "comment": "We thank the reviewer for the valuable feedbacks. \n\nThis paper aims at reporting an interesting star-convex property of the SGD optimization path that has been observed in training a variety of DL models, including MLP, CNN, residual networks and RNN (verified recently). Moreover, our theory is motivated by such a common observation and attempts to justify the role of this property plays in determining the convergence of the optimization in DL. \n\nOur response to the reviewer\u2019s comments are provided as follows.\n\n1) The minimum of the cross entropy loss lies at infinity. It is a sanity to check whether the iterates goes to infinity. \n\n Response: We thank the reviewer for pointing out this, and we are aware of it. We choose to present the results on cross entropy as it is widely used in DL applications. In fact, we verified the star-convexity property for training other losses that by nature can achieve zero such as the MSE loss (please see the experiment results that we added in Fig.7 in Appendix D in supplementary).\nUnder the cross-entropy loss, we found that after the training loss is very close to zero, the l_2 norm of the corresponding iterate grows only logarithmically. This can be clearly seen from the experiments that we added in Fig.6 in Appendix D of supplementary. We further note that such a phenomenon has also been observed and justified in Fig.2 of [1]).  Thus, empirically, it is reasonable to treat the loss value to be approximately zero (i.e., reaches minimum) with a bounded weight norm.\n[1] ``The Implicit Bias of Gradient Descent on Separable Data\u2019\u2019, Soundry et al 2018\n\n2) The phenomenon may depend on the reference point, i.e., last iterate\nSince the minimum is never attained, the empirical check of the star-convexity maybe biased. More precisely, it might be possible that the behavior of the observed phenomenon depends on the reference point, i.e. the last iterate. Therefore, it will be interesting to see if the observed phenomenon still holds when varying the stopping time, for instance plot the star convexity check using the iterates at 60, 80, 100, 120 epochs as reference point. \nIn fact, the experiments shown in Figure 4 implicitly supports that the behavior may change dramatically respect to different reference point. The reason is that the loss in these experiments are far away from 0, meaning that we are far from the minimum, thus checking the star-convexity does not make sense because the star-convexity is only defined respect to the minimum. \n\nResponse: As can be seen in the experiments that we added in Fig.5 in Appendix D of the supplementary materials, we have checked the star-convex property by taking the reference point at different intermediate iterates (60, 80, 100, 120) as the reviewer suggested. We found that star-convexity still holds under these choices of reference points, and therefore such a property does not depend on the choice of reference point so long as their loss are (nearly) zero. This observation is common for over-parameterized networks which can achieve near zero loss and therefore can have common global minimum.\nWe emphasize that the reference point in our star-convexity must be the minimizer that achieves zero loss. Hence, in experiments, we must set the reference point at the epochs where the corresponding loss is nearly zero. The points at intermediate iterates with a high loss value cannot be chosen as reference point of star-convexity, because these points cannot be treated as the common global minimizer.. \nRegarding the experiments in Figure 4, they are conducted on under-parameterized networks where (approximate) zero loss cannot be achieved, and the algorithm in fact does not find a common global minimum. This experiment is to justify the role of the over-parameterization (or the common global minimum) plays in determining the star-convex optimization path.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper535/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper535/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper535/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SGD Converges to Global Minimum in Deep Learning via Star-convex Path", "abstract": "Stochastic gradient descent (SGD) has been found to be surprisingly effective in training a variety of deep neural networks. However, there is still a lack of understanding on how and why SGD can train these complex networks towards a global minimum. In this study, we establish the convergence of SGD to a global minimum for nonconvex optimization problems that are commonly encountered in neural network training. Our argument exploits the following two important properties: 1) the training loss can achieve zero value (approximately), which has been widely observed in deep learning; 2) SGD follows a star-convex path, which is verified by various experiments in this paper.  In such a context, our analysis shows that SGD, although has long been considered as a randomized algorithm, converges in an intrinsically deterministic manner to a global minimum. ", "keywords": ["SGD", "deep learning", "global minimum", "convergence"], "authorids": ["yi.zhou610@duke.edu", "baymax@mail.ustc.edu.cn", "huishuai.zhang@microsoft.com", "liang.889@osu.edu", "vahid.tarokh@duke.edu"], "authors": ["Yi Zhou", "Junjie Yang", "Huishuai Zhang", "Yingbin Liang", "Vahid Tarokh"], "pdf": "/pdf/06cea9de669d2b7ce5b2e75a513d4032fec4063f.pdf", "paperhash": "zhou|sgd_converges_to_global_minimum_in_deep_learning_via_starconvex_path", "_bibtex": "@inproceedings{\nzhou2018sgd,\ntitle={{SGD} Converges to Global Minimum in Deep Learning via Star-convex Path},\nauthor={Yi Zhou and Junjie Yang and Huishuai Zhang and Yingbin Liang and Vahid Tarokh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BylIciRcYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper535/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621617343, "tddate": null, "super": null, "final": null, "reply": {"forum": "BylIciRcYQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper535/Authors", "ICLR.cc/2019/Conference/Paper535/Reviewers", "ICLR.cc/2019/Conference/Paper535/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper535/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper535/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper535/Authors|ICLR.cc/2019/Conference/Paper535/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper535/Reviewers", "ICLR.cc/2019/Conference/Paper535/Authors", "ICLR.cc/2019/Conference/Paper535/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621617343}}}, {"id": "HyxZqj0O27", "original": null, "number": 2, "cdate": 1541102472541, "ddate": null, "tcdate": 1541102472541, "tmdate": 1541533911272, "tddate": null, "forum": "BylIciRcYQ", "replyto": "BylIciRcYQ", "invitation": "ICLR.cc/2019/Conference/-/Paper535/Official_Review", "content": {"title": "Interesting paper, but maybe less significant than it appears to be", "review": "This paper attempts to account for the success of SGD on training deep neural networks. Starting from two empirical observations: (1) deep neural networks can almost achieve zero training loss; (2) the path of iterates generated by SGD on these models follow approximately the \u201cstar convex path\u201d, under the assumptions that individual functions share a global minima with respect to which the path of iterates generated by SGD satisfies the star convexity properties, the papers shows that the iterates converges to the global minima. \n\nIn terms of clarity, I think the paper can definitely benefit if the observations/assumptions/definitions/theorems are stated in a more formal and mathematically rigorous manner. For example:\n- On page 3, \u201cfact 1\u201d: I don\u2019t think \u201cfact\u201d is the right word here. \u201cFact\u201d refers to what has been rigorously proved or verified, which is not the case for what is in the paper here. I believe \u201cobservation\u201d is more appropriate. Also the assumption that l_i is non-negative should be formally added.\n- On page 3, section 3.1: the x^* here is the last iteration produced by SGD. Then how can it be called the \u201cglobal minima\u201d? The caption of Figure 1 on page 4 is simply misleading.\n- On page 4, the statement in definition 1 is more like a theorem than a definition. It is giving readers the impression that any path generated by SGD satisfies the star-convex condition, which is not the case here. A definition should look like \u201cwe call a path generated by SGD a star-convex path if it satisfies \u2026\u201d. Definition 2 on page 6 has the similar issue.\n\nIn terms of quality, while I believe the paper is technically correct,  I have one minor question here:\nPage 3, Fact 1: How can you conclude that the set of common global minimizers are bounded? In fact I don\u2019t believe this is true at all in general. If you have a ReLu network, you can scale the parameters as described in [1], then the model is invariant. Therefore, the set of common minimizer is definitely NOT bounded. \n\nIn terms of significance, I think this paper is very interesting as it attempts to draw the connection between the aforementioned observations and the convergence properties of SGD. Unfortunately I think that this paper is less significant than it has appeared to be, although the analysis appears to be correct. \n\nFirst of all, all the analysis of this paper is based on one very important and very strong assumption, namely, all individual functions $l_i$ share at least one common global minimizer. The authors have attempted to justify this assumption by empirical evidences (figure 1). However, achieving near-zero loss is completely different from achieving exact zero because only when the model achieves exact zero can you argue that a common global minimizer exists. \n\nSecondly, the claim that the iterate converges to the global minima is based on the assumption that the path follows an \u201cepoch-wise star-convex\u201d property. From this property, it only takes simple convex analysis to reach the conclusion of theorem 1 and 2. Meanwhile, the assumption that the path does follow the \u201cepoch-wise start-convex\u201d properties is not at all informative. It is not clear why or when the path would follow such a path. Therefore theorem 1 and 2 are not more informative than simply assuming the sequence converges to a global minimizer. \n\nIn fact, it is well-known that SGD with constant stepsize converges to the unique minimizer if one assumes the loss function F is strongly convex and the variance of the stochastic gradient g_k is bounded by a multiple of the norm-square of the true gradient:\nVar(g_k) <= M ||\u2207F(x_k)||^2\nWhich is naturally satisfied if all individual functions share a common minimizer. Therefore, I don\u2019t think the results shown in the paper is that surprising or novel. \n\nWith respect to the empirical evidence, the loss function l_i is assumed to be continuously differentiable with Lipschitz continuous gradients, which is not true for networks using ReLU-like activations. Then how can the paper use models like Alexnet to justify the theory? Also, if what the authors claim is true, then the stochastic gradient would have vanishing variance as it approaches x^*. Can the authors show this empirically?\n\nIn summary, I think this paper is definitely interesting, but the significance is not as much as it would appear.\n\nRef: \n[1] Dinh, L., Pascanu, R., Bengio, S., & Bengio, Y. (2017). Sharp minima can generalize for deep nets. arXiv preprint arXiv:1703.04933.", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper535/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SGD Converges to Global Minimum in Deep Learning via Star-convex Path", "abstract": "Stochastic gradient descent (SGD) has been found to be surprisingly effective in training a variety of deep neural networks. However, there is still a lack of understanding on how and why SGD can train these complex networks towards a global minimum. In this study, we establish the convergence of SGD to a global minimum for nonconvex optimization problems that are commonly encountered in neural network training. Our argument exploits the following two important properties: 1) the training loss can achieve zero value (approximately), which has been widely observed in deep learning; 2) SGD follows a star-convex path, which is verified by various experiments in this paper.  In such a context, our analysis shows that SGD, although has long been considered as a randomized algorithm, converges in an intrinsically deterministic manner to a global minimum. ", "keywords": ["SGD", "deep learning", "global minimum", "convergence"], "authorids": ["yi.zhou610@duke.edu", "baymax@mail.ustc.edu.cn", "huishuai.zhang@microsoft.com", "liang.889@osu.edu", "vahid.tarokh@duke.edu"], "authors": ["Yi Zhou", "Junjie Yang", "Huishuai Zhang", "Yingbin Liang", "Vahid Tarokh"], "pdf": "/pdf/06cea9de669d2b7ce5b2e75a513d4032fec4063f.pdf", "paperhash": "zhou|sgd_converges_to_global_minimum_in_deep_learning_via_starconvex_path", "_bibtex": "@inproceedings{\nzhou2018sgd,\ntitle={{SGD} Converges to Global Minimum in Deep Learning via Star-convex Path},\nauthor={Yi Zhou and Junjie Yang and Huishuai Zhang and Yingbin Liang and Vahid Tarokh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BylIciRcYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper535/Official_Review", "cdate": 1542234439133, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BylIciRcYQ", "replyto": "BylIciRcYQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper535/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335746392, "tmdate": 1552335746392, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper535/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 10}