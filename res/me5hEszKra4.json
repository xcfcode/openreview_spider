{"notes": [{"id": "me5hEszKra4", "original": "jKhYMOtlSaK", "number": 1009, "cdate": 1601308114228, "ddate": null, "tcdate": 1601308114228, "tmdate": 1614985664785, "tddate": null, "forum": "me5hEszKra4", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "ResPerfNet: Deep Residual Learning for Regressional Performance Modeling of Deep Neural Networks", "authorids": ["chuan-chi.wang@adlinktech.com", "~Ying-Chiao_Liao1", "chiaheng@ncku.edu.tw", "fencer.kao@adlinktech.com", "william.liang@adlinktech.com", "~Shih-Hao_Hung1"], "authors": ["Chuan-Chi Wang", "Ying-Chiao Liao", "Chia-Heng Tu", "Ming-Chang Kao", "Wen-Yew Liang", "Shih-Hao Hung"], "keywords": [], "abstract": "The rapid advancements of computing technology facilitate the development of diverse deep learning applications. Unfortunately, the efficiency of parallel computing infrastructures varies widely with neural network models, which hinders the exploration of the design space to find high-performance neural network architectures on specific computing platforms for a given application. To address such a challenge, we propose a deep learning-based method, ResPerfNet, which trains a residual neural network with representative datasets obtained on the target platform to predict the performance for a deep neural network. Our experimental results show that ResPerfNet can accurately predict the execution time of individual neural network layers and full network models on a variety of platforms. In particular, ResPerfNet achieves 8.4% of mean absolute percentage error for LeNet, AlexNet and VGG16 on the NVIDIA GTX 1080Ti, which is substantially lower than the previously published works.", "pdf": "/pdf/2b503c50f8aca8a4cd1b9674b1a866713156b829.pdf", "supplementary_material": "/attachment/afe2f1cd6aa7e007ae3eb6acccb5da3d0010cbcd.zip", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wang|resperfnet_deep_residual_learning_for_regressional_performance_modeling_of_deep_neural_networks", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=iNwbnw1fKf", "_bibtex": "@misc{\nwang2021resperfnet,\ntitle={ResPerfNet: Deep Residual Learning for Regressional Performance Modeling of Deep Neural Networks},\nauthor={Chuan-Chi Wang and Ying-Chiao Liao and Chia-Heng Tu and Ming-Chang Kao and Wen-Yew Liang and Shih-Hao Hung},\nyear={2021},\nurl={https://openreview.net/forum?id=me5hEszKra4}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "F4WCY0Kmoig", "original": null, "number": 1, "cdate": 1610040496624, "ddate": null, "tcdate": 1610040496624, "tmdate": 1610474102992, "tddate": null, "forum": "me5hEszKra4", "replyto": "me5hEszKra4", "invitation": "ICLR.cc/2021/Conference/Paper1009/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "This paper presents a new method to predict the performance of deep neural networks. It evaluates the method on three different networks: LeNet, AlexNet, and VGG16 under two different frameworks, TensorFlow and TensorRT.\n\nReviewer 2 thought that the results were promising but comparison with other approaches was weak (PerfNet being the only baseline). They also asked for motivation for the selected architecture as well as raised a number of points for clarification. R2 was also concerned that the single baseline appeared to have not yet been published. The authors clarified this in their response (it was published in ACM RACS, obtaining results directly from those authors).\n\nReviewer 1 said that the experiments were extensive, but did not find the approach novel (\u201ca normal application of ResNet\u201d). They suggested NAS as a motivating application rather than stopping at predicting execution time. The authors agreed with the importance of predicting execution time in NAS.\n\nReviewer 3 agreed with Reviewer 1\u2019s assessment of lacking novelty and technical contribution. They also pointed towards NAS, where many methods are already using neural networks to predict execution time. They were also disappointed by the reduced set of architectural elements considered. The authors responded to R3\u2019s comments, but R3 was still not convinced of novelty.\n\nThis looks like a fairly straightforward rejection on the basis of not enough technical merit. The authors are encouraged to explore their approach in the context of NAS as per R1\u2019s suggestion."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ResPerfNet: Deep Residual Learning for Regressional Performance Modeling of Deep Neural Networks", "authorids": ["chuan-chi.wang@adlinktech.com", "~Ying-Chiao_Liao1", "chiaheng@ncku.edu.tw", "fencer.kao@adlinktech.com", "william.liang@adlinktech.com", "~Shih-Hao_Hung1"], "authors": ["Chuan-Chi Wang", "Ying-Chiao Liao", "Chia-Heng Tu", "Ming-Chang Kao", "Wen-Yew Liang", "Shih-Hao Hung"], "keywords": [], "abstract": "The rapid advancements of computing technology facilitate the development of diverse deep learning applications. Unfortunately, the efficiency of parallel computing infrastructures varies widely with neural network models, which hinders the exploration of the design space to find high-performance neural network architectures on specific computing platforms for a given application. To address such a challenge, we propose a deep learning-based method, ResPerfNet, which trains a residual neural network with representative datasets obtained on the target platform to predict the performance for a deep neural network. Our experimental results show that ResPerfNet can accurately predict the execution time of individual neural network layers and full network models on a variety of platforms. In particular, ResPerfNet achieves 8.4% of mean absolute percentage error for LeNet, AlexNet and VGG16 on the NVIDIA GTX 1080Ti, which is substantially lower than the previously published works.", "pdf": "/pdf/2b503c50f8aca8a4cd1b9674b1a866713156b829.pdf", "supplementary_material": "/attachment/afe2f1cd6aa7e007ae3eb6acccb5da3d0010cbcd.zip", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wang|resperfnet_deep_residual_learning_for_regressional_performance_modeling_of_deep_neural_networks", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=iNwbnw1fKf", "_bibtex": "@misc{\nwang2021resperfnet,\ntitle={ResPerfNet: Deep Residual Learning for Regressional Performance Modeling of Deep Neural Networks},\nauthor={Chuan-Chi Wang and Ying-Chiao Liao and Chia-Heng Tu and Ming-Chang Kao and Wen-Yew Liang and Shih-Hao Hung},\nyear={2021},\nurl={https://openreview.net/forum?id=me5hEszKra4}\n}"}, "tags": [], "invitation": {"reply": {"forum": "me5hEszKra4", "replyto": "me5hEszKra4", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040496609, "tmdate": 1610474102976, "id": "ICLR.cc/2021/Conference/Paper1009/-/Decision"}}}, {"id": "itaGGgxaB5s", "original": null, "number": 5, "cdate": 1605514662585, "ddate": null, "tcdate": 1605514662585, "tmdate": 1606200268867, "tddate": null, "forum": "me5hEszKra4", "replyto": "1inVLKpCa68", "invitation": "ICLR.cc/2021/Conference/Paper1009/-/Official_Comment", "content": {"title": "Thanks", "comment": "Thanks for the clarifications. It's good that you relate your work to the RACS paper. However, since it wasn't published yet I didn't want to ask for it because that might have compromised the reviewer anonymity. "}, "signatures": ["ICLR.cc/2021/Conference/Paper1009/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1009/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ResPerfNet: Deep Residual Learning for Regressional Performance Modeling of Deep Neural Networks", "authorids": ["chuan-chi.wang@adlinktech.com", "~Ying-Chiao_Liao1", "chiaheng@ncku.edu.tw", "fencer.kao@adlinktech.com", "william.liang@adlinktech.com", "~Shih-Hao_Hung1"], "authors": ["Chuan-Chi Wang", "Ying-Chiao Liao", "Chia-Heng Tu", "Ming-Chang Kao", "Wen-Yew Liang", "Shih-Hao Hung"], "keywords": [], "abstract": "The rapid advancements of computing technology facilitate the development of diverse deep learning applications. Unfortunately, the efficiency of parallel computing infrastructures varies widely with neural network models, which hinders the exploration of the design space to find high-performance neural network architectures on specific computing platforms for a given application. To address such a challenge, we propose a deep learning-based method, ResPerfNet, which trains a residual neural network with representative datasets obtained on the target platform to predict the performance for a deep neural network. Our experimental results show that ResPerfNet can accurately predict the execution time of individual neural network layers and full network models on a variety of platforms. In particular, ResPerfNet achieves 8.4% of mean absolute percentage error for LeNet, AlexNet and VGG16 on the NVIDIA GTX 1080Ti, which is substantially lower than the previously published works.", "pdf": "/pdf/2b503c50f8aca8a4cd1b9674b1a866713156b829.pdf", "supplementary_material": "/attachment/afe2f1cd6aa7e007ae3eb6acccb5da3d0010cbcd.zip", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wang|resperfnet_deep_residual_learning_for_regressional_performance_modeling_of_deep_neural_networks", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=iNwbnw1fKf", "_bibtex": "@misc{\nwang2021resperfnet,\ntitle={ResPerfNet: Deep Residual Learning for Regressional Performance Modeling of Deep Neural Networks},\nauthor={Chuan-Chi Wang and Ying-Chiao Liao and Chia-Heng Tu and Ming-Chang Kao and Wen-Yew Liang and Shih-Hao Hung},\nyear={2021},\nurl={https://openreview.net/forum?id=me5hEszKra4}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "me5hEszKra4", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1009/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1009/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1009/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1009/Authors|ICLR.cc/2021/Conference/Paper1009/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1009/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923864724, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1009/-/Official_Comment"}}}, {"id": "su1b6BdIypq", "original": null, "number": 8, "cdate": 1606186171651, "ddate": null, "tcdate": 1606186171651, "tmdate": 1606186171651, "tddate": null, "forum": "me5hEszKra4", "replyto": "61jxsvmA14", "invitation": "ICLR.cc/2021/Conference/Paper1009/-/Official_Comment", "content": {"title": "Response to Reviewer #3 ", "comment": "A: Thank you for giving us an opportunity to emphasize the novelty of our work. \nWe aim to tackle the real-world problem: which hardware platform best suits the given DL model, i.e., delivering the best performance, in terms of the model inference time. Nevertheless, we find that it is difficult to achieve the goal with the prior work efficiently. \n\nAs for [1], the averaged scaling parameters for computation and communication are required to give an estimate of the FLOPs delivered by the hardware platform for the target DL model. However, it is hard to anticipate the per-model based averaged scaling parameters on the hardware platform, which highly depends on the software/hardware combination and is hard to obtain such data without physically executing the target DL model on the hardware platform. \n\nFor [2], their proposed method overlooks the communication performance within a given DL model. This limits the applicability of their proposed method, which makes it hard to estimate the inference time when the DL model involves intensive CPU and GPU interactions.\n\nFor [3], their work only predicts fixed layer numbers (at most 25 layers) of DL model performance, fixed batch size, and can not predict the spreading out network case, e,g. inceptionV3, UNet, which do not satisfy the real scenario.\n\nTo the best of our knowledge, the prior work does not provide a viable solution that helps systematically estimate the inference time of a sophisticated DL model on the modern computing hardware, such as GTX 1080 Ti and P1000 GPUs. \nCompared with the above works, together with the three-phase performance modeling approach and the proposed residual regression network, ResPerfNet is able to provide accurate estimates of the representative DL models, such as AlexNet and VGG16. Our latest results show that ResPerfNet delivers similar performance on the InceptionV3 model, which is known to present good results on the ImageNet dataset. Last but not least, we believe that ResPerfNet is practical work and we will open source our efforts to the public, in order to help facilitate the development of DL computing systems.\n\n[1] Hang Qi, Evan R. Sparks, and Ameet Talwalkar. Paleo: A Performance Model for Deep Neural Networks. In International Conference on Learning Representations, 2017.\n[2] Daniel Justus, John Brennan, Stephen Bonner, and Andrew Stephen McGough. Predicting the Com-putational Cost of Deep Learning Models.  In International Conference on Big Data, 2018.\n[3] Han Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhang, and Song Han. Once for all: Train one network and specialize it for efficient deployment. In International Conference on Learning Representations, 2020.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1009/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1009/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ResPerfNet: Deep Residual Learning for Regressional Performance Modeling of Deep Neural Networks", "authorids": ["chuan-chi.wang@adlinktech.com", "~Ying-Chiao_Liao1", "chiaheng@ncku.edu.tw", "fencer.kao@adlinktech.com", "william.liang@adlinktech.com", "~Shih-Hao_Hung1"], "authors": ["Chuan-Chi Wang", "Ying-Chiao Liao", "Chia-Heng Tu", "Ming-Chang Kao", "Wen-Yew Liang", "Shih-Hao Hung"], "keywords": [], "abstract": "The rapid advancements of computing technology facilitate the development of diverse deep learning applications. Unfortunately, the efficiency of parallel computing infrastructures varies widely with neural network models, which hinders the exploration of the design space to find high-performance neural network architectures on specific computing platforms for a given application. To address such a challenge, we propose a deep learning-based method, ResPerfNet, which trains a residual neural network with representative datasets obtained on the target platform to predict the performance for a deep neural network. Our experimental results show that ResPerfNet can accurately predict the execution time of individual neural network layers and full network models on a variety of platforms. In particular, ResPerfNet achieves 8.4% of mean absolute percentage error for LeNet, AlexNet and VGG16 on the NVIDIA GTX 1080Ti, which is substantially lower than the previously published works.", "pdf": "/pdf/2b503c50f8aca8a4cd1b9674b1a866713156b829.pdf", "supplementary_material": "/attachment/afe2f1cd6aa7e007ae3eb6acccb5da3d0010cbcd.zip", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wang|resperfnet_deep_residual_learning_for_regressional_performance_modeling_of_deep_neural_networks", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=iNwbnw1fKf", "_bibtex": "@misc{\nwang2021resperfnet,\ntitle={ResPerfNet: Deep Residual Learning for Regressional Performance Modeling of Deep Neural Networks},\nauthor={Chuan-Chi Wang and Ying-Chiao Liao and Chia-Heng Tu and Ming-Chang Kao and Wen-Yew Liang and Shih-Hao Hung},\nyear={2021},\nurl={https://openreview.net/forum?id=me5hEszKra4}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "me5hEszKra4", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1009/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1009/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1009/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1009/Authors|ICLR.cc/2021/Conference/Paper1009/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1009/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923864724, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1009/-/Official_Comment"}}}, {"id": "61jxsvmA14", "original": null, "number": 6, "cdate": 1605664748634, "ddate": null, "tcdate": 1605664748634, "tmdate": 1605664748634, "tddate": null, "forum": "me5hEszKra4", "replyto": "YYuhncs9w0W", "invitation": "ICLR.cc/2021/Conference/Paper1009/-/Official_Comment", "content": {"title": "Thank you for your reply.", "comment": "Thank you for your reply.\nI still think using a DL-method to estimate the inference time is a bit trivial. Can you list your technical novelty?"}, "signatures": ["ICLR.cc/2021/Conference/Paper1009/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1009/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ResPerfNet: Deep Residual Learning for Regressional Performance Modeling of Deep Neural Networks", "authorids": ["chuan-chi.wang@adlinktech.com", "~Ying-Chiao_Liao1", "chiaheng@ncku.edu.tw", "fencer.kao@adlinktech.com", "william.liang@adlinktech.com", "~Shih-Hao_Hung1"], "authors": ["Chuan-Chi Wang", "Ying-Chiao Liao", "Chia-Heng Tu", "Ming-Chang Kao", "Wen-Yew Liang", "Shih-Hao Hung"], "keywords": [], "abstract": "The rapid advancements of computing technology facilitate the development of diverse deep learning applications. Unfortunately, the efficiency of parallel computing infrastructures varies widely with neural network models, which hinders the exploration of the design space to find high-performance neural network architectures on specific computing platforms for a given application. To address such a challenge, we propose a deep learning-based method, ResPerfNet, which trains a residual neural network with representative datasets obtained on the target platform to predict the performance for a deep neural network. Our experimental results show that ResPerfNet can accurately predict the execution time of individual neural network layers and full network models on a variety of platforms. In particular, ResPerfNet achieves 8.4% of mean absolute percentage error for LeNet, AlexNet and VGG16 on the NVIDIA GTX 1080Ti, which is substantially lower than the previously published works.", "pdf": "/pdf/2b503c50f8aca8a4cd1b9674b1a866713156b829.pdf", "supplementary_material": "/attachment/afe2f1cd6aa7e007ae3eb6acccb5da3d0010cbcd.zip", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wang|resperfnet_deep_residual_learning_for_regressional_performance_modeling_of_deep_neural_networks", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=iNwbnw1fKf", "_bibtex": "@misc{\nwang2021resperfnet,\ntitle={ResPerfNet: Deep Residual Learning for Regressional Performance Modeling of Deep Neural Networks},\nauthor={Chuan-Chi Wang and Ying-Chiao Liao and Chia-Heng Tu and Ming-Chang Kao and Wen-Yew Liang and Shih-Hao Hung},\nyear={2021},\nurl={https://openreview.net/forum?id=me5hEszKra4}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "me5hEszKra4", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1009/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1009/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1009/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1009/Authors|ICLR.cc/2021/Conference/Paper1009/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1009/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923864724, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1009/-/Official_Comment"}}}, {"id": "YYuhncs9w0W", "original": null, "number": 4, "cdate": 1605510404218, "ddate": null, "tcdate": 1605510404218, "tmdate": 1605510561362, "tddate": null, "forum": "me5hEszKra4", "replyto": "y5iurqPbA82", "invitation": "ICLR.cc/2021/Conference/Paper1009/-/Official_Comment", "content": {"title": "Response to Reviewer #3", "comment": "Q1: The main problem is the lack of Novelty and technical contributions. Using a DL-based model to predict the performance is not novel. Many NAS methods using the same trick to estimate the performance of one architecture ahead of running it directly on the hardware.\n\nA1: Thank you for your insightful comments. Yes, the execution time estimation is an important part of NAS. We have done some experiments to show that our work is capable of handling more sophisticated network architectures that are useful of NAS for real applications that require complex models. The existing NAS works have some presumptions/limitations and would not be appropriate for estimating the execution time of a sophisticated model, which is developed with the specific software framework for the hardware platform, e.g., TensorRT for the NVIDIA GPU. For example, the predictor presented in [1] is responsible for identifying a good sub-network under a given trained network and is hard to give an accurate estimate of the execution time of the sophisticated model running on CPU and GPU collaboratively. In addition, in [2], FLOPs is used as one of the metrics for evaluating the neural network model. Nevertheless, it is known that FLOPS is not suitable for estimating the model inference time when the involved operations have non-linear relationships between the number of operations and the incurred execution time. Therefore, regarding the model inference time, it is desired to accurately estimate the model time through a systematic approach, i.e., ResPerfNet.\n\nQ2: Using a DL-base regression is better than normal regression when the sample size is large. The experiment results are not surprising.\n\nA2: We totally agree with your comment on the DL-based and normal regression approaches. However, based on our experiments in this work, we find that the normal regression method, i.e., XGboost, delivers relatively poor performance (MAPE of 29%) when using 80,000 samples to train the XGboost. On the other hand, ResPerfNet requires only 5,000 samples to yield the better MAPE result of 18% for estimating the execution time for a convolutional layer. At the same time, when using 5,000 samples to train XGboost, it produces a high MAPE value of 53%. Hence, in our configuration, it appears that the DL-based approach has the advantage to learn the relationships between the features and the execution time, compared with XGboost.\n\nQ3: Box-cox transformation is a common technique in regression. That is not new.\n\nA3: Thank you for your comment. Our intention is to share our experiences to build the DL-based model predictor among many other design alternatives to construct the predictor, and our experiments show that box-cox transformation outperforms the other transformation schemes when the box-cox transformation is adopted in ResPerfNet.\n\nQ4: Need to prepare the dataset using a lot of samples, i.e. 100000, which is computationally heavy. Require huge storage space to keep the samples for even one platform. Thus the whole method is not efficient. It is hard to generalize it to other hardware/platform\n\nA4: Thank you for your valuable comments. Based on our experiments, 5,000 samples are sufficient to provide a good estimate (MAPE of 18%), which requires 200MB of storage space. 100,000 samples (only 4GB) can further improve the prediction performance (MAPE of 11.78%). \nOn the other hand, we use 5TB to keep the TensorRT model variants, apart from the sample data, in order to save the time required for model conversion and optimization to generate the variants needed to train the predictor for other accelerators. In the future, we plan to leverage transfer learning techniques to eliminate the huge space requirement for keeping the variants while our model is able to predict the execution time for different hardware devices.\n\nQ5: Besides, the method only considers some common operations such as conv, pooling, and FC-layer. However, more operations should be considered for example ROI Pooling, NMS, Spatial-to-depth. Those operations are also commonly used in tasks such as detection and SR.\n\nA5: Thanks for the valuable comments. While we are aware that there are still some common operations, our current work focuses on the essential operations forming the well-known models, such as LeNet, AlexNet, and VGG16 so as to validate the effectiveness of ResPerfNet. We believe that with the corresponding sample data, ResPerfNet is able to characterize the above mentioned operations. \n\n\n[1] Han Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhang, and Song Han. Once for all: Train one network and specialize it for efficient deployment. In International Conference on Learning Representations, 2020.\n\n[2] Xiaoliang Dai, Alvin Wan, Peizhao Zhang, Bichen Wu, Zijian He, Zhen Wei, Kan Chen, Yuandong Tian, Matthew Yu, Peter Vajda, Joseph E. Gonzalez. \u201cFBNetV3: Joint Architecture-Recipe Search using Neural Acquisition Function\u201d in arXiv:2006.02049, 2020.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1009/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1009/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ResPerfNet: Deep Residual Learning for Regressional Performance Modeling of Deep Neural Networks", "authorids": ["chuan-chi.wang@adlinktech.com", "~Ying-Chiao_Liao1", "chiaheng@ncku.edu.tw", "fencer.kao@adlinktech.com", "william.liang@adlinktech.com", "~Shih-Hao_Hung1"], "authors": ["Chuan-Chi Wang", "Ying-Chiao Liao", "Chia-Heng Tu", "Ming-Chang Kao", "Wen-Yew Liang", "Shih-Hao Hung"], "keywords": [], "abstract": "The rapid advancements of computing technology facilitate the development of diverse deep learning applications. Unfortunately, the efficiency of parallel computing infrastructures varies widely with neural network models, which hinders the exploration of the design space to find high-performance neural network architectures on specific computing platforms for a given application. To address such a challenge, we propose a deep learning-based method, ResPerfNet, which trains a residual neural network with representative datasets obtained on the target platform to predict the performance for a deep neural network. Our experimental results show that ResPerfNet can accurately predict the execution time of individual neural network layers and full network models on a variety of platforms. In particular, ResPerfNet achieves 8.4% of mean absolute percentage error for LeNet, AlexNet and VGG16 on the NVIDIA GTX 1080Ti, which is substantially lower than the previously published works.", "pdf": "/pdf/2b503c50f8aca8a4cd1b9674b1a866713156b829.pdf", "supplementary_material": "/attachment/afe2f1cd6aa7e007ae3eb6acccb5da3d0010cbcd.zip", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wang|resperfnet_deep_residual_learning_for_regressional_performance_modeling_of_deep_neural_networks", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=iNwbnw1fKf", "_bibtex": "@misc{\nwang2021resperfnet,\ntitle={ResPerfNet: Deep Residual Learning for Regressional Performance Modeling of Deep Neural Networks},\nauthor={Chuan-Chi Wang and Ying-Chiao Liao and Chia-Heng Tu and Ming-Chang Kao and Wen-Yew Liang and Shih-Hao Hung},\nyear={2021},\nurl={https://openreview.net/forum?id=me5hEszKra4}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "me5hEszKra4", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1009/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1009/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1009/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1009/Authors|ICLR.cc/2021/Conference/Paper1009/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1009/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923864724, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1009/-/Official_Comment"}}}, {"id": "nisAmAgHQMr", "original": null, "number": 3, "cdate": 1605510231362, "ddate": null, "tcdate": 1605510231362, "tmdate": 1605510524770, "tddate": null, "forum": "me5hEszKra4", "replyto": "eSE5YlO8nOL", "invitation": "ICLR.cc/2021/Conference/Paper1009/-/Official_Comment", "content": {"title": "Response to Reviewer #1", "comment": "Thank you for your insightful comments. Yes, the execution time estimation is an important part of NAS. We have done some experiments to show that our work is capable of handling more sophisticated network architectures that are useful of NAS for real applications that require complex models. The existing NAS works have some presumptions/limitations and would not be appropriate for estimating the execution time of a sophisticated model, which is developed with the specific software framework for the hardware platform, e.g., TensorRT for the NVIDIA GPU. For example, the predictor presented in [1] is responsible for identifying a good sub-network under a given trained network and is hard to give an accurate estimate of the execution time of the sophisticated model running on CPU and GPU collaboratively. In addition, in [2], FLOPs is used as one of the metrics for evaluating the neural network model. Nevertheless, it is known that FLOPS is not suitable for estimating the model inference time when the involved operations have non-linear relationships between the number of operations and the incurred execution time. Therefore, regarding the model inference time, it is desired to accurately estimate the model time through a systematic approach, i.e., ResPerfNet.\n\n[1] Han Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhang, and Song Han. Once for all: Train one network and specialize it for efficient deployment. In International Conference on Learning Representations, 2020.\n\n[2] Xiaoliang Dai, Alvin Wan, Peizhao Zhang, Bichen Wu, Zijian He, Zhen Wei, Kan Chen, Yuandong Tian, Matthew Yu, Peter Vajda, Joseph E. Gonzalez. \u201cFBNetV3: Joint Architecture-Recipe Search using Neural Acquisition Function\u201d in arXiv:2006.02049, 2020.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1009/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1009/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ResPerfNet: Deep Residual Learning for Regressional Performance Modeling of Deep Neural Networks", "authorids": ["chuan-chi.wang@adlinktech.com", "~Ying-Chiao_Liao1", "chiaheng@ncku.edu.tw", "fencer.kao@adlinktech.com", "william.liang@adlinktech.com", "~Shih-Hao_Hung1"], "authors": ["Chuan-Chi Wang", "Ying-Chiao Liao", "Chia-Heng Tu", "Ming-Chang Kao", "Wen-Yew Liang", "Shih-Hao Hung"], "keywords": [], "abstract": "The rapid advancements of computing technology facilitate the development of diverse deep learning applications. Unfortunately, the efficiency of parallel computing infrastructures varies widely with neural network models, which hinders the exploration of the design space to find high-performance neural network architectures on specific computing platforms for a given application. To address such a challenge, we propose a deep learning-based method, ResPerfNet, which trains a residual neural network with representative datasets obtained on the target platform to predict the performance for a deep neural network. Our experimental results show that ResPerfNet can accurately predict the execution time of individual neural network layers and full network models on a variety of platforms. In particular, ResPerfNet achieves 8.4% of mean absolute percentage error for LeNet, AlexNet and VGG16 on the NVIDIA GTX 1080Ti, which is substantially lower than the previously published works.", "pdf": "/pdf/2b503c50f8aca8a4cd1b9674b1a866713156b829.pdf", "supplementary_material": "/attachment/afe2f1cd6aa7e007ae3eb6acccb5da3d0010cbcd.zip", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wang|resperfnet_deep_residual_learning_for_regressional_performance_modeling_of_deep_neural_networks", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=iNwbnw1fKf", "_bibtex": "@misc{\nwang2021resperfnet,\ntitle={ResPerfNet: Deep Residual Learning for Regressional Performance Modeling of Deep Neural Networks},\nauthor={Chuan-Chi Wang and Ying-Chiao Liao and Chia-Heng Tu and Ming-Chang Kao and Wen-Yew Liang and Shih-Hao Hung},\nyear={2021},\nurl={https://openreview.net/forum?id=me5hEszKra4}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "me5hEszKra4", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1009/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1009/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1009/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1009/Authors|ICLR.cc/2021/Conference/Paper1009/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1009/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923864724, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1009/-/Official_Comment"}}}, {"id": "1inVLKpCa68", "original": null, "number": 2, "cdate": 1605510169041, "ddate": null, "tcdate": 1605510169041, "tmdate": 1605510496432, "tddate": null, "forum": "me5hEszKra4", "replyto": "3oPQdCd5N2H", "invitation": "ICLR.cc/2021/Conference/Paper1009/-/Official_Comment", "content": {"title": "Response to Reviewer #2", "comment": "Thank you for the valuable comments. We have fixed the equations and contents in the paper according to the comments. The paper will be updated later.\n\nQ: I'm a bit surprised that the dropout layer is very close to the end of the network. Why? Why 0.2 dropout (and not 0.1 or 0.4)?\n\nA: While a dropout layer could be added after a hidden layer, in our study, the dropout layer is mainly used for better and stable prediction results. That is, in some cases, the logarithmic operations in MAPLE will produce erroneous results when unexpected inputs are given, and adding the dropout layer near the end of the network can prevent such a case from happening.\nThe dropout ratio of 0.2 delivers the best accuracy (i.e., dropout ratio of 0.2 for MAPE of 11.78%) in our experiments, whereas the ratio of 0.1 leads to MAPE of 11.98% and 0.4 for 12.33%. The ratio can be adjusted accordingly based on the given inputs.\n\nQ: It is disturbing that one of the main references [Wang 2020] can't be found, despite extensive searching. This is problematic since the only other solution ResPerfNet is compared to is PerfNet, which is published in [Wang 2020]. This limits the possibility to compare this work with previous work. The conference where the [Wang 2020] paper was published took place in mid October 2020, which is after the deadline for ICLR.\n\nA: We are sorry for causing the confusion. As [Wang 2020] is published in ACM RACS 2020, which takes place virtually this year, we obtain the results of [Wang 2020] as soon as the preliminary program of ACM RACS 2020 is released online by asking for their paper and research data offline. We find that the work done by [Wang 2020] is very close to this work, and we finally decide to compare with their work in order to present more comprehensive results on the performance estimations of a neural network model and to show the effectiveness of ResPerfNet.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1009/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1009/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ResPerfNet: Deep Residual Learning for Regressional Performance Modeling of Deep Neural Networks", "authorids": ["chuan-chi.wang@adlinktech.com", "~Ying-Chiao_Liao1", "chiaheng@ncku.edu.tw", "fencer.kao@adlinktech.com", "william.liang@adlinktech.com", "~Shih-Hao_Hung1"], "authors": ["Chuan-Chi Wang", "Ying-Chiao Liao", "Chia-Heng Tu", "Ming-Chang Kao", "Wen-Yew Liang", "Shih-Hao Hung"], "keywords": [], "abstract": "The rapid advancements of computing technology facilitate the development of diverse deep learning applications. Unfortunately, the efficiency of parallel computing infrastructures varies widely with neural network models, which hinders the exploration of the design space to find high-performance neural network architectures on specific computing platforms for a given application. To address such a challenge, we propose a deep learning-based method, ResPerfNet, which trains a residual neural network with representative datasets obtained on the target platform to predict the performance for a deep neural network. Our experimental results show that ResPerfNet can accurately predict the execution time of individual neural network layers and full network models on a variety of platforms. In particular, ResPerfNet achieves 8.4% of mean absolute percentage error for LeNet, AlexNet and VGG16 on the NVIDIA GTX 1080Ti, which is substantially lower than the previously published works.", "pdf": "/pdf/2b503c50f8aca8a4cd1b9674b1a866713156b829.pdf", "supplementary_material": "/attachment/afe2f1cd6aa7e007ae3eb6acccb5da3d0010cbcd.zip", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wang|resperfnet_deep_residual_learning_for_regressional_performance_modeling_of_deep_neural_networks", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=iNwbnw1fKf", "_bibtex": "@misc{\nwang2021resperfnet,\ntitle={ResPerfNet: Deep Residual Learning for Regressional Performance Modeling of Deep Neural Networks},\nauthor={Chuan-Chi Wang and Ying-Chiao Liao and Chia-Heng Tu and Ming-Chang Kao and Wen-Yew Liang and Shih-Hao Hung},\nyear={2021},\nurl={https://openreview.net/forum?id=me5hEszKra4}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "me5hEszKra4", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1009/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1009/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1009/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1009/Authors|ICLR.cc/2021/Conference/Paper1009/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1009/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923864724, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1009/-/Official_Comment"}}}, {"id": "y5iurqPbA82", "original": null, "number": 1, "cdate": 1602989723491, "ddate": null, "tcdate": 1602989723491, "tmdate": 1605024552720, "tddate": null, "forum": "me5hEszKra4", "replyto": "me5hEszKra4", "invitation": "ICLR.cc/2021/Conference/Paper1009/-/Official_Review", "content": {"title": "Lack of Noverty and technical contributions", "review": "Topic\n\nUsing a residual-based network to the performance of another DL-based network.\n\nContributions:\n\nUsing a residual-based network for estimating the computing performance of DL applications on a variety of models-framework-accelerator configurations, which enables the users to explore the hardware/software design space.\nUsing three-phase performance modeling to estimate computation time.\n\nWeakness\n\n1, The main problem is the lack of Novelty and technical contributions. Using a DL-based model to predict the performance is not novel. Many NAS methods using the same trick to estimate the performance of one architecture ahead of running it directly on the hardware.\n\n2, Using a DL-base regression is better than normal regression when the sample size is large. The experiment results are not surprising.\n\n3, Box-cox transformation is a common technique in regression. That is not new.\n\n4, Need to prepare the dataset using a lot of samples, i.e. 100000, which is computationally heavy. \nRequire huge storage space to keep the samples for even one platform.\nThus the whole method is not efficient. It is hard to generalize it to other hardware/platform.\n\n5, Besides, the method only considers some common operations such as conv, pooling, and FC-layer. However, more operations should be considered for example ROI Pooling, NMS, Spatial-to-depth. Those operations are also commonly used in tasks such as detection and SR. \n\n", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1009/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1009/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ResPerfNet: Deep Residual Learning for Regressional Performance Modeling of Deep Neural Networks", "authorids": ["chuan-chi.wang@adlinktech.com", "~Ying-Chiao_Liao1", "chiaheng@ncku.edu.tw", "fencer.kao@adlinktech.com", "william.liang@adlinktech.com", "~Shih-Hao_Hung1"], "authors": ["Chuan-Chi Wang", "Ying-Chiao Liao", "Chia-Heng Tu", "Ming-Chang Kao", "Wen-Yew Liang", "Shih-Hao Hung"], "keywords": [], "abstract": "The rapid advancements of computing technology facilitate the development of diverse deep learning applications. Unfortunately, the efficiency of parallel computing infrastructures varies widely with neural network models, which hinders the exploration of the design space to find high-performance neural network architectures on specific computing platforms for a given application. To address such a challenge, we propose a deep learning-based method, ResPerfNet, which trains a residual neural network with representative datasets obtained on the target platform to predict the performance for a deep neural network. Our experimental results show that ResPerfNet can accurately predict the execution time of individual neural network layers and full network models on a variety of platforms. In particular, ResPerfNet achieves 8.4% of mean absolute percentage error for LeNet, AlexNet and VGG16 on the NVIDIA GTX 1080Ti, which is substantially lower than the previously published works.", "pdf": "/pdf/2b503c50f8aca8a4cd1b9674b1a866713156b829.pdf", "supplementary_material": "/attachment/afe2f1cd6aa7e007ae3eb6acccb5da3d0010cbcd.zip", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wang|resperfnet_deep_residual_learning_for_regressional_performance_modeling_of_deep_neural_networks", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=iNwbnw1fKf", "_bibtex": "@misc{\nwang2021resperfnet,\ntitle={ResPerfNet: Deep Residual Learning for Regressional Performance Modeling of Deep Neural Networks},\nauthor={Chuan-Chi Wang and Ying-Chiao Liao and Chia-Heng Tu and Ming-Chang Kao and Wen-Yew Liang and Shih-Hao Hung},\nyear={2021},\nurl={https://openreview.net/forum?id=me5hEszKra4}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "me5hEszKra4", "replyto": "me5hEszKra4", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1009/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538129328, "tmdate": 1606915799228, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1009/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1009/-/Official_Review"}}}, {"id": "eSE5YlO8nOL", "original": null, "number": 2, "cdate": 1603422851986, "ddate": null, "tcdate": 1603422851986, "tmdate": 1605024552656, "tddate": null, "forum": "me5hEszKra4", "replyto": "me5hEszKra4", "invitation": "ICLR.cc/2021/Conference/Paper1009/-/Official_Review", "content": {"title": "Reviews", "review": "Summary: The authors design a specific ResNet for predicting the model execution time on different platforms.\n\nPros\n- conduct extensive experiments, particularly collect a large scale dataset for measuring different architectures, which can be helpful for further works if it can be released publicly\n\nCons\n- The idea is not novel. The main idea is to utilize a ResNet to perform regression on network latency data, which can only be considered as a normal application of ResNet. \n- The motivation is questionable. In my opinion, making model execution time prediction more accurate should not be the ultimate end. The proposed ResPerfNet should be applied in network evaluation and search stages in Neural Architecture Search (NAS) area, and validate that a more accurate model performance predictor is helpful for architecture search. But I didn't see any supporting experimental results in this paper.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1009/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1009/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ResPerfNet: Deep Residual Learning for Regressional Performance Modeling of Deep Neural Networks", "authorids": ["chuan-chi.wang@adlinktech.com", "~Ying-Chiao_Liao1", "chiaheng@ncku.edu.tw", "fencer.kao@adlinktech.com", "william.liang@adlinktech.com", "~Shih-Hao_Hung1"], "authors": ["Chuan-Chi Wang", "Ying-Chiao Liao", "Chia-Heng Tu", "Ming-Chang Kao", "Wen-Yew Liang", "Shih-Hao Hung"], "keywords": [], "abstract": "The rapid advancements of computing technology facilitate the development of diverse deep learning applications. Unfortunately, the efficiency of parallel computing infrastructures varies widely with neural network models, which hinders the exploration of the design space to find high-performance neural network architectures on specific computing platforms for a given application. To address such a challenge, we propose a deep learning-based method, ResPerfNet, which trains a residual neural network with representative datasets obtained on the target platform to predict the performance for a deep neural network. Our experimental results show that ResPerfNet can accurately predict the execution time of individual neural network layers and full network models on a variety of platforms. In particular, ResPerfNet achieves 8.4% of mean absolute percentage error for LeNet, AlexNet and VGG16 on the NVIDIA GTX 1080Ti, which is substantially lower than the previously published works.", "pdf": "/pdf/2b503c50f8aca8a4cd1b9674b1a866713156b829.pdf", "supplementary_material": "/attachment/afe2f1cd6aa7e007ae3eb6acccb5da3d0010cbcd.zip", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wang|resperfnet_deep_residual_learning_for_regressional_performance_modeling_of_deep_neural_networks", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=iNwbnw1fKf", "_bibtex": "@misc{\nwang2021resperfnet,\ntitle={ResPerfNet: Deep Residual Learning for Regressional Performance Modeling of Deep Neural Networks},\nauthor={Chuan-Chi Wang and Ying-Chiao Liao and Chia-Heng Tu and Ming-Chang Kao and Wen-Yew Liang and Shih-Hao Hung},\nyear={2021},\nurl={https://openreview.net/forum?id=me5hEszKra4}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "me5hEszKra4", "replyto": "me5hEszKra4", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1009/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538129328, "tmdate": 1606915799228, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1009/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1009/-/Official_Review"}}}, {"id": "3oPQdCd5N2H", "original": null, "number": 3, "cdate": 1603810495566, "ddate": null, "tcdate": 1603810495566, "tmdate": 1605024552590, "tddate": null, "forum": "me5hEszKra4", "replyto": "me5hEszKra4", "invitation": "ICLR.cc/2021/Conference/Paper1009/-/Official_Review", "content": {"title": "The paper presents a method, based on a residual CNN, for performance prediction of deep neural networks on different hardware platforms. The results are promising, but there is a limited comparison with previous work.", "review": "The paper presents a method, called ResPerfNet, to predict the performance of deep neural networks. The method relies on a residual neural network that is trained on a large number of different network architectures and performance measures on real hardware. \n\nThe paper evaluates the proposed method on three networks, LeNet, AlexNet, and VGG16, in two different frameworks, i.e., TensorFlow and TensorRT. The results are promising, but comparison with other approaches is weak. For example, the proposed method, ResPerfNet, is only compared to one other approach, PerfNet (from a paper that don't seem to be published yet, at least I couldn't find it), using TensorFlow (but not using TensorRT).\n\nThe paper has potential to have impact, but it needs to be improved before publication. For example, the following issues need to be addressed:\n* Motivation for the selected structure / architecture of ResPerfNet.\n* Some confusion about kernels, filters, etc. in the description of the ResPerfNet architecture (Section. 3 + Fig. 1)\n* I'm a bit surprised that the dropout layer is very close to the end of the network. Why? Why 0.2 dropout (and not 0.1 or 0.4)?\n* It's a bit confusing (and inconsistent) that the index I is left out sometimes and sometimes not, e.g., Eq (2) vs. Eq (3) vs. how it is written in the text flow. \n* C(f,d) in Eq (5) is never defined.\n* Platform for sample selection / data collection should be mentioned in Section 5.2\n* Section 5.4. Although using defining the loss function as MAPLE does reduce the problem with a skewed distribution, it doe not solve it so \"cope with it\" is a bit strong formulation.\n* It is disturbing that one of the main references [Wang 2020] can't be found, despite extensive searching. This is problematic since the only other solution ResPerfNet is compared to is PerfNet, which is published in [Wang 2020]. This limits the possibility to compare this work with previous work. The conference where the [Wang 2020] paper was published took place in mid October 2020, which is after the deadline for ICLR.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1009/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1009/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ResPerfNet: Deep Residual Learning for Regressional Performance Modeling of Deep Neural Networks", "authorids": ["chuan-chi.wang@adlinktech.com", "~Ying-Chiao_Liao1", "chiaheng@ncku.edu.tw", "fencer.kao@adlinktech.com", "william.liang@adlinktech.com", "~Shih-Hao_Hung1"], "authors": ["Chuan-Chi Wang", "Ying-Chiao Liao", "Chia-Heng Tu", "Ming-Chang Kao", "Wen-Yew Liang", "Shih-Hao Hung"], "keywords": [], "abstract": "The rapid advancements of computing technology facilitate the development of diverse deep learning applications. Unfortunately, the efficiency of parallel computing infrastructures varies widely with neural network models, which hinders the exploration of the design space to find high-performance neural network architectures on specific computing platforms for a given application. To address such a challenge, we propose a deep learning-based method, ResPerfNet, which trains a residual neural network with representative datasets obtained on the target platform to predict the performance for a deep neural network. Our experimental results show that ResPerfNet can accurately predict the execution time of individual neural network layers and full network models on a variety of platforms. In particular, ResPerfNet achieves 8.4% of mean absolute percentage error for LeNet, AlexNet and VGG16 on the NVIDIA GTX 1080Ti, which is substantially lower than the previously published works.", "pdf": "/pdf/2b503c50f8aca8a4cd1b9674b1a866713156b829.pdf", "supplementary_material": "/attachment/afe2f1cd6aa7e007ae3eb6acccb5da3d0010cbcd.zip", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wang|resperfnet_deep_residual_learning_for_regressional_performance_modeling_of_deep_neural_networks", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=iNwbnw1fKf", "_bibtex": "@misc{\nwang2021resperfnet,\ntitle={ResPerfNet: Deep Residual Learning for Regressional Performance Modeling of Deep Neural Networks},\nauthor={Chuan-Chi Wang and Ying-Chiao Liao and Chia-Heng Tu and Ming-Chang Kao and Wen-Yew Liang and Shih-Hao Hung},\nyear={2021},\nurl={https://openreview.net/forum?id=me5hEszKra4}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "me5hEszKra4", "replyto": "me5hEszKra4", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1009/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538129328, "tmdate": 1606915799228, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1009/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1009/-/Official_Review"}}}], "count": 11}