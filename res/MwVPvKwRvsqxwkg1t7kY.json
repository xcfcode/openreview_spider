{"notes": [{"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1458579237256, "tcdate": 1458579237256, "id": "MwnDKjJBmCqxwkg1t7PD", "invitation": "ICLR.cc/2016/workshop/-/paper/153/comment", "forum": "MwVPvKwRvsqxwkg1t7kY", "replyto": "oVg3LLM8pfrlgPMRsBOQ", "signatures": ["~Sebastian_Urban1"], "readers": ["everyone"], "writers": ["~Sebastian_Urban1"], "content": {"title": "Response to Reviewer 12", "comment": "Thank you for your comments.\n\nSince we implemented no provisions against overfitting in our preliminary experiments, the tanh network will indeed overfit if trained sufficiently long. \nHowever, the test loss of the proposed exp^(n) network is still better when compared to the best test lost of the tanh network (roughly after 5,000 iterations). This would correspond to an early stopping criterion for the baseline.\n\nAlso from the training and test curves, it seems that the exp^(n) has some natural resiliency against overfitting, which could provide to be a useful property. We will assess overfitting resilency using more advanced method (weight regularization, dropout) in further experiments.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "A Differentiable Transition Between Additive and Multiplicative Neurons", "abstract": "Existing approaches to combine both additive and multiplicative neural units either use a fixed assignment of operations or require discrete optimization to determine what function a neuron should perform. However, this leads to an extensive increase in the computational complexity of the training procedure.\n\nWe present a novel, parameterizable transfer function based on the mathematical concept of non-integer functional iteration that allows the operation each neuron performs to be smoothly and, most importantly, differentiablely adjusted between addition and multiplication. This allows the decision between addition and multiplication to be integrated into the standard backpropagation training procedure.", "pdf": "/pdf/MwVPvKwRvsqxwkg1t7kY.pdf", "paperhash": "koepp|a_differentiable_transition_between_additive_and_multiplicative_neurons", "conflicts": ["tum.de", "brml.org", "fortiss.org"], "authors": ["Wiebke Koepp", "Patrick van der Smagt", "Sebastian Urban"], "authorids": ["koepp@in.tum.de", "smagt@brml.org", "surban@tum.de"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "tmdate": null, "cdate": 1455829327266, "ddate": null, "super": null, "final": null, "tcdate": 1455829327266, "id": "ICLR.cc/2016/workshop/-/paper/153/comment", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "readers": ["everyone"], "reply": {"pdf": null, "replyto": null, "writers": {"values-regex": "~.*"}, "forum": "MwVPvKwRvsqxwkg1t7kY", "signatures": {"values-regex": "~.*", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "invitees": ["~", "ICLR.cc/2016/workshop/paper/153/reviewer/12"], "nonreaders": [], "noninvitees": []}}}, {"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1458575820796, "tcdate": 1458575820796, "id": "D1KNEQkLOf5jEJ1zfEBj", "invitation": "ICLR.cc/2016/workshop/-/paper/153/comment", "forum": "MwVPvKwRvsqxwkg1t7kY", "replyto": "p8j40N7POFnQVOGWfpJm", "signatures": ["~Sebastian_Urban1"], "readers": ["everyone"], "writers": ["~Sebastian_Urban1"], "content": {"title": "Response to the review by David Duvenaud", "comment": "Thank you for your comments.\n\n1) The definition for \\Psi given in (3) is indeed iterative. However, to obtain competitive performance in the experiments we use the procedure described below.\n\n2) We precompute exp^(n)(x) given by (6) for a sensible range of x and n and store the results in a lookup table. During runs of the neural net we use linearly interpolated values from this lookup table. CUDA GPUs provide very fast hardware primitives and a dedicated memory region (texture memory) for that purpose. Hence evaluating the transfer function (8) requires roughly three times (exp^(n_li), sigmoid, exp^(m_li)) the number of operations as for the standard sigmoid. However, in practice performance is better. During internal testing on a nVidia Quadro K2200 GPU we achieve 2/3 of the performance of a standard sigmoid network with the same number of units.\nOf course this method requires that x and n are limited to a sensible range, i.e. input data must be normalized as is common practice in machine learning.\n\n4) Mathematically the domain becomes restricted to positive reals for n <= -1. \nNonetheless, very large negative numbers for x < 0 already occur for n close to -1. A method to avoid this issue is to use complex numbers, further details are provided in our arXiv paper at http://arxiv.org/abs/1503.05724\n\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "A Differentiable Transition Between Additive and Multiplicative Neurons", "abstract": "Existing approaches to combine both additive and multiplicative neural units either use a fixed assignment of operations or require discrete optimization to determine what function a neuron should perform. However, this leads to an extensive increase in the computational complexity of the training procedure.\n\nWe present a novel, parameterizable transfer function based on the mathematical concept of non-integer functional iteration that allows the operation each neuron performs to be smoothly and, most importantly, differentiablely adjusted between addition and multiplication. This allows the decision between addition and multiplication to be integrated into the standard backpropagation training procedure.", "pdf": "/pdf/MwVPvKwRvsqxwkg1t7kY.pdf", "paperhash": "koepp|a_differentiable_transition_between_additive_and_multiplicative_neurons", "conflicts": ["tum.de", "brml.org", "fortiss.org"], "authors": ["Wiebke Koepp", "Patrick van der Smagt", "Sebastian Urban"], "authorids": ["koepp@in.tum.de", "smagt@brml.org", "surban@tum.de"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "tmdate": null, "cdate": 1455829327266, "ddate": null, "super": null, "final": null, "tcdate": 1455829327266, "id": "ICLR.cc/2016/workshop/-/paper/153/comment", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "readers": ["everyone"], "reply": {"pdf": null, "replyto": null, "writers": {"values-regex": "~.*"}, "forum": "MwVPvKwRvsqxwkg1t7kY", "signatures": {"values-regex": "~.*", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "invitees": ["~", "ICLR.cc/2016/workshop/paper/153/reviewer/12"], "nonreaders": [], "noninvitees": []}}}, {"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1457659995353, "tcdate": 1457659995353, "id": "oVg3LLM8pfrlgPMRsBOQ", "invitation": "ICLR.cc/2016/workshop/-/paper/153/review/12", "forum": "MwVPvKwRvsqxwkg1t7kY", "replyto": "MwVPvKwRvsqxwkg1t7kY", "signatures": ["ICLR.cc/2016/workshop/paper/153/reviewer/12"], "readers": ["everyone"], "writers": ["ICLR.cc/2016/workshop/paper/153/reviewer/12"], "content": {"title": "Interesting idea - develop further?", "rating": "7: Good paper, accept", "review": "The paper suggests using a differentiable function which can smoothly interpolate between multiplicative and additive gates in neural networks.  It is an intriguing idea and the paper is well written.  The mathematical ideas introduced are perhaps not novel (a cursory search seems to indicate that Abel's functional equation with f=exp is called the tetration equation and its solution called the iterated logarithm), but their use in machine learning seem to be.\n\nThe experiment section is weaker - the problem seems somewhat contrived and with few datapoints. exp^{(n)} get the best test loss but the worst training loss - are the baselines simply overfitting? (it appears so when looking at the tanh testing curve). ", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "A Differentiable Transition Between Additive and Multiplicative Neurons", "abstract": "Existing approaches to combine both additive and multiplicative neural units either use a fixed assignment of operations or require discrete optimization to determine what function a neuron should perform. However, this leads to an extensive increase in the computational complexity of the training procedure.\n\nWe present a novel, parameterizable transfer function based on the mathematical concept of non-integer functional iteration that allows the operation each neuron performs to be smoothly and, most importantly, differentiablely adjusted between addition and multiplication. This allows the decision between addition and multiplication to be integrated into the standard backpropagation training procedure.", "pdf": "/pdf/MwVPvKwRvsqxwkg1t7kY.pdf", "paperhash": "koepp|a_differentiable_transition_between_additive_and_multiplicative_neurons", "conflicts": ["tum.de", "brml.org", "fortiss.org"], "authors": ["Wiebke Koepp", "Patrick van der Smagt", "Sebastian Urban"], "authorids": ["koepp@in.tum.de", "smagt@brml.org", "surban@tum.de"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1456580029144, "ddate": null, "super": null, "final": null, "duedate": 1460725200000, "tcdate": 1456580029144, "id": "ICLR.cc/2016/workshop/-/paper/153/review/12", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "reply": {"pdf": null, "forum": "MwVPvKwRvsqxwkg1t7kY", "replyto": "MwVPvKwRvsqxwkg1t7kY", "writers": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)"}, "signatures": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "readers": ["everyone", "ICLR.cc/2016/workshop/paper/153/reviewer/12", "ICLR.cc/2016/workshop"], "expdate": 1468501200000}}}, {"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1457656621256, "tcdate": 1457656621256, "id": "p8j40N7POFnQVOGWfpJm", "invitation": "ICLR.cc/2016/workshop/-/paper/153/review/11", "forum": "MwVPvKwRvsqxwkg1t7kY", "replyto": "MwVPvKwRvsqxwkg1t7kY", "signatures": ["~David_Duvenaud1"], "readers": ["everyone"], "writers": ["~David_Duvenaud1"], "content": {"title": "Wherein I review this paper", "rating": "6: Marginally above acceptance threshold", "review": "The main idea seems sensible, and fairly well-explained.  Having a differentiable generalization of both addition and multiplication seems like a useful tool to have in general.\n\nMy main fear is that the idea isn't novel, but I haven't seen it presented in an ML framework before.\n\nProblems:\n1) It's still not clear to me how to compute the proposed function - I assume it's done iteratively?\n2) Part of the motivation is training speed, but the authors didn't measure wallclock time.  This should have been included.\n3) The use of \\psi and something like \\varpsi or \\varphi is confusing.  Please use distinct letters for which we have names.\n4) Is the domain restricted to the positive reals only when n = -1 (and we recover log(x)), or is it restricted whenever n < 0?\n\n\n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "A Differentiable Transition Between Additive and Multiplicative Neurons", "abstract": "Existing approaches to combine both additive and multiplicative neural units either use a fixed assignment of operations or require discrete optimization to determine what function a neuron should perform. However, this leads to an extensive increase in the computational complexity of the training procedure.\n\nWe present a novel, parameterizable transfer function based on the mathematical concept of non-integer functional iteration that allows the operation each neuron performs to be smoothly and, most importantly, differentiablely adjusted between addition and multiplication. This allows the decision between addition and multiplication to be integrated into the standard backpropagation training procedure.", "pdf": "/pdf/MwVPvKwRvsqxwkg1t7kY.pdf", "paperhash": "koepp|a_differentiable_transition_between_additive_and_multiplicative_neurons", "conflicts": ["tum.de", "brml.org", "fortiss.org"], "authors": ["Wiebke Koepp", "Patrick van der Smagt", "Sebastian Urban"], "authorids": ["koepp@in.tum.de", "smagt@brml.org", "surban@tum.de"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1456580029485, "ddate": null, "super": null, "final": null, "duedate": 1460725200000, "tcdate": 1456580029485, "id": "ICLR.cc/2016/workshop/-/paper/153/review/11", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "reply": {"pdf": null, "forum": "MwVPvKwRvsqxwkg1t7kY", "replyto": "MwVPvKwRvsqxwkg1t7kY", "writers": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)"}, "signatures": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "readers": ["everyone", "ICLR.cc/2016/workshop/paper/153/reviewer/11", "ICLR.cc/2016/workshop"], "expdate": 1468501200000}}}, {"tddate": null, "number": null, "replyto": null, "ddate": null, "cdate": null, "tmdate": 1455829324563, "tcdate": 1455829324563, "id": "MwVPvKwRvsqxwkg1t7kY", "invitation": "ICLR.cc/2016/workshop/-/submission", "forum": "MwVPvKwRvsqxwkg1t7kY", "signatures": ["~Sebastian_Urban1"], "readers": ["everyone"], "writers": ["~Sebastian_Urban1"], "content": {"CMT_id": "", "title": "A Differentiable Transition Between Additive and Multiplicative Neurons", "abstract": "Existing approaches to combine both additive and multiplicative neural units either use a fixed assignment of operations or require discrete optimization to determine what function a neuron should perform. However, this leads to an extensive increase in the computational complexity of the training procedure.\n\nWe present a novel, parameterizable transfer function based on the mathematical concept of non-integer functional iteration that allows the operation each neuron performs to be smoothly and, most importantly, differentiablely adjusted between addition and multiplication. This allows the decision between addition and multiplication to be integrated into the standard backpropagation training procedure.", "pdf": "/pdf/MwVPvKwRvsqxwkg1t7kY.pdf", "paperhash": "koepp|a_differentiable_transition_between_additive_and_multiplicative_neurons", "conflicts": ["tum.de", "brml.org", "fortiss.org"], "authors": ["Wiebke Koepp", "Patrick van der Smagt", "Sebastian Urban"], "authorids": ["koepp@in.tum.de", "smagt@brml.org", "surban@tum.de"]}, "nonreaders": [], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1454464564200, "ddate": null, "super": null, "final": null, "duedate": 1455833700000, "tcdate": 1454464564200, "id": "ICLR.cc/2016/workshop/-/submission", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "readers": ["everyone"], "reply": {"pdf": null, "forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"order": 4, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv.", "value-regex": "upload|http://arxiv.org/pdf/.+"}, "title": {"order": 3, "description": "Title of paper.", "value-regex": ".{0,500}"}, "abstract": {"order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"order": 1, "description": "Comma separated list of author names, as they appear in the paper.", "value-regex": "[^,\\n]+(,[^,\\n]+)*"}, "author_emails": {"order": 2, "description": "Comma separated list of author email addresses, in the same order as above.", "value-regex": "[^,\\n]+(,[^,\\n]+)*"}, "conflicts": {"order": 100, "description": "Semi-colon separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.).", "value-regex": "^([a-zA-Z0-9][a-zA-Z0-9-_]{0,61}[a-zA-Z0-9]{0,1}\\.([a-zA-Z]{1,6}|[a-zA-Z0-9-]{1,30}\\.[a-zA-Z]{2,3}))+(;[a-zA-Z0-9][a-zA-Z0-9-_]{0,61}[a-zA-Z0-9]{0,1}\\.([a-zA-Z]{1,6}|[a-zA-Z0-9-]{1,30}\\.[a-zA-Z]{2,3}))*$"}, "CMT_id": {"order": 5, "value-regex": ".*", "description": "If the paper is a resubmission from the ICLR 2016 Conference Track, enter its CMT ID; otherwise, leave blank."}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "expdate": 1463609700000}}}], "count": 5}