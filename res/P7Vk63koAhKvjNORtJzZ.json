{"notes": [{"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1458160141775, "tcdate": 1458160141775, "id": "xnrQJo752I1m7RyVi32D", "invitation": "ICLR.cc/2016/workshop/-/paper/76/comment", "forum": "P7Vk63koAhKvjNORtJzZ", "replyto": "0YrwD8NvXsGJ7gK5tRWJ", "signatures": ["~Jure_Sokolic1"], "readers": ["everyone"], "writers": ["~Jure_Sokolic1"], "content": {"title": "Review reponse", "comment": "We wish to thank the reviewer for the provided feedback and constructive comments. We provide the answers to the comments below:\n\ni) This is a very relevant observation. Indeed, a more correct statement would be: models with lower ERC are better provided that the model is sufficiently powerful to model the task at hand. However, to fully understand the trade-offs between the absolute error and the generalization error as a function of depth we would first need to have a better understanding of the underlying data model. This is beyond the goals of this work.\n\nii) We argue that \u201corthogonality\u201d of the representation (even for the points from the same class) leads to more robust representation. The method where the class membership of data points is considered seems like a very reasonable extension of the current regularizer.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Lessons from the Rademacher Complexity for Deep Learning", "abstract": "Understanding the generalization properties of deep learning models is critical for successful applications, especially in the regimes where the number of training samples is limited. We study the generalization properties of deep neural networks via the empirical Rademacher complexity and show that it is easier to control the complexity of  convolutional networks compared to general fully connected networks. In particular, we justify the usage of small convolutional kernels in deep networks as they lead to a better generalization error. Moreover, we propose a representation based regularization method that allows to decrease the generalization error  by controlling the coherence of the representation. Experiments on the MNIST dataset support these foundations.", "pdf": "/pdf/P7Vk63koAhKvjNORtJzZ.pdf", "paperhash": "sokolic|lessons_from_the_rademacher_complexity_for_deep_learning", "conflicts": ["ucl.ac.uk", "tauex.tau.ac", "duke.edu"], "authors": ["Jure Sokolic", "Raja Giryes", "Guillermo Sapiro", "Miguel R. D. Rodrigues"], "authorids": ["jure.sokolic.13@ucl.ac.uk", "raja@tauex.tau.ac.il", "guillermo.sapiro@duke.edu", "m.rodrigues@ucl.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "tmdate": null, "cdate": 1455756027575, "ddate": null, "super": null, "final": null, "tcdate": 1455756027575, "id": "ICLR.cc/2016/workshop/-/paper/76/comment", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "readers": ["everyone"], "reply": {"pdf": null, "replyto": null, "writers": {"values-regex": "~.*"}, "forum": "P7Vk63koAhKvjNORtJzZ", "signatures": {"values-regex": "~.*", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "invitees": ["~", "ICLR.cc/2016/workshop/paper/76/reviewer/10"], "nonreaders": [], "noninvitees": []}}}, {"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1458160091567, "tcdate": 1458160091567, "id": "k80zpw4PRHOYKX7ji4k6", "invitation": "ICLR.cc/2016/workshop/-/paper/76/comment", "forum": "P7Vk63koAhKvjNORtJzZ", "replyto": "81DG5rwAAC6O2Pl0UVoQ", "signatures": ["~Jure_Sokolic1"], "readers": ["everyone"], "writers": ["~Jure_Sokolic1"], "content": {"title": "Review response ", "comment": "We would like to thank the reviewer for feedback and constructive comments. We provide answers to the Cons (and suggestion) below:\n\n1- Please note that the workshops contributions are limited to 3 pages + references. We understand the reviewers concern however,  we felt that we need to include a proper discussion of the considered model and previous work in order to make the contribution self-contained. We plan to provide more details in the extended version of the paper.\n\n2- Since the proofs are relatively long we felt that there is no reasonable way to include them in this contribution, provided the space constraints. We will provide the proofs in the extended version of the paper.\n\n3- We felt that results in section 2 are well supported by the practical results in the literature. On the other hand,  section 3 discusses a novel approach to regularization and is further supported by the experiments.\n\n4- This is a very important observation. In the practical implementation we used a stochastic gradient descent optimization algorithm and only applied the regularizer to the pairwise inner products between the points in a single batch, which is much smaller than the whole training set. That way the computational complexity does not increase drastically.\n\n5- This is a fair point. However, the term 2^L in the bounds provided in section 2 is due to the ReLU non-linearity. We argue that the ResNet, which also uses ReLUs will also exhibit the 2^L term in the bound because of this.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Lessons from the Rademacher Complexity for Deep Learning", "abstract": "Understanding the generalization properties of deep learning models is critical for successful applications, especially in the regimes where the number of training samples is limited. We study the generalization properties of deep neural networks via the empirical Rademacher complexity and show that it is easier to control the complexity of  convolutional networks compared to general fully connected networks. In particular, we justify the usage of small convolutional kernels in deep networks as they lead to a better generalization error. Moreover, we propose a representation based regularization method that allows to decrease the generalization error  by controlling the coherence of the representation. Experiments on the MNIST dataset support these foundations.", "pdf": "/pdf/P7Vk63koAhKvjNORtJzZ.pdf", "paperhash": "sokolic|lessons_from_the_rademacher_complexity_for_deep_learning", "conflicts": ["ucl.ac.uk", "tauex.tau.ac", "duke.edu"], "authors": ["Jure Sokolic", "Raja Giryes", "Guillermo Sapiro", "Miguel R. D. Rodrigues"], "authorids": ["jure.sokolic.13@ucl.ac.uk", "raja@tauex.tau.ac.il", "guillermo.sapiro@duke.edu", "m.rodrigues@ucl.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "tmdate": null, "cdate": 1455756027575, "ddate": null, "super": null, "final": null, "tcdate": 1455756027575, "id": "ICLR.cc/2016/workshop/-/paper/76/comment", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "readers": ["everyone"], "reply": {"pdf": null, "replyto": null, "writers": {"values-regex": "~.*"}, "forum": "P7Vk63koAhKvjNORtJzZ", "signatures": {"values-regex": "~.*", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "invitees": ["~", "ICLR.cc/2016/workshop/paper/76/reviewer/10"], "nonreaders": [], "noninvitees": []}}}, {"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1457655486603, "tcdate": 1457655486603, "id": "81DG5rwAAC6O2Pl0UVoQ", "invitation": "ICLR.cc/2016/workshop/-/paper/76/review/10", "forum": "P7Vk63koAhKvjNORtJzZ", "replyto": "P7Vk63koAhKvjNORtJzZ", "signatures": ["ICLR.cc/2016/workshop/paper/76/reviewer/10"], "readers": ["everyone"], "writers": ["ICLR.cc/2016/workshop/paper/76/reviewer/10"], "content": {"title": "This paper studies the capacity control for Convolutional Neural Networks with bounded Frobinius norm using Rademacher Complexities. While some of the discussions are interesting, no proof is provided for the theorems presented in the paper. I accepted this if I know the authors will complete the paper.", "rating": "6: Marginally above acceptance threshold", "review": "This paper studies the capacity control for Convolutional Neural Networks with bounded Frobinius norm using Rademacher Complexities.\n\nPros:\n1- Studying the capacity of CNNs is interesting and insightful. The discussions about the effect of filter size and other structural hyper-parameters on the capacity of these networks is very interesting and helpful.\n\n2- A regularizer that penalizes the correlation on the representation layer and the connections to Rademacher complexities are both interesting.\n\nCons (and suggestions):\n\n1- The paper is 4 page long, almost 2 pages of which are abstract, introduction, background and previous works (up to equation 5) and the last page is dedicated to references so the main part of the paper is only about one page and I think authors could have clarified the discussions and experiments much better.\n\n2- Two theorems are presented but without any proof or even a sketch of the proof.\n\n3- While the section 3 is suggesting a regularization term based on the Rademacher complexity, the experiments are not related in anyway to the discussions on section 2 and there is a bit of disconnection here.\n\n4- The suggested regularizer in interesting to study but calculating the gradient for this regularizer seems very time consuming. A short discussion on the computational complexity of using this regularizer could be helpful here.\n\n5- The discussion on the ResNet ignores the fact that ResNet uses skip-layers which is not included in this analysis.\n", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Lessons from the Rademacher Complexity for Deep Learning", "abstract": "Understanding the generalization properties of deep learning models is critical for successful applications, especially in the regimes where the number of training samples is limited. We study the generalization properties of deep neural networks via the empirical Rademacher complexity and show that it is easier to control the complexity of  convolutional networks compared to general fully connected networks. In particular, we justify the usage of small convolutional kernels in deep networks as they lead to a better generalization error. Moreover, we propose a representation based regularization method that allows to decrease the generalization error  by controlling the coherence of the representation. Experiments on the MNIST dataset support these foundations.", "pdf": "/pdf/P7Vk63koAhKvjNORtJzZ.pdf", "paperhash": "sokolic|lessons_from_the_rademacher_complexity_for_deep_learning", "conflicts": ["ucl.ac.uk", "tauex.tau.ac", "duke.edu"], "authors": ["Jure Sokolic", "Raja Giryes", "Guillermo Sapiro", "Miguel R. D. Rodrigues"], "authorids": ["jure.sokolic.13@ucl.ac.uk", "raja@tauex.tau.ac.il", "guillermo.sapiro@duke.edu", "m.rodrigues@ucl.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1456579929967, "ddate": null, "super": null, "final": null, "duedate": 1460725200000, "tcdate": 1456579929967, "id": "ICLR.cc/2016/workshop/-/paper/76/review/10", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "reply": {"pdf": null, "forum": "P7Vk63koAhKvjNORtJzZ", "replyto": "P7Vk63koAhKvjNORtJzZ", "writers": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)"}, "signatures": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "readers": ["everyone", "ICLR.cc/2016/workshop/paper/76/reviewer/10", "ICLR.cc/2016/workshop"], "expdate": 1468501200000}}}, {"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1457582541154, "tcdate": 1457582541154, "id": "0YrwD8NvXsGJ7gK5tRWJ", "invitation": "ICLR.cc/2016/workshop/-/paper/76/review/12", "forum": "P7Vk63koAhKvjNORtJzZ", "replyto": "P7Vk63koAhKvjNORtJzZ", "signatures": ["ICLR.cc/2016/workshop/paper/76/reviewer/12"], "readers": ["everyone"], "writers": ["ICLR.cc/2016/workshop/paper/76/reviewer/12"], "content": {"title": "", "rating": "7: Good paper, accept", "review": "Authors present novel and interesting results on empirical Rademacher complexity (ERC) of deep neural networks, building on recent work of Neyshabur et al.  Specifically they present a new bound on ERC for deep convolutional networks and demonstrate that their bound is tighter than that of Neyshabur et al.  They also provide a deep representation based bound on ERC which is then used to motivate a novel regularization approach that aims to orthogonalize representation for distinct data samples.\n\nFew aspects in which I feel the paper could be improved:\ni) Authors seem to be claiming that models with lower ERC are better.  This is how use of smaller kernels in CNNs in justified.  However, I think a key missing component of the discussion is absolute error (and not just generalization error) for models in a given model family, this has to be taken into account.  This would then perhaps explain the apparent discrepancy of why ERC increases as network depth increases and yet those models empirically do quite well.\nii) The novel regularizer based on coherence (orthogonality) of representation is nice but a little bit counter intuitive for elements that belong to the same class (for a classification network such as ones used in MNIST).  Would it make sense to consider the class membership of data points?  Any insights into this would strengthen the paper I believe.\n\nOverall I think this is a good paper, fairly clearly written and easy to follow.  Should be published and discussed.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Lessons from the Rademacher Complexity for Deep Learning", "abstract": "Understanding the generalization properties of deep learning models is critical for successful applications, especially in the regimes where the number of training samples is limited. We study the generalization properties of deep neural networks via the empirical Rademacher complexity and show that it is easier to control the complexity of  convolutional networks compared to general fully connected networks. In particular, we justify the usage of small convolutional kernels in deep networks as they lead to a better generalization error. Moreover, we propose a representation based regularization method that allows to decrease the generalization error  by controlling the coherence of the representation. Experiments on the MNIST dataset support these foundations.", "pdf": "/pdf/P7Vk63koAhKvjNORtJzZ.pdf", "paperhash": "sokolic|lessons_from_the_rademacher_complexity_for_deep_learning", "conflicts": ["ucl.ac.uk", "tauex.tau.ac", "duke.edu"], "authors": ["Jure Sokolic", "Raja Giryes", "Guillermo Sapiro", "Miguel R. D. Rodrigues"], "authorids": ["jure.sokolic.13@ucl.ac.uk", "raja@tauex.tau.ac.il", "guillermo.sapiro@duke.edu", "m.rodrigues@ucl.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1456579947420, "ddate": null, "super": null, "final": null, "duedate": 1460725200000, "tcdate": 1456579947420, "id": "ICLR.cc/2016/workshop/-/paper/76/review/12", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "reply": {"pdf": null, "forum": "P7Vk63koAhKvjNORtJzZ", "replyto": "P7Vk63koAhKvjNORtJzZ", "writers": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)"}, "signatures": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "readers": ["everyone", "ICLR.cc/2016/workshop/paper/76/reviewer/12", "ICLR.cc/2016/workshop"], "expdate": 1468501200000}}}, {"tddate": null, "number": null, "replyto": null, "ddate": null, "cdate": null, "tmdate": 1455756025175, "tcdate": 1455756025175, "id": "P7Vk63koAhKvjNORtJzZ", "invitation": "ICLR.cc/2016/workshop/-/submission", "forum": "P7Vk63koAhKvjNORtJzZ", "signatures": ["~Jure_Sokolic1"], "readers": ["everyone"], "writers": ["~Jure_Sokolic1"], "content": {"CMT_id": "", "title": "Lessons from the Rademacher Complexity for Deep Learning", "abstract": "Understanding the generalization properties of deep learning models is critical for successful applications, especially in the regimes where the number of training samples is limited. We study the generalization properties of deep neural networks via the empirical Rademacher complexity and show that it is easier to control the complexity of  convolutional networks compared to general fully connected networks. In particular, we justify the usage of small convolutional kernels in deep networks as they lead to a better generalization error. Moreover, we propose a representation based regularization method that allows to decrease the generalization error  by controlling the coherence of the representation. Experiments on the MNIST dataset support these foundations.", "pdf": "/pdf/P7Vk63koAhKvjNORtJzZ.pdf", "paperhash": "sokolic|lessons_from_the_rademacher_complexity_for_deep_learning", "conflicts": ["ucl.ac.uk", "tauex.tau.ac", "duke.edu"], "authors": ["Jure Sokolic", "Raja Giryes", "Guillermo Sapiro", "Miguel R. D. Rodrigues"], "authorids": ["jure.sokolic.13@ucl.ac.uk", "raja@tauex.tau.ac.il", "guillermo.sapiro@duke.edu", "m.rodrigues@ucl.ac.uk"]}, "nonreaders": [], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1454464564200, "ddate": null, "super": null, "final": null, "duedate": 1455833700000, "tcdate": 1454464564200, "id": "ICLR.cc/2016/workshop/-/submission", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "readers": ["everyone"], "reply": {"pdf": null, "forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"order": 4, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv.", "value-regex": "upload|http://arxiv.org/pdf/.+"}, "title": {"order": 3, "description": "Title of paper.", "value-regex": ".{0,500}"}, "abstract": {"order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"order": 1, "description": "Comma separated list of author names, as they appear in the paper.", "value-regex": "[^,\\n]+(,[^,\\n]+)*"}, "author_emails": {"order": 2, "description": "Comma separated list of author email addresses, in the same order as above.", "value-regex": "[^,\\n]+(,[^,\\n]+)*"}, "conflicts": {"order": 100, "description": "Semi-colon separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.).", "value-regex": "^([a-zA-Z0-9][a-zA-Z0-9-_]{0,61}[a-zA-Z0-9]{0,1}\\.([a-zA-Z]{1,6}|[a-zA-Z0-9-]{1,30}\\.[a-zA-Z]{2,3}))+(;[a-zA-Z0-9][a-zA-Z0-9-_]{0,61}[a-zA-Z0-9]{0,1}\\.([a-zA-Z]{1,6}|[a-zA-Z0-9-]{1,30}\\.[a-zA-Z]{2,3}))*$"}, "CMT_id": {"order": 5, "value-regex": ".*", "description": "If the paper is a resubmission from the ICLR 2016 Conference Track, enter its CMT ID; otherwise, leave blank."}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "expdate": 1463609700000}}}], "count": 5}