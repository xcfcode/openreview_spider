{"notes": [{"id": "B1ePui0ctQ", "original": "HJxOseNctQ", "number": 364, "cdate": 1538087791174, "ddate": null, "tcdate": 1538087791174, "tmdate": 1545355377632, "tddate": null, "forum": "B1ePui0ctQ", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "SnapQuant: A Probabilistic and Nested Parameterization for Binary Networks", "abstract": "In this paper, we study the problem of training real binary weight networks (without layer-wise or filter-wise scaling factors) from scratch under the Bayesian deep learning perspective, meaning that the final objective is to approximate the posterior distribution of binary weights rather than reach a point estimation. The proposed method, named as SnapQuant, has two intriguing features: (1) The posterior distribution is parameterized as a policy network trained with a reinforcement learning scheme. During the training phase, we generate binary weights on-the-fly since what we actually maintain is the policy network, and all the binary weights are used in a burn-after-reading style. At the testing phase, we can sample binary weight instances for a given recognition architecture from the learnt policy network. (2) The policy network, which has a nested parameter structure consisting of layer-wise, filter-wise and kernel-wise parameter sharing designs, is applicable to any neural network architecture. Such a nested parameterization explicitly and hierarchically models the joint posterior distribution of binary weights. The performance of SnapQuant is evaluated with several visual recognition tasks including ImageNet. The code will be made publicly available.", "keywords": ["Binary weight networks", "neural network quantization", "reinforcement learning"], "authorids": ["wangkuan15@mails.tsinghua.edu.cn", "hao.zhao@intel.com", "anbang.yao@intel.com", "aojun.zhou@intel.com", "dawei.sun@intel.com", "yurong.chen@intel.com"], "authors": ["Kuan Wang", "Hao Zhao", "Anbang Yao", "Aojun Zhou", "Dawei Sun", "Yurong Chen"], "TL;DR": "We propose SnapQuant, a reinforcement learning method for training binary weight networks from scratch under the Bayesian deep learning perspective, which approximates the posterior distribution of binary weights instead of a single point estimation.", "pdf": "/pdf/f7b3bc57b6782065c6b8e0ba325a402bb83f1eaa.pdf", "paperhash": "wang|snapquant_a_probabilistic_and_nested_parameterization_for_binary_networks", "_bibtex": "@misc{\nwang2019snapquant,\ntitle={SnapQuant: A Probabilistic and Nested Parameterization for Binary Networks},\nauthor={Kuan Wang and Hao Zhao and Anbang Yao and Aojun Zhou and Dawei Sun and Yurong Chen},\nyear={2019},\nurl={https://openreview.net/forum?id=B1ePui0ctQ},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "B1gmZOT-gN", "original": null, "number": 1, "cdate": 1544833019388, "ddate": null, "tcdate": 1544833019388, "tmdate": 1545354531309, "tddate": null, "forum": "B1ePui0ctQ", "replyto": "B1ePui0ctQ", "invitation": "ICLR.cc/2019/Conference/-/Paper364/Meta_Review", "content": {"metareview": "Reviewers mostly recommended to reject. Please take reviewers' comments into consideration to improve your submission should you decide to resubmit.\n", "confidence": "3: The area chair is somewhat confident", "recommendation": "Reject", "title": "Paper decision"}, "signatures": ["ICLR.cc/2019/Conference/Paper364/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper364/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SnapQuant: A Probabilistic and Nested Parameterization for Binary Networks", "abstract": "In this paper, we study the problem of training real binary weight networks (without layer-wise or filter-wise scaling factors) from scratch under the Bayesian deep learning perspective, meaning that the final objective is to approximate the posterior distribution of binary weights rather than reach a point estimation. The proposed method, named as SnapQuant, has two intriguing features: (1) The posterior distribution is parameterized as a policy network trained with a reinforcement learning scheme. During the training phase, we generate binary weights on-the-fly since what we actually maintain is the policy network, and all the binary weights are used in a burn-after-reading style. At the testing phase, we can sample binary weight instances for a given recognition architecture from the learnt policy network. (2) The policy network, which has a nested parameter structure consisting of layer-wise, filter-wise and kernel-wise parameter sharing designs, is applicable to any neural network architecture. Such a nested parameterization explicitly and hierarchically models the joint posterior distribution of binary weights. The performance of SnapQuant is evaluated with several visual recognition tasks including ImageNet. The code will be made publicly available.", "keywords": ["Binary weight networks", "neural network quantization", "reinforcement learning"], "authorids": ["wangkuan15@mails.tsinghua.edu.cn", "hao.zhao@intel.com", "anbang.yao@intel.com", "aojun.zhou@intel.com", "dawei.sun@intel.com", "yurong.chen@intel.com"], "authors": ["Kuan Wang", "Hao Zhao", "Anbang Yao", "Aojun Zhou", "Dawei Sun", "Yurong Chen"], "TL;DR": "We propose SnapQuant, a reinforcement learning method for training binary weight networks from scratch under the Bayesian deep learning perspective, which approximates the posterior distribution of binary weights instead of a single point estimation.", "pdf": "/pdf/f7b3bc57b6782065c6b8e0ba325a402bb83f1eaa.pdf", "paperhash": "wang|snapquant_a_probabilistic_and_nested_parameterization_for_binary_networks", "_bibtex": "@misc{\nwang2019snapquant,\ntitle={SnapQuant: A Probabilistic and Nested Parameterization for Binary Networks},\nauthor={Kuan Wang and Hao Zhao and Anbang Yao and Aojun Zhou and Dawei Sun and Yurong Chen},\nyear={2019},\nurl={https://openreview.net/forum?id=B1ePui0ctQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper364/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353242743, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1ePui0ctQ", "replyto": "B1ePui0ctQ", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper364/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper364/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper364/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353242743}}}, {"id": "SJgTv8Y5R7", "original": null, "number": 4, "cdate": 1543308901284, "ddate": null, "tcdate": 1543308901284, "tmdate": 1543308901284, "tddate": null, "forum": "B1ePui0ctQ", "replyto": "r1eSaciL27", "invitation": "ICLR.cc/2019/Conference/-/Paper364/Official_Comment", "content": {"title": "Thanks for the attentive reading and professional comments.", "comment": "- We thank R2 for pointing out the issue w.r.t. the pseudo-reward for reinforce. For now, we admit it is ad-hoc. We will try to formulate it in a more elegant manner in a future version.\n- We admit that the proposed method is not better than baselines. A potential advantage is versatility of the formulation. However, as rightly pointed out by reviewers, it is not validated by experiments. We will try to demonstrate the versatility in a future version.\n- Currently, the input to the policy network is a globally shared state vector that remains constant. We thank R2 for these professional suggestions on augmenting it into more complex forms.\n- We admit that the experimental protocol is not very convincing in this manuscript. We will use the mean values and variances in a future version.\n- We thank R2 for pointing out the issue about the gradient estimator. We will try to give a better formulation in a future version.\n- Yes, concrete weights refer to binary weights sampled from the variational approximation.\n- We thank R2 for this professional suggestion. We will remove 'modern' in a future version.\n- Actually, I have to admit that I don't know how to answer this question. What is concrete, relax and rebar? As for the bias-variance trade-off, we will try to outline its envelope by varying the prior from 0.1-0.9 to 0.9-0.1, in a future version.\n- We will incorporate experiments showing these features in a future version."}, "signatures": ["ICLR.cc/2019/Conference/Paper364/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper364/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper364/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SnapQuant: A Probabilistic and Nested Parameterization for Binary Networks", "abstract": "In this paper, we study the problem of training real binary weight networks (without layer-wise or filter-wise scaling factors) from scratch under the Bayesian deep learning perspective, meaning that the final objective is to approximate the posterior distribution of binary weights rather than reach a point estimation. The proposed method, named as SnapQuant, has two intriguing features: (1) The posterior distribution is parameterized as a policy network trained with a reinforcement learning scheme. During the training phase, we generate binary weights on-the-fly since what we actually maintain is the policy network, and all the binary weights are used in a burn-after-reading style. At the testing phase, we can sample binary weight instances for a given recognition architecture from the learnt policy network. (2) The policy network, which has a nested parameter structure consisting of layer-wise, filter-wise and kernel-wise parameter sharing designs, is applicable to any neural network architecture. Such a nested parameterization explicitly and hierarchically models the joint posterior distribution of binary weights. The performance of SnapQuant is evaluated with several visual recognition tasks including ImageNet. The code will be made publicly available.", "keywords": ["Binary weight networks", "neural network quantization", "reinforcement learning"], "authorids": ["wangkuan15@mails.tsinghua.edu.cn", "hao.zhao@intel.com", "anbang.yao@intel.com", "aojun.zhou@intel.com", "dawei.sun@intel.com", "yurong.chen@intel.com"], "authors": ["Kuan Wang", "Hao Zhao", "Anbang Yao", "Aojun Zhou", "Dawei Sun", "Yurong Chen"], "TL;DR": "We propose SnapQuant, a reinforcement learning method for training binary weight networks from scratch under the Bayesian deep learning perspective, which approximates the posterior distribution of binary weights instead of a single point estimation.", "pdf": "/pdf/f7b3bc57b6782065c6b8e0ba325a402bb83f1eaa.pdf", "paperhash": "wang|snapquant_a_probabilistic_and_nested_parameterization_for_binary_networks", "_bibtex": "@misc{\nwang2019snapquant,\ntitle={SnapQuant: A Probabilistic and Nested Parameterization for Binary Networks},\nauthor={Kuan Wang and Hao Zhao and Anbang Yao and Aojun Zhou and Dawei Sun and Yurong Chen},\nyear={2019},\nurl={https://openreview.net/forum?id=B1ePui0ctQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper364/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621623021, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1ePui0ctQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper364/Authors", "ICLR.cc/2019/Conference/Paper364/Reviewers", "ICLR.cc/2019/Conference/Paper364/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper364/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper364/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper364/Authors|ICLR.cc/2019/Conference/Paper364/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper364/Reviewers", "ICLR.cc/2019/Conference/Paper364/Authors", "ICLR.cc/2019/Conference/Paper364/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621623021}}}, {"id": "ryls5BYcCm", "original": null, "number": 3, "cdate": 1543308691118, "ddate": null, "tcdate": 1543308691118, "tmdate": 1543308691118, "tddate": null, "forum": "B1ePui0ctQ", "replyto": "S1lmiJ6_nQ", "invitation": "ICLR.cc/2019/Conference/-/Paper364/Official_Comment", "content": {"title": "Thanks for the attentive reading and professional comments.", "comment": "We thank R3 for these encouraging feedbacks on the proposed training scheme and the nested structure.\nWe want to clarify our results obtained by our sampled binary networks. For both baseline and our method, we evaluate the model (just sample one binary model at each epoch) on valid-set at the end of each training epoch and report the best one among the results of last 100 training epochs, so it's a fair comparison. We admit that the experimental methodology is not very convincing in the manuscript. We will incorporate a systematic evaluation with mean values and variances, in a future version.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper364/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper364/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper364/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SnapQuant: A Probabilistic and Nested Parameterization for Binary Networks", "abstract": "In this paper, we study the problem of training real binary weight networks (without layer-wise or filter-wise scaling factors) from scratch under the Bayesian deep learning perspective, meaning that the final objective is to approximate the posterior distribution of binary weights rather than reach a point estimation. The proposed method, named as SnapQuant, has two intriguing features: (1) The posterior distribution is parameterized as a policy network trained with a reinforcement learning scheme. During the training phase, we generate binary weights on-the-fly since what we actually maintain is the policy network, and all the binary weights are used in a burn-after-reading style. At the testing phase, we can sample binary weight instances for a given recognition architecture from the learnt policy network. (2) The policy network, which has a nested parameter structure consisting of layer-wise, filter-wise and kernel-wise parameter sharing designs, is applicable to any neural network architecture. Such a nested parameterization explicitly and hierarchically models the joint posterior distribution of binary weights. The performance of SnapQuant is evaluated with several visual recognition tasks including ImageNet. The code will be made publicly available.", "keywords": ["Binary weight networks", "neural network quantization", "reinforcement learning"], "authorids": ["wangkuan15@mails.tsinghua.edu.cn", "hao.zhao@intel.com", "anbang.yao@intel.com", "aojun.zhou@intel.com", "dawei.sun@intel.com", "yurong.chen@intel.com"], "authors": ["Kuan Wang", "Hao Zhao", "Anbang Yao", "Aojun Zhou", "Dawei Sun", "Yurong Chen"], "TL;DR": "We propose SnapQuant, a reinforcement learning method for training binary weight networks from scratch under the Bayesian deep learning perspective, which approximates the posterior distribution of binary weights instead of a single point estimation.", "pdf": "/pdf/f7b3bc57b6782065c6b8e0ba325a402bb83f1eaa.pdf", "paperhash": "wang|snapquant_a_probabilistic_and_nested_parameterization_for_binary_networks", "_bibtex": "@misc{\nwang2019snapquant,\ntitle={SnapQuant: A Probabilistic and Nested Parameterization for Binary Networks},\nauthor={Kuan Wang and Hao Zhao and Anbang Yao and Aojun Zhou and Dawei Sun and Yurong Chen},\nyear={2019},\nurl={https://openreview.net/forum?id=B1ePui0ctQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper364/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621623021, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1ePui0ctQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper364/Authors", "ICLR.cc/2019/Conference/Paper364/Reviewers", "ICLR.cc/2019/Conference/Paper364/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper364/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper364/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper364/Authors|ICLR.cc/2019/Conference/Paper364/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper364/Reviewers", "ICLR.cc/2019/Conference/Paper364/Authors", "ICLR.cc/2019/Conference/Paper364/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621623021}}}, {"id": "S1lQQSF507", "original": null, "number": 2, "cdate": 1543308571306, "ddate": null, "tcdate": 1543308571306, "tmdate": 1543308571306, "tddate": null, "forum": "B1ePui0ctQ", "replyto": "BJxuov7q3X", "invitation": "ICLR.cc/2019/Conference/-/Paper364/Official_Comment", "content": {"title": "Thanks for the attentive reading and professional comments.", "comment": "We thank R1 for the attentive reading and professional comments. We admit that our experiments are insufficient and will improve them in a future version.\n\n- We thank R1 for pointing out the variance issue and the lack of experiments w.r.t. versatility. Fig.3-a/b is related to the variance issue. We will include a systematic variance evaluation into a future version. We will provide experiments w.r.t. model selection and uncertainty estimation to demonstrate the versatility.\n- We will give a more detailed discussion w.r.t. the variance of the gradient estimator, in a future version.\n- We will improve the experimental mythology in a future version, reporting mean values and confidence intervals.\n\n- Yes, the master network is bigger than the original network by 2~4 times of model size, but the master network is not designed for running on small devices, because what we actually need is the sampled binary network, which is efficient for small devices. We thank R1 for pointing out the possibility of exploring the generalization of the master network.\n- We assume the scenario in which we sample a network on a small validation set to address domain drift issues, then deploy sampled weights onto the device. Unfortunately, as rightfully pointed out by R1, this versatility is not shown in this manuscript. We will incorporate experiments showing this versatility in a future version.\n- As mentioned in the last bullet point, we expect the master network to give a better accuracy on a slightly different CIFAR, after sampling. However, since this is not validated in this manuscript, we admit it is just a potential advantage.\n- We will describe it more clearly in a future version.\n\n- Yes, the policy network is hierarchical upon layers/filters/kernels/weights and we design it for two purposes: a) make the distribution of binary weights related to each other based on their position in CNN architecture. b) reduce the model size of the policy network. We thank R1 for suggesting expressing f(*) with formulas. s is the globally shared state vector to the policy network.\n- We thank R1 for pointing out the issues w.r. MDPs. We will try to improve it.\n- We admit that our method is inspired by the stochastic version of BinaryConnect. If we do not use the nested structure and do not share the parameters of policy, our method will degenerate to a no-share one which is similar to BinaryConnect. Nevertheless, we find the stochastic version of BinaryConnect is hard to train, so we design the nested structure to improve it and use reinforce algorithm to bridge the output of the policy network and sampled binary weight as the sampling process is not differentiable. We admit the pseudo-reward is ad-hoc. We will try to dig deep into the design principle of this reward.\n- We will include experiments using other priors, in a future version.\n\n- We will give a more detailed description of the architecture.\n- We will include these missing numbers in a future version.\n- We will incorporate comparisons with Ternary Connect.\n- We will try to achieve a higher accuracy, perhaps by improving the implementation.\n- We will improve the figures in a future version\n- We will add citations after 'regardless of the availability'\n- We thank R1 for pointing out that experiments are not a separate contribution. We will rephrase this line in a future version.\n\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper364/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper364/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper364/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SnapQuant: A Probabilistic and Nested Parameterization for Binary Networks", "abstract": "In this paper, we study the problem of training real binary weight networks (without layer-wise or filter-wise scaling factors) from scratch under the Bayesian deep learning perspective, meaning that the final objective is to approximate the posterior distribution of binary weights rather than reach a point estimation. The proposed method, named as SnapQuant, has two intriguing features: (1) The posterior distribution is parameterized as a policy network trained with a reinforcement learning scheme. During the training phase, we generate binary weights on-the-fly since what we actually maintain is the policy network, and all the binary weights are used in a burn-after-reading style. At the testing phase, we can sample binary weight instances for a given recognition architecture from the learnt policy network. (2) The policy network, which has a nested parameter structure consisting of layer-wise, filter-wise and kernel-wise parameter sharing designs, is applicable to any neural network architecture. Such a nested parameterization explicitly and hierarchically models the joint posterior distribution of binary weights. The performance of SnapQuant is evaluated with several visual recognition tasks including ImageNet. The code will be made publicly available.", "keywords": ["Binary weight networks", "neural network quantization", "reinforcement learning"], "authorids": ["wangkuan15@mails.tsinghua.edu.cn", "hao.zhao@intel.com", "anbang.yao@intel.com", "aojun.zhou@intel.com", "dawei.sun@intel.com", "yurong.chen@intel.com"], "authors": ["Kuan Wang", "Hao Zhao", "Anbang Yao", "Aojun Zhou", "Dawei Sun", "Yurong Chen"], "TL;DR": "We propose SnapQuant, a reinforcement learning method for training binary weight networks from scratch under the Bayesian deep learning perspective, which approximates the posterior distribution of binary weights instead of a single point estimation.", "pdf": "/pdf/f7b3bc57b6782065c6b8e0ba325a402bb83f1eaa.pdf", "paperhash": "wang|snapquant_a_probabilistic_and_nested_parameterization_for_binary_networks", "_bibtex": "@misc{\nwang2019snapquant,\ntitle={SnapQuant: A Probabilistic and Nested Parameterization for Binary Networks},\nauthor={Kuan Wang and Hao Zhao and Anbang Yao and Aojun Zhou and Dawei Sun and Yurong Chen},\nyear={2019},\nurl={https://openreview.net/forum?id=B1ePui0ctQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper364/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621623021, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1ePui0ctQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper364/Authors", "ICLR.cc/2019/Conference/Paper364/Reviewers", "ICLR.cc/2019/Conference/Paper364/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper364/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper364/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper364/Authors|ICLR.cc/2019/Conference/Paper364/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper364/Reviewers", "ICLR.cc/2019/Conference/Paper364/Authors", "ICLR.cc/2019/Conference/Paper364/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621623021}}}, {"id": "BJxuov7q3X", "original": null, "number": 3, "cdate": 1541187487985, "ddate": null, "tcdate": 1541187487985, "tmdate": 1541534057085, "tddate": null, "forum": "B1ePui0ctQ", "replyto": "B1ePui0ctQ", "invitation": "ICLR.cc/2019/Conference/-/Paper364/Official_Review", "content": {"title": "Bayesian view on deep network binarization - interesting idea but lacks in clarity and experiments", "review": "The paper treats network binarization as learning a bernoulli probability that each weight is 0 or 1. It uses a hierarchical approach to factor the probability in to parameters shared between weights/kernels/layers. The result is a network that can be used to sample a binary network. The main novelty is that full-precision weights are not needed during training. \n\n- There is a variance issue underlying this idea. The goal is to find a binary network that has high performance, but the probabilistic formulation could have a high variance in terms of the performance of the resulting binary networks. In the experiments, the variance is not shown, rather the authors sample 100 networks and pick the best one based on validation set accuracy? (they just say \"pick the best one\"). Should this be accounted for in the objective function? The prior has very high variance (p=0.5). The authors claim that this \"demonstrates the versatility\" -- how?\n- Related to the above, there is a high variance associated with he REINFORCE estimator. In the appendix, the authors use a baseline value to alleviate this, but no discussion is provided in the main text or the experiments.  \n- For the stochastic version of binary connect, the authors report the best out of 100 trials. I would rather like to see the mean and confidence intervals, for this as well as the proposed method.\n\n- Please provide a comparison of the number of hyper parameters used vs the number of binary network parameters. Is it feasible to store the \"master network\" in memory for small devices? It seems you need more parameters than the original network as you have weight specific parameters + kernel/filter/layers specific parameters? Is there any generalization between these hyperparameters that can be shown in experiments e.g. using a compact hierarchy?\n- More generally, how do you see this method being used in practice? Do you sample each binary network on the device? \n- How is this better than other methods of training binary nets, which have better accuracy than your approach (according to Table 2)?\n- In the experiments, the hierarchical structure used for hyperparameters is not clearly described. \n\n- I found the exposition in Section 3.2 to be very confusing using f(*)  whereas it is very simply described in words. The policy network is hierarchical upon layers/filters/kernels/weights. What is \"s\" in equations (1) etc.?\n- In Section 3.3, I found the connection to MDPs tenuous, whereas it is easy to understand that you are using REINFORCE to estimate the gradient of the expectation. \n- The pseudo-reward is completely ad-hoc. Since this is a \"1-step MDP\" (bandit problem?), the reward is maximized when the probability of w=1 is related to the sign of the gradient. In the end, we seem to have arrived at something that is basically similar to BinaryConnect (i.e. using the sign of the gradient). \n- Only experiments with an uniform prior are shown. Can the prior be used in some ways?\n\n- In the experiments, the activation functions used are not described. \n- Some numbers are missing in Table 3.\n- One advantage of TernaryConnect is that it allows sparsity by allowing zero weights, whereas binary nets only allow +/- 1 weights. That is, Ternary Connect should not be dismissed simply and the performance of Ternary Connect should be shown on all the datasets.\n- The error for VGG on CIFAR-10 is very high compared to SOTA (close to 94% accuracy). \n- The legend on Figure 3 is not readable, the font size can be increased. \n- Page 1 \"regardless of the availability ...\" - citation needed. \n- Page 2, experiments are not a separate contribution.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper364/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SnapQuant: A Probabilistic and Nested Parameterization for Binary Networks", "abstract": "In this paper, we study the problem of training real binary weight networks (without layer-wise or filter-wise scaling factors) from scratch under the Bayesian deep learning perspective, meaning that the final objective is to approximate the posterior distribution of binary weights rather than reach a point estimation. The proposed method, named as SnapQuant, has two intriguing features: (1) The posterior distribution is parameterized as a policy network trained with a reinforcement learning scheme. During the training phase, we generate binary weights on-the-fly since what we actually maintain is the policy network, and all the binary weights are used in a burn-after-reading style. At the testing phase, we can sample binary weight instances for a given recognition architecture from the learnt policy network. (2) The policy network, which has a nested parameter structure consisting of layer-wise, filter-wise and kernel-wise parameter sharing designs, is applicable to any neural network architecture. Such a nested parameterization explicitly and hierarchically models the joint posterior distribution of binary weights. The performance of SnapQuant is evaluated with several visual recognition tasks including ImageNet. The code will be made publicly available.", "keywords": ["Binary weight networks", "neural network quantization", "reinforcement learning"], "authorids": ["wangkuan15@mails.tsinghua.edu.cn", "hao.zhao@intel.com", "anbang.yao@intel.com", "aojun.zhou@intel.com", "dawei.sun@intel.com", "yurong.chen@intel.com"], "authors": ["Kuan Wang", "Hao Zhao", "Anbang Yao", "Aojun Zhou", "Dawei Sun", "Yurong Chen"], "TL;DR": "We propose SnapQuant, a reinforcement learning method for training binary weight networks from scratch under the Bayesian deep learning perspective, which approximates the posterior distribution of binary weights instead of a single point estimation.", "pdf": "/pdf/f7b3bc57b6782065c6b8e0ba325a402bb83f1eaa.pdf", "paperhash": "wang|snapquant_a_probabilistic_and_nested_parameterization_for_binary_networks", "_bibtex": "@misc{\nwang2019snapquant,\ntitle={SnapQuant: A Probabilistic and Nested Parameterization for Binary Networks},\nauthor={Kuan Wang and Hao Zhao and Anbang Yao and Aojun Zhou and Dawei Sun and Yurong Chen},\nyear={2019},\nurl={https://openreview.net/forum?id=B1ePui0ctQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper364/Official_Review", "cdate": 1542234478121, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "B1ePui0ctQ", "replyto": "B1ePui0ctQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper364/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335707640, "tmdate": 1552335707640, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper364/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "S1lmiJ6_nQ", "original": null, "number": 2, "cdate": 1541095323071, "ddate": null, "tcdate": 1541095323071, "tmdate": 1541534056841, "tddate": null, "forum": "B1ePui0ctQ", "replyto": "B1ePui0ctQ", "invitation": "ICLR.cc/2019/Conference/-/Paper364/Official_Review", "content": {"title": "Need clarification on the experimental setting", "review": "This paper proposes to binarize all parameters of a CNN where the binary parameters are generated from another policy neural network (let's call it parameter generator). The parameter generator network has a special nested structure to regularize parameters within layers and filters. All parameters in CNN and parameter generator network are jointly trained. Since the gradient is hard to back propagated through binary variables, the paper adopts reinforcement learning approach to back-propagate rewards to the parameter generator. \n\nThe experiments look solid. The results show that the proposed approach is slightly worse than BinaryConnect (baseline) on MINST, CIFAR10 and CIFAR100, but outperforms BinaryConnect on ImageNet by a large margin. The ablation study also verified the need for the proposed nested parameter structure.\n\nThe paper is well written. The proposed method (1) is able to provide the posterior distribution of parameters so that we can use that in other applications such as confidence estimation and model selection (2) is memory and power efficient due to binarization. \n\nHaving said that, the experimental setting falls short. For the proposed method, the paper samples 100 binary networks from the parameter generator and pick the best one, which seems not correct. The results of the proposed method should be based on the average of these 100 binary networks, rather than picking the best one, because we won't be able to know which binary network is the best. Using the best one seems label leakage to me. For now, I'll give the benefit of doubt. Please clarify this during the feedback phase.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper364/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SnapQuant: A Probabilistic and Nested Parameterization for Binary Networks", "abstract": "In this paper, we study the problem of training real binary weight networks (without layer-wise or filter-wise scaling factors) from scratch under the Bayesian deep learning perspective, meaning that the final objective is to approximate the posterior distribution of binary weights rather than reach a point estimation. The proposed method, named as SnapQuant, has two intriguing features: (1) The posterior distribution is parameterized as a policy network trained with a reinforcement learning scheme. During the training phase, we generate binary weights on-the-fly since what we actually maintain is the policy network, and all the binary weights are used in a burn-after-reading style. At the testing phase, we can sample binary weight instances for a given recognition architecture from the learnt policy network. (2) The policy network, which has a nested parameter structure consisting of layer-wise, filter-wise and kernel-wise parameter sharing designs, is applicable to any neural network architecture. Such a nested parameterization explicitly and hierarchically models the joint posterior distribution of binary weights. The performance of SnapQuant is evaluated with several visual recognition tasks including ImageNet. The code will be made publicly available.", "keywords": ["Binary weight networks", "neural network quantization", "reinforcement learning"], "authorids": ["wangkuan15@mails.tsinghua.edu.cn", "hao.zhao@intel.com", "anbang.yao@intel.com", "aojun.zhou@intel.com", "dawei.sun@intel.com", "yurong.chen@intel.com"], "authors": ["Kuan Wang", "Hao Zhao", "Anbang Yao", "Aojun Zhou", "Dawei Sun", "Yurong Chen"], "TL;DR": "We propose SnapQuant, a reinforcement learning method for training binary weight networks from scratch under the Bayesian deep learning perspective, which approximates the posterior distribution of binary weights instead of a single point estimation.", "pdf": "/pdf/f7b3bc57b6782065c6b8e0ba325a402bb83f1eaa.pdf", "paperhash": "wang|snapquant_a_probabilistic_and_nested_parameterization_for_binary_networks", "_bibtex": "@misc{\nwang2019snapquant,\ntitle={SnapQuant: A Probabilistic and Nested Parameterization for Binary Networks},\nauthor={Kuan Wang and Hao Zhao and Anbang Yao and Aojun Zhou and Dawei Sun and Yurong Chen},\nyear={2019},\nurl={https://openreview.net/forum?id=B1ePui0ctQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper364/Official_Review", "cdate": 1542234478121, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "B1ePui0ctQ", "replyto": "B1ePui0ctQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper364/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335707640, "tmdate": 1552335707640, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper364/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "r1eSaciL27", "original": null, "number": 1, "cdate": 1540958908611, "ddate": null, "tcdate": 1540958908611, "tmdate": 1541534056634, "tddate": null, "forum": "B1ePui0ctQ", "replyto": "B1ePui0ctQ", "invitation": "ICLR.cc/2019/Conference/-/Paper364/Official_Review", "content": {"title": "learning binary weight neural networks using a structured variational approximation, gradients estimated using modified reinforce", "review": "Summary: The paper considers a variational inference strategy for learning neural networks with binary weights. In particular, the paper proposes using a structured recognition model to parameterise the variational distribution, which couples the weights in different layers/filters in a non-trivial way. The gradient of the expected likelihood term in the variational lower bound is estimated using the REINFORCE estimator. This paper adjusts this estimator to use the gradient of the log-likelihood wrt the samples. Experiments on several image classification tasks are provided.\n\nevaluation:\n\npros:\n- the idea of the proposed approach is interesting: using variational inference for binary weight neural networks. While recent work on VI for discrete variables only focused on discrete latent variable models, this work shows how VI can be used for binary neural networks.\n \ncons:\n- the writing, in my opinion, needs to be improved [see my comments below]. The VI presentation is cluttered and the justification of using the pseudo-reward for reinforce is not clear.\n- the experimental results are mixed and it's not clear to me how to interpret them/compare to the baselines -- what is the goal here: computational efficiency, compression or accuracy?\n\nSome specific questions/comments:\n\n+ What is the input of the policy/recognition network? It's not clear from the paper whether this includes the inputs of the current batch or outputs or both? If so, how are variable batch sizes handled? What is the input to this network at test time? In contrast to generative models/VAEs, the weights here are global parameters and it's not clear to me these should be varied for different data batches.\n\n+ related to the question above: how is prediction handled at test time? Say the parameters of the variational distribution over weights are generated using the recognition network, then 100 weights are sampled given these parameters which then give 100 predictions -- should these be then averaged out to get the final prediction? I'm not quite sure I understand why the paper chose to *pick the best one* out of 100 predictions and the justification/criterion for this procedure.  \n\n+ The writing is not very clear at places, and it does not help that the references being merged with the text. I'm also not sure about some of the technical jargons/terms used in the papers:\n- reinforcement learning: is this really a reinforcement learning problem? If you tackle this problem from a pure variational perspective, reinforce is used to obtain the gradient of the expected log-likelihood wrt the variational parameters. But instead of using the log likelihood, a learning signal that depends on the gradient of the log-likelihood is used.\n- concrete weights -- what are these? I assume they are just binary weights sampled from the variational approximation.\n- middle of page 3: p(w|X, Y) = p_\\theta(w): this is not precise as p_\\theta(w) is only an approximation to the exact posterior, which then allows us to lower bound the log marginal likelihood. \"common practice in modern variational approximation\": This is the standard way of deriving the lower bound and has been used for many years.\n\n+ the reinforce estimator tends to have high variances since it does not make use of the gradient of the function in the expectation. This paper adjusts the vanilla estimator with a learning signal that involves the gradient. Could you comment on the bias/variance trade-off of the resulting estimator? Much of recent literature on learning discrete variables, as far as I understand, propose ways to not to have to use the vanilla reinforce, for example Concrete, Relax or rebar, albeit the focus on latent variable models.\n\n+ model selection and uncertainty measure: the paper mentions these potential advantages of the proposed approach over deterministic binarisation schemes, but does not fully explore and test these.\n\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper364/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SnapQuant: A Probabilistic and Nested Parameterization for Binary Networks", "abstract": "In this paper, we study the problem of training real binary weight networks (without layer-wise or filter-wise scaling factors) from scratch under the Bayesian deep learning perspective, meaning that the final objective is to approximate the posterior distribution of binary weights rather than reach a point estimation. The proposed method, named as SnapQuant, has two intriguing features: (1) The posterior distribution is parameterized as a policy network trained with a reinforcement learning scheme. During the training phase, we generate binary weights on-the-fly since what we actually maintain is the policy network, and all the binary weights are used in a burn-after-reading style. At the testing phase, we can sample binary weight instances for a given recognition architecture from the learnt policy network. (2) The policy network, which has a nested parameter structure consisting of layer-wise, filter-wise and kernel-wise parameter sharing designs, is applicable to any neural network architecture. Such a nested parameterization explicitly and hierarchically models the joint posterior distribution of binary weights. The performance of SnapQuant is evaluated with several visual recognition tasks including ImageNet. The code will be made publicly available.", "keywords": ["Binary weight networks", "neural network quantization", "reinforcement learning"], "authorids": ["wangkuan15@mails.tsinghua.edu.cn", "hao.zhao@intel.com", "anbang.yao@intel.com", "aojun.zhou@intel.com", "dawei.sun@intel.com", "yurong.chen@intel.com"], "authors": ["Kuan Wang", "Hao Zhao", "Anbang Yao", "Aojun Zhou", "Dawei Sun", "Yurong Chen"], "TL;DR": "We propose SnapQuant, a reinforcement learning method for training binary weight networks from scratch under the Bayesian deep learning perspective, which approximates the posterior distribution of binary weights instead of a single point estimation.", "pdf": "/pdf/f7b3bc57b6782065c6b8e0ba325a402bb83f1eaa.pdf", "paperhash": "wang|snapquant_a_probabilistic_and_nested_parameterization_for_binary_networks", "_bibtex": "@misc{\nwang2019snapquant,\ntitle={SnapQuant: A Probabilistic and Nested Parameterization for Binary Networks},\nauthor={Kuan Wang and Hao Zhao and Anbang Yao and Aojun Zhou and Dawei Sun and Yurong Chen},\nyear={2019},\nurl={https://openreview.net/forum?id=B1ePui0ctQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper364/Official_Review", "cdate": 1542234478121, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "B1ePui0ctQ", "replyto": "B1ePui0ctQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper364/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335707640, "tmdate": 1552335707640, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper364/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 8}