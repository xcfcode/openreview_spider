{"notes": [{"id": "5jzlpHvvRk", "original": "GFmIxWtxR7R", "number": 1364, "cdate": 1601308152248, "ddate": null, "tcdate": 1601308152248, "tmdate": 1615624320952, "tddate": null, "forum": "5jzlpHvvRk", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Loss Function Discovery for Object Detection via Convergence-Simulation Driven Search", "authorids": ["~Peidong_Liu2", "~Gengwei_Zhang1", "~Bochao_Wang2", "~Hang_Xu1", "~Xiaodan_Liang2", "~Yong_Jiang3", "~Zhenguo_Li1"], "authors": ["Peidong Liu", "Gengwei Zhang", "Bochao Wang", "Hang Xu", "Xiaodan Liang", "Yong Jiang", "Zhenguo Li"], "keywords": ["Object detection", "AutoML", "Evolutionary algorithm", "Loss function search"], "abstract": "Designing proper loss functions for vision tasks has been a long-standing research direction to advance the capability of existing models. For object detection, the well-established classification and regression loss functions have been carefully designed by considering diverse learning challenges (e.g. class imbalance, hard negative samples, and scale variances). Inspired by the recent progress in network architecture search, it is interesting to explore the possibility of discovering new loss function formulations via directly searching the primitive operation combinations. So that the learned losses not only fit for diverse object detection challenges to alleviate huge human efforts, but also have better alignment with evaluation metric and good mathematical convergence property. Beyond the previous auto-loss works on face recognition and image classification, our work makes the first attempt to discover new loss functions for the challenging object detection from primitive operation levels and finds the searched losses are insightful. We propose an effective convergence-simulation driven evolutionary search algorithm, called CSE-Autoloss, for speeding up the search progress by regularizing the mathematical rationality of loss candidates via two progressive convergence simulation modules: convergence property verification and model optimization simulation. CSE-Autoloss involves the search space (i.e. 21 mathematical operators, 3 constant-type inputs, and 3 variable-type inputs) that cover a wide range of the possible variants of existing losses and discovers best-searched loss function combination within a short time (around 1.5 wall-clock days with 20x speedup in comparison to the vanilla evolutionary algorithm). We conduct extensive evaluations of loss function search on popular detectors and validate the good generalization capability of searched losses across diverse architectures and various datasets. Our experiments show that the best-discovered loss function combinations outperform default combinations (Cross-entropy/Focal loss for classification and L1 loss for regression) by 1.1% and 0.8% in terms of mAP for two-stage and one-stage detectors on COCO respectively. Our searched losses are available at https://github.com/PerdonLiu/CSE-Autoloss.", "one-sentence_summary": "We propose an effective convergence-simulation driven evolutionary search algorithm, called CSE-Autoloss, for object detection loss function discovery, which achieves 20x speedup via progressive convergence-simulation modules.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "liu|loss_function_discovery_for_object_detection_via_convergencesimulation_driven_search", "pdf": "/pdf/7980b823a80f5de80077d19818798a08c96ab635.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nliu2021loss,\ntitle={Loss Function Discovery for Object Detection via Convergence-Simulation Driven Search},\nauthor={Peidong Liu and Gengwei Zhang and Bochao Wang and Hang Xu and Xiaodan Liang and Yong Jiang and Zhenguo Li},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=5jzlpHvvRk}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "dn2bTg2QzM", "original": null, "number": 1, "cdate": 1610040465668, "ddate": null, "tcdate": 1610040465668, "tmdate": 1610474069231, "tddate": null, "forum": "5jzlpHvvRk", "replyto": "5jzlpHvvRk", "invitation": "ICLR.cc/2021/Conference/Paper1364/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Poster)", "comment": "This paper received borderline scores but overall lean positive. \n\nThe reviewers point out that the paper presents interesting new ideas and an effective solution to the problem of automatically searching for loss functions. The empirical results are convincing, although the baselines are not the strongest possible in terms of absolute performance. Overall, the ACs find that the paper has sufficient novelty and technical contribution to be accepted. "}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Loss Function Discovery for Object Detection via Convergence-Simulation Driven Search", "authorids": ["~Peidong_Liu2", "~Gengwei_Zhang1", "~Bochao_Wang2", "~Hang_Xu1", "~Xiaodan_Liang2", "~Yong_Jiang3", "~Zhenguo_Li1"], "authors": ["Peidong Liu", "Gengwei Zhang", "Bochao Wang", "Hang Xu", "Xiaodan Liang", "Yong Jiang", "Zhenguo Li"], "keywords": ["Object detection", "AutoML", "Evolutionary algorithm", "Loss function search"], "abstract": "Designing proper loss functions for vision tasks has been a long-standing research direction to advance the capability of existing models. For object detection, the well-established classification and regression loss functions have been carefully designed by considering diverse learning challenges (e.g. class imbalance, hard negative samples, and scale variances). Inspired by the recent progress in network architecture search, it is interesting to explore the possibility of discovering new loss function formulations via directly searching the primitive operation combinations. So that the learned losses not only fit for diverse object detection challenges to alleviate huge human efforts, but also have better alignment with evaluation metric and good mathematical convergence property. Beyond the previous auto-loss works on face recognition and image classification, our work makes the first attempt to discover new loss functions for the challenging object detection from primitive operation levels and finds the searched losses are insightful. We propose an effective convergence-simulation driven evolutionary search algorithm, called CSE-Autoloss, for speeding up the search progress by regularizing the mathematical rationality of loss candidates via two progressive convergence simulation modules: convergence property verification and model optimization simulation. CSE-Autoloss involves the search space (i.e. 21 mathematical operators, 3 constant-type inputs, and 3 variable-type inputs) that cover a wide range of the possible variants of existing losses and discovers best-searched loss function combination within a short time (around 1.5 wall-clock days with 20x speedup in comparison to the vanilla evolutionary algorithm). We conduct extensive evaluations of loss function search on popular detectors and validate the good generalization capability of searched losses across diverse architectures and various datasets. Our experiments show that the best-discovered loss function combinations outperform default combinations (Cross-entropy/Focal loss for classification and L1 loss for regression) by 1.1% and 0.8% in terms of mAP for two-stage and one-stage detectors on COCO respectively. Our searched losses are available at https://github.com/PerdonLiu/CSE-Autoloss.", "one-sentence_summary": "We propose an effective convergence-simulation driven evolutionary search algorithm, called CSE-Autoloss, for object detection loss function discovery, which achieves 20x speedup via progressive convergence-simulation modules.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "liu|loss_function_discovery_for_object_detection_via_convergencesimulation_driven_search", "pdf": "/pdf/7980b823a80f5de80077d19818798a08c96ab635.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nliu2021loss,\ntitle={Loss Function Discovery for Object Detection via Convergence-Simulation Driven Search},\nauthor={Peidong Liu and Gengwei Zhang and Bochao Wang and Hang Xu and Xiaodan Liang and Yong Jiang and Zhenguo Li},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=5jzlpHvvRk}\n}"}, "tags": [], "invitation": {"reply": {"forum": "5jzlpHvvRk", "replyto": "5jzlpHvvRk", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040465654, "tmdate": 1610474069214, "id": "ICLR.cc/2021/Conference/Paper1364/-/Decision"}}}, {"id": "NQQKsOgvNdZ", "original": null, "number": 1, "cdate": 1603855208936, "ddate": null, "tcdate": 1603855208936, "tmdate": 1605024463763, "tddate": null, "forum": "5jzlpHvvRk", "replyto": "5jzlpHvvRk", "invitation": "ICLR.cc/2021/Conference/Paper1364/-/Official_Review", "content": {"title": "Evolutionary algorithm for searching loss function for object detectors ", "review": "This paper proposes to use an evolutionary search algorithm to search for better loss functions for the classification and regression branch of an object detector. The algorithm starts with 20 primitive mathematical operations. Due to the highly sparse action space, the vanilla evolutionary algorithm would take a long time to converge. Then the authors propose two ways to reduce the search space. First, they filter out loss functions which generates gradients of large magnitude to well-classified samples and do not converge to zero. Second, they construct a very small dataset by sampling only one image randomly from each category and evaluate the loss function on it to quickly filter out bad loss candidates.\n\nPros:\nThe idea of applying evolutionary algorithm to search for a loss function for training an object detector is interesting and, to the best of my knowledge, new. The proposed way to reduce the search space seems to be effective. The authors apply the new loss function to other detectors, and it seems to generalize well to different detectors.\n\nCons:\nThe baselines provided in this paper are little bit weak. They are pretty far behind from the current state-of-the-art detectors. Although the new loss function is able to improve the performance of all of the baselines, it is unclear if it would generalize well to state-of-the-art results. The authors should consider using baselines with better performance. I also wonder if the loss would generalize to different datasets, such as PASCAL VOC.\n\nIn a subsection of section 3.1, \u201cEffectiveness of the Search Space\u201d, the authors mention that they introduce IoU into the cross entropy and focal loss, and show that they outperform the conventional cross entropy and focal loss. They then said this verifies the effectiveness of the search space. I am not seeing how this is connected to the effectiveness of the search space. Can the authors further elaborate on this?\n\nThe authors use the modified version of the cross entropy or focal loss as the initial loss in the search experiments. The final version of the loss is still pretty much the same as the initial loss. It seems that the search is biased towards to the initial loss. I wonder if the authors have tried other initializations.\n\nThe authors said that the regression branch contains fewer primitive operators because of the simplicity of the regression task. It is unclear which task, classification or regression, is actually simpler. It seems that regression may actually be harder as we have achieved human level results on the task of image classification but not on the detection, which requires precise localization of objects.", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper1364/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1364/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Loss Function Discovery for Object Detection via Convergence-Simulation Driven Search", "authorids": ["~Peidong_Liu2", "~Gengwei_Zhang1", "~Bochao_Wang2", "~Hang_Xu1", "~Xiaodan_Liang2", "~Yong_Jiang3", "~Zhenguo_Li1"], "authors": ["Peidong Liu", "Gengwei Zhang", "Bochao Wang", "Hang Xu", "Xiaodan Liang", "Yong Jiang", "Zhenguo Li"], "keywords": ["Object detection", "AutoML", "Evolutionary algorithm", "Loss function search"], "abstract": "Designing proper loss functions for vision tasks has been a long-standing research direction to advance the capability of existing models. For object detection, the well-established classification and regression loss functions have been carefully designed by considering diverse learning challenges (e.g. class imbalance, hard negative samples, and scale variances). Inspired by the recent progress in network architecture search, it is interesting to explore the possibility of discovering new loss function formulations via directly searching the primitive operation combinations. So that the learned losses not only fit for diverse object detection challenges to alleviate huge human efforts, but also have better alignment with evaluation metric and good mathematical convergence property. Beyond the previous auto-loss works on face recognition and image classification, our work makes the first attempt to discover new loss functions for the challenging object detection from primitive operation levels and finds the searched losses are insightful. We propose an effective convergence-simulation driven evolutionary search algorithm, called CSE-Autoloss, for speeding up the search progress by regularizing the mathematical rationality of loss candidates via two progressive convergence simulation modules: convergence property verification and model optimization simulation. CSE-Autoloss involves the search space (i.e. 21 mathematical operators, 3 constant-type inputs, and 3 variable-type inputs) that cover a wide range of the possible variants of existing losses and discovers best-searched loss function combination within a short time (around 1.5 wall-clock days with 20x speedup in comparison to the vanilla evolutionary algorithm). We conduct extensive evaluations of loss function search on popular detectors and validate the good generalization capability of searched losses across diverse architectures and various datasets. Our experiments show that the best-discovered loss function combinations outperform default combinations (Cross-entropy/Focal loss for classification and L1 loss for regression) by 1.1% and 0.8% in terms of mAP for two-stage and one-stage detectors on COCO respectively. Our searched losses are available at https://github.com/PerdonLiu/CSE-Autoloss.", "one-sentence_summary": "We propose an effective convergence-simulation driven evolutionary search algorithm, called CSE-Autoloss, for object detection loss function discovery, which achieves 20x speedup via progressive convergence-simulation modules.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "liu|loss_function_discovery_for_object_detection_via_convergencesimulation_driven_search", "pdf": "/pdf/7980b823a80f5de80077d19818798a08c96ab635.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nliu2021loss,\ntitle={Loss Function Discovery for Object Detection via Convergence-Simulation Driven Search},\nauthor={Peidong Liu and Gengwei Zhang and Bochao Wang and Hang Xu and Xiaodan Liang and Yong Jiang and Zhenguo Li},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=5jzlpHvvRk}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "5jzlpHvvRk", "replyto": "5jzlpHvvRk", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1364/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538120375, "tmdate": 1606915790876, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1364/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1364/-/Official_Review"}}}, {"id": "2hvAPw350MM", "original": null, "number": 2, "cdate": 1603869992566, "ddate": null, "tcdate": 1603869992566, "tmdate": 1605024463702, "tddate": null, "forum": "5jzlpHvvRk", "replyto": "5jzlpHvvRk", "invitation": "ICLR.cc/2021/Conference/Paper1364/-/Official_Review", "content": {"title": "Official Blind Review #4", "review": "Summary:\n\nIn this paper, the authors propose CSE-Autoloss to search loss functions for object detection. A well-designed search space is carefully designed to explore novel loss functions. In order to reduce the search overhead, convergence-simulation modules are raised.\n\nReasons for score:\n\nOverall, I vote for accepting. My major concern is about the clarity of the paper and some additional experiments (see cons below). Hopefully the authors can address my concern in the rebuttal period.\n\nPros:\n\n1.This paper proposes a new framework to search loss functions for object detection. For me, it is the first attempt to discover loss functions for object detection automatically.\n\n2.To search the novel loss functions for object detection, the authors propose a well-designed search space.\n\n3.To reduce the search overhead caused by evolutionary algorithm, this paper raises two modules: convergence property verification module and model optimization simulation module. \n\nCons:\n\n1.For the predefined initial loss, CEI/FLI loss and GioU loss are used for classification and regression, respectively. If using other losses as the predefined initial loss, how is the performance? Could you provide some experiments?\n\n2.In the search experiments, regression and classification loss functions are searched independently. For me, this can not result in the optimal loss functions. Why does not search jointly?\n\n3.In the evolutionary algorithm, how do the probabilities of cross-over and mutation affect the performance of searched loss?\n\n4.In the experiments, there are some results to verify the generalization of the searched loss on different detectors and datasets. If directly search loss functions on these detectors and datasets, how are the performances?\n\nQuestions during rebuttal period:\n\nPlease address and clarify the cons above", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1364/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1364/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Loss Function Discovery for Object Detection via Convergence-Simulation Driven Search", "authorids": ["~Peidong_Liu2", "~Gengwei_Zhang1", "~Bochao_Wang2", "~Hang_Xu1", "~Xiaodan_Liang2", "~Yong_Jiang3", "~Zhenguo_Li1"], "authors": ["Peidong Liu", "Gengwei Zhang", "Bochao Wang", "Hang Xu", "Xiaodan Liang", "Yong Jiang", "Zhenguo Li"], "keywords": ["Object detection", "AutoML", "Evolutionary algorithm", "Loss function search"], "abstract": "Designing proper loss functions for vision tasks has been a long-standing research direction to advance the capability of existing models. For object detection, the well-established classification and regression loss functions have been carefully designed by considering diverse learning challenges (e.g. class imbalance, hard negative samples, and scale variances). Inspired by the recent progress in network architecture search, it is interesting to explore the possibility of discovering new loss function formulations via directly searching the primitive operation combinations. So that the learned losses not only fit for diverse object detection challenges to alleviate huge human efforts, but also have better alignment with evaluation metric and good mathematical convergence property. Beyond the previous auto-loss works on face recognition and image classification, our work makes the first attempt to discover new loss functions for the challenging object detection from primitive operation levels and finds the searched losses are insightful. We propose an effective convergence-simulation driven evolutionary search algorithm, called CSE-Autoloss, for speeding up the search progress by regularizing the mathematical rationality of loss candidates via two progressive convergence simulation modules: convergence property verification and model optimization simulation. CSE-Autoloss involves the search space (i.e. 21 mathematical operators, 3 constant-type inputs, and 3 variable-type inputs) that cover a wide range of the possible variants of existing losses and discovers best-searched loss function combination within a short time (around 1.5 wall-clock days with 20x speedup in comparison to the vanilla evolutionary algorithm). We conduct extensive evaluations of loss function search on popular detectors and validate the good generalization capability of searched losses across diverse architectures and various datasets. Our experiments show that the best-discovered loss function combinations outperform default combinations (Cross-entropy/Focal loss for classification and L1 loss for regression) by 1.1% and 0.8% in terms of mAP for two-stage and one-stage detectors on COCO respectively. Our searched losses are available at https://github.com/PerdonLiu/CSE-Autoloss.", "one-sentence_summary": "We propose an effective convergence-simulation driven evolutionary search algorithm, called CSE-Autoloss, for object detection loss function discovery, which achieves 20x speedup via progressive convergence-simulation modules.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "liu|loss_function_discovery_for_object_detection_via_convergencesimulation_driven_search", "pdf": "/pdf/7980b823a80f5de80077d19818798a08c96ab635.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nliu2021loss,\ntitle={Loss Function Discovery for Object Detection via Convergence-Simulation Driven Search},\nauthor={Peidong Liu and Gengwei Zhang and Bochao Wang and Hang Xu and Xiaodan Liang and Yong Jiang and Zhenguo Li},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=5jzlpHvvRk}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "5jzlpHvvRk", "replyto": "5jzlpHvvRk", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1364/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538120375, "tmdate": 1606915790876, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1364/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1364/-/Official_Review"}}}, {"id": "rDnT1FW45T", "original": null, "number": 3, "cdate": 1603945960677, "ddate": null, "tcdate": 1603945960677, "tmdate": 1605024463641, "tddate": null, "forum": "5jzlpHvvRk", "replyto": "5jzlpHvvRk", "invitation": "ICLR.cc/2021/Conference/Paper1364/-/Official_Review", "content": {"title": "An effective convergence-simulation driven evolutionary algorithm for searching loss function for object detection", "review": "This paper proposes CSE-Autoloss to deal with searching loss function for object detection with an effective convergence-simulation driven evolutionary search algorithm. CSE-Autoloss outperforms both previous CE/FL loss for classification and L1 loss for regression.\n\nPros:\n1. Motivation is clear and good.\nLoss design is tricky in object detection, since for it needs to deal with class imbalance and fore/background imbalance for classification loss and regression loss. This paper motivated by network architecture search, proposes effective searching algorithm for loss function, which is impressive and novel.\n\n2. Convergence property verification and model optimization simulation are simple and effective.\nPrevious NAS papers usually require large computation resources and time to search the best model. CSE-Autoloss employs monotonicity and convergence of classification loss to and a small verity dataset to easily filter out loss candidate, it speedups the searching process by 10x and reaches similar results with random research in Table 5.\n\nCons:\n1. Some related works on improving Focal loss and GIoU [1][2] are missing, it is better to include them as parts of hand-crafted loss functions for comparisons.\n\n2.Is it possible to search classification and regression loss together? It seems for now classification and regression loss are searched independently.\n\n3. Different formulations of CSE-Autoloss-A_cls and CSE-Autoloss-B_cls.\nI understand that formulation of Autoloss_A_reg can be different from Autoloss_B_reg since their hand-crafted counterpart L1 & IoU loss are different, but for classification loss, FL loss is CE loss with a simple modulating factor, but it seems Autoloss-A_cls and Autoloss-B_cls design are quiet different. Any possible explanation for this difference?\n\n[1] Distance-IoU Loss: Faster and Better Learning for Bounding Box Regression, in AAAI 2020\n[2] Gradient Harmonized Single-stage Detector, in AAAI 2019", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1364/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1364/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Loss Function Discovery for Object Detection via Convergence-Simulation Driven Search", "authorids": ["~Peidong_Liu2", "~Gengwei_Zhang1", "~Bochao_Wang2", "~Hang_Xu1", "~Xiaodan_Liang2", "~Yong_Jiang3", "~Zhenguo_Li1"], "authors": ["Peidong Liu", "Gengwei Zhang", "Bochao Wang", "Hang Xu", "Xiaodan Liang", "Yong Jiang", "Zhenguo Li"], "keywords": ["Object detection", "AutoML", "Evolutionary algorithm", "Loss function search"], "abstract": "Designing proper loss functions for vision tasks has been a long-standing research direction to advance the capability of existing models. For object detection, the well-established classification and regression loss functions have been carefully designed by considering diverse learning challenges (e.g. class imbalance, hard negative samples, and scale variances). Inspired by the recent progress in network architecture search, it is interesting to explore the possibility of discovering new loss function formulations via directly searching the primitive operation combinations. So that the learned losses not only fit for diverse object detection challenges to alleviate huge human efforts, but also have better alignment with evaluation metric and good mathematical convergence property. Beyond the previous auto-loss works on face recognition and image classification, our work makes the first attempt to discover new loss functions for the challenging object detection from primitive operation levels and finds the searched losses are insightful. We propose an effective convergence-simulation driven evolutionary search algorithm, called CSE-Autoloss, for speeding up the search progress by regularizing the mathematical rationality of loss candidates via two progressive convergence simulation modules: convergence property verification and model optimization simulation. CSE-Autoloss involves the search space (i.e. 21 mathematical operators, 3 constant-type inputs, and 3 variable-type inputs) that cover a wide range of the possible variants of existing losses and discovers best-searched loss function combination within a short time (around 1.5 wall-clock days with 20x speedup in comparison to the vanilla evolutionary algorithm). We conduct extensive evaluations of loss function search on popular detectors and validate the good generalization capability of searched losses across diverse architectures and various datasets. Our experiments show that the best-discovered loss function combinations outperform default combinations (Cross-entropy/Focal loss for classification and L1 loss for regression) by 1.1% and 0.8% in terms of mAP for two-stage and one-stage detectors on COCO respectively. Our searched losses are available at https://github.com/PerdonLiu/CSE-Autoloss.", "one-sentence_summary": "We propose an effective convergence-simulation driven evolutionary search algorithm, called CSE-Autoloss, for object detection loss function discovery, which achieves 20x speedup via progressive convergence-simulation modules.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "liu|loss_function_discovery_for_object_detection_via_convergencesimulation_driven_search", "pdf": "/pdf/7980b823a80f5de80077d19818798a08c96ab635.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nliu2021loss,\ntitle={Loss Function Discovery for Object Detection via Convergence-Simulation Driven Search},\nauthor={Peidong Liu and Gengwei Zhang and Bochao Wang and Hang Xu and Xiaodan Liang and Yong Jiang and Zhenguo Li},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=5jzlpHvvRk}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "5jzlpHvvRk", "replyto": "5jzlpHvvRk", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1364/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538120375, "tmdate": 1606915790876, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1364/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1364/-/Official_Review"}}}, {"id": "rF9SacaL9RF", "original": null, "number": 4, "cdate": 1604304694236, "ddate": null, "tcdate": 1604304694236, "tmdate": 1605024463574, "tddate": null, "forum": "5jzlpHvvRk", "replyto": "5jzlpHvvRk", "invitation": "ICLR.cc/2021/Conference/Paper1364/-/Official_Review", "content": {"title": "Incremental improvements based on previous auto loss for image classification", "review": "This paper proposes to automatically discover proper loss functions for object detection. It first designs some unit mathematical operations as search space, and then performs evolutionary algorithm to discover well-performed loss functions for the object detection tasks. Different from image classification, one needs to search both classification and localization losses in object detection. To accelerate the search, the paper proposes convergence property verification and model optimization simulation to effectively evaluate the searched loss and reduce the search space.\n\nOverall, this paper brings new ideas and experimental conclusions to the auto loss discovery in the object detection area. However, the proposed method is heavily based on the works of auto loss for image classification, i.e. \"Improved Training Speed, Accuracy, and Data Utilization Through Loss Function Optimization\". Some key differences lies on the acceleration by checking the Monotonicity and Convergence of the searched loss. Another issue I am curious about is how the method balances between classification and localization losses. Independently searching either cls or localization loss does not guarantee a good performance when combining them together for a high AP. Finally, the searched loss formulation seems complex and tricky, thus raising the doubt of its generalization across a wide range of neural networks and datasets. Do we need to re-search the loss function once we change the network architecture or the training data slightly?\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1364/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1364/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Loss Function Discovery for Object Detection via Convergence-Simulation Driven Search", "authorids": ["~Peidong_Liu2", "~Gengwei_Zhang1", "~Bochao_Wang2", "~Hang_Xu1", "~Xiaodan_Liang2", "~Yong_Jiang3", "~Zhenguo_Li1"], "authors": ["Peidong Liu", "Gengwei Zhang", "Bochao Wang", "Hang Xu", "Xiaodan Liang", "Yong Jiang", "Zhenguo Li"], "keywords": ["Object detection", "AutoML", "Evolutionary algorithm", "Loss function search"], "abstract": "Designing proper loss functions for vision tasks has been a long-standing research direction to advance the capability of existing models. For object detection, the well-established classification and regression loss functions have been carefully designed by considering diverse learning challenges (e.g. class imbalance, hard negative samples, and scale variances). Inspired by the recent progress in network architecture search, it is interesting to explore the possibility of discovering new loss function formulations via directly searching the primitive operation combinations. So that the learned losses not only fit for diverse object detection challenges to alleviate huge human efforts, but also have better alignment with evaluation metric and good mathematical convergence property. Beyond the previous auto-loss works on face recognition and image classification, our work makes the first attempt to discover new loss functions for the challenging object detection from primitive operation levels and finds the searched losses are insightful. We propose an effective convergence-simulation driven evolutionary search algorithm, called CSE-Autoloss, for speeding up the search progress by regularizing the mathematical rationality of loss candidates via two progressive convergence simulation modules: convergence property verification and model optimization simulation. CSE-Autoloss involves the search space (i.e. 21 mathematical operators, 3 constant-type inputs, and 3 variable-type inputs) that cover a wide range of the possible variants of existing losses and discovers best-searched loss function combination within a short time (around 1.5 wall-clock days with 20x speedup in comparison to the vanilla evolutionary algorithm). We conduct extensive evaluations of loss function search on popular detectors and validate the good generalization capability of searched losses across diverse architectures and various datasets. Our experiments show that the best-discovered loss function combinations outperform default combinations (Cross-entropy/Focal loss for classification and L1 loss for regression) by 1.1% and 0.8% in terms of mAP for two-stage and one-stage detectors on COCO respectively. Our searched losses are available at https://github.com/PerdonLiu/CSE-Autoloss.", "one-sentence_summary": "We propose an effective convergence-simulation driven evolutionary search algorithm, called CSE-Autoloss, for object detection loss function discovery, which achieves 20x speedup via progressive convergence-simulation modules.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "liu|loss_function_discovery_for_object_detection_via_convergencesimulation_driven_search", "pdf": "/pdf/7980b823a80f5de80077d19818798a08c96ab635.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nliu2021loss,\ntitle={Loss Function Discovery for Object Detection via Convergence-Simulation Driven Search},\nauthor={Peidong Liu and Gengwei Zhang and Bochao Wang and Hang Xu and Xiaodan Liang and Yong Jiang and Zhenguo Li},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=5jzlpHvvRk}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "5jzlpHvvRk", "replyto": "5jzlpHvvRk", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1364/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538120375, "tmdate": 1606915790876, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1364/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1364/-/Official_Review"}}}], "count": 6}