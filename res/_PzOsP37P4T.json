{"notes": [{"id": "_PzOsP37P4T", "original": "leINdOlO8fQ", "number": 503, "cdate": 1601308062889, "ddate": null, "tcdate": 1601308062889, "tmdate": 1614985655023, "tddate": null, "forum": "_PzOsP37P4T", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Generalized Gumbel-Softmax Gradient Estimator for Generic Discrete Random Variables", "authorids": ["~Weonyoung_Joo1", "~Dongjun_Kim1", "~Seungjae_Shin1", "~Il-chul_Moon1"], "authors": ["Weonyoung Joo", "Dongjun Kim", "Seungjae Shin", "Il-chul Moon"], "keywords": ["Deep learning", "Deep generative model", "Unsupervised learning", "Gradient estimator", "Reparameterization trick", "Discrete distribution", "Gumbel-Softmax"], "abstract": "Estimating the gradients of stochastic nodes, which enables the gradient descent optimization on neural network parameters, is one of the crucial research questions in the deep generative modeling community. When it comes to discrete distributions, Gumbel-Softmax trick reparameterizes Bernoulli and categorical random variables by continuous relaxation. However, gradient estimators of discrete distributions other than the Bernoulli and the categorical have not been explored, and the the Gumbel-Softmax trick is not directly applicable to other discrete distributions. This paper proposes a general version of the Gumbel-Softmax estimator with a theoretical basis, and the proposed estimator is able to reparameterize generic discrete distributions, broader than the Bernoulli and the categorical. In detail, we utilize the truncation of discrete random variables and the Gumbel-Softmax trick with a linear transformation for the relaxed reparameterization. The proposed approach enables the relaxed discrete random variable to be reparameterized through a large-scale stochastic computational graph. Our experiments consist of (1) synthetic data analyses and applications on VAE, which show the efficacy of our methods; and (2) topic models, which demonstrate the value of the proposed estimation in practice.", "one-sentence_summary": "We present a generalized version of Gumbel-Softmax reparameterization trick, which enables estimating the gradients of discrete random nodes in stochastic computational graphs.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "joo|generalized_gumbelsoftmax_gradient_estimator_for_generic_discrete_random_variables", "supplementary_material": "", "pdf": "/pdf/9a9eec1510c6518dcc3e68bff75b905714415f5c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=978VXoFitL", "_bibtex": "@misc{\njoo2021generalized,\ntitle={Generalized Gumbel-Softmax Gradient Estimator for Generic Discrete Random Variables},\nauthor={Weonyoung Joo and Dongjun Kim and Seungjae Shin and Il-chul Moon},\nyear={2021},\nurl={https://openreview.net/forum?id=_PzOsP37P4T}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "XpTooy26Hx1", "original": null, "number": 1, "cdate": 1610040507478, "ddate": null, "tcdate": 1610040507478, "tmdate": 1610474114805, "tddate": null, "forum": "_PzOsP37P4T", "replyto": "_PzOsP37P4T", "invitation": "ICLR.cc/2021/Conference/Paper503/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "All reviewers agree the paper does not meet the acceptance bar, and an authors' rebuttal is not available. Therefore, I recommend rejection."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generalized Gumbel-Softmax Gradient Estimator for Generic Discrete Random Variables", "authorids": ["~Weonyoung_Joo1", "~Dongjun_Kim1", "~Seungjae_Shin1", "~Il-chul_Moon1"], "authors": ["Weonyoung Joo", "Dongjun Kim", "Seungjae Shin", "Il-chul Moon"], "keywords": ["Deep learning", "Deep generative model", "Unsupervised learning", "Gradient estimator", "Reparameterization trick", "Discrete distribution", "Gumbel-Softmax"], "abstract": "Estimating the gradients of stochastic nodes, which enables the gradient descent optimization on neural network parameters, is one of the crucial research questions in the deep generative modeling community. When it comes to discrete distributions, Gumbel-Softmax trick reparameterizes Bernoulli and categorical random variables by continuous relaxation. However, gradient estimators of discrete distributions other than the Bernoulli and the categorical have not been explored, and the the Gumbel-Softmax trick is not directly applicable to other discrete distributions. This paper proposes a general version of the Gumbel-Softmax estimator with a theoretical basis, and the proposed estimator is able to reparameterize generic discrete distributions, broader than the Bernoulli and the categorical. In detail, we utilize the truncation of discrete random variables and the Gumbel-Softmax trick with a linear transformation for the relaxed reparameterization. The proposed approach enables the relaxed discrete random variable to be reparameterized through a large-scale stochastic computational graph. Our experiments consist of (1) synthetic data analyses and applications on VAE, which show the efficacy of our methods; and (2) topic models, which demonstrate the value of the proposed estimation in practice.", "one-sentence_summary": "We present a generalized version of Gumbel-Softmax reparameterization trick, which enables estimating the gradients of discrete random nodes in stochastic computational graphs.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "joo|generalized_gumbelsoftmax_gradient_estimator_for_generic_discrete_random_variables", "supplementary_material": "", "pdf": "/pdf/9a9eec1510c6518dcc3e68bff75b905714415f5c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=978VXoFitL", "_bibtex": "@misc{\njoo2021generalized,\ntitle={Generalized Gumbel-Softmax Gradient Estimator for Generic Discrete Random Variables},\nauthor={Weonyoung Joo and Dongjun Kim and Seungjae Shin and Il-chul Moon},\nyear={2021},\nurl={https://openreview.net/forum?id=_PzOsP37P4T}\n}"}, "tags": [], "invitation": {"reply": {"forum": "_PzOsP37P4T", "replyto": "_PzOsP37P4T", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040507465, "tmdate": 1610474114789, "id": "ICLR.cc/2021/Conference/Paper503/-/Decision"}}}, {"id": "4cP2YiYgay", "original": null, "number": 1, "cdate": 1603751350796, "ddate": null, "tcdate": 1603751350796, "tmdate": 1605024674265, "tddate": null, "forum": "_PzOsP37P4T", "replyto": "_PzOsP37P4T", "invitation": "ICLR.cc/2021/Conference/Paper503/-/Official_Review", "content": {"title": "Marginal contribution, unclear what benefits are", "review": "This paper presents a new methodology for inference of discrete variables in computational graphs.\n\nOverall the paper represents a substantial amount of work, and it is clearly written. Nonetheless, I believe it is not significant enough so I can recommend acceptance.\n\n1)Theoretical analysis, while pertinent, is mostly elementary and unsurprising. I would have expected something closer to rates of convergence but the kind of analysis presented (almost sure convergence when the truncation size goes to infinity) is essentially  obvious for anyone who was taken an elementary course on probability/measure theory. I would expect theoretical results to enlighten some aspects of their methods, but that is absent.\n\n2)I am in general unconvinced by the method. I don't think there is anything specially novel besides truncation. But just proposing truncation is not enough merit, I believe, as truncation is a natural choice. I would have expected an analysis of how truncation affects the results.\n\n3)I am not convinced by experimental results. Authors don't include Gumbel-Softmax as a baseline and I wonder whether any goodness in reported results (Besides the topic model) is a consequence of the action of Gumbel-Softmax instead of their method.\n\n4)The topic modeling experiment is interesting, since it involves a poisson distribution, with infinitely many possible values. However,  there, the relevant comparison is missing. This alternative could be, for example, the detr method (Liu et al, 2019). Notice that I disagree with the authors claim that this method is doing something completely unique. The Liu et al paper can be used with similar purposes.", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper503/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper503/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generalized Gumbel-Softmax Gradient Estimator for Generic Discrete Random Variables", "authorids": ["~Weonyoung_Joo1", "~Dongjun_Kim1", "~Seungjae_Shin1", "~Il-chul_Moon1"], "authors": ["Weonyoung Joo", "Dongjun Kim", "Seungjae Shin", "Il-chul Moon"], "keywords": ["Deep learning", "Deep generative model", "Unsupervised learning", "Gradient estimator", "Reparameterization trick", "Discrete distribution", "Gumbel-Softmax"], "abstract": "Estimating the gradients of stochastic nodes, which enables the gradient descent optimization on neural network parameters, is one of the crucial research questions in the deep generative modeling community. When it comes to discrete distributions, Gumbel-Softmax trick reparameterizes Bernoulli and categorical random variables by continuous relaxation. However, gradient estimators of discrete distributions other than the Bernoulli and the categorical have not been explored, and the the Gumbel-Softmax trick is not directly applicable to other discrete distributions. This paper proposes a general version of the Gumbel-Softmax estimator with a theoretical basis, and the proposed estimator is able to reparameterize generic discrete distributions, broader than the Bernoulli and the categorical. In detail, we utilize the truncation of discrete random variables and the Gumbel-Softmax trick with a linear transformation for the relaxed reparameterization. The proposed approach enables the relaxed discrete random variable to be reparameterized through a large-scale stochastic computational graph. Our experiments consist of (1) synthetic data analyses and applications on VAE, which show the efficacy of our methods; and (2) topic models, which demonstrate the value of the proposed estimation in practice.", "one-sentence_summary": "We present a generalized version of Gumbel-Softmax reparameterization trick, which enables estimating the gradients of discrete random nodes in stochastic computational graphs.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "joo|generalized_gumbelsoftmax_gradient_estimator_for_generic_discrete_random_variables", "supplementary_material": "", "pdf": "/pdf/9a9eec1510c6518dcc3e68bff75b905714415f5c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=978VXoFitL", "_bibtex": "@misc{\njoo2021generalized,\ntitle={Generalized Gumbel-Softmax Gradient Estimator for Generic Discrete Random Variables},\nauthor={Weonyoung Joo and Dongjun Kim and Seungjae Shin and Il-chul Moon},\nyear={2021},\nurl={https://openreview.net/forum?id=_PzOsP37P4T}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "_PzOsP37P4T", "replyto": "_PzOsP37P4T", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper503/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538141698, "tmdate": 1606915802224, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper503/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper503/-/Official_Review"}}}, {"id": "k2c67mCeww", "original": null, "number": 2, "cdate": 1603797467421, "ddate": null, "tcdate": 1603797467421, "tmdate": 1605024674195, "tddate": null, "forum": "_PzOsP37P4T", "replyto": "_PzOsP37P4T", "invitation": "ICLR.cc/2021/Conference/Paper503/-/Official_Review", "content": {"title": "Interesting approach, but have concerns about significance", "review": "Summary: The paper presents a generalization of the Gumbel-Softmax gradient estimator. The original Gumbel-Softmax is usually applied to Bernoulli and categorical random variables. The method proposed in the paper attempts to extend it applicability to other discrete distributions, such as Poisson, multinomial, geometric, among others. The main ideas of the approach are: (1) Random variables that may take countably infinite values are truncated, (2) The sampling process of the random variable is converted to a one-hot scheme (where Gumbel-Softmax relaxation is applied), (3) ``One-hot'' samples are reverted to the original sample space.\n\nClarity: While there's some room for improvement, I think the paper is clear.\n\nPros:\n- The paper tackles a very relevant problem; namely, estimating gradients with respect to the parameters of general discrete random variables. These distributions are used in many scenarios, and some of the existing methods are not always applicable. This paper aims to extend some of the current methods (specifically, Gumbel-Softmax) to general discrete distributions.\n\nCons:\nI have concerns about the method's significance.\n- The main claim is that the proposed method generalizes the Gumbel-Softmax estimator to general discrete distributions. However, I don't see how the method leads to an efficient algorithm for distributions with some combinatorial component. For instance, take the multinomial distribution with parameters (n, k). The number of possible outcomes is finite, but grows fast with n and k. Without truncation, the method can be applied by assigning each possible outcome to a one-hot vector, which becomes very large for moderate values of n and k. An alternative would involve removing potential outcomes with very low probability (i.e., truncating, as suggested in the paper). However the paper does not explain how to apply the truncation in cases like this. The only experiment in the paper with multinomial distributions involves small values of n and k (3 for both), and thus this issue does not arise.\n- The multinomial distribution is just one example where the issue mentioned above occurs. There are several other distributions for which this happens (e.g. a discrete distribution over tree structures), and it seems that no solution is proposed. If I understood the approach correctly, and this is the case, I think that a discussion of this should be included in the main paper. Is there a way to deal with this using the proposed approach?\n- For other non-combinatorial cases (e.g. Poisson, geometric), where the proposed approach may be applied efficiently, the method reduces to truncating the support of the random variable, which may be seen as a simple heuristic.\n\nIn summary, as presented, I think the paper does not provide an efficient approach to deal with general discrete distributions, and reduces to a simple heuristic for some of the simpler cases mentioned above. I think extending the approach to other more challenging distributions (multinomial, etc) could be an interesting direction to explore.\n\nRecommendation: Reject.", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper503/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper503/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generalized Gumbel-Softmax Gradient Estimator for Generic Discrete Random Variables", "authorids": ["~Weonyoung_Joo1", "~Dongjun_Kim1", "~Seungjae_Shin1", "~Il-chul_Moon1"], "authors": ["Weonyoung Joo", "Dongjun Kim", "Seungjae Shin", "Il-chul Moon"], "keywords": ["Deep learning", "Deep generative model", "Unsupervised learning", "Gradient estimator", "Reparameterization trick", "Discrete distribution", "Gumbel-Softmax"], "abstract": "Estimating the gradients of stochastic nodes, which enables the gradient descent optimization on neural network parameters, is one of the crucial research questions in the deep generative modeling community. When it comes to discrete distributions, Gumbel-Softmax trick reparameterizes Bernoulli and categorical random variables by continuous relaxation. However, gradient estimators of discrete distributions other than the Bernoulli and the categorical have not been explored, and the the Gumbel-Softmax trick is not directly applicable to other discrete distributions. This paper proposes a general version of the Gumbel-Softmax estimator with a theoretical basis, and the proposed estimator is able to reparameterize generic discrete distributions, broader than the Bernoulli and the categorical. In detail, we utilize the truncation of discrete random variables and the Gumbel-Softmax trick with a linear transformation for the relaxed reparameterization. The proposed approach enables the relaxed discrete random variable to be reparameterized through a large-scale stochastic computational graph. Our experiments consist of (1) synthetic data analyses and applications on VAE, which show the efficacy of our methods; and (2) topic models, which demonstrate the value of the proposed estimation in practice.", "one-sentence_summary": "We present a generalized version of Gumbel-Softmax reparameterization trick, which enables estimating the gradients of discrete random nodes in stochastic computational graphs.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "joo|generalized_gumbelsoftmax_gradient_estimator_for_generic_discrete_random_variables", "supplementary_material": "", "pdf": "/pdf/9a9eec1510c6518dcc3e68bff75b905714415f5c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=978VXoFitL", "_bibtex": "@misc{\njoo2021generalized,\ntitle={Generalized Gumbel-Softmax Gradient Estimator for Generic Discrete Random Variables},\nauthor={Weonyoung Joo and Dongjun Kim and Seungjae Shin and Il-chul Moon},\nyear={2021},\nurl={https://openreview.net/forum?id=_PzOsP37P4T}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "_PzOsP37P4T", "replyto": "_PzOsP37P4T", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper503/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538141698, "tmdate": 1606915802224, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper503/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper503/-/Official_Review"}}}, {"id": "wFozcC8PTnN", "original": null, "number": 3, "cdate": 1603810375094, "ddate": null, "tcdate": 1603810375094, "tmdate": 1605024674124, "tddate": null, "forum": "_PzOsP37P4T", "replyto": "_PzOsP37P4T", "invitation": "ICLR.cc/2021/Conference/Paper503/-/Official_Review", "content": {"title": "Simple idea of using Gumbel-Softmax to sample from discrete distributions, experimental part can be expanded and improved", "review": "**summary** \nthe paper proposes a method of learning discrete approximate posterior distributions with potentially unbounded support. The idea is to truncate it to a finite set of states and use Gumbel-Softmax relaxation for samples from the truncated distribution. \nThe approach is illustrated on VAE example as well as topic modelling.\n\n**pros**\nThe idea of using Gumbel-Softmax to relax samples from another discrete distribution isn't new, however the application of this idea to distribution discussed in this paper does look novel. Gumbel-Softmax estimator is known to be computationally efficient and performing well in many situations, so it's perhaps not surprising that it performs well in the current context. Experimental results in topic modelling look very impressive.\n\n**cons**\n* I found it hard to follow the experimental section. In Section 7.2 the authors reference Figurnov et al (2018) for the setup however it seems that the reference only discusses continuous distributions. It is not clear what is the form of the prior and approximating posterior used in VAE experiments. This makes it hard to evaluate the paper.\n* I found the proofs that proposed approximation converges to true samples to be of little value: they do not give quantitative bounds on what the truncation region should be for given sample quality.\n* I think that comparison with StoRB and UnOrd estimators using only m=1 is not enough, it is possible that increasing m, although adding more computation, will significantly improve overall performance of the final LL.\n\ncomments:\n* My suggestion for improving the paper would be to provide much more experimental details, regarding the setup, datasets and baselines in the main text of the paper. At the same time the theoretical part can be reduced given that the idea is very straightforward.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper503/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper503/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generalized Gumbel-Softmax Gradient Estimator for Generic Discrete Random Variables", "authorids": ["~Weonyoung_Joo1", "~Dongjun_Kim1", "~Seungjae_Shin1", "~Il-chul_Moon1"], "authors": ["Weonyoung Joo", "Dongjun Kim", "Seungjae Shin", "Il-chul Moon"], "keywords": ["Deep learning", "Deep generative model", "Unsupervised learning", "Gradient estimator", "Reparameterization trick", "Discrete distribution", "Gumbel-Softmax"], "abstract": "Estimating the gradients of stochastic nodes, which enables the gradient descent optimization on neural network parameters, is one of the crucial research questions in the deep generative modeling community. When it comes to discrete distributions, Gumbel-Softmax trick reparameterizes Bernoulli and categorical random variables by continuous relaxation. However, gradient estimators of discrete distributions other than the Bernoulli and the categorical have not been explored, and the the Gumbel-Softmax trick is not directly applicable to other discrete distributions. This paper proposes a general version of the Gumbel-Softmax estimator with a theoretical basis, and the proposed estimator is able to reparameterize generic discrete distributions, broader than the Bernoulli and the categorical. In detail, we utilize the truncation of discrete random variables and the Gumbel-Softmax trick with a linear transformation for the relaxed reparameterization. The proposed approach enables the relaxed discrete random variable to be reparameterized through a large-scale stochastic computational graph. Our experiments consist of (1) synthetic data analyses and applications on VAE, which show the efficacy of our methods; and (2) topic models, which demonstrate the value of the proposed estimation in practice.", "one-sentence_summary": "We present a generalized version of Gumbel-Softmax reparameterization trick, which enables estimating the gradients of discrete random nodes in stochastic computational graphs.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "joo|generalized_gumbelsoftmax_gradient_estimator_for_generic_discrete_random_variables", "supplementary_material": "", "pdf": "/pdf/9a9eec1510c6518dcc3e68bff75b905714415f5c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=978VXoFitL", "_bibtex": "@misc{\njoo2021generalized,\ntitle={Generalized Gumbel-Softmax Gradient Estimator for Generic Discrete Random Variables},\nauthor={Weonyoung Joo and Dongjun Kim and Seungjae Shin and Il-chul Moon},\nyear={2021},\nurl={https://openreview.net/forum?id=_PzOsP37P4T}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "_PzOsP37P4T", "replyto": "_PzOsP37P4T", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper503/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538141698, "tmdate": 1606915802224, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper503/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper503/-/Official_Review"}}}, {"id": "W5RJ3FsxO_t", "original": null, "number": 4, "cdate": 1604033347975, "ddate": null, "tcdate": 1604033347975, "tmdate": 1605024674055, "tddate": null, "forum": "_PzOsP37P4T", "replyto": "_PzOsP37P4T", "invitation": "ICLR.cc/2021/Conference/Paper503/-/Official_Review", "content": {"title": "Simple method and empirical results support claim but writing needs work", "review": "Summary:\n* This paper addresses the issue of applying continuous relaxations to simple countably infinite discrete distributions such as the Poisson or Geometric distributions.\n* The proposed approach truncates discrete distributions and applies a continuous relaxation to the truncated subset, i.e. the Gumbel-Softmax trick.\n* The experiments demonstrate the method outperforms other estimators that do not apply continuous relaxations.\n\nStrengths:\n* The method is straightforward and is a good first step towards applying continuous relaxations to high-dimensional discrete distributions.\n* The approach demonstrates improvements in the test likelihood in the presented experiments. \n\nWeaknesses:\n* A contiguous range must be chosen. This has clear drawbacks if there is no obvious structure as in the simple distributions considered in this paper (Pois, Geom, NegBin).\n* Too much space was dedicated to explaining the method twice, resulting in a lack of space for experiments. The MNIST/Omniglot experiments clearly show that the proposed estimator performs well by fixing the model and varying over different gradient estimators. What is the purpose of the topic modeling experiments? Are the baselines comparable to the proposed model?\n\nDecision: Reject\n* The method is simple and extends the Gumbel-Softmax trick to countably infinite discrete distributions.\n* The empirical results appear to be quite good compared to the reported baselines, although I am not familiar with the datasets used or whether the baselines are fair comparisons.\n* However, the writing needs a lot of work. Most of the experimental details have been relegated to the appendix because of space used to explain the method twice.\n\nSuggestions:\n* The truncation seems unnecessary. Is there a tweak to the method similar to Liu et al [1], such as applying the Gumbel-Softmax trick to the largest atoms (such that their total mass > 1 - 0.0005)? Or exploring learned partitions? Additionally, Liu et al [1] correct for bias via sampling. Is there a similar extension here? For example the (n+1)-th value of c can be sampled randomly from the complement of the support subset C.\n* Sections 3 and 4 are redundant. Get rid of section 3 and, if necessary, simplify the formal description of the method so that it is understandable without section 3. Find the minimal way to convey the method while remaining comprehensive. This should give more room to fully and clearly explain the experiments.\n* Given the simplicity of the method (which is a good thing), is all of the notation necessary?\n* In addition to the current results on topic modeling, I would like to see the fixed NVPDEF model trained with some of the gradient estimators from the MNIST/OMNIGLOT experiments.\n\n[1] R. Liu, J. Regier, N. Tripuraneni, M. I. Jordan, and J. McAuliffe. Rao-blackwellized stochastic gradients for discrete distributions. International Conference on Machine Learning, 2019.\n\n", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper503/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper503/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generalized Gumbel-Softmax Gradient Estimator for Generic Discrete Random Variables", "authorids": ["~Weonyoung_Joo1", "~Dongjun_Kim1", "~Seungjae_Shin1", "~Il-chul_Moon1"], "authors": ["Weonyoung Joo", "Dongjun Kim", "Seungjae Shin", "Il-chul Moon"], "keywords": ["Deep learning", "Deep generative model", "Unsupervised learning", "Gradient estimator", "Reparameterization trick", "Discrete distribution", "Gumbel-Softmax"], "abstract": "Estimating the gradients of stochastic nodes, which enables the gradient descent optimization on neural network parameters, is one of the crucial research questions in the deep generative modeling community. When it comes to discrete distributions, Gumbel-Softmax trick reparameterizes Bernoulli and categorical random variables by continuous relaxation. However, gradient estimators of discrete distributions other than the Bernoulli and the categorical have not been explored, and the the Gumbel-Softmax trick is not directly applicable to other discrete distributions. This paper proposes a general version of the Gumbel-Softmax estimator with a theoretical basis, and the proposed estimator is able to reparameterize generic discrete distributions, broader than the Bernoulli and the categorical. In detail, we utilize the truncation of discrete random variables and the Gumbel-Softmax trick with a linear transformation for the relaxed reparameterization. The proposed approach enables the relaxed discrete random variable to be reparameterized through a large-scale stochastic computational graph. Our experiments consist of (1) synthetic data analyses and applications on VAE, which show the efficacy of our methods; and (2) topic models, which demonstrate the value of the proposed estimation in practice.", "one-sentence_summary": "We present a generalized version of Gumbel-Softmax reparameterization trick, which enables estimating the gradients of discrete random nodes in stochastic computational graphs.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "joo|generalized_gumbelsoftmax_gradient_estimator_for_generic_discrete_random_variables", "supplementary_material": "", "pdf": "/pdf/9a9eec1510c6518dcc3e68bff75b905714415f5c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=978VXoFitL", "_bibtex": "@misc{\njoo2021generalized,\ntitle={Generalized Gumbel-Softmax Gradient Estimator for Generic Discrete Random Variables},\nauthor={Weonyoung Joo and Dongjun Kim and Seungjae Shin and Il-chul Moon},\nyear={2021},\nurl={https://openreview.net/forum?id=_PzOsP37P4T}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "_PzOsP37P4T", "replyto": "_PzOsP37P4T", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper503/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538141698, "tmdate": 1606915802224, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper503/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper503/-/Official_Review"}}}], "count": 6}