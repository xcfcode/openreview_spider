{"notes": [{"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1457640787797, "tcdate": 1457640787797, "id": "OM0mBBk7Gcp57ZJjtNPw", "invitation": "ICLR.cc/2016/workshop/-/paper/23/review/10", "forum": "0YrnoNZ7PTGJ7gK5tNYY", "replyto": "0YrnoNZ7PTGJ7gK5tNYY", "signatures": ["ICLR.cc/2016/workshop/paper/23/reviewer/10"], "readers": ["everyone"], "writers": ["ICLR.cc/2016/workshop/paper/23/reviewer/10"], "content": {"title": "Below the bar", "rating": "3: Clear rejection", "review": "This paper proposes a method for online updates of a variational approximation of the posterior over neural network weights. No experimental evaluation is provided. The presentation is intelligible, but far from clear.\n\nThe idea of using a recursive variational Bayes approximation for streaming data was proposed in Broderick et al.'s SDA-Bayes paper (http://papers.nips.cc/paper/4980-streaming-variational-bayes). But as another reviewer noted, online variational inference has been around since at least Sato's 2001 paper on online model selection with variational Bayes, and in a sense since the 1998 Neal and Hinton paper on incremental EM.\n\nThere have been plenty of papers about variational inference for neural networks, for example, Graves's Practical Inference for Neural Networks (2011) or Hinton's original 1993 variational inference/MDL paper (http://dl.acm.org/citation.cfm?id=168306).\n\nThe idea of using the variational distribution's variance to control step size is interesting. It's sort of related to recent papers that use trust regions/prox algorithms to optimize variational approximations (Theis&Hoffman, 2015; Khan et al., 2015).\n\nHowever, that doesn't mean it will work. With no experimental validation, it's impossible to say whether this is anything more than a cute idea.", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "VARIATIONAL STOCHASTIC GRADIENT DESCENT", "abstract": "In Bayesian approach to probabilistic modeling of data we select a model for probabilities of data that depends on a continuous vector of parameters. For a given data set Bayesian theorem gives a probability distribution of the model parameters. Then the inference of outcomes and probabilities of new data could be found by averaging over the parameter distribution of the model, which is an intractable problem. In this paper we propose to use Variational Bayes (VB) to estimate Gaussian posterior of model parameters for a given Gaussian prior and Bayesian updates in a form that resembles SGD rules. It is shown that with incremental updates of posteriors for a selected sequence of data points and a given number of iterations the variational approximations are defined by a trajectory in space of Gaussian parameters, which depends on a starting point defined by priors of the parameter distribution, which are true hyper-parameters. The same priors are providing a weight decay or L2 regularization for the training. Then a selection of L2 regularization parameters and a number of iterations is completely defining a learning rule for VB SGD optimization, unlike other methods with momentum (Duchi et al., 2011; Kingma & Ba, 2014; Zeiler, 2012) that need selecting learning, regularization rates, etc., separately. We consider application of VB SGD for important practical case of fast training neural networks with very large data. While the speedup is achieved by partitioning data and training in parallel the resulting set of solutions obtained with VB SGD forms a Gaussian mixture. By applying VB SGD optimization to the Gaussian mixture we can merge multiple neural networks of same dimensions into a new single neural network that has almost the same performance as an original Gaussian mixture.\n", "pdf": "/pdf/0YrnoNZ7PTGJ7gK5tNYY.pdf", "paperhash": "tetelman|variational_stochastic_gradient_descent", "conflicts": ["InvenSense.com"], "authors": ["Michael Tetelman"], "authorids": ["michael.tetelman@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1456579932409, "ddate": null, "super": null, "final": null, "duedate": 1460725200000, "tcdate": 1456579932409, "id": "ICLR.cc/2016/workshop/-/paper/23/review/10", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "reply": {"pdf": null, "forum": "0YrnoNZ7PTGJ7gK5tNYY", "replyto": "0YrnoNZ7PTGJ7gK5tNYY", "writers": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)"}, "signatures": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "readers": ["everyone", "ICLR.cc/2016/workshop/paper/23/reviewer/10", "ICLR.cc/2016/workshop"], "expdate": 1468501200000}}}, {"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1457616172597, "tcdate": 1457616172597, "id": "lx9ZgxVyvt2OVPy8Cvq7", "invitation": "ICLR.cc/2016/workshop/-/paper/23/review/12", "forum": "0YrnoNZ7PTGJ7gK5tNYY", "replyto": "0YrnoNZ7PTGJ7gK5tNYY", "signatures": ["~Tapani_Raiko1"], "readers": ["everyone"], "writers": ["~Tapani_Raiko1"], "content": {"title": "Not mature enough even for a workshop presentation", "rating": "3: Clear rejection", "review": "Manuscript describes variational Bayesian (VB) treatment of weights in neural networks and online learning for them.\n\nSimilar ideas have been studied recently, for instance in \nhttp://jmlr.org/proceedings/papers/v37/blundell15.pdf \nbut relationship to existing work is not presented clearly. Instead, using VB for network weights is presented as something novel.\n\nThere is no clear theoretical contribution or any experiments.\n\nThere is one crucial error as well: Bottom of page 3 writes that \"...distribution of the whole ensemble is a mix of...\" whereas the equation on the top of page 3 is a product rather than a mixture.\n\nThis paper might be of interest to the author, covering similar ideas:\nhttps://www.hiit.fi/u/ahonkela/papers/ica2003.pdf", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "VARIATIONAL STOCHASTIC GRADIENT DESCENT", "abstract": "In Bayesian approach to probabilistic modeling of data we select a model for probabilities of data that depends on a continuous vector of parameters. For a given data set Bayesian theorem gives a probability distribution of the model parameters. Then the inference of outcomes and probabilities of new data could be found by averaging over the parameter distribution of the model, which is an intractable problem. In this paper we propose to use Variational Bayes (VB) to estimate Gaussian posterior of model parameters for a given Gaussian prior and Bayesian updates in a form that resembles SGD rules. It is shown that with incremental updates of posteriors for a selected sequence of data points and a given number of iterations the variational approximations are defined by a trajectory in space of Gaussian parameters, which depends on a starting point defined by priors of the parameter distribution, which are true hyper-parameters. The same priors are providing a weight decay or L2 regularization for the training. Then a selection of L2 regularization parameters and a number of iterations is completely defining a learning rule for VB SGD optimization, unlike other methods with momentum (Duchi et al., 2011; Kingma & Ba, 2014; Zeiler, 2012) that need selecting learning, regularization rates, etc., separately. We consider application of VB SGD for important practical case of fast training neural networks with very large data. While the speedup is achieved by partitioning data and training in parallel the resulting set of solutions obtained with VB SGD forms a Gaussian mixture. By applying VB SGD optimization to the Gaussian mixture we can merge multiple neural networks of same dimensions into a new single neural network that has almost the same performance as an original Gaussian mixture.\n", "pdf": "/pdf/0YrnoNZ7PTGJ7gK5tNYY.pdf", "paperhash": "tetelman|variational_stochastic_gradient_descent", "conflicts": ["InvenSense.com"], "authors": ["Michael Tetelman"], "authorids": ["michael.tetelman@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1456579931449, "ddate": null, "super": null, "final": null, "duedate": 1460725200000, "tcdate": 1456579931449, "id": "ICLR.cc/2016/workshop/-/paper/23/review/12", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "reply": {"pdf": null, "forum": "0YrnoNZ7PTGJ7gK5tNYY", "replyto": "0YrnoNZ7PTGJ7gK5tNYY", "writers": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)"}, "signatures": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "readers": ["everyone", "ICLR.cc/2016/workshop/paper/23/reviewer/12", "ICLR.cc/2016/workshop"], "expdate": 1468501200000}}}, {"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1457609226600, "tcdate": 1457609226600, "id": "oVg3PAMLzcrlgPMRsB6n", "invitation": "ICLR.cc/2016/workshop/-/paper/23/unofficial_review", "forum": "0YrnoNZ7PTGJ7gK5tNYY", "replyto": "0YrnoNZ7PTGJ7gK5tNYY", "signatures": ["~Jose_Miguel_Hernandez_Lobato1"], "readers": ["everyone"], "writers": ["~Jose_Miguel_Hernandez_Lobato1"], "content": {"title": "Review for VARIATIONAL STOCHASTIC GRADIENT DESCENT", "rating": "3: Clear rejection", "review": "The authors propose an approach for the on-line maximization of the variational lower bound. The new method is based on iterating over the data and solving individual optimization problems between the current posterior approximation and the product of that posterior approximation and the likelihood function for the current data point. The advantages of the proposed approach with respect to other variational techniques is that it does not require to use learning rates or compute complicated expectations with respect to the variational approximation.\n\nQuality:\n\nThe proposed approach is not validated in any form of experiments. It is not clear how well it is going to work since variational Bayes is known to under-estimate variance and its application in an on-line manner could make more significant this problem because of consecutive under-estimation of variances at each iteration. Another problem is that there is no guarantee that the proposed approach is going to converge to any local minimizer of the original variational bound. In fact, by looking at equation 5, the update for the variance produces increasingly small variances. This means that the proposed approach would converge to a point mass at the mean of the posterior approximation q.\n\nThe mixture of Gaussians in Section 3 does not seem to be the correct approach. The correct approach would be to compute the product of all these Gaussians to obtain a final Gaussian approximation (accounting for the prior being repeated multiple times). The correct approach is given in\n\nExpectation propagation as a way of life\nAndrew Gelman, Aki Vehtari, Pasi Jyl\u00e4nki, Christian Robert, Nicolas Chopin, John P. Cunningham\nhttp://arxiv.org/abs/1412.4869\n\nClarity:\n\nThe work needs to be improved for clarity. It is not clear how equation 4 is obtained. The equation above equation 4 seems to come from performing a Laplace approximation. The authors should clarify this possible connexion with the Laplace approximation.\n\nOriginality:\n\nThe approach proposed seems to be original up to my knowledge.\n\nSignificance:\n\nIt is not clear how significant the proposed method is since one can use stochastic optimization to optimize the variational lower bound. The approach for training neural networks fast by splitting the data seems to be wrong.", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "VARIATIONAL STOCHASTIC GRADIENT DESCENT", "abstract": "In Bayesian approach to probabilistic modeling of data we select a model for probabilities of data that depends on a continuous vector of parameters. For a given data set Bayesian theorem gives a probability distribution of the model parameters. Then the inference of outcomes and probabilities of new data could be found by averaging over the parameter distribution of the model, which is an intractable problem. In this paper we propose to use Variational Bayes (VB) to estimate Gaussian posterior of model parameters for a given Gaussian prior and Bayesian updates in a form that resembles SGD rules. It is shown that with incremental updates of posteriors for a selected sequence of data points and a given number of iterations the variational approximations are defined by a trajectory in space of Gaussian parameters, which depends on a starting point defined by priors of the parameter distribution, which are true hyper-parameters. The same priors are providing a weight decay or L2 regularization for the training. Then a selection of L2 regularization parameters and a number of iterations is completely defining a learning rule for VB SGD optimization, unlike other methods with momentum (Duchi et al., 2011; Kingma & Ba, 2014; Zeiler, 2012) that need selecting learning, regularization rates, etc., separately. We consider application of VB SGD for important practical case of fast training neural networks with very large data. While the speedup is achieved by partitioning data and training in parallel the resulting set of solutions obtained with VB SGD forms a Gaussian mixture. By applying VB SGD optimization to the Gaussian mixture we can merge multiple neural networks of same dimensions into a new single neural network that has almost the same performance as an original Gaussian mixture.\n", "pdf": "/pdf/0YrnoNZ7PTGJ7gK5tNYY.pdf", "paperhash": "tetelman|variational_stochastic_gradient_descent", "conflicts": ["InvenSense.com"], "authors": ["Michael Tetelman"], "authorids": ["michael.tetelman@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "tmdate": null, "cdate": 1455612922962, "ddate": null, "super": null, "final": null, "tcdate": 1455612922962, "id": "ICLR.cc/2016/workshop/-/paper/23/unofficial_review", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["michael.tetelman@gmail.com"], "reply": {"pdf": null, "writers": {"values-regex": "~.*"}, "forum": "0YrnoNZ7PTGJ7gK5tNYY", "replyto": "0YrnoNZ7PTGJ7gK5tNYY", "signatures": {"values-regex": "~.*", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": []}}}, {"tddate": null, "number": null, "replyto": null, "ddate": null, "cdate": null, "tmdate": 1455612921643, "tcdate": 1455612921643, "id": "0YrnoNZ7PTGJ7gK5tNYY", "invitation": "ICLR.cc/2016/workshop/-/submission", "forum": "0YrnoNZ7PTGJ7gK5tNYY", "signatures": ["~Michael_Tetelman1"], "readers": ["everyone"], "writers": ["~Michael_Tetelman1"], "content": {"CMT_id": "", "title": "VARIATIONAL STOCHASTIC GRADIENT DESCENT", "abstract": "In Bayesian approach to probabilistic modeling of data we select a model for probabilities of data that depends on a continuous vector of parameters. For a given data set Bayesian theorem gives a probability distribution of the model parameters. Then the inference of outcomes and probabilities of new data could be found by averaging over the parameter distribution of the model, which is an intractable problem. In this paper we propose to use Variational Bayes (VB) to estimate Gaussian posterior of model parameters for a given Gaussian prior and Bayesian updates in a form that resembles SGD rules. It is shown that with incremental updates of posteriors for a selected sequence of data points and a given number of iterations the variational approximations are defined by a trajectory in space of Gaussian parameters, which depends on a starting point defined by priors of the parameter distribution, which are true hyper-parameters. The same priors are providing a weight decay or L2 regularization for the training. Then a selection of L2 regularization parameters and a number of iterations is completely defining a learning rule for VB SGD optimization, unlike other methods with momentum (Duchi et al., 2011; Kingma & Ba, 2014; Zeiler, 2012) that need selecting learning, regularization rates, etc., separately. We consider application of VB SGD for important practical case of fast training neural networks with very large data. While the speedup is achieved by partitioning data and training in parallel the resulting set of solutions obtained with VB SGD forms a Gaussian mixture. By applying VB SGD optimization to the Gaussian mixture we can merge multiple neural networks of same dimensions into a new single neural network that has almost the same performance as an original Gaussian mixture.\n", "pdf": "/pdf/0YrnoNZ7PTGJ7gK5tNYY.pdf", "paperhash": "tetelman|variational_stochastic_gradient_descent", "conflicts": ["InvenSense.com"], "authors": ["Michael Tetelman"], "authorids": ["michael.tetelman@gmail.com"]}, "nonreaders": [], "details": {"replyCount": 3, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1454464564200, "ddate": null, "super": null, "final": null, "duedate": 1455833700000, "tcdate": 1454464564200, "id": "ICLR.cc/2016/workshop/-/submission", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "readers": ["everyone"], "reply": {"pdf": null, "forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"order": 4, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv.", "value-regex": "upload|http://arxiv.org/pdf/.+"}, "title": {"order": 3, "description": "Title of paper.", "value-regex": ".{0,500}"}, "abstract": {"order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"order": 1, "description": "Comma separated list of author names, as they appear in the paper.", "value-regex": "[^,\\n]+(,[^,\\n]+)*"}, "author_emails": {"order": 2, "description": "Comma separated list of author email addresses, in the same order as above.", "value-regex": "[^,\\n]+(,[^,\\n]+)*"}, "conflicts": {"order": 100, "description": "Semi-colon separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.).", "value-regex": "^([a-zA-Z0-9][a-zA-Z0-9-_]{0,61}[a-zA-Z0-9]{0,1}\\.([a-zA-Z]{1,6}|[a-zA-Z0-9-]{1,30}\\.[a-zA-Z]{2,3}))+(;[a-zA-Z0-9][a-zA-Z0-9-_]{0,61}[a-zA-Z0-9]{0,1}\\.([a-zA-Z]{1,6}|[a-zA-Z0-9-]{1,30}\\.[a-zA-Z]{2,3}))*$"}, "CMT_id": {"order": 5, "value-regex": ".*", "description": "If the paper is a resubmission from the ICLR 2016 Conference Track, enter its CMT ID; otherwise, leave blank."}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "expdate": 1463609700000}}}], "count": 4}