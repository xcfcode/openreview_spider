{"notes": [{"id": "a8Whva4r58", "original": null, "number": 6, "cdate": 1583400833946, "ddate": null, "tcdate": 1583400833946, "tmdate": 1594975387955, "tddate": null, "forum": "HyeqPJHYvH", "replyto": "HyeqPJHYvH", "invitation": "ICLR.cc/2020/Conference/Paper1777/-/Official_Comment", "content": {"title": "New Version & Changes", "comment": "A new version of our article was accepted to ICML 2020 (https://proceedings.icml.cc/book/3249.pdf), with the following main improvements:\n    - we included new results on the challenging Human3.6M dataset, showing that we outperform the state-of-the-art StructVRNN [1];\n    - we computed FVD scores for all tested models;\n    - we clarified the experimental settings and interpretation of the experiment regarding generation at different frame rates;\n    - we improved our results and updated their analysis for the BAIR dataset;\n    - we publicly released the fully documented code at https://github.com/edouardelasalles/srvp;\n    - we publicly released the pretrained models  at https://data.lip6.fr/srvp/.\n\n[1] Minderer et al. Unsupervised Learning of Object Structure and Dynamics from Videos. In NeurIPS 2019.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1777/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1777/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jean-yves.franceschi@lip6.fr", "edouard.delasalles@lip6.fr", "mickael.chen@lip6.fr", "sylvain.lamprier@lip6.fr", "patrick.gallinari@lip6.fr"], "title": "Stochastic Latent Residual Video Prediction", "authors": ["Jean-Yves Franceschi", "Edouard Delasalles", "Mickael Chen", "Sylvain Lamprier", "Patrick Gallinari"], "pdf": "/pdf/01c19eb1a00a822df4d28c39840c317978b2e296.pdf", "abstract": "Video prediction is a challenging task: models have to account for the inherent uncertainty of the future. Most works in the literature are based on stochastic image-autoregressive recurrent networks, raising several performance and applicability issues. An alternative is to use fully latent temporal models which untie frame synthesis and dynamics. However, no such model for video prediction has been proposed in the literature yet, due to design and training difficulties. In this paper, we overcome these difficulties by introducing a novel stochastic temporal model. It is based on residual updates of a latent state, motivated by discretization schemes of differential equations. This first-order principle naturally models video dynamics as it allows our simpler, lightweight, interpretable, latent model to outperform prior state-of-the-art methods on challenging datasets.", "code": "https://sites.google.com/view/srvp/", "keywords": ["stochastic video prediction", "variational autoencoder", "residual dynamics"], "paperhash": "franceschi|stochastic_latent_residual_video_prediction", "original_pdf": "/attachment/c268e015553776923f70d24a3f8f73c7afa4a8fc.pdf", "_bibtex": "@misc{\nfranceschi2020stochastic,\ntitle={Stochastic Latent Residual Video Prediction},\nauthor={Jean-Yves Franceschi and Edouard Delasalles and Mickael Chen and Sylvain Lamprier and Patrick Gallinari},\nyear={2020},\nurl={https://openreview.net/forum?id=HyeqPJHYvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HyeqPJHYvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1777/Authors", "ICLR.cc/2020/Conference/Paper1777/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1777/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1777/Reviewers", "ICLR.cc/2020/Conference/Paper1777/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1777/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1777/Authors|ICLR.cc/2020/Conference/Paper1777/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504151034, "tmdate": 1576860557231, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1777/Authors", "ICLR.cc/2020/Conference/Paper1777/Reviewers", "ICLR.cc/2020/Conference/Paper1777/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1777/-/Official_Comment"}}}, {"id": "HyeqPJHYvH", "original": "rklLxTp_DS", "number": 1777, "cdate": 1569439586084, "ddate": null, "tcdate": 1569439586084, "tmdate": 1577168225019, "tddate": null, "forum": "HyeqPJHYvH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["jean-yves.franceschi@lip6.fr", "edouard.delasalles@lip6.fr", "mickael.chen@lip6.fr", "sylvain.lamprier@lip6.fr", "patrick.gallinari@lip6.fr"], "title": "Stochastic Latent Residual Video Prediction", "authors": ["Jean-Yves Franceschi", "Edouard Delasalles", "Mickael Chen", "Sylvain Lamprier", "Patrick Gallinari"], "pdf": "/pdf/01c19eb1a00a822df4d28c39840c317978b2e296.pdf", "abstract": "Video prediction is a challenging task: models have to account for the inherent uncertainty of the future. Most works in the literature are based on stochastic image-autoregressive recurrent networks, raising several performance and applicability issues. An alternative is to use fully latent temporal models which untie frame synthesis and dynamics. However, no such model for video prediction has been proposed in the literature yet, due to design and training difficulties. In this paper, we overcome these difficulties by introducing a novel stochastic temporal model. It is based on residual updates of a latent state, motivated by discretization schemes of differential equations. This first-order principle naturally models video dynamics as it allows our simpler, lightweight, interpretable, latent model to outperform prior state-of-the-art methods on challenging datasets.", "code": "https://sites.google.com/view/srvp/", "keywords": ["stochastic video prediction", "variational autoencoder", "residual dynamics"], "paperhash": "franceschi|stochastic_latent_residual_video_prediction", "original_pdf": "/attachment/c268e015553776923f70d24a3f8f73c7afa4a8fc.pdf", "_bibtex": "@misc{\nfranceschi2020stochastic,\ntitle={Stochastic Latent Residual Video Prediction},\nauthor={Jean-Yves Franceschi and Edouard Delasalles and Mickael Chen and Sylvain Lamprier and Patrick Gallinari},\nyear={2020},\nurl={https://openreview.net/forum?id=HyeqPJHYvH}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "kxgDTmQY3", "original": null, "number": 1, "cdate": 1576798732264, "ddate": null, "tcdate": 1576798732264, "tmdate": 1576800904173, "tddate": null, "forum": "HyeqPJHYvH", "replyto": "HyeqPJHYvH", "invitation": "ICLR.cc/2020/Conference/Paper1777/-/Decision", "content": {"decision": "Reject", "comment": "The paper proposes a method for learning a latent dynamics model for videos. The main idea is to learn a latent representation and model the dynamics of the latent features via residual connection motivated by ODE. The architectural choice of residual connection itself is not new as many prior works have employed \"skip connections\" in hidden representations but the notion of connecting this with ODE and factoring time as input into the residual function seems a new idea. The experimental results show the promise of the proposed method on moving MNIST, KTH, and BAIR datasets. The experiments on different frame rates are also nice.  In terms of weakness, the evaluation is performed on relatively simple domains (e.g., moving MNIST and KTH) with static backgrounds and the improvement on BAIR dataset (which is not considered as a difficult benchmark) in terms of FVD is not as clear. For the BAIR dataset, it's unclear how the proposed method will handle the interactions between the robot arm and background objects due to the modeling assumption (i.e., static background). In this sense, content swap results on BAIR dataset look quite anecdotal, and the significance is limited. For improvement, I would suggest adding evaluations on other challenging domains, such as Human 3.6M (where human motions are much more uncertain compared to KTH) and other Robot datasets with more complex robot-object interactions. Overall, the paper proposes an interesting architecture with promising results on relatively simple datasets, but the advantage over existing SOTA methods on challenging benchmarks is unclear yet.\n", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jean-yves.franceschi@lip6.fr", "edouard.delasalles@lip6.fr", "mickael.chen@lip6.fr", "sylvain.lamprier@lip6.fr", "patrick.gallinari@lip6.fr"], "title": "Stochastic Latent Residual Video Prediction", "authors": ["Jean-Yves Franceschi", "Edouard Delasalles", "Mickael Chen", "Sylvain Lamprier", "Patrick Gallinari"], "pdf": "/pdf/01c19eb1a00a822df4d28c39840c317978b2e296.pdf", "abstract": "Video prediction is a challenging task: models have to account for the inherent uncertainty of the future. Most works in the literature are based on stochastic image-autoregressive recurrent networks, raising several performance and applicability issues. An alternative is to use fully latent temporal models which untie frame synthesis and dynamics. However, no such model for video prediction has been proposed in the literature yet, due to design and training difficulties. In this paper, we overcome these difficulties by introducing a novel stochastic temporal model. It is based on residual updates of a latent state, motivated by discretization schemes of differential equations. This first-order principle naturally models video dynamics as it allows our simpler, lightweight, interpretable, latent model to outperform prior state-of-the-art methods on challenging datasets.", "code": "https://sites.google.com/view/srvp/", "keywords": ["stochastic video prediction", "variational autoencoder", "residual dynamics"], "paperhash": "franceschi|stochastic_latent_residual_video_prediction", "original_pdf": "/attachment/c268e015553776923f70d24a3f8f73c7afa4a8fc.pdf", "_bibtex": "@misc{\nfranceschi2020stochastic,\ntitle={Stochastic Latent Residual Video Prediction},\nauthor={Jean-Yves Franceschi and Edouard Delasalles and Mickael Chen and Sylvain Lamprier and Patrick Gallinari},\nyear={2020},\nurl={https://openreview.net/forum?id=HyeqPJHYvH}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "HyeqPJHYvH", "replyto": "HyeqPJHYvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795706616, "tmdate": 1576800254719, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1777/-/Decision"}}}, {"id": "r1gsleMpKS", "original": null, "number": 1, "cdate": 1571786739013, "ddate": null, "tcdate": 1571786739013, "tmdate": 1574881760540, "tddate": null, "forum": "HyeqPJHYvH", "replyto": "HyeqPJHYvH", "invitation": "ICLR.cc/2020/Conference/Paper1777/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #3", "review": "Contributions: this submission proposes a video pixel generation framework with the goal to decouple visual appearance and dynamics. The latent dynamics are modeled with a latent residual dynamics model. Empirical evaluations on moving MNIST show that the proposed residual dynamics model outperform MLP or GRU. On more challenging KTH and BAIR datasets, the proposed method achieves on par or better quantitative performance with previous methods, and have nice qualitative results on content \"swap\" and dynamics interpolation.\n\nAssessment:\n- To my knowledge, the proposed model is novel for video generation.\n- The proposed method is supported with strong quantitative results and qualitative analysis, ablation on Moving MNIST shows that the proposed latent residual dynamics model outperforms MLP and GRU baselines.\n- The authors might be interested in related work on video generation with decoupled appearance and dynamics models, such as [1]. It would also be interesting to see evaluation on more challenging datasets, such as Human3.6M.\n- Question: how does the proposed inference framework make sure to decouple appearance with dynamics? Can y_i not encode the appearance information?\n\n\n[1] Minderer et al., Unsupervised Learning of Object Structure and Dynamics from Videos. NeurIPS 2019.\n\n-----------------------------\nPost rebuttal:\nThank you for your answers to my questions and the updated manuscript. My questions have been addressed and the additional results further confirm the performance of the proposed method. Therefore I recommend weak accept of the submission.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1777/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1777/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jean-yves.franceschi@lip6.fr", "edouard.delasalles@lip6.fr", "mickael.chen@lip6.fr", "sylvain.lamprier@lip6.fr", "patrick.gallinari@lip6.fr"], "title": "Stochastic Latent Residual Video Prediction", "authors": ["Jean-Yves Franceschi", "Edouard Delasalles", "Mickael Chen", "Sylvain Lamprier", "Patrick Gallinari"], "pdf": "/pdf/01c19eb1a00a822df4d28c39840c317978b2e296.pdf", "abstract": "Video prediction is a challenging task: models have to account for the inherent uncertainty of the future. Most works in the literature are based on stochastic image-autoregressive recurrent networks, raising several performance and applicability issues. An alternative is to use fully latent temporal models which untie frame synthesis and dynamics. However, no such model for video prediction has been proposed in the literature yet, due to design and training difficulties. In this paper, we overcome these difficulties by introducing a novel stochastic temporal model. It is based on residual updates of a latent state, motivated by discretization schemes of differential equations. This first-order principle naturally models video dynamics as it allows our simpler, lightweight, interpretable, latent model to outperform prior state-of-the-art methods on challenging datasets.", "code": "https://sites.google.com/view/srvp/", "keywords": ["stochastic video prediction", "variational autoencoder", "residual dynamics"], "paperhash": "franceschi|stochastic_latent_residual_video_prediction", "original_pdf": "/attachment/c268e015553776923f70d24a3f8f73c7afa4a8fc.pdf", "_bibtex": "@misc{\nfranceschi2020stochastic,\ntitle={Stochastic Latent Residual Video Prediction},\nauthor={Jean-Yves Franceschi and Edouard Delasalles and Mickael Chen and Sylvain Lamprier and Patrick Gallinari},\nyear={2020},\nurl={https://openreview.net/forum?id=HyeqPJHYvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HyeqPJHYvH", "replyto": "HyeqPJHYvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1777/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1777/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576084602230, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1777/Reviewers"], "noninvitees": [], "tcdate": 1570237732438, "tmdate": 1576084602243, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1777/-/Official_Review"}}}, {"id": "HJxrNhLnjS", "original": null, "number": 4, "cdate": 1573837869200, "ddate": null, "tcdate": 1573837869200, "tmdate": 1573837869200, "tddate": null, "forum": "HyeqPJHYvH", "replyto": "r1gsleMpKS", "invitation": "ICLR.cc/2020/Conference/Paper1777/-/Official_Comment", "content": {"title": "Answer to Reviewer 3", "comment": "We would like to thank you for your supportive feedback. We updated the submission following your remarks and answer your comments below.\n\n    - Decoupling appearance and dynamic:\n\n      Our model manages to decouple appearance and dynamics in the following way.\n      Firstly, the dynamics is made independent from the previously generated frames, in a state-space manner, as described in the manuscript. \n      Secondly, the content variable w is designed to remove as much visual information from y as possible. This relies on the fact that the dynamic variables y and z are regularized by KL, while the content variable w is not regularized in the loss function. w is instead prevented from containing any temporal information as it is inferred during training with randomly sampled frames and a permutation-invariant network. w is otherwise free to contain any non temporal information. The dynamic variables y and z are encouraged, because of the KL penalty, to contain only necessary information, and therefore should only contain temporal information that could not be captured by w.\n      We clarify this point in Section 3.2 of the revised submission.\n\n\n    - Human3.6M:\n\n      We thank the reviewer for suggesting this relevant dataset. We would have been pleased to show results of our method on this dataset. Unfortunately, it is not publicly available and we were only granted access only a couple of days ago. Please note, however, that the KTH dataset that we consider is somewhat similar to Human3.6M, as both datasets consist in videos of actions performed by subjects in front of a camera.\n\n\n    - Related work:\n\n      We thank the reviewer for pointing out the recent missing reference [1] that we included in the manuscript as related work along with discussion regarding differences with our work. \n      It is orthogonal to our work as they improve the VRNN dynamic model by using  frame-wise key-point representations instead of raw frames, while we focus on improving the dynamic model itself.\n\n\nWe hope that we were able to answer your questions.\n\n\n[1] Minderer et al. Unsupervised Learning of Object Structure and Dynamics from Videos. In NeurIPS 2019."}, "signatures": ["ICLR.cc/2020/Conference/Paper1777/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1777/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jean-yves.franceschi@lip6.fr", "edouard.delasalles@lip6.fr", "mickael.chen@lip6.fr", "sylvain.lamprier@lip6.fr", "patrick.gallinari@lip6.fr"], "title": "Stochastic Latent Residual Video Prediction", "authors": ["Jean-Yves Franceschi", "Edouard Delasalles", "Mickael Chen", "Sylvain Lamprier", "Patrick Gallinari"], "pdf": "/pdf/01c19eb1a00a822df4d28c39840c317978b2e296.pdf", "abstract": "Video prediction is a challenging task: models have to account for the inherent uncertainty of the future. Most works in the literature are based on stochastic image-autoregressive recurrent networks, raising several performance and applicability issues. An alternative is to use fully latent temporal models which untie frame synthesis and dynamics. However, no such model for video prediction has been proposed in the literature yet, due to design and training difficulties. In this paper, we overcome these difficulties by introducing a novel stochastic temporal model. It is based on residual updates of a latent state, motivated by discretization schemes of differential equations. This first-order principle naturally models video dynamics as it allows our simpler, lightweight, interpretable, latent model to outperform prior state-of-the-art methods on challenging datasets.", "code": "https://sites.google.com/view/srvp/", "keywords": ["stochastic video prediction", "variational autoencoder", "residual dynamics"], "paperhash": "franceschi|stochastic_latent_residual_video_prediction", "original_pdf": "/attachment/c268e015553776923f70d24a3f8f73c7afa4a8fc.pdf", "_bibtex": "@misc{\nfranceschi2020stochastic,\ntitle={Stochastic Latent Residual Video Prediction},\nauthor={Jean-Yves Franceschi and Edouard Delasalles and Mickael Chen and Sylvain Lamprier and Patrick Gallinari},\nyear={2020},\nurl={https://openreview.net/forum?id=HyeqPJHYvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HyeqPJHYvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1777/Authors", "ICLR.cc/2020/Conference/Paper1777/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1777/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1777/Reviewers", "ICLR.cc/2020/Conference/Paper1777/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1777/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1777/Authors|ICLR.cc/2020/Conference/Paper1777/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504151034, "tmdate": 1576860557231, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1777/Authors", "ICLR.cc/2020/Conference/Paper1777/Reviewers", "ICLR.cc/2020/Conference/Paper1777/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1777/-/Official_Comment"}}}, {"id": "rygBpsU2oB", "original": null, "number": 3, "cdate": 1573837757010, "ddate": null, "tcdate": 1573837757010, "tmdate": 1573837757010, "tddate": null, "forum": "HyeqPJHYvH", "replyto": "r1xt2GQpFH", "invitation": "ICLR.cc/2020/Conference/Paper1777/-/Official_Comment", "content": {"title": "Answer to Reviewer 2", "comment": "We would like to thank the reviewer for his encouraging feedback and helpful suggestions. We updated the submission accordingly and provide answers and comments below.\n\n    - Impact and difference with SVG:\n\n      The difference between our model and SVG is twofold. Firstly, unlike SVG, our model decouples frame synthesis and dynamics, thanks to its state-space nature, a different prediction model and the use of a content variable. Secondly, our model introduces a stochastic residual update that is shown to significantly improve the performance of our model compared to a regular recurrent network.\n\n      Combined, these improvements allow our model to reach the state of the art of stochastic video prediction, besides obtaining desirable properties for its latent space that could not be achieved by prior models.\n      We think that these contributions could have substantial impact in the community, as the general principles of our model (state-space, residual dynamic, static content variable) can be generally applied to other models as well, even though this is out of the scope of this paper. For instance, replacing the VRNN model in [1] to model the evolution of key-points could bring additional performance gains. Moreover, the state-space nature of our model allows us to learn meaningful dynamic representations, typically used in model-based reinforcement learning [2], and illustrated in Figure 9.\n\n      We emphasized these points in the revised version of the paper in Sections 2, 4 and 5.\n\n\n    - Origin of improvements:\n\n      Our model shares the same encoder / decoder architecture as SVG, showing that its performance is due to our inference and dynamic systems rather than our choice of neural architecture.\n\n      To further study the origin of improvements, we presented, in the original submission, two baselines (GRU and MLP). The only difference between those baselines and our model is dynamic-related (they either use recurrent networks such as those of VRNN [3] or recurrent MLPs as an update method, instead of our residual updates). From our experiments, we reach two conclusions. First, all three versions of our method (residual, MLP, GRU) outperform prior methods. Therefore, this improvement is due to their common inference method, latent nature and content variable. Secondly, the residual update provide an additional gain of performance compared to the other methods, that can only be explained by the difference in the update method.\n\n      This analysis, that was originally performed on both deterministic and stochastic versions of Moving MNIST, have been confirmed by *new experiments on KTH*.\n\n      We clarified these points and included the new results in the revised version of the submission in Section 4 and Table 2. \n\n\n    - FVD metric: \n\n      We would like to thank the reviewer for suggesting this additional metric. We provide additional FVD scores for all tested methods on KTH and BAIR, as well as an analysis of these results in Table 2 and 3, and  Section 4. These results experimentally confirm our previous findings with respect to PSNR, SSIM, and LPIPS, i.e., we outperform the state of the art on KTH and are on par with state-of-the-art methods on BAIR. We believe that FVD is complementary to previously considered metrics, and we include an additional discussion on this matter in Section 4.\n\n\n    - PlaNet:\n\n      We thank the reviewer for pointing out the missing reference [4], that we added in the related work section. The differences with our model are the following. It is a control model that focuses and is evaluated on planning tasks which are out of scope of our paper. Also, [4] uses a dynamic model that is close to the one of [5] (with additional latent overshooting) based on recurrent neural networks, unlike our residual model.\n\n\n    - Website:\n\n      We provide an anonymized website at:  https://sites.google.com/view/srvp/. It hosts generated videos with qualitative comparisons, along with the corresponding anonymized code.\n\n\nWe hope that we were able to answer the questions adequately and to alleviate raised concerns.\n\n\n[1] Minderer et al. Unsupervised Learning of Object Structure and Dynamics from Videos. In NeurIPS 2019.\n[2] Gregor et al. Temporal Difference Variational Auto-Encoder. In ICLR 2019.\n[3] Chung et al. A Recurrent Latent Variable Model for Sequential Data. In NIPS 2015.\n[4] Hafner et al. Learning Latent Dynamics for Planning from Pixels. In ICML 2019.\n[5] Fraccaro et al. Sequential Neural Models with Stochastic Layers. In NIPS 2016.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1777/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1777/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jean-yves.franceschi@lip6.fr", "edouard.delasalles@lip6.fr", "mickael.chen@lip6.fr", "sylvain.lamprier@lip6.fr", "patrick.gallinari@lip6.fr"], "title": "Stochastic Latent Residual Video Prediction", "authors": ["Jean-Yves Franceschi", "Edouard Delasalles", "Mickael Chen", "Sylvain Lamprier", "Patrick Gallinari"], "pdf": "/pdf/01c19eb1a00a822df4d28c39840c317978b2e296.pdf", "abstract": "Video prediction is a challenging task: models have to account for the inherent uncertainty of the future. Most works in the literature are based on stochastic image-autoregressive recurrent networks, raising several performance and applicability issues. An alternative is to use fully latent temporal models which untie frame synthesis and dynamics. However, no such model for video prediction has been proposed in the literature yet, due to design and training difficulties. In this paper, we overcome these difficulties by introducing a novel stochastic temporal model. It is based on residual updates of a latent state, motivated by discretization schemes of differential equations. This first-order principle naturally models video dynamics as it allows our simpler, lightweight, interpretable, latent model to outperform prior state-of-the-art methods on challenging datasets.", "code": "https://sites.google.com/view/srvp/", "keywords": ["stochastic video prediction", "variational autoencoder", "residual dynamics"], "paperhash": "franceschi|stochastic_latent_residual_video_prediction", "original_pdf": "/attachment/c268e015553776923f70d24a3f8f73c7afa4a8fc.pdf", "_bibtex": "@misc{\nfranceschi2020stochastic,\ntitle={Stochastic Latent Residual Video Prediction},\nauthor={Jean-Yves Franceschi and Edouard Delasalles and Mickael Chen and Sylvain Lamprier and Patrick Gallinari},\nyear={2020},\nurl={https://openreview.net/forum?id=HyeqPJHYvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HyeqPJHYvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1777/Authors", "ICLR.cc/2020/Conference/Paper1777/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1777/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1777/Reviewers", "ICLR.cc/2020/Conference/Paper1777/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1777/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1777/Authors|ICLR.cc/2020/Conference/Paper1777/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504151034, "tmdate": 1576860557231, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1777/Authors", "ICLR.cc/2020/Conference/Paper1777/Reviewers", "ICLR.cc/2020/Conference/Paper1777/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1777/-/Official_Comment"}}}, {"id": "BJePei82sr", "original": null, "number": 2, "cdate": 1573837550632, "ddate": null, "tcdate": 1573837550632, "tmdate": 1573837550632, "tddate": null, "forum": "HyeqPJHYvH", "replyto": "S1efBB1AYS", "invitation": "ICLR.cc/2020/Conference/Paper1777/-/Official_Comment", "content": {"title": "Answer to Reviewer 1", "comment": "We would like to thank the reviewer for his comprehensive and useful feedback. We updated the submission accordingly and provide answers and comments below.\n\n    - Missing references and discussions:\n\n      We thank the reviewer for pointing out missing references [1] and the recent [2]. We included them in the manuscript as related work along with discussions regarding differences with our work.\n\n      [1] tackles video prediction in a deterministic setting, while we focus on stochastic prediction instead. As its architecture doesn\u2019t include stochastic components, it cannot produce a diversity of predictions by design.\n\n      [2] is orthogonal to our work as they improve the VRNN dynamic model by using  frame-wise key-point representations instead of raw frames, while we focus on improving the dynamic model itself. Specifically, we pointed out that re-using predicted frames to perform future frame predictions leads to prediction error accumulation over time and should be avoided. As [2] uses VRNN as a backbone dynamic model, it is autoregressive, albeit in the key-point space instead of the pixel space. While this change could mitigate the above-mentioned problem, the extent of such mitigation is unclear. We believe both approaches to be complementary. It would be an interesting future work to study the behavior of our model in the setting of [2] by replacing their VRNN-based temporal component by ours. This is, however, outside the scope of this paper.\n\n\n    - Generation at arbitrary frame rates:\n\n      We would like to bring to the reviewer\u2019s attention that our model was not meant to serve as an interpolation method and is not trained as such. Instead, we presented experiments regarding the ability to generate at higher frame rates in order to analyze the dynamics learned by our model. The fact that our model maintains the same performance when generating at a higher framerate, without further training, shows that it learned a continuous dynamic driven by a piecewise ODE (i.e., the learned dynamic of each interval between two consecutive frames is driven by an ODE). We emphasized this point in Sections 3.1 and 4 of the manuscript revision.\n\n\n    - Further study of the update rule:\n\n      In addition to the previous experiments on deterministic and stochastic Moving MNIST, we performed further experiments on the KTH dataset to complement our analysis of the effects of the update rule. Results are presented in Table 2 and further discussed in Section 4 of the revised manuscript. They confirm the structural advantage in terms of performance of residual updates compared to other considered update methods (GRU and MLP). The MLP version better captures dynamics than GRU (in terms of PSNR and SSIM), but drops in terms of realism (captured by LPIPS and FVD). On the other hand, the residual version slightly improves this gain in terms of dynamics but significantly pushes further the realism of the generated videos. This observed advantage comes from the well documented structural prior that the residual update establishes in the network [3, 4].\n\n\n    - Interpolation of dynamics:\n\n      The experiment showing interpolation of dynamics in Figure 9 is designed to assess the interpretability and representation quality of the learned latent space. Such experiments are standard in the literature of generative models [5],  but could not have been proposed in the stochastic video prediction community before, as it takes advantage of the state-space nature of our model. We believe that these considerations are another interesting argument in favor of state-space generative models.\n\n\n    - FVD metric:\n\n      We would like to thank the reviewer for suggesting this additional metric. We provide additional FVD scores for all tested methods on KTH and BAIR, as well as an analysis of these results in Table 2 and 3, and  Section 4. These results experimentally confirm our previous findings with respect to PSNR, SSIM, and LPIPS, i.e., we outperform the state of the art on KTH and are on par with state-of-the-art methods on BAIR. We believe that FVD is complementary to previously considered metrics, and we include an additional discussion on this matter in Section 4.\n\n\n    - We changed the wording from \u201cresidue\u201d to \u201cresidual\u201d.\n\n    - The error bars correspond to the 95%-confidence interval of the plotted mean, similarly to [6]. We added this information in the figure captions.\n\n\nWe hope that we were able to alleviate raised concerns.\n\n\n[1] Wichers et al. Hierarchical Long-term Video Prediction without Supervision. In ICML 2018.\n[2] Minderer et al. Unsupervised Learning of Object Structure and Dynamics from Videos. In NeurIPS 2019.\n[3] Chen et al. Neural Ordinary Differential Equations. In NeurIPS 2018.\n[4] He et al. Deep Residual Learning for Image Recognition. In CVPR 2016.\n[5] Kingma et al. Glow: Generative Flow with Invertible 1x1 Convolutions. In NeurIPS 2018."}, "signatures": ["ICLR.cc/2020/Conference/Paper1777/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1777/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jean-yves.franceschi@lip6.fr", "edouard.delasalles@lip6.fr", "mickael.chen@lip6.fr", "sylvain.lamprier@lip6.fr", "patrick.gallinari@lip6.fr"], "title": "Stochastic Latent Residual Video Prediction", "authors": ["Jean-Yves Franceschi", "Edouard Delasalles", "Mickael Chen", "Sylvain Lamprier", "Patrick Gallinari"], "pdf": "/pdf/01c19eb1a00a822df4d28c39840c317978b2e296.pdf", "abstract": "Video prediction is a challenging task: models have to account for the inherent uncertainty of the future. Most works in the literature are based on stochastic image-autoregressive recurrent networks, raising several performance and applicability issues. An alternative is to use fully latent temporal models which untie frame synthesis and dynamics. However, no such model for video prediction has been proposed in the literature yet, due to design and training difficulties. In this paper, we overcome these difficulties by introducing a novel stochastic temporal model. It is based on residual updates of a latent state, motivated by discretization schemes of differential equations. This first-order principle naturally models video dynamics as it allows our simpler, lightweight, interpretable, latent model to outperform prior state-of-the-art methods on challenging datasets.", "code": "https://sites.google.com/view/srvp/", "keywords": ["stochastic video prediction", "variational autoencoder", "residual dynamics"], "paperhash": "franceschi|stochastic_latent_residual_video_prediction", "original_pdf": "/attachment/c268e015553776923f70d24a3f8f73c7afa4a8fc.pdf", "_bibtex": "@misc{\nfranceschi2020stochastic,\ntitle={Stochastic Latent Residual Video Prediction},\nauthor={Jean-Yves Franceschi and Edouard Delasalles and Mickael Chen and Sylvain Lamprier and Patrick Gallinari},\nyear={2020},\nurl={https://openreview.net/forum?id=HyeqPJHYvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HyeqPJHYvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1777/Authors", "ICLR.cc/2020/Conference/Paper1777/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1777/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1777/Reviewers", "ICLR.cc/2020/Conference/Paper1777/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1777/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1777/Authors|ICLR.cc/2020/Conference/Paper1777/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504151034, "tmdate": 1576860557231, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1777/Authors", "ICLR.cc/2020/Conference/Paper1777/Reviewers", "ICLR.cc/2020/Conference/Paper1777/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1777/-/Official_Comment"}}}, {"id": "r1xmnYInjH", "original": null, "number": 1, "cdate": 1573837227152, "ddate": null, "tcdate": 1573837227152, "tmdate": 1573837227152, "tddate": null, "forum": "HyeqPJHYvH", "replyto": "HyeqPJHYvH", "invitation": "ICLR.cc/2020/Conference/Paper1777/-/Official_Comment", "content": {"title": "Rebuttal and Revision", "comment": "We would like to thank the reviewers for their comprehensive, insightful and encouraging comments. We answered individually to each of them. Below, we summarize the main changes made in the revision of the manuscript.\n\n    - We added FVD scores for all compared methods in Tables 2 and 3, as well as an additional analysis in Section 4. We outperform other methods on this metric on the KTH dataset, and are on a par with the state of the art on BAIR.\n\n    - We extend the study of our dynamic model to the KTH dataset on Section 4, and reported new results on Table 2. They confirm our findings on the Moving MNIST dataset showing the structural advantage of residual dynamics.\n\n    - We emphasized the role of the content variable in decoupling dynamic model and visual features in Section 3.2 with an additional explanation.\n\n    - We further clarified the differences between our method and SVG in Sections 2 and 4. \n\n    - We added references [1, 2, 3] to the related work with additional discussions.\n\nWe would like to highlight the anonymized project webpage containing the code corresponding to our method as well as video samples: https://sites.google.com/view/srvp/.\n\n[1] Minderer et al. Unsupervised Learning of Object Structure and Dynamics from Videos. In NeurIPS 2019.\n[2] Wichers et al. Hierarchical Long-term Video Prediction without Supervision. In ICML 2018.\n[3] Hafner et al. Learning Latent Dynamics for Planning from Pixels. In ICML 2019."}, "signatures": ["ICLR.cc/2020/Conference/Paper1777/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1777/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jean-yves.franceschi@lip6.fr", "edouard.delasalles@lip6.fr", "mickael.chen@lip6.fr", "sylvain.lamprier@lip6.fr", "patrick.gallinari@lip6.fr"], "title": "Stochastic Latent Residual Video Prediction", "authors": ["Jean-Yves Franceschi", "Edouard Delasalles", "Mickael Chen", "Sylvain Lamprier", "Patrick Gallinari"], "pdf": "/pdf/01c19eb1a00a822df4d28c39840c317978b2e296.pdf", "abstract": "Video prediction is a challenging task: models have to account for the inherent uncertainty of the future. Most works in the literature are based on stochastic image-autoregressive recurrent networks, raising several performance and applicability issues. An alternative is to use fully latent temporal models which untie frame synthesis and dynamics. However, no such model for video prediction has been proposed in the literature yet, due to design and training difficulties. In this paper, we overcome these difficulties by introducing a novel stochastic temporal model. It is based on residual updates of a latent state, motivated by discretization schemes of differential equations. This first-order principle naturally models video dynamics as it allows our simpler, lightweight, interpretable, latent model to outperform prior state-of-the-art methods on challenging datasets.", "code": "https://sites.google.com/view/srvp/", "keywords": ["stochastic video prediction", "variational autoencoder", "residual dynamics"], "paperhash": "franceschi|stochastic_latent_residual_video_prediction", "original_pdf": "/attachment/c268e015553776923f70d24a3f8f73c7afa4a8fc.pdf", "_bibtex": "@misc{\nfranceschi2020stochastic,\ntitle={Stochastic Latent Residual Video Prediction},\nauthor={Jean-Yves Franceschi and Edouard Delasalles and Mickael Chen and Sylvain Lamprier and Patrick Gallinari},\nyear={2020},\nurl={https://openreview.net/forum?id=HyeqPJHYvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HyeqPJHYvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1777/Authors", "ICLR.cc/2020/Conference/Paper1777/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1777/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1777/Reviewers", "ICLR.cc/2020/Conference/Paper1777/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1777/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1777/Authors|ICLR.cc/2020/Conference/Paper1777/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504151034, "tmdate": 1576860557231, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1777/Authors", "ICLR.cc/2020/Conference/Paper1777/Reviewers", "ICLR.cc/2020/Conference/Paper1777/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1777/-/Official_Comment"}}}, {"id": "S1efBB1AYS", "original": null, "number": 3, "cdate": 1571841338374, "ddate": null, "tcdate": 1571841338374, "tmdate": 1572972424777, "tddate": null, "forum": "HyeqPJHYvH", "replyto": "HyeqPJHYvH", "invitation": "ICLR.cc/2020/Conference/Paper1777/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary:\n\nThe paper proposes a video prediction method based on State-Space Models. The paper describes two main contributions:\n\n1. By learning dynamics in the latent state space, the method avoids the high computational cost and accumulating image reconstruction errors of autoregressive models that condition on generated frames.\n\n2. To model dynamics, the paper proposes a residual update rule inspired by Euler\u2019s method to solve ODEs. According to this rule, the update to the latent state y_t is modeled as an additive residual f(y_t, z_{t+1}). This has the advantage that the step size of the discretization can be adjusted freely, e.g. between training and inference. \n\nThe paper provides extensive experimental comparison of their model to the SVG and SAVP models on several standard datasets. The paper further contains experiments illustrating features of the model such as disentangling dynamics and content, and interpolation of dynamics in the latent space.\n\nDecision:\n\nThe paper is written clearly and the mathematical treatment and experiments appear rigorous. The idea of predicting video using state-space models is interesting and promising. However, as described below, the paper overstates its novelty and falls short of showing the advantages of the method beyond incremental improvements on frame-wise image quality metrics. I therefore suggest rejection in its current version.\n\nSupporting arguments and suggestions:\n\n1. The idea to use fully latent models for video prediction, to untie frame synthesis and dynamics, is not new and the paper does not fully cite this literature. For example, [1] and [2] perform unsupervised, non-autoregressive video prediction. The differences to these models should be discussed.\n\n2. The advantages of the residual update rule are not made clear enough. The parallels to the ODE literature seem tenuous. The main advantage described in the paper is the ability to synthesize videos at different frame rates, but interpolation over such short time horizons is not a hard problem. At least, the paper should compare to existing methods for frame interpolation. Apart from interpolation (variable step size), it appears that the update rule could be changed from y_{t+1} = y_t + f(y_t, z_{t+1}) to y_{t+1} = f(y_t, z_{t+1}) without impact to the model. How is it different from the standard VRNN formulation [3]? More experiments to show the advantage of the proposed update rule would be helpful.\n\n3. Some of the experiments seem like interesting starting points but do not support general claims. For example, Fig 2 (b) shows that the proposed dynamics model is better than an MLP or GRU on deterministic Moving MNIST, but is this also true on real datasets, which have much more complex dynamics? Similarly, the interpolation in Figure 9 is intriguing, but it would be helpful to describe and test how this ability is useful for applications of the predictive model.\n\n4. The comparisons use frame-wise metrics of image quality (PSNR, SSIM, LPIPS). Even though they are common in the literature, these metrics are unsuitable for comparing long video sequences due to their stochasticity. The metrics are probably dominated by relatively uninteresting features such as the quality of the static background. Metrics for comparing entire videos exist (e.g. FVD [4]) and should be used. Even better, the paper should demonstrate the usefulness of the model for downstream tasks such as reinforcement learning, although I understand that this may be out of scope.\n\nMinor comments:\n\n- As far as I know, the correct term for error terms is residual, not residue.\n- What do the error bars in the figures show? Please add this information to the figure legends.\n\n[1] Wichers et al., 2018, https://arxiv.org/pdf/1806.04768.pdf\n[2] Minderer et al., 2019, https://arxiv.org/abs/1906.07889\n[3] Chung et al, 2015, https://arxiv.org/abs/1506.02216\n[4] Unterthiner et al., 2018, https://arxiv.org/abs/1812.01717"}, "signatures": ["ICLR.cc/2020/Conference/Paper1777/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1777/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jean-yves.franceschi@lip6.fr", "edouard.delasalles@lip6.fr", "mickael.chen@lip6.fr", "sylvain.lamprier@lip6.fr", "patrick.gallinari@lip6.fr"], "title": "Stochastic Latent Residual Video Prediction", "authors": ["Jean-Yves Franceschi", "Edouard Delasalles", "Mickael Chen", "Sylvain Lamprier", "Patrick Gallinari"], "pdf": "/pdf/01c19eb1a00a822df4d28c39840c317978b2e296.pdf", "abstract": "Video prediction is a challenging task: models have to account for the inherent uncertainty of the future. Most works in the literature are based on stochastic image-autoregressive recurrent networks, raising several performance and applicability issues. An alternative is to use fully latent temporal models which untie frame synthesis and dynamics. However, no such model for video prediction has been proposed in the literature yet, due to design and training difficulties. In this paper, we overcome these difficulties by introducing a novel stochastic temporal model. It is based on residual updates of a latent state, motivated by discretization schemes of differential equations. This first-order principle naturally models video dynamics as it allows our simpler, lightweight, interpretable, latent model to outperform prior state-of-the-art methods on challenging datasets.", "code": "https://sites.google.com/view/srvp/", "keywords": ["stochastic video prediction", "variational autoencoder", "residual dynamics"], "paperhash": "franceschi|stochastic_latent_residual_video_prediction", "original_pdf": "/attachment/c268e015553776923f70d24a3f8f73c7afa4a8fc.pdf", "_bibtex": "@misc{\nfranceschi2020stochastic,\ntitle={Stochastic Latent Residual Video Prediction},\nauthor={Jean-Yves Franceschi and Edouard Delasalles and Mickael Chen and Sylvain Lamprier and Patrick Gallinari},\nyear={2020},\nurl={https://openreview.net/forum?id=HyeqPJHYvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HyeqPJHYvH", "replyto": "HyeqPJHYvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1777/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1777/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576084602230, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1777/Reviewers"], "noninvitees": [], "tcdate": 1570237732438, "tmdate": 1576084602243, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1777/-/Official_Review"}}}, {"id": "r1xt2GQpFH", "original": null, "number": 2, "cdate": 1571791536871, "ddate": null, "tcdate": 1571791536871, "tmdate": 1572972424726, "tddate": null, "forum": "HyeqPJHYvH", "replyto": "HyeqPJHYvH", "invitation": "ICLR.cc/2020/Conference/Paper1777/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The paper proposes a video prediction model which explicitly decouples frame synthesis and motion dynamics. This is a very subtle change (compared to the current models) that can result in higher quality predictions.\n\nFirst of all, the paper is extremely well written. It provides clear motivations and goals, as well as an impressively comprehensive related work that discusses their shortcomings. The experiments are comprehensive and provide good support for the claims. And finally, the appendix presents additional visualization and information. \n\nOn the main proposed method, it is a very subtle but reasonable change. Therefore, my suggestion to the authors is to provide a more thorough comparison with existing methods specifically SVG (Denton 2018) since the models share a lot of similarities. It is also quite similar to PlaNet (Hafner 2019). This is where the paper can be improved.\n\nFor the experiments, although they are quite comprehensive, there is still room for improvement. First, none of the metrics used are good evaluation metrics for frame prediction (I know they are quite common but that doesn't make them good) as they do not give us an objective evaluation in the sense of the semantic quality of predicted frames, specially for long videos. It really helps if the authors present additional quantitative evaluation to show that the predicted frames contain useful semantic information with metrics such as FVD and Inception score. Second, a pretribulation study is required to see where the improvements are coming from. Is it from a different architecture or the separation of dynamics?  Finally, a website with generated videos really helps for qualitative comparison! \n\nOverall, this is a well-written paper with clear motivations and goals. I find the impact of the paper to be marginal (given the quality difference with already existing models) which can be improved by emphasizing more on other aspects such as disentanglement.  "}, "signatures": ["ICLR.cc/2020/Conference/Paper1777/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1777/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jean-yves.franceschi@lip6.fr", "edouard.delasalles@lip6.fr", "mickael.chen@lip6.fr", "sylvain.lamprier@lip6.fr", "patrick.gallinari@lip6.fr"], "title": "Stochastic Latent Residual Video Prediction", "authors": ["Jean-Yves Franceschi", "Edouard Delasalles", "Mickael Chen", "Sylvain Lamprier", "Patrick Gallinari"], "pdf": "/pdf/01c19eb1a00a822df4d28c39840c317978b2e296.pdf", "abstract": "Video prediction is a challenging task: models have to account for the inherent uncertainty of the future. Most works in the literature are based on stochastic image-autoregressive recurrent networks, raising several performance and applicability issues. An alternative is to use fully latent temporal models which untie frame synthesis and dynamics. However, no such model for video prediction has been proposed in the literature yet, due to design and training difficulties. In this paper, we overcome these difficulties by introducing a novel stochastic temporal model. It is based on residual updates of a latent state, motivated by discretization schemes of differential equations. This first-order principle naturally models video dynamics as it allows our simpler, lightweight, interpretable, latent model to outperform prior state-of-the-art methods on challenging datasets.", "code": "https://sites.google.com/view/srvp/", "keywords": ["stochastic video prediction", "variational autoencoder", "residual dynamics"], "paperhash": "franceschi|stochastic_latent_residual_video_prediction", "original_pdf": "/attachment/c268e015553776923f70d24a3f8f73c7afa4a8fc.pdf", "_bibtex": "@misc{\nfranceschi2020stochastic,\ntitle={Stochastic Latent Residual Video Prediction},\nauthor={Jean-Yves Franceschi and Edouard Delasalles and Mickael Chen and Sylvain Lamprier and Patrick Gallinari},\nyear={2020},\nurl={https://openreview.net/forum?id=HyeqPJHYvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HyeqPJHYvH", "replyto": "HyeqPJHYvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1777/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1777/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576084602230, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1777/Reviewers"], "noninvitees": [], "tcdate": 1570237732438, "tmdate": 1576084602243, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1777/-/Official_Review"}}}], "count": 10}