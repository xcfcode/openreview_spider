{"notes": [{"id": "HyeuP2EtDB", "original": "rJghCqTWBS", "number": 10, "cdate": 1569438816193, "ddate": null, "tcdate": 1569438816193, "tmdate": 1577168288722, "tddate": null, "forum": "HyeuP2EtDB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Scoring-Aggregating-Planning: Learning task-agnostic priors from interactions and sparse rewards for zero-shot generalization", "authors": ["Huazhe Xu", "Boyuan Chen", "Yang Gao", "Trevor Darrell"], "authorids": ["huazhe_xu@eecs.berkeley.edu", "boyuanchen@berkeley.edu", "yg@eecs.berkeley.edu", "trevordarrell@eecs.berkeley.edu"], "keywords": ["learning priors from exploration data", "policy zero-shot generalization", "reward shaping", "model-based"], "TL;DR": "We learn dense scores and dynamics model as priors from exploration data and use them to induce a good policy in new tasks in zero-shot condition.", "abstract": "Humans can learn task-agnostic priors from interactive experience and utilize the priors for novel tasks without any finetuning. In this paper, we propose Scoring-Aggregating-Planning (SAP), a framework that can learn task-agnostic semantics and dynamics priors from arbitrary quality interactions as well as the corresponding sparse rewards and then plan on unseen tasks in zero-shot condition. The framework finds a neural score function for local regional state and action pairs that can be aggregated to approximate the quality of a full trajectory; moreover, a dynamics model that is learned with self-supervision can be incorporated for planning. Many of previous works that leverage interactive data for policy learning either need massive on-policy environmental interactions or assume access to expert data while we can achieve a similar goal with pure off-policy imperfect data. Instantiating our framework results in a generalizable policy to unseen tasks. Experiments demonstrate that the proposed method can outperform baseline methods on a wide range of applications including gridworld, robotics tasks and video games.", "pdf": "/pdf/a86483704a793bacecf55a3183a94d366ff62e2f.pdf", "paperhash": "xu|scoringaggregatingplanning_learning_taskagnostic_priors_from_interactions_and_sparse_rewards_for_zeroshot_generalization", "original_pdf": "/attachment/6dc2567dbddb5de472e79327c50bf13e970ebc8c.pdf", "_bibtex": "@misc{\nxu2020scoringaggregatingplanning,\ntitle={Scoring-Aggregating-Planning: Learning task-agnostic priors from interactions and sparse rewards for zero-shot generalization},\nauthor={Huazhe Xu and Boyuan Chen and Yang Gao and Trevor Darrell},\nyear={2020},\nurl={https://openreview.net/forum?id=HyeuP2EtDB}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "s6kX83JFzL", "original": null, "number": 1, "cdate": 1576798684828, "ddate": null, "tcdate": 1576798684828, "tmdate": 1576800950037, "tddate": null, "forum": "HyeuP2EtDB", "replyto": "HyeuP2EtDB", "invitation": "ICLR.cc/2020/Conference/Paper10/-/Decision", "content": {"decision": "Reject", "comment": "The paper proposes an algorithm for zero-shot generalization in RL via learning a scoring a function from.\n\nThe reviewers had mixed feelings, and many were not from the area. A shared theme was doubts about the significance of the experimental setting, and also the generality of the approach.\n\nAs this is my field, I read the paper, and recommend rejection at this time. The proposed method is quite laborious and requires quite a bit of assumptions on the environments to work, as well as fine tuning parameters for each considered task (number of regions, etc). I also agree that the evaluation is not convincing -- stronger baselines need to be considered and the experiments to better address the zero-shot transfer aspect that the paper is motivated by. I encourage the authors to take the review feedback into account and submit a future version to another venue.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Scoring-Aggregating-Planning: Learning task-agnostic priors from interactions and sparse rewards for zero-shot generalization", "authors": ["Huazhe Xu", "Boyuan Chen", "Yang Gao", "Trevor Darrell"], "authorids": ["huazhe_xu@eecs.berkeley.edu", "boyuanchen@berkeley.edu", "yg@eecs.berkeley.edu", "trevordarrell@eecs.berkeley.edu"], "keywords": ["learning priors from exploration data", "policy zero-shot generalization", "reward shaping", "model-based"], "TL;DR": "We learn dense scores and dynamics model as priors from exploration data and use them to induce a good policy in new tasks in zero-shot condition.", "abstract": "Humans can learn task-agnostic priors from interactive experience and utilize the priors for novel tasks without any finetuning. In this paper, we propose Scoring-Aggregating-Planning (SAP), a framework that can learn task-agnostic semantics and dynamics priors from arbitrary quality interactions as well as the corresponding sparse rewards and then plan on unseen tasks in zero-shot condition. The framework finds a neural score function for local regional state and action pairs that can be aggregated to approximate the quality of a full trajectory; moreover, a dynamics model that is learned with self-supervision can be incorporated for planning. Many of previous works that leverage interactive data for policy learning either need massive on-policy environmental interactions or assume access to expert data while we can achieve a similar goal with pure off-policy imperfect data. Instantiating our framework results in a generalizable policy to unseen tasks. Experiments demonstrate that the proposed method can outperform baseline methods on a wide range of applications including gridworld, robotics tasks and video games.", "pdf": "/pdf/a86483704a793bacecf55a3183a94d366ff62e2f.pdf", "paperhash": "xu|scoringaggregatingplanning_learning_taskagnostic_priors_from_interactions_and_sparse_rewards_for_zeroshot_generalization", "original_pdf": "/attachment/6dc2567dbddb5de472e79327c50bf13e970ebc8c.pdf", "_bibtex": "@misc{\nxu2020scoringaggregatingplanning,\ntitle={Scoring-Aggregating-Planning: Learning task-agnostic priors from interactions and sparse rewards for zero-shot generalization},\nauthor={Huazhe Xu and Boyuan Chen and Yang Gao and Trevor Darrell},\nyear={2020},\nurl={https://openreview.net/forum?id=HyeuP2EtDB}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "HyeuP2EtDB", "replyto": "HyeuP2EtDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795711523, "tmdate": 1576800260748, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper10/-/Decision"}}}, {"id": "r1lfJzG3qS", "original": null, "number": 3, "cdate": 1572770265972, "ddate": null, "tcdate": 1572770265972, "tmdate": 1573461185360, "tddate": null, "forum": "HyeuP2EtDB", "replyto": "HyeuP2EtDB", "invitation": "ICLR.cc/2020/Conference/Paper10/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #3", "review": "The paper proposes a framework (Scoring-Aggregating-Planning (SAP)) for learning task-agnostic priors that allow generalization to new tasks without finetuning. The motivation for this is very clear - humans can perform much better than machines in zero-shot conditions because humans have learned priors about objects, semantics, physics, etc. This is achieved by learning a scoring function based on the final reward and a self-supervised learned dynamics model.\n\nOverall, the paper is very clear and easy to follow.\nThe presented task is realistic and important, and the paper seems to address it in a reasonable approach. \nHowever, the evaluation seems lacking to me - the evaluation convinced me that SAP works, but I am not convinced that it works better than existing approaches (see below), and especially did not convince me that it is better in the zero-shot test environment.\nThe (anonymized) website contains nice videos that support the submission.\n\nQuestions for the authors:\n\n1. Page 3, 3rd paragraph of Section 3: the paper says that \"The proposed formulation requires much less information and thus more realistic and feasible\" - I agree that this is more realistic, but is it really more feasible? The requirement of much less information makes the proposed formulation much more sparse.\n\n2. A basic assumption in the SAP framework is that a local region score is a sum of all the sub-regions. As phrased in the paper: \"in the physical world, there is usually some level of rotational or transnational invariance\". I'm not sure that this assumption makes sense neither in the Mario case or in other tasks, e.g., robotics. Doesn't it matter if you have a \"turtle\" right in front of you (which means that the turtle is going to hit you), or below you (which means that you are going hit the turtle)?\n\n3. A question about the planning phase - page 5 says: \"We select the action sequence that gives us the best-aggregated score and execute the first action\". Do you select the entire sequence of actions in the new environment in advance? Can the agent observe the new state after every action, and decide on the next action based on the actual step that the action has reached, rather than on the state that was approximated in advance?\nIn other words - what happens if the first action in the new test environment yields an unexpected state, that was not predicted well by the dynamics model; does the agent continue on the initial planned trajectory (that ignores the \"surprise\"), or does it compute its next action based on the unexpected state?\n \n4. Experiments: in Gridworld and Mario - are there any stronger baselines in the literature, or reductions of known baselines to the zero-shot scenario? Are the chosen \"Human Priors\", BC-random and BC-SAP just strawmen? \nSince the main goal of this paper is the zero-shot task, what would convince me is a state-of-the-art model that does possibly *better than SAP on the training level*, but *worse than SAP in generalizing to the new level*. Additionally, are there other baselines that specifically address the zero-shot task in the literature?\n\nMinor (did not impact score):\nPage 2, 1st paragraph: \"... we show that how an intelligent agent\"...\nPage 3, 3rd paragraph: \"... in model-free RL problem\" - missing an \"a\" or \"problem*s*\"?\nPage 3, 3rd paragraph: \". Model based method ...\" - missing an \"a\" as well?\nPage 4, 1st paragraph:: \"... utilizing the to get the ...\"\nPage 4, last row: missing a dot after the loss equation, before the word \"In\".\nPage 7, Table 1: \"BC-random\" is called \"BC-data\" in the text. Aren't they the same thing?\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper10/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper10/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Scoring-Aggregating-Planning: Learning task-agnostic priors from interactions and sparse rewards for zero-shot generalization", "authors": ["Huazhe Xu", "Boyuan Chen", "Yang Gao", "Trevor Darrell"], "authorids": ["huazhe_xu@eecs.berkeley.edu", "boyuanchen@berkeley.edu", "yg@eecs.berkeley.edu", "trevordarrell@eecs.berkeley.edu"], "keywords": ["learning priors from exploration data", "policy zero-shot generalization", "reward shaping", "model-based"], "TL;DR": "We learn dense scores and dynamics model as priors from exploration data and use them to induce a good policy in new tasks in zero-shot condition.", "abstract": "Humans can learn task-agnostic priors from interactive experience and utilize the priors for novel tasks without any finetuning. In this paper, we propose Scoring-Aggregating-Planning (SAP), a framework that can learn task-agnostic semantics and dynamics priors from arbitrary quality interactions as well as the corresponding sparse rewards and then plan on unseen tasks in zero-shot condition. The framework finds a neural score function for local regional state and action pairs that can be aggregated to approximate the quality of a full trajectory; moreover, a dynamics model that is learned with self-supervision can be incorporated for planning. Many of previous works that leverage interactive data for policy learning either need massive on-policy environmental interactions or assume access to expert data while we can achieve a similar goal with pure off-policy imperfect data. Instantiating our framework results in a generalizable policy to unseen tasks. Experiments demonstrate that the proposed method can outperform baseline methods on a wide range of applications including gridworld, robotics tasks and video games.", "pdf": "/pdf/a86483704a793bacecf55a3183a94d366ff62e2f.pdf", "paperhash": "xu|scoringaggregatingplanning_learning_taskagnostic_priors_from_interactions_and_sparse_rewards_for_zeroshot_generalization", "original_pdf": "/attachment/6dc2567dbddb5de472e79327c50bf13e970ebc8c.pdf", "_bibtex": "@misc{\nxu2020scoringaggregatingplanning,\ntitle={Scoring-Aggregating-Planning: Learning task-agnostic priors from interactions and sparse rewards for zero-shot generalization},\nauthor={Huazhe Xu and Boyuan Chen and Yang Gao and Trevor Darrell},\nyear={2020},\nurl={https://openreview.net/forum?id=HyeuP2EtDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HyeuP2EtDB", "replyto": "HyeuP2EtDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper10/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper10/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575652862939, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper10/Reviewers"], "noninvitees": [], "tcdate": 1570237758448, "tmdate": 1575652862953, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper10/-/Official_Review"}}}, {"id": "rJlU4taNsr", "original": null, "number": 9, "cdate": 1573341486021, "ddate": null, "tcdate": 1573341486021, "tmdate": 1573353667310, "tddate": null, "forum": "HyeuP2EtDB", "replyto": "HyghHQBr9S", "invitation": "ICLR.cc/2020/Conference/Paper10/-/Official_Comment", "content": {"title": "Response - part (2/2)", "comment": "\n\u2014-\u201d it is not intuitive whether it has something to do with learning zero-shot generalization\u201d\n\nThe proposed architecture learns to decompose the final evaluation into subregion- action pair scores. This score is zero-shot transferable when the visual similarity between subregions holds. This is intuitive that visually similar subregions will share similar scores across different configurations. Previous works focus on learning transferable skills between domains, while our method focuses on transferable subregions in the observation space. Empirically, we find such improvement can lead to zero-shot behavioral transfer with a learned dynamics model.   \n \n\n\u2014- \u201cThe method will also rely on the similarity between the pretrained task and the target task, and such a scope constraint is not discussed in the paper. not quite sure a better architecture is fundamental progress towards zero-shot RL.\u201d\n\nThe reviewer questions whether a better architecture is a fundamental progress towards zero-shot RL. We think that the answer is yes, and at the same time, we admit and agree that the success of zero-shot reinforcement learning might also depend on other innovations, such as data representations, compositional skills, etc. This has been proved in the image classification task. People found that convolution neural network is significantly better than the previous hand-designed SIFT + Fisher vector methods. The input and output haven't changed, but only the architecture of the learner has changed. These architectural improvements have also brought us better generalization, including zero-shot generalization in the image classification domain. We believe that architectural improvements will also be fundamental in policy generalizations as well. However, incorporating SAP with other innovations such as compositional skills can be a promising future direction.\n\nYou are also correct about the similarity assumption; however, this similarity is texture/color/instance level visual similarity rather than task-level similarity. Even with this assumption, the tasks still leave the generalization between different configurations burden to the learning algorithm. In Dubey, Rachit, et al. \"Investigating human priors for playing video games.\" arXiv preprint arXiv:1802.10217 (2018). , they thoroughly described different types of priors in video games including visual similarity. \n\n[1] Higgins, Irina, et al. \"Darla: Improving zero-shot transfer in reinforcement learning.\" Proceedings of the 34th International Conference on Machine Learning-Volume 70. JMLR. org, 2017.\n[2]Arjona-Medina, Jose A., et al. \"Rudder: Return decomposition for delayed rewards.\" arXiv preprint arXiv:1806.07857 (2018).\n[3] Burda, Yuri, et al. \"Exploration by random network distillation.\" arXiv preprint arXiv:1810.12894 (2018)."}, "signatures": ["ICLR.cc/2020/Conference/Paper10/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper10/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Scoring-Aggregating-Planning: Learning task-agnostic priors from interactions and sparse rewards for zero-shot generalization", "authors": ["Huazhe Xu", "Boyuan Chen", "Yang Gao", "Trevor Darrell"], "authorids": ["huazhe_xu@eecs.berkeley.edu", "boyuanchen@berkeley.edu", "yg@eecs.berkeley.edu", "trevordarrell@eecs.berkeley.edu"], "keywords": ["learning priors from exploration data", "policy zero-shot generalization", "reward shaping", "model-based"], "TL;DR": "We learn dense scores and dynamics model as priors from exploration data and use them to induce a good policy in new tasks in zero-shot condition.", "abstract": "Humans can learn task-agnostic priors from interactive experience and utilize the priors for novel tasks without any finetuning. In this paper, we propose Scoring-Aggregating-Planning (SAP), a framework that can learn task-agnostic semantics and dynamics priors from arbitrary quality interactions as well as the corresponding sparse rewards and then plan on unseen tasks in zero-shot condition. The framework finds a neural score function for local regional state and action pairs that can be aggregated to approximate the quality of a full trajectory; moreover, a dynamics model that is learned with self-supervision can be incorporated for planning. Many of previous works that leverage interactive data for policy learning either need massive on-policy environmental interactions or assume access to expert data while we can achieve a similar goal with pure off-policy imperfect data. Instantiating our framework results in a generalizable policy to unseen tasks. Experiments demonstrate that the proposed method can outperform baseline methods on a wide range of applications including gridworld, robotics tasks and video games.", "pdf": "/pdf/a86483704a793bacecf55a3183a94d366ff62e2f.pdf", "paperhash": "xu|scoringaggregatingplanning_learning_taskagnostic_priors_from_interactions_and_sparse_rewards_for_zeroshot_generalization", "original_pdf": "/attachment/6dc2567dbddb5de472e79327c50bf13e970ebc8c.pdf", "_bibtex": "@misc{\nxu2020scoringaggregatingplanning,\ntitle={Scoring-Aggregating-Planning: Learning task-agnostic priors from interactions and sparse rewards for zero-shot generalization},\nauthor={Huazhe Xu and Boyuan Chen and Yang Gao and Trevor Darrell},\nyear={2020},\nurl={https://openreview.net/forum?id=HyeuP2EtDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HyeuP2EtDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper10/Authors", "ICLR.cc/2020/Conference/Paper10/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper10/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper10/Reviewers", "ICLR.cc/2020/Conference/Paper10/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper10/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper10/Authors|ICLR.cc/2020/Conference/Paper10/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504177740, "tmdate": 1576860531830, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper10/Authors", "ICLR.cc/2020/Conference/Paper10/Reviewers", "ICLR.cc/2020/Conference/Paper10/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper10/-/Official_Comment"}}}, {"id": "S1lkGHaNjB", "original": null, "number": 5, "cdate": 1573340423255, "ddate": null, "tcdate": 1573340423255, "tmdate": 1573353641641, "tddate": null, "forum": "HyeuP2EtDB", "replyto": "r1lfJzG3qS", "invitation": "ICLR.cc/2020/Conference/Paper10/-/Official_Comment", "content": {"title": "Response - part (2/2)", "comment": "\u2014-  \u201cbetter than SAP on the training level*, but *worse than SAP in generalizing to the new level*\u201d\n\nSAP outperforms all the baselines including DARLA on both training and testing environments. This is because SAP is robust to *suboptimal demonstrations* while most existing works cannot handle this. However, to stack the card against SAP and to be more convincing, we follow the request and conduct experiments where the other method performs well on the training level: \n\nWe collected 8000 near-optimal trajectories from the training environment. Then we train an imitation learning agent that mimics the near-optimal data. We call this method \u201cprivileged BC\u201d because it imitates the data that achieves average return 1833 on the training task.  The other method we compare is a curiosity-driven reinforcement learning method [3] that is trained with PPO on the training task. This method also has a certain level of generalization ability as claimed in the original paper. \n\nThe table below shows that in the training environment, both methods achieve similar performance compared to SAP; however, they significantly drop on the testing environment. We attribute the failure of them to the overfitting effect of supervised learning/reinforcement learning.  We also note again, this experiment favors the baseline (since the imitative method uses near-optimal trajectories). It is an ablative study to check the \u201czero-shot\u201d ability between different methods.  Quantitative results are as follows:\n\n|                      | Mario-Train | Mario-Test |\n+-----------------+-----------------+-----------------+\n| Exploration | 856.7           |       N/A        |\n+-----------------+-----------------+-----------------+\n| BC                | 910.2            | 447.8            |\n+-----------------+-----------------+-----------------+\n| NHP             | 1041.3          | 587.8           |\n+-----------------+-----------------+-----------------+\n| *Priv BC*    |  1241.4        | 432.4            |\n+-----------------+-----------------+-----------------+\n| *curiosity*  | 1183            |  347              |\n+-----------------+-----------------+-----------------+\n| SAP (ours)   | 1359.3         | 790.1            |\n+-----------------+-----------------+-----------------+\n\nWe will incorporate this into the next revision of our paper.     \n\n\u2014- \u201cAre the chosen \"Human Priors\", BC-random and BC-SAP just strawmen?\u201d\n\nThe proposed baselines are strong baselines, especially the human prior baseline. The \u201cNaive Human Prior\u201d baseline is a reward that is designed manually based on human priors about the environment. This is the routine way people deal with a sparse reward environment. SAP outperforms this mainly because the learned scores are more informative than the manually designed rewards; hence robust even under a learned dynamics model. As for BC baseline, this is generally trying to show that compared with the SAP method, a parameterized policy can hardly compete on unseen tasks.  We will add a discussion in the next revision.\n\n\nMinor:\nMinor (did not impact score):\nPage 2, 1st paragraph: \"... we show that how an intelligent agent\"...\nPage 3, 3rd paragraph: \"... in model-free RL problem\" - missing an \"a\" or \"problem*s*\"?\nPage 3, 3rd paragraph: \". Model based method ...\" - missing an \"a\" as well?\nPage 4, 1st paragraph:: \"... utilizing the to get the ...\"\nPage 4, last row: missing a dot after the loss equation, before the word \"In\".\nPage 7, Table 1: \"BC-random\" is called \"BC-data\" in the text. Aren't they the same thing?\n\nThank you for pointing out the typos.  We will revise the paper to address them.\n\n[1] Higgins, Irina, et al. \"Darla: Improving zero-shot transfer in reinforcement learning.\" Proceedings of the 34th International Conference on Machine Learning-Volume 70. JMLR. org, 2017.\n[2]Arjona-Medina, Jose A., et al. \"Rudder: Return decomposition for delayed rewards.\" arXiv preprint arXiv:1806.07857 (2018).\n[3] Burda, Yuri, et al. \"Exploration by random network distillation.\" arXiv preprint arXiv:1810.12894 (2018).\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper10/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper10/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Scoring-Aggregating-Planning: Learning task-agnostic priors from interactions and sparse rewards for zero-shot generalization", "authors": ["Huazhe Xu", "Boyuan Chen", "Yang Gao", "Trevor Darrell"], "authorids": ["huazhe_xu@eecs.berkeley.edu", "boyuanchen@berkeley.edu", "yg@eecs.berkeley.edu", "trevordarrell@eecs.berkeley.edu"], "keywords": ["learning priors from exploration data", "policy zero-shot generalization", "reward shaping", "model-based"], "TL;DR": "We learn dense scores and dynamics model as priors from exploration data and use them to induce a good policy in new tasks in zero-shot condition.", "abstract": "Humans can learn task-agnostic priors from interactive experience and utilize the priors for novel tasks without any finetuning. In this paper, we propose Scoring-Aggregating-Planning (SAP), a framework that can learn task-agnostic semantics and dynamics priors from arbitrary quality interactions as well as the corresponding sparse rewards and then plan on unseen tasks in zero-shot condition. The framework finds a neural score function for local regional state and action pairs that can be aggregated to approximate the quality of a full trajectory; moreover, a dynamics model that is learned with self-supervision can be incorporated for planning. Many of previous works that leverage interactive data for policy learning either need massive on-policy environmental interactions or assume access to expert data while we can achieve a similar goal with pure off-policy imperfect data. Instantiating our framework results in a generalizable policy to unseen tasks. Experiments demonstrate that the proposed method can outperform baseline methods on a wide range of applications including gridworld, robotics tasks and video games.", "pdf": "/pdf/a86483704a793bacecf55a3183a94d366ff62e2f.pdf", "paperhash": "xu|scoringaggregatingplanning_learning_taskagnostic_priors_from_interactions_and_sparse_rewards_for_zeroshot_generalization", "original_pdf": "/attachment/6dc2567dbddb5de472e79327c50bf13e970ebc8c.pdf", "_bibtex": "@misc{\nxu2020scoringaggregatingplanning,\ntitle={Scoring-Aggregating-Planning: Learning task-agnostic priors from interactions and sparse rewards for zero-shot generalization},\nauthor={Huazhe Xu and Boyuan Chen and Yang Gao and Trevor Darrell},\nyear={2020},\nurl={https://openreview.net/forum?id=HyeuP2EtDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HyeuP2EtDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper10/Authors", "ICLR.cc/2020/Conference/Paper10/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper10/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper10/Reviewers", "ICLR.cc/2020/Conference/Paper10/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper10/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper10/Authors|ICLR.cc/2020/Conference/Paper10/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504177740, "tmdate": 1576860531830, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper10/Authors", "ICLR.cc/2020/Conference/Paper10/Reviewers", "ICLR.cc/2020/Conference/Paper10/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper10/-/Official_Comment"}}}, {"id": "HyxxmupVsB", "original": null, "number": 7, "cdate": 1573341207568, "ddate": null, "tcdate": 1573341207568, "tmdate": 1573353086807, "tddate": null, "forum": "HyeuP2EtDB", "replyto": "HyghHQBr9S", "invitation": "ICLR.cc/2020/Conference/Paper10/-/Official_Comment", "content": {"title": "Response - part (1/2) ", "comment": "We thank the reviewer for the insightful review and comments on our work! We address the questions and comments: \n \n\u201cFirst, the baselines presented in the experiments are relatively weak. In Related Work, the authors discuss the differences between the proposed method and the related methods, but few of the related methods are used as baselines for comparison with the proposed method. \u201d\n\nThere are mainly three related domains: zero-shot policy generalization, inverse reinforcement learning, and better architecture design. \n\n\nSome works attacked zero-shot learning in computer vision where the main challenge is to transfer knowledge between different *visual* domains. However, these methods can hardly be any stronger than the proposed baselines in the paper because they did not have any ability to adapt an actionable policy and further deal with different configurations between environments. Another line of work researches mainly on how to leverage the compositionality of trajectories in grid-world like environments so that the skills can be transferred in zero-shot in an unseen world. However, these methods all have strong restrictions about the simulated world being near-discrete (although it can be in a 3D navigation environment) and containing explicit objects with their corresponding concepts. \n\nWe did not find any existing Inverse RL baselines that can be applied to the zero-shot with a suboptimal demonstration setting.  Most of IRL methods assume expert demonstration. It then optimizes the objective for learning a family of reward function, with which the demonstrations are superior to any other policies. Moreover, the model-based nature of our method does not require any further interaction with environments while IRL requires *extensive interactions*. We also discussed this in our related work. \n\nTo demonstrate the effectiveness of our method, we take the closest work, DARLA (DisentAngled Representation Learning Agent)[1] and adapt to our setting. DARLA is a method designed to zero-shot generalize to novel environments with complex visual inputs. It achieves the generalization by learning to see before learning to act with a disentangled representation from a beta-Variational Autoencoder ($\\beta-VAE$) followed by a Denoising Autoencoder (DAE). We compare DARLA with our method and will incorporate this in our next revision:\n\nWe note that for Mario, *higher is better*. For robotic control, *lower is better*.\t\n                      |Mario-Train| Mario-Test |\n| Explore     | 856.7           | N/A            |\n+---------------+----------------+----------------+\n| BC              | 910.2          | 447.8           |\n+---------------+----------------+----------------+\n| NHP          | 1041.3         | 587.8          | \n+---------------+----------------+----------------+\n| *DARLA*  | 876.7          | 436.5           |\n+---------------+----------------+----------------+\n| SAP (ours)| 1359.3        | 790.1           |\n\n\nFor comparison between architectures that take in local-subregion-based observations and global observations, we believe that the BC baseline that looks at the global image performs worse than SAP on both the training task as well as the generalization task. This proves that the proposed architecture in SAP with local observations is superior to simply use global observations.   \n\nWe will incorporate the comparison and more discussion in our next revision. \n\n\n--\u201cMoreover, the experiments are quite insufficient in terms of ablating different components of the proposed methods.\u201d\n\nWe conducted experiments for ablative study including the effect of 1) dynamic models, 2) ``\"done\" signal at the end of an episode, and 3) the number of planning horizons.\n\nIn addition to that, we also compare it with another baseline that uses rudder[2] as the aggregator.  This baseline shows that even the proposed summation aggregator is very simple, it is surprisingly more effective than RUDDER aggregator. This might be due to the long horizon of the proposed tasks because RUDDER is designed to fit atari games. Quantitative results are as follows:\n\n                                     | Mario-Train | Mario-Test |\n| Explore                    | 856.7             | N/A             |\n+--------------------------+------------------+----------------+\n| BC                            | 910.2             | 447.8           |\n+--------------------------+------------------+----------------+\n| NHP                         | 1041.3           | 587.8          |\n+--------------------------+------------------+----------------+\n| *Rudder Variant* |  314               | 306              |\n+--------------------------+------------------+----------------+\n| SAP (ours)              | 1359.3           | 790.1           |\n\n We will add the results to the next revision. Beyond this, we will also modify our existing discussion about the ablative study including the effect of dynamic models, done signal, and planning steps."}, "signatures": ["ICLR.cc/2020/Conference/Paper10/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper10/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Scoring-Aggregating-Planning: Learning task-agnostic priors from interactions and sparse rewards for zero-shot generalization", "authors": ["Huazhe Xu", "Boyuan Chen", "Yang Gao", "Trevor Darrell"], "authorids": ["huazhe_xu@eecs.berkeley.edu", "boyuanchen@berkeley.edu", "yg@eecs.berkeley.edu", "trevordarrell@eecs.berkeley.edu"], "keywords": ["learning priors from exploration data", "policy zero-shot generalization", "reward shaping", "model-based"], "TL;DR": "We learn dense scores and dynamics model as priors from exploration data and use them to induce a good policy in new tasks in zero-shot condition.", "abstract": "Humans can learn task-agnostic priors from interactive experience and utilize the priors for novel tasks without any finetuning. In this paper, we propose Scoring-Aggregating-Planning (SAP), a framework that can learn task-agnostic semantics and dynamics priors from arbitrary quality interactions as well as the corresponding sparse rewards and then plan on unseen tasks in zero-shot condition. The framework finds a neural score function for local regional state and action pairs that can be aggregated to approximate the quality of a full trajectory; moreover, a dynamics model that is learned with self-supervision can be incorporated for planning. Many of previous works that leverage interactive data for policy learning either need massive on-policy environmental interactions or assume access to expert data while we can achieve a similar goal with pure off-policy imperfect data. Instantiating our framework results in a generalizable policy to unseen tasks. Experiments demonstrate that the proposed method can outperform baseline methods on a wide range of applications including gridworld, robotics tasks and video games.", "pdf": "/pdf/a86483704a793bacecf55a3183a94d366ff62e2f.pdf", "paperhash": "xu|scoringaggregatingplanning_learning_taskagnostic_priors_from_interactions_and_sparse_rewards_for_zeroshot_generalization", "original_pdf": "/attachment/6dc2567dbddb5de472e79327c50bf13e970ebc8c.pdf", "_bibtex": "@misc{\nxu2020scoringaggregatingplanning,\ntitle={Scoring-Aggregating-Planning: Learning task-agnostic priors from interactions and sparse rewards for zero-shot generalization},\nauthor={Huazhe Xu and Boyuan Chen and Yang Gao and Trevor Darrell},\nyear={2020},\nurl={https://openreview.net/forum?id=HyeuP2EtDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HyeuP2EtDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper10/Authors", "ICLR.cc/2020/Conference/Paper10/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper10/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper10/Reviewers", "ICLR.cc/2020/Conference/Paper10/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper10/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper10/Authors|ICLR.cc/2020/Conference/Paper10/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504177740, "tmdate": 1576860531830, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper10/Authors", "ICLR.cc/2020/Conference/Paper10/Reviewers", "ICLR.cc/2020/Conference/Paper10/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper10/-/Official_Comment"}}}, {"id": "SJxcRM64jS", "original": null, "number": 4, "cdate": 1573339857548, "ddate": null, "tcdate": 1573339857548, "tmdate": 1573352553797, "tddate": null, "forum": "HyeuP2EtDB", "replyto": "r1lfJzG3qS", "invitation": "ICLR.cc/2020/Conference/Paper10/-/Official_Comment", "content": {"title": "Response - part (1/2)", "comment": "We thank the reviewer for the insightful review and comments on our work! We address the questions and comments: \n \n\u2014 \u201cPage 3, 3rd paragraph of Section 3: the paper says that \"The proposed formulation requires much less information and thus more realistic and feasible\" - I agree that this is more realistic, but is it really more feasible? The requirement of much less information makes the proposed formulation much more sparse.\u201d   \n\nWhen the formulation is realistic, it can be easily applied to more general real-world problems; more complex formulations usually requires to engineer the original problem. This is how we interpret \u201cfeasible\u201d. We will rephrase and clarify this in the revision. \n\n\u2014 \u201cA basic assumption in the SAP framework is that a local region score is a sum of all the sub-regions. As phrased in the paper: \"in the physical world, there is usually some level of rotational or transnational invariance\". I'm not sure that this assumption makes sense neither in the Mario case or in other tasks, e.g., robotics. Doesn't it matter if you have a \"turtle\" right in front of you, or below you?\u201d\n\nA turtle below Mario and a turtle in front of Mario are different indeed! We clarify that the \u201crotational or translational invariance\u201d is referring the pixels inside one subregion meaning that when there is some small translational shift/rotational shift inside of one subregion, the scoring network can still give a reasonable output. We do consider the difference between different locations among subregions in the paper. And from Appendix. A.2.7, Figure 9, we can see that the score for a turtle in front is much lower than a turtle below Mario. This reassures your point! We will update the text and clarify this in the next revision. \n\n\u2014 \u201cA question about the planning phase - page 5 says: \"We select the action sequence that gives us the best-aggregated score and execute the first action\". ... In other words - what happens if the first action in the new test environment yields an unexpected state, that was not predicted well by the dynamics model; does the agent continue on the initial planned trajectory (that ignores the \"surprise\"), or does it compute its next action based on the unexpected state?\u201d\n \nYes, the planning method used in this SAP is adaptive if it meets an unexpected state. The extract model-predictive control algorithm can be described as:  We select the action sequence that gives us the best-aggregated score based on the learned dynamics and executes the first action in the real environment. Then it reaches a new state which can be similar or very different from the predicted state. Then we redo everything again until the end of an episode.  Thus, we conclude that the planning algorithm will plan based on an unexpected state rather than stick to a predicted one.\n\n\u2014 \u201cAre there any stronger baselines in the literature, or reductions of known baselines to the zero-shot scenario?\u201d\n \nFor zero-shot learning in computer vision, the main challenge is to transfer knowledge between different visual domains. However, these methods can hardly be any stronger than the proposed baselines in the paper because they did not have any ability to adapt an actionable policy and further deal with different configurations between environments. Another line of work researches mainly on how to leverage the compositionality of trajectories in grid-world like environments so that the skills can be transferred in zero-shot in an unseen world. However, these methods all have strong restrictions about the world containing explicit objects and their corresponding concepts. \n\nWe find the closest and state-of-art work[1], which can be reformulated to the setting in our paper and perform zero-shot transfer between domains. The author proposed a new multi-stage RL agent, DARLA (Disentangled Representation Learning Agent), which learns to see before learning to act by learning a disentangle representation from a beta-Variational Autoencoder ($\\beta$-VAE) followed by a Denoising Autoencoder (DAE). By decomposing the vision and action, DARLA has achieved state-of-art results in a variety of RL environments and RL algorithms. We compare DARLA with our method (and will be added to our next revision):\n\nWe note that for the task, *higher is better*. \t\t\t\t\n\n                   | Mario-Train | Mario-Test |\n| Explor     | 856.7             | N/A            |\n+-------------+------------------+----------------+\n| BC           | 910.2             | 447.8          |\n+-------------+------------------+----------------+\n| NHP        | 1041.3          | 587.8           | \n+-------------+------------------+---------------+\n| *DARLA*| 876.7            | 436.5          |\n+-------------+------------------+---------------+\n| SAP(ours)| 1359.3         | 790.1          |\n\nWe also will incorporate this into our paper in the revision.  \n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper10/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper10/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Scoring-Aggregating-Planning: Learning task-agnostic priors from interactions and sparse rewards for zero-shot generalization", "authors": ["Huazhe Xu", "Boyuan Chen", "Yang Gao", "Trevor Darrell"], "authorids": ["huazhe_xu@eecs.berkeley.edu", "boyuanchen@berkeley.edu", "yg@eecs.berkeley.edu", "trevordarrell@eecs.berkeley.edu"], "keywords": ["learning priors from exploration data", "policy zero-shot generalization", "reward shaping", "model-based"], "TL;DR": "We learn dense scores and dynamics model as priors from exploration data and use them to induce a good policy in new tasks in zero-shot condition.", "abstract": "Humans can learn task-agnostic priors from interactive experience and utilize the priors for novel tasks without any finetuning. In this paper, we propose Scoring-Aggregating-Planning (SAP), a framework that can learn task-agnostic semantics and dynamics priors from arbitrary quality interactions as well as the corresponding sparse rewards and then plan on unseen tasks in zero-shot condition. The framework finds a neural score function for local regional state and action pairs that can be aggregated to approximate the quality of a full trajectory; moreover, a dynamics model that is learned with self-supervision can be incorporated for planning. Many of previous works that leverage interactive data for policy learning either need massive on-policy environmental interactions or assume access to expert data while we can achieve a similar goal with pure off-policy imperfect data. Instantiating our framework results in a generalizable policy to unseen tasks. Experiments demonstrate that the proposed method can outperform baseline methods on a wide range of applications including gridworld, robotics tasks and video games.", "pdf": "/pdf/a86483704a793bacecf55a3183a94d366ff62e2f.pdf", "paperhash": "xu|scoringaggregatingplanning_learning_taskagnostic_priors_from_interactions_and_sparse_rewards_for_zeroshot_generalization", "original_pdf": "/attachment/6dc2567dbddb5de472e79327c50bf13e970ebc8c.pdf", "_bibtex": "@misc{\nxu2020scoringaggregatingplanning,\ntitle={Scoring-Aggregating-Planning: Learning task-agnostic priors from interactions and sparse rewards for zero-shot generalization},\nauthor={Huazhe Xu and Boyuan Chen and Yang Gao and Trevor Darrell},\nyear={2020},\nurl={https://openreview.net/forum?id=HyeuP2EtDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HyeuP2EtDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper10/Authors", "ICLR.cc/2020/Conference/Paper10/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper10/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper10/Reviewers", "ICLR.cc/2020/Conference/Paper10/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper10/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper10/Authors|ICLR.cc/2020/Conference/Paper10/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504177740, "tmdate": 1576860531830, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper10/Authors", "ICLR.cc/2020/Conference/Paper10/Reviewers", "ICLR.cc/2020/Conference/Paper10/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper10/-/Official_Comment"}}}, {"id": "HklUOIpEjH", "original": null, "number": 6, "cdate": 1573340781958, "ddate": null, "tcdate": 1573340781958, "tmdate": 1573340858711, "tddate": null, "forum": "HyeuP2EtDB", "replyto": "SkxYYEW4qr", "invitation": "ICLR.cc/2020/Conference/Paper10/-/Official_Comment", "content": {"title": "Response", "comment": "We thank the reviewer for the insightful review and the comments! We address the questions and comments as follows:\n\n\u2014  \u201cI am a bit confused by this setting. The model never sees any rewards for E1 but it does see rewards for E2? How is this zero-shot?\u201d\n\t\nThe agent will not see any reward during the test/generalization stage. In the paper page 3 section 3.1, we illustrate that r(\\tau) here is only used for evaluation. Hence, it does not violate the zero-shot setting.  \n\nIn the original paper, we decompose the data in E1 (training env) into two parts: the trajectory and the reward. We assume the trajectory is collected initially without any reward in mind, and when later we need to perform some task, we specify the task by providing rewards in E1.\n\nThanks for pointing this out! We will clarify that the reward is hidden from the agent in the testing environments in our revision.\n\n\u2014   \u201cI wish some of it had been described more rigorously with math (e.g. the loss etc.) so it was easier to understand for people not in the domain\u201d \n\nWe agree that more rigorous math would help us improve the quality of the paper.  We first clarify the losses defined in our paper here. For the score-aggregation objective, it is $\\min_{\\theta} \\frac{1}{2}(J_\\theta(\\mathbf{\\tau})-r(\\tau))^2$ where J is the aggregated score and $r(\\tau)$is final evaluation.  The model training objective is $\\min_\\phi \\frac{1}{2}(\\mathcal{M}_\\phi(\\mathrm{s}_t,\\mathrm{a}_t)-\\mathrm{s}_{t+1})^2$ where $\\mathcal{M}$ is a neural dynamics model parameterized by $\\phi$, and $s_{t+1}$ is the state in the future timestep. The objective of the Model-Predictive Control is thoroughly described in the preliminary section. We will improve regarding this aspect in our revision.  \n\n\u2014  Terminology explanation\n\nWe are happy to explain more terminology in the paper! We will slightly expand the preliminary and the related work section to add more explanations.  \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper10/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper10/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Scoring-Aggregating-Planning: Learning task-agnostic priors from interactions and sparse rewards for zero-shot generalization", "authors": ["Huazhe Xu", "Boyuan Chen", "Yang Gao", "Trevor Darrell"], "authorids": ["huazhe_xu@eecs.berkeley.edu", "boyuanchen@berkeley.edu", "yg@eecs.berkeley.edu", "trevordarrell@eecs.berkeley.edu"], "keywords": ["learning priors from exploration data", "policy zero-shot generalization", "reward shaping", "model-based"], "TL;DR": "We learn dense scores and dynamics model as priors from exploration data and use them to induce a good policy in new tasks in zero-shot condition.", "abstract": "Humans can learn task-agnostic priors from interactive experience and utilize the priors for novel tasks without any finetuning. In this paper, we propose Scoring-Aggregating-Planning (SAP), a framework that can learn task-agnostic semantics and dynamics priors from arbitrary quality interactions as well as the corresponding sparse rewards and then plan on unseen tasks in zero-shot condition. The framework finds a neural score function for local regional state and action pairs that can be aggregated to approximate the quality of a full trajectory; moreover, a dynamics model that is learned with self-supervision can be incorporated for planning. Many of previous works that leverage interactive data for policy learning either need massive on-policy environmental interactions or assume access to expert data while we can achieve a similar goal with pure off-policy imperfect data. Instantiating our framework results in a generalizable policy to unseen tasks. Experiments demonstrate that the proposed method can outperform baseline methods on a wide range of applications including gridworld, robotics tasks and video games.", "pdf": "/pdf/a86483704a793bacecf55a3183a94d366ff62e2f.pdf", "paperhash": "xu|scoringaggregatingplanning_learning_taskagnostic_priors_from_interactions_and_sparse_rewards_for_zeroshot_generalization", "original_pdf": "/attachment/6dc2567dbddb5de472e79327c50bf13e970ebc8c.pdf", "_bibtex": "@misc{\nxu2020scoringaggregatingplanning,\ntitle={Scoring-Aggregating-Planning: Learning task-agnostic priors from interactions and sparse rewards for zero-shot generalization},\nauthor={Huazhe Xu and Boyuan Chen and Yang Gao and Trevor Darrell},\nyear={2020},\nurl={https://openreview.net/forum?id=HyeuP2EtDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HyeuP2EtDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper10/Authors", "ICLR.cc/2020/Conference/Paper10/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper10/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper10/Reviewers", "ICLR.cc/2020/Conference/Paper10/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper10/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper10/Authors|ICLR.cc/2020/Conference/Paper10/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504177740, "tmdate": 1576860531830, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper10/Authors", "ICLR.cc/2020/Conference/Paper10/Reviewers", "ICLR.cc/2020/Conference/Paper10/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper10/-/Official_Comment"}}}, {"id": "SkxYYEW4qr", "original": null, "number": 1, "cdate": 1572242561319, "ddate": null, "tcdate": 1572242561319, "tmdate": 1572972649960, "tddate": null, "forum": "HyeuP2EtDB", "replyto": "HyeuP2EtDB", "invitation": "ICLR.cc/2020/Conference/Paper10/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "I am not from this area and don't know much about reinforcement learning.\n\nThe paper discusses zero shot generalization (adaptation) into new environments. The authors propose an approach and then show results on Grid-World, Super Mario Bros, and 3D Robotics. \n\nIn the training environment E1 = (S, A, p) the algorithm sees a bank of exploratory trajectories \\tau_i = {(s_t, a_t)}_{t=1}^{T} but not rewards. The authors  then say that algorithm is tested on the test environment E2. They \" propose to only inform the new task per trajectory terminal evaluation r(\u03c4 ) in E1\" to give the training signal (where r is the reward).\n\nI am a bit confused by this setting. The model never sees any rewards for E1 but it does see rewards for E2? How is this zero shot?\n\nThe authors then propose their approach, I wish some of it had been described more rigorously with math (e.g. the loss etc.) so it was easier to understand for people not in the domain and familiar with some of the terminology. \n\nEmpirically the authors show results for 3 datasets and this seems thorough. "}, "signatures": ["ICLR.cc/2020/Conference/Paper10/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper10/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Scoring-Aggregating-Planning: Learning task-agnostic priors from interactions and sparse rewards for zero-shot generalization", "authors": ["Huazhe Xu", "Boyuan Chen", "Yang Gao", "Trevor Darrell"], "authorids": ["huazhe_xu@eecs.berkeley.edu", "boyuanchen@berkeley.edu", "yg@eecs.berkeley.edu", "trevordarrell@eecs.berkeley.edu"], "keywords": ["learning priors from exploration data", "policy zero-shot generalization", "reward shaping", "model-based"], "TL;DR": "We learn dense scores and dynamics model as priors from exploration data and use them to induce a good policy in new tasks in zero-shot condition.", "abstract": "Humans can learn task-agnostic priors from interactive experience and utilize the priors for novel tasks without any finetuning. In this paper, we propose Scoring-Aggregating-Planning (SAP), a framework that can learn task-agnostic semantics and dynamics priors from arbitrary quality interactions as well as the corresponding sparse rewards and then plan on unseen tasks in zero-shot condition. The framework finds a neural score function for local regional state and action pairs that can be aggregated to approximate the quality of a full trajectory; moreover, a dynamics model that is learned with self-supervision can be incorporated for planning. Many of previous works that leverage interactive data for policy learning either need massive on-policy environmental interactions or assume access to expert data while we can achieve a similar goal with pure off-policy imperfect data. Instantiating our framework results in a generalizable policy to unseen tasks. Experiments demonstrate that the proposed method can outperform baseline methods on a wide range of applications including gridworld, robotics tasks and video games.", "pdf": "/pdf/a86483704a793bacecf55a3183a94d366ff62e2f.pdf", "paperhash": "xu|scoringaggregatingplanning_learning_taskagnostic_priors_from_interactions_and_sparse_rewards_for_zeroshot_generalization", "original_pdf": "/attachment/6dc2567dbddb5de472e79327c50bf13e970ebc8c.pdf", "_bibtex": "@misc{\nxu2020scoringaggregatingplanning,\ntitle={Scoring-Aggregating-Planning: Learning task-agnostic priors from interactions and sparse rewards for zero-shot generalization},\nauthor={Huazhe Xu and Boyuan Chen and Yang Gao and Trevor Darrell},\nyear={2020},\nurl={https://openreview.net/forum?id=HyeuP2EtDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HyeuP2EtDB", "replyto": "HyeuP2EtDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper10/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper10/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575652862939, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper10/Reviewers"], "noninvitees": [], "tcdate": 1570237758448, "tmdate": 1575652862953, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper10/-/Official_Review"}}}, {"id": "HyghHQBr9S", "original": null, "number": 2, "cdate": 1572324163652, "ddate": null, "tcdate": 1572324163652, "tmdate": 1572972649917, "tddate": null, "forum": "HyeuP2EtDB", "replyto": "HyeuP2EtDB", "invitation": "ICLR.cc/2020/Conference/Paper10/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The paper describes a method that aims to learn task-agnostic priors for zero-shot generalization. The main idea is to employ the following modeling approach on top of the model-based RL framework: a local convolution network is used to compute a score for each local state action pair, and then another network is used to aggregate all the scores. While the problem being studied is important and the experimental results seem positive, there are a few concerns.\n\nFirst, the baselines presented in the experiments are relatively weak. In Related Work, the authors discuss the differences between the proposed method and the related methods, but few of the related methods are used as baselines for comparison with the proposed method. Moreover, the experiments are quite insufficient in terms of ablating different components of the proposed methods.\n\nSecond, essentially the proposed method is trying to solve the zero-shot generalization by parameter initialization; a model is pretrained on related tasks and used as initializations for target tasks. The authors claim that it is different from prior work mainly because of the neural architecture that deals with sparse rewards via score aggregation. While the proposed architecture might be more suitable for solving tasks with sparse rewards, it is not intuitive whether it has something to do with learning zero-shot generalization. And apparently, the method will also rely on the similarity between the pretrained task and the target task, and such a scope constraint is not discussed in the paper. In other words, I'm not quite sure a better architecture is fundamental progress towards zero-shot RL."}, "signatures": ["ICLR.cc/2020/Conference/Paper10/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper10/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Scoring-Aggregating-Planning: Learning task-agnostic priors from interactions and sparse rewards for zero-shot generalization", "authors": ["Huazhe Xu", "Boyuan Chen", "Yang Gao", "Trevor Darrell"], "authorids": ["huazhe_xu@eecs.berkeley.edu", "boyuanchen@berkeley.edu", "yg@eecs.berkeley.edu", "trevordarrell@eecs.berkeley.edu"], "keywords": ["learning priors from exploration data", "policy zero-shot generalization", "reward shaping", "model-based"], "TL;DR": "We learn dense scores and dynamics model as priors from exploration data and use them to induce a good policy in new tasks in zero-shot condition.", "abstract": "Humans can learn task-agnostic priors from interactive experience and utilize the priors for novel tasks without any finetuning. In this paper, we propose Scoring-Aggregating-Planning (SAP), a framework that can learn task-agnostic semantics and dynamics priors from arbitrary quality interactions as well as the corresponding sparse rewards and then plan on unseen tasks in zero-shot condition. The framework finds a neural score function for local regional state and action pairs that can be aggregated to approximate the quality of a full trajectory; moreover, a dynamics model that is learned with self-supervision can be incorporated for planning. Many of previous works that leverage interactive data for policy learning either need massive on-policy environmental interactions or assume access to expert data while we can achieve a similar goal with pure off-policy imperfect data. Instantiating our framework results in a generalizable policy to unseen tasks. Experiments demonstrate that the proposed method can outperform baseline methods on a wide range of applications including gridworld, robotics tasks and video games.", "pdf": "/pdf/a86483704a793bacecf55a3183a94d366ff62e2f.pdf", "paperhash": "xu|scoringaggregatingplanning_learning_taskagnostic_priors_from_interactions_and_sparse_rewards_for_zeroshot_generalization", "original_pdf": "/attachment/6dc2567dbddb5de472e79327c50bf13e970ebc8c.pdf", "_bibtex": "@misc{\nxu2020scoringaggregatingplanning,\ntitle={Scoring-Aggregating-Planning: Learning task-agnostic priors from interactions and sparse rewards for zero-shot generalization},\nauthor={Huazhe Xu and Boyuan Chen and Yang Gao and Trevor Darrell},\nyear={2020},\nurl={https://openreview.net/forum?id=HyeuP2EtDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HyeuP2EtDB", "replyto": "HyeuP2EtDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper10/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper10/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575652862939, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper10/Reviewers"], "noninvitees": [], "tcdate": 1570237758448, "tmdate": 1575652862953, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper10/-/Official_Review"}}}], "count": 10}