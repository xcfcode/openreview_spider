{"notes": [{"id": "B1lPETVFPS", "original": "ryg7NVXwvH", "number": 490, "cdate": 1569439023367, "ddate": null, "tcdate": 1569439023367, "tmdate": 1577168226274, "tddate": null, "forum": "B1lPETVFPS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Towards Principled Objectives for Contrastive Disentanglement", "authors": ["Anwesa Choudhuri", "Ashok Vardhan Makkuva", "Ranvir Rana", "Sewoong Oh", "Girish Chowdhary", "Alexander Schwing"], "authorids": ["anwesac2@illinois.edu", "makkuva2@illinois.edu", "rbrana2@illinois.edu", "sewoong@cs.washington.edu", "girishc@illinois.edu", "aschwing@illinois.edu"], "keywords": ["Disentanglement", "Contrastive"], "abstract": "Unsupervised learning is an important tool that has received a significant amount of attention for decades. Its goal is `unsupervised recovery,' i.e., extracting salient factors/properties  from unlabeled data. Because of the challenges in defining salient properties, recently, `contrastive disentanglement' has gained popularity to discover the additional variations that are enhanced in one dataset relative to another. %In fact, contrastive disentanglement and unsupervised recovery are often combined in that we seek additional variations that exhibit salient factors/properties. \nExisting formulations have devised a variety of losses for this task. However, all present day methods exhibit two major shortcomings: (1) encodings for data that does not exhibit salient factors is not pushed to carry no signal; and (2) introduced losses are often hard to estimate and require additional trainable parameters. We present a new formulation for contrastive disentanglement which avoids both shortcomings by carefully formulating a probabilistic model and by using non-parametric yet easily computable metrics. We show on four challenging datasets that the proposed approach is able to better disentangle salient factors.\n", "pdf": "/pdf/88f11550ec9b8d15bac6f49bb02b96aad7d29b65.pdf", "paperhash": "choudhuri|towards_principled_objectives_for_contrastive_disentanglement", "code": "https://drive.google.com/file/d/1IlBRrf2Zm4YOOb67IQOrwXlaXlO6-4Q4/view?usp=sharing", "original_pdf": "/attachment/35954b7c799e83c4bb57e7f87ecf0910096cf4f1.pdf", "_bibtex": "@misc{\nchoudhuri2020towards,\ntitle={Towards Principled Objectives for Contrastive Disentanglement},\nauthor={Anwesa Choudhuri and Ashok Vardhan Makkuva and Ranvir Rana and Sewoong Oh and Girish Chowdhary and Alexander Schwing},\nyear={2020},\nurl={https://openreview.net/forum?id=B1lPETVFPS}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "Flqnlcg2Pa", "original": null, "number": 1, "cdate": 1576798698008, "ddate": null, "tcdate": 1576798698008, "tmdate": 1576800937791, "tddate": null, "forum": "B1lPETVFPS", "replyto": "B1lPETVFPS", "invitation": "ICLR.cc/2020/Conference/Paper490/-/Decision", "content": {"decision": "Reject", "comment": "The paper proposes new regularizations on contrastive disentanglement. After reading the author's response,  all the reviewers still think that the contribution is too limited and all agree to reject.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards Principled Objectives for Contrastive Disentanglement", "authors": ["Anwesa Choudhuri", "Ashok Vardhan Makkuva", "Ranvir Rana", "Sewoong Oh", "Girish Chowdhary", "Alexander Schwing"], "authorids": ["anwesac2@illinois.edu", "makkuva2@illinois.edu", "rbrana2@illinois.edu", "sewoong@cs.washington.edu", "girishc@illinois.edu", "aschwing@illinois.edu"], "keywords": ["Disentanglement", "Contrastive"], "abstract": "Unsupervised learning is an important tool that has received a significant amount of attention for decades. Its goal is `unsupervised recovery,' i.e., extracting salient factors/properties  from unlabeled data. Because of the challenges in defining salient properties, recently, `contrastive disentanglement' has gained popularity to discover the additional variations that are enhanced in one dataset relative to another. %In fact, contrastive disentanglement and unsupervised recovery are often combined in that we seek additional variations that exhibit salient factors/properties. \nExisting formulations have devised a variety of losses for this task. However, all present day methods exhibit two major shortcomings: (1) encodings for data that does not exhibit salient factors is not pushed to carry no signal; and (2) introduced losses are often hard to estimate and require additional trainable parameters. We present a new formulation for contrastive disentanglement which avoids both shortcomings by carefully formulating a probabilistic model and by using non-parametric yet easily computable metrics. We show on four challenging datasets that the proposed approach is able to better disentangle salient factors.\n", "pdf": "/pdf/88f11550ec9b8d15bac6f49bb02b96aad7d29b65.pdf", "paperhash": "choudhuri|towards_principled_objectives_for_contrastive_disentanglement", "code": "https://drive.google.com/file/d/1IlBRrf2Zm4YOOb67IQOrwXlaXlO6-4Q4/view?usp=sharing", "original_pdf": "/attachment/35954b7c799e83c4bb57e7f87ecf0910096cf4f1.pdf", "_bibtex": "@misc{\nchoudhuri2020towards,\ntitle={Towards Principled Objectives for Contrastive Disentanglement},\nauthor={Anwesa Choudhuri and Ashok Vardhan Makkuva and Ranvir Rana and Sewoong Oh and Girish Chowdhary and Alexander Schwing},\nyear={2020},\nurl={https://openreview.net/forum?id=B1lPETVFPS}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "B1lPETVFPS", "replyto": "B1lPETVFPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795716213, "tmdate": 1576800266303, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper490/-/Decision"}}}, {"id": "rkgcvpn-cH", "original": null, "number": 3, "cdate": 1572093281651, "ddate": null, "tcdate": 1572093281651, "tmdate": 1574346408690, "tddate": null, "forum": "B1lPETVFPS", "replyto": "B1lPETVFPS", "invitation": "ICLR.cc/2020/Conference/Paper490/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #2", "review": "This paper is concerned with contrastive disentanglement. The considered problem is interesting and important in the community, and the proposed method seems to be of practical use, according to the empirical results. My concern is that the contribution of the paper on the theoretical or methodological side seems a bit weak.\n\nThe proposed method relies on the VAE framework for contrastive disentanglement. The authors proposed two modifications: one is to explicit enforce the posterior of the content representation to be a Delta distribution, and the other is to make sure that the background representation has the same distribution across the background and the target. In the original cVAE framework for contrastive disentanglement, the two constraints were considered in a slightly more implicit way. It is not surprising to see that the proposed method seems to outperform cVAE only slightly. To see how much we can really gain by further explicitly enforcing the two constraints, the authors may do some further studies with varied sample sizes. BTW, in the result for CelebA in Table 1, cVAE seems to outperform the proposed method, denoted by 'Ours', according to Acc of z_x, but it was indicated in the text (as well as in by the bold font) that 'Ours' performed the best--is there a typo?\n\nI acknowledge I read the authors' response and other reviews and would like to keep my original rating.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper490/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper490/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards Principled Objectives for Contrastive Disentanglement", "authors": ["Anwesa Choudhuri", "Ashok Vardhan Makkuva", "Ranvir Rana", "Sewoong Oh", "Girish Chowdhary", "Alexander Schwing"], "authorids": ["anwesac2@illinois.edu", "makkuva2@illinois.edu", "rbrana2@illinois.edu", "sewoong@cs.washington.edu", "girishc@illinois.edu", "aschwing@illinois.edu"], "keywords": ["Disentanglement", "Contrastive"], "abstract": "Unsupervised learning is an important tool that has received a significant amount of attention for decades. Its goal is `unsupervised recovery,' i.e., extracting salient factors/properties  from unlabeled data. Because of the challenges in defining salient properties, recently, `contrastive disentanglement' has gained popularity to discover the additional variations that are enhanced in one dataset relative to another. %In fact, contrastive disentanglement and unsupervised recovery are often combined in that we seek additional variations that exhibit salient factors/properties. \nExisting formulations have devised a variety of losses for this task. However, all present day methods exhibit two major shortcomings: (1) encodings for data that does not exhibit salient factors is not pushed to carry no signal; and (2) introduced losses are often hard to estimate and require additional trainable parameters. We present a new formulation for contrastive disentanglement which avoids both shortcomings by carefully formulating a probabilistic model and by using non-parametric yet easily computable metrics. We show on four challenging datasets that the proposed approach is able to better disentangle salient factors.\n", "pdf": "/pdf/88f11550ec9b8d15bac6f49bb02b96aad7d29b65.pdf", "paperhash": "choudhuri|towards_principled_objectives_for_contrastive_disentanglement", "code": "https://drive.google.com/file/d/1IlBRrf2Zm4YOOb67IQOrwXlaXlO6-4Q4/view?usp=sharing", "original_pdf": "/attachment/35954b7c799e83c4bb57e7f87ecf0910096cf4f1.pdf", "_bibtex": "@misc{\nchoudhuri2020towards,\ntitle={Towards Principled Objectives for Contrastive Disentanglement},\nauthor={Anwesa Choudhuri and Ashok Vardhan Makkuva and Ranvir Rana and Sewoong Oh and Girish Chowdhary and Alexander Schwing},\nyear={2020},\nurl={https://openreview.net/forum?id=B1lPETVFPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "B1lPETVFPS", "replyto": "B1lPETVFPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper490/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper490/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574942248201, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper490/Reviewers"], "noninvitees": [], "tcdate": 1570237751374, "tmdate": 1574942248213, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper490/-/Official_Review"}}}, {"id": "HJe9QrU9iH", "original": null, "number": 2, "cdate": 1573704994345, "ddate": null, "tcdate": 1573704994345, "tmdate": 1573752422996, "tddate": null, "forum": "B1lPETVFPS", "replyto": "S1gXN-wZ5H", "invitation": "ICLR.cc/2020/Conference/Paper490/-/Official_Comment", "content": {"title": "Response to Review #3", "comment": "Thanks a lot for your helpful feedback. We address your concerns one by one below.\n\n\n\u201cWhile authors point out that existing formulation in Abid & Zou (2019); Ruiz et al. (2019) ignores about the prior ....... result on L MNIST(similar), CelebA(worse), and Affectnet(worse) dataset.\u201d\n\nWe apologize for a typo in Table 1, row `Ours (Eqn. 7)\u2019 for the CelebA dataset, which we corrected. In addition we think the reviewer may have misunderstood this admittedly confusing table: our methods perform best across all datasets. For columns denoted by $z_x$, both lower accuracy and lower SS are better; for columns titled $s_x$, higher accuracy and higher SS are better. This is explained in the caption of Table 1. To clarify this confusion we redesigned this table in the revised version. \n\nAlso note: our methods perform better than both baselines despite having fewer neural networks to train and hence fewer parameters. In both [1] and [2], the authors propose a new KL divergence term which is hard to compute. Hence the authors approximate it using the density-ratio trick, which requires a new discriminator neural network that needs to be trained along with the VAE parameters. Thus this adversarial formulation incurs extra computational cost in the form of discriminator training and additional hyper-parameters which may not be easy to specify. In contrast, we propose two principled and well motivated loss terms that are easier to compute, do not require any additional parameters and perform much better than these baselines.\n\n\n\u201cAlso, visualization of salient features in Figure 4 show that representation balancing effects of proposed regularization terms are marginal compared to cVAE.\u201d\n\nThe following link (https://drive.google.com/file/d/1X5P2Rx7mlF5WwZ5ZXx8bQO74rwD4ZNCT/view?usp=sharing) shows the zoomed in visualization of the salient features from the target dataset. As illustrated in this enlarged figure, target features learned by our approach are visually better separated from each other than the existing works, indicating that the learnt target signal indeed corresponds to the underlying MNIST digits. Moreover, the clustering accuracy quantitatively validates our better performance. \n\n\n\u201cAnother concern is proposed regularization 1,2 might hurt the original ELBO objective. This concern is also related to beta-VAE, the paper showed that large coefficient for prior regularization simply makes the disentanglement effect for VAE.\u201d\n\nIn the beta-VAE setup [5], more weight is placed on the KL divergence term as compared to the reconstruction loss. However, we do not place any such extra weight on the KL divergence. Instead, we only add positive terms to the RHS of an upper bound on the negative log-likelihood in Equation 6 (e.g., adding the Wasserstein loss term in Equation 8). Thus the upper bound on the negative log-likelihood of the data is still valid with our regularized loss in Equation 8.\n\n\n\u201cOn the other hand, proposed objective (7) requires 'large constant' .... does not induce such unintended numerical problem.\u201d\n\n$\\lambda_s = 1$ is sufficient in all our experiments. Also, the value of $\\lambda_z$ appearing in Equation 8 is set to 1. These values for $\\lambda$ don\u2019t induce any negative effects. For reproducibility we submitted code for all experiments. \n\n\n\u201cA possible solution might be showing asymptotic theoretical guarantees as in semi-implicit variational inference.\u201d\n\nThanks for pointing out this reference, we think [5] is only tangentially related to the contrastive setting. Specifically, we do not assume any distributions on the VAE parameters as done in [5].\n\n\u201cThe objective (8) seems to require additional computational cost to solve linear programming problem. It is not clear if it is effective enough to bear that extra cost; it is hard to conclude that proposed regularization performs better than previous works without confidence interval.\u201d\n\nTo compute the Wasserstein loss $W_2^2(q_t(z),q_b(z))$, we use mini-batches of samples from both the distributions $q_t(z)$ and  $q_b(z)$. The batch size is 64. Hence we can efficiently compute the optimal matching matrix using standard optimal transport libraries such as [3]. Hence our proposed loss term only incurs a small computational overhead. This is reflected in the running times of our algorithm with and without the loss term $L_{W_2}$: \n\nTime to run 1 epoch:\nOurs (Eqn. 7) [without the term $L_{W_2}$]: 16.9 secs\nOurs (Eqn. 8) [with the term  $L_{W_2}$]: 17.3 secs\n\n\nReferences:\n[1] Contrastive Variational Autoencoder Enhances Salient Features: https://arxiv.org/abs/1902.04601\n[2] Learning Disentangled Representations with Reference-Based Variational Autoencoders: https://arxiv.org/pdf/1901.08534.pdf\n[3] Python Optimal Transport: https://github.com/rflamary/POT\n[4] Semi-Implicit Variational Inference: https://arxiv.org/pdf/1805.11183.pdf\n[5] beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework: https://openreview.net/pdf?id=Sy2fzU9gl"}, "signatures": ["ICLR.cc/2020/Conference/Paper490/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper490/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards Principled Objectives for Contrastive Disentanglement", "authors": ["Anwesa Choudhuri", "Ashok Vardhan Makkuva", "Ranvir Rana", "Sewoong Oh", "Girish Chowdhary", "Alexander Schwing"], "authorids": ["anwesac2@illinois.edu", "makkuva2@illinois.edu", "rbrana2@illinois.edu", "sewoong@cs.washington.edu", "girishc@illinois.edu", "aschwing@illinois.edu"], "keywords": ["Disentanglement", "Contrastive"], "abstract": "Unsupervised learning is an important tool that has received a significant amount of attention for decades. Its goal is `unsupervised recovery,' i.e., extracting salient factors/properties  from unlabeled data. Because of the challenges in defining salient properties, recently, `contrastive disentanglement' has gained popularity to discover the additional variations that are enhanced in one dataset relative to another. %In fact, contrastive disentanglement and unsupervised recovery are often combined in that we seek additional variations that exhibit salient factors/properties. \nExisting formulations have devised a variety of losses for this task. However, all present day methods exhibit two major shortcomings: (1) encodings for data that does not exhibit salient factors is not pushed to carry no signal; and (2) introduced losses are often hard to estimate and require additional trainable parameters. We present a new formulation for contrastive disentanglement which avoids both shortcomings by carefully formulating a probabilistic model and by using non-parametric yet easily computable metrics. We show on four challenging datasets that the proposed approach is able to better disentangle salient factors.\n", "pdf": "/pdf/88f11550ec9b8d15bac6f49bb02b96aad7d29b65.pdf", "paperhash": "choudhuri|towards_principled_objectives_for_contrastive_disentanglement", "code": "https://drive.google.com/file/d/1IlBRrf2Zm4YOOb67IQOrwXlaXlO6-4Q4/view?usp=sharing", "original_pdf": "/attachment/35954b7c799e83c4bb57e7f87ecf0910096cf4f1.pdf", "_bibtex": "@misc{\nchoudhuri2020towards,\ntitle={Towards Principled Objectives for Contrastive Disentanglement},\nauthor={Anwesa Choudhuri and Ashok Vardhan Makkuva and Ranvir Rana and Sewoong Oh and Girish Chowdhary and Alexander Schwing},\nyear={2020},\nurl={https://openreview.net/forum?id=B1lPETVFPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1lPETVFPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper490/Authors", "ICLR.cc/2020/Conference/Paper490/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper490/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper490/Reviewers", "ICLR.cc/2020/Conference/Paper490/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper490/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper490/Authors|ICLR.cc/2020/Conference/Paper490/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504170706, "tmdate": 1576860556745, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper490/Authors", "ICLR.cc/2020/Conference/Paper490/Reviewers", "ICLR.cc/2020/Conference/Paper490/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper490/-/Official_Comment"}}}, {"id": "HJx6LMUqsS", "original": null, "number": 1, "cdate": 1573704276791, "ddate": null, "tcdate": 1573704276791, "tmdate": 1573752382791, "tddate": null, "forum": "B1lPETVFPS", "replyto": "rkgcvpn-cH", "invitation": "ICLR.cc/2020/Conference/Paper490/-/Official_Comment", "content": {"title": "Response to Review #2", "comment": "Thanks a lot for your helpful feedback. We address your concerns one by one below.\n\n\n\u201cThe proposed method relies on the VAR framework for contrastive disentanglement.\u201d\n \nWe assume this should have read \u201cVAE framework for contrastive disentanglement\u201d?\n\n\n\u201cIn the original VAE framework for contrastive disentanglement, the two constraints were considered in a slightly more implicit way.\u201d\n\nIf we understand correctly, you mean to refer to cVAE by \u201coriginal VAE framework for contrastive disentanglement\u201d. If true, we emphasize that the cVAE setup in [1] does not consider the background latent prior to be a Delta distribution at zero in its loss function. The crucial KL divergence term is missing. We highlight this in Section 3.1 of our paper. The assumption about background latent factors being the same is considered implicitly in [1]. However, empirical results for [1] in our Table 1 corresponding to column $L_{W_2}$ suggest that a stronger enforcement is necessary and this improves results significantly from 38.26 to 11.07 in case of CelebA.\n\n\n\u201cIt is not surprising to see that the proposed method seems to outperform cVAE only slightly.\u201d\n\nWe respectfully disagree with this comment. As highlighted in Table 1 of the paper, our method shows a healthy and consistent 3% improvement on linear MNIST, 5% improvement on non-linear MNIST, 4% improvement on CelebA and 5% improvement on the Affectnet datasets. This clearly demonstrates the superiority of our method over existing state-of-the-art approaches. We obtain this better performance with having fewer neural networks to train and fewer parameters (and hence lower computational cost). We use principled and well motivated loss functions. Hence we believe our qualitative and quantitative results show significant improvements over the state-of-the-art.\n\n\n\u201cTo see how much we can really gain by further explicitly enforcing the two constraints, the authors may do some further studies with varied sample sizes.\u201d\n\nThanks for the suggestion. We have performed additional experiments with varied sample sizes and found that our methods still consistently outperform the existing approach from [1] across a variety of sample sizes. These results are provided in the link (https://drive.google.com/file/d/1vDjfCZ1aulVp1drTMTMik3EZqh77wC1h/view?usp=sharing). The experimental details are as follows: In our experiments, we found that [1] performs the best among all the other baselines as seen from Table 1 of our paper, hence we consider [1] to be our baseline. With regards to varied sample sizes, for the target dataset containing MNIST linearly superimposed on grass (L-MNIST), we chose the number of samples from each digit class to be 1000, 2500 and 4986. Similarly, for the background dataset of just grass images, the total number of samples are 4986, 2500 and 1000 respectively.\n\n\n\u201cBTW, in the result for CelebA in Table 1, cVAE seems to outperform the proposed method, denoted by 'Ours', according to Acc of $z_x$, but it was indicated in the text (as well as in by the bold font) that 'Ours' performed the best--is there a typo?\u201d\n\nWe acknowledge that this is indeed a typo. We apologize for the resulting confusion surrounding the accuracy scores in Table 1. We updated our revised table with the correct results. Indeed, as we can now see from Table 1,  \u201cOurs (Eqn. 7)\u201d performs the best in the case of CelebA. Thanks for bringing this up.\n\n\nReferences:\n[1] Contrastive Variational Autoencoder Enhances Salient Features, https://arxiv.org/abs/1902.04601"}, "signatures": ["ICLR.cc/2020/Conference/Paper490/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper490/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards Principled Objectives for Contrastive Disentanglement", "authors": ["Anwesa Choudhuri", "Ashok Vardhan Makkuva", "Ranvir Rana", "Sewoong Oh", "Girish Chowdhary", "Alexander Schwing"], "authorids": ["anwesac2@illinois.edu", "makkuva2@illinois.edu", "rbrana2@illinois.edu", "sewoong@cs.washington.edu", "girishc@illinois.edu", "aschwing@illinois.edu"], "keywords": ["Disentanglement", "Contrastive"], "abstract": "Unsupervised learning is an important tool that has received a significant amount of attention for decades. Its goal is `unsupervised recovery,' i.e., extracting salient factors/properties  from unlabeled data. Because of the challenges in defining salient properties, recently, `contrastive disentanglement' has gained popularity to discover the additional variations that are enhanced in one dataset relative to another. %In fact, contrastive disentanglement and unsupervised recovery are often combined in that we seek additional variations that exhibit salient factors/properties. \nExisting formulations have devised a variety of losses for this task. However, all present day methods exhibit two major shortcomings: (1) encodings for data that does not exhibit salient factors is not pushed to carry no signal; and (2) introduced losses are often hard to estimate and require additional trainable parameters. We present a new formulation for contrastive disentanglement which avoids both shortcomings by carefully formulating a probabilistic model and by using non-parametric yet easily computable metrics. We show on four challenging datasets that the proposed approach is able to better disentangle salient factors.\n", "pdf": "/pdf/88f11550ec9b8d15bac6f49bb02b96aad7d29b65.pdf", "paperhash": "choudhuri|towards_principled_objectives_for_contrastive_disentanglement", "code": "https://drive.google.com/file/d/1IlBRrf2Zm4YOOb67IQOrwXlaXlO6-4Q4/view?usp=sharing", "original_pdf": "/attachment/35954b7c799e83c4bb57e7f87ecf0910096cf4f1.pdf", "_bibtex": "@misc{\nchoudhuri2020towards,\ntitle={Towards Principled Objectives for Contrastive Disentanglement},\nauthor={Anwesa Choudhuri and Ashok Vardhan Makkuva and Ranvir Rana and Sewoong Oh and Girish Chowdhary and Alexander Schwing},\nyear={2020},\nurl={https://openreview.net/forum?id=B1lPETVFPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1lPETVFPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper490/Authors", "ICLR.cc/2020/Conference/Paper490/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper490/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper490/Reviewers", "ICLR.cc/2020/Conference/Paper490/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper490/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper490/Authors|ICLR.cc/2020/Conference/Paper490/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504170706, "tmdate": 1576860556745, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper490/Authors", "ICLR.cc/2020/Conference/Paper490/Reviewers", "ICLR.cc/2020/Conference/Paper490/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper490/-/Official_Comment"}}}, {"id": "rJloyeDcjH", "original": null, "number": 4, "cdate": 1573707746784, "ddate": null, "tcdate": 1573707746784, "tmdate": 1573707875269, "tddate": null, "forum": "B1lPETVFPS", "replyto": "B1lPETVFPS", "invitation": "ICLR.cc/2020/Conference/Paper490/-/Official_Comment", "content": {"title": "General comments", "comment": "We would like to thank all the reviewers for their constructive feedback and valuable suggestions. We have now updated our paper taking these comments into account which has helped us make our submission stronger. The following is a summary of updates in our revised version:\n\n1. In our earlier version there was a typo in Table 1 which we have now corrected. Also it seems that there has been some confusion regarding our Table 1 in the paper. To clarify this confusion we redesigned this table. Please note that for columns denoted by $z_x$, both lower clustering accuracy (CA) and lower silhouette score (SS) indicate better performance, whereas for columns titled $s_x$, higher CA and higher SS are better. Thus our method shows a healthy and consistent improvement over the state-of-the-art across all the datasets.\n\n2. We have now added a proof for the fact that our proposed quadratic loss term $E_q[\\mu(y)^2+\\sigma(y)^2]$ exactly equals the squared second-order Wasserstein distance $W_2^2(q(s|y), \\delta\\{s=0\\} )$ averaged over $y$. We included this as Lemma 2 in the Appendix of our revised paper. We thank Reviewer #1 for this suggestion.\n\n3. We have included new additional experiments with varied sample sizes for background and target datasets (as suggested by Reviewer #2) and found that our methods still consistently outperform the existing approach from [1] across a variety of sample sizes. Appendix A.4 contains these full details.\n\nReferences:\n[1] Contrastive Variational Autoencoder Enhances Salient Features, https://arxiv.org/abs/1902.04601\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper490/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper490/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards Principled Objectives for Contrastive Disentanglement", "authors": ["Anwesa Choudhuri", "Ashok Vardhan Makkuva", "Ranvir Rana", "Sewoong Oh", "Girish Chowdhary", "Alexander Schwing"], "authorids": ["anwesac2@illinois.edu", "makkuva2@illinois.edu", "rbrana2@illinois.edu", "sewoong@cs.washington.edu", "girishc@illinois.edu", "aschwing@illinois.edu"], "keywords": ["Disentanglement", "Contrastive"], "abstract": "Unsupervised learning is an important tool that has received a significant amount of attention for decades. Its goal is `unsupervised recovery,' i.e., extracting salient factors/properties  from unlabeled data. Because of the challenges in defining salient properties, recently, `contrastive disentanglement' has gained popularity to discover the additional variations that are enhanced in one dataset relative to another. %In fact, contrastive disentanglement and unsupervised recovery are often combined in that we seek additional variations that exhibit salient factors/properties. \nExisting formulations have devised a variety of losses for this task. However, all present day methods exhibit two major shortcomings: (1) encodings for data that does not exhibit salient factors is not pushed to carry no signal; and (2) introduced losses are often hard to estimate and require additional trainable parameters. We present a new formulation for contrastive disentanglement which avoids both shortcomings by carefully formulating a probabilistic model and by using non-parametric yet easily computable metrics. We show on four challenging datasets that the proposed approach is able to better disentangle salient factors.\n", "pdf": "/pdf/88f11550ec9b8d15bac6f49bb02b96aad7d29b65.pdf", "paperhash": "choudhuri|towards_principled_objectives_for_contrastive_disentanglement", "code": "https://drive.google.com/file/d/1IlBRrf2Zm4YOOb67IQOrwXlaXlO6-4Q4/view?usp=sharing", "original_pdf": "/attachment/35954b7c799e83c4bb57e7f87ecf0910096cf4f1.pdf", "_bibtex": "@misc{\nchoudhuri2020towards,\ntitle={Towards Principled Objectives for Contrastive Disentanglement},\nauthor={Anwesa Choudhuri and Ashok Vardhan Makkuva and Ranvir Rana and Sewoong Oh and Girish Chowdhary and Alexander Schwing},\nyear={2020},\nurl={https://openreview.net/forum?id=B1lPETVFPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1lPETVFPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper490/Authors", "ICLR.cc/2020/Conference/Paper490/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper490/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper490/Reviewers", "ICLR.cc/2020/Conference/Paper490/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper490/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper490/Authors|ICLR.cc/2020/Conference/Paper490/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504170706, "tmdate": 1576860556745, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper490/Authors", "ICLR.cc/2020/Conference/Paper490/Reviewers", "ICLR.cc/2020/Conference/Paper490/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper490/-/Official_Comment"}}}, {"id": "rJl3Jw85or", "original": null, "number": 3, "cdate": 1573705444201, "ddate": null, "tcdate": 1573705444201, "tmdate": 1573705687772, "tddate": null, "forum": "B1lPETVFPS", "replyto": "r1gmvxTkqH", "invitation": "ICLR.cc/2020/Conference/Paper490/-/Official_Comment", "content": {"title": "Response to Review #1", "comment": "We really appreciate your encouraging words on our contributions.\n\n\n\u201cIf I get it correct, the replacing term $E_q[\\mu(y)^2+\\sigma(y)^2]=H(q(s|y))$ is the entropy of $q(s|y)$.\u201d\n\nWe would like to clarify that our proposed loss term $E_q[\\mu(y)^2+\\sigma(y)^2]$ does not equal to the entropy $H(q(s|y))$. The reason: $q(s|y)$ is a Gaussian distribution with a diagonal covariance matrix $\\Sigma$, its differential entropy is given by $H(q(s|y)) = \\frac{1}{2} \\log(det(2 \\pi e\\Sigma))$ [1]. Note that the diagonal entries of $\\Sigma$ are denoted by the variance vector $\\sigma(y)$. Moreover, the entropy $H(q(s|y))$ does not depend on the mean $\\mu(y)$ because differential entropy is translation invariant. Thus our loss term is not the same as the Gaussian entropy.\n\n\n\u201cThis means the behavior of the proposed loss tries to minimize the entropy while the original term tries to maximize the entropy, regardless of it is ill-defined. Thus, the replacement of the loss term seems reasonable to me at first glance but does not quite directly fix the problem of an ill-defined KL-divergence.\u201d\n\nAs mentioned, our proposed loss does not nullify the original loss which maximizes entropy: the loss term is not the Gaussian entropy. We also like to emphasize that KL divergence between any two distributions $P$ and $Q$, i.e., $KL(P || Q)$, is always well defined [2]. If $P$ is not absolutely continuous with respect to $Q$, which is the case here since $Q$ is a Delta distribution, it is simply defined as infinity. The fact that we obtain a term containing KL divergence between a Gaussian and a Delta distribution follows naturally from the classical VAE style upper bound on the negative log-likelihood. We propose a new differentiable loss term instead of this KL divergence so as to obtain gradients.\n\n\n\u201c A more reasonable solution is to replace the KL divergence with the Wasserstein distance\u201d\n\nThis is a very good suggestion. Indeed, our proposed loss term $E_q[\\mu(y)^2+\\sigma(y)^2]$ exactly equals the squared second-order Wasserstein distance $W_2^2(q(s|y), \\delta\\{s=0\\} )$ averaged over $y$. We apologize for not stressing upon this fact in the original version. Our updated version highlights this contribution. In particular, we have provided a mathematical proof for the above fact as Lemma 2 in the Appendix of our revised paper.\n\n\n\u201cI'm more curious about the way to evaluate the gradient.... Is that a heavy computational burden?\u201d\n\nTo compute the Wasserstein loss $W_2^2(q_t(z),q_b(z))$, we use mini-batches of samples from both the distributions $q_t(z)$ and $q_b(z)$. The batch size is 64. Hence we can efficiently compute the optimal matching matrix using standard optimal transport libraries such as [4]. With regards to computing the gradient, if $T^\\ast$ denotes the optimal transport matrix and $C$ is the cost matrix which depends on the VAE parameters $\\phi$, the gradient of $W_2$ loss term is given by $\\nabla_\\phi L = \\sum_{i,j} T^\\ast_{ij} \\nabla_\\phi C_{ij}$. Note that this follows from Danskin\u2019s theorem [5]. This is also highlighted in the last paragraph of Section 3.2 of the paper. Hence our proposed loss only incurs a negligible additional computational cost. In fact this is reflected by the running times of our algorithm with and without the loss $L_{W_2}$: \n\nTime to run 1 epoch:\nOurs (Eqn 7) [without $L_{W_2}$]: 16.9 secs\nOurs (Eqn 8) [with $L_{W_2}$]: 17.3 secs\n\n\n\u201cAnother concern is the numerical result seems not as good as the qualitative results.\u201d\n\nWe respectfully disagree. Please note that in Table 1, for columns denoted by $z_x$, both lower clustering accuracy (CA) and lower silhouette score (SS) indicate better performance, whereas for columns titled $s_x$, higher CA and higher SS are better. This is also explained in the caption of Table 1. To clarify this confusion we redesigned this table. Thus our method shows a healthy and consistent 3% improvement on linear MNIST, 5% improvement on non-linear MNIST, 4% improvement on CelebA and 5% improvement on Affectnet datasets. This clearly demonstrates the superiority of our method over the existing state-of-the-art approaches. We obtain better performance with much fewer number of parameters to train and using principled and well motivated loss functions. Hence we believe our qualitative and quantitative results show significant improvements over the state-of-the-art.\n\n\nReferences:\n[1]  https://en.wikipedia.org/wiki/Multivariate_normal_distribution#Differential_entropy\n[2] Definition 1.4, Page 18 in http://www.stat.yale.edu/~yw562/teaching/itlectures.pdf\n[3] Theorem 2.12 of Topics in Optimal Transportation, Cedric Villani. Link: https://people.math.gatech.edu/~gangbo/Cedric-Villani.pdf\n[4] Python Optimal Transport: https://github.com/rflamary/POT\n[5] https://www.jstor.org/stable/2946123"}, "signatures": ["ICLR.cc/2020/Conference/Paper490/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper490/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards Principled Objectives for Contrastive Disentanglement", "authors": ["Anwesa Choudhuri", "Ashok Vardhan Makkuva", "Ranvir Rana", "Sewoong Oh", "Girish Chowdhary", "Alexander Schwing"], "authorids": ["anwesac2@illinois.edu", "makkuva2@illinois.edu", "rbrana2@illinois.edu", "sewoong@cs.washington.edu", "girishc@illinois.edu", "aschwing@illinois.edu"], "keywords": ["Disentanglement", "Contrastive"], "abstract": "Unsupervised learning is an important tool that has received a significant amount of attention for decades. Its goal is `unsupervised recovery,' i.e., extracting salient factors/properties  from unlabeled data. Because of the challenges in defining salient properties, recently, `contrastive disentanglement' has gained popularity to discover the additional variations that are enhanced in one dataset relative to another. %In fact, contrastive disentanglement and unsupervised recovery are often combined in that we seek additional variations that exhibit salient factors/properties. \nExisting formulations have devised a variety of losses for this task. However, all present day methods exhibit two major shortcomings: (1) encodings for data that does not exhibit salient factors is not pushed to carry no signal; and (2) introduced losses are often hard to estimate and require additional trainable parameters. We present a new formulation for contrastive disentanglement which avoids both shortcomings by carefully formulating a probabilistic model and by using non-parametric yet easily computable metrics. We show on four challenging datasets that the proposed approach is able to better disentangle salient factors.\n", "pdf": "/pdf/88f11550ec9b8d15bac6f49bb02b96aad7d29b65.pdf", "paperhash": "choudhuri|towards_principled_objectives_for_contrastive_disentanglement", "code": "https://drive.google.com/file/d/1IlBRrf2Zm4YOOb67IQOrwXlaXlO6-4Q4/view?usp=sharing", "original_pdf": "/attachment/35954b7c799e83c4bb57e7f87ecf0910096cf4f1.pdf", "_bibtex": "@misc{\nchoudhuri2020towards,\ntitle={Towards Principled Objectives for Contrastive Disentanglement},\nauthor={Anwesa Choudhuri and Ashok Vardhan Makkuva and Ranvir Rana and Sewoong Oh and Girish Chowdhary and Alexander Schwing},\nyear={2020},\nurl={https://openreview.net/forum?id=B1lPETVFPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1lPETVFPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper490/Authors", "ICLR.cc/2020/Conference/Paper490/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper490/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper490/Reviewers", "ICLR.cc/2020/Conference/Paper490/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper490/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper490/Authors|ICLR.cc/2020/Conference/Paper490/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504170706, "tmdate": 1576860556745, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper490/Authors", "ICLR.cc/2020/Conference/Paper490/Reviewers", "ICLR.cc/2020/Conference/Paper490/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper490/-/Official_Comment"}}}, {"id": "r1gmvxTkqH", "original": null, "number": 1, "cdate": 1571962970905, "ddate": null, "tcdate": 1571962970905, "tmdate": 1572972588963, "tddate": null, "forum": "B1lPETVFPS", "replyto": "B1lPETVFPS", "invitation": "ICLR.cc/2020/Conference/Paper490/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Contributions:\n1. This paper fixes two problems that appeared in common contrastive disentanglement methods.\n2. The paper evaluates its method in multiple experiments.\n\nOverall, this paper does a nice contribution to improving existing methods of contrastive disentanglement. My major concern is the novelty in this paper since all the contributions can be summarized into adding two additional loss terms, which I would like to discuss below.\n\n1. [The missing term KL[q(s|y)||p_y(s)]]. One major claim of this paper is to add back the additional term KL[q(s|y)||p_y(s)] in order to push the encoder q(s|y) to converge to a point mass \\delta\\{s=0\\} for background images y. According to the mathematical formula, this term is natural, but the major concern is the non-overlapping support between q(s|y) and p_y(s), since in the paper they assume p_y(s)=\\delta\\{s=0\\}. This paper claims they fix this problem. But I do not think the solution is satisfactory, because they use another term E_q[\\mu_{\\phi,s}(y)^2+\\sigma2_{\\phi,s}(y)] to replace KL[q(s|y)||\\delta\\{s=0\\}]. If I get it correct, the replacing term E_q[\\mu(y)^2+\\sigma2(y)]=H(q(s|y)) is the entropy of q(s|y). But if we check the true divergence term, KL[q(s|y)||\\delta\\{s=0\\}]=-E_q[\\log\\delta\\{s=0\\}]-H(q(s|y)) that consists of a negative entropy term. This means the behavior of the proposed loss tries to minimize the entropy while the original term tries to maximize the entropy, regardless of it is ill-defined. Thus, the replacement of the loss term seems reasonable to me at first glance but does not quite directly fix the problem of an ill-defined KL-divergence. A more reasonable solution is to replace the KL divergence with the Wasserstein distance. Is there any difficulty if we do that?\n\n2. [The additional distributional matching term W_2^2(q_t(z),q_b(z))]. This term makes perfect sense to me, even though it is just added artificially. I'm more curious about the way to evaluate the gradient. Normally, computing Wasserstein distance is hard due to a hard linear programming matching algorithm. In this paper, the authors seem to use an existing library to compute the optimal transportation matrix. Is that a heavy computational burden? Or one could resort to some entropy regularization methods with sinkhorn iterative solvers. I'm curious about how these two methods compare with each other.\n\n3. Another concern is the numerical result seems not as good as the qualitative results.\n\nOverall, I think this paper does make some changes regarding previous works on contrastive disentanglement. But their main contributions need some more explanation and justification."}, "signatures": ["ICLR.cc/2020/Conference/Paper490/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper490/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards Principled Objectives for Contrastive Disentanglement", "authors": ["Anwesa Choudhuri", "Ashok Vardhan Makkuva", "Ranvir Rana", "Sewoong Oh", "Girish Chowdhary", "Alexander Schwing"], "authorids": ["anwesac2@illinois.edu", "makkuva2@illinois.edu", "rbrana2@illinois.edu", "sewoong@cs.washington.edu", "girishc@illinois.edu", "aschwing@illinois.edu"], "keywords": ["Disentanglement", "Contrastive"], "abstract": "Unsupervised learning is an important tool that has received a significant amount of attention for decades. Its goal is `unsupervised recovery,' i.e., extracting salient factors/properties  from unlabeled data. Because of the challenges in defining salient properties, recently, `contrastive disentanglement' has gained popularity to discover the additional variations that are enhanced in one dataset relative to another. %In fact, contrastive disentanglement and unsupervised recovery are often combined in that we seek additional variations that exhibit salient factors/properties. \nExisting formulations have devised a variety of losses for this task. However, all present day methods exhibit two major shortcomings: (1) encodings for data that does not exhibit salient factors is not pushed to carry no signal; and (2) introduced losses are often hard to estimate and require additional trainable parameters. We present a new formulation for contrastive disentanglement which avoids both shortcomings by carefully formulating a probabilistic model and by using non-parametric yet easily computable metrics. We show on four challenging datasets that the proposed approach is able to better disentangle salient factors.\n", "pdf": "/pdf/88f11550ec9b8d15bac6f49bb02b96aad7d29b65.pdf", "paperhash": "choudhuri|towards_principled_objectives_for_contrastive_disentanglement", "code": "https://drive.google.com/file/d/1IlBRrf2Zm4YOOb67IQOrwXlaXlO6-4Q4/view?usp=sharing", "original_pdf": "/attachment/35954b7c799e83c4bb57e7f87ecf0910096cf4f1.pdf", "_bibtex": "@misc{\nchoudhuri2020towards,\ntitle={Towards Principled Objectives for Contrastive Disentanglement},\nauthor={Anwesa Choudhuri and Ashok Vardhan Makkuva and Ranvir Rana and Sewoong Oh and Girish Chowdhary and Alexander Schwing},\nyear={2020},\nurl={https://openreview.net/forum?id=B1lPETVFPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "B1lPETVFPS", "replyto": "B1lPETVFPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper490/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper490/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574942248201, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper490/Reviewers"], "noninvitees": [], "tcdate": 1570237751374, "tmdate": 1574942248213, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper490/-/Official_Review"}}}, {"id": "S1gXN-wZ5H", "original": null, "number": 2, "cdate": 1572069675196, "ddate": null, "tcdate": 1572069675196, "tmdate": 1572972588922, "tddate": null, "forum": "B1lPETVFPS", "replyto": "B1lPETVFPS", "invitation": "ICLR.cc/2020/Conference/Paper490/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Existing formulations for contrastive disentanglement ignore the information about the prior, i.e., the salient features of the background data should be zero, and moreover,\u00a0additional KL-divergence-based losses which are hard to estimate in practice are introduced to improve disentanglement.\n\nTo resolve this issue, the authors propose new regularizations still based on VAE.\u00a0Specifically, they propose to\n\nRegularization 1. penalize the generated mean and standard deviation of background latent variable towards (0,0) which means Dirac-delta distribution on 0.\n\nRegularization 2. match common latent variable distributions for background dataset and target dataset using Wasserstein distance.\n\nWhile authors point out that existing formulation in Abid & Zou (2019); Ruiz et al. (2019) ignores about the prior, the effect of regularizing salient feature seems marginal. This can be found in experimental results in Table, proposed objective (7) gives similar or worse result on L MNIST(similar), CelebA(worse), and Affectnet(worse) dataset. Also, visualization of salient features in Figure 4 show that representation balancing effects of proposed regularization terms are marginal compared to cVAE.\n\nAnother cocern is proposed regularization 1,2 might hurt the original ELBO objective. This concern is also related to beta-VAE, the paper showed that large coefficient for prior regularization simply makes the disentanglement effect for VAE. On the other hand, proposed objective (7) requires 'large constant' for approximate quadratic loss. This might induce unintentional negative effects for the objective (7) and objective (8). It would be great if authors can show the proposed regularization does not induce such unintended numerical problem. A possible solution might be showing asymptotic theoretical guarantees as in semi-implicit variational inference.\n\nThe objective (8) seems to require additional computational cost to solve linear programming problem. It is not clear if it is effective enough to bear that extra cost; it is hard to conclude that proposed regularization performs better than previous works without confidence interval."}, "signatures": ["ICLR.cc/2020/Conference/Paper490/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper490/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards Principled Objectives for Contrastive Disentanglement", "authors": ["Anwesa Choudhuri", "Ashok Vardhan Makkuva", "Ranvir Rana", "Sewoong Oh", "Girish Chowdhary", "Alexander Schwing"], "authorids": ["anwesac2@illinois.edu", "makkuva2@illinois.edu", "rbrana2@illinois.edu", "sewoong@cs.washington.edu", "girishc@illinois.edu", "aschwing@illinois.edu"], "keywords": ["Disentanglement", "Contrastive"], "abstract": "Unsupervised learning is an important tool that has received a significant amount of attention for decades. Its goal is `unsupervised recovery,' i.e., extracting salient factors/properties  from unlabeled data. Because of the challenges in defining salient properties, recently, `contrastive disentanglement' has gained popularity to discover the additional variations that are enhanced in one dataset relative to another. %In fact, contrastive disentanglement and unsupervised recovery are often combined in that we seek additional variations that exhibit salient factors/properties. \nExisting formulations have devised a variety of losses for this task. However, all present day methods exhibit two major shortcomings: (1) encodings for data that does not exhibit salient factors is not pushed to carry no signal; and (2) introduced losses are often hard to estimate and require additional trainable parameters. We present a new formulation for contrastive disentanglement which avoids both shortcomings by carefully formulating a probabilistic model and by using non-parametric yet easily computable metrics. We show on four challenging datasets that the proposed approach is able to better disentangle salient factors.\n", "pdf": "/pdf/88f11550ec9b8d15bac6f49bb02b96aad7d29b65.pdf", "paperhash": "choudhuri|towards_principled_objectives_for_contrastive_disentanglement", "code": "https://drive.google.com/file/d/1IlBRrf2Zm4YOOb67IQOrwXlaXlO6-4Q4/view?usp=sharing", "original_pdf": "/attachment/35954b7c799e83c4bb57e7f87ecf0910096cf4f1.pdf", "_bibtex": "@misc{\nchoudhuri2020towards,\ntitle={Towards Principled Objectives for Contrastive Disentanglement},\nauthor={Anwesa Choudhuri and Ashok Vardhan Makkuva and Ranvir Rana and Sewoong Oh and Girish Chowdhary and Alexander Schwing},\nyear={2020},\nurl={https://openreview.net/forum?id=B1lPETVFPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "B1lPETVFPS", "replyto": "B1lPETVFPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper490/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper490/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574942248201, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper490/Reviewers"], "noninvitees": [], "tcdate": 1570237751374, "tmdate": 1574942248213, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper490/-/Official_Review"}}}], "count": 9}