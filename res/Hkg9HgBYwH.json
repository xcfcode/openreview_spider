{"notes": [{"id": "Hkg9HgBYwH", "original": "HygNT_gtwH", "number": 2295, "cdate": 1569439809790, "ddate": null, "tcdate": 1569439809790, "tmdate": 1577168238754, "tddate": null, "forum": "Hkg9HgBYwH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["kechoi@cs.stanford.edu", "fjord@google.com", "iansimon@google.com", "noms@google.com", "jesseengel@google.com"], "title": "Encoding Musical Style with Transformer Autoencoders", "authors": ["Kristy Choi", "Curtis Hawthorne", "Ian Simon", "Monica Dinculescu", "Jesse Engel"], "pdf": "/pdf/f7f3ea5c1ab7861ea3c6be419e284cde8ac9831a.pdf", "abstract": "We consider the problem of learning high-level controls over the global structure of sequence generation, particularly in the context of symbolic music generation with complex language models. In this work, we present the Transformer autoencoder, which aggregates encodings of the input data across time to obtain a global representation of style from a given performance.  We show it is possible to combine this global embedding with other temporally distributed embeddings, enabling improved control over the separate aspects of performance style and  and melody. Empirically, we demonstrate the effectiveness of our method on a variety of music generation tasks on the MAESTRO dataset and an internal, 10,000+ hour dataset of piano performances, where we achieve improvements in terms of log-likelihood and mean listening scores as compared to relevant baselines.", "keywords": ["music generation", "sequence-to-sequence model", "controllable generation"], "paperhash": "choi|encoding_musical_style_with_transformer_autoencoders", "original_pdf": "/attachment/acc2b57c2b36fce1091d4eec696d9526a7240986.pdf", "_bibtex": "@misc{\nchoi2020encoding,\ntitle={Encoding Musical Style with Transformer Autoencoders},\nauthor={Kristy Choi and Curtis Hawthorne and Ian Simon and Monica Dinculescu and Jesse Engel},\nyear={2020},\nurl={https://openreview.net/forum?id=Hkg9HgBYwH}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "r5Iy5tlmjt", "original": null, "number": 1, "cdate": 1576798745484, "ddate": null, "tcdate": 1576798745484, "tmdate": 1576800890657, "tddate": null, "forum": "Hkg9HgBYwH", "replyto": "Hkg9HgBYwH", "invitation": "ICLR.cc/2020/Conference/Paper2295/-/Decision", "content": {"decision": "Reject", "comment": "Main content:\n\nBlind review #3 summarizes it well:\n\nThis paper presents a technique for encoding the high level \u201cstyle\u201d of pieces of symbolic music. The music is represented as a variant of the MIDI format. The main strategy is to condition a Music Transformer architecture on this global \u201cstyle embedding\u201d.  Additionally, the Music Transformer model is also conditioned on a combination of both \u201cstyle\u201d and \u201cmelody\u201d embeddings to try and generate music \u201csimilar\u201d to the conditioning melody but in the style of the performance embedding. \n\n--\n\nDiscussion:\n\nThe reviewers questioned the novelty. Blind review #2 wrote: \"Overall, I think the paper presents an interesting application and parts of it are well written, however I have concerns with the technical presentation in parts of the paper and some of the methodology. Firstly, I think the algorithmic novelty in the paper is fairly limited. The performance conditioning vector is generated by an additional encoding transformer, compared to the Music Transformer paper (Huang et. al. 2019b). However, the limited algorithmic novelty is not the main concern. The authors also mention an internal dataset of music audio and transcriptions, which can be a major contribution to the music information retrieval (MIR) community. However it is not clear if this dataset will be publicly released or is only for internal experiments.\"\n\nHowever, after revision, the same reviewer has upgraded the review to a weak accept, as the authors wrote \"We emphasize that our goal is to provide users with more fine-grained control over the outputs generated by a seq2seq language model. Despite its simplicity, our method is able to learn a global representation of style for a Transformer, which to the best of our knowledge is a novel contribution for music generation. Additionally, we can synthesize an arbitrary melody into the style of another performance, and we demonstrate the effectiveness of our results both quantitatively (metrics) and qualitatively (interpolations, samples, and user listening studies).\"\n\n--\n\nRecommendation and justification:\n\nThis paper is borderline for the reasons above, and due to the large number of strong papers, is not accepted at this time. As one comment, this work might actually be more suitable for a more specialized conference like ISMIR, as its novel contribution is more to music applications than to fundamental machine learning approaches.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["kechoi@cs.stanford.edu", "fjord@google.com", "iansimon@google.com", "noms@google.com", "jesseengel@google.com"], "title": "Encoding Musical Style with Transformer Autoencoders", "authors": ["Kristy Choi", "Curtis Hawthorne", "Ian Simon", "Monica Dinculescu", "Jesse Engel"], "pdf": "/pdf/f7f3ea5c1ab7861ea3c6be419e284cde8ac9831a.pdf", "abstract": "We consider the problem of learning high-level controls over the global structure of sequence generation, particularly in the context of symbolic music generation with complex language models. In this work, we present the Transformer autoencoder, which aggregates encodings of the input data across time to obtain a global representation of style from a given performance.  We show it is possible to combine this global embedding with other temporally distributed embeddings, enabling improved control over the separate aspects of performance style and  and melody. Empirically, we demonstrate the effectiveness of our method on a variety of music generation tasks on the MAESTRO dataset and an internal, 10,000+ hour dataset of piano performances, where we achieve improvements in terms of log-likelihood and mean listening scores as compared to relevant baselines.", "keywords": ["music generation", "sequence-to-sequence model", "controllable generation"], "paperhash": "choi|encoding_musical_style_with_transformer_autoencoders", "original_pdf": "/attachment/acc2b57c2b36fce1091d4eec696d9526a7240986.pdf", "_bibtex": "@misc{\nchoi2020encoding,\ntitle={Encoding Musical Style with Transformer Autoencoders},\nauthor={Kristy Choi and Curtis Hawthorne and Ian Simon and Monica Dinculescu and Jesse Engel},\nyear={2020},\nurl={https://openreview.net/forum?id=Hkg9HgBYwH}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "Hkg9HgBYwH", "replyto": "Hkg9HgBYwH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795723429, "tmdate": 1576800274900, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2295/-/Decision"}}}, {"id": "Hkx-_FQnjS", "original": null, "number": 4, "cdate": 1573824873450, "ddate": null, "tcdate": 1573824873450, "tmdate": 1573824873450, "tddate": null, "forum": "Hkg9HgBYwH", "replyto": "BygAqcDQsS", "invitation": "ICLR.cc/2020/Conference/Paper2295/-/Official_Comment", "content": {"title": "Thanks for the changes. ", "comment": "Dear Authors, \n\nThank you for all the changes to the draft. I think the paper is much improved due to all the changes. I need some time to go through all the changes in detail and reconsider my rating for the paper. "}, "signatures": ["ICLR.cc/2020/Conference/Paper2295/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2295/AnonReviewer3", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["kechoi@cs.stanford.edu", "fjord@google.com", "iansimon@google.com", "noms@google.com", "jesseengel@google.com"], "title": "Encoding Musical Style with Transformer Autoencoders", "authors": ["Kristy Choi", "Curtis Hawthorne", "Ian Simon", "Monica Dinculescu", "Jesse Engel"], "pdf": "/pdf/f7f3ea5c1ab7861ea3c6be419e284cde8ac9831a.pdf", "abstract": "We consider the problem of learning high-level controls over the global structure of sequence generation, particularly in the context of symbolic music generation with complex language models. In this work, we present the Transformer autoencoder, which aggregates encodings of the input data across time to obtain a global representation of style from a given performance.  We show it is possible to combine this global embedding with other temporally distributed embeddings, enabling improved control over the separate aspects of performance style and  and melody. Empirically, we demonstrate the effectiveness of our method on a variety of music generation tasks on the MAESTRO dataset and an internal, 10,000+ hour dataset of piano performances, where we achieve improvements in terms of log-likelihood and mean listening scores as compared to relevant baselines.", "keywords": ["music generation", "sequence-to-sequence model", "controllable generation"], "paperhash": "choi|encoding_musical_style_with_transformer_autoencoders", "original_pdf": "/attachment/acc2b57c2b36fce1091d4eec696d9526a7240986.pdf", "_bibtex": "@misc{\nchoi2020encoding,\ntitle={Encoding Musical Style with Transformer Autoencoders},\nauthor={Kristy Choi and Curtis Hawthorne and Ian Simon and Monica Dinculescu and Jesse Engel},\nyear={2020},\nurl={https://openreview.net/forum?id=Hkg9HgBYwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Hkg9HgBYwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2295/Authors", "ICLR.cc/2020/Conference/Paper2295/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2295/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2295/Reviewers", "ICLR.cc/2020/Conference/Paper2295/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2295/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2295/Authors|ICLR.cc/2020/Conference/Paper2295/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504143469, "tmdate": 1576860551669, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2295/Authors", "ICLR.cc/2020/Conference/Paper2295/Reviewers", "ICLR.cc/2020/Conference/Paper2295/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2295/-/Official_Comment"}}}, {"id": "BygAqcDQsS", "original": null, "number": 3, "cdate": 1573251734316, "ddate": null, "tcdate": 1573251734316, "tmdate": 1573251734316, "tddate": null, "forum": "Hkg9HgBYwH", "replyto": "Hygtfbj6FH", "invitation": "ICLR.cc/2020/Conference/Paper2295/-/Official_Comment", "content": {"title": "Specific Comments", "comment": "In addition to the common concerns as written above, we address Reviewer #3's specific concerns below:\n\n> 1. What does \u201ctypically incorporate global conditioning\u201d mean in the Introduction?\n\nThe generative models which \u201ctypically incorporate global conditioning...\u201d are simply conditional variants of models such as the conditional VAE (Sohn et al. 2015) and conditional GAN (Mizra et. al 2014) which perform generation by conditioning on a global signal, such as a one-hot encoding of the class label.\n\n> 2. Need more clarification about \u201cinternal dataset\u201d and \u201cpreprocessing procedure\u201d for melody extraction.\nAs the reviewer noted, we did our best to anonymize the submission with respect to the dataset and preprocessing techniques used in the paper. Our internal dataset is comprised of approximately 400K piano performances which comprise the 10,000+ hours of audio. Due to licensing restrictions we are unable to release the internal piano performance dataset -- however, we will provide pre-trained models based on this dataset for public use. \n\nFor the melody representation (vocabulary), we followed (Waite et. al 2016) to encode the melody as a sequence of tokens and quantized it to a 100ms grid. For the melody extraction procedure, we used an algorithm as in the open-sourced code (Anonymous for review), where we use a heuristic to extract the note with the highest in a given performance. Specifically, we construct a transition matrix of melody pitches and use the Viterbi algorithm to infer the most likely sequence of melody events within a given frame. We will add additional details regarding the melody extraction and encoding in the Supplement. \n\n>3. There needs to be additional clarification of how the model is trained.\n\nThis is a good point. As noted by the reviewer, for performance-only conditioning, the decoder is tasked with predicting the same performance that was fed as input to the encoder. In this way, we encourage the model to learn global representations (the mean-aggregated performance embedding from the encoder) that will faithfully be able to reconstruct the input performance. For melody & performance conditioning, the Transformer autoencoder is trained to predict a new performance using the combined melody+performance embedding, where the loss is computed with respect to the conditioned input performance that is provided to the encoder.\n\nTo make this point more clear, we will update the submission with a new version of Figure 1 with the reviewer\u2019s suggestions. We have also added these additional details on the model training procedure in the supplemental materials in the revision.\n\n\nReferences:\nSohn et. al 2015: Learning Structured Output Representation using Deep Conditional Generative Models\nMizra et. al 2014: Conditional Generative Adversarial Nets\nWaite et. al 2016: https://magenta.tensorflow.org/2016/07/15/lookback-rnn-attention-rnn"}, "signatures": ["ICLR.cc/2020/Conference/Paper2295/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2295/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["kechoi@cs.stanford.edu", "fjord@google.com", "iansimon@google.com", "noms@google.com", "jesseengel@google.com"], "title": "Encoding Musical Style with Transformer Autoencoders", "authors": ["Kristy Choi", "Curtis Hawthorne", "Ian Simon", "Monica Dinculescu", "Jesse Engel"], "pdf": "/pdf/f7f3ea5c1ab7861ea3c6be419e284cde8ac9831a.pdf", "abstract": "We consider the problem of learning high-level controls over the global structure of sequence generation, particularly in the context of symbolic music generation with complex language models. In this work, we present the Transformer autoencoder, which aggregates encodings of the input data across time to obtain a global representation of style from a given performance.  We show it is possible to combine this global embedding with other temporally distributed embeddings, enabling improved control over the separate aspects of performance style and  and melody. Empirically, we demonstrate the effectiveness of our method on a variety of music generation tasks on the MAESTRO dataset and an internal, 10,000+ hour dataset of piano performances, where we achieve improvements in terms of log-likelihood and mean listening scores as compared to relevant baselines.", "keywords": ["music generation", "sequence-to-sequence model", "controllable generation"], "paperhash": "choi|encoding_musical_style_with_transformer_autoencoders", "original_pdf": "/attachment/acc2b57c2b36fce1091d4eec696d9526a7240986.pdf", "_bibtex": "@misc{\nchoi2020encoding,\ntitle={Encoding Musical Style with Transformer Autoencoders},\nauthor={Kristy Choi and Curtis Hawthorne and Ian Simon and Monica Dinculescu and Jesse Engel},\nyear={2020},\nurl={https://openreview.net/forum?id=Hkg9HgBYwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Hkg9HgBYwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2295/Authors", "ICLR.cc/2020/Conference/Paper2295/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2295/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2295/Reviewers", "ICLR.cc/2020/Conference/Paper2295/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2295/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2295/Authors|ICLR.cc/2020/Conference/Paper2295/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504143469, "tmdate": 1576860551669, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2295/Authors", "ICLR.cc/2020/Conference/Paper2295/Reviewers", "ICLR.cc/2020/Conference/Paper2295/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2295/-/Official_Comment"}}}, {"id": "Byecdcvmsr", "original": null, "number": 2, "cdate": 1573251698128, "ddate": null, "tcdate": 1573251698128, "tmdate": 1573251698128, "tddate": null, "forum": "Hkg9HgBYwH", "replyto": "rJlAcEsTtS", "invitation": "ICLR.cc/2020/Conference/Paper2295/-/Official_Comment", "content": {"title": "Specific comments", "comment": "In addition to the common concerns as written above, we address Reviewer #1's specific concerns below:\n\n1. Is there a mathematical definition of style-specific generation, with more relevant baselines (e.g. cycle-consistency)? \nWe appreciate the reviewer\u2019s suggestion and additional references. Because our method is learning a conditional generative model, we do not incorporate any additional style-specific terms in our learning objective as we perform maximum-likelihood training. However, developing a more fine-grained notion of style for music generation is certainly interesting.\n\nWe originally compared against the Music Transformer to ensure that conditioning would improve the model, and added various versions of our model (e.g. melody-only conditioning) as relevant baselines for comparison. To the best of our knowledge, our work is the first to incorporate conditioning information in sequential language models for music generation.\n\nAdding a consistency loss term is a good idea for a baseline, but for our problem it was ill-posed because we did not have a straightforward way of partitioning our data into different categories. In image translation literature (e.g. Zhu et. al 2018), there exists a clear source and target domain even if the images themselves are unlabeled. For both MAESTRO and the internal dataset, such separations between the source and target domains are unclear (e.g. musical tempo, rhythm, pitch, etc.). Nevertheless, we agree that this would be interesting to explore as future work.\n\nReferences:\nZhu et. al 2018: Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2295/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2295/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["kechoi@cs.stanford.edu", "fjord@google.com", "iansimon@google.com", "noms@google.com", "jesseengel@google.com"], "title": "Encoding Musical Style with Transformer Autoencoders", "authors": ["Kristy Choi", "Curtis Hawthorne", "Ian Simon", "Monica Dinculescu", "Jesse Engel"], "pdf": "/pdf/f7f3ea5c1ab7861ea3c6be419e284cde8ac9831a.pdf", "abstract": "We consider the problem of learning high-level controls over the global structure of sequence generation, particularly in the context of symbolic music generation with complex language models. In this work, we present the Transformer autoencoder, which aggregates encodings of the input data across time to obtain a global representation of style from a given performance.  We show it is possible to combine this global embedding with other temporally distributed embeddings, enabling improved control over the separate aspects of performance style and  and melody. Empirically, we demonstrate the effectiveness of our method on a variety of music generation tasks on the MAESTRO dataset and an internal, 10,000+ hour dataset of piano performances, where we achieve improvements in terms of log-likelihood and mean listening scores as compared to relevant baselines.", "keywords": ["music generation", "sequence-to-sequence model", "controllable generation"], "paperhash": "choi|encoding_musical_style_with_transformer_autoencoders", "original_pdf": "/attachment/acc2b57c2b36fce1091d4eec696d9526a7240986.pdf", "_bibtex": "@misc{\nchoi2020encoding,\ntitle={Encoding Musical Style with Transformer Autoencoders},\nauthor={Kristy Choi and Curtis Hawthorne and Ian Simon and Monica Dinculescu and Jesse Engel},\nyear={2020},\nurl={https://openreview.net/forum?id=Hkg9HgBYwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Hkg9HgBYwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2295/Authors", "ICLR.cc/2020/Conference/Paper2295/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2295/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2295/Reviewers", "ICLR.cc/2020/Conference/Paper2295/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2295/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2295/Authors|ICLR.cc/2020/Conference/Paper2295/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504143469, "tmdate": 1576860551669, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2295/Authors", "ICLR.cc/2020/Conference/Paper2295/Reviewers", "ICLR.cc/2020/Conference/Paper2295/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2295/-/Official_Comment"}}}, {"id": "HyeZH9PXjr", "original": null, "number": 1, "cdate": 1573251641106, "ddate": null, "tcdate": 1573251641106, "tmdate": 1573251641106, "tddate": null, "forum": "Hkg9HgBYwH", "replyto": "Hkg9HgBYwH", "invitation": "ICLR.cc/2020/Conference/Paper2295/-/Official_Comment", "content": {"title": "Addressing Common Concerns", "comment": "We thank the reviewers for their insightful comments! Before we address common threads of concern below, then respond to each reviewer individually, we highlight significant changes we have made to the paper:\n\n- Following helpful reviewer suggestions, we have completely reworked the evaluation section (Section 4) of the paper, and uploaded a *revised* version of the paper.  \n- For more meaningful and fine-grained evaluation, we have adopted a collection 8 commonly used musical similarity metrics with which to compare samples. \n- As an aggregate similarity metric, we now use the average of the 8 similarity metrics, rather than the IMQ kernel, which is more intuitive and better motivated.\n- We have added tables 3 and 4 to report these fine-grained new metrics, and updated figures 2 and 3 to use the new aggregate similarity metric (Section 5).\n- All of the key findings of the paper remain the same with these new metrics, and many effects are actually more pronounced than with the original kernel similarity metric. \n\n> R1/R3: Why not use existing techniques for measuring similarity between musical performances? Why not compare the conditioning melody with the generated performance similar to query-by-humming (QBH)?\n\nWe appreciate the reviewers\u2019 feedback regarding our kernel evaluation metric. Upon further reflection, we agree that there are simpler and more intuitive ways to evaluate musical similarity, and as described above we have reworked large parts of the paper to reflect that. In the revised version of the paper, we follow existing techniques (Yang & Lerch 2018) using the Overlapping Area (OA) of common similarity metrics including:\n- Note Density\n- Pitch Range\n- Mean/Var Pitch\n- Mean/Var Velocity\n- Mean/Var Duration\nWe note that certain features in the paper are not applicable to our setting (e.g. note length transition matrix) because they were developed for monophonic melodies, while we are evaluating polyphonic piano performances. \n\nAlthough we no longer use the IMQ kernel as the similarity metric, we emphasize that the key results remain the same. For clarity, we quickly examine here why that is the case and what originally motivated the kernel approach. The kernel approach assumes that the conditioning performances (x~p(x)) and generated performances (y, y\u2019~q(y)) are drawn from two different distributions, and MMD computes the degree to which these two distributions are similar in a kernel feature space. We experimented with a variety of kernels commonly used in the literature (e.g. RBF kernel) and found that the IMQ worked best empirically. Our results remained unchanged because the MMD distributional similarity correlates well with the average difference in extracted similarity features. \n\n\nWe did not compare an input melody to the generated performance (as in QBH) because we wanted to compare the similarities of polyphonic sequences. As the melody is represented using a different encoding and vocabulary than the performance, comparison is not straightforward and would not provide relevant information. We do note that our user listening studies also implicitly serve as a proxy to measure melodic similarity. Thus we performed our similarity evaluations against the original performance from which the melody was extracted. This is reflected in the melody & performance conditioning case: we average two OA terms, OA(source performance of extracted melody, generated sample)  and OA(conditioning performance, generated sample), as our final metric. In this way, we account for the contributions of both the conditioning melody and performance sequence.\n\n> R1/R3: Algorithmic novelty is somewhat limited.\nWe emphasize that our goal is to provide users with more fine-grained control over the outputs generated by a seq2seq language model. Despite its simplicity, our method is able to learn a global representation of style for a Transformer, which to the best of our knowledge is a novel contribution for music generation. Additionally, we can synthesize an arbitrary melody into the style of another performance, and we demonstrate the effectiveness of our results both quantitatively (metrics) and qualitatively (interpolations, samples, and user listening studies).\n\nReferences:\nHung et. al 2018: Improving Automatic Jazz Melody Generation by Transfer Learning Techniques\nYang & Lerch 2018: On the evaluation of generative models in music"}, "signatures": ["ICLR.cc/2020/Conference/Paper2295/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2295/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["kechoi@cs.stanford.edu", "fjord@google.com", "iansimon@google.com", "noms@google.com", "jesseengel@google.com"], "title": "Encoding Musical Style with Transformer Autoencoders", "authors": ["Kristy Choi", "Curtis Hawthorne", "Ian Simon", "Monica Dinculescu", "Jesse Engel"], "pdf": "/pdf/f7f3ea5c1ab7861ea3c6be419e284cde8ac9831a.pdf", "abstract": "We consider the problem of learning high-level controls over the global structure of sequence generation, particularly in the context of symbolic music generation with complex language models. In this work, we present the Transformer autoencoder, which aggregates encodings of the input data across time to obtain a global representation of style from a given performance.  We show it is possible to combine this global embedding with other temporally distributed embeddings, enabling improved control over the separate aspects of performance style and  and melody. Empirically, we demonstrate the effectiveness of our method on a variety of music generation tasks on the MAESTRO dataset and an internal, 10,000+ hour dataset of piano performances, where we achieve improvements in terms of log-likelihood and mean listening scores as compared to relevant baselines.", "keywords": ["music generation", "sequence-to-sequence model", "controllable generation"], "paperhash": "choi|encoding_musical_style_with_transformer_autoencoders", "original_pdf": "/attachment/acc2b57c2b36fce1091d4eec696d9526a7240986.pdf", "_bibtex": "@misc{\nchoi2020encoding,\ntitle={Encoding Musical Style with Transformer Autoencoders},\nauthor={Kristy Choi and Curtis Hawthorne and Ian Simon and Monica Dinculescu and Jesse Engel},\nyear={2020},\nurl={https://openreview.net/forum?id=Hkg9HgBYwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Hkg9HgBYwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2295/Authors", "ICLR.cc/2020/Conference/Paper2295/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2295/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2295/Reviewers", "ICLR.cc/2020/Conference/Paper2295/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2295/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2295/Authors|ICLR.cc/2020/Conference/Paper2295/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504143469, "tmdate": 1576860551669, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2295/Authors", "ICLR.cc/2020/Conference/Paper2295/Reviewers", "ICLR.cc/2020/Conference/Paper2295/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2295/-/Official_Comment"}}}, {"id": "Hygtfbj6FH", "original": null, "number": 1, "cdate": 1571823888773, "ddate": null, "tcdate": 1571823888773, "tmdate": 1572972357478, "tddate": null, "forum": "Hkg9HgBYwH", "replyto": "Hkg9HgBYwH", "invitation": "ICLR.cc/2020/Conference/Paper2295/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper presents a technique for encoding the high level \u201cstyle\u201d of pieces of symbolic music. The music is represented as a variant of the MIDI format. The main strategy is to condition a Music Transformer architecture on this global \u201cstyle embedding\u201d.  Additionally, the Music Transformer model is also conditioned on a combination of both \u201cstyle\u201d and \u201cmelody\u201d embeddings to try and generate music \u201csimilar\u201d to the conditioning melody but in the style of the performance embedding. \n\nOverall, I think the paper presents an interesting application and parts of it are well written, however I have concerns with the technical presentation in parts of the paper and some of the methodology. Firstly, I think the algorithmic novelty in the paper is fairly limited. The performance conditioning vector is generated by an additional encoding transformer, compared to the Music Transformer paper (Huang et. al. 2019b). However, the limited algorithmic novelty is not the main concern. The authors also mention an internal dataset of music audio and transcriptions, which can be a major contribution to the music information retrieval (MIR) community. However it is not clear if this dataset will be publicly released or is only for internal experiments. \n\nIn terms of technical presentation, I think the authors should clarify how the model is trained. It took me a couple of passes and reading the Music Transformer paper to realise that in the melody and performance conditioning case, the aim is to generate the full score (melody and accompaniment) while conditioning on the performance style and melody (which is represented using a different vocabulary). This point can be easily clarified in Figure 1, by adding the input to the encoder as input to the decoder for computing the loss. Although I understand the need for anonymity and constraints while referring to unreleased datasets, it would still be useful for the reader/reviewer to have some details of how the melody was extracted and represented. \u201cAn internal procedure\u201d is quite mysterious. \n\nMeasuring music similarity is a difficult problem and the topic has been the subject of at least 2 decades of research. I find the description of the \u201cperformance feature\u201d to be lacking in necessary background and detail. Firstly, I am not sure what the final dimensionality of the feature vector is. Is it real valued? The authors mention (Yang and Lerch, 2018) but use a totally different set of attributes compared to that paper. I also don\u2019t see the connection between this proposed feature vector and using the IMQ kernel for measuring similarity. This connection is not motivated adequately and after reading (Jitkrittum et. al. 2019) its not obvious to me why this is the most appropriate metric. Finally, it would be useful if the authors comment on existing methods for measuring music similarity in symbolic music and how their proposed feature fits into existing work. A lot of work has been published on this topic, most recently in the context of Query-by-Humming [1]. \n\nMinor Comments\n\n1. \u201c...which typically incorporate global conditioning as part pf the training procedure\u201d Could you elaborate on this point? Is the global conditioning the samples from the noise distribution? \n2. Figure 1 should be clarified or another figure should be added to show how the melody conditioning works. Maybe a comment on the melody vocabulary or a reference would also be useful. \n3. The MAESTRO dataset is described in terms of the number of performances while the internal dataset is described in terms of the number of hours of audio. Its not possible for the reader to get a sense of the relative sizes of the 2 datasets and how the results should be interpreted. \n4. There should be more background and description in Section 4. Where does the performance feature come from? Why use this feature compared to existing techniques for measuring similarity between symbolic music pieces? Is it computational efficiency? Why not compare the conditioning melody with the generated performance similar to query-by-humming? Where does the IMQ kernel come from? What is the size of the feature vector? \n5. In section 5.2, a conditioning sample, a generated sequence and an unconditional sample are used to compute the similarity measure. Which terms do these correspond to in the MMD-like term (x,y,y\u2019)? \n6. I like the experiments performed in Section 5.3 with the linear combination of 2 performance embeddings. \n\n[1] A Survey of Query-By-Humming Similarity Methods: http://vlm1.uta.edu/~athitsos/publications/kotsifakos_petra2012.pdf\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2295/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2295/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["kechoi@cs.stanford.edu", "fjord@google.com", "iansimon@google.com", "noms@google.com", "jesseengel@google.com"], "title": "Encoding Musical Style with Transformer Autoencoders", "authors": ["Kristy Choi", "Curtis Hawthorne", "Ian Simon", "Monica Dinculescu", "Jesse Engel"], "pdf": "/pdf/f7f3ea5c1ab7861ea3c6be419e284cde8ac9831a.pdf", "abstract": "We consider the problem of learning high-level controls over the global structure of sequence generation, particularly in the context of symbolic music generation with complex language models. In this work, we present the Transformer autoencoder, which aggregates encodings of the input data across time to obtain a global representation of style from a given performance.  We show it is possible to combine this global embedding with other temporally distributed embeddings, enabling improved control over the separate aspects of performance style and  and melody. Empirically, we demonstrate the effectiveness of our method on a variety of music generation tasks on the MAESTRO dataset and an internal, 10,000+ hour dataset of piano performances, where we achieve improvements in terms of log-likelihood and mean listening scores as compared to relevant baselines.", "keywords": ["music generation", "sequence-to-sequence model", "controllable generation"], "paperhash": "choi|encoding_musical_style_with_transformer_autoencoders", "original_pdf": "/attachment/acc2b57c2b36fce1091d4eec696d9526a7240986.pdf", "_bibtex": "@misc{\nchoi2020encoding,\ntitle={Encoding Musical Style with Transformer Autoencoders},\nauthor={Kristy Choi and Curtis Hawthorne and Ian Simon and Monica Dinculescu and Jesse Engel},\nyear={2020},\nurl={https://openreview.net/forum?id=Hkg9HgBYwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Hkg9HgBYwH", "replyto": "Hkg9HgBYwH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2295/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2295/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575885165296, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2295/Reviewers"], "noninvitees": [], "tcdate": 1570237724868, "tmdate": 1575885165309, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2295/-/Official_Review"}}}, {"id": "rJlAcEsTtS", "original": null, "number": 2, "cdate": 1571824790290, "ddate": null, "tcdate": 1571824790290, "tmdate": 1572972357432, "tddate": null, "forum": "Hkg9HgBYwH", "replyto": "Hkg9HgBYwH", "invitation": "ICLR.cc/2020/Conference/Paper2295/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "## summary\nIn this paper, the author extends the standard music Transformer into a conditional version: two encoders are evolved, one for encoding the performance and the other is used for encoding the melody. The output representation has to be similar to the input. The authors conduct experiments on the MAESTRO dataset and an internal, 10,000+ hour dataset of piano performances to verify the proposed algorithm.\n\n## Novelty \nThe application is interesting, but the novelty of the architecture itself is limited. Multiple encoder structure has been widely investigated in machine translation.\n\n## Questions\n1.\tIn section 4.2, how do you use the $\\mathcal{Y}$? Since it is defined but never used. What does the $p()$ and $q()$ mean ? You mentioned that \u201cWe omit the usual first term in the MMD loss \u2026\u201d but if so, why do you introduce this term to evaluation metric?\n2.\tBy checking the music Transformer, in Table 3, it is not surprising to see that the proposed method outperforms the corresponding baselines, because no conditional information is used. \n3.     It is better to give some mathematical definition of music generation with specific style. I am not working on music generation but I list two CV related papers about conditional image translation, which mathematically describes \"an image with specific style\".\n4.\tConsidering that this is an unsupervised setting that two styles are transformed, can cycle-consistency be implemented as a baseline? The following two papers are about conditional unsupervised image-to-image translation, which build a cycle-consistency loss during the feedback and might help improve the performances.\n\n\n## Reference\n[ref1] Multimodal Unsupervised Image-to-Image Translation, ECCV\u201918\n[ref2] Conditional image-to-image translation, CVPR\u201918\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2295/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2295/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["kechoi@cs.stanford.edu", "fjord@google.com", "iansimon@google.com", "noms@google.com", "jesseengel@google.com"], "title": "Encoding Musical Style with Transformer Autoencoders", "authors": ["Kristy Choi", "Curtis Hawthorne", "Ian Simon", "Monica Dinculescu", "Jesse Engel"], "pdf": "/pdf/f7f3ea5c1ab7861ea3c6be419e284cde8ac9831a.pdf", "abstract": "We consider the problem of learning high-level controls over the global structure of sequence generation, particularly in the context of symbolic music generation with complex language models. In this work, we present the Transformer autoencoder, which aggregates encodings of the input data across time to obtain a global representation of style from a given performance.  We show it is possible to combine this global embedding with other temporally distributed embeddings, enabling improved control over the separate aspects of performance style and  and melody. Empirically, we demonstrate the effectiveness of our method on a variety of music generation tasks on the MAESTRO dataset and an internal, 10,000+ hour dataset of piano performances, where we achieve improvements in terms of log-likelihood and mean listening scores as compared to relevant baselines.", "keywords": ["music generation", "sequence-to-sequence model", "controllable generation"], "paperhash": "choi|encoding_musical_style_with_transformer_autoencoders", "original_pdf": "/attachment/acc2b57c2b36fce1091d4eec696d9526a7240986.pdf", "_bibtex": "@misc{\nchoi2020encoding,\ntitle={Encoding Musical Style with Transformer Autoencoders},\nauthor={Kristy Choi and Curtis Hawthorne and Ian Simon and Monica Dinculescu and Jesse Engel},\nyear={2020},\nurl={https://openreview.net/forum?id=Hkg9HgBYwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Hkg9HgBYwH", "replyto": "Hkg9HgBYwH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2295/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2295/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575885165296, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2295/Reviewers"], "noninvitees": [], "tcdate": 1570237724868, "tmdate": 1575885165309, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2295/-/Official_Review"}}}], "count": 8}