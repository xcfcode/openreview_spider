{"notes": [{"ddate": null, "legacy_migration": true, "tmdate": 1393554060000, "tcdate": 1393554060000, "number": 1, "id": "pZNfKh1T62ZEM", "invitation": "ICLR.cc/2014/-/submission/workshop/reply", "forum": "IgLmlBsymQnyP", "replyto": "ggC0Jddw0PgjR", "signatures": ["anonymous reviewer 4d8c"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "This is indeed what I had in mind... And indeed, in the word2vec setup, they'll stay at 0 (because all gradients will be zero, in the word2vec parametrization, which isn't necessarily the case in, say, a neural network language model)... I had not realized this."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Distributional Models and Deep Learning Embeddings: Combining the Best of Both Worlds", "decision": "submitted, no decision", "abstract": "There are two main approaches to the distributed representation of words: low-dimensional deep learning embeddings and high-dimensional distributional models, in which each dimension corresponds to a context word. In this paper, we combine these two approaches by learning embeddings based on distributional-model vectors - as opposed to one-hot vectors as is standardly done in deep learning. We show that the combined approach has better performance on a word relatedness judgment task.", "pdf": "https://arxiv.org/abs/1312.5559", "paperhash": "sergienya|distributional_models_and_deep_learning_embeddings_combining_the_best_of_both_worlds", "keywords": [], "conflicts": [], "authors": ["Irina Sergienya", "Hinrich Sch\u00fctze"], "authorids": ["sergienya@gmail.com", "inquiries@cislmu.org"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392824460000, "tcdate": 1392824460000, "number": 4, "id": "wvfSwIGySBvdV", "invitation": "ICLR.cc/2014/-/submission/workshop/review", "forum": "IgLmlBsymQnyP", "replyto": "IgLmlBsymQnyP", "signatures": ["Irina Sergienya"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "We uploaded a new version of paper with the Related work section, where a relevant work of Hai Son Le et al. (EMNLP2010) is discussed.\r\n\r\nThanks to the reviewer who pointed out this paper.\r\n\r\nRegards, Irina"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Distributional Models and Deep Learning Embeddings: Combining the Best of Both Worlds", "decision": "submitted, no decision", "abstract": "There are two main approaches to the distributed representation of words: low-dimensional deep learning embeddings and high-dimensional distributional models, in which each dimension corresponds to a context word. In this paper, we combine these two approaches by learning embeddings based on distributional-model vectors - as opposed to one-hot vectors as is standardly done in deep learning. We show that the combined approach has better performance on a word relatedness judgment task.", "pdf": "https://arxiv.org/abs/1312.5559", "paperhash": "sergienya|distributional_models_and_deep_learning_embeddings_combining_the_best_of_both_worlds", "keywords": [], "conflicts": [], "authors": ["Irina Sergienya", "Hinrich Sch\u00fctze"], "authorids": ["sergienya@gmail.com", "inquiries@cislmu.org"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392674520000, "tcdate": 1392674520000, "number": 1, "id": "FdstFZk6tuIGQ", "invitation": "ICLR.cc/2014/-/submission/workshop/reply", "forum": "IgLmlBsymQnyP", "replyto": "K9CR9nnAg6-4W", "signatures": ["Irina Sergienya"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "Dear reviewer,\r\nPlease, find our reply for your comments below."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Distributional Models and Deep Learning Embeddings: Combining the Best of Both Worlds", "decision": "submitted, no decision", "abstract": "There are two main approaches to the distributed representation of words: low-dimensional deep learning embeddings and high-dimensional distributional models, in which each dimension corresponds to a context word. In this paper, we combine these two approaches by learning embeddings based on distributional-model vectors - as opposed to one-hot vectors as is standardly done in deep learning. We show that the combined approach has better performance on a word relatedness judgment task.", "pdf": "https://arxiv.org/abs/1312.5559", "paperhash": "sergienya|distributional_models_and_deep_learning_embeddings_combining_the_best_of_both_worlds", "keywords": [], "conflicts": [], "authors": ["Irina Sergienya", "Hinrich Sch\u00fctze"], "authorids": ["sergienya@gmail.com", "inquiries@cislmu.org"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392674280000, "tcdate": 1392674280000, "number": 3, "id": "f4NY4EleY1FeR", "invitation": "ICLR.cc/2014/-/submission/workshop/review", "forum": "IgLmlBsymQnyP", "replyto": "IgLmlBsymQnyP", "signatures": ["Irina Sergienya"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "Dear reviewer,\r\nThank you for your review and comments!\r\n\r\nIndeed, there are systems with performance on MEN and WordSim higher than our numbers.  Right now we are running experiments on a bigger training corpus. The preliminary numbers are much closer to the state-of-the-art performance. We will present results at the conference if\r\nthe paper gets accepted.\r\n\r\nThank you for pointing out the paper of Hai Son Le et al, which is very relevant.  They propose three initialization schemes.  Two of them, re-initialization and iterative re-initialization, use vectors from prediction space to initialize the context space during training.  This approach is both more complex and less efficient than ours. \r\n\r\nThe third initialization scheme, one vector initialization, initializes all word embeddings with the same random vector: this helps to keep rare words close to each other as an outcome of rare updates.  However, this approach is also less efficient than ours since the initial embedding is much denser than in our approach.  We are planning to run experiments with this approach and should be able to present results at the conference if the paper is accepted.\r\n\r\nRegards, Irina"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Distributional Models and Deep Learning Embeddings: Combining the Best of Both Worlds", "decision": "submitted, no decision", "abstract": "There are two main approaches to the distributed representation of words: low-dimensional deep learning embeddings and high-dimensional distributional models, in which each dimension corresponds to a context word. In this paper, we combine these two approaches by learning embeddings based on distributional-model vectors - as opposed to one-hot vectors as is standardly done in deep learning. We show that the combined approach has better performance on a word relatedness judgment task.", "pdf": "https://arxiv.org/abs/1312.5559", "paperhash": "sergienya|distributional_models_and_deep_learning_embeddings_combining_the_best_of_both_worlds", "keywords": [], "conflicts": [], "authors": ["Irina Sergienya", "Hinrich Sch\u00fctze"], "authorids": ["sergienya@gmail.com", "inquiries@cislmu.org"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392658680000, "tcdate": 1392658680000, "number": 1, "id": "ggC0Jddw0PgjR", "invitation": "ICLR.cc/2014/-/submission/workshop/reply", "forum": "IgLmlBsymQnyP", "replyto": "O3dl_qqbXKYbU", "signatures": ["Irina Sergienya"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "Dear reviewer,\r\nThank you for your review and comments!\r\n\r\nYour idea to initialize the embeddings with zero is interesting, but in the word2vec setup we are using embeddings initialized as zero remain zero vectors during training. We confirmed this in an experiment in which we\r\ninitialized the vectors as zero vectors.\r\n\r\nWe would like to ask whether this is the setting you were suggesting or did you have something else in mind?\r\n\r\nRegards, Irina"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Distributional Models and Deep Learning Embeddings: Combining the Best of Both Worlds", "decision": "submitted, no decision", "abstract": "There are two main approaches to the distributed representation of words: low-dimensional deep learning embeddings and high-dimensional distributional models, in which each dimension corresponds to a context word. In this paper, we combine these two approaches by learning embeddings based on distributional-model vectors - as opposed to one-hot vectors as is standardly done in deep learning. We show that the combined approach has better performance on a word relatedness judgment task.", "pdf": "https://arxiv.org/abs/1312.5559", "paperhash": "sergienya|distributional_models_and_deep_learning_embeddings_combining_the_best_of_both_worlds", "keywords": [], "conflicts": [], "authors": ["Irina Sergienya", "Hinrich Sch\u00fctze"], "authorids": ["sergienya@gmail.com", "inquiries@cislmu.org"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391787900000, "tcdate": 1391787900000, "number": 2, "id": "K9CR9nnAg6-4W", "invitation": "ICLR.cc/2014/-/submission/workshop/review", "forum": "IgLmlBsymQnyP", "replyto": "IgLmlBsymQnyP", "signatures": ["anonymous reviewer 4683"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Distributional Models and Deep Learning Embeddings: Combining the Best of Both Worlds", "review": "This paper proposes to derive distributional representation for words\r\nthat can be used to improve word embeddings. Distributional vectors\r\ncan present to the neural network that learns embeddings, instead of\r\npresenting one-hot vectors. One motivation is that distributional\r\nrepresentation could make the learning task easier for rare words. The\r\nauthors apply this approach only to rare words since word embeddings\r\nfor frequent words is frequently updated and then can be considered as\r\nsatisfactory.\r\n\r\nThe idea is nice. However, my main concern is about the experimental\r\npart. I don't understand the results. For the 'WordSim' task, the\r\npaper of E. Huang (ACL2012) exhibits spearman correlation above 50. So\r\nwether the results are incredibly below the baseline systems used in\r\n2012 (and thus, what can we conclude from this paper since it is\r\nstraightforward to improve a very poor system), or this need\r\nclarification. Anyway, baseline exists and should be mentioned.\r\n\r\nA minor comment about the last paragraph of the introduction. The\r\npaper (Hai Son Le et al. at EMNLP2010) addressed the issue of the\r\ninitialization of word embeddings and this seems to perform quite well\r\nespecially for rare words."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Distributional Models and Deep Learning Embeddings: Combining the Best of Both Worlds", "decision": "submitted, no decision", "abstract": "There are two main approaches to the distributed representation of words: low-dimensional deep learning embeddings and high-dimensional distributional models, in which each dimension corresponds to a context word. In this paper, we combine these two approaches by learning embeddings based on distributional-model vectors - as opposed to one-hot vectors as is standardly done in deep learning. We show that the combined approach has better performance on a word relatedness judgment task.", "pdf": "https://arxiv.org/abs/1312.5559", "paperhash": "sergienya|distributional_models_and_deep_learning_embeddings_combining_the_best_of_both_worlds", "keywords": [], "conflicts": [], "authors": ["Irina Sergienya", "Hinrich Sch\u00fctze"], "authorids": ["sergienya@gmail.com", "inquiries@cislmu.org"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1390789980000, "tcdate": 1390789980000, "number": 1, "id": "O3dl_qqbXKYbU", "invitation": "ICLR.cc/2014/-/submission/workshop/review", "forum": "IgLmlBsymQnyP", "replyto": "IgLmlBsymQnyP", "signatures": ["anonymous reviewer 4d8c"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Distributional Models and Deep Learning Embeddings: Combining the Best of Both Worlds", "review": "This paper investigates the use of so-called distributional models of words to improve the quality of learned word embeddings. These models are essentially high-dimensional vector representations of words, which indicate with what other words in the vocabulary has a word cooccurred (within some context). Since word embeddings can be understood as a linear lower-dimensional projection of the basic one-hot representations of words, these distributional model representations can be exploited by trainable word embeddings algorithms by simply concatenating them to the basic 'one-hot' representation or by replacing the one-hot representation by the distributed one for infrequent words. This paper shows that this approach can improve the quality of the word embeddings, as measured by an average correlation with human judgement of word similarity.\r\n\r\nOverall, I think this is a good workshop paper. Learning good words embeddings is an important topic of research, and this paper describes a nice, fairly simple trick to improve word embeddings. \r\n\r\nOne motivation for using it seems to be that infrequent words will not be able to move far enough away from their random initialization, so I wonder whether initializing all these word embeddings to 0 instead might have been enough to solve this problem. I think this would be a good baseline to compare with. \r\n\r\nI would also have liked to see whether any gains are obtained in a real NLP task, i.e. confirm that better correlation with human judgement is actually giving us something in practice.\r\n\r\nBut these are overall minor problems, for a workshop paper."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Distributional Models and Deep Learning Embeddings: Combining the Best of Both Worlds", "decision": "submitted, no decision", "abstract": "There are two main approaches to the distributed representation of words: low-dimensional deep learning embeddings and high-dimensional distributional models, in which each dimension corresponds to a context word. In this paper, we combine these two approaches by learning embeddings based on distributional-model vectors - as opposed to one-hot vectors as is standardly done in deep learning. We show that the combined approach has better performance on a word relatedness judgment task.", "pdf": "https://arxiv.org/abs/1312.5559", "paperhash": "sergienya|distributional_models_and_deep_learning_embeddings_combining_the_best_of_both_worlds", "keywords": [], "conflicts": [], "authors": ["Irina Sergienya", "Hinrich Sch\u00fctze"], "authorids": ["sergienya@gmail.com", "inquiries@cislmu.org"]}, "tags": [], "invitation": {}}}, {"replyto": null, "ddate": null, "legacy_migration": true, "tmdate": 1387549800000, "tcdate": 1387549800000, "number": 5, "id": "IgLmlBsymQnyP", "invitation": "ICLR.cc/2014/workshop/-/submission", "forum": "IgLmlBsymQnyP", "signatures": ["sergienya@gmail.com"], "readers": ["everyone"], "content": {"title": "Distributional Models and Deep Learning Embeddings: Combining the Best of Both Worlds", "decision": "submitted, no decision", "abstract": "There are two main approaches to the distributed representation of words: low-dimensional deep learning embeddings and high-dimensional distributional models, in which each dimension corresponds to a context word. In this paper, we combine these two approaches by learning embeddings based on distributional-model vectors - as opposed to one-hot vectors as is standardly done in deep learning. We show that the combined approach has better performance on a word relatedness judgment task.", "pdf": "https://arxiv.org/abs/1312.5559", "paperhash": "sergienya|distributional_models_and_deep_learning_embeddings_combining_the_best_of_both_worlds", "keywords": [], "conflicts": [], "authors": ["Irina Sergienya", "Hinrich Sch\u00fctze"], "authorids": ["sergienya@gmail.com", "inquiries@cislmu.org"]}, "writers": [], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1369422751717, "tmdate": 1496674357014, "id": "ICLR.cc/2014/workshop/-/submission", "writers": ["ICLR.cc/2014"], "signatures": ["OpenReview.net"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": []}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1377198751717, "cdate": 1496674357014}}}], "count": 8}