{"notes": [{"id": "ZVBtN6B_6i7", "original": "R9Yrb0yhaHJ", "number": 3282, "cdate": 1601308364540, "ddate": null, "tcdate": 1601308364540, "tmdate": 1614985716035, "tddate": null, "forum": "ZVBtN6B_6i7", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Not All Memories are Created Equal: Learning to Expire", "authorids": ["~Sainbayar_Sukhbaatar1", "~Da_JU1", "spoff@fb.com", "~Stephen_Roller1", "~Arthur_Szlam1", "~Jason_E_Weston1", "~Angela_Fan2"], "authors": ["Sainbayar Sukhbaatar", "Da JU", "Spencer Poff", "Stephen Roller", "Arthur Szlam", "Jason E Weston", "Angela Fan"], "keywords": ["expire", "long attention", "memory", "transformers"], "abstract": "Attention mechanisms have shown promising results in sequence modeling tasks that require long-term memory. Recent work has investigated mechanisms to reduce the computational cost of preserving and storing the memories. However, not all content in the past is equally important to remember. We propose Expire-Span, a method that learns to retain the most important information and \nexpire the irrelevant information. This enables Transformers to scale to attend to tens of thousands of previous timesteps efficiently, as not all hidden states from previous timesteps are preserved. We demonstrate that Expire-Span can help models identify and retain critical information and show it can achieve state of the art results on long-context language modeling, reinforcement learning, and algorithmic tasks. Finally, we show that Expire-Span can scale to memories that are tens of thousands in size, which is helpful on incredibly long context tasks such as character-level PG-19 and a frame-by-frame moving objects task.\n", "one-sentence_summary": "Scale attention mechanisms in Transformers by learning what to forget from the past.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sukhbaatar|not_all_memories_are_created_equal_learning_to_expire", "pdf": "/pdf/9b9a886588f342f4c08355a2099454303aa6a5a6.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=cGL2wzQVP", "_bibtex": "@misc{\nsukhbaatar2021not,\ntitle={Not All Memories are Created Equal: Learning to Expire},\nauthor={Sainbayar Sukhbaatar and Da JU and Spencer Poff and Stephen Roller and Arthur Szlam and Jason E Weston and Angela Fan},\nyear={2021},\nurl={https://openreview.net/forum?id=ZVBtN6B_6i7}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "o-eF6ru2Lww", "original": null, "number": 1, "cdate": 1610040426630, "ddate": null, "tcdate": 1610040426630, "tmdate": 1610474026176, "tddate": null, "forum": "ZVBtN6B_6i7", "replyto": "ZVBtN6B_6i7", "invitation": "ICLR.cc/2021/Conference/Paper3282/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "The paper studies the problem of identifying what information to forget in attention mechanisms, with the goal of enabling attention mechanisms to deal with longer contexts. This is a simple yet intuitive extension:  self-attention is augmented with an expiration value  prediction. Experiments were carried out on NLP and RL tasks.\nOverall, the paper has novelty in the proposed idea, however, there are concerns about the strength of the experiments; that the experiments fall short."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Not All Memories are Created Equal: Learning to Expire", "authorids": ["~Sainbayar_Sukhbaatar1", "~Da_JU1", "spoff@fb.com", "~Stephen_Roller1", "~Arthur_Szlam1", "~Jason_E_Weston1", "~Angela_Fan2"], "authors": ["Sainbayar Sukhbaatar", "Da JU", "Spencer Poff", "Stephen Roller", "Arthur Szlam", "Jason E Weston", "Angela Fan"], "keywords": ["expire", "long attention", "memory", "transformers"], "abstract": "Attention mechanisms have shown promising results in sequence modeling tasks that require long-term memory. Recent work has investigated mechanisms to reduce the computational cost of preserving and storing the memories. However, not all content in the past is equally important to remember. We propose Expire-Span, a method that learns to retain the most important information and \nexpire the irrelevant information. This enables Transformers to scale to attend to tens of thousands of previous timesteps efficiently, as not all hidden states from previous timesteps are preserved. We demonstrate that Expire-Span can help models identify and retain critical information and show it can achieve state of the art results on long-context language modeling, reinforcement learning, and algorithmic tasks. Finally, we show that Expire-Span can scale to memories that are tens of thousands in size, which is helpful on incredibly long context tasks such as character-level PG-19 and a frame-by-frame moving objects task.\n", "one-sentence_summary": "Scale attention mechanisms in Transformers by learning what to forget from the past.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sukhbaatar|not_all_memories_are_created_equal_learning_to_expire", "pdf": "/pdf/9b9a886588f342f4c08355a2099454303aa6a5a6.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=cGL2wzQVP", "_bibtex": "@misc{\nsukhbaatar2021not,\ntitle={Not All Memories are Created Equal: Learning to Expire},\nauthor={Sainbayar Sukhbaatar and Da JU and Spencer Poff and Stephen Roller and Arthur Szlam and Jason E Weston and Angela Fan},\nyear={2021},\nurl={https://openreview.net/forum?id=ZVBtN6B_6i7}\n}"}, "tags": [], "invitation": {"reply": {"forum": "ZVBtN6B_6i7", "replyto": "ZVBtN6B_6i7", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040426615, "tmdate": 1610474026159, "id": "ICLR.cc/2021/Conference/Paper3282/-/Decision"}}}, {"id": "BnBOom1Xd_U", "original": null, "number": 2, "cdate": 1603862192914, "ddate": null, "tcdate": 1603862192914, "tmdate": 1606804187855, "tddate": null, "forum": "ZVBtN6B_6i7", "replyto": "ZVBtN6B_6i7", "invitation": "ICLR.cc/2021/Conference/Paper3282/-/Official_Review", "content": {"title": "Impactful Idea with Clear Exposition, but Experiments Fall Short", "review": "Modified score: thank you authors for your thorough response. Given the new information and baselines, I think this is a promising paper that passes the acceptance threshold.\n\nOverall Quality: The authors present a method to improve the efficiency of transformer models when computing attention over previous time steps. Although this presents a neat idea that has the potential to improve an increasingly important model architecture, the experiments fall short of matching the claim that this method provides enables more efficient attention computation over memories _in practice_. Specifically, their baselines do not include relevant transformer modifications aimed at efficiency and they provide no detailed analysis on the memory size in practice. If the authors included more thorough experiments, this would be a strong paper. In their absence, it is marginally below the acceptance threshold. \n\nClarity: The abstract, introduction, background, and methods section were detailed yet easy to follow. The comparison of time complexity of prior work in the background section was particularly helpful. However, this precision did not carry over into the experimental section, which lacked thorough experimentation (detailed under weaknesses below) and figures 3-5 were out of order relative to the prose (the latter point is minor and does not affect my rating).\n\nSignificance: The potential impact is very high, especially as applications for transformers grow. If the authors could address the weaknesses outlined below, this could be an enormously helpful augmentation to the transformer architecture.\n\nStrengths:\n- The authors focus on an important problem for a very relevant architecture.\n- The writing is clear and enjoyable. Section 3 in particular is a very friendly introduction to transformer time complexity.\n- Evaluations performed over a variety of applications, spanning simple/toy to more realistic tasks.\n\nWeaknesses:\n- Corridor, instruction, portal, copy, pg-19, and colliding objects tasks only show comparisons for standard transformer models, as opposed to (at least one or two) comparable efficiency-optimized models. Giving the authors the benefit of the doubt, the first few experiments may serve more as proofs of concepts, where direct comparison with prior work is not as relevant or useful. But this leaves only one task in the paper with comparison to prior work on improving transformer efficiency: en-wiki-8. On en-wiki-8, the authors compare with just 1 modification and the improvement seems rather small. Small margins of improvement alone are not enough to reject a paper, but, given that this is the only result with a head to head comparison of efficiency optimized transformers, it makes it difficult for the community to discern the contribution of the work. Furthermore, on pg-19, copy task, and object collision, the authors do not provide the memory size/average memory size/effective memory size. This makes it difficult to understand if performance gains correspond with performance improvements, which is the methods stated purpose.\n- Intuitively, an inductive bias to expire memories would make a learned model more brittle when transferring to new tasks. E.g., in the instruction task a new form of instruction may become relevant in a test task that was never relevant in training tasks Why is this a reasonable trade-off to make?\n\nQuestion:\n- What value is shown in table 2? The caption says bit-per-byte, but the numbers are inconsistent with figure 7.\n- in figure 11, how is memory computed?", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3282/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3282/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Not All Memories are Created Equal: Learning to Expire", "authorids": ["~Sainbayar_Sukhbaatar1", "~Da_JU1", "spoff@fb.com", "~Stephen_Roller1", "~Arthur_Szlam1", "~Jason_E_Weston1", "~Angela_Fan2"], "authors": ["Sainbayar Sukhbaatar", "Da JU", "Spencer Poff", "Stephen Roller", "Arthur Szlam", "Jason E Weston", "Angela Fan"], "keywords": ["expire", "long attention", "memory", "transformers"], "abstract": "Attention mechanisms have shown promising results in sequence modeling tasks that require long-term memory. Recent work has investigated mechanisms to reduce the computational cost of preserving and storing the memories. However, not all content in the past is equally important to remember. We propose Expire-Span, a method that learns to retain the most important information and \nexpire the irrelevant information. This enables Transformers to scale to attend to tens of thousands of previous timesteps efficiently, as not all hidden states from previous timesteps are preserved. We demonstrate that Expire-Span can help models identify and retain critical information and show it can achieve state of the art results on long-context language modeling, reinforcement learning, and algorithmic tasks. Finally, we show that Expire-Span can scale to memories that are tens of thousands in size, which is helpful on incredibly long context tasks such as character-level PG-19 and a frame-by-frame moving objects task.\n", "one-sentence_summary": "Scale attention mechanisms in Transformers by learning what to forget from the past.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sukhbaatar|not_all_memories_are_created_equal_learning_to_expire", "pdf": "/pdf/9b9a886588f342f4c08355a2099454303aa6a5a6.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=cGL2wzQVP", "_bibtex": "@misc{\nsukhbaatar2021not,\ntitle={Not All Memories are Created Equal: Learning to Expire},\nauthor={Sainbayar Sukhbaatar and Da JU and Spencer Poff and Stephen Roller and Arthur Szlam and Jason E Weston and Angela Fan},\nyear={2021},\nurl={https://openreview.net/forum?id=ZVBtN6B_6i7}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "ZVBtN6B_6i7", "replyto": "ZVBtN6B_6i7", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3282/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538078632, "tmdate": 1606915779958, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3282/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3282/-/Official_Review"}}}, {"id": "VVvapY4GkJa", "original": null, "number": 7, "cdate": 1606167698027, "ddate": null, "tcdate": 1606167698027, "tmdate": 1606167698027, "tddate": null, "forum": "ZVBtN6B_6i7", "replyto": "Zq9Rx3To9T", "invitation": "ICLR.cc/2021/Conference/Paper3282/-/Official_Comment", "content": {"title": "General Response, continued", "comment": "continued response on the efficiency of Expire-Span, from above\n\n**Character-level Language Modeling on enwiki8**\n\n|Method| Max Span  | Dev BPC  | Peak GPU Memory  | Train Time/Batch |\n|---|---|---|---|---|\n| Transformer-XL | 2k  | 1.058 | 25.6GB  | 658ms |\n| Compressive Transformer | 4k  | 1.015 | 21.3GB  | 805ms |\n| Adaptive-Span  | 8k  | 1.036  | 20.3GB  | 483ms |\n| Expire-Span | 16k | 1.034 | 14.6GB | 408ms |\n\nWe added a comparison of the actual efficiency against three baselines: Transformer-XL, Adaptive-Span and Compressive Transformer. All models are trained on 32 GPUs. Among them, our method is the most efficient in terms for speed and memory usage, and obtains the best performance. The Transformer-XL has a fixed memory that can\u2019t adapt to data, so it quickly becomes slow and memory hungry even at a context size of 2k.  Adaptive-Span can adapt to data to adjust its memory size, so it does much better than the Transformer-XL. But, its memory size is fixed after training, so it lacks the flexibility of Expire-Span where memory size depends on the context. This disadvantage hurts it\u2019s efficiency as shown in the table.\n\nWe implemented a mean-pooling version of the Compressive Transformer. It has a worse performance and uses more GPU memory and is slower to train per batch when the memory size is 4k. Increasing the memory size 8k caused out-of-memory.\n\n**Character level language modeling on PG19**\n\n|Method| Max Span  | Dev BPC  | Peak GPU Memory  | Train Time/Batch |\n|---|---|---|---|---|\n| Adaptive-Span  | 16k  | 1.12  | 13.5GB  | 515ms |\n| Expire-Span | 16k | 1.12 | 12.9GB | 585ms |\n\nWe added an Adaptive-Span baseline for PG19 to compare to our Expire-Span model. We found that Adaptive-Span has similar performance to Expire-Span, most likely because the Expire-Span span size is too large. We are investigating increasing the Expire-Span loss to reduce the span size. For efficiency, Expire-Span and Adaptive-Span are similar, with Expire-Span using less GPU memory. We weren\u2019t able to train a Compressive Transformer in the given rebuttal period. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3282/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3282/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Not All Memories are Created Equal: Learning to Expire", "authorids": ["~Sainbayar_Sukhbaatar1", "~Da_JU1", "spoff@fb.com", "~Stephen_Roller1", "~Arthur_Szlam1", "~Jason_E_Weston1", "~Angela_Fan2"], "authors": ["Sainbayar Sukhbaatar", "Da JU", "Spencer Poff", "Stephen Roller", "Arthur Szlam", "Jason E Weston", "Angela Fan"], "keywords": ["expire", "long attention", "memory", "transformers"], "abstract": "Attention mechanisms have shown promising results in sequence modeling tasks that require long-term memory. Recent work has investigated mechanisms to reduce the computational cost of preserving and storing the memories. However, not all content in the past is equally important to remember. We propose Expire-Span, a method that learns to retain the most important information and \nexpire the irrelevant information. This enables Transformers to scale to attend to tens of thousands of previous timesteps efficiently, as not all hidden states from previous timesteps are preserved. We demonstrate that Expire-Span can help models identify and retain critical information and show it can achieve state of the art results on long-context language modeling, reinforcement learning, and algorithmic tasks. Finally, we show that Expire-Span can scale to memories that are tens of thousands in size, which is helpful on incredibly long context tasks such as character-level PG-19 and a frame-by-frame moving objects task.\n", "one-sentence_summary": "Scale attention mechanisms in Transformers by learning what to forget from the past.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sukhbaatar|not_all_memories_are_created_equal_learning_to_expire", "pdf": "/pdf/9b9a886588f342f4c08355a2099454303aa6a5a6.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=cGL2wzQVP", "_bibtex": "@misc{\nsukhbaatar2021not,\ntitle={Not All Memories are Created Equal: Learning to Expire},\nauthor={Sainbayar Sukhbaatar and Da JU and Spencer Poff and Stephen Roller and Arthur Szlam and Jason E Weston and Angela Fan},\nyear={2021},\nurl={https://openreview.net/forum?id=ZVBtN6B_6i7}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ZVBtN6B_6i7", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3282/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3282/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3282/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3282/Authors|ICLR.cc/2021/Conference/Paper3282/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3282/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839151, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3282/-/Official_Comment"}}}, {"id": "Zq9Rx3To9T", "original": null, "number": 6, "cdate": 1606167400034, "ddate": null, "tcdate": 1606167400034, "tmdate": 1606167400034, "tddate": null, "forum": "ZVBtN6B_6i7", "replyto": "ZVBtN6B_6i7", "invitation": "ICLR.cc/2021/Conference/Paper3282/-/Official_Comment", "content": {"title": "General Response", "comment": "We thank the reviewers. Expire-Span provides the general capability for models to learn what is important to remember and forget in long-memory tasks, which we demonstrate across a large variety of tasks. We are excited to see that \u201cidea seems to be quite interesting\u201d and \u201cthe presentation of the paper is clear and sound\u201d (R1), with \u201chigh potential impact\u201d (R2), and the mechanism is \u201cis elegant and seems novel within Transformer context\u201d (R4). We make two general points:\n\n**(1) Baselines**- Reviewers want to see additional baselines. We have added additional baselines: Adaptive-Span and Compressive Transformer for several tasks in the paper. Our proposed Expire-Span method outperforms both baselines. We\u2019d also like to clarify that our main baseline shown before these additions is actually Transformer-XL. We have updated the paper to make this clear. \nWe also provide a summary diagram to clarify our contribution. A large number of Transformer variant models have been proposed recently - many of these directly adjust the attention mechanism, which is actually completely orthogonal to our work - our work adjusts not the processing of the current block, but what to remember from previously processed blocks. Expire-Span can be added to any mechanism that adjusts attention, because Expire-Span operates on models that process blockwise and cache the past (hence Transformer-XL being our main baseline). \n\nModels that are baselines are models such as Transformer-XL and Compressive Transformer, as these modify the block-wise processing of long sequences. For example, Compressive Transformer compresses all previous tokens the same amount, while Expire-Span learns what to forget and what to keep salient. Adaptive-Span can adjust, based on the data, the amount of attention context size required, but Expire-Span can operate on drastically longer timescale and has stronger performance even at similar memory sizes. \n\nPlease click this link to see our summary figure: https://imgur.com/a/QnRetg8 (hosted on imgur for anonymization)\n\nFinally, we\u2019d like to emphasize that Expire-Span provides the ability for Transformers to scale to very large attention spans. It is unclear if tasks such as Language Modeling actually benefit from this ability --- e.g. do you really need to know a word mentioned 64K words ago? We believe nevertheless that demonstrating this ability is important, and show on various constructed tasks that are designed to require this size of memory. Various other tasks can be proposed in the future that would require such memory size, for example in video processing. \n\n**(2) Memory and Efficiency of Expire-Span** -  Reviewers would like more explicit efficiency comparisons. Below, we provide the peak GPU memory usage and Training time per Batch for Expire-Span compared to various baselines (TransformerXL, Adaptive-Span, and Compressive Transformer). We also provide the comparative results on the evaluation sets. Overall, we demonstrate across various tasks that Expire-Span is more efficient in terms of GPU memory usage and per-batch training time (per suggestion of R2 for these metrics), while achieving better performance. We include results on real tasks, enwiki8 and PG-19, per the suggestion of R1. \n\n**Collision Task**\n\n|Method| Max Span  | Test Error  | Peak GPU Memory  | Train Time/Batch |\n|---|---|---|---|---|\n| Adaptive-Span  | 16k  | 59.8% | 17.2GB  | 365ms |\n| Compressive Transformer  | 8.5k  | 63.8%  | 11.6GB  | 327ms |\n| Expire-Span | 16k | 52.2% | 11.9GB | 130ms |\n\nWe added a strong Adaptive-Span baseline for the collision task to compare against our method. Adaptive-Span\u2019s performance is slightly worse than our method, but its efficiency is much worse: 44% more memory usage and 2.8x slower to train on a single batch (measure on the final checkpoint). This result is not surprising because the adaptive-span model needs to process all frames after the collision, while the expire-span can forget them and only process the collision frame itself. \n\nWe also included a Compressive Transformer baseline with a maximum span of 8.5k (512 normal memories and 2k memories compressed by 4x). This model performs worse than our model, and increasing its span to 16k does not help. It uses the same GPU memory as our 16k model, but runs 2.5x slower, despite having a smaller span. All models are trained in the same setting (32 V100 GPUs).\n\n**Instruction Task** \n\n|Method| Max Span  | Test Accuracy  | Peak GPU Memory  | Train Time/Batch |\n|---|---|---|---|---|\n| Adaptive-Span  | 1.5k  | 68% | 14.2GB  | 240ms |\n| Compressive Transformer  |1.5k  | 71%  | 9.8GB  | 210ms |\n| Expire-Span | 1.5k | 74% | 8.4GB | 90ms |\n\nWe show that Adaptive-Span has worse performance and worse efficiency. Compressive Transformer uses only marginally more GPU memory, and the performance is not that far behind Expire-Span,  but the training time per batch is much slower --- over double that of Expire-Span. All models are trained in the same setting. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3282/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3282/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Not All Memories are Created Equal: Learning to Expire", "authorids": ["~Sainbayar_Sukhbaatar1", "~Da_JU1", "spoff@fb.com", "~Stephen_Roller1", "~Arthur_Szlam1", "~Jason_E_Weston1", "~Angela_Fan2"], "authors": ["Sainbayar Sukhbaatar", "Da JU", "Spencer Poff", "Stephen Roller", "Arthur Szlam", "Jason E Weston", "Angela Fan"], "keywords": ["expire", "long attention", "memory", "transformers"], "abstract": "Attention mechanisms have shown promising results in sequence modeling tasks that require long-term memory. Recent work has investigated mechanisms to reduce the computational cost of preserving and storing the memories. However, not all content in the past is equally important to remember. We propose Expire-Span, a method that learns to retain the most important information and \nexpire the irrelevant information. This enables Transformers to scale to attend to tens of thousands of previous timesteps efficiently, as not all hidden states from previous timesteps are preserved. We demonstrate that Expire-Span can help models identify and retain critical information and show it can achieve state of the art results on long-context language modeling, reinforcement learning, and algorithmic tasks. Finally, we show that Expire-Span can scale to memories that are tens of thousands in size, which is helpful on incredibly long context tasks such as character-level PG-19 and a frame-by-frame moving objects task.\n", "one-sentence_summary": "Scale attention mechanisms in Transformers by learning what to forget from the past.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sukhbaatar|not_all_memories_are_created_equal_learning_to_expire", "pdf": "/pdf/9b9a886588f342f4c08355a2099454303aa6a5a6.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=cGL2wzQVP", "_bibtex": "@misc{\nsukhbaatar2021not,\ntitle={Not All Memories are Created Equal: Learning to Expire},\nauthor={Sainbayar Sukhbaatar and Da JU and Spencer Poff and Stephen Roller and Arthur Szlam and Jason E Weston and Angela Fan},\nyear={2021},\nurl={https://openreview.net/forum?id=ZVBtN6B_6i7}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ZVBtN6B_6i7", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3282/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3282/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3282/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3282/Authors|ICLR.cc/2021/Conference/Paper3282/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3282/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839151, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3282/-/Official_Comment"}}}, {"id": "Me6xfcZrr91", "original": null, "number": 5, "cdate": 1606166818846, "ddate": null, "tcdate": 1606166818846, "tmdate": 1606166818846, "tddate": null, "forum": "ZVBtN6B_6i7", "replyto": "uPX8CM4JVas", "invitation": "ICLR.cc/2021/Conference/Paper3282/-/Official_Comment", "content": {"title": "review response", "comment": "Thanks for your review. We\u2019ve responded to your points in detail below:\n\n**re: How does the model understand how long to keep the memory?**\n \nThe Expire-Span values are not directly affected by future events, but they are being updated during training. If a future event that requires a specific memory occurs within its ramp steps (fixed R steps after expiring), it still can access that memory. The ramp suppresses the memory in a differentiable way, so the future event can cause the expire-span to grow during training.\n\n**re: model is brittle to regularization hyperparameters**\n\nWe introduce only one new hyperparameter, which is the loss coefficient for penalizing long spans. This parameter is actually useful in controlling the trade-off between computation and performance. A lower loss coefficient means longer spans, which are good for performance but will use more compute. If compute is limited, then a higher loss should be used to reduce spans, which might have a negative effect on performance. Other Transformers have such a parameter too: TransformerXL has a fixed \u201cattention length\u201d parameter that does the same thing and needs to be tuned.\n\nRegarding regularization --- in general, neural networks require regularization to train them stably. We identified this regularization issue and propose a straightforward way to fix it. In general, we feel this is not very different from e.g. a specialized learning rate schedule. This capability to forget memories to extend to very long context is a novel contribution, and understanding how to properly regularize is part of our work in showing how to train such a model. These ideas are generally useful for others interested in this direction. \n\n**re: missing important work from Gers et al**\n \nThanks for the reference, we will include in the paper. However, our approach targets attention on external memory because of their computation complexity. Forgetting reduces the number vectors to attend over, so it reduces computation and memory footprint. In LSTMs, however, memory is always a single vector, and forgetting means resetting that vector. So forgetting in a LSTM does not make it run faster, or reduce its memory footprint.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3282/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3282/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Not All Memories are Created Equal: Learning to Expire", "authorids": ["~Sainbayar_Sukhbaatar1", "~Da_JU1", "spoff@fb.com", "~Stephen_Roller1", "~Arthur_Szlam1", "~Jason_E_Weston1", "~Angela_Fan2"], "authors": ["Sainbayar Sukhbaatar", "Da JU", "Spencer Poff", "Stephen Roller", "Arthur Szlam", "Jason E Weston", "Angela Fan"], "keywords": ["expire", "long attention", "memory", "transformers"], "abstract": "Attention mechanisms have shown promising results in sequence modeling tasks that require long-term memory. Recent work has investigated mechanisms to reduce the computational cost of preserving and storing the memories. However, not all content in the past is equally important to remember. We propose Expire-Span, a method that learns to retain the most important information and \nexpire the irrelevant information. This enables Transformers to scale to attend to tens of thousands of previous timesteps efficiently, as not all hidden states from previous timesteps are preserved. We demonstrate that Expire-Span can help models identify and retain critical information and show it can achieve state of the art results on long-context language modeling, reinforcement learning, and algorithmic tasks. Finally, we show that Expire-Span can scale to memories that are tens of thousands in size, which is helpful on incredibly long context tasks such as character-level PG-19 and a frame-by-frame moving objects task.\n", "one-sentence_summary": "Scale attention mechanisms in Transformers by learning what to forget from the past.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sukhbaatar|not_all_memories_are_created_equal_learning_to_expire", "pdf": "/pdf/9b9a886588f342f4c08355a2099454303aa6a5a6.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=cGL2wzQVP", "_bibtex": "@misc{\nsukhbaatar2021not,\ntitle={Not All Memories are Created Equal: Learning to Expire},\nauthor={Sainbayar Sukhbaatar and Da JU and Spencer Poff and Stephen Roller and Arthur Szlam and Jason E Weston and Angela Fan},\nyear={2021},\nurl={https://openreview.net/forum?id=ZVBtN6B_6i7}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ZVBtN6B_6i7", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3282/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3282/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3282/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3282/Authors|ICLR.cc/2021/Conference/Paper3282/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3282/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839151, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3282/-/Official_Comment"}}}, {"id": "qW2UlpaisbI", "original": null, "number": 4, "cdate": 1606166780618, "ddate": null, "tcdate": 1606166780618, "tmdate": 1606166780618, "tddate": null, "forum": "ZVBtN6B_6i7", "replyto": "BnBOom1Xd_U", "invitation": "ICLR.cc/2021/Conference/Paper3282/-/Official_Comment", "content": {"title": "review response", "comment": "Thanks for your review! We are happy to see that you find the idea impactful. Below, we\u2019ve responded to each of your questions. \n\n**re: many tasks only have comparisons to standard transformers and there is no efficiency analysis**\n\nThanks! Based on your feedback, we have added several baselines for multiple tasks (including PG-19, per your suggestion to add results for a real task), and have completed a detailed efficiency analysis for all baselines and Expire-Span. We quantify both time to process a batch and peak GPU memory usage to capture efficiency. These results are in the general response and added to our paper. \n\n**re: does expiration make the learned model more brittle during transfer learning?**\n \nWe haven\u2019t tested our method in a transfer learning setting. But, if a model can be trained on the new task, then expire-span can learn to change and adapt to new requirements, just like the rest of the network. If there is no training on the new task, then a model without expire-span is brittle too because self-attention can assign 0 probability to important memories that are not used during training.\n\n**re: what value is shown in Table 2? It\u2019s inconsistent with Figure 7**\n\nThey are both bit-per-byte, but the Table 2 number is further optimized with a smaller learning. This finetuning of a final model (or using decaying learning rate schedule) is a common practice employed by the other baselines. \n\n**re: in figure 11, how is the memory computed?**\n\nIt is computed in this way: for each query, we count key and value vectors it attends to (i.e., by excluding the expired memories), then average over all queries. This measure directly correlates to the number of FLOPS and the memory usage.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3282/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3282/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Not All Memories are Created Equal: Learning to Expire", "authorids": ["~Sainbayar_Sukhbaatar1", "~Da_JU1", "spoff@fb.com", "~Stephen_Roller1", "~Arthur_Szlam1", "~Jason_E_Weston1", "~Angela_Fan2"], "authors": ["Sainbayar Sukhbaatar", "Da JU", "Spencer Poff", "Stephen Roller", "Arthur Szlam", "Jason E Weston", "Angela Fan"], "keywords": ["expire", "long attention", "memory", "transformers"], "abstract": "Attention mechanisms have shown promising results in sequence modeling tasks that require long-term memory. Recent work has investigated mechanisms to reduce the computational cost of preserving and storing the memories. However, not all content in the past is equally important to remember. We propose Expire-Span, a method that learns to retain the most important information and \nexpire the irrelevant information. This enables Transformers to scale to attend to tens of thousands of previous timesteps efficiently, as not all hidden states from previous timesteps are preserved. We demonstrate that Expire-Span can help models identify and retain critical information and show it can achieve state of the art results on long-context language modeling, reinforcement learning, and algorithmic tasks. Finally, we show that Expire-Span can scale to memories that are tens of thousands in size, which is helpful on incredibly long context tasks such as character-level PG-19 and a frame-by-frame moving objects task.\n", "one-sentence_summary": "Scale attention mechanisms in Transformers by learning what to forget from the past.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sukhbaatar|not_all_memories_are_created_equal_learning_to_expire", "pdf": "/pdf/9b9a886588f342f4c08355a2099454303aa6a5a6.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=cGL2wzQVP", "_bibtex": "@misc{\nsukhbaatar2021not,\ntitle={Not All Memories are Created Equal: Learning to Expire},\nauthor={Sainbayar Sukhbaatar and Da JU and Spencer Poff and Stephen Roller and Arthur Szlam and Jason E Weston and Angela Fan},\nyear={2021},\nurl={https://openreview.net/forum?id=ZVBtN6B_6i7}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ZVBtN6B_6i7", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3282/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3282/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3282/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3282/Authors|ICLR.cc/2021/Conference/Paper3282/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3282/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839151, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3282/-/Official_Comment"}}}, {"id": "xE2cHh7b-dN", "original": null, "number": 3, "cdate": 1606166727824, "ddate": null, "tcdate": 1606166727824, "tmdate": 1606166727824, "tddate": null, "forum": "ZVBtN6B_6i7", "replyto": "7jX2QcJU19", "invitation": "ICLR.cc/2021/Conference/Paper3282/-/Official_Comment", "content": {"title": "review response", "comment": "Thanks for your review and all of the detailed questions!\n\n**re: insufficient comparison with baselines**\n\nBased on your feedback, we have added several baselines for multiple tasks and have completed a detailed efficiency analysis for all baselines and Expire-Span. We quantify both time to process a batch and peak GPU memory usage to capture efficiency. These results are in the general response and added to our paper. \n\n**re: equation in section 4.1 requires a summation over i**\n\nThanks, fixed!\n\n**re: can you add sparse attention as a baseline?**\n\nOur goal is to improve the efficiency of long-term memory. In contrast, [1,2,3] focus on different aspects of sparse attention (improved gradient flow, more interpretable improved generalization). For example, [1] does top-k sparse attention, which still requires attention over all memories first. The same is true for sparsemax in [2] and [3], which is not more efficient than the full attention.  Please see our general response for some additional details. \n\n**re: efficiency analysis**\n\nIn the general response, we have added exactly your suggestion, to compare running time and physical memory usage. Thanks for the constructive feedback!\n\n**re: is the Transformer baseline a vanilla Transformer? How do you control the memory size of this baseline?**\n \nOur vanilla Transformer baseline is actually based on TransformerXL (caching mechanism, relative position embedding) --- we have now clarified this in the paper. The memory size is referred to as \u201cattention length\u201d in TransformerXL, which is a hyperparameter that can be adjusted. It simply restricts how many previous tokens can be attended at time t. \n\n**re: for the synthetic tasks, please include stronger baselines**\n\nWe have added baselines for many tasks, synthetic and not synthetic. We experimented with the Compressive Transformer and Adaptive-Span baselines to supplement our original Transformer-XL comparisons. Please see the general response, thanks!\n\n**re: performance gap in Table 2, can you make models larger?**\n\nOur goal is not to set a new SOTA, but rather propose an efficient model with a good performance. We added baselines similar to our model in size to Table 2, and our model outperforms them by a large margin. Training a large model is possible, but it requires more GPUs and careful tuning of regularization parameters, which takes a lot of resources. (note that the current SOTA, the Feedback Transformer, is actually a less efficient architecture that takes much longer to train)\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3282/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3282/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Not All Memories are Created Equal: Learning to Expire", "authorids": ["~Sainbayar_Sukhbaatar1", "~Da_JU1", "spoff@fb.com", "~Stephen_Roller1", "~Arthur_Szlam1", "~Jason_E_Weston1", "~Angela_Fan2"], "authors": ["Sainbayar Sukhbaatar", "Da JU", "Spencer Poff", "Stephen Roller", "Arthur Szlam", "Jason E Weston", "Angela Fan"], "keywords": ["expire", "long attention", "memory", "transformers"], "abstract": "Attention mechanisms have shown promising results in sequence modeling tasks that require long-term memory. Recent work has investigated mechanisms to reduce the computational cost of preserving and storing the memories. However, not all content in the past is equally important to remember. We propose Expire-Span, a method that learns to retain the most important information and \nexpire the irrelevant information. This enables Transformers to scale to attend to tens of thousands of previous timesteps efficiently, as not all hidden states from previous timesteps are preserved. We demonstrate that Expire-Span can help models identify and retain critical information and show it can achieve state of the art results on long-context language modeling, reinforcement learning, and algorithmic tasks. Finally, we show that Expire-Span can scale to memories that are tens of thousands in size, which is helpful on incredibly long context tasks such as character-level PG-19 and a frame-by-frame moving objects task.\n", "one-sentence_summary": "Scale attention mechanisms in Transformers by learning what to forget from the past.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sukhbaatar|not_all_memories_are_created_equal_learning_to_expire", "pdf": "/pdf/9b9a886588f342f4c08355a2099454303aa6a5a6.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=cGL2wzQVP", "_bibtex": "@misc{\nsukhbaatar2021not,\ntitle={Not All Memories are Created Equal: Learning to Expire},\nauthor={Sainbayar Sukhbaatar and Da JU and Spencer Poff and Stephen Roller and Arthur Szlam and Jason E Weston and Angela Fan},\nyear={2021},\nurl={https://openreview.net/forum?id=ZVBtN6B_6i7}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ZVBtN6B_6i7", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3282/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3282/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3282/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3282/Authors|ICLR.cc/2021/Conference/Paper3282/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3282/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839151, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3282/-/Official_Comment"}}}, {"id": "uPX8CM4JVas", "original": null, "number": 1, "cdate": 1603848798503, "ddate": null, "tcdate": 1603848798503, "tmdate": 1605024030116, "tddate": null, "forum": "ZVBtN6B_6i7", "replyto": "ZVBtN6B_6i7", "invitation": "ICLR.cc/2021/Conference/Paper3282/-/Official_Review", "content": {"title": "Overcoming the long-term memory bottleneck of transformers", "review": "**Summary**\nThe paper proposes a method for overcoming the long-term memory bottleneck of transformers. The idea is to assign a value (expire-span) to each formed memory, which indicates how long the memory should be stored and be available for the transformer to access it. The authors demonstrate the performance of their approach on a set of\nsynthetic and character-level language modeling benchmarks. \n\n**Significance**\nWhile the idea seems to be quite interesting and the presentation of the paper is clear and sound, I have the following concerns:\n\n- As the expire-span does not seem to be updated, the model must know how long to keep the memory when the memory is formed. Couldn't this potential cause issues when information arriving in the future would influence\nthe span of how long the memory should be kept? \n- From the author's descriptions, the method appears relatively brittle to hyperparameter choice. In particular, the method requires some sophisticated form of regularization for the performed benchmarks. Thus, raising my concerns about the stability and scalability of the approach. \n\nI would appreciate it if the authors could elaborate on my concerns.\n\n- The paper misses important related work in this domain. The paper Gers et al. \"Learning to forget continual prediction with LSTM\" already proposes a mechanism to remove memories that are not needed anymore. Moreover, the proposed approach is adaptive as for each token, the network decides if it should clear some of its memory. \n\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3282/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3282/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Not All Memories are Created Equal: Learning to Expire", "authorids": ["~Sainbayar_Sukhbaatar1", "~Da_JU1", "spoff@fb.com", "~Stephen_Roller1", "~Arthur_Szlam1", "~Jason_E_Weston1", "~Angela_Fan2"], "authors": ["Sainbayar Sukhbaatar", "Da JU", "Spencer Poff", "Stephen Roller", "Arthur Szlam", "Jason E Weston", "Angela Fan"], "keywords": ["expire", "long attention", "memory", "transformers"], "abstract": "Attention mechanisms have shown promising results in sequence modeling tasks that require long-term memory. Recent work has investigated mechanisms to reduce the computational cost of preserving and storing the memories. However, not all content in the past is equally important to remember. We propose Expire-Span, a method that learns to retain the most important information and \nexpire the irrelevant information. This enables Transformers to scale to attend to tens of thousands of previous timesteps efficiently, as not all hidden states from previous timesteps are preserved. We demonstrate that Expire-Span can help models identify and retain critical information and show it can achieve state of the art results on long-context language modeling, reinforcement learning, and algorithmic tasks. Finally, we show that Expire-Span can scale to memories that are tens of thousands in size, which is helpful on incredibly long context tasks such as character-level PG-19 and a frame-by-frame moving objects task.\n", "one-sentence_summary": "Scale attention mechanisms in Transformers by learning what to forget from the past.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sukhbaatar|not_all_memories_are_created_equal_learning_to_expire", "pdf": "/pdf/9b9a886588f342f4c08355a2099454303aa6a5a6.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=cGL2wzQVP", "_bibtex": "@misc{\nsukhbaatar2021not,\ntitle={Not All Memories are Created Equal: Learning to Expire},\nauthor={Sainbayar Sukhbaatar and Da JU and Spencer Poff and Stephen Roller and Arthur Szlam and Jason E Weston and Angela Fan},\nyear={2021},\nurl={https://openreview.net/forum?id=ZVBtN6B_6i7}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "ZVBtN6B_6i7", "replyto": "ZVBtN6B_6i7", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3282/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538078632, "tmdate": 1606915779958, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3282/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3282/-/Official_Review"}}}, {"id": "7jX2QcJU19", "original": null, "number": 3, "cdate": 1603881108406, "ddate": null, "tcdate": 1603881108406, "tmdate": 1605024029984, "tddate": null, "forum": "ZVBtN6B_6i7", "replyto": "ZVBtN6B_6i7", "invitation": "ICLR.cc/2021/Conference/Paper3282/-/Official_Review", "content": {"title": "A solid improvement of Transformer's attention mechanism, yet the baselines and empirical results are not strong", "review": "To help Transformer learn long sequence efficiently, the paper performs attention on selective timesteps that have high expire-span scores. For each timestep, the expire-span score is computed by mapping the corresponding hidden feature to a number, which is learnt during training. Soft masking is applied to make the learning differentiable. An additional loss is introduced to reduce the average span, making the attention sparse. The proposed attention is integrated into each layer of Transformer and tested on several synthetic tasks and two language modelling datasets, yielding promising results. \n\nPros:\n- The proposed solution (computing expire score and minimizing the average span) is elegant and seems novel within Transformer context\n- The properties and behaviours of the method are well illustrated with detailed analysis and visualization\n- Diverse experiments are conducted\n\nCons:\n- Insufficient comparison with other Transformer-based baselines \n- The results on real data are weak\n\nDetail comments and questions\n\n- Sec 4.1, the equation computing o_t should be a summation over i\n- Before Transformer, sparse attention has been studied deeply in the literature. It may be beneficial to review some works (e.g., [1,2,3]) and try to integrate them into Transformer as additional baselines to make the experiment stronger. \n- No experimental result demonstrates that the method can reduce computation complexity. Please consider including a comparison of running time or physical memory usages between your method and other Transformers\n- It is unclear what are the baselines mentioned in the experiments. Are they vanilla Transformers? How did the authors control the memory size of the baseline as in Fig.3, 4 and 7?\n- For some synthetic tasks, it is better to include stronger baselines [4,5] to show the advantage of the proposed method over other variants of Transformer\n- In Table 2, the performance gap is significant. Is it possible to improve your performance with more parameters? \n\n[1] Ke, Nan Rosemary, Anirudh Goyal ALIAS PARTH GOYAL, Olexa Bilaniuk, Jonathan Binas, Michael C. Mozer, Chris Pal, and Yoshua Bengio. \"Sparse attentive backtracking: Temporal credit assignment through reminding.\" In Advances in neural information processing systems, pp. 7640-7651. 2018. \n\n[2] Martins, Andre, and Ramon Astudillo. \"From softmax to sparsemax: A sparse model of attention and multi-label classification.\" In International Conference on Machine Learning, pp. 1614-1623. 2016. \n\n[3] Niculae, Vlad, and Mathieu Blondel. \"A regularized framework for sparse and structured neural attention.\" In Advances in neural information processing systems, pp. 3338-3348. 2017. \n\n[4] Gonc\u00b8alo M Correia, Vlad Niculae, and Andre FT Martins. Adaptively sparse transformers. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 2174\u20132184, 2019.  \n\n[5] Sainbayar Sukhbaatar, Edouard Grave, Piotr Bojanowski, and Armand Joulin. Adaptive attention \u00b4 span in transformers. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 331\u2013335, 2019a. \n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3282/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3282/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Not All Memories are Created Equal: Learning to Expire", "authorids": ["~Sainbayar_Sukhbaatar1", "~Da_JU1", "spoff@fb.com", "~Stephen_Roller1", "~Arthur_Szlam1", "~Jason_E_Weston1", "~Angela_Fan2"], "authors": ["Sainbayar Sukhbaatar", "Da JU", "Spencer Poff", "Stephen Roller", "Arthur Szlam", "Jason E Weston", "Angela Fan"], "keywords": ["expire", "long attention", "memory", "transformers"], "abstract": "Attention mechanisms have shown promising results in sequence modeling tasks that require long-term memory. Recent work has investigated mechanisms to reduce the computational cost of preserving and storing the memories. However, not all content in the past is equally important to remember. We propose Expire-Span, a method that learns to retain the most important information and \nexpire the irrelevant information. This enables Transformers to scale to attend to tens of thousands of previous timesteps efficiently, as not all hidden states from previous timesteps are preserved. We demonstrate that Expire-Span can help models identify and retain critical information and show it can achieve state of the art results on long-context language modeling, reinforcement learning, and algorithmic tasks. Finally, we show that Expire-Span can scale to memories that are tens of thousands in size, which is helpful on incredibly long context tasks such as character-level PG-19 and a frame-by-frame moving objects task.\n", "one-sentence_summary": "Scale attention mechanisms in Transformers by learning what to forget from the past.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sukhbaatar|not_all_memories_are_created_equal_learning_to_expire", "pdf": "/pdf/9b9a886588f342f4c08355a2099454303aa6a5a6.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=cGL2wzQVP", "_bibtex": "@misc{\nsukhbaatar2021not,\ntitle={Not All Memories are Created Equal: Learning to Expire},\nauthor={Sainbayar Sukhbaatar and Da JU and Spencer Poff and Stephen Roller and Arthur Szlam and Jason E Weston and Angela Fan},\nyear={2021},\nurl={https://openreview.net/forum?id=ZVBtN6B_6i7}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "ZVBtN6B_6i7", "replyto": "ZVBtN6B_6i7", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3282/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538078632, "tmdate": 1606915779958, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3282/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3282/-/Official_Review"}}}], "count": 10}