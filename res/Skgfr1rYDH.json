{"notes": [{"id": "Skgfr1rYDH", "original": "ByeyGWTOwS", "number": 1683, "cdate": 1569439546066, "ddate": null, "tcdate": 1569439546066, "tmdate": 1577168279401, "tddate": null, "forum": "Skgfr1rYDH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["abe@sourceress.co", "christina@sourceress.co", "josh@sourceress.co"], "title": "SoftAdam: Unifying SGD and Adam for better stochastic gradient descent", "authors": ["Abraham J. Fetterman", "Christina H. Kim", "Joshua Albrecht"], "pdf": "/pdf/d00a4fb387d1c792e50a9296ec3fb79fb83c40f1.pdf", "TL;DR": "An algorithm for unifying SGD and Adam and empirical study of its performance", "abstract": "Abstract Stochastic gradient descent (SGD) and Adam are commonly used to optimize deep neural networks, but choosing one usually means making tradeoffs between speed, accuracy and stability. Here we present an intuition for why the tradeoffs exist as well as a method for unifying the two in a continuous way. This makes it possible to control the way models are trained in much greater detail. We show that for default parameters, the new algorithm equals or outperforms SGD and Adam across a range of models for image classification tasks and outperforms SGD for language modeling tasks.", "keywords": ["Optimization", "SGD", "Adam", "Generalization", "Deep Learning"], "paperhash": "fetterman|softadam_unifying_sgd_and_adam_for_better_stochastic_gradient_descent", "original_pdf": "/attachment/b2c2948852244e41b341b9372ea85c9c8c2ca82e.pdf", "_bibtex": "@misc{\nfetterman2020softadam,\ntitle={SoftAdam: Unifying {\\{}SGD{\\}} and Adam for better stochastic gradient descent},\nauthor={Abraham J. Fetterman and Christina H. Kim and Joshua Albrecht},\nyear={2020},\nurl={https://openreview.net/forum?id=Skgfr1rYDH}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "BDotz4oli", "original": null, "number": 1, "cdate": 1576798729766, "ddate": null, "tcdate": 1576798729766, "tmdate": 1576800906743, "tddate": null, "forum": "Skgfr1rYDH", "replyto": "Skgfr1rYDH", "invitation": "ICLR.cc/2020/Conference/Paper1683/-/Decision", "content": {"decision": "Reject", "comment": "The reviewers all agreed that the proposed modification was minor. I encourage the authors to pursue in this direction, as they mentioned in their rebuttal, before resubmitting to another conference.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["abe@sourceress.co", "christina@sourceress.co", "josh@sourceress.co"], "title": "SoftAdam: Unifying SGD and Adam for better stochastic gradient descent", "authors": ["Abraham J. Fetterman", "Christina H. Kim", "Joshua Albrecht"], "pdf": "/pdf/d00a4fb387d1c792e50a9296ec3fb79fb83c40f1.pdf", "TL;DR": "An algorithm for unifying SGD and Adam and empirical study of its performance", "abstract": "Abstract Stochastic gradient descent (SGD) and Adam are commonly used to optimize deep neural networks, but choosing one usually means making tradeoffs between speed, accuracy and stability. Here we present an intuition for why the tradeoffs exist as well as a method for unifying the two in a continuous way. This makes it possible to control the way models are trained in much greater detail. We show that for default parameters, the new algorithm equals or outperforms SGD and Adam across a range of models for image classification tasks and outperforms SGD for language modeling tasks.", "keywords": ["Optimization", "SGD", "Adam", "Generalization", "Deep Learning"], "paperhash": "fetterman|softadam_unifying_sgd_and_adam_for_better_stochastic_gradient_descent", "original_pdf": "/attachment/b2c2948852244e41b341b9372ea85c9c8c2ca82e.pdf", "_bibtex": "@misc{\nfetterman2020softadam,\ntitle={SoftAdam: Unifying {\\{}SGD{\\}} and Adam for better stochastic gradient descent},\nauthor={Abraham J. Fetterman and Christina H. Kim and Joshua Albrecht},\nyear={2020},\nurl={https://openreview.net/forum?id=Skgfr1rYDH}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "Skgfr1rYDH", "replyto": "Skgfr1rYDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795712862, "tmdate": 1576800262338, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1683/-/Decision"}}}, {"id": "rkl8RuKjir", "original": null, "number": 4, "cdate": 1573783757682, "ddate": null, "tcdate": 1573783757682, "tmdate": 1573783757682, "tddate": null, "forum": "Skgfr1rYDH", "replyto": "S1ghLD70FH", "invitation": "ICLR.cc/2020/Conference/Paper1683/-/Official_Comment", "content": {"title": "Response to review #3", "comment": "Thank you very much for the detailed feedback. Overall, we want to note that the algorithm significantly outperforms Adam and even outperforms SGD in computer vision tasks. The changes here have a significant effect on the generalization performance and constitute novel research.\n\nBelow are responses to your points:\n\n- \"There is many modifications proposed, but most are secondary corrections for stability, such as the warm-up schedule with the redefinition of beta_2. These modifications could as well be incorporated in Adam without the core modification that is the smoothing presented in section 3. These additional modifications also make it difficult to measure the importance of the core contribution. Without getting rid of them, an ablation study on toy problems (even synthetic data) would be necessary for a better understanding.\"\n- \"In section 3.1, the temporal definition of beta_2t is integrating a warm-up. While the reason for doing so is supported in introduction of section 3, the effect of this modification should be weighted against no warm-up, and also compared with its effect on Adam.\"\n\nThe warmup schedule itself has been studied fairly extensively, especially recently by Liu (2019) and Ma (2019). The modification to the beta_2 for debiasing does not significantly impact the performance and is only used to make the algorithm better align with the theoretical warmup schedule\u2014using the traditional Adam m_t debiasing does not have any impact on the results. We added a comment to this effect.\n\nWe also updated the results to specifically use AdamW, which is more comparable to our algorithm.\n\n- \"There is an error in algorithm 1. The last element of the last line (Perform the update) should be \\alpha (1 - \\eta) m_t/d_t. The code in Appendix corroborates this correction. Minor related note, the use of d_t to define the denominator of what d_i represents in section 3 is very confusing. I would suggest to use the ratio notation of d_i from the equations in the algorithm for coherence.\"\n\nThis is correct, thank you for the suggestion. We have adjusted the notation based on your feedback.\n\n- \"If we get pass the warm-up scheduling, by massaging the equation we get that the algorithm is different from Adam on 2 points, 1) the bias are not corrected and 2) the denominator sqrt(v) + epsilon is replaced by sqrt(v) + sqrt(mean(v) + epsilon^2). I have difficulty convincing myself that smoothing by the average is solving the issues raised in the paper and there is no experiments to study its effect directly.\"\n\n(1) the second order bias is corrected, just in a different way. As mentioned before, there is no practical difference between the debiasing methods. \n\n(2) In addition to changing the denominator to sqrt(v) + eta * sqrt(mean(v) + epsilon^2), there is also a scaling of the learning by 1 + eta * sqrt(mean(v)), which changes the implicit learning schedule created by Adam to be like SGD. This is what allows the Adam-like limit of eta\u2192 infinity and the SGDM limit of eta\u21920. We intend future studies that will dis-entangle these two effects (learning rate schedule versus smoothing direction). The purpose of this paper is to share the algorithm that allows such a study in the first place, which is unique in this regard.\n\n- \"The experiments are on 3 datasets, but only the computer vision ones are run on multiple architectures...\"\n\nWe regret we cannot train on every architecture, but we focused on representative samples for different problems. We have added a comment that the models have different initializations and the hyper-parameters are tuned separately for each algorithm. More runs have been generated to provide standard deviations for the mentioned parameters.\n\nMinor comments have been addressed in the updated draft. Note both z and \\xi_i are vectors, so the notation is already correct."}, "signatures": ["ICLR.cc/2020/Conference/Paper1683/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1683/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["abe@sourceress.co", "christina@sourceress.co", "josh@sourceress.co"], "title": "SoftAdam: Unifying SGD and Adam for better stochastic gradient descent", "authors": ["Abraham J. Fetterman", "Christina H. Kim", "Joshua Albrecht"], "pdf": "/pdf/d00a4fb387d1c792e50a9296ec3fb79fb83c40f1.pdf", "TL;DR": "An algorithm for unifying SGD and Adam and empirical study of its performance", "abstract": "Abstract Stochastic gradient descent (SGD) and Adam are commonly used to optimize deep neural networks, but choosing one usually means making tradeoffs between speed, accuracy and stability. Here we present an intuition for why the tradeoffs exist as well as a method for unifying the two in a continuous way. This makes it possible to control the way models are trained in much greater detail. We show that for default parameters, the new algorithm equals or outperforms SGD and Adam across a range of models for image classification tasks and outperforms SGD for language modeling tasks.", "keywords": ["Optimization", "SGD", "Adam", "Generalization", "Deep Learning"], "paperhash": "fetterman|softadam_unifying_sgd_and_adam_for_better_stochastic_gradient_descent", "original_pdf": "/attachment/b2c2948852244e41b341b9372ea85c9c8c2ca82e.pdf", "_bibtex": "@misc{\nfetterman2020softadam,\ntitle={SoftAdam: Unifying {\\{}SGD{\\}} and Adam for better stochastic gradient descent},\nauthor={Abraham J. Fetterman and Christina H. Kim and Joshua Albrecht},\nyear={2020},\nurl={https://openreview.net/forum?id=Skgfr1rYDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Skgfr1rYDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1683/Authors", "ICLR.cc/2020/Conference/Paper1683/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1683/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1683/Reviewers", "ICLR.cc/2020/Conference/Paper1683/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1683/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1683/Authors|ICLR.cc/2020/Conference/Paper1683/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504152407, "tmdate": 1576860535724, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1683/Authors", "ICLR.cc/2020/Conference/Paper1683/Reviewers", "ICLR.cc/2020/Conference/Paper1683/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1683/-/Official_Comment"}}}, {"id": "HJlSAwFoir", "original": null, "number": 3, "cdate": 1573783500529, "ddate": null, "tcdate": 1573783500529, "tmdate": 1573783500529, "tddate": null, "forum": "Skgfr1rYDH", "replyto": "ByeabQxpFS", "invitation": "ICLR.cc/2020/Conference/Paper1683/-/Official_Comment", "content": {"title": "Response to review #2", "comment": "Thank you for your feedback. Here is a response to your points:\n\n- The novelty of this work is limited....\n\nThank you for the feedback. We intend future studies that will provide more explanation of why softAdam outperforms other existing algorithms. This is not a trivial problem to answer, as we do not know why SGD and Adam generalize well in the first place. Although theoretical guarantees are not proven in this paper, it can be seen by inspection of the algorithm that the step size is always smaller than the SGD step size, and so the SGD convergence guarantees will be held.\n\n- The theoretical analysis about existing adaptive methods provides nothing new....\n\nThe theoretical analysis of the quadratic model here provides intuition for the algorithm\u2019s behavior on convex optimization. An additional study of adaptive methods for non-convex optimization is well described in the reference you provided, \u201cOn the convergence of stochastic gradient descent with adaptive stepsizes.\u201d\n\n- The settings of experiments are limited....\n\nCIFAR has provided an adequate testing ground for optimization algorithms for many years. We wanted to have the most comparable results with those other optimization papers.\n\n- This paper lacks some references in this area. \n\nThank you for these references, we have included them."}, "signatures": ["ICLR.cc/2020/Conference/Paper1683/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1683/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["abe@sourceress.co", "christina@sourceress.co", "josh@sourceress.co"], "title": "SoftAdam: Unifying SGD and Adam for better stochastic gradient descent", "authors": ["Abraham J. Fetterman", "Christina H. Kim", "Joshua Albrecht"], "pdf": "/pdf/d00a4fb387d1c792e50a9296ec3fb79fb83c40f1.pdf", "TL;DR": "An algorithm for unifying SGD and Adam and empirical study of its performance", "abstract": "Abstract Stochastic gradient descent (SGD) and Adam are commonly used to optimize deep neural networks, but choosing one usually means making tradeoffs between speed, accuracy and stability. Here we present an intuition for why the tradeoffs exist as well as a method for unifying the two in a continuous way. This makes it possible to control the way models are trained in much greater detail. We show that for default parameters, the new algorithm equals or outperforms SGD and Adam across a range of models for image classification tasks and outperforms SGD for language modeling tasks.", "keywords": ["Optimization", "SGD", "Adam", "Generalization", "Deep Learning"], "paperhash": "fetterman|softadam_unifying_sgd_and_adam_for_better_stochastic_gradient_descent", "original_pdf": "/attachment/b2c2948852244e41b341b9372ea85c9c8c2ca82e.pdf", "_bibtex": "@misc{\nfetterman2020softadam,\ntitle={SoftAdam: Unifying {\\{}SGD{\\}} and Adam for better stochastic gradient descent},\nauthor={Abraham J. Fetterman and Christina H. Kim and Joshua Albrecht},\nyear={2020},\nurl={https://openreview.net/forum?id=Skgfr1rYDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Skgfr1rYDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1683/Authors", "ICLR.cc/2020/Conference/Paper1683/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1683/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1683/Reviewers", "ICLR.cc/2020/Conference/Paper1683/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1683/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1683/Authors|ICLR.cc/2020/Conference/Paper1683/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504152407, "tmdate": 1576860535724, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1683/Authors", "ICLR.cc/2020/Conference/Paper1683/Reviewers", "ICLR.cc/2020/Conference/Paper1683/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1683/-/Official_Comment"}}}, {"id": "ByxbUvYoiS", "original": null, "number": 2, "cdate": 1573783369097, "ddate": null, "tcdate": 1573783369097, "tmdate": 1573783369097, "tddate": null, "forum": "Skgfr1rYDH", "replyto": "rkx2eAItFH", "invitation": "ICLR.cc/2020/Conference/Paper1683/-/Official_Comment", "content": {"title": "Response to review #1", "comment": "Thank you for the feedback. Here is a response to your points:\n\n1. Section 2 helps to understand the intuition for the paper by defining SGD and Adam in a consistent way. \n\n2. Thanks for the feedback. We have added some clarifications in the paper. z is the input to the learning function. n_t and n_\\infty are now defined again in section 3. We removed the equations adding weight decay and nesterov momentum as these were unnecessarily confusing.\n\n3. The optimal learning rate (eq 1) must take into account both min and max eigenvalues or else one will dominate the time to convergence.  The modification to Adam allows the algorithm to significantly outperform Adam and SGD in computer vision tasks. \n\n4. The colors were not switched, but the legend order was not consistent. This has been fixed, and the confidence range has been added to the results where available."}, "signatures": ["ICLR.cc/2020/Conference/Paper1683/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1683/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["abe@sourceress.co", "christina@sourceress.co", "josh@sourceress.co"], "title": "SoftAdam: Unifying SGD and Adam for better stochastic gradient descent", "authors": ["Abraham J. Fetterman", "Christina H. Kim", "Joshua Albrecht"], "pdf": "/pdf/d00a4fb387d1c792e50a9296ec3fb79fb83c40f1.pdf", "TL;DR": "An algorithm for unifying SGD and Adam and empirical study of its performance", "abstract": "Abstract Stochastic gradient descent (SGD) and Adam are commonly used to optimize deep neural networks, but choosing one usually means making tradeoffs between speed, accuracy and stability. Here we present an intuition for why the tradeoffs exist as well as a method for unifying the two in a continuous way. This makes it possible to control the way models are trained in much greater detail. We show that for default parameters, the new algorithm equals or outperforms SGD and Adam across a range of models for image classification tasks and outperforms SGD for language modeling tasks.", "keywords": ["Optimization", "SGD", "Adam", "Generalization", "Deep Learning"], "paperhash": "fetterman|softadam_unifying_sgd_and_adam_for_better_stochastic_gradient_descent", "original_pdf": "/attachment/b2c2948852244e41b341b9372ea85c9c8c2ca82e.pdf", "_bibtex": "@misc{\nfetterman2020softadam,\ntitle={SoftAdam: Unifying {\\{}SGD{\\}} and Adam for better stochastic gradient descent},\nauthor={Abraham J. Fetterman and Christina H. Kim and Joshua Albrecht},\nyear={2020},\nurl={https://openreview.net/forum?id=Skgfr1rYDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Skgfr1rYDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1683/Authors", "ICLR.cc/2020/Conference/Paper1683/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1683/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1683/Reviewers", "ICLR.cc/2020/Conference/Paper1683/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1683/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1683/Authors|ICLR.cc/2020/Conference/Paper1683/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504152407, "tmdate": 1576860535724, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1683/Authors", "ICLR.cc/2020/Conference/Paper1683/Reviewers", "ICLR.cc/2020/Conference/Paper1683/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1683/-/Official_Comment"}}}, {"id": "rkx2eAItFH", "original": null, "number": 1, "cdate": 1571544564076, "ddate": null, "tcdate": 1571544564076, "tmdate": 1572972436847, "tddate": null, "forum": "Skgfr1rYDH", "replyto": "Skgfr1rYDH", "invitation": "ICLR.cc/2020/Conference/Paper1683/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The paper tends to explain how the tradeoffs between convergence speed and convergence performance are made by different optimization methods. Moreover, the paper modifies Adam-like updating rules and proposes a novel optimization methods, SoftAdam. Finally, the paper performs numerical experiments on traditional image classification tasks as well as language modeling tasks.\n\nPros\nThe paper involves the language modeling tasks in empirical results besides traditional image classification tasks, which helps to explain the convergence performance of optimization methods in a wider range of applications.\n\nCons\n1. The writing of this paper is not well organised. Section 1 lacks detailed description of the main idea and the proposed optimization methods, which actually confuses the reader. Section 2 describes too much details of SGD and Adam, and lacks a clear \"intuition\" which readers exactly expect.\n2. The notation in the paper is little confusing. In the update rule of z_{t+1} in Section 2, what is meaning of z? In Section 3, n_t and n_\\infty are used before a proper defination, and what is relationship between \\alpha and \\alpha_t in the implementation of the SoftAdam?\n3. The motivation of the proposed method is weak. Such a weak motivation is mainly because of the insufficient \"intuition\" in Section 2. The author mentions \"the Hessians in deep learning problems are not diagonal\", but does not provide further explanation on why more importance should be lay on serving both max and min eigenvalues.\n4. There are also several minor problems on the numerical results. Firstly, why the colors of \"softAdam\" and \"sgd\" are switched several times in Figure 1? Secondly, the figural result in Figure 1 and he numerical results on language processing models both lack a display of the confidence range.\n     "}, "signatures": ["ICLR.cc/2020/Conference/Paper1683/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1683/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["abe@sourceress.co", "christina@sourceress.co", "josh@sourceress.co"], "title": "SoftAdam: Unifying SGD and Adam for better stochastic gradient descent", "authors": ["Abraham J. Fetterman", "Christina H. Kim", "Joshua Albrecht"], "pdf": "/pdf/d00a4fb387d1c792e50a9296ec3fb79fb83c40f1.pdf", "TL;DR": "An algorithm for unifying SGD and Adam and empirical study of its performance", "abstract": "Abstract Stochastic gradient descent (SGD) and Adam are commonly used to optimize deep neural networks, but choosing one usually means making tradeoffs between speed, accuracy and stability. Here we present an intuition for why the tradeoffs exist as well as a method for unifying the two in a continuous way. This makes it possible to control the way models are trained in much greater detail. We show that for default parameters, the new algorithm equals or outperforms SGD and Adam across a range of models for image classification tasks and outperforms SGD for language modeling tasks.", "keywords": ["Optimization", "SGD", "Adam", "Generalization", "Deep Learning"], "paperhash": "fetterman|softadam_unifying_sgd_and_adam_for_better_stochastic_gradient_descent", "original_pdf": "/attachment/b2c2948852244e41b341b9372ea85c9c8c2ca82e.pdf", "_bibtex": "@misc{\nfetterman2020softadam,\ntitle={SoftAdam: Unifying {\\{}SGD{\\}} and Adam for better stochastic gradient descent},\nauthor={Abraham J. Fetterman and Christina H. Kim and Joshua Albrecht},\nyear={2020},\nurl={https://openreview.net/forum?id=Skgfr1rYDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Skgfr1rYDH", "replyto": "Skgfr1rYDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1683/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1683/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575662142398, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1683/Reviewers"], "noninvitees": [], "tcdate": 1570237733815, "tmdate": 1575662142415, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1683/-/Official_Review"}}}, {"id": "ByeabQxpFS", "original": null, "number": 2, "cdate": 1571779333506, "ddate": null, "tcdate": 1571779333506, "tmdate": 1572972436813, "tddate": null, "forum": "Skgfr1rYDH", "replyto": "Skgfr1rYDH", "invitation": "ICLR.cc/2020/Conference/Paper1683/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Summary:\n\nThis work proposed a new algorithm called softAdam to unify the advantages of both Adam and SGD. The authors also showed experiments to backup their theoretical results. \n\nPros:\n\nThe authors provided analysis of different algorithms including Adam and SGD on simple quadratic function, then proposed a new algorithm called softAdam which outperforms both Adam and SGD. Experiment results backup their theory. \n\nCons:\n\n- The novelty of this work is limited. The main contribution of this work is to provide a new adaptive gradient method called softAdam, which changes the update rules for some parameters including \\beta. However, neither intuition or theoretical guarantees are provided in this paper. I recommend the authors to add some explanation about why softAdam outperforms other existing algorithms. Besides, the difference between softAdam and original Adam method is little. \n- The theoretical analysis about existing adaptive methods provides nothing new. The authors showed some analysis on quadratic model, which is a very simple model and hence can not reflect the true model we face in the practice. I suggest the authors provide some analysis on more general model, including convex functions and non-convex functions. \n- The settings of experiments are limited. The authors should at least compare softAdam with other baseline algorithms on some modern deep learning tasks including Imagenet.  \n\nMinor comments:\n\n- Page 4, section 3, 'this understanding of has'... lacks object.\n- This paper lacks some references in this area. \n\nJ. Chen and Q. Gu. Closing the generalization gap of adaptive gradient methods in training\ndeep neural networks. arXiv preprint arXiv:1806.06763, 2018.\nWard, R., Wu, X. and Bottou, L. (2018). Adagrad stepsizes: Sharp convergence over nonconvex\nlandscapes, from any initialization. arXiv preprint arXiv:1806.01811 .\nLi, X. and Orabona, F. (2018). On the convergence of stochastic gradient descent with adaptive\nstepsizes. arXiv preprint arXiv:1805.08114 .\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1683/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1683/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["abe@sourceress.co", "christina@sourceress.co", "josh@sourceress.co"], "title": "SoftAdam: Unifying SGD and Adam for better stochastic gradient descent", "authors": ["Abraham J. Fetterman", "Christina H. Kim", "Joshua Albrecht"], "pdf": "/pdf/d00a4fb387d1c792e50a9296ec3fb79fb83c40f1.pdf", "TL;DR": "An algorithm for unifying SGD and Adam and empirical study of its performance", "abstract": "Abstract Stochastic gradient descent (SGD) and Adam are commonly used to optimize deep neural networks, but choosing one usually means making tradeoffs between speed, accuracy and stability. Here we present an intuition for why the tradeoffs exist as well as a method for unifying the two in a continuous way. This makes it possible to control the way models are trained in much greater detail. We show that for default parameters, the new algorithm equals or outperforms SGD and Adam across a range of models for image classification tasks and outperforms SGD for language modeling tasks.", "keywords": ["Optimization", "SGD", "Adam", "Generalization", "Deep Learning"], "paperhash": "fetterman|softadam_unifying_sgd_and_adam_for_better_stochastic_gradient_descent", "original_pdf": "/attachment/b2c2948852244e41b341b9372ea85c9c8c2ca82e.pdf", "_bibtex": "@misc{\nfetterman2020softadam,\ntitle={SoftAdam: Unifying {\\{}SGD{\\}} and Adam for better stochastic gradient descent},\nauthor={Abraham J. Fetterman and Christina H. Kim and Joshua Albrecht},\nyear={2020},\nurl={https://openreview.net/forum?id=Skgfr1rYDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Skgfr1rYDH", "replyto": "Skgfr1rYDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1683/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1683/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575662142398, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1683/Reviewers"], "noninvitees": [], "tcdate": 1570237733815, "tmdate": 1575662142415, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1683/-/Official_Review"}}}, {"id": "S1ghLD70FH", "original": null, "number": 3, "cdate": 1571858259541, "ddate": null, "tcdate": 1571858259541, "tmdate": 1572972436770, "tddate": null, "forum": "Skgfr1rYDH", "replyto": "Skgfr1rYDH", "invitation": "ICLR.cc/2020/Conference/Paper1683/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a new algorithm which brings closer SGD and Adam while also incorporating new corrections to improve behavior of the optimizer in contexts where there is very small or very large eigen values.\n\nDecision\n\nI vote for weak rejection because the core modification proposed to Adam is minor and is mostly supported by intuition and preliminary experiments.\n\nJustification\n\nThere is many modifications proposed, but most are secondary corrections for stability, such as the warm-up schedule with the redefinition of beta_2. These modifications could as well be incorporated in Adam without the core modification that is the smoothing presented in section 3. These additional modifications also make it difficult to measure the importance of the core contribution. Without getting rid of them, an ablation study on toy problems (even synthetic data) would be necessary for a better understanding.\n\nIn section 3.1, the temporal definition of beta_2t is integrating a warm-up. While the reason for doing so is supported in introduction of section 3, the effect of this modification should be weighted against no warm-up, and also compared with its effect on Adam.\n\nThere is an error in algorithm 1. The last element of the last line (Perform the update) should be \\alpha (1 - \\eta) m_t/d_t. The code in Appendix corroborates this correction. Minor related note, the use of d_t to define the denominator of what d_i represents in section 3 is very confusing. I would suggest to use the ratio notation of d_i from the equations in the algorithm for coherence.\n\nIf we get pass the warm-up scheduling, by massaging the equation we get that the algorithm is different from Adam on 2 points, 1) the bias are not corrected and 2) the denominator sqrt(v) + epsilon is replaced by sqrt(v) + sqrt(mean(v) + epsilon^2). I have difficulty convincing myself that smoothing by the average is solving the issues raised in the paper and there is no experiments to study its effect directly.\n\nThe experiments are on 3 datasets, but only the computer vision ones are run on multiple architectures. Caption of figure 1 explains that each model is trained 3 times, but the source of variation between each run is not described. Are the models initialized differently? In any case, there is an overlap for 3 of the 5 models between SGD and SoftAdam which makes the comparison rather unconvincing. There is no standard deviation for Adam, and none on Penn Treebank dataset and IWSLT. For a better comparison, all hyper-parameters of the algorithms should be optimized for each run. I understand that SoftAdam is meant to be close to both SGD and Adam, but using the same hyper-parameters may induces misleading results by favoring some (model, optimizer) combination nevertheless.\n\nMinor comments\n\nIn section 2, second paragraph, the term 'mini-batch' should be used instead of 'batch'.\nIn section 2, last sentence, the betas should have no t.\nIn section 2.1, fourth equation (unnumbered), the eigen vector xi_i is presented as a vector and then used as a scalar. Notation should be uniformed.\nIn Section 2.1 around equation (2), the use of i and j is incoherent.\nIn Section 3:\n- Overall, this understanding *of* has\n- we consider the *an* update"}, "signatures": ["ICLR.cc/2020/Conference/Paper1683/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1683/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["abe@sourceress.co", "christina@sourceress.co", "josh@sourceress.co"], "title": "SoftAdam: Unifying SGD and Adam for better stochastic gradient descent", "authors": ["Abraham J. Fetterman", "Christina H. Kim", "Joshua Albrecht"], "pdf": "/pdf/d00a4fb387d1c792e50a9296ec3fb79fb83c40f1.pdf", "TL;DR": "An algorithm for unifying SGD and Adam and empirical study of its performance", "abstract": "Abstract Stochastic gradient descent (SGD) and Adam are commonly used to optimize deep neural networks, but choosing one usually means making tradeoffs between speed, accuracy and stability. Here we present an intuition for why the tradeoffs exist as well as a method for unifying the two in a continuous way. This makes it possible to control the way models are trained in much greater detail. We show that for default parameters, the new algorithm equals or outperforms SGD and Adam across a range of models for image classification tasks and outperforms SGD for language modeling tasks.", "keywords": ["Optimization", "SGD", "Adam", "Generalization", "Deep Learning"], "paperhash": "fetterman|softadam_unifying_sgd_and_adam_for_better_stochastic_gradient_descent", "original_pdf": "/attachment/b2c2948852244e41b341b9372ea85c9c8c2ca82e.pdf", "_bibtex": "@misc{\nfetterman2020softadam,\ntitle={SoftAdam: Unifying {\\{}SGD{\\}} and Adam for better stochastic gradient descent},\nauthor={Abraham J. Fetterman and Christina H. Kim and Joshua Albrecht},\nyear={2020},\nurl={https://openreview.net/forum?id=Skgfr1rYDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Skgfr1rYDH", "replyto": "Skgfr1rYDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1683/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1683/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575662142398, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1683/Reviewers"], "noninvitees": [], "tcdate": 1570237733815, "tmdate": 1575662142415, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1683/-/Official_Review"}}}, {"id": "Bylf0S_7KB", "original": null, "number": 1, "cdate": 1571157449710, "ddate": null, "tcdate": 1571157449710, "tmdate": 1571157449710, "tddate": null, "forum": "Skgfr1rYDH", "replyto": "Skek3-qxYB", "invitation": "ICLR.cc/2020/Conference/Paper1683/-/Official_Comment", "content": {"comment": "$n_t$ represents the effective number of elements in the average for $v_t$. Using an exponential weighted average as is used in Adam, for large t, $n_t \\approx n_\\infty \\approx 2/(1-\\beta_2)$. However, if $t \\ll n_\\infty$, $n_t \\approx t$.", "title": "Definition of n_t and n_\\infty"}, "signatures": ["ICLR.cc/2020/Conference/Paper1683/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1683/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["abe@sourceress.co", "christina@sourceress.co", "josh@sourceress.co"], "title": "SoftAdam: Unifying SGD and Adam for better stochastic gradient descent", "authors": ["Abraham J. Fetterman", "Christina H. Kim", "Joshua Albrecht"], "pdf": "/pdf/d00a4fb387d1c792e50a9296ec3fb79fb83c40f1.pdf", "TL;DR": "An algorithm for unifying SGD and Adam and empirical study of its performance", "abstract": "Abstract Stochastic gradient descent (SGD) and Adam are commonly used to optimize deep neural networks, but choosing one usually means making tradeoffs between speed, accuracy and stability. Here we present an intuition for why the tradeoffs exist as well as a method for unifying the two in a continuous way. This makes it possible to control the way models are trained in much greater detail. We show that for default parameters, the new algorithm equals or outperforms SGD and Adam across a range of models for image classification tasks and outperforms SGD for language modeling tasks.", "keywords": ["Optimization", "SGD", "Adam", "Generalization", "Deep Learning"], "paperhash": "fetterman|softadam_unifying_sgd_and_adam_for_better_stochastic_gradient_descent", "original_pdf": "/attachment/b2c2948852244e41b341b9372ea85c9c8c2ca82e.pdf", "_bibtex": "@misc{\nfetterman2020softadam,\ntitle={SoftAdam: Unifying {\\{}SGD{\\}} and Adam for better stochastic gradient descent},\nauthor={Abraham J. Fetterman and Christina H. Kim and Joshua Albrecht},\nyear={2020},\nurl={https://openreview.net/forum?id=Skgfr1rYDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Skgfr1rYDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1683/Authors", "ICLR.cc/2020/Conference/Paper1683/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1683/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1683/Reviewers", "ICLR.cc/2020/Conference/Paper1683/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1683/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1683/Authors|ICLR.cc/2020/Conference/Paper1683/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504152407, "tmdate": 1576860535724, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1683/Authors", "ICLR.cc/2020/Conference/Paper1683/Reviewers", "ICLR.cc/2020/Conference/Paper1683/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1683/-/Official_Comment"}}}, {"id": "Skek3-qxYB", "original": null, "number": 1, "cdate": 1570967975309, "ddate": null, "tcdate": 1570967975309, "tmdate": 1570967975309, "tddate": null, "forum": "Skgfr1rYDH", "replyto": "Skgfr1rYDH", "invitation": "ICLR.cc/2020/Conference/Paper1683/-/Public_Comment", "content": {"comment": "What do you mean by n_t and n_\\infty in Section 3?", "title": "Several Problems about Notations"}, "signatures": ["~Hao_Jin1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Hao_Jin1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["abe@sourceress.co", "christina@sourceress.co", "josh@sourceress.co"], "title": "SoftAdam: Unifying SGD and Adam for better stochastic gradient descent", "authors": ["Abraham J. Fetterman", "Christina H. Kim", "Joshua Albrecht"], "pdf": "/pdf/d00a4fb387d1c792e50a9296ec3fb79fb83c40f1.pdf", "TL;DR": "An algorithm for unifying SGD and Adam and empirical study of its performance", "abstract": "Abstract Stochastic gradient descent (SGD) and Adam are commonly used to optimize deep neural networks, but choosing one usually means making tradeoffs between speed, accuracy and stability. Here we present an intuition for why the tradeoffs exist as well as a method for unifying the two in a continuous way. This makes it possible to control the way models are trained in much greater detail. We show that for default parameters, the new algorithm equals or outperforms SGD and Adam across a range of models for image classification tasks and outperforms SGD for language modeling tasks.", "keywords": ["Optimization", "SGD", "Adam", "Generalization", "Deep Learning"], "paperhash": "fetterman|softadam_unifying_sgd_and_adam_for_better_stochastic_gradient_descent", "original_pdf": "/attachment/b2c2948852244e41b341b9372ea85c9c8c2ca82e.pdf", "_bibtex": "@misc{\nfetterman2020softadam,\ntitle={SoftAdam: Unifying {\\{}SGD{\\}} and Adam for better stochastic gradient descent},\nauthor={Abraham J. Fetterman and Christina H. Kim and Joshua Albrecht},\nyear={2020},\nurl={https://openreview.net/forum?id=Skgfr1rYDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Skgfr1rYDH", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504191271, "tmdate": 1576860569451, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper1683/Authors", "ICLR.cc/2020/Conference/Paper1683/Reviewers", "ICLR.cc/2020/Conference/Paper1683/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1683/-/Public_Comment"}}}], "count": 10}