{"notes": [{"id": "r1l-5pEtDr", "original": "HklVDF6Dwr", "number": 697, "cdate": 1569439113438, "ddate": null, "tcdate": 1569439113438, "tmdate": 1577168257517, "tddate": null, "forum": "r1l-5pEtDr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "AdaX: Adaptive Gradient Descent with Exponential Long Term Memory", "authors": ["Wenjie Li", "Zhaoyang Zhang", "Xinjiang Wang", "Ping Luo"], "authorids": ["li3549@purdue.edu", "zhaoyangzhang@link.cuhk.edu.hk", "swanxinjiang@gmail.com", "pluo.lhi@gmail.com"], "keywords": ["Optimization Algorithm", "Machine Learning", "Deep Learning", "Adam"], "TL;DR": "A novel adaptive algorithm with extraordinary performance in deep learning tasks.", "abstract": "Adaptive optimization algorithms such as RMSProp and Adam have fast convergence and smooth learning process. Despite their successes, they are proven to have non-convergence issue even in convex optimization problems as well as weak performance compared with the first order gradient methods such as stochastic gradient descent (SGD). Several other algorithms, for example AMSGrad and AdaShift, have been proposed to alleviate these issues but only minor effect has been observed. This paper further analyzes the performance of such algorithms in a non-convex setting by extending their non-convergence issue into a simple non-convex case and show that Adam's design of update steps would possibly lead the algorithm to local minimums. To address the above problems, we propose a novel adaptive gradient descent algorithm, named AdaX, which accumulates the long-term past gradient information exponentially. We prove the convergence of AdaX in both convex and non-convex settings. Extensive experiments show that AdaX outperforms Adam in various tasks of computer vision and natural language processing and can catch up with SGD.\n", "pdf": "/pdf/6a6dc5c31d5a57b8fe48696a4d5e0256c0107447.pdf", "paperhash": "li|adax_adaptive_gradient_descent_with_exponential_long_term_memory", "original_pdf": "/attachment/fdd5df6539ab38b58cb7c4c61fd43d0b875a69bf.pdf", "_bibtex": "@misc{\nli2020adax,\ntitle={AdaX: Adaptive Gradient Descent with Exponential Long Term Memory},\nauthor={Wenjie Li and Zhaoyang Zhang and Xinjiang Wang and Ping Luo},\nyear={2020},\nurl={https://openreview.net/forum?id=r1l-5pEtDr}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "xTVs_Gu3jo", "original": null, "number": 1, "cdate": 1576798703573, "ddate": null, "tcdate": 1576798703573, "tmdate": 1576800932457, "tddate": null, "forum": "r1l-5pEtDr", "replyto": "r1l-5pEtDr", "invitation": "ICLR.cc/2020/Conference/Paper697/-/Decision", "content": {"decision": "Reject", "comment": "This paper analyzes the non-convergence issue in Adam in a simple non-convex case. The authors propose a new adaptive gradient descent algorithm based on exponential long term memory, and analyze its convergence in both convex and non-convex settings. The major weakness of this paper pointed out by many reviewers is its experimental evaluation, ranging from experimental design to missing comparison with strong baseline algorithms. I agree with the reviewers\u2019 evaluation and thus recommend reject.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "AdaX: Adaptive Gradient Descent with Exponential Long Term Memory", "authors": ["Wenjie Li", "Zhaoyang Zhang", "Xinjiang Wang", "Ping Luo"], "authorids": ["li3549@purdue.edu", "zhaoyangzhang@link.cuhk.edu.hk", "swanxinjiang@gmail.com", "pluo.lhi@gmail.com"], "keywords": ["Optimization Algorithm", "Machine Learning", "Deep Learning", "Adam"], "TL;DR": "A novel adaptive algorithm with extraordinary performance in deep learning tasks.", "abstract": "Adaptive optimization algorithms such as RMSProp and Adam have fast convergence and smooth learning process. Despite their successes, they are proven to have non-convergence issue even in convex optimization problems as well as weak performance compared with the first order gradient methods such as stochastic gradient descent (SGD). Several other algorithms, for example AMSGrad and AdaShift, have been proposed to alleviate these issues but only minor effect has been observed. This paper further analyzes the performance of such algorithms in a non-convex setting by extending their non-convergence issue into a simple non-convex case and show that Adam's design of update steps would possibly lead the algorithm to local minimums. To address the above problems, we propose a novel adaptive gradient descent algorithm, named AdaX, which accumulates the long-term past gradient information exponentially. We prove the convergence of AdaX in both convex and non-convex settings. Extensive experiments show that AdaX outperforms Adam in various tasks of computer vision and natural language processing and can catch up with SGD.\n", "pdf": "/pdf/6a6dc5c31d5a57b8fe48696a4d5e0256c0107447.pdf", "paperhash": "li|adax_adaptive_gradient_descent_with_exponential_long_term_memory", "original_pdf": "/attachment/fdd5df6539ab38b58cb7c4c61fd43d0b875a69bf.pdf", "_bibtex": "@misc{\nli2020adax,\ntitle={AdaX: Adaptive Gradient Descent with Exponential Long Term Memory},\nauthor={Wenjie Li and Zhaoyang Zhang and Xinjiang Wang and Ping Luo},\nyear={2020},\nurl={https://openreview.net/forum?id=r1l-5pEtDr}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "r1l-5pEtDr", "replyto": "r1l-5pEtDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795707281, "tmdate": 1576800255478, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper697/-/Decision"}}}, {"id": "rkeCnYcy9S", "original": null, "number": 3, "cdate": 1571953077526, "ddate": null, "tcdate": 1571953077526, "tmdate": 1574490118726, "tddate": null, "forum": "r1l-5pEtDr", "replyto": "r1l-5pEtDr", "invitation": "ICLR.cc/2020/Conference/Paper697/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #4", "review": "This paper proposed a new adaptive gradient descent algorithm with exponential long term memory. The authors analyzed the non-convergence issue in Adam into a simple non-convex case. The authors also presented the convergence of the proposed AdaX in both convex and non-convex settings.\n\n- The proposed algorithm revisited the non-convergence issue in Adam and proposed a new algorithm design to try to address this issue. However, the new algorithm design is a bit strange to me, especially in Line 6 of Algorithm 2, the authors proposed to update v_t by (1+ \\beta_2) v_{t-1} + \\beta_2 g_t^2, where normally people would use (1-\\beta2) and \\beta2 as the coefficients. I am not quite get the intuition of using such as strange design. I wonder if the authors could further explain that.\n\n- The authors also add change \\beta_1 in Line 5 of Algorithm 2 into \\beta_3. And then in theory, the authors again choose \\beta_3 as \\beta_1. It seems that \\beta_3 is not contributing to any theoretical result. It seems to me to just have another parameter to tune in order to get better performances. Can the authors gives more justification on why introducing such a term here? And also show how the different choice of \\beta_3 affects the final result? \n\n- Missing some important references closely related to this paper:\n\nChen, Jinghui, and Quanquan Gu. \"Closing the generalization gap of adaptive gradient methods in training deep neural networks.\" arXiv preprint arXiv:1806.06763 (2018).\nZaheer, Manzil, et al. \"Adaptive methods for nonconvex optimization.\" Advances in Neural Information Processing Systems. 2018.\nZhou, Dongruo, et al. \"On the convergence of adaptive gradient methods for nonconvex optimization.\" arXiv preprint arXiv:1808.05671 (2018).\n\nI would suggest the authors to also compare with the above mentioned baselines to better demonstrate its performances and theoretical results.\n\n- The authors include theoretical analysis in both convex and non-convex settings, which is appreciated, however, the theoretical result seems to show similar convergence guarantees with AMSGrad. I wonder if the authors could provide theoretical justifications on why the proposed method is better than prior arts, probably some sharper convergences or some generalization guarantees?\n\n- In the experiments part, I wonder why the authors did not compare with AMSGrad, RMSProp in later parts such as ImageNet, IoU and RNN parts? I makes no sense to drop them for those experiments. Also, are the authors fully tuned the hyper-parameters for other baselines such as step size and weight decay on SGDM?\n\n================\nafter the rebuttal\n\nI thank the authors for their response but I still feel that the intuition of this paper is not clear enough and comparison with more baselines is needed. Therefore I decided to keep my score unchanged.  ", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper697/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper697/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "AdaX: Adaptive Gradient Descent with Exponential Long Term Memory", "authors": ["Wenjie Li", "Zhaoyang Zhang", "Xinjiang Wang", "Ping Luo"], "authorids": ["li3549@purdue.edu", "zhaoyangzhang@link.cuhk.edu.hk", "swanxinjiang@gmail.com", "pluo.lhi@gmail.com"], "keywords": ["Optimization Algorithm", "Machine Learning", "Deep Learning", "Adam"], "TL;DR": "A novel adaptive algorithm with extraordinary performance in deep learning tasks.", "abstract": "Adaptive optimization algorithms such as RMSProp and Adam have fast convergence and smooth learning process. Despite their successes, they are proven to have non-convergence issue even in convex optimization problems as well as weak performance compared with the first order gradient methods such as stochastic gradient descent (SGD). Several other algorithms, for example AMSGrad and AdaShift, have been proposed to alleviate these issues but only minor effect has been observed. This paper further analyzes the performance of such algorithms in a non-convex setting by extending their non-convergence issue into a simple non-convex case and show that Adam's design of update steps would possibly lead the algorithm to local minimums. To address the above problems, we propose a novel adaptive gradient descent algorithm, named AdaX, which accumulates the long-term past gradient information exponentially. We prove the convergence of AdaX in both convex and non-convex settings. Extensive experiments show that AdaX outperforms Adam in various tasks of computer vision and natural language processing and can catch up with SGD.\n", "pdf": "/pdf/6a6dc5c31d5a57b8fe48696a4d5e0256c0107447.pdf", "paperhash": "li|adax_adaptive_gradient_descent_with_exponential_long_term_memory", "original_pdf": "/attachment/fdd5df6539ab38b58cb7c4c61fd43d0b875a69bf.pdf", "_bibtex": "@misc{\nli2020adax,\ntitle={AdaX: Adaptive Gradient Descent with Exponential Long Term Memory},\nauthor={Wenjie Li and Zhaoyang Zhang and Xinjiang Wang and Ping Luo},\nyear={2020},\nurl={https://openreview.net/forum?id=r1l-5pEtDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "r1l-5pEtDr", "replyto": "r1l-5pEtDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper697/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper697/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575708126839, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper697/Reviewers"], "noninvitees": [], "tcdate": 1570237748396, "tmdate": 1575708126851, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper697/-/Official_Review"}}}, {"id": "BJluZCr19H", "original": null, "number": 2, "cdate": 1571933696076, "ddate": null, "tcdate": 1571933696076, "tmdate": 1574352240567, "tddate": null, "forum": "r1l-5pEtDr", "replyto": "r1l-5pEtDr", "invitation": "ICLR.cc/2020/Conference/Paper697/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #1", "review": "In this paper, the authors propose a new adaptive gradient algorithm AdaX, which the authors claim results in better convergence and generalization properties compared to previous adaptive gradient methods. The paper overall is fairly clear, although the writing can be improved in places.\n\nSection 3 is interesting, although calling the section the \"nonconvergence of Adam\" is a bit misleading, since the algorithm does converge to a local minimum.\n\nI have some concerns about the rest of the paper however. I am a bit confused about the changes proposed to Adam that gives rise to the AdaX algorithm. \n\n1. Won't replacing beta2 with 1+beta2 keep increasing the magnitude of the denominator of the algorithm just like AdaGrad does? In that case, is the algorithm expected to work better when having sparse gradients?\n\n2. I also do not quite understand how to interpret using a separate hyperparameter for the momentum term (ie the beta3 hyperparameter that is introduced). How is beta1 and beta3 related? The numerator loses the interpretation of momentum, i.e., averaged past gradients, when using a separate beta3 parameter, and this does not feel like a principled change.\n\nI have a number of questions about the experiments as well, which makes it hard for me to interpret the significance of the empirical results presented:\n\n1. What is the minibatch size used? Do any of the conclusions presented change if the minibatch size is changed?\n\n2. Is the learning rate tuned? The authors mention the initial learning rate used for the experiments, but it is not clear why those values are used? Was the same initial learning rate used for all algorithms?\n\n3. Were the beta1, beta2 and beta3 values tuned for AdaX? What about beta1 and beta2 for Adam? What are the optimal values of these parameters that were observed?\n\n4. How sensitive is performance to the values of these hyperparameters?\n\nOverall I think this work requires quite a bit of work before it is ready for publication, and would benefit from a much more thorough empirical evaluation of the algorithm.\n\n==================================\n\nEdit after rebutall:\nI thank the authors for their response. While the paper has definitely improved in the newer draft, after having read the other reviews and the updated draft, I believe the paper still requires a bit of work before being ready for publication. I am sticking to my score.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper697/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper697/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "AdaX: Adaptive Gradient Descent with Exponential Long Term Memory", "authors": ["Wenjie Li", "Zhaoyang Zhang", "Xinjiang Wang", "Ping Luo"], "authorids": ["li3549@purdue.edu", "zhaoyangzhang@link.cuhk.edu.hk", "swanxinjiang@gmail.com", "pluo.lhi@gmail.com"], "keywords": ["Optimization Algorithm", "Machine Learning", "Deep Learning", "Adam"], "TL;DR": "A novel adaptive algorithm with extraordinary performance in deep learning tasks.", "abstract": "Adaptive optimization algorithms such as RMSProp and Adam have fast convergence and smooth learning process. Despite their successes, they are proven to have non-convergence issue even in convex optimization problems as well as weak performance compared with the first order gradient methods such as stochastic gradient descent (SGD). Several other algorithms, for example AMSGrad and AdaShift, have been proposed to alleviate these issues but only minor effect has been observed. This paper further analyzes the performance of such algorithms in a non-convex setting by extending their non-convergence issue into a simple non-convex case and show that Adam's design of update steps would possibly lead the algorithm to local minimums. To address the above problems, we propose a novel adaptive gradient descent algorithm, named AdaX, which accumulates the long-term past gradient information exponentially. We prove the convergence of AdaX in both convex and non-convex settings. Extensive experiments show that AdaX outperforms Adam in various tasks of computer vision and natural language processing and can catch up with SGD.\n", "pdf": "/pdf/6a6dc5c31d5a57b8fe48696a4d5e0256c0107447.pdf", "paperhash": "li|adax_adaptive_gradient_descent_with_exponential_long_term_memory", "original_pdf": "/attachment/fdd5df6539ab38b58cb7c4c61fd43d0b875a69bf.pdf", "_bibtex": "@misc{\nli2020adax,\ntitle={AdaX: Adaptive Gradient Descent with Exponential Long Term Memory},\nauthor={Wenjie Li and Zhaoyang Zhang and Xinjiang Wang and Ping Luo},\nyear={2020},\nurl={https://openreview.net/forum?id=r1l-5pEtDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "r1l-5pEtDr", "replyto": "r1l-5pEtDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper697/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper697/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575708126839, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper697/Reviewers"], "noninvitees": [], "tcdate": 1570237748396, "tmdate": 1575708126851, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper697/-/Official_Review"}}}, {"id": "rkxMdxqNsr", "original": null, "number": 3, "cdate": 1573326953904, "ddate": null, "tcdate": 1573326953904, "tmdate": 1573351898960, "tddate": null, "forum": "r1l-5pEtDr", "replyto": "BJluZCr19H", "invitation": "ICLR.cc/2020/Conference/Paper697/-/Official_Comment", "content": {"title": "Rebuttal: about your questions and concerns", "comment": "Thank you for your interest in our paper. We hope that we can resolve your concerns in this rebuttal.\n\n$>>>$ Won't replacing beta2 with 1+beta2 keep increasing the magnitude of the denominator of the algorithm just like AdaGrad does? In that case, is the algorithm expected to work better when having sparse gradients.\n\nThe magnitude of $v_t$ does not necessarily keep increasing because of the existence of the bias correction term$(1+\\beta_2)^t-1$. Therefore our algorithm is totally different from the AdaGrad algorithm. However with a strongly decreasing step size $\\alpha_t = \\alpha/\\sqrt{t}$ as in the theory proofs, $v_t/\\alpha_t$ does keep increasing and this fact corresponds to our claim that the matrix $\\Gamma_t = \\frac{\\sqrt{V_{t+1}}}{\\alpha_{t+1}}-\\frac{\\sqrt{V_{t}}}{\\alpha_t}$ is positive definite in our algorithm. We tested the performance of AdaX on the one-billion-word dataset where sparse gradients existed and our algorithm did perform even better than Adam.\n\n$>>>$I also do not quite understand how to interpret using a separate hyperparameter for the momentum term (ie the beta3 hyperparameter that is introduced). How is beta1 and beta3 related? \n\nWe have found that this new parameter $\\beta_3$ makes our reviewers confused and we are genuinely sorry for the confusion. The $\\beta_3$ parameter is introduced mainly to help proving the theoretical convergence results as in Theorem 4.1. However, it does not change the interpretation of momentum at all because the constant $(1-\\beta_3)$ can be factored out of each term of the first-order momentum sum. As long as $\\alpha (1-\\beta_3)$ does not change, the effective update steps of AdaX remain exactly the same. We will include more experiments to show this fact in future versions of our paper. Thank you for pointing it out.\n\n$>>>$ What is the minibatch size used? Do any of the conclusions presented change if the minibatch size is changed?\n\nAs mentioned in our paper, the mini-batch sizes used in our experiments are the same as those used in the original papers and studies, i.e. 128 for CIFAR-10 and 256(32*8) for ImageNet as in [1], 10 for VOC2012 as in [2], and 128 for One-Billion word as in [3]. We tried changing the batch sizes in our experiments, but the conclusions don't change.\n\n$>>>$  Is the learning rate tuned? The authors mention the initial learning rate used for the experiments, but it is not clear why those values are used? Was the same initial learning rate used for all algorithms?\n\nYes, we have tuned the learning rates for different optimizers. The values we choose are the parameters that yield the best results in our experiments, and they are the same as those in the original papers [1][2][3]. As we mentioned in the experiments section, different initial step sizes should be used for different optimizers due to their different designs. For example in image classification tasks, Adam is known to work well with initial step size from 1e-4 to 1e-3 while SGD needs much larger initial step sizes, such as 0.1. \n\n$>>>$ Were the beta1, beta2 and beta3 values tuned for AdaX? What about beta1 and beta2 for Adam? What are the optimal values of these parameters that were observed?\n\nYes, we have tuned the $\\beta_1$ and $\\beta_2$ values in our experiments. $\\beta_1 = 0.9$ generates the best results when the other parameters are held constant. We found that the value of $\\beta_2$ can be chosen from $1e-3$ to $1e-5$ and it does not affect the experiment results. We choose $1e-4$ only to avoid the large computation cost of large numbers. Tuning $\\beta_3$ is the same as tuning the step sizes $\\alpha_t$ as mentioned earlier, and we will show this fact by conducting more experiments. We have also tuned the $\\beta_1$ and $\\beta_2$ parameters for Adam, and we found that the default parameters recommended by [4] (i.e. $\\beta_1=0.9, \\beta_2=0.999$) performed the best.\n\n$>>>$ How sensitive is performance to the values of these hyperparameter?\n\nIn AdaX, the overall performance is not sensitive to $\\beta_2$ as mentioned earlier, but it is sensitive to $\\beta_1$ and we found that $\\beta_1 =0.9$ yields the best performances. The parameter $\\beta_3$ can be changed and the performance is not affected as long as $(1-\\beta_3)\\alpha$ remains constant.\n\nAgain, thank you for your interesting questions and suggestions about our paper. We will add more experiments to our paper very soon.\n\n[1] He et al. Deep residual learning for image recognition. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),2016\n[2] Chen et al.Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and\nfully connected crfs. IEEE Transactions on Pattern Analysis and Machine Intelligence, 40:834\u2013848, 2016.\n[3] Rdspring1. Pytorch gbw lm. https://github.com/rdspring1/PyTorch_GBW_LM.\n[4] Kingma and Ba. Adam: A method for stochastic optimization. Proceedings of the 3rd International Conference on Learning Representations (ICLR), 2015."}, "signatures": ["ICLR.cc/2020/Conference/Paper697/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper697/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "AdaX: Adaptive Gradient Descent with Exponential Long Term Memory", "authors": ["Wenjie Li", "Zhaoyang Zhang", "Xinjiang Wang", "Ping Luo"], "authorids": ["li3549@purdue.edu", "zhaoyangzhang@link.cuhk.edu.hk", "swanxinjiang@gmail.com", "pluo.lhi@gmail.com"], "keywords": ["Optimization Algorithm", "Machine Learning", "Deep Learning", "Adam"], "TL;DR": "A novel adaptive algorithm with extraordinary performance in deep learning tasks.", "abstract": "Adaptive optimization algorithms such as RMSProp and Adam have fast convergence and smooth learning process. Despite their successes, they are proven to have non-convergence issue even in convex optimization problems as well as weak performance compared with the first order gradient methods such as stochastic gradient descent (SGD). Several other algorithms, for example AMSGrad and AdaShift, have been proposed to alleviate these issues but only minor effect has been observed. This paper further analyzes the performance of such algorithms in a non-convex setting by extending their non-convergence issue into a simple non-convex case and show that Adam's design of update steps would possibly lead the algorithm to local minimums. To address the above problems, we propose a novel adaptive gradient descent algorithm, named AdaX, which accumulates the long-term past gradient information exponentially. We prove the convergence of AdaX in both convex and non-convex settings. Extensive experiments show that AdaX outperforms Adam in various tasks of computer vision and natural language processing and can catch up with SGD.\n", "pdf": "/pdf/6a6dc5c31d5a57b8fe48696a4d5e0256c0107447.pdf", "paperhash": "li|adax_adaptive_gradient_descent_with_exponential_long_term_memory", "original_pdf": "/attachment/fdd5df6539ab38b58cb7c4c61fd43d0b875a69bf.pdf", "_bibtex": "@misc{\nli2020adax,\ntitle={AdaX: Adaptive Gradient Descent with Exponential Long Term Memory},\nauthor={Wenjie Li and Zhaoyang Zhang and Xinjiang Wang and Ping Luo},\nyear={2020},\nurl={https://openreview.net/forum?id=r1l-5pEtDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1l-5pEtDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper697/Authors", "ICLR.cc/2020/Conference/Paper697/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper697/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper697/Reviewers", "ICLR.cc/2020/Conference/Paper697/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper697/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper697/Authors|ICLR.cc/2020/Conference/Paper697/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504167594, "tmdate": 1576860544335, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper697/Authors", "ICLR.cc/2020/Conference/Paper697/Reviewers", "ICLR.cc/2020/Conference/Paper697/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper697/-/Official_Comment"}}}, {"id": "SkxM4h9VoS", "original": null, "number": 4, "cdate": 1573329961561, "ddate": null, "tcdate": 1573329961561, "tmdate": 1573351636537, "tddate": null, "forum": "r1l-5pEtDr", "replyto": "BkenmE9a_r", "invitation": "ICLR.cc/2020/Conference/Paper697/-/Official_Comment", "content": {"title": "Rebuttal: about the \"problems\"", "comment": "$>>>$ The experiments are lacking important details. How many independent runs of the experiment \nwere the experimental results averaged over? All of the experiments have random initial conditions \n(e.g. initialization of the network), and should be ran multiple times, not just once. \n\nOur results are averaged over multiple independent runs (e.g. more than 5 runs for CIFAR and 3 for ImageNet). However, thank you for pointing it out. We will consider adding error bars to our experiments if time allows.\n\n$>>>$ How were the hyperparameters and step-size schedules chosen? The performance of Adam, AMSGrad, and RMSProp are quite sensitive to their hyperparameters, and the optimal hyperparameters are problem-dependent. \n\nWe have carefully and thoroughly tuned the parameters and we will add our tuning strategy to our paper as soon as possible. The results in the paper are all the best results we are able to obtain.\n\n$>>>$ The experiments do not provide any new intuition or understanding of the methods, showing \nonly the relative performances in terms of learning curves on a somewhat random collection of supervised learning problems. Why were these specific problems chosen? What makes these problems ideal for showcasing the performance of AdaX? If AdaX is an improvement over Adam, why? What exactly is happening with its effective step-sizes that leads \nto the better performance? Can you show how their step-sizes differ over time? \n\nThese experiments are definitely not a random collection of supervised learning problems. They cover the most heated research topics in deep learning, i.e. computer vision, natural language processing, and transfer learning. Many optimization papers have chosen these tasks to show the superiority of their algorithms, such as [1]-[5]. We will show the effective step sizes of different algorithms in our toy example (Problem (3)) in the future versions of our paper, but the difference of their step sizes can be easily observed from the training curves in the experiments. AdaX outperforms Adam because Adam converges too fast and may end up in local minimums and we mentioned this reason in our section 3 and 4. The experiments verify our claim by showing the slightly slower convergence speed but far better performances. We are strongly against your statement that the experiments do not provide any new intuition or understanding of the methods and are just a random collection.\n\n$>>>$ Adaptive optimization algorithms such as RMSProp and Adam... as well as weak performance \n     compared to the first order gradient methods such as SGD\" (Abstract). This needs a citation\n\nWe believe that it is recommended not to cite other people's work in the abstract, and such a convention can be observed from this year's papers as well, for example (https://openreview.net/pdf?id=S1xJ4JHFvS), (https://openreview.net/pdf?id=rye5YaEtPr) and (https://openreview.net/pdf?id=rkgz2aEKDr).\n\n$>>>$I'm unaware of work (other than theoretical) that shows that SGD significantly outperforms Adam in deep neural networks.\n\nThere are many papers (both theoretical and experimental) that show SGD(M) significantly outperforms Adam in deep neural networks, for example [1], [3], and [4]. As mentioned by reviewer 2 and reviewer 4, the poor generalization of adaptive methods is one of the main reasons why people develop so many different algorithms.\n\nWe apologize for the other missing citations and will fix those as soon as possible.\n\nIn conclusion, we thank the reviewer for suggesting error bars, parameter tuning, and some missing citations, but we do not agree with the other \"problems\" in our paper mentioned in the review. We hope the AC and reviewer can reconsider this evaluation.\n\n\n[1]Haiwen Huang, Chang Wang, and Bin Dong. Nostalgic adam: Weighting more of the past gradients\nwhen designing the adaptive learning rate. arXiv preprint arXiv: 1805.07557, 2019.\n\n[2]Diederik P Kingma and Jimmy Lei Ba. Adam: A method for stochastic optimization. Proceedings\nof the 3rd International Conference on Learning Representations (ICLR), 2015.\n\n[3]Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. Proceedings of 7th\nInternational Conference on Learning Representations (ICLR), 2019.\n\n[4]Liangchen Luo, Yuanhao Xiong, Yan Liu, and Xu Sun. Adaptive gradient methods with dynamic\nbound of learning rate. Proceedings of 7th International Conference on Learning Representations,\n2019.\n\n[5]Sashank J. Reddi, Stayen Kale, and Sanjiv Kumar. On the convergence of adam and beyond."}, "signatures": ["ICLR.cc/2020/Conference/Paper697/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper697/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "AdaX: Adaptive Gradient Descent with Exponential Long Term Memory", "authors": ["Wenjie Li", "Zhaoyang Zhang", "Xinjiang Wang", "Ping Luo"], "authorids": ["li3549@purdue.edu", "zhaoyangzhang@link.cuhk.edu.hk", "swanxinjiang@gmail.com", "pluo.lhi@gmail.com"], "keywords": ["Optimization Algorithm", "Machine Learning", "Deep Learning", "Adam"], "TL;DR": "A novel adaptive algorithm with extraordinary performance in deep learning tasks.", "abstract": "Adaptive optimization algorithms such as RMSProp and Adam have fast convergence and smooth learning process. Despite their successes, they are proven to have non-convergence issue even in convex optimization problems as well as weak performance compared with the first order gradient methods such as stochastic gradient descent (SGD). Several other algorithms, for example AMSGrad and AdaShift, have been proposed to alleviate these issues but only minor effect has been observed. This paper further analyzes the performance of such algorithms in a non-convex setting by extending their non-convergence issue into a simple non-convex case and show that Adam's design of update steps would possibly lead the algorithm to local minimums. To address the above problems, we propose a novel adaptive gradient descent algorithm, named AdaX, which accumulates the long-term past gradient information exponentially. We prove the convergence of AdaX in both convex and non-convex settings. Extensive experiments show that AdaX outperforms Adam in various tasks of computer vision and natural language processing and can catch up with SGD.\n", "pdf": "/pdf/6a6dc5c31d5a57b8fe48696a4d5e0256c0107447.pdf", "paperhash": "li|adax_adaptive_gradient_descent_with_exponential_long_term_memory", "original_pdf": "/attachment/fdd5df6539ab38b58cb7c4c61fd43d0b875a69bf.pdf", "_bibtex": "@misc{\nli2020adax,\ntitle={AdaX: Adaptive Gradient Descent with Exponential Long Term Memory},\nauthor={Wenjie Li and Zhaoyang Zhang and Xinjiang Wang and Ping Luo},\nyear={2020},\nurl={https://openreview.net/forum?id=r1l-5pEtDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1l-5pEtDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper697/Authors", "ICLR.cc/2020/Conference/Paper697/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper697/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper697/Reviewers", "ICLR.cc/2020/Conference/Paper697/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper697/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper697/Authors|ICLR.cc/2020/Conference/Paper697/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504167594, "tmdate": 1576860544335, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper697/Authors", "ICLR.cc/2020/Conference/Paper697/Reviewers", "ICLR.cc/2020/Conference/Paper697/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper697/-/Official_Comment"}}}, {"id": "H1e6eJX-oH", "original": null, "number": 2, "cdate": 1573101301309, "ddate": null, "tcdate": 1573101301309, "tmdate": 1573101301309, "tddate": null, "forum": "r1l-5pEtDr", "replyto": "rkeCnYcy9S", "invitation": "ICLR.cc/2020/Conference/Paper697/-/Official_Comment", "content": {"title": "Rebuttal: about the questions and the suggestions", "comment": "Thank you for your interest in our paper and your constructive feedback! We want to clarify the following things about our paper.\n\n$>>>$ However, the new algorithm design is a bit strange to me, especially in Line 6 of Algorithm 2, the authors proposed to update $v_t$ by $(1+ \\beta_2) v_{t-1} + \\beta_2 g_t^2$, where normally people would use $(1-\\beta_2)$ and $\\beta_2$ as the coefficients. I am not quite get the intuition of using such as strange design. I wonder if the authors could further explain that.\n\nThe change from exponential moving average($\\beta_2 v_{t-1} + (1-\\beta_2) g_t^2$) to exponential long term memory($(1+ \\beta_2) v_{t-1} + \\beta_2 g_t^2$) is the most important reason why our algorithm outperforms Adam. As shown in our toy example (3), because Adam assigns high weights on recent gradients and low weights on past gradients, its second momentum decreases with approximately the same rate as the first momentum. As a result, the effective update steps of Adam ignore the gradient decrease information and will be larger than a constant. Adam converges very fast but will ultimately be trapped in the local minimum. Therefore, we conclude that current gradients are not trustworthy and past information should not be forgotten as in Adam. We change the moving average design to exponential long term memory in order to emphasize the importance of past memory (high weight $(1+\\beta_2)$) and assign less weight ($\\beta_2$) on current gradients. Such a design slows down the convergence speed of Adam a little, but results in better performances in our toy example as the gradient decrease information is not lost. In our experiment section, we also observe that AdaX converges slightly slower than Adam at the beginning of different tasks, but its final performance can catch up with SGDM.\n\n$>>>$The authors also add change $\\beta_1$ in Line 5 of Algorithm 2 into $\\beta_3$. And then in theory, the authors again choose $\\beta_3$ as $\\beta_1$. It seems that $\\beta_3$ is not contributing to any theoretical result. It seems to me to just have another parameter to tune in order to get better performances. Can the authors gives more justification on why introducing such a term here? And also show how the different choice of $\\beta_3$ affects the final result?\n\nWe have found that the new hyperparameter $\\beta_3$ has caused some confusion and we want to apologize for that. However, we want to clarify that $\\beta_3$ does help to prove the theoretical conclusion in Theorem 4.1 as we state $\\beta_{3t} = 1-1/\\sqrt{t}$ in the assumptions. The reason that $\\beta_3$ is changed to $\\beta_1$ in Theorem 4.2 is to match up with the settings of the original paper(Chen et. al.  On the convergence of a class of adam-type algorithm for non-convex optimization, 2019). Besides, $(1-\\beta_3)$ only scales the effective update steps by a constant and tuning this parameter is the same as tuning the step size $\\alpha$. Therefore the better performance of AdaX does not originate from this new parameter. We will add more experiments in future versions of our paper to show that as long as $\\alpha(1-\\beta_3)$ remains the same, the performance of AdaX does not change.\n\n$>>>$I would suggest the authors to also compare with the above mentioned baselines to better demonstrate its performances and theoretical results.\n\nWe will consider adding these baselines in theory and experiments to show our superiority if time allows. Thank you for bringing them up. \n\n$>>>$In the experiments part, I wonder why the authors did not compare with AMSGrad, RMSProp in later parts such as ImageNet, IoU and RNN parts? I makes no sense to drop them for those experiments. Also, are the authors fully tuned the hyper-parameters for other baselines such as step size and weight decay on SGDM\n\nWe didn't compare with AMSGrad, RMSProp in the later experiments because their performances are generally close to or even worse than Adam (as shown in the CIFAR experiments). Because our method can catch up with SGDM in so many different tasks, we are confident that our method will outperform these methods. However, we will consider adding their performances in later versions of our paper. Thank you for your suggestion. We have definitely fully tuned the hyper-parameters of other baselines, especially for Adam and SGD. All the shown experiments are the best results we are able to obtain and we will add our tuning strategies to the Appendix.\n\nAgain, thank you for your feedback and we hope our rebuttal can address your concerns."}, "signatures": ["ICLR.cc/2020/Conference/Paper697/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper697/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "AdaX: Adaptive Gradient Descent with Exponential Long Term Memory", "authors": ["Wenjie Li", "Zhaoyang Zhang", "Xinjiang Wang", "Ping Luo"], "authorids": ["li3549@purdue.edu", "zhaoyangzhang@link.cuhk.edu.hk", "swanxinjiang@gmail.com", "pluo.lhi@gmail.com"], "keywords": ["Optimization Algorithm", "Machine Learning", "Deep Learning", "Adam"], "TL;DR": "A novel adaptive algorithm with extraordinary performance in deep learning tasks.", "abstract": "Adaptive optimization algorithms such as RMSProp and Adam have fast convergence and smooth learning process. Despite their successes, they are proven to have non-convergence issue even in convex optimization problems as well as weak performance compared with the first order gradient methods such as stochastic gradient descent (SGD). Several other algorithms, for example AMSGrad and AdaShift, have been proposed to alleviate these issues but only minor effect has been observed. This paper further analyzes the performance of such algorithms in a non-convex setting by extending their non-convergence issue into a simple non-convex case and show that Adam's design of update steps would possibly lead the algorithm to local minimums. To address the above problems, we propose a novel adaptive gradient descent algorithm, named AdaX, which accumulates the long-term past gradient information exponentially. We prove the convergence of AdaX in both convex and non-convex settings. Extensive experiments show that AdaX outperforms Adam in various tasks of computer vision and natural language processing and can catch up with SGD.\n", "pdf": "/pdf/6a6dc5c31d5a57b8fe48696a4d5e0256c0107447.pdf", "paperhash": "li|adax_adaptive_gradient_descent_with_exponential_long_term_memory", "original_pdf": "/attachment/fdd5df6539ab38b58cb7c4c61fd43d0b875a69bf.pdf", "_bibtex": "@misc{\nli2020adax,\ntitle={AdaX: Adaptive Gradient Descent with Exponential Long Term Memory},\nauthor={Wenjie Li and Zhaoyang Zhang and Xinjiang Wang and Ping Luo},\nyear={2020},\nurl={https://openreview.net/forum?id=r1l-5pEtDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1l-5pEtDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper697/Authors", "ICLR.cc/2020/Conference/Paper697/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper697/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper697/Reviewers", "ICLR.cc/2020/Conference/Paper697/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper697/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper697/Authors|ICLR.cc/2020/Conference/Paper697/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504167594, "tmdate": 1576860544335, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper697/Authors", "ICLR.cc/2020/Conference/Paper697/Reviewers", "ICLR.cc/2020/Conference/Paper697/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper697/-/Official_Comment"}}}, {"id": "B1ekF1OesH", "original": null, "number": 1, "cdate": 1573056375126, "ddate": null, "tcdate": 1573056375126, "tmdate": 1573056511411, "tddate": null, "forum": "r1l-5pEtDr", "replyto": "BylElFr-cH", "invitation": "ICLR.cc/2020/Conference/Paper697/-/Official_Comment", "content": {"title": "Rebuttal: about the cons", "comment": "Thank you for your valuable comments and inspiring questions. We want to clarify the following points about our new AdaX algorithm.\n\n$>>>$What is default hyperparameters for AdaX in Algorithm 1? Is it the same as AdaX-W (Algorithm 3) in Appendix?\n\n  Yes. The hyperparameters for AdaX are the same as in AdaX-W, i.e. $\\beta_1=0.9, \\beta_2=1e-4,\\beta_3=0.999$. Actually, we mentioned in the Appendix of our paper that Algorithm 3 presented the \"detailed implementations of AdaX and AdaX-W\". The only difference between AdaX and AdaX-W is whether to use L-2 regularization or weight decay in the algorithm. \n  \n$>>>$So, It is not clear that the second momentum estimate of Ada-X is really stable. For this, it would be interesting to see how the trajectories of second-order momentum estimates of Adam, AMSGrad, Ada-X are different.\n\n  The stability of the second momentum of AdaX is guaranteed by the convergence theorems (Theorem 4.1 and 4.2) in both the convex and the non-convex cases. In fact, the large bias-correction term would be scaled by the exponentially decreasing $1/\\sqrt{(1+\\beta_2)v_{t-1} + \\beta_2 g_t^2}$ and hence the second momentum estimate of AdaX remains stable in the long term. However, we definitely agree that it would be interesting to see the different trajectories of second-order momentum estimates of different algorithms. We will add the trajectories of the different algorithms in our toy example (Problem (3)) to the final version of the paper. Thank you for such an interesting suggestion.\n  \n$>>>$ Although the authors effectively remove log T in the numerator in Corollary 3.2 of Chen et al. (2019) using their lemma 4.1 (I think this is the key point), the assumption that $\\beta_{2t} = \\beta_2/t$ seems quite strong, and original Adam paper has no such assumptions.\n\n    Yes, you are right and the condition on $\\beta_{2t}$ is quite strong. The original Adam paper doesn't have this strong assumption because it uses the exponential moving average of the square of the past gradients. Therefore, its second moment and bias correction will be bounded if the gradients are bounded. However, when we choose to employ exponential long-term memory, both terms will possibly go to infinity and such a condition is needed in the convergence proofs. We will consider improving this condition to $\\beta_{2t} = \\beta_2/\\sqrt{t}$ in the future. Theorem 4.2 uses it to match with the settings of Theorem 4.1, and Corollary 4.2 actually shows that we don't need such a condition when analyzing convergence based on gradient sizes. \n    \n$>>>$... In other words, the authors should use the same policy on constructing the first-order momentum estimate for both Adam and Ada-X. Also, as the authors add an additional hyperparameter $\\beta_3$, the effect of $\\beta_3$ on performance should be discussed at least empirically.\n    \n    We emphasize that $(1-\\beta_{3})$ helps to prove the convergence of AdaX and it only scales the step sizes by a constant. We can obtain the same strategy as Adam's first-order momentum by changing $(1-\\beta_3)$ to $(1-\\beta_1)$ and decreasing the learning rate by $(1-\\beta_3)/(1-\\beta_1)$. In such a case, the weight decay would naturally increase by $(1-\\beta_1)/(1-\\beta_3)$. We agree that we should add more discussions on the effect of $\\beta_3$ in the experiments section to reveal this fact, and we will provide such experiments as soon as possible.\n\n$>>>$In this context, Zaheer et al. (2018, Adaptive methods for non-convex optimization) propose a large epsilon value (numerical stability parameter) such as for better generalization. It will be more interesting to see the comparisons in this regime.\n \n   Including a large epsilon is one way of fixing the poor generalization and we agree that such comparisons could be interesting to explore. Thank you for pointing it out. However, a large epsilon will also harm the adaptive ability of the algorithm since small gradients will be dominated by the large epsilon value in the second momentum and the algorithm will be similar to SGD. We will try running a few experiments in this direction if time allows.\n   \n$>>>$In my experience with Adam-W (Decoupled weight decay regularization), Adam-W requires a relatively large weight decay parameter $\\lambda$...\n\n  Thank you for your advice. However, we believe that this is just an implementation difference. As can be observed in Figure 1c, the higher weight decay 5e-4 already makes the AdamW algorithm converge as slow as SGDM. We also tried AdamW with higher and lower weight decays in our experiments, but they ended up with even worse results.\n\n$>>>$For the minor errors\n\n  Yes, you are absolutely right and thank you for being such a careful reader. We will certainly fix these issues in our next version of the paper."}, "signatures": ["ICLR.cc/2020/Conference/Paper697/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper697/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "AdaX: Adaptive Gradient Descent with Exponential Long Term Memory", "authors": ["Wenjie Li", "Zhaoyang Zhang", "Xinjiang Wang", "Ping Luo"], "authorids": ["li3549@purdue.edu", "zhaoyangzhang@link.cuhk.edu.hk", "swanxinjiang@gmail.com", "pluo.lhi@gmail.com"], "keywords": ["Optimization Algorithm", "Machine Learning", "Deep Learning", "Adam"], "TL;DR": "A novel adaptive algorithm with extraordinary performance in deep learning tasks.", "abstract": "Adaptive optimization algorithms such as RMSProp and Adam have fast convergence and smooth learning process. Despite their successes, they are proven to have non-convergence issue even in convex optimization problems as well as weak performance compared with the first order gradient methods such as stochastic gradient descent (SGD). Several other algorithms, for example AMSGrad and AdaShift, have been proposed to alleviate these issues but only minor effect has been observed. This paper further analyzes the performance of such algorithms in a non-convex setting by extending their non-convergence issue into a simple non-convex case and show that Adam's design of update steps would possibly lead the algorithm to local minimums. To address the above problems, we propose a novel adaptive gradient descent algorithm, named AdaX, which accumulates the long-term past gradient information exponentially. We prove the convergence of AdaX in both convex and non-convex settings. Extensive experiments show that AdaX outperforms Adam in various tasks of computer vision and natural language processing and can catch up with SGD.\n", "pdf": "/pdf/6a6dc5c31d5a57b8fe48696a4d5e0256c0107447.pdf", "paperhash": "li|adax_adaptive_gradient_descent_with_exponential_long_term_memory", "original_pdf": "/attachment/fdd5df6539ab38b58cb7c4c61fd43d0b875a69bf.pdf", "_bibtex": "@misc{\nli2020adax,\ntitle={AdaX: Adaptive Gradient Descent with Exponential Long Term Memory},\nauthor={Wenjie Li and Zhaoyang Zhang and Xinjiang Wang and Ping Luo},\nyear={2020},\nurl={https://openreview.net/forum?id=r1l-5pEtDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1l-5pEtDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper697/Authors", "ICLR.cc/2020/Conference/Paper697/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper697/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper697/Reviewers", "ICLR.cc/2020/Conference/Paper697/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper697/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper697/Authors|ICLR.cc/2020/Conference/Paper697/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504167594, "tmdate": 1576860544335, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper697/Authors", "ICLR.cc/2020/Conference/Paper697/Reviewers", "ICLR.cc/2020/Conference/Paper697/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper697/-/Official_Comment"}}}, {"id": "BkenmE9a_r", "original": null, "number": 1, "cdate": 1570772004332, "ddate": null, "tcdate": 1570772004332, "tmdate": 1572972563472, "tddate": null, "forum": "r1l-5pEtDr", "replyto": "r1l-5pEtDr", "invitation": "ICLR.cc/2020/Conference/Paper697/-/Official_Review", "content": {"rating": "1: Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper introduces a new step-size adaptation algorithm called AdaX. AdaX \nbuilds on the ideas of the Adam algorithm to address instability and non-convergence issues. \nConvergence of AdaX is proven in both convex and non-convex settings. The paper \nalso provides an empirical comparison of AdaX against its predecessors \n(SGD, RMSProp, Adam, AMSGrad) on a variety of tasks.\n\nI recommend the paper be rejected. I believe the convergence results could be a significant contribution, but the \nquality of the paper is hampered by its experimental design. The paper felt generally unpolished, containing\nfrequent grammatical errors, imprecise language, and uncited statements.\n\nMy main issue with the paper is the experimental design. I am not convinced that we can \ndraw valid conclusions from the experimental results for the following reasons:\n  - The experiments are lacking important details. How many independent runs of the experiment \n    were the experimental results averaged over? All of the experiments have random initial conditions \n    (e.g. initialization of the network), and should be ran multiple times, not just once. \n    There's no error bars in any of the plots, so it's unclear whether AdaX really \n    does provide a statistically significant improvement over the baselines. \n    Similarly, the data in all the tables is quite similar, so without indicating the \n    spread of these estimates its impossible to tell whether these results are significant or not.\n\n  - How were the hyperparameters and step-size schedules chosen? The performance of Adam, AMSGrad, and \n    RMSProp are quite sensitive to their hyperparameters, and the optimal hyperparameters are problem-dependent. \n    Some of the experiments just use the default hyperparameters; this is insufficient when trying to directly \n    compare the performance of these methods, as their performance can vary greatly with different values of\n    these parameters. I'm not convinced that we should be drawing conclusions about the relative \n    performance of these algorithms from any of the experiments for this reason.\n\nOf course, meaningful empirical results are not necessarily characterized by statistically outperforming the baselines. \nWell designed experiments can highlight important ways in which the performances differ, providing the community \nwith a deeper understanding of the methods investigated. I would argue that the experiments in the paper do not \nachieve this either; the experiments do not provide any new intuition or understanding of the methods, showing \nonly the relative performances in terms of learning curves on a somewhat random collection of supervised learning problems. Why were these specific problems chosen? What makes these problems ideal for showcasing the performance of AdaX? If AdaX is an improvement over Adam, why? What exactly is happening with it's effective step-sizes that leads \nto the better performance? Can you show how their step-sizes differ over time? \n\nStatements that need citation or revision:\n  - \"Adaptive optimization algorithms such as RMSProp and Adam... as well as weak performance \n     compared to the first order gradient methods such as SGD\" (Abstract). This needs a citation. \n     Similarly, \"AdaX outperforms various tasks of computer vision and natural language processing and can catch \n     up with SGD\"; as above, I'm unaware of work (other than theoretical) that shows that SGD significantly \n     outperforms Adam in deep neural networks.\n  -  \"In the era of deep learning, SGD ... remains the most effective algorithm in training deep neural \n      networks\" (Introduction). What are you referring to here? Vanilla SGD? Or are you including Adam etc here? \n      As above, this should have a citation. Adam's popularity is largely due to its effectiveness in training \n      deep neural networks.\n  - \"However, Adam has worse performance (i.e. generalization ability in testing stage) compared with SGD\" \n     (Introduction). Citation needed.\n  - In the last paragraph of the Introduction, you introduced AdaX twice: \"To address the above issues, we propose a \n    new adaptive optimization method, termed AdaX, which guarantees convergence...\", and, \"To address \n    the above problems, we introduce a novel AdaX algorithm and theoreetically prove that it converges...\"\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper697/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper697/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "AdaX: Adaptive Gradient Descent with Exponential Long Term Memory", "authors": ["Wenjie Li", "Zhaoyang Zhang", "Xinjiang Wang", "Ping Luo"], "authorids": ["li3549@purdue.edu", "zhaoyangzhang@link.cuhk.edu.hk", "swanxinjiang@gmail.com", "pluo.lhi@gmail.com"], "keywords": ["Optimization Algorithm", "Machine Learning", "Deep Learning", "Adam"], "TL;DR": "A novel adaptive algorithm with extraordinary performance in deep learning tasks.", "abstract": "Adaptive optimization algorithms such as RMSProp and Adam have fast convergence and smooth learning process. Despite their successes, they are proven to have non-convergence issue even in convex optimization problems as well as weak performance compared with the first order gradient methods such as stochastic gradient descent (SGD). Several other algorithms, for example AMSGrad and AdaShift, have been proposed to alleviate these issues but only minor effect has been observed. This paper further analyzes the performance of such algorithms in a non-convex setting by extending their non-convergence issue into a simple non-convex case and show that Adam's design of update steps would possibly lead the algorithm to local minimums. To address the above problems, we propose a novel adaptive gradient descent algorithm, named AdaX, which accumulates the long-term past gradient information exponentially. We prove the convergence of AdaX in both convex and non-convex settings. Extensive experiments show that AdaX outperforms Adam in various tasks of computer vision and natural language processing and can catch up with SGD.\n", "pdf": "/pdf/6a6dc5c31d5a57b8fe48696a4d5e0256c0107447.pdf", "paperhash": "li|adax_adaptive_gradient_descent_with_exponential_long_term_memory", "original_pdf": "/attachment/fdd5df6539ab38b58cb7c4c61fd43d0b875a69bf.pdf", "_bibtex": "@misc{\nli2020adax,\ntitle={AdaX: Adaptive Gradient Descent with Exponential Long Term Memory},\nauthor={Wenjie Li and Zhaoyang Zhang and Xinjiang Wang and Ping Luo},\nyear={2020},\nurl={https://openreview.net/forum?id=r1l-5pEtDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "r1l-5pEtDr", "replyto": "r1l-5pEtDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper697/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper697/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575708126839, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper697/Reviewers"], "noninvitees": [], "tcdate": 1570237748396, "tmdate": 1575708126851, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper697/-/Official_Review"}}}, {"id": "BylElFr-cH", "original": null, "number": 4, "cdate": 1572063468007, "ddate": null, "tcdate": 1572063468007, "tmdate": 1572972563350, "tddate": null, "forum": "r1l-5pEtDr", "replyto": "r1l-5pEtDr", "invitation": "ICLR.cc/2020/Conference/Paper697/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper points out that existing adaptive methods (especially for the methods designing second-order momentum estimates in a exponentially moving average fashion) do not consider gradient decrease information and this might lead to\u00a0suboptimal convergences via simple non-cvx toy example. \nBased on this observation, the authors provide a novel optimization algorithm for long-term memory of past gradients by modifying second-order momentum design. Also, they provide aconvex regret analysis and convergence analysis for non-convex optimization. Finally, the authors evaluate their methods on various deep learning problems.\n\nSignificance/Novelty: While there have been many studies on non-convergence of Adam,\u00a0raising an issue on ignoring the gradient decrease information seems novel. \n\nPros:\n1. The motivating toy example in Section 3 is useful for readers to get intuitions.\n\n2. By introducing long-term memory on past-gradients, the authors fix the Adam's issues and they can also improve the convergence rate in a non-convex optimization (Corollary 4.2).\n\n3. Empirical studies show superiority to original Adam (Section 5).\n\nCons:\nWhile they provide a significant study on Adam's failure and a novel optimization algorithm, I have several concerns:\n1. What is default hyperparameters for AdaX in Algorithm 1? Is it the same as AdaX-W (Algorithm 3) in Appendix? The bias correction term in the line 7 of Algorithm 1 will be very large even with small $\\beta_2$ since it is expoential (For example, (1 + 0.0001)^(100000) ~ 20000 for $\\beta_2$ = 10^(-4)). So, it is not clear that the second momentum estimate of Ada-X is really stable. For this, it would be interesting to see how the trajectories of second-order momentum estimates of Adam, AMSGrad, Ada-X are different. I think this will help to understand Ada-X better.\n\n2. In terms of theory, I think the Lemma 4.1 is inevitable for convergence guarantees in Theorem 4.1 and Theorem 4.2. Although the authors effectively remove log T in the numerator in Corollary 3.2 of Chen et al. (2019) using their lemma 4.1 (I think this is the key point), the assumption that $\\beta_{2t} = \\beta_2 / t$ seems quite strong, and original Adam paper has no such assumptions. For a real deep learning problems such as training ResNet on CIFAR-10, the $\\beta_{2t}$ is almost zero after even one or two epochs where Ada-X behaves like vanilla SGD. Is there no room for relaxing this assumption such as $\\beta_{2t} = \\beta_2 / \\sqrt{t}$? Also, it is not clear how the authors derive Corollary 4.2 from Theorem 4.2 since Theorem 4.2 assumes $\\beta_{2t} = \\beta_2 / t$ while Corollary 4.2 does not.\n\n3. In the experiment, it is not clear that the authors use the same strategy for constructing first-order momentum for Adam with a newly introduced parameter $\\beta_3$. In other words, the authors should use the same policy on constructing the first-order momentum estimate for both Adam and Ada-X. Also, as the authors add an additional hyperparameter $\\beta_3$, the effect of $\\beta_3$ on performance should be discussed at least empirically.\n\n4. There are many studies on fixing poor generalization of adaptive methods (such as AdaBound which the authors cited). In this context, Zaheer et al. (2018, Adaptive methods for non-convex optimization) propose a large epsilon value (numerical stability parameter) such as $\\epsilon = 10^{-3}$ for better generalization. It will be more interesting to see the comparisons in this regime.\n\n5. In my experience with Adam-W (Decoupled weight decay regularization), Adam-W requires a relatively large weight decay parameter $\\lambda$.\nAs an example, DenseNet-BC-100-12 shows a similar validation accuracy with Adam-W $\\lambda = 0.05$ under the learning rate scheduling in (Huang et al. 2016, DenseNet)  as vanilla SGD.\nTherefore, the authors should consider more broader range of weight decay parameters for at least image classification tasks.\n\nMinor:\n1. In eq (2), the domain of x should be mentioned: according to Reddie et al, it is [-1,1].\n2. In both theorem 4.1 and corollary 4.1, $D_{\\infty^2}$ should be $D_{\\infty}^2$?"}, "signatures": ["ICLR.cc/2020/Conference/Paper697/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper697/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "AdaX: Adaptive Gradient Descent with Exponential Long Term Memory", "authors": ["Wenjie Li", "Zhaoyang Zhang", "Xinjiang Wang", "Ping Luo"], "authorids": ["li3549@purdue.edu", "zhaoyangzhang@link.cuhk.edu.hk", "swanxinjiang@gmail.com", "pluo.lhi@gmail.com"], "keywords": ["Optimization Algorithm", "Machine Learning", "Deep Learning", "Adam"], "TL;DR": "A novel adaptive algorithm with extraordinary performance in deep learning tasks.", "abstract": "Adaptive optimization algorithms such as RMSProp and Adam have fast convergence and smooth learning process. Despite their successes, they are proven to have non-convergence issue even in convex optimization problems as well as weak performance compared with the first order gradient methods such as stochastic gradient descent (SGD). Several other algorithms, for example AMSGrad and AdaShift, have been proposed to alleviate these issues but only minor effect has been observed. This paper further analyzes the performance of such algorithms in a non-convex setting by extending their non-convergence issue into a simple non-convex case and show that Adam's design of update steps would possibly lead the algorithm to local minimums. To address the above problems, we propose a novel adaptive gradient descent algorithm, named AdaX, which accumulates the long-term past gradient information exponentially. We prove the convergence of AdaX in both convex and non-convex settings. Extensive experiments show that AdaX outperforms Adam in various tasks of computer vision and natural language processing and can catch up with SGD.\n", "pdf": "/pdf/6a6dc5c31d5a57b8fe48696a4d5e0256c0107447.pdf", "paperhash": "li|adax_adaptive_gradient_descent_with_exponential_long_term_memory", "original_pdf": "/attachment/fdd5df6539ab38b58cb7c4c61fd43d0b875a69bf.pdf", "_bibtex": "@misc{\nli2020adax,\ntitle={AdaX: Adaptive Gradient Descent with Exponential Long Term Memory},\nauthor={Wenjie Li and Zhaoyang Zhang and Xinjiang Wang and Ping Luo},\nyear={2020},\nurl={https://openreview.net/forum?id=r1l-5pEtDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "r1l-5pEtDr", "replyto": "r1l-5pEtDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper697/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper697/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575708126839, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper697/Reviewers"], "noninvitees": [], "tcdate": 1570237748396, "tmdate": 1575708126851, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper697/-/Official_Review"}}}], "count": 10}