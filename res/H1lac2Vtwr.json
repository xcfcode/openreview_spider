{"notes": [{"id": "H1lac2Vtwr", "original": "SJg0YxPlwH", "number": 133, "cdate": 1569438869498, "ddate": null, "tcdate": 1569438869498, "tmdate": 1577168287631, "tddate": null, "forum": "H1lac2Vtwr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "SesameBERT: Attention for Anywhere", "authors": ["Ta-Chun Su", "Hsiang-Chih Cheng"], "authorids": ["gene11117@gmail.com", "musicmilif@gmail.com"], "keywords": ["Natural Language Processing", "Deep Learning", "Self Attention"], "TL;DR": "We proposed SesameBERT, a generalized fine-tuning method that enables the extraction of global information among all layers through Squeeze and Excitation and enriches local information by capturing neighboring contexts via Gaussian blurring.", "abstract": "Fine-tuning with pre-trained models has achieved exceptional results for many language tasks. In this study, we focused on one such self-attention network model, namely BERT, which has performed well in terms of stacking layers across diverse language-understanding benchmarks. However, in many downstream tasks, information between layers is ignored by BERT for fine-tuning. In addition, although self-attention networks are well-known for their ability to capture global dependencies, room for improvement remains in terms of emphasizing the importance of local contexts. In light of these advantages and disadvantages, this paper proposes SesameBERT, a generalized fine-tuning method that (1) enables the extraction of global information among all layers through Squeeze and Excitation and (2) enriches local information by capturing neighboring contexts via Gaussian blurring. Furthermore, we demonstrated the effectiveness of our approach in the HANS dataset, which is used to determine whether models have adopted shallow heuristics instead of learning underlying generalizations. The experiments revealed that SesameBERT outperformed BERT with respect to GLUE benchmark and the HANS evaluation set.", "pdf": "/pdf/b5dbd88c368f6ceb7e3fe869e0f8b68047c0b980.pdf", "code": "https://github.com/ICLR2020Sesame/SesameBert", "paperhash": "su|sesamebert_attention_for_anywhere", "original_pdf": "/attachment/b5dbd88c368f6ceb7e3fe869e0f8b68047c0b980.pdf", "_bibtex": "@misc{\nsu2020sesamebert,\ntitle={Sesame{\\{}BERT{\\}}: Attention for Anywhere},\nauthor={Ta-Chun Su and Hsiang-Chih Cheng},\nyear={2020},\nurl={https://openreview.net/forum?id=H1lac2Vtwr}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "Q_EUd531i", "original": null, "number": 1, "cdate": 1576798688354, "ddate": null, "tcdate": 1576798688354, "tmdate": 1576800946736, "tddate": null, "forum": "H1lac2Vtwr", "replyto": "H1lac2Vtwr", "invitation": "ICLR.cc/2020/Conference/Paper133/-/Decision", "content": {"decision": "Reject", "comment": "This paper proposes a few architectural modifications to the BERT model for language understanding, which are meant to apply during fine-tuning for target tasks. \n\nAll three reviewers had concerns about the motivation for at least one of the proposed methods, and none of three reviewers found the primary experimental results convincing: The proposed methods yield a small improvement on average across target tasks, but one that is not consistent across tasks, and that may not be statistically significant.\n\nThe authors clarified some points, but did not substantially rebut any of the reviewers concerns. Even though the reviewers express relatively low confidence, their concerns sound serious and uncontested, so I don't think we can accept this paper as is.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SesameBERT: Attention for Anywhere", "authors": ["Ta-Chun Su", "Hsiang-Chih Cheng"], "authorids": ["gene11117@gmail.com", "musicmilif@gmail.com"], "keywords": ["Natural Language Processing", "Deep Learning", "Self Attention"], "TL;DR": "We proposed SesameBERT, a generalized fine-tuning method that enables the extraction of global information among all layers through Squeeze and Excitation and enriches local information by capturing neighboring contexts via Gaussian blurring.", "abstract": "Fine-tuning with pre-trained models has achieved exceptional results for many language tasks. In this study, we focused on one such self-attention network model, namely BERT, which has performed well in terms of stacking layers across diverse language-understanding benchmarks. However, in many downstream tasks, information between layers is ignored by BERT for fine-tuning. In addition, although self-attention networks are well-known for their ability to capture global dependencies, room for improvement remains in terms of emphasizing the importance of local contexts. In light of these advantages and disadvantages, this paper proposes SesameBERT, a generalized fine-tuning method that (1) enables the extraction of global information among all layers through Squeeze and Excitation and (2) enriches local information by capturing neighboring contexts via Gaussian blurring. Furthermore, we demonstrated the effectiveness of our approach in the HANS dataset, which is used to determine whether models have adopted shallow heuristics instead of learning underlying generalizations. The experiments revealed that SesameBERT outperformed BERT with respect to GLUE benchmark and the HANS evaluation set.", "pdf": "/pdf/b5dbd88c368f6ceb7e3fe869e0f8b68047c0b980.pdf", "code": "https://github.com/ICLR2020Sesame/SesameBert", "paperhash": "su|sesamebert_attention_for_anywhere", "original_pdf": "/attachment/b5dbd88c368f6ceb7e3fe869e0f8b68047c0b980.pdf", "_bibtex": "@misc{\nsu2020sesamebert,\ntitle={Sesame{\\{}BERT{\\}}: Attention for Anywhere},\nauthor={Ta-Chun Su and Hsiang-Chih Cheng},\nyear={2020},\nurl={https://openreview.net/forum?id=H1lac2Vtwr}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "H1lac2Vtwr", "replyto": "H1lac2Vtwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795730351, "tmdate": 1576800283128, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper133/-/Decision"}}}, {"id": "r1eSgtSIir", "original": null, "number": 6, "cdate": 1573439724936, "ddate": null, "tcdate": 1573439724936, "tmdate": 1573440058751, "tddate": null, "forum": "H1lac2Vtwr", "replyto": "H1lac2Vtwr", "invitation": "ICLR.cc/2020/Conference/Paper133/-/Official_Comment", "content": {"title": "Official response to a common comment", "comment": "We thank the reviewer for the detailed comments. In what follows, we address in detail the raised issues. Here we explain some common questions.\n\n1. In this paper, because the adjustment is on fine-tuning process related to BERT, the results on GLUE score are not that significant, although there are some significant improvement on some GLUE tasks. Here we propose to use a more comprehensive and innovative way in dealing with fine-tuning process. Also, we're applying this approaches not only on BERT but also other models, including XLNet, in the process. This comparison might be done in the future. \n\n2. In the paper \"Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference\", there are some detailed sentence examples in each evaluation metrics. In our paper, here we just show the superficial accuracy on HANS dataset, we will put the detailed results accuracy on each examples in the final version. In sum, we assume that blur method could prevent those sentences look similar but are in different meaning. We will do further research on how blur method influence the sentence accuracy variance in the future. "}, "signatures": ["ICLR.cc/2020/Conference/Paper133/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper133/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SesameBERT: Attention for Anywhere", "authors": ["Ta-Chun Su", "Hsiang-Chih Cheng"], "authorids": ["gene11117@gmail.com", "musicmilif@gmail.com"], "keywords": ["Natural Language Processing", "Deep Learning", "Self Attention"], "TL;DR": "We proposed SesameBERT, a generalized fine-tuning method that enables the extraction of global information among all layers through Squeeze and Excitation and enriches local information by capturing neighboring contexts via Gaussian blurring.", "abstract": "Fine-tuning with pre-trained models has achieved exceptional results for many language tasks. In this study, we focused on one such self-attention network model, namely BERT, which has performed well in terms of stacking layers across diverse language-understanding benchmarks. However, in many downstream tasks, information between layers is ignored by BERT for fine-tuning. In addition, although self-attention networks are well-known for their ability to capture global dependencies, room for improvement remains in terms of emphasizing the importance of local contexts. In light of these advantages and disadvantages, this paper proposes SesameBERT, a generalized fine-tuning method that (1) enables the extraction of global information among all layers through Squeeze and Excitation and (2) enriches local information by capturing neighboring contexts via Gaussian blurring. Furthermore, we demonstrated the effectiveness of our approach in the HANS dataset, which is used to determine whether models have adopted shallow heuristics instead of learning underlying generalizations. The experiments revealed that SesameBERT outperformed BERT with respect to GLUE benchmark and the HANS evaluation set.", "pdf": "/pdf/b5dbd88c368f6ceb7e3fe869e0f8b68047c0b980.pdf", "code": "https://github.com/ICLR2020Sesame/SesameBert", "paperhash": "su|sesamebert_attention_for_anywhere", "original_pdf": "/attachment/b5dbd88c368f6ceb7e3fe869e0f8b68047c0b980.pdf", "_bibtex": "@misc{\nsu2020sesamebert,\ntitle={Sesame{\\{}BERT{\\}}: Attention for Anywhere},\nauthor={Ta-Chun Su and Hsiang-Chih Cheng},\nyear={2020},\nurl={https://openreview.net/forum?id=H1lac2Vtwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1lac2Vtwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper133/Authors", "ICLR.cc/2020/Conference/Paper133/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper133/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper133/Reviewers", "ICLR.cc/2020/Conference/Paper133/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper133/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper133/Authors|ICLR.cc/2020/Conference/Paper133/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504175858, "tmdate": 1576860532314, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper133/Authors", "ICLR.cc/2020/Conference/Paper133/Reviewers", "ICLR.cc/2020/Conference/Paper133/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper133/-/Official_Comment"}}}, {"id": "H1enF_SUir", "original": null, "number": 5, "cdate": 1573439620325, "ddate": null, "tcdate": 1573439620325, "tmdate": 1573439833325, "tddate": null, "forum": "H1lac2Vtwr", "replyto": "r1g_66GAKr", "invitation": "ICLR.cc/2020/Conference/Paper133/-/Official_Comment", "content": {"title": "Answer to Reviewer #2", "comment": "We thank the reviewer for the detailed comments. In what follows, we address in detail the raised issues.\n\n1. Thanks for your advice. We will do more comprehensive research in the future.\n\n2\u30013. Like we've mentioned in official response.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper133/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper133/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SesameBERT: Attention for Anywhere", "authors": ["Ta-Chun Su", "Hsiang-Chih Cheng"], "authorids": ["gene11117@gmail.com", "musicmilif@gmail.com"], "keywords": ["Natural Language Processing", "Deep Learning", "Self Attention"], "TL;DR": "We proposed SesameBERT, a generalized fine-tuning method that enables the extraction of global information among all layers through Squeeze and Excitation and enriches local information by capturing neighboring contexts via Gaussian blurring.", "abstract": "Fine-tuning with pre-trained models has achieved exceptional results for many language tasks. In this study, we focused on one such self-attention network model, namely BERT, which has performed well in terms of stacking layers across diverse language-understanding benchmarks. However, in many downstream tasks, information between layers is ignored by BERT for fine-tuning. In addition, although self-attention networks are well-known for their ability to capture global dependencies, room for improvement remains in terms of emphasizing the importance of local contexts. In light of these advantages and disadvantages, this paper proposes SesameBERT, a generalized fine-tuning method that (1) enables the extraction of global information among all layers through Squeeze and Excitation and (2) enriches local information by capturing neighboring contexts via Gaussian blurring. Furthermore, we demonstrated the effectiveness of our approach in the HANS dataset, which is used to determine whether models have adopted shallow heuristics instead of learning underlying generalizations. The experiments revealed that SesameBERT outperformed BERT with respect to GLUE benchmark and the HANS evaluation set.", "pdf": "/pdf/b5dbd88c368f6ceb7e3fe869e0f8b68047c0b980.pdf", "code": "https://github.com/ICLR2020Sesame/SesameBert", "paperhash": "su|sesamebert_attention_for_anywhere", "original_pdf": "/attachment/b5dbd88c368f6ceb7e3fe869e0f8b68047c0b980.pdf", "_bibtex": "@misc{\nsu2020sesamebert,\ntitle={Sesame{\\{}BERT{\\}}: Attention for Anywhere},\nauthor={Ta-Chun Su and Hsiang-Chih Cheng},\nyear={2020},\nurl={https://openreview.net/forum?id=H1lac2Vtwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1lac2Vtwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper133/Authors", "ICLR.cc/2020/Conference/Paper133/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper133/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper133/Reviewers", "ICLR.cc/2020/Conference/Paper133/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper133/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper133/Authors|ICLR.cc/2020/Conference/Paper133/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504175858, "tmdate": 1576860532314, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper133/Authors", "ICLR.cc/2020/Conference/Paper133/Reviewers", "ICLR.cc/2020/Conference/Paper133/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper133/-/Official_Comment"}}}, {"id": "rJlbEDBLjr", "original": null, "number": 4, "cdate": 1573439272598, "ddate": null, "tcdate": 1573439272598, "tmdate": 1573439780600, "tddate": null, "forum": "H1lac2Vtwr", "replyto": "rJezoUU0tr", "invitation": "ICLR.cc/2020/Conference/Paper133/-/Official_Comment", "content": {"title": "Answer to Reviewer #3", "comment": "We thank the reviewer for the detailed comments. In what follows, we address in detail the raised issues.\n\n1\u30013. Like we've mentioned in official response.\n\n2. Sorry, not pretty sure what you mean.\n\n4. We first talked GLUE results on the section because we wanted to prove that the significant improvement on HANS dataset was based on a model with similar accuracy on GLUE tasks. We will highlight more on HANS dataset in the final version."}, "signatures": ["ICLR.cc/2020/Conference/Paper133/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper133/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SesameBERT: Attention for Anywhere", "authors": ["Ta-Chun Su", "Hsiang-Chih Cheng"], "authorids": ["gene11117@gmail.com", "musicmilif@gmail.com"], "keywords": ["Natural Language Processing", "Deep Learning", "Self Attention"], "TL;DR": "We proposed SesameBERT, a generalized fine-tuning method that enables the extraction of global information among all layers through Squeeze and Excitation and enriches local information by capturing neighboring contexts via Gaussian blurring.", "abstract": "Fine-tuning with pre-trained models has achieved exceptional results for many language tasks. In this study, we focused on one such self-attention network model, namely BERT, which has performed well in terms of stacking layers across diverse language-understanding benchmarks. However, in many downstream tasks, information between layers is ignored by BERT for fine-tuning. In addition, although self-attention networks are well-known for their ability to capture global dependencies, room for improvement remains in terms of emphasizing the importance of local contexts. In light of these advantages and disadvantages, this paper proposes SesameBERT, a generalized fine-tuning method that (1) enables the extraction of global information among all layers through Squeeze and Excitation and (2) enriches local information by capturing neighboring contexts via Gaussian blurring. Furthermore, we demonstrated the effectiveness of our approach in the HANS dataset, which is used to determine whether models have adopted shallow heuristics instead of learning underlying generalizations. The experiments revealed that SesameBERT outperformed BERT with respect to GLUE benchmark and the HANS evaluation set.", "pdf": "/pdf/b5dbd88c368f6ceb7e3fe869e0f8b68047c0b980.pdf", "code": "https://github.com/ICLR2020Sesame/SesameBert", "paperhash": "su|sesamebert_attention_for_anywhere", "original_pdf": "/attachment/b5dbd88c368f6ceb7e3fe869e0f8b68047c0b980.pdf", "_bibtex": "@misc{\nsu2020sesamebert,\ntitle={Sesame{\\{}BERT{\\}}: Attention for Anywhere},\nauthor={Ta-Chun Su and Hsiang-Chih Cheng},\nyear={2020},\nurl={https://openreview.net/forum?id=H1lac2Vtwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1lac2Vtwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper133/Authors", "ICLR.cc/2020/Conference/Paper133/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper133/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper133/Reviewers", "ICLR.cc/2020/Conference/Paper133/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper133/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper133/Authors|ICLR.cc/2020/Conference/Paper133/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504175858, "tmdate": 1576860532314, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper133/Authors", "ICLR.cc/2020/Conference/Paper133/Reviewers", "ICLR.cc/2020/Conference/Paper133/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper133/-/Official_Comment"}}}, {"id": "Hyl-ENSUjB", "original": null, "number": 3, "cdate": 1573438505198, "ddate": null, "tcdate": 1573438505198, "tmdate": 1573439758163, "tddate": null, "forum": "H1lac2Vtwr", "replyto": "rkeEfJhCKS", "invitation": "ICLR.cc/2020/Conference/Paper133/-/Official_Comment", "content": {"title": "Answer to Reviewer #1 ", "comment": "We thank the reviewer for the detailed comments. In what follows, we address in detail the raised issues.\n\n1. Most paper only revealed averaged results rather than the variance. We've released our code on the gitHub, you may run the variance in each GLUE tasks through our approaches. In our paper, we've run the results in 10 random seeds, and picking up the top 5 metrics then averaged them. Although the final GLUE score is not that significant, there are some obvious difference in some GLUE tasks. \n\n2. Like we've mentioned in official response.\n\n3. Because the results in left fig of Figure 3 are started from 0.9X, we rescale the x-axis to make it visible. We will edit the figure in final version, thanks!"}, "signatures": ["ICLR.cc/2020/Conference/Paper133/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper133/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SesameBERT: Attention for Anywhere", "authors": ["Ta-Chun Su", "Hsiang-Chih Cheng"], "authorids": ["gene11117@gmail.com", "musicmilif@gmail.com"], "keywords": ["Natural Language Processing", "Deep Learning", "Self Attention"], "TL;DR": "We proposed SesameBERT, a generalized fine-tuning method that enables the extraction of global information among all layers through Squeeze and Excitation and enriches local information by capturing neighboring contexts via Gaussian blurring.", "abstract": "Fine-tuning with pre-trained models has achieved exceptional results for many language tasks. In this study, we focused on one such self-attention network model, namely BERT, which has performed well in terms of stacking layers across diverse language-understanding benchmarks. However, in many downstream tasks, information between layers is ignored by BERT for fine-tuning. In addition, although self-attention networks are well-known for their ability to capture global dependencies, room for improvement remains in terms of emphasizing the importance of local contexts. In light of these advantages and disadvantages, this paper proposes SesameBERT, a generalized fine-tuning method that (1) enables the extraction of global information among all layers through Squeeze and Excitation and (2) enriches local information by capturing neighboring contexts via Gaussian blurring. Furthermore, we demonstrated the effectiveness of our approach in the HANS dataset, which is used to determine whether models have adopted shallow heuristics instead of learning underlying generalizations. The experiments revealed that SesameBERT outperformed BERT with respect to GLUE benchmark and the HANS evaluation set.", "pdf": "/pdf/b5dbd88c368f6ceb7e3fe869e0f8b68047c0b980.pdf", "code": "https://github.com/ICLR2020Sesame/SesameBert", "paperhash": "su|sesamebert_attention_for_anywhere", "original_pdf": "/attachment/b5dbd88c368f6ceb7e3fe869e0f8b68047c0b980.pdf", "_bibtex": "@misc{\nsu2020sesamebert,\ntitle={Sesame{\\{}BERT{\\}}: Attention for Anywhere},\nauthor={Ta-Chun Su and Hsiang-Chih Cheng},\nyear={2020},\nurl={https://openreview.net/forum?id=H1lac2Vtwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1lac2Vtwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper133/Authors", "ICLR.cc/2020/Conference/Paper133/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper133/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper133/Reviewers", "ICLR.cc/2020/Conference/Paper133/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper133/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper133/Authors|ICLR.cc/2020/Conference/Paper133/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504175858, "tmdate": 1576860532314, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper133/Authors", "ICLR.cc/2020/Conference/Paper133/Reviewers", "ICLR.cc/2020/Conference/Paper133/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper133/-/Official_Comment"}}}, {"id": "rJezoUU0tr", "original": null, "number": 2, "cdate": 1571870362008, "ddate": null, "tcdate": 1571870362008, "tmdate": 1572972634589, "tddate": null, "forum": "H1lac2Vtwr", "replyto": "H1lac2Vtwr", "invitation": "ICLR.cc/2020/Conference/Paper133/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposes fine-tune methodologies for BERT-like models (namely, SeasameBERT).  This includes a method that considers all BERT layers and captures local information via Gaussian blurring. The methods were evaluated on several baseline datasets (e.g., GLUE, HANS)\n\nStrengths: \n\n* The paper is easy to follow. \n\n*  Squeeze-and-extraction was used to incorporate all hidden layers instead of the common-practice of averaging last 4-layers. I find it both logical and useful. \n\n* The suggested gaussian blurring method is able to capture local dependencies, which is missing in attention-based transformer layer.\n\n*  SesameBERT improves performance on some GLUE metrics and on HANS dataset. Also ablation analysis suggests squeeze-and-extraction is a good technique to extract features from BERT model compared to other common practices. \u2028\n\n\nWeaknesses:\n\n* In my opinion, the paper novelty is not significant enough. Although useful, the suggested techniques are based on existing methods. \n\n*  Incorporate spatial/context-information is usually done by concatenating a location-based embedding with the original word embedding. I\u2019m curious if the blurring Gaussian will be as useful compared to such version. \n\n* Since the suggested methods are generic, It can be more convincing to see results on recent models, and not only BERT. Currently, the results are not significantly better.  \n\n* The HANS DATASET RESULTS section seems rushed, will be good to elaborate more about HANS. also the first sentences of the section discusses GLUE results not HANS. \n\nTo conclude: The paper is easy to follow, suggests two nice methods for fine-tune BERT. But although useful, the suggested methods are not novel enough. The performance does not significantly improves, and the methods are applied only to BERT model. "}, "signatures": ["ICLR.cc/2020/Conference/Paper133/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper133/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SesameBERT: Attention for Anywhere", "authors": ["Ta-Chun Su", "Hsiang-Chih Cheng"], "authorids": ["gene11117@gmail.com", "musicmilif@gmail.com"], "keywords": ["Natural Language Processing", "Deep Learning", "Self Attention"], "TL;DR": "We proposed SesameBERT, a generalized fine-tuning method that enables the extraction of global information among all layers through Squeeze and Excitation and enriches local information by capturing neighboring contexts via Gaussian blurring.", "abstract": "Fine-tuning with pre-trained models has achieved exceptional results for many language tasks. In this study, we focused on one such self-attention network model, namely BERT, which has performed well in terms of stacking layers across diverse language-understanding benchmarks. However, in many downstream tasks, information between layers is ignored by BERT for fine-tuning. In addition, although self-attention networks are well-known for their ability to capture global dependencies, room for improvement remains in terms of emphasizing the importance of local contexts. In light of these advantages and disadvantages, this paper proposes SesameBERT, a generalized fine-tuning method that (1) enables the extraction of global information among all layers through Squeeze and Excitation and (2) enriches local information by capturing neighboring contexts via Gaussian blurring. Furthermore, we demonstrated the effectiveness of our approach in the HANS dataset, which is used to determine whether models have adopted shallow heuristics instead of learning underlying generalizations. The experiments revealed that SesameBERT outperformed BERT with respect to GLUE benchmark and the HANS evaluation set.", "pdf": "/pdf/b5dbd88c368f6ceb7e3fe869e0f8b68047c0b980.pdf", "code": "https://github.com/ICLR2020Sesame/SesameBert", "paperhash": "su|sesamebert_attention_for_anywhere", "original_pdf": "/attachment/b5dbd88c368f6ceb7e3fe869e0f8b68047c0b980.pdf", "_bibtex": "@misc{\nsu2020sesamebert,\ntitle={Sesame{\\{}BERT{\\}}: Attention for Anywhere},\nauthor={Ta-Chun Su and Hsiang-Chih Cheng},\nyear={2020},\nurl={https://openreview.net/forum?id=H1lac2Vtwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "H1lac2Vtwr", "replyto": "H1lac2Vtwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper133/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper133/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574906011811, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper133/Reviewers"], "noninvitees": [], "tcdate": 1570237756586, "tmdate": 1574906011825, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper133/-/Official_Review"}}}, {"id": "rkeEfJhCKS", "original": null, "number": 3, "cdate": 1571893003874, "ddate": null, "tcdate": 1571893003874, "tmdate": 1572972634547, "tddate": null, "forum": "H1lac2Vtwr", "replyto": "H1lac2Vtwr", "invitation": "ICLR.cc/2020/Conference/Paper133/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "Summary:\nThe paper proposes adding two mechanisms to the BERT architecture for NLU. The first is based on integrating information from all layers of the encoder via a method called Squeeze and Excitation. The second uses Gaussian blurring to encourage information sharing among neighboring words. The proposed method improves modestly on BERT on the GLUE suite of problems. It also substantially improves on BERT with respect to a class of examples that are designed to confound models that learn superficial heuristics based on word occurrence.\n\nI learn toward rejecting this paper. The method shows some performance gains over BERT on some GLUE tasks, but these are fairly small for the most part, and BERT outperforms the proposed method by a similar amount on a similar number of tasks. The strongest result is the HANS \"lexical_overlap\" case, where the proposed method has a clear advantage. I have no experience with these kinds of NLU models, so I can't say with confidence whether the architectural additions proposed are well-motivated, but to me it feels like there is not a strong justification for adding these particular features to the BERT architecture, and the results do not clearly demonstrate their utility except in the \"lexical_overlap\" case.\n\nDetails / Questions:\n* It seems to me that the GLUE results might be within the margin of error. Is it feasible to replicate training with different random seeds to see what the variance in the performance numbers might be? I suspect that a statistical analysis [1] might conclude that BERT and the proposed method are indistinguishable on the GLUE suite.\n\n* Were the proposed architectural additions conceived with the HANS \"counterexamples\" in mind (i.e. is there a specific reason to think that these types of methods would avoid the \"superficial\" reasoning that these examples are supposed to reveal)? Were other methods of adding context considered?\n\n* I suggest using the same x-axis scale on the two charts in Figure 3 to avoid confusion about the magnitudes of the differences.\n\nReferences:\n[1] Dem\u0161ar, J. (2006). Statistical comparisons of classifiers over multiple data sets. Journal of Machine Learning Research, 7(Jan), 1-30."}, "signatures": ["ICLR.cc/2020/Conference/Paper133/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper133/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SesameBERT: Attention for Anywhere", "authors": ["Ta-Chun Su", "Hsiang-Chih Cheng"], "authorids": ["gene11117@gmail.com", "musicmilif@gmail.com"], "keywords": ["Natural Language Processing", "Deep Learning", "Self Attention"], "TL;DR": "We proposed SesameBERT, a generalized fine-tuning method that enables the extraction of global information among all layers through Squeeze and Excitation and enriches local information by capturing neighboring contexts via Gaussian blurring.", "abstract": "Fine-tuning with pre-trained models has achieved exceptional results for many language tasks. In this study, we focused on one such self-attention network model, namely BERT, which has performed well in terms of stacking layers across diverse language-understanding benchmarks. However, in many downstream tasks, information between layers is ignored by BERT for fine-tuning. In addition, although self-attention networks are well-known for their ability to capture global dependencies, room for improvement remains in terms of emphasizing the importance of local contexts. In light of these advantages and disadvantages, this paper proposes SesameBERT, a generalized fine-tuning method that (1) enables the extraction of global information among all layers through Squeeze and Excitation and (2) enriches local information by capturing neighboring contexts via Gaussian blurring. Furthermore, we demonstrated the effectiveness of our approach in the HANS dataset, which is used to determine whether models have adopted shallow heuristics instead of learning underlying generalizations. The experiments revealed that SesameBERT outperformed BERT with respect to GLUE benchmark and the HANS evaluation set.", "pdf": "/pdf/b5dbd88c368f6ceb7e3fe869e0f8b68047c0b980.pdf", "code": "https://github.com/ICLR2020Sesame/SesameBert", "paperhash": "su|sesamebert_attention_for_anywhere", "original_pdf": "/attachment/b5dbd88c368f6ceb7e3fe869e0f8b68047c0b980.pdf", "_bibtex": "@misc{\nsu2020sesamebert,\ntitle={Sesame{\\{}BERT{\\}}: Attention for Anywhere},\nauthor={Ta-Chun Su and Hsiang-Chih Cheng},\nyear={2020},\nurl={https://openreview.net/forum?id=H1lac2Vtwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "H1lac2Vtwr", "replyto": "H1lac2Vtwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper133/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper133/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574906011811, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper133/Reviewers"], "noninvitees": [], "tcdate": 1570237756586, "tmdate": 1574906011825, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper133/-/Official_Review"}}}, {"id": "r1g_66GAKr", "original": null, "number": 1, "cdate": 1571855808238, "ddate": null, "tcdate": 1571855808238, "tmdate": 1572972634493, "tddate": null, "forum": "H1lac2Vtwr", "replyto": "H1lac2Vtwr", "invitation": "ICLR.cc/2020/Conference/Paper133/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proposes a novel BERT based neural architecture, SESAME-BERT, which consists of \u201cSqueeze and Excitation\u201d method and Gaussian blurring. \u201cSqueeze and Excitation\u201d method extracts features from BERT by calculating a weighted sum of layers in BERT to feed the feature vectors to a downstream classifier. To capture the local context of a word, they apply Gaussian blurring on output layers of the self-attention layer in BERT. The authors show their model\u2019s performance on GLUE and HANS dataset.\n\nStrengths\n*This paper claims the importance of the local context of a word and shows an effect of their method on the various datasets: GLUE, and HANS.\n\nWeaknesses\n* It seems like the self-attention layer can learn the local context information. Finding important words and predicts contextual vector representation of a word is what self-attention does.\nSo, if using local-context information, which is information in important near words, is an important feature for some downstream tasks, then the self-attention layer can learn such important near words by training the key, query, and value weight parameters to connect the near important words.\nIt would be nice if the authors provide some evidence that self-attention can't learn such a local-context feature.\n\n*In table 1, their experimental results show a slight improvement by using their method, but it's not significant.\n\n* On HANS dataset, they show using local-context can prevent models from easily adopting heuristics. How Gaussian blurring can prevent that problem? More explanation about the relation between local-context and adopting heuristics is required.\n\n\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper133/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper133/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SesameBERT: Attention for Anywhere", "authors": ["Ta-Chun Su", "Hsiang-Chih Cheng"], "authorids": ["gene11117@gmail.com", "musicmilif@gmail.com"], "keywords": ["Natural Language Processing", "Deep Learning", "Self Attention"], "TL;DR": "We proposed SesameBERT, a generalized fine-tuning method that enables the extraction of global information among all layers through Squeeze and Excitation and enriches local information by capturing neighboring contexts via Gaussian blurring.", "abstract": "Fine-tuning with pre-trained models has achieved exceptional results for many language tasks. In this study, we focused on one such self-attention network model, namely BERT, which has performed well in terms of stacking layers across diverse language-understanding benchmarks. However, in many downstream tasks, information between layers is ignored by BERT for fine-tuning. In addition, although self-attention networks are well-known for their ability to capture global dependencies, room for improvement remains in terms of emphasizing the importance of local contexts. In light of these advantages and disadvantages, this paper proposes SesameBERT, a generalized fine-tuning method that (1) enables the extraction of global information among all layers through Squeeze and Excitation and (2) enriches local information by capturing neighboring contexts via Gaussian blurring. Furthermore, we demonstrated the effectiveness of our approach in the HANS dataset, which is used to determine whether models have adopted shallow heuristics instead of learning underlying generalizations. The experiments revealed that SesameBERT outperformed BERT with respect to GLUE benchmark and the HANS evaluation set.", "pdf": "/pdf/b5dbd88c368f6ceb7e3fe869e0f8b68047c0b980.pdf", "code": "https://github.com/ICLR2020Sesame/SesameBert", "paperhash": "su|sesamebert_attention_for_anywhere", "original_pdf": "/attachment/b5dbd88c368f6ceb7e3fe869e0f8b68047c0b980.pdf", "_bibtex": "@misc{\nsu2020sesamebert,\ntitle={Sesame{\\{}BERT{\\}}: Attention for Anywhere},\nauthor={Ta-Chun Su and Hsiang-Chih Cheng},\nyear={2020},\nurl={https://openreview.net/forum?id=H1lac2Vtwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "H1lac2Vtwr", "replyto": "H1lac2Vtwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper133/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper133/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574906011811, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper133/Reviewers"], "noninvitees": [], "tcdate": 1570237756586, "tmdate": 1574906011825, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper133/-/Official_Review"}}}], "count": 9}