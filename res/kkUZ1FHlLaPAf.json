{"notes": [{"ddate": null, "legacy_migration": true, "tmdate": 1394937240000, "tcdate": 1394937240000, "number": 8, "id": "cOEBOC2QB3OyI", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "kkUZ1FHlLaPAf", "replyto": "kkUZ1FHlLaPAf", "signatures": ["Daniel Silver"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "My thanks to the reviewers for their great comments.    My apologies for not providing feedback sooner.  New responsibilities had to be dealt with in Jan and Feb.  This having been said, we are now moving forward with the advancement of this initial research, most notably the use of multiple mixed modalities (ear - audio in, voice - audio out, eye - image in, drawing - image out).   One item I will clarify for the reviewers -- A method such as BP that can clearly do a great job fine-tuning the weights between one of the RBM stacks and another via the top layer, does not scale well to more than two modalities.  This is the reason we use the back-fitting fine-tuning method - it trains the recognition weights local to each RBM stack and the top layer - so it does scale to multiple modalities."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Paired-associate Images with An Unsupervised Deep Learning Architecture", "decision": "submitted, no decision", "abstract": "This paper presents an unsupervised multi-modal learning system that learns associative representation from two input modalities (channels) such that input on one channel will correctly generate the associated response at the other channel and vice versa. In this way, the system develops a kind of supervised classification model meant to simulate aspects of human associative memory. The system uses a deep learning architecture (DLA) composed of two input/output channels formed from stacked Restricted Boltzmann Machines (RBM) and an associative memory network that combines the two channels. The DLA is trained on pairs of MNIST handwritten digit images to develop hierarchical features and associative representations that are able to reconstruct one image given its paired-associate. Experiments show that the multi-modal learning system generates models that are as accurate as back-propagation networks but with the advantage of unsupervised learning from either paired or non-paired training examples.", "pdf": "https://arxiv.org/abs/1312.6171", "paperhash": "wang|learning_pairedassociate_images_with_an_unsupervised_deep_learning_architecture", "keywords": [], "conflicts": [], "authors": ["Ti Wang", "Daniel L. Silver"], "authorids": ["108787w@acadiau.ca", "danny.silver@acadiau.ca"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391867760000, "tcdate": 1391867760000, "number": 7, "id": "DD_QMgXNwToHG", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "kkUZ1FHlLaPAf", "replyto": "kkUZ1FHlLaPAf", "signatures": ["anonymous reviewer c0c4"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Learning Paired-associate Images with An Unsupervised Deep Learning Architecture", "review": "The authors propose a an algorithm, in the setting of multimodal data, for learning to generate one modality given the other. The algorithm contains stack of RBM's for each modality, a join RBM on the top and then a predictor of joint probabilities from one modality. The experiments are too weak to demonstrate strength of the proposed approach.\r\n\r\nNovelty: Small. A standard multimodal deep belief net with an addition of a relatively obvious idea, which is anyway quite similar to previous ideas.\r\nQuality: Experiments are too weak. \r\n\r\nDetails:\r\nFirst, the main complaint - the weakness of the experiments. Whether feedforward network Figure 6 can learn the task depends on how you optimise it. I think if you do a good optimisation then it would do it. In particular you can take your pre trained deep network Figure 4 and, go up the left and then down the right as your feedforward net - which is essentially the idea in 'Multimodal deep learning' (http://ai.stanford.edu/~ang/papers/icml11-MultimodalDeepLearning.pdf).\r\n\r\nThe task is also too simple - may be it would be at least good to pair a random digit from one class with a random digit from another. But even better (really necessary) it would be to do some different sets of data as in the above mentioned paper. \r\n\r\nOther details: \r\nPage 1, paragraph 2 - I wouldn't say deep learning typically uses RBM's. (e.g. feedforward networks, auto encoders\u2026)\r\nPage 2, Background - I don't think we have an ability to recover *complete* information. Also 'many do not work in the same fashion as the human visual system' - and which do?\u2026\r\nPage 3, 4th paragraph - you should write the cost function between the two probabilities\r\nPage 7 - What does it mean 'Features \u2026 for the purpose of mapping and not reconstruction'\r\nPage 8 - Results and Discussion - First you say that the results are not significant (within error bars) and then you draw a conclusion as if they were."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Paired-associate Images with An Unsupervised Deep Learning Architecture", "decision": "submitted, no decision", "abstract": "This paper presents an unsupervised multi-modal learning system that learns associative representation from two input modalities (channels) such that input on one channel will correctly generate the associated response at the other channel and vice versa. In this way, the system develops a kind of supervised classification model meant to simulate aspects of human associative memory. The system uses a deep learning architecture (DLA) composed of two input/output channels formed from stacked Restricted Boltzmann Machines (RBM) and an associative memory network that combines the two channels. The DLA is trained on pairs of MNIST handwritten digit images to develop hierarchical features and associative representations that are able to reconstruct one image given its paired-associate. Experiments show that the multi-modal learning system generates models that are as accurate as back-propagation networks but with the advantage of unsupervised learning from either paired or non-paired training examples.", "pdf": "https://arxiv.org/abs/1312.6171", "paperhash": "wang|learning_pairedassociate_images_with_an_unsupervised_deep_learning_architecture", "keywords": [], "conflicts": [], "authors": ["Ti Wang", "Daniel L. Silver"], "authorids": ["108787w@acadiau.ca", "danny.silver@acadiau.ca"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391379540000, "tcdate": 1391379540000, "number": 6, "id": "YP3gsIWOuRsJ5", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "kkUZ1FHlLaPAf", "replyto": "kkUZ1FHlLaPAf", "signatures": ["anonymous reviewer ded8"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Learning Paired-associate Images with An Unsupervised Deep Learning Architecture", "review": "Summary:\r\n\r\nThis paper presents a neural network that predicts a structured output. The authors proposed a new algorithm for training this structure. The new algorithm firstly pretrains the neural network as a stack of RBMs, and subsequently finetunes the model by untying the recognition and generation weights (which is as authors mention reminiscent of wake-sleep algorithm.)\r\n\r\nNovelty:\r\n\r\nUnfortunately, I don't see any novelty in the proposed approach. The exactly same neural network was proposed earlier by Ngiam et al. (2011). Also, it is very close to the multi-modal DBM proposed by Srivastava & Salakhutdinov (2012). The authors may argue that Ngiam et al. (2011) did not attempt to actually generate a missing modality, but precisely that was done by S & S (2012). Furthermore, I don't think the proposed wake-sleep-type algorithm should be better than finetuning the whole model (or each path) as an (denoising) autoencoder. \r\n\r\nPros:\r\n\r\n(1) Representation learning from multiple modalities is important.\r\n\r\nCons:\r\n\r\n(1) Experiments are weak.\r\n(2) Explanation of the methods could be done better.\r\n(3) The relationship to the previous research should be made more explicit and clear.\r\n\r\nDetailed Comments:\r\n\r\n- Sec 2. 1st paragraph: 'back-prop ANNs are ... not as good for reconstructing, or recalling a pattern' => I don't really understand this sentence. Does it mean that the backpropagation (or SGD with backprop) is not able to find a solution of a neural net even if the input and target were constructed explicitly to make the neural net reconstruct corruption or noisy input? \r\n\r\n- Sec 2.1, the end of the 1st paragraph: I don't think the last sentence is correct. First of all, what is the energy state? Isn't it a simple scalar corresponding to the log of unnormalized probability? Then, if two orthogonal patterns were in data, shouldn't their probabilities (or energy states) be close to each other regardless of their orthogonality?\r\n\r\n- 'Weight is updated until the global energy E reduces below a threshold' What is the global energy? Do you mean the average of all training samples' energies?\r\n\r\n- Sec. 2.1., the end of the 2nd paragraph: 'w_ij is equal to the probability of feature h_j given input v_i'. Why is it so? Shouldn't the probability of h_j given v_i require marginalizing out all v_j (j \neq i), which probably is computational intractable and without any analytical form? \r\n\r\n- Sec. 2.2, the end of the third paragraph: I believe many people consider convent as one of deep learning methods as well.\r\n\r\n- Sec. 3.1, 2nd paragraph: 'RBM is unable to recall patterns when only half of the visible neurons are given correct pattern values' I completely disagree with this sentence. It may highly depend on data as well as the model size. On MNIST (which the authors used for their experiments), I believe reconstructing the missing half is not too difficult and can be done pretty well with a reasonably large and well-trained RBM. \r\n\r\n- Sec. 3: What is noticeably missing in Sec. 3 is how the authors actually reconstruct the missing modality given the other modality. Is it simply a single forward pass from one modality to the other using the recognition, then generation weights? Does it involve sampling at each layer in between? If so, it's likely that p(x2 | x1) has multiple modes (x2 missing modality, m1 observed modality). How do you resolve among multiple possible reconstructions? If there's no sampling involved, why does the proposed model work better than a conventional autoencoder (two-way) trained with SGD and backprop? Is it possible that the problem of the conventional NN is due to learning difficulty only?\r\n\r\n- Sec. 3.1, the last paragraph: I believe S&S (2012) did not do supervised finetuning for all experiments. For instance, for image retrieval, multimodal DBM does not require any discriminative finetuning.\r\n\r\n- Sec. 4: In general, I'm not sure why the authors had to use only that small dataset. And, due to this small size dataset and the pretraining strategy used during the experiments, each parameter of the BP-ANN gets unfairly smaller number updates.\r\n\r\n- Sec. 4.1, the last paragraph: why do you suspect that? I think it's simply that BP-ANN wasn't trained enough.\r\n\r\n- Sec. 4.2: you should state the type of noise you used.\r\n\r\n- Sec. 4.2, the last paragraph: 'DLAs attempt to probabilistically differentiate features from noises' I think I understand what the authors are trying to say, but I'm not entirely sure if that's correct or I'm not misunderstanding. Anyway, this sentence sounds somewhat weird (if not wrong). \r\n\r\n- Sec. 4.2, the last paragraph: 'BP ANNs .. features ... are for the purpose of mapping and not reconstruction' I'm lost here. How does 'mapping from one modality to the other' differ from 'reconstructing the other modality from one modality'? \r\n\r\n- Sec. 4.3, Results and Discussion: 'using non-paired examples to better develop .. associate learning system' Either this sentence has been mistyped, or I'm misunderstanding the Fig. 11 completely. It seems to me that adding non-paired examples does not help at all. Also, the remaining of the paragraph after this sentence is extremely difficult to understand, and I'd suggest to rephrase it."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Paired-associate Images with An Unsupervised Deep Learning Architecture", "decision": "submitted, no decision", "abstract": "This paper presents an unsupervised multi-modal learning system that learns associative representation from two input modalities (channels) such that input on one channel will correctly generate the associated response at the other channel and vice versa. In this way, the system develops a kind of supervised classification model meant to simulate aspects of human associative memory. The system uses a deep learning architecture (DLA) composed of two input/output channels formed from stacked Restricted Boltzmann Machines (RBM) and an associative memory network that combines the two channels. The DLA is trained on pairs of MNIST handwritten digit images to develop hierarchical features and associative representations that are able to reconstruct one image given its paired-associate. Experiments show that the multi-modal learning system generates models that are as accurate as back-propagation networks but with the advantage of unsupervised learning from either paired or non-paired training examples.", "pdf": "https://arxiv.org/abs/1312.6171", "paperhash": "wang|learning_pairedassociate_images_with_an_unsupervised_deep_learning_architecture", "keywords": [], "conflicts": [], "authors": ["Ti Wang", "Daniel L. Silver"], "authorids": ["108787w@acadiau.ca", "danny.silver@acadiau.ca"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1390944660000, "tcdate": 1390944660000, "number": 5, "id": "77VF4PBnF67gl", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "kkUZ1FHlLaPAf", "replyto": "kkUZ1FHlLaPAf", "signatures": ["anonymous reviewer acbe"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Learning Paired-associate Images with An Unsupervised Deep Learning Architecture", "review": "This paper is trying to employ DBNs at the traditional task of associative memory. This is an interesting problem as the human brain is thought to contain an \u201cassociation cortex\u201d dedicated to combining sensory modalities. However, while the paper is written reasonably well, it did not introduce any significantly new learning method. The experimental section is weak and leaves this reviewer unconvinced of their conclusions.\r\n\r\nThe framework of having dual modalities has been proposed by the original DBN paper [8] (the reference is missing a third author). Contrastive wake-sleep algorithm is proposed for unsupervised learning in [8], which seems more principled than what this paper proposes in the paragraph starting with \u201cTo fine-tune channel 1\u2026\u201d.\r\n\r\nIn the experimental section, the natural comparison is to an autoencoder (e.g. net in figure 6), which is also trained in an unsupervised manner. However, it is hard to believe that a network with 500-1000-500 hidden nodes can\u2019t reconstruct better than what is shown in Fig. 7. last row. What kind of learning algorithm was used? CG or SGD, did the optimization converge?  The authors also ref \u201cHinton\u2019s software\u201d and it\u2019s 1.15% error on MNIST. However, that is a totally different net with 10 1-of-k label units. It is unclear what the authors mean by using hinton\u2019s software: was there no additional learning for your particular image pair association task been performed? If that is the case, then the results are believable but not a good baseline.\r\n\r\nSeveral suggestions to improve the paper: compare with the contrastive wake-sleep algorithm and autoencoder trained with CG/SGD. Investigate more in depth on your proposed algorithm, is it approximating some objective? You should truly use multimodal data like images and speech, since it is so often mentioned in the introduction/background sections. It is a lot of work to combine audio and natural images are much higher dimensionality, but this would make the paper stronger.\r\n\r\nThe claim in section 2.2 that \u201cThe mammalian brain is..\u201d is a huge claim which is not proven. It is the current mainstream theory, but you should not treat it as a fact. Note that models like [14] do not even do learning except the last SVM layer.\r\n\r\nThe claim that RBM can\u2019t recall patterns when half of is corrupted is not convincing and maybe should be qualified to a particular task/learning algorithm. There are papers on rbm and noise+occlusion which suggest otherwise. It is true that how RBM performs is dependent on how it is trained (e.g. using fast pcd is critical)."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Paired-associate Images with An Unsupervised Deep Learning Architecture", "decision": "submitted, no decision", "abstract": "This paper presents an unsupervised multi-modal learning system that learns associative representation from two input modalities (channels) such that input on one channel will correctly generate the associated response at the other channel and vice versa. In this way, the system develops a kind of supervised classification model meant to simulate aspects of human associative memory. The system uses a deep learning architecture (DLA) composed of two input/output channels formed from stacked Restricted Boltzmann Machines (RBM) and an associative memory network that combines the two channels. The DLA is trained on pairs of MNIST handwritten digit images to develop hierarchical features and associative representations that are able to reconstruct one image given its paired-associate. Experiments show that the multi-modal learning system generates models that are as accurate as back-propagation networks but with the advantage of unsupervised learning from either paired or non-paired training examples.", "pdf": "https://arxiv.org/abs/1312.6171", "paperhash": "wang|learning_pairedassociate_images_with_an_unsupervised_deep_learning_architecture", "keywords": [], "conflicts": [], "authors": ["Ti Wang", "Daniel L. Silver"], "authorids": ["108787w@acadiau.ca", "danny.silver@acadiau.ca"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1389415080000, "tcdate": 1389415080000, "number": 1, "id": "LLqZOlEyzAOfh", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "kkUZ1FHlLaPAf", "replyto": "kkUZ1FHlLaPAf", "signatures": ["Daniel Silver"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "Ok .. Sorry.. apparently I did not complete the update on arXiv.  Completed that today, and the new version should appear on Jan 14.  Regardless, the only difference in the paper is as stated above."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Paired-associate Images with An Unsupervised Deep Learning Architecture", "decision": "submitted, no decision", "abstract": "This paper presents an unsupervised multi-modal learning system that learns associative representation from two input modalities (channels) such that input on one channel will correctly generate the associated response at the other channel and vice versa. In this way, the system develops a kind of supervised classification model meant to simulate aspects of human associative memory. The system uses a deep learning architecture (DLA) composed of two input/output channels formed from stacked Restricted Boltzmann Machines (RBM) and an associative memory network that combines the two channels. The DLA is trained on pairs of MNIST handwritten digit images to develop hierarchical features and associative representations that are able to reconstruct one image given its paired-associate. Experiments show that the multi-modal learning system generates models that are as accurate as back-propagation networks but with the advantage of unsupervised learning from either paired or non-paired training examples.", "pdf": "https://arxiv.org/abs/1312.6171", "paperhash": "wang|learning_pairedassociate_images_with_an_unsupervised_deep_learning_architecture", "keywords": [], "conflicts": [], "authors": ["Ti Wang", "Daniel L. Silver"], "authorids": ["108787w@acadiau.ca", "danny.silver@acadiau.ca"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1389389760000, "tcdate": 1389389760000, "number": 2, "id": "VKHoV-M4TDq6l", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "kkUZ1FHlLaPAf", "replyto": "kkUZ1FHlLaPAf", "signatures": ["Daniel Silver"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "Thanks .. An update  was made on Dec 30, 2013."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Paired-associate Images with An Unsupervised Deep Learning Architecture", "decision": "submitted, no decision", "abstract": "This paper presents an unsupervised multi-modal learning system that learns associative representation from two input modalities (channels) such that input on one channel will correctly generate the associated response at the other channel and vice versa. In this way, the system develops a kind of supervised classification model meant to simulate aspects of human associative memory. The system uses a deep learning architecture (DLA) composed of two input/output channels formed from stacked Restricted Boltzmann Machines (RBM) and an associative memory network that combines the two channels. The DLA is trained on pairs of MNIST handwritten digit images to develop hierarchical features and associative representations that are able to reconstruct one image given its paired-associate. Experiments show that the multi-modal learning system generates models that are as accurate as back-propagation networks but with the advantage of unsupervised learning from either paired or non-paired training examples.", "pdf": "https://arxiv.org/abs/1312.6171", "paperhash": "wang|learning_pairedassociate_images_with_an_unsupervised_deep_learning_architecture", "keywords": [], "conflicts": [], "authors": ["Ti Wang", "Daniel L. Silver"], "authorids": ["108787w@acadiau.ca", "danny.silver@acadiau.ca"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1388872680000, "tcdate": 1388872680000, "number": 3, "id": "HHsj_oAEjGHvi", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "kkUZ1FHlLaPAf", "replyto": "kkUZ1FHlLaPAf", "signatures": ["Yann LeCun"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "Daniel, you should update your paper right now. There is no limit on how many times you can update your paper."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Paired-associate Images with An Unsupervised Deep Learning Architecture", "decision": "submitted, no decision", "abstract": "This paper presents an unsupervised multi-modal learning system that learns associative representation from two input modalities (channels) such that input on one channel will correctly generate the associated response at the other channel and vice versa. In this way, the system develops a kind of supervised classification model meant to simulate aspects of human associative memory. The system uses a deep learning architecture (DLA) composed of two input/output channels formed from stacked Restricted Boltzmann Machines (RBM) and an associative memory network that combines the two channels. The DLA is trained on pairs of MNIST handwritten digit images to develop hierarchical features and associative representations that are able to reconstruct one image given its paired-associate. Experiments show that the multi-modal learning system generates models that are as accurate as back-propagation networks but with the advantage of unsupervised learning from either paired or non-paired training examples.", "pdf": "https://arxiv.org/abs/1312.6171", "paperhash": "wang|learning_pairedassociate_images_with_an_unsupervised_deep_learning_architecture", "keywords": [], "conflicts": [], "authors": ["Ti Wang", "Daniel L. Silver"], "authorids": ["108787w@acadiau.ca", "danny.silver@acadiau.ca"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1388090820000, "tcdate": 1388090820000, "number": 4, "id": "WP-vWodmvRHtG", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "kkUZ1FHlLaPAf", "replyto": "kkUZ1FHlLaPAf", "signatures": ["Daniel Silver"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "We just noticed that in the last figure, Figure 11, the odd->even value for model 4 is incorrect, it should be 89.0, not 90.88, and the error bars are approximately the same height as the other odd-->even stats.  The average of 90.34 for model 4 and its error bars are correct.   Will ensure correction is made in final version."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Paired-associate Images with An Unsupervised Deep Learning Architecture", "decision": "submitted, no decision", "abstract": "This paper presents an unsupervised multi-modal learning system that learns associative representation from two input modalities (channels) such that input on one channel will correctly generate the associated response at the other channel and vice versa. In this way, the system develops a kind of supervised classification model meant to simulate aspects of human associative memory. The system uses a deep learning architecture (DLA) composed of two input/output channels formed from stacked Restricted Boltzmann Machines (RBM) and an associative memory network that combines the two channels. The DLA is trained on pairs of MNIST handwritten digit images to develop hierarchical features and associative representations that are able to reconstruct one image given its paired-associate. Experiments show that the multi-modal learning system generates models that are as accurate as back-propagation networks but with the advantage of unsupervised learning from either paired or non-paired training examples.", "pdf": "https://arxiv.org/abs/1312.6171", "paperhash": "wang|learning_pairedassociate_images_with_an_unsupervised_deep_learning_architecture", "keywords": [], "conflicts": [], "authors": ["Ti Wang", "Daniel L. Silver"], "authorids": ["108787w@acadiau.ca", "danny.silver@acadiau.ca"]}, "tags": [], "invitation": {}}}, {"replyto": null, "ddate": null, "legacy_migration": true, "tmdate": 1387964340000, "tcdate": 1387964340000, "number": 56, "id": "kkUZ1FHlLaPAf", "invitation": "ICLR.cc/2014/conference/-/submission", "forum": "kkUZ1FHlLaPAf", "signatures": ["108787w@acadiau.ca"], "readers": ["everyone"], "content": {"title": "Learning Paired-associate Images with An Unsupervised Deep Learning Architecture", "decision": "submitted, no decision", "abstract": "This paper presents an unsupervised multi-modal learning system that learns associative representation from two input modalities (channels) such that input on one channel will correctly generate the associated response at the other channel and vice versa. In this way, the system develops a kind of supervised classification model meant to simulate aspects of human associative memory. The system uses a deep learning architecture (DLA) composed of two input/output channels formed from stacked Restricted Boltzmann Machines (RBM) and an associative memory network that combines the two channels. The DLA is trained on pairs of MNIST handwritten digit images to develop hierarchical features and associative representations that are able to reconstruct one image given its paired-associate. Experiments show that the multi-modal learning system generates models that are as accurate as back-propagation networks but with the advantage of unsupervised learning from either paired or non-paired training examples.", "pdf": "https://arxiv.org/abs/1312.6171", "paperhash": "wang|learning_pairedassociate_images_with_an_unsupervised_deep_learning_architecture", "keywords": [], "conflicts": [], "authors": ["Ti Wang", "Daniel L. Silver"], "authorids": ["108787w@acadiau.ca", "danny.silver@acadiau.ca"]}, "writers": [], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1369422751717, "tmdate": 1496674357195, "id": "ICLR.cc/2014/conference/-/submission", "writers": ["ICLR.cc/2014"], "signatures": ["OpenReview.net"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": []}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1377198751717, "cdate": 1496674357195}}}], "count": 9}