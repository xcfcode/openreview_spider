{"notes": [{"id": "BJgum4Qgu4", "original": "SkgmFGLYv4", "number": 25, "cdate": 1553114144362, "ddate": null, "tcdate": 1553114144362, "tmdate": 1562082106123, "tddate": null, "forum": "BJgum4Qgu4", "replyto": null, "invitation": "ICLR.cc/2019/Workshop/LLD/-/Blind_Submission", "content": {"title": "Learning Entity Representations for Few-Shot Reconstruction of Wikipedia Categories", "authors": ["Jeffrey Ling", "Nicholas FitzGerald", "Livio Baldini Soares", "David Weiss", "Tom Kwiatkowski"], "authorids": ["jeffreyling@google.com", "nfitz@google.com", "liviobs@google.com", "djweiss@google.com", "tomkwiat@google.com"], "keywords": ["representation learning", "entities", "few-shot", "Wikipedia"], "TL;DR": "We learn entity representations that can reconstruct Wikipedia categories with just a few exemplars.", "abstract": "Language modeling tasks, in which words are predicted on the basis of a local context, have been very effective for learning word embeddings and context dependent representations of phrases. Motivated by the observation that efforts to code\nworld knowledge into machine readable knowledge bases tend to be entity-centric,\nwe investigate the use of a fill-in-the-blank task to learn context independent representations of entities from the contexts in which those entities were mentioned.\nWe show that large scale training of neural models allows us to learn extremely high fidelity entity typing information, which we demonstrate with few-shot reconstruction of Wikipedia categories. Our learning approach is powerful enough\nto encode specialized topics such as Giro d\u2019Italia cyclists.", "pdf": "/pdf/3149c0ac80667c12f4038a535443fcd6c694b8c8.pdf", "paperhash": "ling|learning_entity_representations_for_fewshot_reconstruction_of_wikipedia_categories"}, "signatures": ["ICLR.cc/2019/Workshop/LLD"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/LLD"], "details": {"replyCount": 3, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/LLD/-/Blind_Submission", "cdate": 1548689671889, "reply": {"forum": null, "replyto": null, "readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2019/Workshop/LLD"]}, "signatures": {"values": ["ICLR.cc/2019/Workshop/LLD"]}, "content": {"authors": {"values-regex": ".*"}, "authorids": {"values-regex": ".*"}}}, "tcdate": 1548689671889, "tmdate": 1557933709646, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/LLD"], "invitees": ["~"], "signatures": ["ICLR.cc/2019/Workshop/LLD"], "details": {"writable": true}}}, "tauthor": "OpenReview.net"}, {"id": "rJxY33bSFV", "original": null, "number": 1, "cdate": 1554484400692, "ddate": null, "tcdate": 1554484400692, "tmdate": 1555512026349, "tddate": null, "forum": "BJgum4Qgu4", "replyto": "BJgum4Qgu4", "invitation": "ICLR.cc/2019/Workshop/LLD/-/Paper25/Official_Review", "content": {"title": "Sound approach, but the results are somewhat unconvincing", "review": "This paper presents an approach to obtaining representations of entities from raw text alone. They compare their method, 'RELIC', with some presumably competitive baselines, i.e., two ways of encoding entities with BERT (Devlin et al. 2018). The authors do a good job of connecting their work to previous work, noting the similarities to entity linking, entity typing, and learning entity representations from knowledge bases.\n\nIn terms of the BERT context encoder, I'm not so clear on how these representations are projected into the d-dimensional space. (I assume that d=300, as noted in 3.2?). The authors mention a transformation matrix W, but do not seem to take up how this is learned. Depending on how this is done, this could have a substantial impact on the results reported. What accuracies does BERT get without this dimensionality reduction?\n\nThe results are not altogether convincing, as the majority of entities only occur relatively rarely -- more than half have a frequency in the interval [0, 10). I interpret this as showing that the RELIC system is not particularly successful at the few-shot task, as only relatively high frequency entities have better representations with RELIC than with the BERT comparisons. \n\nFinally, the conclusions strike me as somewhat odd. What do you mean by that RELIC \"[...] allows us to learn highly interesting representations\"? An added qualitative analysis of the representations could serve to highlight this further.\n\nMinor comments:\n* Footnotes should be after punctuation\n* Is the 'All' row in Table 2 micro or macro averaged?", "rating": "3: Marginally above acceptance threshold", "confidence": "2: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Workshop/LLD/Paper25/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/LLD/Paper25/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Entity Representations for Few-Shot Reconstruction of Wikipedia Categories", "authors": ["Jeffrey Ling", "Nicholas FitzGerald", "Livio Baldini Soares", "David Weiss", "Tom Kwiatkowski"], "authorids": ["jeffreyling@google.com", "nfitz@google.com", "liviobs@google.com", "djweiss@google.com", "tomkwiat@google.com"], "keywords": ["representation learning", "entities", "few-shot", "Wikipedia"], "TL;DR": "We learn entity representations that can reconstruct Wikipedia categories with just a few exemplars.", "abstract": "Language modeling tasks, in which words are predicted on the basis of a local context, have been very effective for learning word embeddings and context dependent representations of phrases. Motivated by the observation that efforts to code\nworld knowledge into machine readable knowledge bases tend to be entity-centric,\nwe investigate the use of a fill-in-the-blank task to learn context independent representations of entities from the contexts in which those entities were mentioned.\nWe show that large scale training of neural models allows us to learn extremely high fidelity entity typing information, which we demonstrate with few-shot reconstruction of Wikipedia categories. Our learning approach is powerful enough\nto encode specialized topics such as Giro d\u2019Italia cyclists.", "pdf": "/pdf/3149c0ac80667c12f4038a535443fcd6c694b8c8.pdf", "paperhash": "ling|learning_entity_representations_for_fewshot_reconstruction_of_wikipedia_categories"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/LLD/-/Paper25/Official_Review", "cdate": 1553713418062, "expdate": 1555718400000, "duedate": 1554681600000, "reply": {"forum": "BJgum4Qgu4", "replyto": "BJgum4Qgu4", "writers": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2019/Workshop/LLD/Paper25/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2019/Workshop/LLD/Paper25/AnonReviewer[0-9]+"}, "readers": {"values": ["everyone"], "description": "The users who will be allowed to read the above content."}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["5: Top 15% of accepted papers, strong accept", "4: Top 50% of accepted papers, clear accept", "3: Marginally above acceptance threshold", "2: Marginally below acceptance threshold", "1: Strong rejection"], "required": true}, "confidence": {"order": 4, "value-radio": ["3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "2: The reviewer is fairly confident that the evaluation is correct", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "tcdate": 1553713418062, "tmdate": 1555511815807, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/LLD"], "invitees": ["ICLR.cc/2019/Workshop/LLD/Paper25/Reviewers"], "signatures": ["ICLR.cc/2019/Workshop/LLD"]}}}, {"id": "S1eSanZ_YN", "original": null, "number": 2, "cdate": 1554681021230, "ddate": null, "tcdate": 1554681021230, "tmdate": 1555511887191, "tddate": null, "forum": "BJgum4Qgu4", "replyto": "BJgum4Qgu4", "invitation": "ICLR.cc/2019/Workshop/LLD/-/Paper25/Official_Review", "content": {"title": "Small but solid contribution", "review": "The paper describes a method for embeddings entities, making use of the pre-trained BERT model.\nEntities are represented either as simple embeddings, by encoding the entity name with BERT, or by encoding the first paragraph of the wikipedia entry with BERT.\nThe contexts from entity mentions are also encoded with BERT and then optimized to be in the same space as the entity representations.\nThe model is evaluated on a task of producing more entities for a given category, given some examples. The proposed method performs best with more frequent entities and is outperformed on less frequent entities.\n\nThe method is fairly straightforward, with limited novelty, applying the existing BERT model to encode textual representations. \nHowever, the paper does present a comparison and analysis for different model variations on this task, which provides some useful insight. \nAlso, the frequency-based analysis of the entities is interesting, showing a clear boundary where the proposed model starts outperforming the baseline.\n\nIt is unclear what is the difference between the \"small number of exemplars (3-10)\" and \"one correct candidate entity\" and how these are used differently.\nAlso, for experiments where the candidates do not cover all possible entities, how were the candidates chosen?", "rating": "4: Top 50% of accepted papers, clear accept", "confidence": "3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Workshop/LLD/Paper25/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/LLD/Paper25/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Entity Representations for Few-Shot Reconstruction of Wikipedia Categories", "authors": ["Jeffrey Ling", "Nicholas FitzGerald", "Livio Baldini Soares", "David Weiss", "Tom Kwiatkowski"], "authorids": ["jeffreyling@google.com", "nfitz@google.com", "liviobs@google.com", "djweiss@google.com", "tomkwiat@google.com"], "keywords": ["representation learning", "entities", "few-shot", "Wikipedia"], "TL;DR": "We learn entity representations that can reconstruct Wikipedia categories with just a few exemplars.", "abstract": "Language modeling tasks, in which words are predicted on the basis of a local context, have been very effective for learning word embeddings and context dependent representations of phrases. Motivated by the observation that efforts to code\nworld knowledge into machine readable knowledge bases tend to be entity-centric,\nwe investigate the use of a fill-in-the-blank task to learn context independent representations of entities from the contexts in which those entities were mentioned.\nWe show that large scale training of neural models allows us to learn extremely high fidelity entity typing information, which we demonstrate with few-shot reconstruction of Wikipedia categories. Our learning approach is powerful enough\nto encode specialized topics such as Giro d\u2019Italia cyclists.", "pdf": "/pdf/3149c0ac80667c12f4038a535443fcd6c694b8c8.pdf", "paperhash": "ling|learning_entity_representations_for_fewshot_reconstruction_of_wikipedia_categories"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/LLD/-/Paper25/Official_Review", "cdate": 1553713418062, "expdate": 1555718400000, "duedate": 1554681600000, "reply": {"forum": "BJgum4Qgu4", "replyto": "BJgum4Qgu4", "writers": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2019/Workshop/LLD/Paper25/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2019/Workshop/LLD/Paper25/AnonReviewer[0-9]+"}, "readers": {"values": ["everyone"], "description": "The users who will be allowed to read the above content."}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["5: Top 15% of accepted papers, strong accept", "4: Top 50% of accepted papers, clear accept", "3: Marginally above acceptance threshold", "2: Marginally below acceptance threshold", "1: Strong rejection"], "required": true}, "confidence": {"order": 4, "value-radio": ["3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "2: The reviewer is fairly confident that the evaluation is correct", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "tcdate": 1553713418062, "tmdate": 1555511815807, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/LLD"], "invitees": ["ICLR.cc/2019/Workshop/LLD/Paper25/Reviewers"], "signatures": ["ICLR.cc/2019/Workshop/LLD"]}}}, {"id": "BylJr5hMcE", "original": null, "number": 1, "cdate": 1555380790991, "ddate": null, "tcdate": 1555380790991, "tmdate": 1555510979616, "tddate": null, "forum": "BJgum4Qgu4", "replyto": "BJgum4Qgu4", "invitation": "ICLR.cc/2019/Workshop/LLD/-/Paper25/Decision", "content": {"title": "Acceptance Decision", "decision": "Accept"}, "signatures": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Entity Representations for Few-Shot Reconstruction of Wikipedia Categories", "authors": ["Jeffrey Ling", "Nicholas FitzGerald", "Livio Baldini Soares", "David Weiss", "Tom Kwiatkowski"], "authorids": ["jeffreyling@google.com", "nfitz@google.com", "liviobs@google.com", "djweiss@google.com", "tomkwiat@google.com"], "keywords": ["representation learning", "entities", "few-shot", "Wikipedia"], "TL;DR": "We learn entity representations that can reconstruct Wikipedia categories with just a few exemplars.", "abstract": "Language modeling tasks, in which words are predicted on the basis of a local context, have been very effective for learning word embeddings and context dependent representations of phrases. Motivated by the observation that efforts to code\nworld knowledge into machine readable knowledge bases tend to be entity-centric,\nwe investigate the use of a fill-in-the-blank task to learn context independent representations of entities from the contexts in which those entities were mentioned.\nWe show that large scale training of neural models allows us to learn extremely high fidelity entity typing information, which we demonstrate with few-shot reconstruction of Wikipedia categories. Our learning approach is powerful enough\nto encode specialized topics such as Giro d\u2019Italia cyclists.", "pdf": "/pdf/3149c0ac80667c12f4038a535443fcd6c694b8c8.pdf", "paperhash": "ling|learning_entity_representations_for_fewshot_reconstruction_of_wikipedia_categories"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/LLD/-/Paper25/Decision", "cdate": 1554736067009, "reply": {"forum": "BJgum4Qgu4", "replyto": "BJgum4Qgu4", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-regex": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "description": "How your identity will be displayed."}, "signatures": {"values": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"title": {"order": 1, "required": true, "value": "Acceptance Decision"}, "decision": {"order": 2, "required": true, "value-radio": ["Accept", "Reject"], "description": "Acceptance decision"}, "comment": {"order": 3, "required": false, "value-regex": "[\\S\\s]{0,5000}", "description": ""}}}, "tcdate": 1554736067009, "tmdate": 1555510971668, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/LLD"], "invitees": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "signatures": ["ICLR.cc/2019/Workshop/LLD"]}}}], "count": 4}