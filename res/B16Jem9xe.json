{"notes": [{"tddate": null, "tmdate": 1487946292955, "tcdate": 1487946292955, "number": 4, "id": "S1TVMT6Yx", "invitation": "ICLR.cc/2017/conference/-/paper187/official/comment", "forum": "B16Jem9xe", "replyto": "Hk74i-qKl", "signatures": ["ICLR.cc/2017/conference/paper187/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper187/AnonReviewer1"], "content": {"title": "thanks for the clarification", "comment": "Hi,\n\nApologies for both mistakes.\n\nRegarding Eqn. (14): indeed, I have misread what you wrote in the paper, my bad. I remember going down exactly the same rabbit hole before...\n\nRe: acceptance, I got confused as I've only read the subtitle of the PC's comment \"ICLR 2017 conference paper187 acceptance\", not the actual decision."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning in Implicit Generative Models", "abstract": "Generative adversarial networks (GANs) provide an algorithmic framework for constructing generative models with several appealing properties: they do not require a likelihood function to be specified, only a generating procedure; they provide samples that are sharp and compelling; and they allow us to harness our knowledge of building highly accurate neural network classifiers. Here, we develop  our understanding of GANs with the aim of forming a rich view of this growing area of machine learning---to build connections to the diverse set of statistical thinking on this topic, of which much can be gained by a mutual exchange of ideas. We frame GANs within the wider landscape of algorithms for learning in implicit generative models---models that only specify a stochastic procedure with which to generate data---and relate these ideas to modelling problems in related fields, such as econometrics and approximate Bayesian computation. We develop likelihood-free inference methods and highlight hypothesis testing as a principle for learning in implicit generative models, using which we are able to derive the objective function used by GANs, and many other related objectives. The testing viewpoint directs our focus to the general problem of density ratio estimation. There are four approaches for density ratio estimation, one of which is a solution using classifiers to distinguish real from generated data. Other approaches such as divergence minimisation and moment matching have also been explored in the GAN literature, and we synthesise these views to form an understanding  in terms of the relationships between them and the wider literature, highlighting avenues for future exploration and cross-pollination.", "pdf": "/pdf/8ff6ce7b79a72960e71e4619260412c89d7366e2.pdf", "TL;DR": "Showing connections between GANs, ABC, ratio estimation and other approaches for learning in deep generative models.", "paperhash": "mohamed|learning_in_implicit_generative_models", "keywords": ["Unsupervised Learning"], "conflicts": ["google.com"], "authors": ["Shakir Mohamed", "Balaji Lakshminarayanan"], "authorids": ["shakir@google.com", "balajiln@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287694232, "id": "ICLR.cc/2017/conference/-/paper187/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "B16Jem9xe", "writers": {"values-regex": "ICLR.cc/2017/conference/paper187/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper187/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper187/reviewers", "ICLR.cc/2017/conference/paper187/areachairs"], "cdate": 1485287694232}}}, {"tddate": null, "tmdate": 1487702826794, "tcdate": 1487702826794, "number": 7, "id": "Hk74i-qKl", "invitation": "ICLR.cc/2017/conference/-/paper187/public/comment", "forum": "B16Jem9xe", "replyto": "HkCf2vYtx", "signatures": ["~Balaji_Lakshminarayanan1"], "readers": ["everyone"], "writers": ["~Balaji_Lakshminarayanan1"], "content": {"title": "Response", "comment": "Hi,\n\nThe references are indeed mixed up, we will fix them.\n\nNot sure if we understand your question correct, but I assume you are referring to the paragraph below (14)?\nEq (14) gives us a ratio loss (similar to KLIEP). There is no issue optimizing this with respect to r. The issue is the optimization with respect to q --- this is not possible due to the log q term which is not available for implicit models.\n\nBalaji\n\nPS: Our paper wasn't accepted to the conference btw."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning in Implicit Generative Models", "abstract": "Generative adversarial networks (GANs) provide an algorithmic framework for constructing generative models with several appealing properties: they do not require a likelihood function to be specified, only a generating procedure; they provide samples that are sharp and compelling; and they allow us to harness our knowledge of building highly accurate neural network classifiers. Here, we develop  our understanding of GANs with the aim of forming a rich view of this growing area of machine learning---to build connections to the diverse set of statistical thinking on this topic, of which much can be gained by a mutual exchange of ideas. We frame GANs within the wider landscape of algorithms for learning in implicit generative models---models that only specify a stochastic procedure with which to generate data---and relate these ideas to modelling problems in related fields, such as econometrics and approximate Bayesian computation. We develop likelihood-free inference methods and highlight hypothesis testing as a principle for learning in implicit generative models, using which we are able to derive the objective function used by GANs, and many other related objectives. The testing viewpoint directs our focus to the general problem of density ratio estimation. There are four approaches for density ratio estimation, one of which is a solution using classifiers to distinguish real from generated data. Other approaches such as divergence minimisation and moment matching have also been explored in the GAN literature, and we synthesise these views to form an understanding  in terms of the relationships between them and the wider literature, highlighting avenues for future exploration and cross-pollination.", "pdf": "/pdf/8ff6ce7b79a72960e71e4619260412c89d7366e2.pdf", "TL;DR": "Showing connections between GANs, ABC, ratio estimation and other approaches for learning in deep generative models.", "paperhash": "mohamed|learning_in_implicit_generative_models", "keywords": ["Unsupervised Learning"], "conflicts": ["google.com"], "authors": ["Shakir Mohamed", "Balaji Lakshminarayanan"], "authorids": ["shakir@google.com", "balajiln@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287694373, "id": "ICLR.cc/2017/conference/-/paper187/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B16Jem9xe", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper187/reviewers", "ICLR.cc/2017/conference/paper187/areachairs"], "cdate": 1485287694373}}}, {"tddate": null, "nonreaders": null, "tmdate": 1487662217093, "tcdate": 1487662102554, "number": 3, "id": "HkCf2vYtx", "invitation": "ICLR.cc/2017/conference/-/paper187/official/comment", "forum": "B16Jem9xe", "replyto": "B16Jem9xe", "signatures": ["ICLR.cc/2017/conference/paper187/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper187/AnonReviewer1"], "content": {"title": "please correct: references KLIEP in the wrong place + comment on Eqn. (14)", "comment": "Hello Authors,\n\nCongratulations on the acceptance of the paper.\n\nI've just reread parts of the revised paper and noticed a few things that you might want to consider and change before the camera-ready deadline.\n\n* You now include a reference to KLIEP after Eqn. (16), but this procedure is in fact known as least-squares importance estimation.\n* in turn, Eqn. (14) is actually more akin to KLIEP, the main difference being the use of the unnormalised form of the KL-divergence. So I think you meant to put the KLIEP reference here.\n\nFurther comment on making Eqn. (14) practical:\nIf you read the KLIEP paper, they formulate the procedure as a constrained optimisation problem:\nmaximise\n$$\nE_{p^*} log r_\\phi(x)\n$$\nsubject to the constraint that\n$$\nE_{q_\\theta} r_\\phi(x) = 1\n$$\n\nCompare this constrained optimisation to your solution, it is easy to make the connection: if you introduce a Lagrange multiplier to handle the constraint, one obtains the following unconstrained optimisation problem:\n\nseek stationary points of\n\n$$\n\\ell(\\phi, \\lambda) = E_{p^*} log r_\\phi(x) - \\lambda E_{q_\\theta} (r_\\phi(x) - 1)\n$$\n\nI do think that solving this unconstrained optimisation problem is actually possible, you can do that via stochastic gradient descent, and it does not include your nasty cross-entropy term.\n\nWhat am I missing?\n\nThanks,\n\nRev1"}, "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning in Implicit Generative Models", "abstract": "Generative adversarial networks (GANs) provide an algorithmic framework for constructing generative models with several appealing properties: they do not require a likelihood function to be specified, only a generating procedure; they provide samples that are sharp and compelling; and they allow us to harness our knowledge of building highly accurate neural network classifiers. Here, we develop  our understanding of GANs with the aim of forming a rich view of this growing area of machine learning---to build connections to the diverse set of statistical thinking on this topic, of which much can be gained by a mutual exchange of ideas. We frame GANs within the wider landscape of algorithms for learning in implicit generative models---models that only specify a stochastic procedure with which to generate data---and relate these ideas to modelling problems in related fields, such as econometrics and approximate Bayesian computation. We develop likelihood-free inference methods and highlight hypothesis testing as a principle for learning in implicit generative models, using which we are able to derive the objective function used by GANs, and many other related objectives. The testing viewpoint directs our focus to the general problem of density ratio estimation. There are four approaches for density ratio estimation, one of which is a solution using classifiers to distinguish real from generated data. Other approaches such as divergence minimisation and moment matching have also been explored in the GAN literature, and we synthesise these views to form an understanding  in terms of the relationships between them and the wider literature, highlighting avenues for future exploration and cross-pollination.", "pdf": "/pdf/8ff6ce7b79a72960e71e4619260412c89d7366e2.pdf", "TL;DR": "Showing connections between GANs, ABC, ratio estimation and other approaches for learning in deep generative models.", "paperhash": "mohamed|learning_in_implicit_generative_models", "keywords": ["Unsupervised Learning"], "conflicts": ["google.com"], "authors": ["Shakir Mohamed", "Balaji Lakshminarayanan"], "authorids": ["shakir@google.com", "balajiln@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287694232, "id": "ICLR.cc/2017/conference/-/paper187/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "B16Jem9xe", "writers": {"values-regex": "ICLR.cc/2017/conference/paper187/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper187/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper187/reviewers", "ICLR.cc/2017/conference/paper187/areachairs"], "cdate": 1485287694232}}}, {"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396420391, "tcdate": 1486396420391, "number": 1, "id": "BJ2b2zU_l", "invitation": "ICLR.cc/2017/conference/-/paper187/acceptance", "forum": "B16Jem9xe", "replyto": "B16Jem9xe", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "This paper provides a unifying review of various forms of generative model. The paper offers some neat perspectives that could encourage links between areas of machine learning and statistics. However, there aren't specific new proposals, and so there is no empirical evaluation either. The PCs thus believe this contribution is more appropriate for the Workshop Track.", "decision": "Invite to Workshop Track"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning in Implicit Generative Models", "abstract": "Generative adversarial networks (GANs) provide an algorithmic framework for constructing generative models with several appealing properties: they do not require a likelihood function to be specified, only a generating procedure; they provide samples that are sharp and compelling; and they allow us to harness our knowledge of building highly accurate neural network classifiers. Here, we develop  our understanding of GANs with the aim of forming a rich view of this growing area of machine learning---to build connections to the diverse set of statistical thinking on this topic, of which much can be gained by a mutual exchange of ideas. We frame GANs within the wider landscape of algorithms for learning in implicit generative models---models that only specify a stochastic procedure with which to generate data---and relate these ideas to modelling problems in related fields, such as econometrics and approximate Bayesian computation. We develop likelihood-free inference methods and highlight hypothesis testing as a principle for learning in implicit generative models, using which we are able to derive the objective function used by GANs, and many other related objectives. The testing viewpoint directs our focus to the general problem of density ratio estimation. There are four approaches for density ratio estimation, one of which is a solution using classifiers to distinguish real from generated data. Other approaches such as divergence minimisation and moment matching have also been explored in the GAN literature, and we synthesise these views to form an understanding  in terms of the relationships between them and the wider literature, highlighting avenues for future exploration and cross-pollination.", "pdf": "/pdf/8ff6ce7b79a72960e71e4619260412c89d7366e2.pdf", "TL;DR": "Showing connections between GANs, ABC, ratio estimation and other approaches for learning in deep generative models.", "paperhash": "mohamed|learning_in_implicit_generative_models", "keywords": ["Unsupervised Learning"], "conflicts": ["google.com"], "authors": ["Shakir Mohamed", "Balaji Lakshminarayanan"], "authorids": ["shakir@google.com", "balajiln@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396420898, "id": "ICLR.cc/2017/conference/-/paper187/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "B16Jem9xe", "replyto": "B16Jem9xe", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396420898}}}, {"tddate": null, "tmdate": 1484825757011, "tcdate": 1484825757011, "number": 2, "id": "HyBiVmAIl", "invitation": "ICLR.cc/2017/conference/-/paper187/official/comment", "forum": "B16Jem9xe", "replyto": "Hkw34vLLe", "signatures": ["ICLR.cc/2017/conference/paper187/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper187/AnonReviewer1"], "content": {"title": "Thank you for your response", "comment": "Thanks for your detailed response and the pointers to the slides. I want to reiterate both to authors and to fellow reviewers/area chairs, that I do like this review and I think it is valuable to have it. I agree with other reviewers in that it is hard to evauate this paper as a conference submission, which also explains my relatively positive review with a not so good score.\n\nI would argue for accepting this paper, unless it would be at the cost of rejecting excellent papers that present more original ideas."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning in Implicit Generative Models", "abstract": "Generative adversarial networks (GANs) provide an algorithmic framework for constructing generative models with several appealing properties: they do not require a likelihood function to be specified, only a generating procedure; they provide samples that are sharp and compelling; and they allow us to harness our knowledge of building highly accurate neural network classifiers. Here, we develop  our understanding of GANs with the aim of forming a rich view of this growing area of machine learning---to build connections to the diverse set of statistical thinking on this topic, of which much can be gained by a mutual exchange of ideas. We frame GANs within the wider landscape of algorithms for learning in implicit generative models---models that only specify a stochastic procedure with which to generate data---and relate these ideas to modelling problems in related fields, such as econometrics and approximate Bayesian computation. We develop likelihood-free inference methods and highlight hypothesis testing as a principle for learning in implicit generative models, using which we are able to derive the objective function used by GANs, and many other related objectives. The testing viewpoint directs our focus to the general problem of density ratio estimation. There are four approaches for density ratio estimation, one of which is a solution using classifiers to distinguish real from generated data. Other approaches such as divergence minimisation and moment matching have also been explored in the GAN literature, and we synthesise these views to form an understanding  in terms of the relationships between them and the wider literature, highlighting avenues for future exploration and cross-pollination.", "pdf": "/pdf/8ff6ce7b79a72960e71e4619260412c89d7366e2.pdf", "TL;DR": "Showing connections between GANs, ABC, ratio estimation and other approaches for learning in deep generative models.", "paperhash": "mohamed|learning_in_implicit_generative_models", "keywords": ["Unsupervised Learning"], "conflicts": ["google.com"], "authors": ["Shakir Mohamed", "Balaji Lakshminarayanan"], "authorids": ["shakir@google.com", "balajiln@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287694232, "id": "ICLR.cc/2017/conference/-/paper187/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "B16Jem9xe", "writers": {"values-regex": "ICLR.cc/2017/conference/paper187/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper187/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper187/reviewers", "ICLR.cc/2017/conference/paper187/areachairs"], "cdate": 1485287694232}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1484585393799, "tcdate": 1478270949454, "number": 187, "id": "B16Jem9xe", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "B16Jem9xe", "signatures": ["~Shakir_Mohamed1"], "readers": ["everyone"], "content": {"title": "Learning in Implicit Generative Models", "abstract": "Generative adversarial networks (GANs) provide an algorithmic framework for constructing generative models with several appealing properties: they do not require a likelihood function to be specified, only a generating procedure; they provide samples that are sharp and compelling; and they allow us to harness our knowledge of building highly accurate neural network classifiers. Here, we develop  our understanding of GANs with the aim of forming a rich view of this growing area of machine learning---to build connections to the diverse set of statistical thinking on this topic, of which much can be gained by a mutual exchange of ideas. We frame GANs within the wider landscape of algorithms for learning in implicit generative models---models that only specify a stochastic procedure with which to generate data---and relate these ideas to modelling problems in related fields, such as econometrics and approximate Bayesian computation. We develop likelihood-free inference methods and highlight hypothesis testing as a principle for learning in implicit generative models, using which we are able to derive the objective function used by GANs, and many other related objectives. The testing viewpoint directs our focus to the general problem of density ratio estimation. There are four approaches for density ratio estimation, one of which is a solution using classifiers to distinguish real from generated data. Other approaches such as divergence minimisation and moment matching have also been explored in the GAN literature, and we synthesise these views to form an understanding  in terms of the relationships between them and the wider literature, highlighting avenues for future exploration and cross-pollination.", "pdf": "/pdf/8ff6ce7b79a72960e71e4619260412c89d7366e2.pdf", "TL;DR": "Showing connections between GANs, ABC, ratio estimation and other approaches for learning in deep generative models.", "paperhash": "mohamed|learning_in_implicit_generative_models", "keywords": ["Unsupervised Learning"], "conflicts": ["google.com"], "authors": ["Shakir Mohamed", "Balaji Lakshminarayanan"], "authorids": ["shakir@google.com", "balajiln@google.com"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 15, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "tmdate": 1484318514649, "tcdate": 1484318514649, "number": 6, "id": "SJjEwvLIg", "invitation": "ICLR.cc/2017/conference/-/paper187/public/comment", "forum": "B16Jem9xe", "replyto": "SkBDDOofe", "signatures": ["~Balaji_Lakshminarayanan1"], "readers": ["everyone"], "writers": ["~Balaji_Lakshminarayanan1"], "content": {"title": "response to question", "comment": "\u201cWhich method should I choose when I want to learn an implicit model? Or can you comment a bit on the difficulty of training for different objectives?\u201d\n\nThis question is similar to the question of \u201cwhich approximate inference method should I use for my model?\u201d. And the answer is,  (perhaps not surprisingly) it really depends on the problem. We don\u2019t yet have a clear understanding of if and when one method is necessarily theoretically superior to another for all possible models (and the family of implicit models is huge!). There are some practical differences, e.g. a classifier as a discriminator is more amenable to minibatch training as opposed to moment matching which could affected by minibatch noise; hence, if the dataset is large, classifier approach might be preferable to moment matching. One could also think of combining multiple discriminator losses; this could help avoid the generator overfitting to any particular discriminator.  "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning in Implicit Generative Models", "abstract": "Generative adversarial networks (GANs) provide an algorithmic framework for constructing generative models with several appealing properties: they do not require a likelihood function to be specified, only a generating procedure; they provide samples that are sharp and compelling; and they allow us to harness our knowledge of building highly accurate neural network classifiers. Here, we develop  our understanding of GANs with the aim of forming a rich view of this growing area of machine learning---to build connections to the diverse set of statistical thinking on this topic, of which much can be gained by a mutual exchange of ideas. We frame GANs within the wider landscape of algorithms for learning in implicit generative models---models that only specify a stochastic procedure with which to generate data---and relate these ideas to modelling problems in related fields, such as econometrics and approximate Bayesian computation. We develop likelihood-free inference methods and highlight hypothesis testing as a principle for learning in implicit generative models, using which we are able to derive the objective function used by GANs, and many other related objectives. The testing viewpoint directs our focus to the general problem of density ratio estimation. There are four approaches for density ratio estimation, one of which is a solution using classifiers to distinguish real from generated data. Other approaches such as divergence minimisation and moment matching have also been explored in the GAN literature, and we synthesise these views to form an understanding  in terms of the relationships between them and the wider literature, highlighting avenues for future exploration and cross-pollination.", "pdf": "/pdf/8ff6ce7b79a72960e71e4619260412c89d7366e2.pdf", "TL;DR": "Showing connections between GANs, ABC, ratio estimation and other approaches for learning in deep generative models.", "paperhash": "mohamed|learning_in_implicit_generative_models", "keywords": ["Unsupervised Learning"], "conflicts": ["google.com"], "authors": ["Shakir Mohamed", "Balaji Lakshminarayanan"], "authorids": ["shakir@google.com", "balajiln@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287694373, "id": "ICLR.cc/2017/conference/-/paper187/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B16Jem9xe", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper187/reviewers", "ICLR.cc/2017/conference/paper187/areachairs"], "cdate": 1485287694373}}}, {"tddate": null, "tmdate": 1484317989005, "tcdate": 1484317989005, "number": 5, "id": "ByaQSD8Ig", "invitation": "ICLR.cc/2017/conference/-/paper187/public/comment", "forum": "B16Jem9xe", "replyto": "BJKXGcCQe", "signatures": ["~Balaji_Lakshminarayanan1"], "readers": ["everyone"], "writers": ["~Balaji_Lakshminarayanan1"], "content": {"title": "Response to AnonReviewer3", "comment": "Thank you for your positive comments. Our aim is to provide clarity, share a different perspective, and to highlight all the related work addressing the same problem that is not otherwise cited and used. We believe there is both novelty in our presentation, and is a good fit and of relevance to ICLR. Please see the more detailed response we provide in general rebuttal to the paper, since this was highlighted by other reviewers as well.\n\nThe work of Sugiyama et al. is especially important and one that has been a great inspiration to us. It is important to keep in mind that estimating the density ratio is only part of the problem when learning in GANs and implicit generative models. Showing how this can be used to form both the ratio and generative losses is an important aspect of this paper.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning in Implicit Generative Models", "abstract": "Generative adversarial networks (GANs) provide an algorithmic framework for constructing generative models with several appealing properties: they do not require a likelihood function to be specified, only a generating procedure; they provide samples that are sharp and compelling; and they allow us to harness our knowledge of building highly accurate neural network classifiers. Here, we develop  our understanding of GANs with the aim of forming a rich view of this growing area of machine learning---to build connections to the diverse set of statistical thinking on this topic, of which much can be gained by a mutual exchange of ideas. We frame GANs within the wider landscape of algorithms for learning in implicit generative models---models that only specify a stochastic procedure with which to generate data---and relate these ideas to modelling problems in related fields, such as econometrics and approximate Bayesian computation. We develop likelihood-free inference methods and highlight hypothesis testing as a principle for learning in implicit generative models, using which we are able to derive the objective function used by GANs, and many other related objectives. The testing viewpoint directs our focus to the general problem of density ratio estimation. There are four approaches for density ratio estimation, one of which is a solution using classifiers to distinguish real from generated data. Other approaches such as divergence minimisation and moment matching have also been explored in the GAN literature, and we synthesise these views to form an understanding  in terms of the relationships between them and the wider literature, highlighting avenues for future exploration and cross-pollination.", "pdf": "/pdf/8ff6ce7b79a72960e71e4619260412c89d7366e2.pdf", "TL;DR": "Showing connections between GANs, ABC, ratio estimation and other approaches for learning in deep generative models.", "paperhash": "mohamed|learning_in_implicit_generative_models", "keywords": ["Unsupervised Learning"], "conflicts": ["google.com"], "authors": ["Shakir Mohamed", "Balaji Lakshminarayanan"], "authorids": ["shakir@google.com", "balajiln@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287694373, "id": "ICLR.cc/2017/conference/-/paper187/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B16Jem9xe", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper187/reviewers", "ICLR.cc/2017/conference/paper187/areachairs"], "cdate": 1485287694373}}}, {"tddate": null, "tmdate": 1484317911890, "tcdate": 1484317911890, "number": 4, "id": "H1ekrvULg", "invitation": "ICLR.cc/2017/conference/-/paper187/public/comment", "forum": "B16Jem9xe", "replyto": "Bk54BMGEe", "signatures": ["~Balaji_Lakshminarayanan1"], "readers": ["everyone"], "writers": ["~Balaji_Lakshminarayanan1"], "content": {"title": "Response to AnonReviewer4", "comment": "Thank you for your positive and encouraging feedback!"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning in Implicit Generative Models", "abstract": "Generative adversarial networks (GANs) provide an algorithmic framework for constructing generative models with several appealing properties: they do not require a likelihood function to be specified, only a generating procedure; they provide samples that are sharp and compelling; and they allow us to harness our knowledge of building highly accurate neural network classifiers. Here, we develop  our understanding of GANs with the aim of forming a rich view of this growing area of machine learning---to build connections to the diverse set of statistical thinking on this topic, of which much can be gained by a mutual exchange of ideas. We frame GANs within the wider landscape of algorithms for learning in implicit generative models---models that only specify a stochastic procedure with which to generate data---and relate these ideas to modelling problems in related fields, such as econometrics and approximate Bayesian computation. We develop likelihood-free inference methods and highlight hypothesis testing as a principle for learning in implicit generative models, using which we are able to derive the objective function used by GANs, and many other related objectives. The testing viewpoint directs our focus to the general problem of density ratio estimation. There are four approaches for density ratio estimation, one of which is a solution using classifiers to distinguish real from generated data. Other approaches such as divergence minimisation and moment matching have also been explored in the GAN literature, and we synthesise these views to form an understanding  in terms of the relationships between them and the wider literature, highlighting avenues for future exploration and cross-pollination.", "pdf": "/pdf/8ff6ce7b79a72960e71e4619260412c89d7366e2.pdf", "TL;DR": "Showing connections between GANs, ABC, ratio estimation and other approaches for learning in deep generative models.", "paperhash": "mohamed|learning_in_implicit_generative_models", "keywords": ["Unsupervised Learning"], "conflicts": ["google.com"], "authors": ["Shakir Mohamed", "Balaji Lakshminarayanan"], "authorids": ["shakir@google.com", "balajiln@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287694373, "id": "ICLR.cc/2017/conference/-/paper187/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B16Jem9xe", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper187/reviewers", "ICLR.cc/2017/conference/paper187/areachairs"], "cdate": 1485287694373}}}, {"tddate": null, "tmdate": 1484317871310, "tcdate": 1484317871310, "number": 3, "id": "Hkw34vLLe", "invitation": "ICLR.cc/2017/conference/-/paper187/public/comment", "forum": "B16Jem9xe", "replyto": "B1zK_4_Ve", "signatures": ["~Balaji_Lakshminarayanan1"], "readers": ["everyone"], "writers": ["~Balaji_Lakshminarayanan1"], "content": {"title": "Response to AnonReviewer1", "comment": "Thank you for your positive comments. Our aim is to provide clarity, share a different perspective, and to highlight all the related work addressing the same problem that is not otherwise cited and used. We believe there is both novelty in our presentation, and is a good fit and of relevance to ICLR. Please see the more detailed response we provide in general rebuttal to the paper, since this was highlighted by other reviewers as well.\n\nThe work of Sugiyama et al., is especially important and one that has been a great inspiration to us. We aim to cite it as much as possible, citing the book as our key source (rather than individual papers and specific names such as KLIEP and LSIE). It is important to keep in mind that estimating the density ratio is only part of the problem when learning in GANs and implicit generative models, and why our discussion has taken the form it has.\n\n\u201cI think it would be interesting to think about how this function is related to the witness function commonly used in MMD and what the properties of this function are compared to the witness function - perhaps showing the two things for simple distributions.\u201d \nSriperumbudur et al showed in the paper \u201cOn the empirical estimation of integral probability metrics\u201d 2012 that f-divergences and integral probability metrics (MMD, Wasserstein) intersect only at the total variation distance. See also Arthur Gretton\u2019s slides at the NIPS adversarial training workshop:  http://www.gatsby.ucl.ac.uk/~gretton/papers/testing_workshop.pdf\n\nFor likelihood ratio estimation, we do not want to estimate the convex function of the ratio directly, since this will not allow us to exploit the duality properties of the divergence.\n\nThe importance of hypothesis testing is perhaps a philosophical one. Recognising the role of hypothesis testing (to test if two distributions are equal) helps to ask the question of why we should compute the density ratio or density difference, which is otherwise vague and unclear. Once this is recognised, the path to using the myriad of methods for comparing two hypotheses such as two-sample tests, density ratios, density-differences, and classical tests such as Wald and likelihood ratio test, are all valid tools for learning implicit models. This view has been recognised by other authors as well, e.g., Lopez-Paz and Oquab (2016). \n\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning in Implicit Generative Models", "abstract": "Generative adversarial networks (GANs) provide an algorithmic framework for constructing generative models with several appealing properties: they do not require a likelihood function to be specified, only a generating procedure; they provide samples that are sharp and compelling; and they allow us to harness our knowledge of building highly accurate neural network classifiers. Here, we develop  our understanding of GANs with the aim of forming a rich view of this growing area of machine learning---to build connections to the diverse set of statistical thinking on this topic, of which much can be gained by a mutual exchange of ideas. We frame GANs within the wider landscape of algorithms for learning in implicit generative models---models that only specify a stochastic procedure with which to generate data---and relate these ideas to modelling problems in related fields, such as econometrics and approximate Bayesian computation. We develop likelihood-free inference methods and highlight hypothesis testing as a principle for learning in implicit generative models, using which we are able to derive the objective function used by GANs, and many other related objectives. The testing viewpoint directs our focus to the general problem of density ratio estimation. There are four approaches for density ratio estimation, one of which is a solution using classifiers to distinguish real from generated data. Other approaches such as divergence minimisation and moment matching have also been explored in the GAN literature, and we synthesise these views to form an understanding  in terms of the relationships between them and the wider literature, highlighting avenues for future exploration and cross-pollination.", "pdf": "/pdf/8ff6ce7b79a72960e71e4619260412c89d7366e2.pdf", "TL;DR": "Showing connections between GANs, ABC, ratio estimation and other approaches for learning in deep generative models.", "paperhash": "mohamed|learning_in_implicit_generative_models", "keywords": ["Unsupervised Learning"], "conflicts": ["google.com"], "authors": ["Shakir Mohamed", "Balaji Lakshminarayanan"], "authorids": ["shakir@google.com", "balajiln@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287694373, "id": "ICLR.cc/2017/conference/-/paper187/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B16Jem9xe", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper187/reviewers", "ICLR.cc/2017/conference/paper187/areachairs"], "cdate": 1485287694373}}}, {"tddate": null, "tmdate": 1484317761420, "tcdate": 1484317761420, "number": 2, "id": "SyFr4v8Lx", "invitation": "ICLR.cc/2017/conference/-/paper187/public/comment", "forum": "B16Jem9xe", "replyto": "B16Jem9xe", "signatures": ["~Balaji_Lakshminarayanan1"], "readers": ["everyone"], "writers": ["~Balaji_Lakshminarayanan1"], "content": {"title": "author rebuttal", "comment": "The reviewers have two common concerns (1) relevance of this paper to ICLR, and (2) its novelty. We address the common concerns here and address other questions individually.\n\nThe aim of our paper is to review different approaches for learning in implicit generative models; GANs are a special case of implicit generative models and our work helps understand connections between GAN variants as well as understand how GANs are related to the wider statistical literature. We are not aware of any other work in the GAN literature which discusses the connections to approximate Bayesian computation, moment-matching and two sample-testing, unsupervised-as-supervised learning, optimal transport, likelihood free inference, or non-maximum likelihood estimation, even though these methods are clearly related. We believe these connections would be interesting to ICLR community for at least two reasons: (1) we can borrow tools from the related literature to improve optimisation and analyse convergence and (2) we can use GAN-like techniques to train differentiable simulators in other application domains (e.g high energy physics, economics, ecology), thereby opening up exciting research directions. AnonReviewer4 writes \u201cI believe this work is significant - it provides a bridge for language and methods used in multiple parts of statistics and machine learning. This has the potential to accelerate progress.\u201d And AnonReviewer1 writes \u201cIt is well written and a good read, and one I would recommend to people who would like to get involved in GANs.\u201d This was our aim, and why this paper is of relevance to ICLR.\n\nNovelty: The view of hypothesis testing gives an insight about the learning principle used in likelihood free / implicit models. This view also helps understand connections between variants of GANs; for instance, methods that use the density ratio (e.g. the original GAN paper, f-GAN and b-GAN) or the density difference (e.g. generative moment matching networks). To the best of our knowledge, this unifying view through hypothesis testing is novel (see also the concurrent submissions by Lopez-paz and Oquab (2016) and Uehara et al. (2016)). To quote AnonReviewer4, \u201cthe individual pieces are not novel, and yet the exposition of all of them in the same space with clear outline of the connections between them is novel.\u201d \nThe unifying view opens up lots of interesting research directions, e.g., since our initial arxiv submission, Kareletsos (2016) has published \u201cAdversarial Message Passing For Graphical Models\u201d https://arxiv.org/pdf/1612.05048.pdf where he proposes GAN-like techniques for minimising divergence for message passing in graphical models. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning in Implicit Generative Models", "abstract": "Generative adversarial networks (GANs) provide an algorithmic framework for constructing generative models with several appealing properties: they do not require a likelihood function to be specified, only a generating procedure; they provide samples that are sharp and compelling; and they allow us to harness our knowledge of building highly accurate neural network classifiers. Here, we develop  our understanding of GANs with the aim of forming a rich view of this growing area of machine learning---to build connections to the diverse set of statistical thinking on this topic, of which much can be gained by a mutual exchange of ideas. We frame GANs within the wider landscape of algorithms for learning in implicit generative models---models that only specify a stochastic procedure with which to generate data---and relate these ideas to modelling problems in related fields, such as econometrics and approximate Bayesian computation. We develop likelihood-free inference methods and highlight hypothesis testing as a principle for learning in implicit generative models, using which we are able to derive the objective function used by GANs, and many other related objectives. The testing viewpoint directs our focus to the general problem of density ratio estimation. There are four approaches for density ratio estimation, one of which is a solution using classifiers to distinguish real from generated data. Other approaches such as divergence minimisation and moment matching have also been explored in the GAN literature, and we synthesise these views to form an understanding  in terms of the relationships between them and the wider literature, highlighting avenues for future exploration and cross-pollination.", "pdf": "/pdf/8ff6ce7b79a72960e71e4619260412c89d7366e2.pdf", "TL;DR": "Showing connections between GANs, ABC, ratio estimation and other approaches for learning in deep generative models.", "paperhash": "mohamed|learning_in_implicit_generative_models", "keywords": ["Unsupervised Learning"], "conflicts": ["google.com"], "authors": ["Shakir Mohamed", "Balaji Lakshminarayanan"], "authorids": ["shakir@google.com", "balajiln@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287694373, "id": "ICLR.cc/2017/conference/-/paper187/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B16Jem9xe", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper187/reviewers", "ICLR.cc/2017/conference/paper187/areachairs"], "cdate": 1485287694373}}}, {"tddate": null, "tmdate": 1482340474150, "tcdate": 1482340474150, "number": 3, "id": "B1zK_4_Ve", "invitation": "ICLR.cc/2017/conference/-/paper187/official/review", "forum": "B16Jem9xe", "replyto": "B16Jem9xe", "signatures": ["ICLR.cc/2017/conference/paper187/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper187/AnonReviewer1"], "content": {"title": "Review", "rating": "6: Marginally above acceptance threshold", "review": "I just noticed I submitted my review as a pre-review question - sorry about this. Here it is again, with a few more thoughts added...\n\nThe authors present a great and - as far as I can tell - accurate and honest overview of the emerging theory about GANs from a likelihood ratio estimation/divergence minimisation perspective. It is well written and a good read, and one I would recommend to people who would like to get involved in GANs.\n\nMy main problem with this submission is that it is hard as a reviewer to pin down what precisely the novelty is - beyond perhaps articulating these views better than other papers have done in the past. A sentence from the paper \"But it has left us unsatisfied since we have not gained the insight needed to choose between them.\u201d summarises my feeling about this paper: this is a nice 'unifying review\u2019 type paper that - for me - lacks a novel insight.\n\nIn summary, my assessment is mixed: I think this is a great paper, I enjoyed reading it. I was left a bit disappointed by the lack of novel insight, or a singular key new idea which you often expect in conference presentations, and this is why I\u2019m not highly confident about this as a conference submission (and hence my low score) I am open to be convinced either way.\n\nDetailed comments:\n\nI think the authors should probably discuss the connection of Eq. (13) to KLIEP: Kullback-Leibler Importance Estimation by Shugiyama and colleagues.\n\nI don\u2019t quite see how the part with equation (13) and (14) fit into the flow of the paper. By this point the authors have established the view that GANs are about estimating likelihood ratios - and then using these likelihood ratios to improve the generator. These paragraphs read like: we also tried to derive another particular formulation for doing this but we failed to do it in a practical way.\n\nThere is a typo in spelling Csiszar divergence\n\nEquation (15) is known (to me) as Least Squares Importance Estimation by Kanamori et al (2009). A variant of least-squares likelihood estimation uses the kernel trick, and finds a function from an RKHS that best represents the likelihood ratio between the two distributions in a least squares sense. I think it would be interesting to think about how this function is related to the witness function commonly used in MMD and what the properties of this function are compared to the witness function - perhaps showing the two things for simple distributions.\n\nI have stumbled upon the work of Sugiyama and collaborators on direct density ratio estimation before, and I found that work very insightful. Generally, while some of this work is cited in this paper, I felt that the authors could do more to highlight the great work of this group, who have made highly significant contributions to density ratio estimation, albeit with a different goal in mind.\n\nOn likelihood ratio estimation: some methods approximate the likelihood ratio directly (such as least-squares importance estimation), some can be thought of more as approximating the log of this quantity (logistic regression, denoising autoencoders). An unbiased estimate of the ratio will provide a biased estimate of the logarithm and vice versa. To me it feels like estimating the log of the ratio directly is more useful, and in more generality estimating the convex function of the ratio which is used to define the f-divergence seems like a good approach. Could the authors comment on this?\n\nI think the hypothesis testing angle is oversold in the paper.  I\u2019m not sure what additional insight is gained by mixing in some hypothesis testing terminology. Other than using quantities that appear in hypothesis testing as tests statistics, his work does not really talk about hypothesis testing, nor does it use any tools from the hypothesis testing literature. In this sense, this paper is in contrast with Sutherland et al (in review for ICLR) who do borrow concepts from two-sample testing to optimise hyperparameters of the divergence used.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning in Implicit Generative Models", "abstract": "Generative adversarial networks (GANs) provide an algorithmic framework for constructing generative models with several appealing properties: they do not require a likelihood function to be specified, only a generating procedure; they provide samples that are sharp and compelling; and they allow us to harness our knowledge of building highly accurate neural network classifiers. Here, we develop  our understanding of GANs with the aim of forming a rich view of this growing area of machine learning---to build connections to the diverse set of statistical thinking on this topic, of which much can be gained by a mutual exchange of ideas. We frame GANs within the wider landscape of algorithms for learning in implicit generative models---models that only specify a stochastic procedure with which to generate data---and relate these ideas to modelling problems in related fields, such as econometrics and approximate Bayesian computation. We develop likelihood-free inference methods and highlight hypothesis testing as a principle for learning in implicit generative models, using which we are able to derive the objective function used by GANs, and many other related objectives. The testing viewpoint directs our focus to the general problem of density ratio estimation. There are four approaches for density ratio estimation, one of which is a solution using classifiers to distinguish real from generated data. Other approaches such as divergence minimisation and moment matching have also been explored in the GAN literature, and we synthesise these views to form an understanding  in terms of the relationships between them and the wider literature, highlighting avenues for future exploration and cross-pollination.", "pdf": "/pdf/8ff6ce7b79a72960e71e4619260412c89d7366e2.pdf", "TL;DR": "Showing connections between GANs, ABC, ratio estimation and other approaches for learning in deep generative models.", "paperhash": "mohamed|learning_in_implicit_generative_models", "keywords": ["Unsupervised Learning"], "conflicts": ["google.com"], "authors": ["Shakir Mohamed", "Balaji Lakshminarayanan"], "authorids": ["shakir@google.com", "balajiln@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512671241, "id": "ICLR.cc/2017/conference/-/paper187/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper187/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper187/AnonReviewer3", "ICLR.cc/2017/conference/paper187/AnonReviewer4", "ICLR.cc/2017/conference/paper187/AnonReviewer1"], "reply": {"forum": "B16Jem9xe", "replyto": "B16Jem9xe", "writers": {"values-regex": "ICLR.cc/2017/conference/paper187/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper187/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512671241}}}, {"tddate": null, "tmdate": 1481938226493, "tcdate": 1481938226493, "number": 2, "id": "Bk54BMGEe", "invitation": "ICLR.cc/2017/conference/-/paper187/official/review", "forum": "B16Jem9xe", "replyto": "B16Jem9xe", "signatures": ["ICLR.cc/2017/conference/paper187/AnonReviewer4"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper187/AnonReviewer4"], "content": {"title": "An important paper", "rating": "8: Top 50% of accepted papers, clear accept", "review": "The paper provides an exposition of multiple ways of learning in implicit generative models, of which generative adversarial networks are an example. The paper is very clear, the exposition is insightful, and the presented material is clearly important.\n\nIt is hard to assess \"novelty\" of this work, as the individual pieces are not novel, and yet the exposition of all of them in the same space with clear outline of the connections between them is novel.\n\nI believe this work is significant - it provides a bridge for language and methods used in multiple parts of statistics and machine learning. This has the potential to accelerate progress.\n\nI recommend publishing this paper at ICLR, even though it is not the \"typical\" paper that get published at this conference (in that it doesn't offer empirical validation, nor makes a particular claim about relative merits of different methods).", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning in Implicit Generative Models", "abstract": "Generative adversarial networks (GANs) provide an algorithmic framework for constructing generative models with several appealing properties: they do not require a likelihood function to be specified, only a generating procedure; they provide samples that are sharp and compelling; and they allow us to harness our knowledge of building highly accurate neural network classifiers. Here, we develop  our understanding of GANs with the aim of forming a rich view of this growing area of machine learning---to build connections to the diverse set of statistical thinking on this topic, of which much can be gained by a mutual exchange of ideas. We frame GANs within the wider landscape of algorithms for learning in implicit generative models---models that only specify a stochastic procedure with which to generate data---and relate these ideas to modelling problems in related fields, such as econometrics and approximate Bayesian computation. We develop likelihood-free inference methods and highlight hypothesis testing as a principle for learning in implicit generative models, using which we are able to derive the objective function used by GANs, and many other related objectives. The testing viewpoint directs our focus to the general problem of density ratio estimation. There are four approaches for density ratio estimation, one of which is a solution using classifiers to distinguish real from generated data. Other approaches such as divergence minimisation and moment matching have also been explored in the GAN literature, and we synthesise these views to form an understanding  in terms of the relationships between them and the wider literature, highlighting avenues for future exploration and cross-pollination.", "pdf": "/pdf/8ff6ce7b79a72960e71e4619260412c89d7366e2.pdf", "TL;DR": "Showing connections between GANs, ABC, ratio estimation and other approaches for learning in deep generative models.", "paperhash": "mohamed|learning_in_implicit_generative_models", "keywords": ["Unsupervised Learning"], "conflicts": ["google.com"], "authors": ["Shakir Mohamed", "Balaji Lakshminarayanan"], "authorids": ["shakir@google.com", "balajiln@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512671241, "id": "ICLR.cc/2017/conference/-/paper187/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper187/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper187/AnonReviewer3", "ICLR.cc/2017/conference/paper187/AnonReviewer4", "ICLR.cc/2017/conference/paper187/AnonReviewer1"], "reply": {"forum": "B16Jem9xe", "replyto": "B16Jem9xe", "writers": {"values-regex": "ICLR.cc/2017/conference/paper187/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper187/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512671241}}}, {"tddate": null, "tmdate": 1481926010804, "tcdate": 1481926010804, "number": 2, "id": "SkmtHkfVe", "invitation": "ICLR.cc/2017/conference/-/paper187/pre-review/question", "forum": "B16Jem9xe", "replyto": "B16Jem9xe", "signatures": ["ICLR.cc/2017/conference/paper187/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper187/AnonReviewer1"], "content": {"title": "Review: Great paper, clear overview of state of theory, but I felt it lacks novel insight.", "question": "The authors present a great and - as far as I can tell - accurate and honest overview of the emerging theory about GANs from a likelihood ratio estimation/divergence minimisation perspective. It is well written and a good read, and one I would recommend to people who would like to get involved in GANs.\n\nMy main problem with this submission is that it is unclear what precisely the novelty is - beyond perhaps articulating these views better than other papers have done in the past. A sentence from the paper \"But it has left us unsatisfied since we have not gained the insight needed to choose between them.\u201d summarises my feeling about this paper: this is a nice 'unifying review\u2019 type paper that - for me - lacks a novel insight.\n\nSo my assessment is mixed: I think this is a great paper, I enjoyed reading it. I was left a bit disappointed by the lack of novel insight, and this is why I\u2019m not highly confident this has to be a conference paper. I am open to be convinced either way.\n\nDetailed comments:\n\nI think the authors should probably discuss the connection of Eq. (13) to KLIEP: Kullback-Leibler Importance Estimation by Shugiyama and colleagues.\n\nI don\u2019t quite see how the part with equation (13) and (14) fit into the flow of the paper. By this point the authors have established the view that GANs are about estimating likelihood ratios - and then using these likelihood ratios to improve the generator. These paragraphs read like: we also tried to derive another particular formulation for doing this but we failed to do it in a practical way.\n\nTypo in Csiszar divergence\n\nEquation (15) is known (to me) as Least Squares Importance Estimation by Kanamori et al (2009).\n\nI have stumbled upon the work of Sugiyama and collaborators on direct density ratio estimation before, and I found that work very insightful. Generally, while some of this work is cited in this paper, I felt that the authors could do more to highlight the great work of this group, who have made highly significant contributions to density ratio estimation, albeit with a different goal in mind.\n\nI think the hypothesis testing angle is a bit oversold in the paper.  I\u2019m not sure what additional insight is gained by mixing in some hypothesis testing terminology. Other than using quantities that appear in hypothesis testing as tests statistics, his work does not really talk about hypothesis testing, nor does it use any tools from the hypothesis testing literature. In this sense, this paper is in contrast with Sutherland et al (in review for ICLR) who do borrow concepts from two-sample testing to optimise hyperparameters of the divergence used."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning in Implicit Generative Models", "abstract": "Generative adversarial networks (GANs) provide an algorithmic framework for constructing generative models with several appealing properties: they do not require a likelihood function to be specified, only a generating procedure; they provide samples that are sharp and compelling; and they allow us to harness our knowledge of building highly accurate neural network classifiers. Here, we develop  our understanding of GANs with the aim of forming a rich view of this growing area of machine learning---to build connections to the diverse set of statistical thinking on this topic, of which much can be gained by a mutual exchange of ideas. We frame GANs within the wider landscape of algorithms for learning in implicit generative models---models that only specify a stochastic procedure with which to generate data---and relate these ideas to modelling problems in related fields, such as econometrics and approximate Bayesian computation. We develop likelihood-free inference methods and highlight hypothesis testing as a principle for learning in implicit generative models, using which we are able to derive the objective function used by GANs, and many other related objectives. The testing viewpoint directs our focus to the general problem of density ratio estimation. There are four approaches for density ratio estimation, one of which is a solution using classifiers to distinguish real from generated data. Other approaches such as divergence minimisation and moment matching have also been explored in the GAN literature, and we synthesise these views to form an understanding  in terms of the relationships between them and the wider literature, highlighting avenues for future exploration and cross-pollination.", "pdf": "/pdf/8ff6ce7b79a72960e71e4619260412c89d7366e2.pdf", "TL;DR": "Showing connections between GANs, ABC, ratio estimation and other approaches for learning in deep generative models.", "paperhash": "mohamed|learning_in_implicit_generative_models", "keywords": ["Unsupervised Learning"], "conflicts": ["google.com"], "authors": ["Shakir Mohamed", "Balaji Lakshminarayanan"], "authorids": ["shakir@google.com", "balajiln@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481926011652, "id": "ICLR.cc/2017/conference/-/paper187/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper187/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper187/AnonReviewer3", "ICLR.cc/2017/conference/paper187/AnonReviewer1"], "reply": {"forum": "B16Jem9xe", "replyto": "B16Jem9xe", "writers": {"values-regex": "ICLR.cc/2017/conference/paper187/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper187/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481926011652}}}, {"tddate": null, "tmdate": 1481726614229, "tcdate": 1481708065510, "number": 1, "id": "BJKXGcCQe", "invitation": "ICLR.cc/2017/conference/-/paper187/official/review", "forum": "B16Jem9xe", "replyto": "B16Jem9xe", "signatures": ["ICLR.cc/2017/conference/paper187/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper187/AnonReviewer3"], "content": {"title": "This is a very good paper for connecting methods, but I'm not exactly sure if that fits to the ICLR conference", "rating": "7: Good paper, accept", "review": "Thank you for an interesting read.\n\nGiven the huge interest in generative modelling nowadays, this paper is very timely and does provide very clear connections between methods that don't use maximum likelihood for training. It made a very useful observation that the generative and the discriminative loss do **not** need to be coupled with each other. I think this paper in summary provides some very useful insights to the practitioners on how to select the objective function to train the implicit generative model.\n\nThe only reason that I decided to hold back my strong acceptance recommendation is that I don't understand the acceptance criteria of ICLR. First this paper has the style very similar to the Sugiyama et al. papers that are cited (e.g. presenting in different perspectives that were all covered in those papers but in a different context), making me unsure about how to evaluate the novelty. Second this paper has no experiment nor mathematical theorem, and I'm not exactly sure what kinds of contributions the ICLR committee is looking for.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning in Implicit Generative Models", "abstract": "Generative adversarial networks (GANs) provide an algorithmic framework for constructing generative models with several appealing properties: they do not require a likelihood function to be specified, only a generating procedure; they provide samples that are sharp and compelling; and they allow us to harness our knowledge of building highly accurate neural network classifiers. Here, we develop  our understanding of GANs with the aim of forming a rich view of this growing area of machine learning---to build connections to the diverse set of statistical thinking on this topic, of which much can be gained by a mutual exchange of ideas. We frame GANs within the wider landscape of algorithms for learning in implicit generative models---models that only specify a stochastic procedure with which to generate data---and relate these ideas to modelling problems in related fields, such as econometrics and approximate Bayesian computation. We develop likelihood-free inference methods and highlight hypothesis testing as a principle for learning in implicit generative models, using which we are able to derive the objective function used by GANs, and many other related objectives. The testing viewpoint directs our focus to the general problem of density ratio estimation. There are four approaches for density ratio estimation, one of which is a solution using classifiers to distinguish real from generated data. Other approaches such as divergence minimisation and moment matching have also been explored in the GAN literature, and we synthesise these views to form an understanding  in terms of the relationships between them and the wider literature, highlighting avenues for future exploration and cross-pollination.", "pdf": "/pdf/8ff6ce7b79a72960e71e4619260412c89d7366e2.pdf", "TL;DR": "Showing connections between GANs, ABC, ratio estimation and other approaches for learning in deep generative models.", "paperhash": "mohamed|learning_in_implicit_generative_models", "keywords": ["Unsupervised Learning"], "conflicts": ["google.com"], "authors": ["Shakir Mohamed", "Balaji Lakshminarayanan"], "authorids": ["shakir@google.com", "balajiln@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512671241, "id": "ICLR.cc/2017/conference/-/paper187/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper187/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper187/AnonReviewer3", "ICLR.cc/2017/conference/paper187/AnonReviewer4", "ICLR.cc/2017/conference/paper187/AnonReviewer1"], "reply": {"forum": "B16Jem9xe", "replyto": "B16Jem9xe", "writers": {"values-regex": "ICLR.cc/2017/conference/paper187/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper187/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512671241}}}, {"tddate": null, "tmdate": 1480456028702, "tcdate": 1480456028696, "number": 1, "id": "SkBDDOofe", "invitation": "ICLR.cc/2017/conference/-/paper187/pre-review/question", "forum": "B16Jem9xe", "replyto": "B16Jem9xe", "signatures": ["ICLR.cc/2017/conference/paper187/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper187/AnonReviewer3"], "content": {"title": "any guidlines for practitioners?", "question": "Thank you for an interesting read. \n\nI guess all the theory presented are already around but I still like the presentation which walked through each route in a very clear way.\n\nThe only question from a practitioner perspective is: which method should I choose when I want to learn an implicit model? Or can you comment a bit on the difficulty of training for different objectives?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning in Implicit Generative Models", "abstract": "Generative adversarial networks (GANs) provide an algorithmic framework for constructing generative models with several appealing properties: they do not require a likelihood function to be specified, only a generating procedure; they provide samples that are sharp and compelling; and they allow us to harness our knowledge of building highly accurate neural network classifiers. Here, we develop  our understanding of GANs with the aim of forming a rich view of this growing area of machine learning---to build connections to the diverse set of statistical thinking on this topic, of which much can be gained by a mutual exchange of ideas. We frame GANs within the wider landscape of algorithms for learning in implicit generative models---models that only specify a stochastic procedure with which to generate data---and relate these ideas to modelling problems in related fields, such as econometrics and approximate Bayesian computation. We develop likelihood-free inference methods and highlight hypothesis testing as a principle for learning in implicit generative models, using which we are able to derive the objective function used by GANs, and many other related objectives. The testing viewpoint directs our focus to the general problem of density ratio estimation. There are four approaches for density ratio estimation, one of which is a solution using classifiers to distinguish real from generated data. Other approaches such as divergence minimisation and moment matching have also been explored in the GAN literature, and we synthesise these views to form an understanding  in terms of the relationships between them and the wider literature, highlighting avenues for future exploration and cross-pollination.", "pdf": "/pdf/8ff6ce7b79a72960e71e4619260412c89d7366e2.pdf", "TL;DR": "Showing connections between GANs, ABC, ratio estimation and other approaches for learning in deep generative models.", "paperhash": "mohamed|learning_in_implicit_generative_models", "keywords": ["Unsupervised Learning"], "conflicts": ["google.com"], "authors": ["Shakir Mohamed", "Balaji Lakshminarayanan"], "authorids": ["shakir@google.com", "balajiln@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481926011652, "id": "ICLR.cc/2017/conference/-/paper187/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper187/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper187/AnonReviewer3", "ICLR.cc/2017/conference/paper187/AnonReviewer1"], "reply": {"forum": "B16Jem9xe", "replyto": "B16Jem9xe", "writers": {"values-regex": "ICLR.cc/2017/conference/paper187/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper187/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481926011652}}}], "count": 16}