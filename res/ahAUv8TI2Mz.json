{"notes": [{"id": "ahAUv8TI2Mz", "original": "0Uyt1nlFQ_k", "number": 2713, "cdate": 1601308300708, "ddate": null, "tcdate": 1601308300708, "tmdate": 1615956621715, "tddate": null, "forum": "ahAUv8TI2Mz", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Adaptive and Generative Zero-Shot Learning", "authorids": ["~Yu-Ying_Chou1", "~Hsuan-Tien_Lin1", "~Tyng-Luh_Liu1"], "authors": ["Yu-Ying Chou", "Hsuan-Tien Lin", "Tyng-Luh Liu"], "keywords": ["Generalized zero-shot learning", "mixup"], "abstract": "We address the problem of generalized zero-shot learning (GZSL) where the task is to predict the class label of a target image whether its label belongs to the seen or unseen category. Similar to ZSL, the learning setting assumes that all class-level semantic features are given, while only the images of seen classes are available for training. By exploring the correlation between image features and the corresponding semantic features, the main idea of the proposed approach is to enrich the semantic-to-visual (S2V) embeddings via a seamless fusion of adaptive and generative learning. To this end, we extend the semantic features of each class by supplementing image-adaptive attention so that the learned S2V embedding can account for not only inter-class but also intra-class variations. In addition, to break the limit of training with images only from seen classes, we design a generative scheme to simultaneously generate virtual class labels and their visual features by sampling and interpolating over seen counterparts. In inference, a testing image will give rise to two different S2V embeddings, seen and virtual. The former is used to decide whether the underlying label is of the unseen category or otherwise a specific seen class; the latter is to predict an unseen class label. To demonstrate the effectiveness of our method, we report state-of-the-art results on four standard GZSL datasets, including an ablation study of the proposed modules. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chou|adaptive_and_generative_zeroshot_learning", "pdf": "/pdf/c95de71bec56a004df30033ab55061c714367261.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nchou2021adaptive,\ntitle={Adaptive and Generative Zero-Shot Learning},\nauthor={Yu-Ying Chou and Hsuan-Tien Lin and Tyng-Luh Liu},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ahAUv8TI2Mz}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "hxOF_5_TyHY", "original": null, "number": 1, "cdate": 1610040370752, "ddate": null, "tcdate": 1610040370752, "tmdate": 1610473962175, "tddate": null, "forum": "ahAUv8TI2Mz", "replyto": "ahAUv8TI2Mz", "invitation": "ICLR.cc/2021/Conference/Paper2713/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Poster)", "comment": "The submission combines meta-learning and attention mechanism for generalised zero-shot learning. The image-guided attention on the semantic space helps to adapt the better class specific semantic information while separate experts operate on the seen and unseen classes. The unseen class expert is trained with the pseudo negative samples with pseudo negative labels. Meta-learning based training adapts the model to few-shot learning scenario. The submission has received two accept, two weak accept and one weak reject reviews. All reviewers found the methodology interesting but they found it moderately novel. The experimental evaluation has been found strong. The rebuttal addressed all the reviewers' concerns and during the discussion phase all reviewers recommended acceptance. The meta reviewer follows the consensus of all the reviewers and recommends acceptance."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive and Generative Zero-Shot Learning", "authorids": ["~Yu-Ying_Chou1", "~Hsuan-Tien_Lin1", "~Tyng-Luh_Liu1"], "authors": ["Yu-Ying Chou", "Hsuan-Tien Lin", "Tyng-Luh Liu"], "keywords": ["Generalized zero-shot learning", "mixup"], "abstract": "We address the problem of generalized zero-shot learning (GZSL) where the task is to predict the class label of a target image whether its label belongs to the seen or unseen category. Similar to ZSL, the learning setting assumes that all class-level semantic features are given, while only the images of seen classes are available for training. By exploring the correlation between image features and the corresponding semantic features, the main idea of the proposed approach is to enrich the semantic-to-visual (S2V) embeddings via a seamless fusion of adaptive and generative learning. To this end, we extend the semantic features of each class by supplementing image-adaptive attention so that the learned S2V embedding can account for not only inter-class but also intra-class variations. In addition, to break the limit of training with images only from seen classes, we design a generative scheme to simultaneously generate virtual class labels and their visual features by sampling and interpolating over seen counterparts. In inference, a testing image will give rise to two different S2V embeddings, seen and virtual. The former is used to decide whether the underlying label is of the unseen category or otherwise a specific seen class; the latter is to predict an unseen class label. To demonstrate the effectiveness of our method, we report state-of-the-art results on four standard GZSL datasets, including an ablation study of the proposed modules. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chou|adaptive_and_generative_zeroshot_learning", "pdf": "/pdf/c95de71bec56a004df30033ab55061c714367261.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nchou2021adaptive,\ntitle={Adaptive and Generative Zero-Shot Learning},\nauthor={Yu-Ying Chou and Hsuan-Tien Lin and Tyng-Luh Liu},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ahAUv8TI2Mz}\n}"}, "tags": [], "invitation": {"reply": {"forum": "ahAUv8TI2Mz", "replyto": "ahAUv8TI2Mz", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040370737, "tmdate": 1610473962157, "id": "ICLR.cc/2021/Conference/Paper2713/-/Decision"}}}, {"id": "wIPBh8P7nZ0", "original": null, "number": 6, "cdate": 1606237664917, "ddate": null, "tcdate": 1606237664917, "tmdate": 1606237664917, "tddate": null, "forum": "ahAUv8TI2Mz", "replyto": "CEeF3wtaskH", "invitation": "ICLR.cc/2021/Conference/Paper2713/-/Official_Comment", "content": {"title": "Responses to AnonReviewer1", "comment": "4. We have made our code available on github, https://github.com/anonmous529/AGZSL.\nAs shown in Figure 3, IAS is a single fc layer which transfers 2048-D image features to semantic features and utilizes ReLU on the output. The L_2 in Figure 3 means L_2 normalization. The network architecture of S2V embedding is described in the Implementation of Section 4.1.\nFor learning the unseen expert via meta-learning, we apply 200 epochs and 1000 episodes for each epoch. In each episode we randomly generated 16 ~ 20 virtual classes and 4 samples for each virtual class. The unseen expert is trained by Adam optimizer with a learning rate 5*10^-5 for all datasets.\n\n\n5. Thank you for the suggestion. We have found the following three papers on generative ZSL in CVPR'20 and ECCV'20. Among them, [b] is transductive learning which is different from our setting and [c] is already included in our comparisons. In the revised paper, we have included [a] in the comparisons as in Table 1 of Section 4.\n\n    [a] Generalized Zero-Shot Learning Via Over-Complete Distribution, cvpr2020.\n\n    [b] Self-supervised Domain-aware Generative Network for Generalized Zero-shot, cvpr2020.\n\n   [c] Latent Embedding Feedback and Discriminative Features for Zero-Shot Classification, eccv2020.\n\n\n6. Thank you for the suggestion. In the revised paper, we have added a new section, namely, Section 4.5, to show the PCA distributions of the virtual classes.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2713/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2713/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive and Generative Zero-Shot Learning", "authorids": ["~Yu-Ying_Chou1", "~Hsuan-Tien_Lin1", "~Tyng-Luh_Liu1"], "authors": ["Yu-Ying Chou", "Hsuan-Tien Lin", "Tyng-Luh Liu"], "keywords": ["Generalized zero-shot learning", "mixup"], "abstract": "We address the problem of generalized zero-shot learning (GZSL) where the task is to predict the class label of a target image whether its label belongs to the seen or unseen category. Similar to ZSL, the learning setting assumes that all class-level semantic features are given, while only the images of seen classes are available for training. By exploring the correlation between image features and the corresponding semantic features, the main idea of the proposed approach is to enrich the semantic-to-visual (S2V) embeddings via a seamless fusion of adaptive and generative learning. To this end, we extend the semantic features of each class by supplementing image-adaptive attention so that the learned S2V embedding can account for not only inter-class but also intra-class variations. In addition, to break the limit of training with images only from seen classes, we design a generative scheme to simultaneously generate virtual class labels and their visual features by sampling and interpolating over seen counterparts. In inference, a testing image will give rise to two different S2V embeddings, seen and virtual. The former is used to decide whether the underlying label is of the unseen category or otherwise a specific seen class; the latter is to predict an unseen class label. To demonstrate the effectiveness of our method, we report state-of-the-art results on four standard GZSL datasets, including an ablation study of the proposed modules. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chou|adaptive_and_generative_zeroshot_learning", "pdf": "/pdf/c95de71bec56a004df30033ab55061c714367261.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nchou2021adaptive,\ntitle={Adaptive and Generative Zero-Shot Learning},\nauthor={Yu-Ying Chou and Hsuan-Tien Lin and Tyng-Luh Liu},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ahAUv8TI2Mz}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ahAUv8TI2Mz", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2713/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2713/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2713/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2713/Authors|ICLR.cc/2021/Conference/Paper2713/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2713/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923845245, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2713/-/Official_Comment"}}}, {"id": "4038m3D9DH6", "original": null, "number": 5, "cdate": 1606237053458, "ddate": null, "tcdate": 1606237053458, "tmdate": 1606237053458, "tddate": null, "forum": "ahAUv8TI2Mz", "replyto": "f1F0sD9ejJy", "invitation": "ICLR.cc/2021/Conference/Paper2713/-/Official_Comment", "content": {"title": "Responses to AnonReviewer5", "comment": "3. We have made our code available on github, https://github.com/anonmous529/AGZSL.\nIn section 3.4, we describe how to apply episodic meta-learning to learn the unseen expert. Instead of forming each training episode based on samples from the seen classes, the meta-learning is carried out over virtual classes and their samples, which are obtained by the mixup scheme. (See equations (7) and (8).) More specifically, for each training episode, we use mixup to randomly generate 16  ~ 20 virtual classes and 4 samples for each class. In inference, the classification result by the seen and unseen experts is decided by equations (1) and (2).\n\n\n4. Thank you for the information. In the revision, we have included them in the related work and the experiment comparisons, as listed in Table 1 of Section 4. \nOur method is a two-expert (seen and unseen) approach. Even the baseline model of our ablation study in Section 4.3 comprises two experts. If we exclude the unseen expert from the meta-learning, then the two-expert baseline in Table 2 will be reduced to a seen-expert baseline. As a result, take, for example, the baseline performance on CUB: The general unseen accuracy (U) will drop from 68.6 to 60.8 and the unseen accuracy (T) will drop from 73.7 to 69.4.\n\n\n5. We have made our code available on github, https://github.com/anonmous529/AGZSL.\nIn our experiments, we find that fine-tuning the backbone for feature representation would dramatically improve the GZSL performance on CUB.\n\n\n6. In Section 4.1, the paragraph for Evaluation, we have stated that T denotes the average per-class top-1 accuracy. In the revision, we have changed it to \"T1\" to avoid possible confusions.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2713/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2713/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive and Generative Zero-Shot Learning", "authorids": ["~Yu-Ying_Chou1", "~Hsuan-Tien_Lin1", "~Tyng-Luh_Liu1"], "authors": ["Yu-Ying Chou", "Hsuan-Tien Lin", "Tyng-Luh Liu"], "keywords": ["Generalized zero-shot learning", "mixup"], "abstract": "We address the problem of generalized zero-shot learning (GZSL) where the task is to predict the class label of a target image whether its label belongs to the seen or unseen category. Similar to ZSL, the learning setting assumes that all class-level semantic features are given, while only the images of seen classes are available for training. By exploring the correlation between image features and the corresponding semantic features, the main idea of the proposed approach is to enrich the semantic-to-visual (S2V) embeddings via a seamless fusion of adaptive and generative learning. To this end, we extend the semantic features of each class by supplementing image-adaptive attention so that the learned S2V embedding can account for not only inter-class but also intra-class variations. In addition, to break the limit of training with images only from seen classes, we design a generative scheme to simultaneously generate virtual class labels and their visual features by sampling and interpolating over seen counterparts. In inference, a testing image will give rise to two different S2V embeddings, seen and virtual. The former is used to decide whether the underlying label is of the unseen category or otherwise a specific seen class; the latter is to predict an unseen class label. To demonstrate the effectiveness of our method, we report state-of-the-art results on four standard GZSL datasets, including an ablation study of the proposed modules. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chou|adaptive_and_generative_zeroshot_learning", "pdf": "/pdf/c95de71bec56a004df30033ab55061c714367261.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nchou2021adaptive,\ntitle={Adaptive and Generative Zero-Shot Learning},\nauthor={Yu-Ying Chou and Hsuan-Tien Lin and Tyng-Luh Liu},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ahAUv8TI2Mz}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ahAUv8TI2Mz", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2713/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2713/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2713/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2713/Authors|ICLR.cc/2021/Conference/Paper2713/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2713/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923845245, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2713/-/Official_Comment"}}}, {"id": "_8KZnTTgDST", "original": null, "number": 4, "cdate": 1606235909490, "ddate": null, "tcdate": 1606235909490, "tmdate": 1606235909490, "tddate": null, "forum": "ahAUv8TI2Mz", "replyto": "yABHy-V4BAq", "invitation": "ICLR.cc/2021/Conference/Paper2713/-/Official_Comment", "content": {"title": "Responses to AnonReviewer2", "comment": "2.1 : \u201cA large part of the improvement stems from the virtual classes, which is essentially mixup applied to ZSL. This is somewhat derivative.\u201d\n\nWe agree with the reviewer that mixup is a key component of our method. However, we point out that its effectiveness is largely enhanced by our proposed formulation. Our method is a two-expert (seen and unseen) approach, while the virtual classes/data by mixup are solely used in the episodic meta-learning for the unseen expert. On the other hand, simply applying mixup to other single-expert formulations most likely could not achieve comparable improvements in GZSL in that even with mixup, a single-expert classifier tends to bias the seen classes.\n\n\n2.2: \u201cHow does the paper work with actual generated samples as in the GAN/VAE-based models? Do these extra samples work well with the virtual samples? Is the information redundant/complementary?\u201d\n\nThis is a reasonable suggestion. We need further experiments to assess if it could yield further improvements. We will include such discussions and additional experiments in the final version of our work.\n\nIn the proposed episodic meta-learning for GZSL, we have the flexibility of working with not only virtual samples but also virtual classes. We find that for those datasets (e.g., AWA2) of a small number of classes, increasing the number of classes by mixup is advantageous for learning the S2V embedding. In comparison, the GAN/VAE approaches learn to generate samples for a fixed number of unseen classes.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2713/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2713/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive and Generative Zero-Shot Learning", "authorids": ["~Yu-Ying_Chou1", "~Hsuan-Tien_Lin1", "~Tyng-Luh_Liu1"], "authors": ["Yu-Ying Chou", "Hsuan-Tien Lin", "Tyng-Luh Liu"], "keywords": ["Generalized zero-shot learning", "mixup"], "abstract": "We address the problem of generalized zero-shot learning (GZSL) where the task is to predict the class label of a target image whether its label belongs to the seen or unseen category. Similar to ZSL, the learning setting assumes that all class-level semantic features are given, while only the images of seen classes are available for training. By exploring the correlation between image features and the corresponding semantic features, the main idea of the proposed approach is to enrich the semantic-to-visual (S2V) embeddings via a seamless fusion of adaptive and generative learning. To this end, we extend the semantic features of each class by supplementing image-adaptive attention so that the learned S2V embedding can account for not only inter-class but also intra-class variations. In addition, to break the limit of training with images only from seen classes, we design a generative scheme to simultaneously generate virtual class labels and their visual features by sampling and interpolating over seen counterparts. In inference, a testing image will give rise to two different S2V embeddings, seen and virtual. The former is used to decide whether the underlying label is of the unseen category or otherwise a specific seen class; the latter is to predict an unseen class label. To demonstrate the effectiveness of our method, we report state-of-the-art results on four standard GZSL datasets, including an ablation study of the proposed modules. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chou|adaptive_and_generative_zeroshot_learning", "pdf": "/pdf/c95de71bec56a004df30033ab55061c714367261.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nchou2021adaptive,\ntitle={Adaptive and Generative Zero-Shot Learning},\nauthor={Yu-Ying Chou and Hsuan-Tien Lin and Tyng-Luh Liu},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ahAUv8TI2Mz}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ahAUv8TI2Mz", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2713/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2713/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2713/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2713/Authors|ICLR.cc/2021/Conference/Paper2713/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2713/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923845245, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2713/-/Official_Comment"}}}, {"id": "ciTf0lKc_7U", "original": null, "number": 3, "cdate": 1606234962639, "ddate": null, "tcdate": 1606234962639, "tmdate": 1606234962639, "tddate": null, "forum": "ahAUv8TI2Mz", "replyto": "d956RA2nEAL", "invitation": "ICLR.cc/2021/Conference/Paper2713/-/Official_Comment", "content": {"title": "Responses to AnonReviewer3", "comment": "Thank you for the suggestion. We will improve the writing in the revision."}, "signatures": ["ICLR.cc/2021/Conference/Paper2713/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2713/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive and Generative Zero-Shot Learning", "authorids": ["~Yu-Ying_Chou1", "~Hsuan-Tien_Lin1", "~Tyng-Luh_Liu1"], "authors": ["Yu-Ying Chou", "Hsuan-Tien Lin", "Tyng-Luh Liu"], "keywords": ["Generalized zero-shot learning", "mixup"], "abstract": "We address the problem of generalized zero-shot learning (GZSL) where the task is to predict the class label of a target image whether its label belongs to the seen or unseen category. Similar to ZSL, the learning setting assumes that all class-level semantic features are given, while only the images of seen classes are available for training. By exploring the correlation between image features and the corresponding semantic features, the main idea of the proposed approach is to enrich the semantic-to-visual (S2V) embeddings via a seamless fusion of adaptive and generative learning. To this end, we extend the semantic features of each class by supplementing image-adaptive attention so that the learned S2V embedding can account for not only inter-class but also intra-class variations. In addition, to break the limit of training with images only from seen classes, we design a generative scheme to simultaneously generate virtual class labels and their visual features by sampling and interpolating over seen counterparts. In inference, a testing image will give rise to two different S2V embeddings, seen and virtual. The former is used to decide whether the underlying label is of the unseen category or otherwise a specific seen class; the latter is to predict an unseen class label. To demonstrate the effectiveness of our method, we report state-of-the-art results on four standard GZSL datasets, including an ablation study of the proposed modules. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chou|adaptive_and_generative_zeroshot_learning", "pdf": "/pdf/c95de71bec56a004df30033ab55061c714367261.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nchou2021adaptive,\ntitle={Adaptive and Generative Zero-Shot Learning},\nauthor={Yu-Ying Chou and Hsuan-Tien Lin and Tyng-Luh Liu},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ahAUv8TI2Mz}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ahAUv8TI2Mz", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2713/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2713/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2713/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2713/Authors|ICLR.cc/2021/Conference/Paper2713/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2713/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923845245, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2713/-/Official_Comment"}}}, {"id": "kPgxk-5kvT", "original": null, "number": 2, "cdate": 1606234795543, "ddate": null, "tcdate": 1606234795543, "tmdate": 1606234795543, "tddate": null, "forum": "ahAUv8TI2Mz", "replyto": "4Hwqf6mgXl", "invitation": "ICLR.cc/2021/Conference/Paper2713/-/Official_Comment", "content": {"title": "Responses to AnonReviewer4", "comment": "1. The seen expert f_s and unseen expert f_u are two different models, which do not share any parameters. They are trained independently, where the seen expert is trained with the real data and the unseen expert is meta-trained with the virtual data. The architecture of such an expert is illustrated in Figure 3.\n\n\n2. We will improve our writing to better explain the implementation of the baseline in Section 4.3. For the ablation study, the baseline also includes the seen and unseen experts. They do not include IAS and both use the image features from the fine-tuned ResNet101 backbone. So, learning the unseen expert of the baseline is now reduced to learning the S2V embedding for the unseen classes. We adopt the conventional zero-shot learning technique that employs episodic meta-learning over the seen classes/samples to obtain the zero-shot S2V embedding.\n\n\n3. Although the seen expert is trained only with the samples from the seen classes, its class prediction is indeed over all classes (including both seen and unseen), as indicated in equation (1). In learning the seen expert, we expect most of the training samples from the various seen classes can be correctly predicted. Under this assumption, if a training sample is predicted by the seen expert not within any of the seen classes, it would be considered abnormal. In other words, we do not assume that a sample of an unseen class would yield high probabilities of unseen classes by the seen expert. After all, we do not have access to those data of unseen classes in training. In inference, if a sample is predicted by the seen expert as abnormal (i.e., not within the seen category), either it is from a seen class but cannot be correctly predicted by the seen expert, or it is from an unseen class. Either way, its classification will be decided by the unseen expert.\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2713/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2713/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive and Generative Zero-Shot Learning", "authorids": ["~Yu-Ying_Chou1", "~Hsuan-Tien_Lin1", "~Tyng-Luh_Liu1"], "authors": ["Yu-Ying Chou", "Hsuan-Tien Lin", "Tyng-Luh Liu"], "keywords": ["Generalized zero-shot learning", "mixup"], "abstract": "We address the problem of generalized zero-shot learning (GZSL) where the task is to predict the class label of a target image whether its label belongs to the seen or unseen category. Similar to ZSL, the learning setting assumes that all class-level semantic features are given, while only the images of seen classes are available for training. By exploring the correlation between image features and the corresponding semantic features, the main idea of the proposed approach is to enrich the semantic-to-visual (S2V) embeddings via a seamless fusion of adaptive and generative learning. To this end, we extend the semantic features of each class by supplementing image-adaptive attention so that the learned S2V embedding can account for not only inter-class but also intra-class variations. In addition, to break the limit of training with images only from seen classes, we design a generative scheme to simultaneously generate virtual class labels and their visual features by sampling and interpolating over seen counterparts. In inference, a testing image will give rise to two different S2V embeddings, seen and virtual. The former is used to decide whether the underlying label is of the unseen category or otherwise a specific seen class; the latter is to predict an unseen class label. To demonstrate the effectiveness of our method, we report state-of-the-art results on four standard GZSL datasets, including an ablation study of the proposed modules. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chou|adaptive_and_generative_zeroshot_learning", "pdf": "/pdf/c95de71bec56a004df30033ab55061c714367261.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nchou2021adaptive,\ntitle={Adaptive and Generative Zero-Shot Learning},\nauthor={Yu-Ying Chou and Hsuan-Tien Lin and Tyng-Luh Liu},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ahAUv8TI2Mz}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ahAUv8TI2Mz", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2713/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2713/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2713/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2713/Authors|ICLR.cc/2021/Conference/Paper2713/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2713/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923845245, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2713/-/Official_Comment"}}}, {"id": "4Hwqf6mgXl", "original": null, "number": 1, "cdate": 1603680566970, "ddate": null, "tcdate": 1603680566970, "tmdate": 1605024148472, "tddate": null, "forum": "ahAUv8TI2Mz", "replyto": "ahAUv8TI2Mz", "invitation": "ICLR.cc/2021/Conference/Paper2713/-/Official_Review", "content": {"title": "An interesting approach for zero-shot learning leveraging the instance attention and synthetic data to enhance the unseen classifier", "review": "Summary: The authors proposed an interesting method for zero-shot learning. In particular, the authors adopted an attention mechanism from the input feature in the semantic to visual mapping, to introduce intra-class variations in the visual space. They also propose a process to synthesize \"fake\" class representations such that a classifier for unseen classes can be trained.  Combining these two the authors demonstrated significant results on benchmark zero-shot learning datasets.\n\nReason for score: I think this paper is marginally below the acceptance threshold. It proposed an elegant method which produces state-of-the-art results on benchmark datasets. But more details need to be revealed to help readers understand the method (discussed below), and I hope the authors can address my concerns during the rebuttal.\n\nPros:\n1. The paper proposed an interesting attention-based method to introduce intra-class variations to the representations for each class. \n\n2. The authors proposed to synthesize virtual classes to enrich the data for training an unseen classifier.\n\n3. The extensive experiments on the benchmark datasets illustrate the superiority of the proposed method.\n\nCons:\n1. The overall training scheme is not very clear and I hope the authors can clarify them. For example, what is the order of training the seen/unseen experts? Do they share any component, like the IAS or S2V module?\n\n2. Baseline explanation is missing. In Sec. 4.3, the authors demonstrate the ablation study where the baseline model does not have IAS or the virtual classes training scheme. How is this baseline implemented?  Without the virtual classes how would you train the unseen expert? My understanding is that without the IAS the model reduces to pure S2V and without the virtual classes the model can only train on seen classes, in which the entire model reduce to a single cosine similarity classifier in the visual feature space (given the seen features and semantics). Then how do you leverage the seen/unseen expert inference strategy? I am curious about it because this baseline reaches considerably high performance in Table 2, in fact already outperforms many state-of-the-art methods in Table 1. It is kind of shocking to me that such a naive baseline contributes this level of accuracy improvement, if I didn't misunderstand it. I would be appreciated if the authors can deliberate this part, so readers can understand the contribution of each module in the proposed method.\n\n3. Why is seen expert detecting the abnormal? In Sec. 3.3, the authors claim that \"Such optimization (the cross-entropy) can be thought of as one-category anomaly detection over only seen classes\". During training, since the seen expert has no data for unseen classes, the classifier is easily biased to the seen classes, even the new data is from an unseen class in the inference time. That's one of the reasons why many embedding based zsl methods have very poor generalized zero-shot learning performance [Xian et al, cvpr 2017]. Could authors deliberate more on why this specific optimization (the cross-entropy on the seen) can address this issue, such that the model can work as a anomaly detector and predict higher score for unseen classes when unseen example is input? It would also be helpful if the authors can demonstrate the predicted scores of the unseen examples empirically.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper2713/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2713/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive and Generative Zero-Shot Learning", "authorids": ["~Yu-Ying_Chou1", "~Hsuan-Tien_Lin1", "~Tyng-Luh_Liu1"], "authors": ["Yu-Ying Chou", "Hsuan-Tien Lin", "Tyng-Luh Liu"], "keywords": ["Generalized zero-shot learning", "mixup"], "abstract": "We address the problem of generalized zero-shot learning (GZSL) where the task is to predict the class label of a target image whether its label belongs to the seen or unseen category. Similar to ZSL, the learning setting assumes that all class-level semantic features are given, while only the images of seen classes are available for training. By exploring the correlation between image features and the corresponding semantic features, the main idea of the proposed approach is to enrich the semantic-to-visual (S2V) embeddings via a seamless fusion of adaptive and generative learning. To this end, we extend the semantic features of each class by supplementing image-adaptive attention so that the learned S2V embedding can account for not only inter-class but also intra-class variations. In addition, to break the limit of training with images only from seen classes, we design a generative scheme to simultaneously generate virtual class labels and their visual features by sampling and interpolating over seen counterparts. In inference, a testing image will give rise to two different S2V embeddings, seen and virtual. The former is used to decide whether the underlying label is of the unseen category or otherwise a specific seen class; the latter is to predict an unseen class label. To demonstrate the effectiveness of our method, we report state-of-the-art results on four standard GZSL datasets, including an ablation study of the proposed modules. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chou|adaptive_and_generative_zeroshot_learning", "pdf": "/pdf/c95de71bec56a004df30033ab55061c714367261.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nchou2021adaptive,\ntitle={Adaptive and Generative Zero-Shot Learning},\nauthor={Yu-Ying Chou and Hsuan-Tien Lin and Tyng-Luh Liu},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ahAUv8TI2Mz}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "ahAUv8TI2Mz", "replyto": "ahAUv8TI2Mz", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2713/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538090196, "tmdate": 1606915764291, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2713/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2713/-/Official_Review"}}}, {"id": "CEeF3wtaskH", "original": null, "number": 2, "cdate": 1603731969371, "ddate": null, "tcdate": 1603731969371, "tmdate": 1605024148408, "tddate": null, "forum": "ahAUv8TI2Mz", "replyto": "ahAUv8TI2Mz", "invitation": "ICLR.cc/2021/Conference/Paper2713/-/Official_Review", "content": {"title": "Please find below for the detailed comments", "review": "This paper solves ZSL and GZSL problem by an elegant fusion of adaptive and generative learning. Different from previous generative models that synthesize unseen samples for training the model, authors create virtual classes as unseen classes in the training phase. In addition, four standard GZSL datasets demonstrate the effectiveness of the proposed method. \nStrengths:\n1. The motivation and the contribution are clearly presented in this manuscript. In general, this paper is well-written.\n2. Instead of generating synthetic data of unseen classes, authors propose to yield virtual classes and data by mixup interpolations.\n3. The architectures of IAS and S2V are simple and effective in terms of experimental results.\nWeaknesses:\n1. Parameter settings for the network are unclear.\n2. More generative ZSL methods from CVPR\u201920 or ECCV\u201920 should be compared.\n3. More experimental results such as a tsne illustration for generated virtual classes should be given.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2713/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2713/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive and Generative Zero-Shot Learning", "authorids": ["~Yu-Ying_Chou1", "~Hsuan-Tien_Lin1", "~Tyng-Luh_Liu1"], "authors": ["Yu-Ying Chou", "Hsuan-Tien Lin", "Tyng-Luh Liu"], "keywords": ["Generalized zero-shot learning", "mixup"], "abstract": "We address the problem of generalized zero-shot learning (GZSL) where the task is to predict the class label of a target image whether its label belongs to the seen or unseen category. Similar to ZSL, the learning setting assumes that all class-level semantic features are given, while only the images of seen classes are available for training. By exploring the correlation between image features and the corresponding semantic features, the main idea of the proposed approach is to enrich the semantic-to-visual (S2V) embeddings via a seamless fusion of adaptive and generative learning. To this end, we extend the semantic features of each class by supplementing image-adaptive attention so that the learned S2V embedding can account for not only inter-class but also intra-class variations. In addition, to break the limit of training with images only from seen classes, we design a generative scheme to simultaneously generate virtual class labels and their visual features by sampling and interpolating over seen counterparts. In inference, a testing image will give rise to two different S2V embeddings, seen and virtual. The former is used to decide whether the underlying label is of the unseen category or otherwise a specific seen class; the latter is to predict an unseen class label. To demonstrate the effectiveness of our method, we report state-of-the-art results on four standard GZSL datasets, including an ablation study of the proposed modules. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chou|adaptive_and_generative_zeroshot_learning", "pdf": "/pdf/c95de71bec56a004df30033ab55061c714367261.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nchou2021adaptive,\ntitle={Adaptive and Generative Zero-Shot Learning},\nauthor={Yu-Ying Chou and Hsuan-Tien Lin and Tyng-Luh Liu},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ahAUv8TI2Mz}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "ahAUv8TI2Mz", "replyto": "ahAUv8TI2Mz", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2713/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538090196, "tmdate": 1606915764291, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2713/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2713/-/Official_Review"}}}, {"id": "yABHy-V4BAq", "original": null, "number": 3, "cdate": 1603836113524, "ddate": null, "tcdate": 1603836113524, "tmdate": 1605024148351, "tddate": null, "forum": "ahAUv8TI2Mz", "replyto": "ahAUv8TI2Mz", "invitation": "ICLR.cc/2021/Conference/Paper2713/-/Official_Review", "content": {"title": "Review of \"Adaptive and Generative Zero-Shot Learning\"", "review": "I. Summary\n\nThe authors consider (generative) zero-shot classification. Their approach, combines two main aspects: (1) generating \"virtual\" classes by mixup interpolation and (2) they introduce an attention mechanism, dubbed \"image-attentive attention\" to allow their approach to model both intra- and extra- class variations.\n\nII. Strong and weak points.\n\nPositive: \n- Strong results: overall the results are good. Yes, the paper may not account for the latest crop of papers from e.g. CVPR but I feel that is besides the point as these methods are structurally different.\n- Relatively extensive evaluation: there is an ablation study, and a distinction over whether the image encoder is fine-tuned.\n- Writing is clear, paper is reasonably easy to follow.\n\nNegative:\n- A large part of the improvement stems from the virtual classes, which is essentially mixup applied to ZSL. This is somewhat derivative.\n\nIII. Rating and questions to authors\nDespite the negative comment I mentioned, I feel the paper should be accepted. How does the paper work with actual generated samples as in the GAN/VAE-based models? Do these extra samples work well with the virtual samples? Is the information redundant/complementary?", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2713/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2713/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive and Generative Zero-Shot Learning", "authorids": ["~Yu-Ying_Chou1", "~Hsuan-Tien_Lin1", "~Tyng-Luh_Liu1"], "authors": ["Yu-Ying Chou", "Hsuan-Tien Lin", "Tyng-Luh Liu"], "keywords": ["Generalized zero-shot learning", "mixup"], "abstract": "We address the problem of generalized zero-shot learning (GZSL) where the task is to predict the class label of a target image whether its label belongs to the seen or unseen category. Similar to ZSL, the learning setting assumes that all class-level semantic features are given, while only the images of seen classes are available for training. By exploring the correlation between image features and the corresponding semantic features, the main idea of the proposed approach is to enrich the semantic-to-visual (S2V) embeddings via a seamless fusion of adaptive and generative learning. To this end, we extend the semantic features of each class by supplementing image-adaptive attention so that the learned S2V embedding can account for not only inter-class but also intra-class variations. In addition, to break the limit of training with images only from seen classes, we design a generative scheme to simultaneously generate virtual class labels and their visual features by sampling and interpolating over seen counterparts. In inference, a testing image will give rise to two different S2V embeddings, seen and virtual. The former is used to decide whether the underlying label is of the unseen category or otherwise a specific seen class; the latter is to predict an unseen class label. To demonstrate the effectiveness of our method, we report state-of-the-art results on four standard GZSL datasets, including an ablation study of the proposed modules. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chou|adaptive_and_generative_zeroshot_learning", "pdf": "/pdf/c95de71bec56a004df30033ab55061c714367261.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nchou2021adaptive,\ntitle={Adaptive and Generative Zero-Shot Learning},\nauthor={Yu-Ying Chou and Hsuan-Tien Lin and Tyng-Luh Liu},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ahAUv8TI2Mz}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "ahAUv8TI2Mz", "replyto": "ahAUv8TI2Mz", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2713/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538090196, "tmdate": 1606915764291, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2713/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2713/-/Official_Review"}}}, {"id": "d956RA2nEAL", "original": null, "number": 4, "cdate": 1604064942818, "ddate": null, "tcdate": 1604064942818, "tmdate": 1605024148287, "tddate": null, "forum": "ahAUv8TI2Mz", "replyto": "ahAUv8TI2Mz", "invitation": "ICLR.cc/2021/Conference/Paper2713/-/Official_Review", "content": {"title": "This paper describes a method for GZSL using a combination of adaptive and generative techniques to create virtual classes that help account for unseen classes.", "review": "Due to time shortage this will be a short review. I have gone through the paper carefully.\nMotivation\nThe authors motivate their work well and have a thorough literature survey.\nMethod\nThe authors essentially create clusters that can anticipate unseen classes. That idea is not new in and of itself but the authors' realization of the idea through mix-ups and image-adaptive semantics is new and interesting. The overall approach is technically sound and each component is well motivated and described.\n\nResults\nThe results are convincing. The authors get an across the board improvement over the state of the art.\nClarity\nThe paper is clearly written and has a good logical flow. I would recommend not using adjectives like elegant and insightful for one's own work. Perhaps such assessments are best left to the reviewers.\nQuality, originality and significance\nThe paper presents moderate innovation in my view. It is thorough and hence does deserve consideration. In short, good quality, reasonable originality and decent significance.\n", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper2713/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2713/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive and Generative Zero-Shot Learning", "authorids": ["~Yu-Ying_Chou1", "~Hsuan-Tien_Lin1", "~Tyng-Luh_Liu1"], "authors": ["Yu-Ying Chou", "Hsuan-Tien Lin", "Tyng-Luh Liu"], "keywords": ["Generalized zero-shot learning", "mixup"], "abstract": "We address the problem of generalized zero-shot learning (GZSL) where the task is to predict the class label of a target image whether its label belongs to the seen or unseen category. Similar to ZSL, the learning setting assumes that all class-level semantic features are given, while only the images of seen classes are available for training. By exploring the correlation between image features and the corresponding semantic features, the main idea of the proposed approach is to enrich the semantic-to-visual (S2V) embeddings via a seamless fusion of adaptive and generative learning. To this end, we extend the semantic features of each class by supplementing image-adaptive attention so that the learned S2V embedding can account for not only inter-class but also intra-class variations. In addition, to break the limit of training with images only from seen classes, we design a generative scheme to simultaneously generate virtual class labels and their visual features by sampling and interpolating over seen counterparts. In inference, a testing image will give rise to two different S2V embeddings, seen and virtual. The former is used to decide whether the underlying label is of the unseen category or otherwise a specific seen class; the latter is to predict an unseen class label. To demonstrate the effectiveness of our method, we report state-of-the-art results on four standard GZSL datasets, including an ablation study of the proposed modules. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chou|adaptive_and_generative_zeroshot_learning", "pdf": "/pdf/c95de71bec56a004df30033ab55061c714367261.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nchou2021adaptive,\ntitle={Adaptive and Generative Zero-Shot Learning},\nauthor={Yu-Ying Chou and Hsuan-Tien Lin and Tyng-Luh Liu},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ahAUv8TI2Mz}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "ahAUv8TI2Mz", "replyto": "ahAUv8TI2Mz", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2713/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538090196, "tmdate": 1606915764291, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2713/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2713/-/Official_Review"}}}, {"id": "f1F0sD9ejJy", "original": null, "number": 5, "cdate": 1604707135595, "ddate": null, "tcdate": 1604707135595, "tmdate": 1605024148223, "tddate": null, "forum": "ahAUv8TI2Mz", "replyto": "ahAUv8TI2Mz", "invitation": "ICLR.cc/2021/Conference/Paper2713/-/Official_Review", "content": {"title": "Adaptive and Generative Zero-Shot Learning", "review": "The paper proposes a framework for the GZSL using the meta-learning and attention mechanism. The image-guided attention on the semantic space helps to adapt the better class specific semantic information. The modified semantic space projected to the visual space and in the visual space, cosine similarity is measured. The paper learns separate expert for the seen and unseen classes. The unseen class expert is trained with the pseudo negative samples with pseudo negative labels. Meta-learning based training helps to learn the model when only a few examples per class are available.\n\nComment: \n1: The adaptive modification of the semantic space is novel in the ZSL setup with clear intuition. Also, the mixup idea in the ZSL setup seems to work well, it helps to separate the seen and unseen class data hence choosing the seen/unseen expert is easy. \n2: The paper shows a strong result on the standard ZSL dataset. The ablation over the various component shows the efficacy of the proposed component. \n\n3: How meta-learning is used in the paper is not clear also in the implementation details it's not mentioned. What is the task definition? How meta-train and meta-test are defined? The paper mention they use meta-learning, but nothing is clear about this. Therefore reproducibility is the main issue, and I think using the provided information the reported result can not be reproduced. Please update the experimental setup and provide the detail so that result can be easily reproduced.\n\n4: There are a few recent works [a,b,c,d] that explore the meta-learning framework for the ZSL setup. The author should mention these paper and add the advantage of the proposed model over the other meta-learning approach also these result should be compared. Note: [d] is mentioned in the paper. Also please add an ablation if the model has advantage using the meta-learning, i.e. what is the result of the proposed component if we use the simple network without meta-learning.\n\n[a] Meta-Learning for Generalized Zero-Shot Learning, AAAI-20\n[d] Episode-based prototype generating network for zero-shot learning, CVPR-20\n[b] Learning to Compare: Relation Network for Few-Shot Learning, CVPR-18\n[c] Correction Networks: Meta-Learning for Zero-Shot Learning, ArXiv\n\n5: Reproducibility is the main challenge here I request the author, please provide the code for the CUB dataset (since its look too high, I suspect the result in the inductive setting).\n\n6: In the table-1, what is T for the ZSL result? Is it transductive setting?\n\nOverall I like the idea, and paper shows a good result. Reproducibility is the main challenge, and CUB results look too high, I like to verify the result, therefore requesting the author to submit the code for the CUB dataset. Also, please discuss the suggested paper that is also using meta-learning based training. I will update my score on the successful verification of the code and result and the updated paper quality.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper2713/AnonReviewer5"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2713/AnonReviewer5"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive and Generative Zero-Shot Learning", "authorids": ["~Yu-Ying_Chou1", "~Hsuan-Tien_Lin1", "~Tyng-Luh_Liu1"], "authors": ["Yu-Ying Chou", "Hsuan-Tien Lin", "Tyng-Luh Liu"], "keywords": ["Generalized zero-shot learning", "mixup"], "abstract": "We address the problem of generalized zero-shot learning (GZSL) where the task is to predict the class label of a target image whether its label belongs to the seen or unseen category. Similar to ZSL, the learning setting assumes that all class-level semantic features are given, while only the images of seen classes are available for training. By exploring the correlation between image features and the corresponding semantic features, the main idea of the proposed approach is to enrich the semantic-to-visual (S2V) embeddings via a seamless fusion of adaptive and generative learning. To this end, we extend the semantic features of each class by supplementing image-adaptive attention so that the learned S2V embedding can account for not only inter-class but also intra-class variations. In addition, to break the limit of training with images only from seen classes, we design a generative scheme to simultaneously generate virtual class labels and their visual features by sampling and interpolating over seen counterparts. In inference, a testing image will give rise to two different S2V embeddings, seen and virtual. The former is used to decide whether the underlying label is of the unseen category or otherwise a specific seen class; the latter is to predict an unseen class label. To demonstrate the effectiveness of our method, we report state-of-the-art results on four standard GZSL datasets, including an ablation study of the proposed modules. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chou|adaptive_and_generative_zeroshot_learning", "pdf": "/pdf/c95de71bec56a004df30033ab55061c714367261.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nchou2021adaptive,\ntitle={Adaptive and Generative Zero-Shot Learning},\nauthor={Yu-Ying Chou and Hsuan-Tien Lin and Tyng-Luh Liu},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ahAUv8TI2Mz}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "ahAUv8TI2Mz", "replyto": "ahAUv8TI2Mz", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2713/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538090196, "tmdate": 1606915764291, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2713/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2713/-/Official_Review"}}}], "count": 12}