{"notes": [{"tddate": null, "ddate": null, "cdate": null, "original": null, "tmdate": 1490028623388, "tcdate": 1490028623388, "number": 1, "id": "HkxPIOtasl", "invitation": "ICLR.cc/2017/workshop/-/paper137/acceptance", "forum": "S15PPJStl", "replyto": "S15PPJStl", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Accept", "title": "ICLR committee final decision"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Loss is its own Reward: Self-Supervision for Reinforcement Learning", "abstract": "Reinforcement learning, driven by reward, addresses tasks by optimizing policies for expected return. Need the supervision be so narrow? Reward is delayed and sparse for many tasks, so we argue that reward alone is a noisy and impoverished signal for end-to-end optimization. To augment reward, we consider self-supervised tasks that incorporate states, actions, and successors to provide auxiliary losses. These losses offer ubiquitous and instantaneous supervision for representation learning even in the absence of reward. Self-supervised pre-training improves the data efficiency and returns of end-to-end reinforcement learning on Atari.", "pdf": "/pdf/d2d1a0fe879d5a877ff67c8460204afe8acb54a3.pdf", "TL;DR": "Pre-training with auxiliary losses improves the data efficiency of policy optimization on Atari.", "paperhash": "shelhamer|loss_is_its_own_reward_selfsupervision_for_reinforcement_learning", "keywords": ["Deep learning", "Unsupervised Learning", "Reinforcement Learning"], "conflicts": ["cs.berkeley.edu", "openai.com"], "authors": ["Evan Shelhamer", "Parsa Mahmoudieh", "Max Argus", "Trevor Darrell"], "authorids": ["shelhamer@cs.berkeley.edu", "parsa.m@berkeley.edu", "argus.max@gmail.com", "trevor@cs.berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1490028623943, "id": "ICLR.cc/2017/workshop/-/paper137/acceptance", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "S15PPJStl", "replyto": "S15PPJStl", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept", "Reject"]}}}, "nonreaders": [], "cdate": 1490028623943}}}, {"tddate": null, "nonreaders": null, "tmdate": 1489746370266, "tcdate": 1489746233257, "number": 2, "id": "BJZHFNtjl", "invitation": "ICLR.cc/2017/workshop/-/paper137/public/comment", "forum": "S15PPJStl", "replyto": "H1emyzvsx", "signatures": ["~Evan_G_Shelhamer1"], "readers": ["everyone"], "writers": ["~Evan_G_Shelhamer1"], "content": {"title": "Authors' Response", "comment": "Thank you for the review.\n\n> It seems magic to me why some auxiliary tasks unrelated to the true reward can lead to high score.\n> there are already many previous works on auxiliary tasks\n\nWe agree that self-supervision for RL deserves further attention and explanation. This is why we carry out a fair comparison of various auxiliary losses including novel, discriminative forms. While there are other works on auxiliary tasks for RL, the union of the approaches and results is more informative as each sheds light on different aspects.\n\n> self-supervision as surrogate rewards\n\nOur self-supervisory signals are formulated as losses, unlike the surrogate/pseudo rewards or measurements of the ICLR17 papers by Mnih et al. and Dosovitskiy et al. On every transition our auxiliary tasks yield instantaneous gradients for representation learning (with lower variance and faster convergence than longer horizon optimization). The losses we define are more universal than the spatial navigation losses of Mirowski et al. in ICLR17.\n\n> it is not clear what the baseline is\n\nThe baseline is vanilla A3C (Mnih et al. 2016), as our self-supervised policies are instantiated as variations of A3C that take the actor-critic as an encoder to which each task attaches its own decoder. A3C achieves reasonable scores on these environments (which we inherit from the original paper).\n\n> dynamics (predicting the next state)\n\nOur dynamics task is not next state prediction, but verification of true/corrupted timesteps (it's a classification, not reconstruction). A trend throughout our results is that discriminative losses help more than reconstruction losses.\n\n> we could learn the low-level feature better with auxiliary tasks?\n\nWe have already done experiments to investigate the cause(s) of improvement. That comparable improvement can be observed when transferring only the conv. parameters and reinitializing the fully-connected layer shared by actor-critic and auxiliary heads suggests this is the case. Furthermore, decoding from the (fixed) RL representation to our auxiliary tasks shows a certain degree of commonality. Lastly, the lower improvement for data-dependent init. relative to self-supervision suggests that the effect is not purely from better conditioning of the weights.\n\nTo reproduce and further explore our results we will make our self-supervision and policy optimization code public.\n\nFor more detail please refer to our arxiv (which more thoroughly explains the tasks and reports policy decoding results): https://arxiv.org/abs/1612.07307"}, "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Loss is its own Reward: Self-Supervision for Reinforcement Learning", "abstract": "Reinforcement learning, driven by reward, addresses tasks by optimizing policies for expected return. Need the supervision be so narrow? Reward is delayed and sparse for many tasks, so we argue that reward alone is a noisy and impoverished signal for end-to-end optimization. To augment reward, we consider self-supervised tasks that incorporate states, actions, and successors to provide auxiliary losses. These losses offer ubiquitous and instantaneous supervision for representation learning even in the absence of reward. Self-supervised pre-training improves the data efficiency and returns of end-to-end reinforcement learning on Atari.", "pdf": "/pdf/d2d1a0fe879d5a877ff67c8460204afe8acb54a3.pdf", "TL;DR": "Pre-training with auxiliary losses improves the data efficiency of policy optimization on Atari.", "paperhash": "shelhamer|loss_is_its_own_reward_selfsupervision_for_reinforcement_learning", "keywords": ["Deep learning", "Unsupervised Learning", "Reinforcement Learning"], "conflicts": ["cs.berkeley.edu", "openai.com"], "authors": ["Evan Shelhamer", "Parsa Mahmoudieh", "Max Argus", "Trevor Darrell"], "authorids": ["shelhamer@cs.berkeley.edu", "parsa.m@berkeley.edu", "argus.max@gmail.com", "trevor@cs.berkeley.edu"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487365987333, "tcdate": 1487365987333, "id": "ICLR.cc/2017/workshop/-/paper137/public/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2017/workshop/paper137/reviewers"], "reply": {"forum": "S15PPJStl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1487365987333}}}, {"tddate": null, "nonreaders": null, "tmdate": 1489604566718, "tcdate": 1489604375656, "number": 2, "id": "H1emyzvsx", "invitation": "ICLR.cc/2017/workshop/-/paper137/official/review", "forum": "S15PPJStl", "replyto": "S15PPJStl", "signatures": ["ICLR.cc/2017/workshop/paper137/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper137/AnonReviewer3"], "content": {"title": "Review", "rating": "5: Marginally below acceptance threshold", "review": "The paper proposes to use self-supervision as surrogate rewards for deep reinforcement learning. Such self-supervision includes reward by itself, dynamics (predicting the next state) and inverse dynamics (predicting the action given start/end state) and reconstruction (with VAE, etc). \n\nThe paper shows some promising results. In particular, the sample efficiency is higher than the baseline (although it is not clear what the baseline is from the paper...). It seems magic to me why some auxiliary tasks unrelated to the true reward can lead to high score. Is that because we could learn the low-level feature better with auxiliary tasks? Obviously there is much more to experiment and explain.\n\nFor novelty, there are already many previous works on auxiliary tasks. So the idea is not new (except for reconstruction of the state via VAE).  How the auxiliary tasks are done is also quite standard (one network with multiple heads to predict them). \n\nPros:\nSome promising results. Paper is easy to read. \n\nCons:\nIdeas are not novel. Key performance gain in the paper are not explained well. More experiments are needed. \n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Loss is its own Reward: Self-Supervision for Reinforcement Learning", "abstract": "Reinforcement learning, driven by reward, addresses tasks by optimizing policies for expected return. Need the supervision be so narrow? Reward is delayed and sparse for many tasks, so we argue that reward alone is a noisy and impoverished signal for end-to-end optimization. To augment reward, we consider self-supervised tasks that incorporate states, actions, and successors to provide auxiliary losses. These losses offer ubiquitous and instantaneous supervision for representation learning even in the absence of reward. Self-supervised pre-training improves the data efficiency and returns of end-to-end reinforcement learning on Atari.", "pdf": "/pdf/d2d1a0fe879d5a877ff67c8460204afe8acb54a3.pdf", "TL;DR": "Pre-training with auxiliary losses improves the data efficiency of policy optimization on Atari.", "paperhash": "shelhamer|loss_is_its_own_reward_selfsupervision_for_reinforcement_learning", "keywords": ["Deep learning", "Unsupervised Learning", "Reinforcement Learning"], "conflicts": ["cs.berkeley.edu", "openai.com"], "authors": ["Evan Shelhamer", "Parsa Mahmoudieh", "Max Argus", "Trevor Darrell"], "authorids": ["shelhamer@cs.berkeley.edu", "parsa.m@berkeley.edu", "argus.max@gmail.com", "trevor@cs.berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489604376449, "id": "ICLR.cc/2017/workshop/-/paper137/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper137/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper137/AnonReviewer2", "ICLR.cc/2017/workshop/paper137/AnonReviewer3"], "reply": {"forum": "S15PPJStl", "replyto": "S15PPJStl", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper137/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper137/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489604376449}}}, {"tddate": null, "tmdate": 1489460743390, "tcdate": 1489460743390, "number": 1, "id": "By1GA0Eig", "invitation": "ICLR.cc/2017/workshop/-/paper137/public/comment", "forum": "S15PPJStl", "replyto": "SJD_neMix", "signatures": ["~Evan_G_Shelhamer1"], "readers": ["everyone"], "writers": ["~Evan_G_Shelhamer1"], "content": {"title": "Authors' Response", "comment": "Thank you for the review.\n\nWith respect to originality, our work experimentally demonstrates a representation bottleneck, introduces new auxiliary losses, and shows that self-supervised pre-training alone can improve data efficiency.\n\nTo further situate our work in the context of the other ICLR submissions mentioned, we consider the following contrasts:\n\n- Our self-supervised tasks do not require additional privileged information. Dosovitskiy et al. encode game quantities like health and ammunition while Mirowski et al. require depth input and perfect odometry. Our losses augment standard policy optimization without needing any further supervisory signals.\n- We focus on discriminative formulations of auxiliary losses. The dynamics verification task (recognizing true successors) and inverse dynamics task (recognizing the effects of actions) are novel auxiliary tasks for representation learning. These are distinct from the the tasks in related works and reconstructive/generative approaches.\n- Our auxiliary tasks are framed as direct losses and not control. Mnih et al. define pseudo-rewards that are then optimized by off-policy Q-learning while Dosovitskiy et al. define measurements and act according to their temporal differences across multiple long horizons. Our work shows that auxiliary losses without control suffice to improve optimization.\n\nFor more detail please refer to our arxiv (first uploaded in December): https://arxiv.org/abs/1612.07307"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Loss is its own Reward: Self-Supervision for Reinforcement Learning", "abstract": "Reinforcement learning, driven by reward, addresses tasks by optimizing policies for expected return. Need the supervision be so narrow? Reward is delayed and sparse for many tasks, so we argue that reward alone is a noisy and impoverished signal for end-to-end optimization. To augment reward, we consider self-supervised tasks that incorporate states, actions, and successors to provide auxiliary losses. These losses offer ubiquitous and instantaneous supervision for representation learning even in the absence of reward. Self-supervised pre-training improves the data efficiency and returns of end-to-end reinforcement learning on Atari.", "pdf": "/pdf/d2d1a0fe879d5a877ff67c8460204afe8acb54a3.pdf", "TL;DR": "Pre-training with auxiliary losses improves the data efficiency of policy optimization on Atari.", "paperhash": "shelhamer|loss_is_its_own_reward_selfsupervision_for_reinforcement_learning", "keywords": ["Deep learning", "Unsupervised Learning", "Reinforcement Learning"], "conflicts": ["cs.berkeley.edu", "openai.com"], "authors": ["Evan Shelhamer", "Parsa Mahmoudieh", "Max Argus", "Trevor Darrell"], "authorids": ["shelhamer@cs.berkeley.edu", "parsa.m@berkeley.edu", "argus.max@gmail.com", "trevor@cs.berkeley.edu"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487365987333, "tcdate": 1487365987333, "id": "ICLR.cc/2017/workshop/-/paper137/public/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2017/workshop/paper137/reviewers"], "reply": {"forum": "S15PPJStl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1487365987333}}}, {"tddate": null, "tmdate": 1489271918565, "tcdate": 1489271918565, "number": 1, "id": "SJD_neMix", "invitation": "ICLR.cc/2017/workshop/-/paper137/official/review", "forum": "S15PPJStl", "replyto": "S15PPJStl", "signatures": ["ICLR.cc/2017/workshop/paper137/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper137/AnonReviewer2"], "content": {"title": "Minor, but timely, contribution", "rating": "4: Ok but not good enough - rejection", "review": "The submission uses a number of different self-supervised losses to improve data efficiency on Atari games. The authors experiment with pre-training the net using reward prediction, state or action prediction, and reconstruction, and obtain a moderate speed up on the subsequent RL task. \n\nAlthough the quality and clarity of the submission is high, it is hard to see any originality here, given the recent papers from Mnih et al, Mirowski et al, and Dosovitskiy et al. The authors state that their paper is 'concurrent' with these other papers, but this is misleading - the other three papers were published well before the workshop deadline and at least the first two were presented at NIPS workshops in December. \n\nPros: the paper is well written and motivated, with clear results. This is an active area of research in deep RL, so even incremental results are of interest.\nCons: the only really new result is the use of the VAE for pretraining, which was a negative result (it hurt the RL performance).  In my view, workshop papers should really have more novelty.\n\n", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Loss is its own Reward: Self-Supervision for Reinforcement Learning", "abstract": "Reinforcement learning, driven by reward, addresses tasks by optimizing policies for expected return. Need the supervision be so narrow? Reward is delayed and sparse for many tasks, so we argue that reward alone is a noisy and impoverished signal for end-to-end optimization. To augment reward, we consider self-supervised tasks that incorporate states, actions, and successors to provide auxiliary losses. These losses offer ubiquitous and instantaneous supervision for representation learning even in the absence of reward. Self-supervised pre-training improves the data efficiency and returns of end-to-end reinforcement learning on Atari.", "pdf": "/pdf/d2d1a0fe879d5a877ff67c8460204afe8acb54a3.pdf", "TL;DR": "Pre-training with auxiliary losses improves the data efficiency of policy optimization on Atari.", "paperhash": "shelhamer|loss_is_its_own_reward_selfsupervision_for_reinforcement_learning", "keywords": ["Deep learning", "Unsupervised Learning", "Reinforcement Learning"], "conflicts": ["cs.berkeley.edu", "openai.com"], "authors": ["Evan Shelhamer", "Parsa Mahmoudieh", "Max Argus", "Trevor Darrell"], "authorids": ["shelhamer@cs.berkeley.edu", "parsa.m@berkeley.edu", "argus.max@gmail.com", "trevor@cs.berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489604376449, "id": "ICLR.cc/2017/workshop/-/paper137/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper137/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper137/AnonReviewer2", "ICLR.cc/2017/workshop/paper137/AnonReviewer3"], "reply": {"forum": "S15PPJStl", "replyto": "S15PPJStl", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper137/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper137/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489604376449}}}, {"tddate": null, "replyto": null, "nonreaders": null, "ddate": null, "tmdate": 1487468386203, "tcdate": 1487365986407, "number": 137, "id": "S15PPJStl", "invitation": "ICLR.cc/2017/workshop/-/submission", "forum": "S15PPJStl", "signatures": ["~Evan_G_Shelhamer1"], "readers": ["everyone"], "content": {"title": "Loss is its own Reward: Self-Supervision for Reinforcement Learning", "abstract": "Reinforcement learning, driven by reward, addresses tasks by optimizing policies for expected return. Need the supervision be so narrow? Reward is delayed and sparse for many tasks, so we argue that reward alone is a noisy and impoverished signal for end-to-end optimization. To augment reward, we consider self-supervised tasks that incorporate states, actions, and successors to provide auxiliary losses. These losses offer ubiquitous and instantaneous supervision for representation learning even in the absence of reward. Self-supervised pre-training improves the data efficiency and returns of end-to-end reinforcement learning on Atari.", "pdf": "/pdf/d2d1a0fe879d5a877ff67c8460204afe8acb54a3.pdf", "TL;DR": "Pre-training with auxiliary losses improves the data efficiency of policy optimization on Atari.", "paperhash": "shelhamer|loss_is_its_own_reward_selfsupervision_for_reinforcement_learning", "keywords": ["Deep learning", "Unsupervised Learning", "Reinforcement Learning"], "conflicts": ["cs.berkeley.edu", "openai.com"], "authors": ["Evan Shelhamer", "Parsa Mahmoudieh", "Max Argus", "Trevor Darrell"], "authorids": ["shelhamer@cs.berkeley.edu", "parsa.m@berkeley.edu", "argus.max@gmail.com", "trevor@cs.berkeley.edu"]}, "writers": [], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1487690420000, "tmdate": 1484242559574, "id": "ICLR.cc/2017/workshop/-/submission", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1495466420000, "cdate": 1484242559574}}}], "count": 6}