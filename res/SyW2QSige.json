{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396674170, "tcdate": 1486396674170, "number": 1, "id": "SJ9WaMU_x", "invitation": "ICLR.cc/2017/conference/-/paper554/acceptance", "forum": "SyW2QSige", "replyto": "SyW2QSige", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "This paper proposes information gain as an intermediate reward signal to train deep networks to answer questions. The motivation and model are interesting, however the experiments fail to deliver. There is a lack of comparative simple baselines, the performance of the model is not sufficiently analyzed, and the actual tasks proposed are too simple to promise that the results would easily generalize to more useful tasks. This paper has a lot of good directions but definitely requires more work. I encourage the authors to follow the advice of reviewers and explore the various directions proposed so that this work can live up to its potential."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards Information-Seeking Agents", "abstract": "We develop a general problem setting for training and testing the ability of agents to gather information efficiently. Specifically, we present a collection of tasks in which success requires searching through a partially-observed environment, for fragments of information which can be pieced together to accomplish various goals. We combine deep architectures with techniques from reinforcement learning to develop agents that solve our tasks. We shape the behavior of these agents by combining extrinsic and intrinsic rewards. We empirically demonstrate that these agents learn to search actively and intelligently for new information to reduce their uncertainty, and to exploit information they have already acquired.", "pdf": "/pdf/1f5ea4d7a782bc373d3b7149d1e96313e80ce8a4.pdf", "TL;DR": "We investigate the behavior of models trained to answer questions by asking sequences of simple questions.", "paperhash": "bachman|towards_informationseeking_agents", "keywords": [], "conflicts": ["maluuba.com", "mcgill.ca", "umontreal.ca"], "authors": ["Philip Bachman", "Alessandro Sordoni", "Adam Trischler"], "authorids": ["phil.bachman@maluuba.com", "alessandro.sordoni@maluuba.com", "adam.trischler@maluuba.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396674750, "id": "ICLR.cc/2017/conference/-/paper554/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "SyW2QSige", "replyto": "SyW2QSige", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396674750}}}, {"tddate": null, "tmdate": 1482269822515, "tcdate": 1482269822515, "number": 3, "id": "S1LFEmwEg", "invitation": "ICLR.cc/2017/conference/-/paper554/official/review", "forum": "SyW2QSige", "replyto": "SyW2QSige", "signatures": ["ICLR.cc/2017/conference/paper554/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper554/AnonReviewer2"], "content": {"title": "Review of \"Towards Information Seeking Agents\"", "rating": "6: Marginally above acceptance threshold", "review": "Pros:\n\n* The general idea behind the paper seems pretty novel and potentially quite cool.\n* The specific technical implementation seems pretty reasonable and well-thought through.\n* The general types of the tasks that they try out their approach on spans a wide and interesting spectrum of cognition abilities. \n* The writing is pretty clear.  I basically felt like I could replicate much of what they did from their paper descriptions. \n\n\nCons:\n\n* The evaluation of the success of these ideas, as compared to other possible approaches, or as compared to human performance on similar tasks, is extremely cursory. \n\n* The specific tasks that they try are quite simple.   I really don't know whether their approach is better than a bunch of simpler things on these tasks.   \n\nTaking these two cons together, it feels like the authors basically get the implementation done and working somewhat, and then just wrote up the paper.  (I know how it feels to be under a deadline without a complete set of results.)   If the authors had used their approach to solve an obviously hard problem that previously was completely unsolved, even the type of cursory evaluation level chosen here would have been fine.  Or if they had done a very thorough evaluation of a bunch of standard models on each task (and humans too, ideally), and compared their model to those results, that would have been great.  But given the complexity of their methods and the fact that the tasks are either not well-known benchmarks or very challenging as such, it's really hard to tell how much of an advance is made here.     But it does seem like a potentially fruitful research direction.   ", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards Information-Seeking Agents", "abstract": "We develop a general problem setting for training and testing the ability of agents to gather information efficiently. Specifically, we present a collection of tasks in which success requires searching through a partially-observed environment, for fragments of information which can be pieced together to accomplish various goals. We combine deep architectures with techniques from reinforcement learning to develop agents that solve our tasks. We shape the behavior of these agents by combining extrinsic and intrinsic rewards. We empirically demonstrate that these agents learn to search actively and intelligently for new information to reduce their uncertainty, and to exploit information they have already acquired.", "pdf": "/pdf/1f5ea4d7a782bc373d3b7149d1e96313e80ce8a4.pdf", "TL;DR": "We investigate the behavior of models trained to answer questions by asking sequences of simple questions.", "paperhash": "bachman|towards_informationseeking_agents", "keywords": [], "conflicts": ["maluuba.com", "mcgill.ca", "umontreal.ca"], "authors": ["Philip Bachman", "Alessandro Sordoni", "Adam Trischler"], "authorids": ["phil.bachman@maluuba.com", "alessandro.sordoni@maluuba.com", "adam.trischler@maluuba.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512544229, "id": "ICLR.cc/2017/conference/-/paper554/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper554/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper554/AnonReviewer3", "ICLR.cc/2017/conference/paper554/AnonReviewer1", "ICLR.cc/2017/conference/paper554/AnonReviewer2"], "reply": {"forum": "SyW2QSige", "replyto": "SyW2QSige", "writers": {"values-regex": "ICLR.cc/2017/conference/paper554/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper554/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512544229}}}, {"tddate": null, "tmdate": 1481909187372, "tcdate": 1481909187372, "number": 2, "id": "Hys67iWNe", "invitation": "ICLR.cc/2017/conference/-/paper554/official/review", "forum": "SyW2QSige", "replyto": "SyW2QSige", "signatures": ["ICLR.cc/2017/conference/paper554/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper554/AnonReviewer1"], "content": {"title": "Promising but unfinished paper", "rating": "4: Ok but not good enough - rejection", "review": "This paper proposes a setting to learn models that will seek information (e.g., by asking question) in order to solve a given task. They introduce a set of tasks that were designed for that goal. They show that it is possible to train models to solve these tasks with reinforcement learning.\n\nOne key motivation for the tasks proposed in this work are the existence of games like 20Q or battleships where an agent needs to ask questions to solve a given task. It is quite surprising that the authors do not actually consider these games as potential tasks to explore (beside the Hangman). It is also not completely clear how the tasks have been selected. A significant amount of work has been dedicated in the past to understand the property of games like 20Q (e.g., Navarro et al., 2010) and how humans solve them.  It would interesting to see how the tasks proposed in this work distinguish themselves from the ones studied in the existing literature, and how humans would perform on them.  In particular, Cohen & Lake, 2016m have recently studied the 20 questions games in their paper \u201cSearching large hypothesis spaces by asking questions\u201d where they both evaluate the performance of humans and computer. I believe that this paper would really benefits from a similar study.\n\nDeveloping the ability of models to actively seek for information to solve a task is a very interesting but challenging problem. In this paper, all of the  tasks require the agent to select a questions from a finite set of clean and informative possibilities. This allows a simpler analysis of how a given agent may perform but at the cost of a reducing the level of noise that would appear in more realistic settings.\n\nThis paper also show that by using a relatively standard mix of deep learning models and reinforcement learning, they are able to train agents that can solve these tasks in the way it was intended to. This validates their empirical setting but also may exhibit some of the limitation of their approach; using relatively toy-ish settings with perfect information and a fixed number of questions may be too simple. \n\nWhile it is interesting to see that their agent are able to perform well on all of their tasks, the absence of baselines limit the conclusions we can draw from these experiments. For example in the Hangman experiment, it seems that the frequency based model obtains promising performance. It would interesting to see how good are baselines that may use the co-occurrence of letters or the frequency of character n-grams.\n\n\nOverall, this paper explores a very interesting direction of research and propose a set of promising tasks to test the capability of a model to learn from asking question. However, the current analysis of the tasks is a bit limited, and it is hard to draw any conclusion from them. It would be good if the paper would focus more on how humans perform on these tasks, on strong simple baselines and on more tasks related to natural language (since it is one of the motivation of this work) rather than on solving them with relatively sophisticated models.\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards Information-Seeking Agents", "abstract": "We develop a general problem setting for training and testing the ability of agents to gather information efficiently. Specifically, we present a collection of tasks in which success requires searching through a partially-observed environment, for fragments of information which can be pieced together to accomplish various goals. We combine deep architectures with techniques from reinforcement learning to develop agents that solve our tasks. We shape the behavior of these agents by combining extrinsic and intrinsic rewards. We empirically demonstrate that these agents learn to search actively and intelligently for new information to reduce their uncertainty, and to exploit information they have already acquired.", "pdf": "/pdf/1f5ea4d7a782bc373d3b7149d1e96313e80ce8a4.pdf", "TL;DR": "We investigate the behavior of models trained to answer questions by asking sequences of simple questions.", "paperhash": "bachman|towards_informationseeking_agents", "keywords": [], "conflicts": ["maluuba.com", "mcgill.ca", "umontreal.ca"], "authors": ["Philip Bachman", "Alessandro Sordoni", "Adam Trischler"], "authorids": ["phil.bachman@maluuba.com", "alessandro.sordoni@maluuba.com", "adam.trischler@maluuba.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512544229, "id": "ICLR.cc/2017/conference/-/paper554/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper554/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper554/AnonReviewer3", "ICLR.cc/2017/conference/paper554/AnonReviewer1", "ICLR.cc/2017/conference/paper554/AnonReviewer2"], "reply": {"forum": "SyW2QSige", "replyto": "SyW2QSige", "writers": {"values-regex": "ICLR.cc/2017/conference/paper554/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper554/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512544229}}}, {"tddate": null, "tmdate": 1481839602817, "tcdate": 1481839602809, "number": 2, "id": "ryje4cgNx", "invitation": "ICLR.cc/2017/conference/-/paper554/public/comment", "forum": "SyW2QSige", "replyto": "BytGYPeNe", "signatures": ["~Philip_Bachman1"], "readers": ["everyone"], "writers": ["~Philip_Bachman1"], "content": {"title": "response to review", "comment": "The intent of our paper is to explore the idea of information seeking, to propose several tasks which demand that an agent exhibit information seeking behaviour, and to show that existing techniques (with a bit of effort) are capable of solving these tasks. Our primary focus is not on the particular optimization method (i.e. GAE), model architecture (i.e. DNN), or training heuristics (e.g. information gain and various low-level tweaks that improved the performance of our models). Our focus is on the tasks themselves and whether or not agents can be trained to perform the information seeking required to solve them. We didn't discuss training details extensively in the paper, since that was not our focus, but some non-trivial tweaks and tricks were required for maximizing the performance of our models on these tasks."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards Information-Seeking Agents", "abstract": "We develop a general problem setting for training and testing the ability of agents to gather information efficiently. Specifically, we present a collection of tasks in which success requires searching through a partially-observed environment, for fragments of information which can be pieced together to accomplish various goals. We combine deep architectures with techniques from reinforcement learning to develop agents that solve our tasks. We shape the behavior of these agents by combining extrinsic and intrinsic rewards. We empirically demonstrate that these agents learn to search actively and intelligently for new information to reduce their uncertainty, and to exploit information they have already acquired.", "pdf": "/pdf/1f5ea4d7a782bc373d3b7149d1e96313e80ce8a4.pdf", "TL;DR": "We investigate the behavior of models trained to answer questions by asking sequences of simple questions.", "paperhash": "bachman|towards_informationseeking_agents", "keywords": [], "conflicts": ["maluuba.com", "mcgill.ca", "umontreal.ca"], "authors": ["Philip Bachman", "Alessandro Sordoni", "Adam Trischler"], "authorids": ["phil.bachman@maluuba.com", "alessandro.sordoni@maluuba.com", "adam.trischler@maluuba.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287523188, "id": "ICLR.cc/2017/conference/-/paper554/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SyW2QSige", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper554/reviewers", "ICLR.cc/2017/conference/paper554/areachairs"], "cdate": 1485287523188}}}, {"tddate": null, "tmdate": 1481828625205, "tcdate": 1481828625198, "number": 1, "id": "BytGYPeNe", "invitation": "ICLR.cc/2017/conference/-/paper554/official/review", "forum": "SyW2QSige", "replyto": "SyW2QSige", "signatures": ["ICLR.cc/2017/conference/paper554/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper554/AnonReviewer3"], "content": {"title": "", "rating": "4: Ok but not good enough - rejection", "review": "This paper proposed to use Generalized Advantage Estimation (GAE) to optimize DNNs for information seeking tasks. The task is posed as a reinforcement learning problem and the proposed method explicitly promotes information gain to encourage exploration.\n\nBoth GAE and DNN have been used for RL before. The novelty in this paper seems to be the explicit modeling of information gain. However, there is insufficient empirical evidence to demonstrate the benefit and generality of the proposed method. An apple to apple comparison to previous RL framework that doesn't model information gain is missing. For example, the cluttered MNIST experiment tried to compare against Mnih et al. (2014) (which is a little out dated) with two settings. But in both setting the input to the two methods are different. Thus it is unclear what contributed to the performance difference.\n\nThe experiment section is cluttered and hard to read. A table that summarizes the numbers would be much better.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards Information-Seeking Agents", "abstract": "We develop a general problem setting for training and testing the ability of agents to gather information efficiently. Specifically, we present a collection of tasks in which success requires searching through a partially-observed environment, for fragments of information which can be pieced together to accomplish various goals. We combine deep architectures with techniques from reinforcement learning to develop agents that solve our tasks. We shape the behavior of these agents by combining extrinsic and intrinsic rewards. We empirically demonstrate that these agents learn to search actively and intelligently for new information to reduce their uncertainty, and to exploit information they have already acquired.", "pdf": "/pdf/1f5ea4d7a782bc373d3b7149d1e96313e80ce8a4.pdf", "TL;DR": "We investigate the behavior of models trained to answer questions by asking sequences of simple questions.", "paperhash": "bachman|towards_informationseeking_agents", "keywords": [], "conflicts": ["maluuba.com", "mcgill.ca", "umontreal.ca"], "authors": ["Philip Bachman", "Alessandro Sordoni", "Adam Trischler"], "authorids": ["phil.bachman@maluuba.com", "alessandro.sordoni@maluuba.com", "adam.trischler@maluuba.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512544229, "id": "ICLR.cc/2017/conference/-/paper554/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper554/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper554/AnonReviewer3", "ICLR.cc/2017/conference/paper554/AnonReviewer1", "ICLR.cc/2017/conference/paper554/AnonReviewer2"], "reply": {"forum": "SyW2QSige", "replyto": "SyW2QSige", "writers": {"values-regex": "ICLR.cc/2017/conference/paper554/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper554/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512544229}}}, {"tddate": null, "tmdate": 1481319778551, "tcdate": 1481319778541, "number": 1, "id": "SkqDBodml", "invitation": "ICLR.cc/2017/conference/-/paper554/public/comment", "forum": "SyW2QSige", "replyto": "BkYqj6PXl", "signatures": ["~Philip_Bachman1"], "readers": ["everyone"], "writers": ["~Philip_Bachman1"], "content": {"title": "Response to pre-review question.", "comment": "The information seeking tasks proposed in our paper are all standard RL tasks. The actions performed by our agents are the questions they ask. Question answering is performed by the environment in the settings we explore, and is thus not included in the set of actions for our agents. However, it would be possible to develop settings in which both asking and answering are performed by one or more agents, rather than by the environment.\n\nThe method we use for training our agents, i.e. Generalized Advantage Estimation, is a standard RL method which extends the advantage-based actor-critic approach used in A3C in a manner analogous to that in which TD(lambda) extends standard TD(k). GAE trains the policy using a weighted sum of n-step truncated actor-critic estimates of the policy gradient for many values of n. See the GAE paper by Schulman et al. for more details. We have not directly compared basic GAE with asynchronous GAE (which would represent the direct swap of GAE for standard actor-critic in A3C). Comparing against methods which use multiple slightly out-of-synch versions of the policy (i.e. A3C) for improved stability/exploration in the policy-gradient setting, and against methods which use value-based estimation (i.e. DQN) would be a useful addition to our current set of experiments. We wanted to a/b test policy gradient versus Q-learning for the original submission but were unable to do so due to time constraints.\n\nRegarding whether our comparison with DRAW is fair for the cluttered MNIST tasks, we directly address this point in the paper. The \"information efficiency\" criterion which we use for measuring agent performance was not explicitly considered by the developers of DRAW. This makes the comparison unfair in the sense that we designed our agents to perform well according to this criterion, while DRAW optimized only for final accuracy on this task. In our second setting our agent does not have access to more information than would be available to DRAW. If DRAW were to place its first glimpse such that the glimpse was at the center of the image and scaled to cover the entire image, it would receive roughly the same information as our agent receives through the summary (which is simply a 13x13 \"DRAW glimpse\" scaled to cover the entire image). Comparisons on new tasks or performance criteria are often challenging, since naive baselines can perform poorly enough to be uninformative and existing powerful methods have not been optimized for the new task/criteria. This is the case with our cluttered MNIST setting, so we chose an existing powerful method which offered a strong baseline according to our criterion."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards Information-Seeking Agents", "abstract": "We develop a general problem setting for training and testing the ability of agents to gather information efficiently. Specifically, we present a collection of tasks in which success requires searching through a partially-observed environment, for fragments of information which can be pieced together to accomplish various goals. We combine deep architectures with techniques from reinforcement learning to develop agents that solve our tasks. We shape the behavior of these agents by combining extrinsic and intrinsic rewards. We empirically demonstrate that these agents learn to search actively and intelligently for new information to reduce their uncertainty, and to exploit information they have already acquired.", "pdf": "/pdf/1f5ea4d7a782bc373d3b7149d1e96313e80ce8a4.pdf", "TL;DR": "We investigate the behavior of models trained to answer questions by asking sequences of simple questions.", "paperhash": "bachman|towards_informationseeking_agents", "keywords": [], "conflicts": ["maluuba.com", "mcgill.ca", "umontreal.ca"], "authors": ["Philip Bachman", "Alessandro Sordoni", "Adam Trischler"], "authorids": ["phil.bachman@maluuba.com", "alessandro.sordoni@maluuba.com", "adam.trischler@maluuba.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287523188, "id": "ICLR.cc/2017/conference/-/paper554/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SyW2QSige", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper554/reviewers", "ICLR.cc/2017/conference/paper554/areachairs"], "cdate": 1485287523188}}}, {"tddate": null, "tmdate": 1481264017087, "tcdate": 1481264017081, "number": 1, "id": "BkYqj6PXl", "invitation": "ICLR.cc/2017/conference/-/paper554/pre-review/question", "forum": "SyW2QSige", "replyto": "SyW2QSige", "signatures": ["ICLR.cc/2017/conference/paper554/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper554/AnonReviewer3"], "content": {"title": "Question about comparisons", "question": "Can the proposed information task be posed as a standard RL task? I think if you map both asking and answering to actions then this is a special case of standard RL setting. Have you compared to standard RL methods including DQN and A3C?\n\nThe experiment section is cluttered and hard to understand. For example, is there a fair comparison between DRAW and proposed method for cluttered MNIST task? Looks like the first setting proposed method use smaller field of view and does worse while in the second setting the proposed method has more information (the summary) and does better. But none of the settings offer fair comparison.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards Information-Seeking Agents", "abstract": "We develop a general problem setting for training and testing the ability of agents to gather information efficiently. Specifically, we present a collection of tasks in which success requires searching through a partially-observed environment, for fragments of information which can be pieced together to accomplish various goals. We combine deep architectures with techniques from reinforcement learning to develop agents that solve our tasks. We shape the behavior of these agents by combining extrinsic and intrinsic rewards. We empirically demonstrate that these agents learn to search actively and intelligently for new information to reduce their uncertainty, and to exploit information they have already acquired.", "pdf": "/pdf/1f5ea4d7a782bc373d3b7149d1e96313e80ce8a4.pdf", "TL;DR": "We investigate the behavior of models trained to answer questions by asking sequences of simple questions.", "paperhash": "bachman|towards_informationseeking_agents", "keywords": [], "conflicts": ["maluuba.com", "mcgill.ca", "umontreal.ca"], "authors": ["Philip Bachman", "Alessandro Sordoni", "Adam Trischler"], "authorids": ["phil.bachman@maluuba.com", "alessandro.sordoni@maluuba.com", "adam.trischler@maluuba.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481264017670, "id": "ICLR.cc/2017/conference/-/paper554/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper554/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper554/AnonReviewer3"], "reply": {"forum": "SyW2QSige", "replyto": "SyW2QSige", "writers": {"values-regex": "ICLR.cc/2017/conference/paper554/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper554/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481264017670}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1481009865864, "tcdate": 1478345640876, "number": 554, "id": "SyW2QSige", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "SyW2QSige", "signatures": ["~Philip_Bachman1"], "readers": ["everyone"], "content": {"title": "Towards Information-Seeking Agents", "abstract": "We develop a general problem setting for training and testing the ability of agents to gather information efficiently. Specifically, we present a collection of tasks in which success requires searching through a partially-observed environment, for fragments of information which can be pieced together to accomplish various goals. We combine deep architectures with techniques from reinforcement learning to develop agents that solve our tasks. We shape the behavior of these agents by combining extrinsic and intrinsic rewards. We empirically demonstrate that these agents learn to search actively and intelligently for new information to reduce their uncertainty, and to exploit information they have already acquired.", "pdf": "/pdf/1f5ea4d7a782bc373d3b7149d1e96313e80ce8a4.pdf", "TL;DR": "We investigate the behavior of models trained to answer questions by asking sequences of simple questions.", "paperhash": "bachman|towards_informationseeking_agents", "keywords": [], "conflicts": ["maluuba.com", "mcgill.ca", "umontreal.ca"], "authors": ["Philip Bachman", "Alessandro Sordoni", "Adam Trischler"], "authorids": ["phil.bachman@maluuba.com", "alessandro.sordoni@maluuba.com", "adam.trischler@maluuba.com"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}], "count": 8}