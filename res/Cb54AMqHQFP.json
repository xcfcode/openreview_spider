{"notes": [{"id": "Cb54AMqHQFP", "original": "6Rjx_AiuQz3", "number": 69, "cdate": 1601308016828, "ddate": null, "tcdate": 1601308016828, "tmdate": 1615853589927, "tddate": null, "forum": "Cb54AMqHQFP", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Network Pruning That Matters:  A Case Study on Retraining Variants", "authorids": ["~Duong_Hoang_Le2", "~Binh-Son_Hua1"], "authors": ["Duong Hoang Le", "Binh-Son Hua"], "keywords": ["Network Pruning"], "abstract": "Network pruning is an effective method to reduce the computational expense of over-parameterized neural networks for deployment on low-resource systems. Recent state-of-the-art techniques for retraining pruned networks such as weight rewinding and learning rate rewinding have been shown to outperform the traditional fine-tuning technique in recovering the lost accuracy (Renda et al., 2020), but so far it is unclear what accounts for such performance. In this work, we conduct extensive experiments to verify and analyze the uncanny effectiveness of learning rate rewinding. We find that the reason behind the success of learning rate rewinding is the usage of a large learning rate. Similar phenomenon can be observed in other learning rate schedules that involve large learning rates, e.g., the 1-cycle learning rate schedule (Smith et al., 2019). By leveraging the right learning rate schedule in retraining, we demonstrate a counter-intuitive phenomenon in that randomly pruned networks could even achieve better performance than methodically pruned networks (fine-tuned with the conventional approach). Our results emphasize the cruciality of the learning rate schedule in pruned network retraining - a detail often overlooked by practitioners during the implementation of network pruning. ", "one-sentence_summary": "We study the effective of different retraining mechanisms while doing pruning", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "le|network_pruning_that_matters_a_case_study_on_retraining_variants", "pdf": "/pdf/cd800d481759bd2472d081c050f8be1d94f91760.pdf", "supplementary_material": "", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nle2021network,\ntitle={Network Pruning That Matters:  A Case Study on Retraining Variants},\nauthor={Duong Hoang Le and Binh-Son Hua},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Cb54AMqHQFP}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "OFjHAoyL17", "original": null, "number": 1, "cdate": 1610040439065, "ddate": null, "tcdate": 1610040439065, "tmdate": 1610474039925, "tddate": null, "forum": "Cb54AMqHQFP", "replyto": "Cb54AMqHQFP", "invitation": "ICLR.cc/2021/Conference/Paper69/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Poster)", "comment": "This paper follows the observations of Renda et al. (2020) that the learning rate in the fine-tuning or retraining phase of neural network pruning is an under-considered component of the pruning process. Renda et al. (2020) argue for a technique that uses the learning rate schedule of the original training regime for fine-tuning. However, their work does not offer a hypothesis or an explanation for why this works.\n\nThis work instead offers more insight into why reusing the original learning rate is productive. Specifically, it shows that using high learning rates is the key component. To demonstrate this, the paper includes a study of using the original step-wise learning from the original training regimen, except accelerated for a given number of fine-tuning epochs. The paper also demonstrates that Cyclic Learning Rate Restarting (CLR) also provides an effective, if not better, learning rate schedule for fine-tuning.\n\nAs noted by the reviewers, the core observations and contributions of this work are modest, but are still a valuable addition to the literature in the pruning community.  \n\nHaving said that, there are some confounding issues with CLR. Specifically, that CLR itself may simply be a more effective learning rate schedule for training neural networks, independent of the particular application to fine-tuning (Reviewer 1). The revision includes an additional appendix that dispels some of this concern. However, indeed, the CLR does improve the base network performance for some configurations.\n\nBroadly, the value proposition here is a thorough demonstration of learning rate schedules for fine-tuning with an overall take that comparisons between techniques need be more sensitive to this choice as previous work perhaps has not thoroughly considered alternative learning rates.\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Network Pruning That Matters:  A Case Study on Retraining Variants", "authorids": ["~Duong_Hoang_Le2", "~Binh-Son_Hua1"], "authors": ["Duong Hoang Le", "Binh-Son Hua"], "keywords": ["Network Pruning"], "abstract": "Network pruning is an effective method to reduce the computational expense of over-parameterized neural networks for deployment on low-resource systems. Recent state-of-the-art techniques for retraining pruned networks such as weight rewinding and learning rate rewinding have been shown to outperform the traditional fine-tuning technique in recovering the lost accuracy (Renda et al., 2020), but so far it is unclear what accounts for such performance. In this work, we conduct extensive experiments to verify and analyze the uncanny effectiveness of learning rate rewinding. We find that the reason behind the success of learning rate rewinding is the usage of a large learning rate. Similar phenomenon can be observed in other learning rate schedules that involve large learning rates, e.g., the 1-cycle learning rate schedule (Smith et al., 2019). By leveraging the right learning rate schedule in retraining, we demonstrate a counter-intuitive phenomenon in that randomly pruned networks could even achieve better performance than methodically pruned networks (fine-tuned with the conventional approach). Our results emphasize the cruciality of the learning rate schedule in pruned network retraining - a detail often overlooked by practitioners during the implementation of network pruning. ", "one-sentence_summary": "We study the effective of different retraining mechanisms while doing pruning", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "le|network_pruning_that_matters_a_case_study_on_retraining_variants", "pdf": "/pdf/cd800d481759bd2472d081c050f8be1d94f91760.pdf", "supplementary_material": "", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nle2021network,\ntitle={Network Pruning That Matters:  A Case Study on Retraining Variants},\nauthor={Duong Hoang Le and Binh-Son Hua},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Cb54AMqHQFP}\n}"}, "tags": [], "invitation": {"reply": {"forum": "Cb54AMqHQFP", "replyto": "Cb54AMqHQFP", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040439050, "tmdate": 1610474039909, "id": "ICLR.cc/2021/Conference/Paper69/-/Decision"}}}, {"id": "5ZDmrArjDtK", "original": null, "number": 2, "cdate": 1603845000130, "ddate": null, "tcdate": 1603845000130, "tmdate": 1606779050202, "tddate": null, "forum": "Cb54AMqHQFP", "replyto": "Cb54AMqHQFP", "invitation": "ICLR.cc/2021/Conference/Paper69/-/Official_Review", "content": {"title": "Paper that explores learning rate schedules when re-training after pruning, and shows that re-training learning rate can matter more than pruning saliency metric.", "review": "# Summary\n\nThis paper analyzes the role of learning rate in re-training after pruning, building on previous findings that changing the learning rate schedule of re-training can result in higher accuracy than low-learning-rate fine-tuning. The paper proposes several learning rate schedules to compare, specifically a cyclic learning rate (gradually ramping up to and back down from the maximum learning rate schedule used during the original training phase) and a compressed version of the original learning rate schedule, and shows that these learning rate schedules outperform standard fine-tuning and also learning rate rewinding, showing that the findings of prior work come from using a higher learning rate in general and not any specific schedule. The paper than shows that choice of re-training learning rate schedule can have more impact on final accuracy than choice of saliency metric.\n\n# Strengths\n\nThe paper, fairly conclusively, finds the following novel results:\n- Re-training with a cyclic learning rate outperforms learning rate rewinding, seemingly leading to a new state-of-the-art re-training algorithm\n- Re-training with a scaled learning rate schedule attains similar accuracy to re-training with learning rate rewinding\n- Re-training a network that has already been pruned and trained with some scheme (e.g., Soft Filter Pruning, Taylor expansions) can reach higher accuracy with cyclic learning rate re-training than standard fine-tuning re-training.\n\nLess conclusively, though still with reasonable evidence, the paper finds:\n- The choice of learning rate schedule when re-training can have a higher impact on accuracy than the choice of saliency metric: specifically, at higher sparsities, re-training a randomly pruned netowrk with a cyclic learning rate schedule results in higher accuracy than re-training a magnitude pruned network with fine-tuning at a low learning rate.\n\n# Weaknesses\n\n- The evaluation of the paper focuses heavily on structured pruning. However, structured pruning can be an unreliable testbed for many of these techniques, as shown by [1]. The paper would benefit from discussion of [1] in relation to the structured pruning results -- for example, can similar accuracy be attained by training a randomly initialized pruned network with the same effective learning rate schedule? Without discussion or evaluation of baselines from [1], it's hard to know quite how to interpret the structured pruning results.\n- The findings about the interplay between pruning saliency metrics and re-training schedule (Section 4), while interesting, are only minimally validated. Specifically, it would be interesting to see full curves of accuracies for each of these techniques like the curves in Figure 4 (at least for weight-level pruning) with a plot showing the accuracy of ResNet-56 with \"Random Pruning + Fine-tuning\", \"Magnitude Pruning + Fine-tuning\", \"Random Pruning + CLR\", \"Magnitude Pruning + CLR\" across different sparsities, which would generate a lot more confidence in this result\n- The paper lacks specific hypotheses which are tested and validated/falsified. Specifically -- it's hard to know what conclusion to pull from Sections 3.1 and 3.2 other than that CLR > SLR >= LRW, and it's hard to know what conclusions to draw from Section 3.3.\n- The more conclusive findings of the paper, that a high learning rate is important for optimization of pruned networks and that cyclic learning rates improve on learning rate rewinding, are a relatively incremental contribution\n\n\n# Overall recommendation\n\n5: Weak reject\n\nI would be willing to raise this score if the authors address some of the weaknesses listed above. Specifically, if the authors can demonstrate that the results on MWP in Table 3 consistently generalize to other sparsities, or generate more confidence in the structured pruning findings (e.g., by showing that they result in higher accuracy than the trained-from-scratch structured pruned networks in [1]).\n\n# Other comments and suggestions\n\n- Minor typo in Figure 2 caption: \"LLRW\"\n- HRank in Section 3.3: if the results are not presented and discussed in the main body of the paper, it'd probably be better to move the entire discussion of HRank to the appendix\n- what does \"Params\" in the tables mean? I assume it means sparsity (i.e., percentage of parameters that are pruned away), but this is never made explicitly clear.\n- why is R-FT missing for MWP in Table 3?\n\n# References used in review:\n\n[1] Zhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, and Trevor Darrell. \"Rethinking the value of network pruning\"\n\n\n# Update post author response:\n\nThanks to the authors for the response. The newly reported results (specifically, those of Appendix E.2) satisfactorily address my concerns about both the generalization of the pruning method v.s. re-training scheme results, both in terms of sparsity levels and unstructured/structured pruning (though the observation of the relationship between R-CLR and FT do not hold quite as strongly for unstructured pruning, they do hold at high enough sparsities to be interesting). I\u2019ve raised my score to a 6 as a result.\n\n", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper69/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper69/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Network Pruning That Matters:  A Case Study on Retraining Variants", "authorids": ["~Duong_Hoang_Le2", "~Binh-Son_Hua1"], "authors": ["Duong Hoang Le", "Binh-Son Hua"], "keywords": ["Network Pruning"], "abstract": "Network pruning is an effective method to reduce the computational expense of over-parameterized neural networks for deployment on low-resource systems. Recent state-of-the-art techniques for retraining pruned networks such as weight rewinding and learning rate rewinding have been shown to outperform the traditional fine-tuning technique in recovering the lost accuracy (Renda et al., 2020), but so far it is unclear what accounts for such performance. In this work, we conduct extensive experiments to verify and analyze the uncanny effectiveness of learning rate rewinding. We find that the reason behind the success of learning rate rewinding is the usage of a large learning rate. Similar phenomenon can be observed in other learning rate schedules that involve large learning rates, e.g., the 1-cycle learning rate schedule (Smith et al., 2019). By leveraging the right learning rate schedule in retraining, we demonstrate a counter-intuitive phenomenon in that randomly pruned networks could even achieve better performance than methodically pruned networks (fine-tuned with the conventional approach). Our results emphasize the cruciality of the learning rate schedule in pruned network retraining - a detail often overlooked by practitioners during the implementation of network pruning. ", "one-sentence_summary": "We study the effective of different retraining mechanisms while doing pruning", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "le|network_pruning_that_matters_a_case_study_on_retraining_variants", "pdf": "/pdf/cd800d481759bd2472d081c050f8be1d94f91760.pdf", "supplementary_material": "", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nle2021network,\ntitle={Network Pruning That Matters:  A Case Study on Retraining Variants},\nauthor={Duong Hoang Le and Binh-Son Hua},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Cb54AMqHQFP}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Cb54AMqHQFP", "replyto": "Cb54AMqHQFP", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper69/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538150914, "tmdate": 1606915783507, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper69/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper69/-/Official_Review"}}}, {"id": "O1bANpi_hVi", "original": null, "number": 4, "cdate": 1603952359326, "ddate": null, "tcdate": 1603952359326, "tmdate": 1606772709688, "tddate": null, "forum": "Cb54AMqHQFP", "replyto": "Cb54AMqHQFP", "invitation": "ICLR.cc/2021/Conference/Paper69/-/Official_Review", "content": {"title": "Review", "review": "## Summary \nThis work focuses on evaluating different fine-tuning strategies after structured and unstructured pruning. The results show that high learning rate schedules (like cosine schedule) attain best performance in many different setting. With this high learning rate random (structured) pruning seems to work as well as the other pruning criteria. Overall the work has a strong coverage of experiments and the results could be helpful to the community. However, I think the work misses some important baselines and require a bit more work on writing. \n\n## Pros\n- A comprehensive coverage of different pruning algorithms is definitely a plus. Experiments are focusing more on the structured pruning methods, which I am not sure useful given the results of [1] (see cons below). \n\n- Authors seem to have a good knowledge of recent structured pruning methods, which makes the study and the experiments convincing/strong.\n\n## Cons\n- I don't think the following statement is true (at least many of the unstructured pruning methods: \"In most cases, pruning consists of three steps: training..prune..retrain\" The paper starts with this premise and ignores many of the other iterative pruning methods (which prune during training). Many of the best unstructured pruning methods are iterative GMP (https://arxiv.org/abs/1710.01878), DNW (https://arxiv.org/pdf/1906.00586.pdf), STR (https://arxiv.org/abs/2002.03231). \n\n- Only exception to the point above is the Table-1 (SFP); however in that table the authors miss an important baseline which is scaling the original training proportional to the Training+FT like it is done in [1, 2, 3]. This baseline should be added to the table and considered wherever possible. \n\n- \"Recently, Renda et al. (2020) proposed a state-of-the-art technique for retraining pruned networks namely learning rate rewinding\" I don't think this sentence is accurate. It's true authors claim SOTA, but I would argue they miss an important baseline in which the original training schedule of the iterative pruning algorithms are scaled according to the training budget as it is done in [1, 2, 3]. In my experience this method performs better than LRW. Therefore it would be nice to include this baseline whenever possible. \n\n- Does CLR results for l1-pruning exceed Scratch-B results of [1]. If yes, this is very important to mention/highlighy. Otherwise, I like to see a discussion about why we should care about structured pruning methods as the main focus of the work seems to be those. \n\n- What is the difference between PFEC and PFEC-B? And why do authors use this acronym? \"l1-structured\" might be a more appropriate choice. And what are the multipliers in Figure-2 (i.e. 1.12x, 1.45x...)? It would be better to use \"sparsity\". \n\n- \"A well-known practice is fine-tuning, which aims to train the pruned model with a small fixed learning rate.\" and \"More advanced learning rate schedules exist, which we generally refer to as retraining.\" I rather call of them fine-tuning or warm-restart, as all networks start with a pretrained network. This terminology would align better with the previous work. Then you call call constant-lr, lrw, clr, etc...\n\n## Minor Points\n- \"Here we hypothesize that the initial pruned network is a suboptimal solution, staying in a local minima.\" I found this statement a bit vague. Networks are usually not not converged after they are pruned (or even at the end of an ImageNet training). We also don't know whether with long enough training the small learning rate finetuning would get same good results or not. It would be nice to make this statement more precise. Maybe something like \"high learning rate would help find better minima faster.\" \n- \"as `1-norm filters pruning\" filter pruning. Structured pruning is used more often similarly \"weights pruning\" -> unstructured pruning\n\"For simplicity, we always use the largest learning rate of the original training for learning rate restarting...\" I didn't understand this statement. Is this for CLR?\n- \"For CLR and SLR, the learning rate is increased from the smallest learning rate of original training to the largest one according to cosine function\" probably the other way around Learning rate is decayed over time?\n\n- \"For ImageNet, we run each experiment once.\" It would be great to run few more to get more precise results before the final version. \n\n- \"under both setting learning rate restarting approaches consistently...\" -> under both settings lr-restart consistently...\n\n- Is Figure-3 a/b MWP? It would be nice to mention this in the title or caption.\n\n- \"Being that said,\" -> That being said\n\n- \"there are notable differents between\" ->  differences\n\n- \"for future works.\" -> for future work\n\n[1] RETHINKING THE VALUE OF NETWORK PRUNING, https://arxiv.org/pdf/1810.05270.pdf\n[2] The State of Sparsity in Deep Neural Networks, https://arxiv.org/abs/1902.09574\n[3] Rigging the Lottery: Making All Tickets Winners, https://arxiv.org/pdf/1911.11134.pdf\n\n## After Rebuttal\n- I thank authors for considering my suggestions. I increase my score to 5. Having a quick look (I am sorry that I didn't have more time) at the new results; most results on structured pruning seem to agree with [1]; with some improvements over baselines when CLR is used when training from scratch. Results on unstructured pruning seems minimal and focuses mostly on one-shot pruning; and furthermore the baseline suggested above (i.e. scaling the entire learning_rate schedule)  is not added to the iterative pruning results. Overall, I like the direction of the paper, but I think the motivation should be improved and results should be distilled.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper69/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper69/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Network Pruning That Matters:  A Case Study on Retraining Variants", "authorids": ["~Duong_Hoang_Le2", "~Binh-Son_Hua1"], "authors": ["Duong Hoang Le", "Binh-Son Hua"], "keywords": ["Network Pruning"], "abstract": "Network pruning is an effective method to reduce the computational expense of over-parameterized neural networks for deployment on low-resource systems. Recent state-of-the-art techniques for retraining pruned networks such as weight rewinding and learning rate rewinding have been shown to outperform the traditional fine-tuning technique in recovering the lost accuracy (Renda et al., 2020), but so far it is unclear what accounts for such performance. In this work, we conduct extensive experiments to verify and analyze the uncanny effectiveness of learning rate rewinding. We find that the reason behind the success of learning rate rewinding is the usage of a large learning rate. Similar phenomenon can be observed in other learning rate schedules that involve large learning rates, e.g., the 1-cycle learning rate schedule (Smith et al., 2019). By leveraging the right learning rate schedule in retraining, we demonstrate a counter-intuitive phenomenon in that randomly pruned networks could even achieve better performance than methodically pruned networks (fine-tuned with the conventional approach). Our results emphasize the cruciality of the learning rate schedule in pruned network retraining - a detail often overlooked by practitioners during the implementation of network pruning. ", "one-sentence_summary": "We study the effective of different retraining mechanisms while doing pruning", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "le|network_pruning_that_matters_a_case_study_on_retraining_variants", "pdf": "/pdf/cd800d481759bd2472d081c050f8be1d94f91760.pdf", "supplementary_material": "", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nle2021network,\ntitle={Network Pruning That Matters:  A Case Study on Retraining Variants},\nauthor={Duong Hoang Le and Binh-Son Hua},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Cb54AMqHQFP}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Cb54AMqHQFP", "replyto": "Cb54AMqHQFP", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper69/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538150914, "tmdate": 1606915783507, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper69/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper69/-/Official_Review"}}}, {"id": "qxek1HUNtoe", "original": null, "number": 1, "cdate": 1603791447434, "ddate": null, "tcdate": 1603791447434, "tmdate": 1606166255344, "tddate": null, "forum": "Cb54AMqHQFP", "replyto": "Cb54AMqHQFP", "invitation": "ICLR.cc/2021/Conference/Paper69/-/Official_Review", "content": {"title": "initial review", "review": "The paper conducts extensive experiments to understand the reason behind the uncanny effectiveness of learning rate rewinding: the usage of a large learning rate.\n\n### pros\n* The paper, in general, is well-written, and the main message is very clear.\n* The paper identifies the reason behind the success of learning rate rewinding through several aspects: retraining cost, model size, and pruning algorithms. It provides guidelines alternative to fine-tuning for practitioners to obtain compact models with better performance after network pruning.\n* The observations of the random pruning are interesting and are aligned with the prior work [1].\n\n### cons\n1. The paper needs to improve its clarity.\n    * It is suggested to include the details for the scores (e.g. 1.12 x in the title) of the subfigures in Fig. 2. Right now it is unclear to me how to calculate the value and why it is meaningful to present the results under these values.\n    * Can you justify why it is necessary to include the learning rate warmup scheme for SLR and CLR, and why the paper only uses 10% of the total retraining budget? How much will the different fractions of the warmup epochs impact the retraining performance? An ablation study is required here.\n    * The observations for random pruning with learning rate restarting are interesting but more details are required. E.g., it is unclear to me the form of performing the random pruning. Is it layerwise random pruning (same sparsity per layer) or global-wise random pruning?\n2. Potential unfair comparison by using CLR. \n    * The paper investigates the impact of different learning rate schedules for the re-training, after pruning on the model pre-trained by the standard stage-wise learning rate schedule. This design choice is sufficient to provide some practical guidelines, but it may also blur the contribution: as CLR is quite different from the other learning rate schemes, it is natural to question if the performance gain is solely due to a better learning rate schedule (but not large learning rate). Can you also provide an ablation study in terms of using CLR for both the training from scratch and re-training, and then compare both the accuracy and the accuracy drop scores?\n    * The paper demonstrates the efficacy of CLR in terms of re-training the pruned model on the standard image classification benchmark. However, it is unclear to me if the same observations can be generalized to other CV tasks or even NLP tasks. It is encouraged to include some preliminary results to argue the generalization ability of the provided practical guidelines.\n\n### reference\n1. Sanity-Checking Pruning Methods: Random Tickets can Win the Jackpot, NeurIPS 2020.\n\n### post-rebuttal\nThe authors have addressed most of my concerns, thus I will increase my score from 5 to 6.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper69/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper69/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Network Pruning That Matters:  A Case Study on Retraining Variants", "authorids": ["~Duong_Hoang_Le2", "~Binh-Son_Hua1"], "authors": ["Duong Hoang Le", "Binh-Son Hua"], "keywords": ["Network Pruning"], "abstract": "Network pruning is an effective method to reduce the computational expense of over-parameterized neural networks for deployment on low-resource systems. Recent state-of-the-art techniques for retraining pruned networks such as weight rewinding and learning rate rewinding have been shown to outperform the traditional fine-tuning technique in recovering the lost accuracy (Renda et al., 2020), but so far it is unclear what accounts for such performance. In this work, we conduct extensive experiments to verify and analyze the uncanny effectiveness of learning rate rewinding. We find that the reason behind the success of learning rate rewinding is the usage of a large learning rate. Similar phenomenon can be observed in other learning rate schedules that involve large learning rates, e.g., the 1-cycle learning rate schedule (Smith et al., 2019). By leveraging the right learning rate schedule in retraining, we demonstrate a counter-intuitive phenomenon in that randomly pruned networks could even achieve better performance than methodically pruned networks (fine-tuned with the conventional approach). Our results emphasize the cruciality of the learning rate schedule in pruned network retraining - a detail often overlooked by practitioners during the implementation of network pruning. ", "one-sentence_summary": "We study the effective of different retraining mechanisms while doing pruning", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "le|network_pruning_that_matters_a_case_study_on_retraining_variants", "pdf": "/pdf/cd800d481759bd2472d081c050f8be1d94f91760.pdf", "supplementary_material": "", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nle2021network,\ntitle={Network Pruning That Matters:  A Case Study on Retraining Variants},\nauthor={Duong Hoang Le and Binh-Son Hua},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Cb54AMqHQFP}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Cb54AMqHQFP", "replyto": "Cb54AMqHQFP", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper69/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538150914, "tmdate": 1606915783507, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper69/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper69/-/Official_Review"}}}, {"id": "XDi86Hz0nj5", "original": null, "number": 6, "cdate": 1605782566519, "ddate": null, "tcdate": 1605782566519, "tmdate": 1606149704263, "tddate": null, "forum": "Cb54AMqHQFP", "replyto": "Cb54AMqHQFP", "invitation": "ICLR.cc/2021/Conference/Paper69/-/Official_Comment", "content": {"title": "Revision update and response to common questions", "comment": "We are grateful to the reviewers for providing constructive feedback. All reviewers agreed that our experiments are comprehensive, and \"the results could be helpful to the community\" (Reviewer 3), having \"practical value in practice of network compression, and the consistent, somewhat surprising observation raises interesting questions.\" (Reviewer 4), being \"fairly conclusively\" with novel results (Reviewer 2), and \"the observations of the random pruning are interesting and are aligned with the prior work\" (Reviewer 1). The reviewers also raised several issues that we have attempted to consider and address. We have already revised the paper accordingly. Please find our responses below. \n\n\n\n### Revision Summary\n- We added a new Section 3.4 to include comparison between CLR and the requested baseline (i.e. scaling scratch training) as suggested by Reviewer 2 and 3. Some results in Section 3.3 is shorten and shifted to Appendix.\n- We added Section 4.1 to validate the finding of \"the importance of retraining\" by providing evidence that PFEC+CLR (*without hyperparameter tuning*) can very well surpass other complex pruning algorithms with the *same* retraining budgets and compression ratio (while the architectures are **not** necessarily the same). Specifically, we compare PFEC+CLR with Taylor Pruning [1], GAL [2], HRankPlus (extension of HRank[3]) in Section 4.1 and Discrimination-aware Channel Pruning [4], Provable Filters Pruning [5] in Appendix D.2.\n- In Section 4.2., we updated the comparison of random pruning with more methods, i.e., we added Taylor Pruning on ImageNet and HRankPlus (an extension of HRank which is more efficient and effective) on CIFAR-10. See Table 5.\n- We improved the clarity of the writing based on the suggestion from the reviewers. The revised text is highlighted in blue.\n\n### Common questions\n- **Extra baselines**: Reviewer 2 and 3 suggested that comparison to the baseline by Liu et al. [6] is necessary. We added two new baselines, Scratch-B and Scratch-E that randomly initialize and train a pruned network with a fair budget compared to other methods, to a new Section 3.4 accordingly. In our experiments, it can be seen that Scratch-B and Scratch-E outperforms fine-tuning, which aligns to the result by Liu et al. [6]. More importantly, CLR has similar performance to such baselines on CIFAR, and outperforms these baselines on ImageNet. Our result demonstrates an example that structured pruning can be effective and worth more future investigations. \n- **Generalization to NLP tasks**: In principle, our findings should generalize to NLP tasks, but running the empirical studies and analysis in this domain deserves a separate future work.\n\n**Reference**\n\n [1] Importance Estimation for Neural Network Pruning, CVPR 2019.\n\n [2] Towards Efficient Model Compression via Learned Global Ranking, CVPR 2020.\n\n [3] HRank: Filter Pruning using High-Rank Feature Map, CVPR 2020.\n\n [4] Discrimination-aware Channel Pruning for Deep Neural Networks, NeurIPS 2018.\n\n [5] Provable Filter Pruning for Efficient Neural Networks, ICLR 2020.\n\n [6] Rethinking the Value of Network Pruning, ICLR 2019.\n\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper69/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper69/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Network Pruning That Matters:  A Case Study on Retraining Variants", "authorids": ["~Duong_Hoang_Le2", "~Binh-Son_Hua1"], "authors": ["Duong Hoang Le", "Binh-Son Hua"], "keywords": ["Network Pruning"], "abstract": "Network pruning is an effective method to reduce the computational expense of over-parameterized neural networks for deployment on low-resource systems. Recent state-of-the-art techniques for retraining pruned networks such as weight rewinding and learning rate rewinding have been shown to outperform the traditional fine-tuning technique in recovering the lost accuracy (Renda et al., 2020), but so far it is unclear what accounts for such performance. In this work, we conduct extensive experiments to verify and analyze the uncanny effectiveness of learning rate rewinding. We find that the reason behind the success of learning rate rewinding is the usage of a large learning rate. Similar phenomenon can be observed in other learning rate schedules that involve large learning rates, e.g., the 1-cycle learning rate schedule (Smith et al., 2019). By leveraging the right learning rate schedule in retraining, we demonstrate a counter-intuitive phenomenon in that randomly pruned networks could even achieve better performance than methodically pruned networks (fine-tuned with the conventional approach). Our results emphasize the cruciality of the learning rate schedule in pruned network retraining - a detail often overlooked by practitioners during the implementation of network pruning. ", "one-sentence_summary": "We study the effective of different retraining mechanisms while doing pruning", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "le|network_pruning_that_matters_a_case_study_on_retraining_variants", "pdf": "/pdf/cd800d481759bd2472d081c050f8be1d94f91760.pdf", "supplementary_material": "", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nle2021network,\ntitle={Network Pruning That Matters:  A Case Study on Retraining Variants},\nauthor={Duong Hoang Le and Binh-Son Hua},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Cb54AMqHQFP}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Cb54AMqHQFP", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper69/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper69/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper69/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper69/Authors|ICLR.cc/2021/Conference/Paper69/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper69/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923874852, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper69/-/Official_Comment"}}}, {"id": "kHNf8SEsXBp", "original": null, "number": 8, "cdate": 1606128400762, "ddate": null, "tcdate": 1606128400762, "tmdate": 1606133107137, "tddate": null, "forum": "Cb54AMqHQFP", "replyto": "Y-d-tiKkKqV", "invitation": "ICLR.cc/2021/Conference/Paper69/-/Official_Comment", "content": {"title": "Added more experiments for ablation study", "comment": "Thanks for your response. We have amended the paper to address your concern as below:\n\n**Which table/figure are you referring to, in terms of \"In the paper, we had an experiment that we compare CLR to SLR.\"?**: Through our extensive experiments with both CLR, SLR, Fine-tuning (as presented in Figure 2,3,4) we found that the large learning rate exceeds the performance of fine-tuning especially with high compression ratio and higher retraining budgets.\n\n**Do you have any results comparing (1) CLR (training from scratch) and (2) CLR (re-training after the pruning on the model from (1))? A pointer would be very helpful.** Thanks for your interesting suggestion, we have added Section G of the Appendix of the revised paper to address your concern. Specifically, we observed that CLR and SLR also outperform fine-tuning regardless of the learning rate schedule of baseline models.\n\nWe also added some experiments in Section C of the Appendix to demonstrate the effectiveness of CLR for Taylor Pruning with other compression ratios.\n\nThank you.\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper69/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper69/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Network Pruning That Matters:  A Case Study on Retraining Variants", "authorids": ["~Duong_Hoang_Le2", "~Binh-Son_Hua1"], "authors": ["Duong Hoang Le", "Binh-Son Hua"], "keywords": ["Network Pruning"], "abstract": "Network pruning is an effective method to reduce the computational expense of over-parameterized neural networks for deployment on low-resource systems. Recent state-of-the-art techniques for retraining pruned networks such as weight rewinding and learning rate rewinding have been shown to outperform the traditional fine-tuning technique in recovering the lost accuracy (Renda et al., 2020), but so far it is unclear what accounts for such performance. In this work, we conduct extensive experiments to verify and analyze the uncanny effectiveness of learning rate rewinding. We find that the reason behind the success of learning rate rewinding is the usage of a large learning rate. Similar phenomenon can be observed in other learning rate schedules that involve large learning rates, e.g., the 1-cycle learning rate schedule (Smith et al., 2019). By leveraging the right learning rate schedule in retraining, we demonstrate a counter-intuitive phenomenon in that randomly pruned networks could even achieve better performance than methodically pruned networks (fine-tuned with the conventional approach). Our results emphasize the cruciality of the learning rate schedule in pruned network retraining - a detail often overlooked by practitioners during the implementation of network pruning. ", "one-sentence_summary": "We study the effective of different retraining mechanisms while doing pruning", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "le|network_pruning_that_matters_a_case_study_on_retraining_variants", "pdf": "/pdf/cd800d481759bd2472d081c050f8be1d94f91760.pdf", "supplementary_material": "", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nle2021network,\ntitle={Network Pruning That Matters:  A Case Study on Retraining Variants},\nauthor={Duong Hoang Le and Binh-Son Hua},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Cb54AMqHQFP}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Cb54AMqHQFP", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper69/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper69/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper69/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper69/Authors|ICLR.cc/2021/Conference/Paper69/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper69/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923874852, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper69/-/Official_Comment"}}}, {"id": "Y-d-tiKkKqV", "original": null, "number": 7, "cdate": 1605868165477, "ddate": null, "tcdate": 1605868165477, "tmdate": 1605868165477, "tddate": null, "forum": "Cb54AMqHQFP", "replyto": "-si8-V4TwH_", "invitation": "ICLR.cc/2021/Conference/Paper69/-/Official_Comment", "content": {"title": "post-rebuttal", "comment": "Thank you for adding more (and extensive) results in the submission.\n\nYour responses have addressed most of my concerns, except the comparison between CLR and SLR:\n1. Which table/figure are you referring to, in terms of \"In the paper, we had an experiment that we compare CLR to SLR.\"?\n2. Do you have any results comparing (1) CLR (training from scratch) and (2) CLR (re-training after the pruning on the model from (1))? A pointer would be very helpful.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper69/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper69/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Network Pruning That Matters:  A Case Study on Retraining Variants", "authorids": ["~Duong_Hoang_Le2", "~Binh-Son_Hua1"], "authors": ["Duong Hoang Le", "Binh-Son Hua"], "keywords": ["Network Pruning"], "abstract": "Network pruning is an effective method to reduce the computational expense of over-parameterized neural networks for deployment on low-resource systems. Recent state-of-the-art techniques for retraining pruned networks such as weight rewinding and learning rate rewinding have been shown to outperform the traditional fine-tuning technique in recovering the lost accuracy (Renda et al., 2020), but so far it is unclear what accounts for such performance. In this work, we conduct extensive experiments to verify and analyze the uncanny effectiveness of learning rate rewinding. We find that the reason behind the success of learning rate rewinding is the usage of a large learning rate. Similar phenomenon can be observed in other learning rate schedules that involve large learning rates, e.g., the 1-cycle learning rate schedule (Smith et al., 2019). By leveraging the right learning rate schedule in retraining, we demonstrate a counter-intuitive phenomenon in that randomly pruned networks could even achieve better performance than methodically pruned networks (fine-tuned with the conventional approach). Our results emphasize the cruciality of the learning rate schedule in pruned network retraining - a detail often overlooked by practitioners during the implementation of network pruning. ", "one-sentence_summary": "We study the effective of different retraining mechanisms while doing pruning", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "le|network_pruning_that_matters_a_case_study_on_retraining_variants", "pdf": "/pdf/cd800d481759bd2472d081c050f8be1d94f91760.pdf", "supplementary_material": "", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nle2021network,\ntitle={Network Pruning That Matters:  A Case Study on Retraining Variants},\nauthor={Duong Hoang Le and Binh-Son Hua},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Cb54AMqHQFP}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Cb54AMqHQFP", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper69/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper69/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper69/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper69/Authors|ICLR.cc/2021/Conference/Paper69/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper69/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923874852, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper69/-/Official_Comment"}}}, {"id": "9y1WVvZt0nO", "original": null, "number": 4, "cdate": 1605782104780, "ddate": null, "tcdate": 1605782104780, "tmdate": 1605807831145, "tddate": null, "forum": "Cb54AMqHQFP", "replyto": "820tCAKilvN", "invitation": "ICLR.cc/2021/Conference/Paper69/-/Official_Comment", "content": {"title": "Thank for your constructing reviews and recommendation. Here are our answers. ", "comment": "**\"Why is large LR helpful in recovering the accuracy of sparse nets?\"**: We hypothesis that the flatness of the loss landscape might change substantially after pruning similar to quantization in [1] which might trap the pruned network in suboptimal solution. Moreover, the pruned network might fail to converge with just small number of retraining epochs, thus, large learning rate helps increase the convergence speed of trimmed networks in these cases. However, a rigorous theoretical explanation is left for future work.\n\n**\"How does weight value rewinding interact with LR?\"**: We further elaborated the interplay of pruning algorithms and learning rate schedule in Section 4.1. We leave the interesting idea of using weight value rewinding as future work. \n\n**\"For Sec. 4, do fine-grain unstructured pruning methods present the same results?\"**: we present the results of (iterative and oneshot) random pruning with MWP in Appendix E.2\n\nReference:\n\n[1] HLHLp: Quantized Neural Networks Training for Reaching Flat Minima in Loss Surface, AAAI 2020."}, "signatures": ["ICLR.cc/2021/Conference/Paper69/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper69/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Network Pruning That Matters:  A Case Study on Retraining Variants", "authorids": ["~Duong_Hoang_Le2", "~Binh-Son_Hua1"], "authors": ["Duong Hoang Le", "Binh-Son Hua"], "keywords": ["Network Pruning"], "abstract": "Network pruning is an effective method to reduce the computational expense of over-parameterized neural networks for deployment on low-resource systems. Recent state-of-the-art techniques for retraining pruned networks such as weight rewinding and learning rate rewinding have been shown to outperform the traditional fine-tuning technique in recovering the lost accuracy (Renda et al., 2020), but so far it is unclear what accounts for such performance. In this work, we conduct extensive experiments to verify and analyze the uncanny effectiveness of learning rate rewinding. We find that the reason behind the success of learning rate rewinding is the usage of a large learning rate. Similar phenomenon can be observed in other learning rate schedules that involve large learning rates, e.g., the 1-cycle learning rate schedule (Smith et al., 2019). By leveraging the right learning rate schedule in retraining, we demonstrate a counter-intuitive phenomenon in that randomly pruned networks could even achieve better performance than methodically pruned networks (fine-tuned with the conventional approach). Our results emphasize the cruciality of the learning rate schedule in pruned network retraining - a detail often overlooked by practitioners during the implementation of network pruning. ", "one-sentence_summary": "We study the effective of different retraining mechanisms while doing pruning", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "le|network_pruning_that_matters_a_case_study_on_retraining_variants", "pdf": "/pdf/cd800d481759bd2472d081c050f8be1d94f91760.pdf", "supplementary_material": "", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nle2021network,\ntitle={Network Pruning That Matters:  A Case Study on Retraining Variants},\nauthor={Duong Hoang Le and Binh-Son Hua},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Cb54AMqHQFP}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Cb54AMqHQFP", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper69/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper69/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper69/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper69/Authors|ICLR.cc/2021/Conference/Paper69/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper69/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923874852, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper69/-/Official_Comment"}}}, {"id": "O-CPqHXzzh9", "original": null, "number": 3, "cdate": 1605781824510, "ddate": null, "tcdate": 1605781824510, "tmdate": 1605798432755, "tddate": null, "forum": "Cb54AMqHQFP", "replyto": "5ZDmrArjDtK", "invitation": "ICLR.cc/2021/Conference/Paper69/-/Official_Comment", "content": {"title": "Thank for your detailed and constructing reviews. Here are our answers. ", "comment": "**\"The findings of the interplay between pruning saliency metrics and re-training schedule (Section 4), while interesting, are only minimally validated\"**: We have updated a few more experiments with other pruning algorithms (Taylor Pruning, the extension of HRank namely HRankPlus) in the revised version (Section 4.2). \n\n**\"The paper lacks specific hypotheses which are tested and validated/falsified\"**: we focused on empirical studies in this work that raise awareness of the implementation details for making fair comparisons in network pruning. Studying a hypothesis is left as future work. \n\n**\"The more conclusive findings of the paper, that a high learning rate is important for optimization of pruned networks and that cyclic learning rates improve on learning rate rewinding, are a relatively incremental contribution\"**: we agree with this perspective. The use of a large learning rate and different learning rate rewinding are often overlooked. And there could exist better learning rate schedules than what we used in this paper. \n\n**\"The results on MWP in Table 3\" and \"structured pruning findings\"**: We updated the results as requested. Please see Appendix E.2 and Section 3.4, respectively. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper69/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper69/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Network Pruning That Matters:  A Case Study on Retraining Variants", "authorids": ["~Duong_Hoang_Le2", "~Binh-Son_Hua1"], "authors": ["Duong Hoang Le", "Binh-Son Hua"], "keywords": ["Network Pruning"], "abstract": "Network pruning is an effective method to reduce the computational expense of over-parameterized neural networks for deployment on low-resource systems. Recent state-of-the-art techniques for retraining pruned networks such as weight rewinding and learning rate rewinding have been shown to outperform the traditional fine-tuning technique in recovering the lost accuracy (Renda et al., 2020), but so far it is unclear what accounts for such performance. In this work, we conduct extensive experiments to verify and analyze the uncanny effectiveness of learning rate rewinding. We find that the reason behind the success of learning rate rewinding is the usage of a large learning rate. Similar phenomenon can be observed in other learning rate schedules that involve large learning rates, e.g., the 1-cycle learning rate schedule (Smith et al., 2019). By leveraging the right learning rate schedule in retraining, we demonstrate a counter-intuitive phenomenon in that randomly pruned networks could even achieve better performance than methodically pruned networks (fine-tuned with the conventional approach). Our results emphasize the cruciality of the learning rate schedule in pruned network retraining - a detail often overlooked by practitioners during the implementation of network pruning. ", "one-sentence_summary": "We study the effective of different retraining mechanisms while doing pruning", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "le|network_pruning_that_matters_a_case_study_on_retraining_variants", "pdf": "/pdf/cd800d481759bd2472d081c050f8be1d94f91760.pdf", "supplementary_material": "", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nle2021network,\ntitle={Network Pruning That Matters:  A Case Study on Retraining Variants},\nauthor={Duong Hoang Le and Binh-Son Hua},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Cb54AMqHQFP}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Cb54AMqHQFP", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper69/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper69/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper69/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper69/Authors|ICLR.cc/2021/Conference/Paper69/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper69/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923874852, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper69/-/Official_Comment"}}}, {"id": "eLXwldsHizU", "original": null, "number": 5, "cdate": 1605782332753, "ddate": null, "tcdate": 1605782332753, "tmdate": 1605783046254, "tddate": null, "forum": "Cb54AMqHQFP", "replyto": "O1bANpi_hVi", "invitation": "ICLR.cc/2021/Conference/Paper69/-/Official_Comment", "content": {"title": "Thank you for your insigntful comments. Here are our answers. ", "comment": "**\"The difference between PFEC and PFEC-B? And why do authors use this acronym? And what are the multipliers in Figure-2 (i.e. 1.12x, 1.45x...)?\"**: 1.12x denotes the compression ratio (#parameter_before_pruning / #parameter_after_pruning) of pruned networks. We use these compression ratios as in the original paper of Li et al., 2016 [1]. To improve the clarity of the paper we have changed this value to sparsity in the revised version according to the request of the reviewer. \n\n**\"For simplicity, we always use the largest learning rate of the original training for learning rate restarting... I didn't understand this statement. Is this for CLR?\"**: Since we restart the learning rate to a (relatively) high value in both CLR and SLR, we employ learning rate warming up for both retraining schemes. Specifically, the learning rate is increased from the smallest value during training to the highest value in the first 10% retraining epochs. We also conducted an ablation study that varies the number of warming up in Appendix F.1. We found that the final performance is not sensitive to this choice.\n\n**\"For ImageNet, we run each experiment once.\"**: We have tried to run ImageNet three times in Section 3.4. Beyond that, we ran experiments on ImageNet with a wide range of pruning algorithms as shown in Section 4. \n\nReference\n\n[1] Pruning Filters for Efficient ConvNets, ICLR 2017.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper69/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper69/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Network Pruning That Matters:  A Case Study on Retraining Variants", "authorids": ["~Duong_Hoang_Le2", "~Binh-Son_Hua1"], "authors": ["Duong Hoang Le", "Binh-Son Hua"], "keywords": ["Network Pruning"], "abstract": "Network pruning is an effective method to reduce the computational expense of over-parameterized neural networks for deployment on low-resource systems. Recent state-of-the-art techniques for retraining pruned networks such as weight rewinding and learning rate rewinding have been shown to outperform the traditional fine-tuning technique in recovering the lost accuracy (Renda et al., 2020), but so far it is unclear what accounts for such performance. In this work, we conduct extensive experiments to verify and analyze the uncanny effectiveness of learning rate rewinding. We find that the reason behind the success of learning rate rewinding is the usage of a large learning rate. Similar phenomenon can be observed in other learning rate schedules that involve large learning rates, e.g., the 1-cycle learning rate schedule (Smith et al., 2019). By leveraging the right learning rate schedule in retraining, we demonstrate a counter-intuitive phenomenon in that randomly pruned networks could even achieve better performance than methodically pruned networks (fine-tuned with the conventional approach). Our results emphasize the cruciality of the learning rate schedule in pruned network retraining - a detail often overlooked by practitioners during the implementation of network pruning. ", "one-sentence_summary": "We study the effective of different retraining mechanisms while doing pruning", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "le|network_pruning_that_matters_a_case_study_on_retraining_variants", "pdf": "/pdf/cd800d481759bd2472d081c050f8be1d94f91760.pdf", "supplementary_material": "", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nle2021network,\ntitle={Network Pruning That Matters:  A Case Study on Retraining Variants},\nauthor={Duong Hoang Le and Binh-Son Hua},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Cb54AMqHQFP}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Cb54AMqHQFP", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper69/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper69/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper69/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper69/Authors|ICLR.cc/2021/Conference/Paper69/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper69/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923874852, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper69/-/Official_Comment"}}}, {"id": "-si8-V4TwH_", "original": null, "number": 2, "cdate": 1605781680977, "ddate": null, "tcdate": 1605781680977, "tmdate": 1605781680977, "tddate": null, "forum": "Cb54AMqHQFP", "replyto": "qxek1HUNtoe", "invitation": "ICLR.cc/2021/Conference/Paper69/-/Official_Comment", "content": {"title": "Thank for your kind response. Here are our answers. ", "comment": "**\u201cThe details for the scores (e.g. 1.12 x in the title) of the subfigures in Fig. 2\u201d**:\nIn fact, this is the compression ratio of the pruned network (i.e. #param_before_pruning / #param_after_pruning). The reason we report the performance of these is that we adopt the pruning configuration of PFEC.\n\n**\u201cWhy it is necessary to include the learning rate warmup scheme for SLR and CLR, and why the paper only uses 10% of the total retraining budget? How much will the different fractions of the warmup epochs impact the retraining performance? An ablation study is required here.\u201d**:\nAs we just retrain a pretrained network, we adopt the common heuristic of using warming up learning rate. We would say that 10% budget for retraining is an arbitrary choice. Thus, we also do an ablation study with different number of warming up epochs in Appendix F.1. In summary, we do not find any noticeable difference between the performance of these settings and all of them consistently achieve higher accuracy than fine-tuning.\n\n**Random pruning type**: We have clarified this detail in the paper. See Paragraph 2 in Section 4.2.\n\u201cPotential unfair comparison by using CLR\u201d and \u201cCLR and the performance gain\u201d: In the paper, we had an experiment that we compare CLR to SLR. In fact, both CLR and SLR consistently outperform fine-tuning in our experiments. So learning rate schedule matters."}, "signatures": ["ICLR.cc/2021/Conference/Paper69/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper69/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Network Pruning That Matters:  A Case Study on Retraining Variants", "authorids": ["~Duong_Hoang_Le2", "~Binh-Son_Hua1"], "authors": ["Duong Hoang Le", "Binh-Son Hua"], "keywords": ["Network Pruning"], "abstract": "Network pruning is an effective method to reduce the computational expense of over-parameterized neural networks for deployment on low-resource systems. Recent state-of-the-art techniques for retraining pruned networks such as weight rewinding and learning rate rewinding have been shown to outperform the traditional fine-tuning technique in recovering the lost accuracy (Renda et al., 2020), but so far it is unclear what accounts for such performance. In this work, we conduct extensive experiments to verify and analyze the uncanny effectiveness of learning rate rewinding. We find that the reason behind the success of learning rate rewinding is the usage of a large learning rate. Similar phenomenon can be observed in other learning rate schedules that involve large learning rates, e.g., the 1-cycle learning rate schedule (Smith et al., 2019). By leveraging the right learning rate schedule in retraining, we demonstrate a counter-intuitive phenomenon in that randomly pruned networks could even achieve better performance than methodically pruned networks (fine-tuned with the conventional approach). Our results emphasize the cruciality of the learning rate schedule in pruned network retraining - a detail often overlooked by practitioners during the implementation of network pruning. ", "one-sentence_summary": "We study the effective of different retraining mechanisms while doing pruning", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "le|network_pruning_that_matters_a_case_study_on_retraining_variants", "pdf": "/pdf/cd800d481759bd2472d081c050f8be1d94f91760.pdf", "supplementary_material": "", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nle2021network,\ntitle={Network Pruning That Matters:  A Case Study on Retraining Variants},\nauthor={Duong Hoang Le and Binh-Son Hua},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Cb54AMqHQFP}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Cb54AMqHQFP", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper69/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper69/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper69/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper69/Authors|ICLR.cc/2021/Conference/Paper69/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper69/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923874852, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper69/-/Official_Comment"}}}, {"id": "820tCAKilvN", "original": null, "number": 3, "cdate": 1603934407335, "ddate": null, "tcdate": 1603934407335, "tmdate": 1605024769472, "tddate": null, "forum": "Cb54AMqHQFP", "replyto": "Cb54AMqHQFP", "invitation": "ICLR.cc/2021/Conference/Paper69/-/Official_Review", "content": {"title": "Re-training matters as much as sparsification", "review": "The authors conducted a comprehensive set of experiments on choices of learning rate schedules for re-training/fine-tuning during iterative or after 1-shot pruning of deep convnets.  Empirically, they reported that high learning rate (LR) is particularly helpful in recovering generalization performance of the resultant sparse model.  The results are purely empirical, well-documented observations from well-designed experiments, which is of practical value in practice of network compression, and the consistent, somewhat surprising observation raises interesting questions.  \n\nNotably, this work has brought to attention an important but often overlooked aspect of network pruning: there exist complex interactions between the dynamics of optimization and sparsification, and as a consequence, it is only fair to compare two sparsification techniques when each of them are put in the _best_ optimization setup, respectively.  \n\nI have a few comments that I wish the authors would address here, discuss in revision or note for future work:\n\n(1) Why is large LR helpful in recovering the accuracy of sparse nets?  There is little information provided in these experimental results to shed light on this question.  There has been loss landscape studies of sparse nets during training (such as arxiv:1906.10732, arxiv:1912.05671)--perhaps these could be applied to study the problem.  If the high LR's role were to knock the solution out of bad local minima, then does adding noise to gradients or smaller batch size achieve similar effect at the initial phase of re-training?  \n\n(2) Given a fixed re-training flop budget, after a pruning operation on the network, both (a) weight value rewinding (as in the Lottery Ticket Hypothesis training), (b) re-training LR schedule (as in this work) might be potentially helpful.  How does weight value rewinding interact with LR?  \n\n(3) For the random pruning results in Sec. 4, do fine-grain unstructured pruning methods present the same results?  \n\n(4) Does the result generalize to transformer models?  What about optimizers?  Does Adam present a same story as SGDM? \n\nPage 5, line1 of the 3rd paragraph of Sec. 3.2: typo \"reachs\"", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper69/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper69/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Network Pruning That Matters:  A Case Study on Retraining Variants", "authorids": ["~Duong_Hoang_Le2", "~Binh-Son_Hua1"], "authors": ["Duong Hoang Le", "Binh-Son Hua"], "keywords": ["Network Pruning"], "abstract": "Network pruning is an effective method to reduce the computational expense of over-parameterized neural networks for deployment on low-resource systems. Recent state-of-the-art techniques for retraining pruned networks such as weight rewinding and learning rate rewinding have been shown to outperform the traditional fine-tuning technique in recovering the lost accuracy (Renda et al., 2020), but so far it is unclear what accounts for such performance. In this work, we conduct extensive experiments to verify and analyze the uncanny effectiveness of learning rate rewinding. We find that the reason behind the success of learning rate rewinding is the usage of a large learning rate. Similar phenomenon can be observed in other learning rate schedules that involve large learning rates, e.g., the 1-cycle learning rate schedule (Smith et al., 2019). By leveraging the right learning rate schedule in retraining, we demonstrate a counter-intuitive phenomenon in that randomly pruned networks could even achieve better performance than methodically pruned networks (fine-tuned with the conventional approach). Our results emphasize the cruciality of the learning rate schedule in pruned network retraining - a detail often overlooked by practitioners during the implementation of network pruning. ", "one-sentence_summary": "We study the effective of different retraining mechanisms while doing pruning", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "le|network_pruning_that_matters_a_case_study_on_retraining_variants", "pdf": "/pdf/cd800d481759bd2472d081c050f8be1d94f91760.pdf", "supplementary_material": "", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nle2021network,\ntitle={Network Pruning That Matters:  A Case Study on Retraining Variants},\nauthor={Duong Hoang Le and Binh-Son Hua},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Cb54AMqHQFP}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Cb54AMqHQFP", "replyto": "Cb54AMqHQFP", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper69/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538150914, "tmdate": 1606915783507, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper69/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper69/-/Official_Review"}}}], "count": 13}