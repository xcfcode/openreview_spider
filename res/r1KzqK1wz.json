{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1528124446145, "tcdate": 1518471697084, "number": 303, "cdate": 1518471697084, "id": "r1KzqK1wz", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "r1KzqK1wz", "signatures": ["~Sil_C._van_de_Leemput1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "MemCNN: a Framework for Developing Memory Efficient Deep Invertible Networks", "abstract": "Reversible operations have recently been successfully applied to classification problems to reduce memory requirements during neural network training. This feature is accomplished by removing the need to store the input activation for computing the gradients at the backward pass and instead reconstruct them on demand. However, current approaches rely on custom implementations of backpropagation, which limits applicability and extendibility. We present MemCNN, a novel PyTorch framework which simplifies the application of reversible functions by removing the need for a customized backpropagation. The framework contains a set of practical generalized tools, which can wrap common operations like convolutions and batch normalization and which take care of the memory management. We validate the presented framework by reproducing state-of-the-art experiments comparing classification accuracy and training time on Cifar-10 and Cifar-100 with the existing state-of-the-art, achieving similar classification accuracy and faster training times.", "paperhash": "leemput|memcnn_a_framework_for_developing_memory_efficient_deep_invertible_networks", "keywords": ["MemCNN", "Memory efficient", "deep learning", "neural networks", "invertible networks", "reversible networks", "PyTorch", "framework"], "_bibtex": "@misc{\n  leemput2018memcnn:,\n  title={MemCNN: a Framework for Developing Memory Efficient Deep Invertible Networks},\n  author={Sil C. van de Leemput and Jonas Teuwen and Rashindra Manniesing},\n  year={2018},\n  url={https://openreview.net/forum?id=r1KzqK1wz}\n}", "authorids": ["silvandeleemput@gmail.com", "jonasteuwen@gmail.com", "rashindra@gmail.com"], "authors": ["Sil C. van de Leemput", "Jonas Teuwen", "Rashindra Manniesing"], "TL;DR": "A novel Framework for Developing Memory Efficient Deep Invertible Networks based on Reversible Operations.", "pdf": "/pdf/c6967a799c56756c1c1ac1f50bb12b39efc1a420.pdf"}, "nonreaders": [], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582976668, "tcdate": 1519930881785, "number": 1, "cdate": 1519930881785, "id": "HJcbCaHOf", "invitation": "ICLR.cc/2018/Workshop/-/Paper303/Official_Review", "forum": "r1KzqK1wz", "replyto": "r1KzqK1wz", "signatures": ["ICLR.cc/2018/Workshop/Paper303/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper303/AnonReviewer1"], "content": {"title": "Faster Implementation of RevNets", "rating": "6: Marginally above acceptance threshold", "review": "This paper describes a software library based on PyTorch that provides easier mechanism to implement reversible neural network architectures. the main contribution seem to be a general API that does some clever memory handling. By utilizing the PyTorch's builtin autograd mechanism, it is shown to be faster than Tensorflow based implementation that uses custom gradient function.\n\nIt is of potential interest to the community to use this tool to build their own reversible architectures and do related research. However, this paper could potentially be improved\n\n- As a paper describing a library, it might worse spending more time describing the main contributions and novelty in the design and implementation of this software (than describing the literature of RevNet researches).\n\n- It might be good to have brief examples (maybe code) showing how end users use this.\n\n- MemCNN is a confusing name. It sounds like something related to differentiable memory architectures.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "MemCNN: a Framework for Developing Memory Efficient Deep Invertible Networks", "abstract": "Reversible operations have recently been successfully applied to classification problems to reduce memory requirements during neural network training. This feature is accomplished by removing the need to store the input activation for computing the gradients at the backward pass and instead reconstruct them on demand. However, current approaches rely on custom implementations of backpropagation, which limits applicability and extendibility. We present MemCNN, a novel PyTorch framework which simplifies the application of reversible functions by removing the need for a customized backpropagation. The framework contains a set of practical generalized tools, which can wrap common operations like convolutions and batch normalization and which take care of the memory management. We validate the presented framework by reproducing state-of-the-art experiments comparing classification accuracy and training time on Cifar-10 and Cifar-100 with the existing state-of-the-art, achieving similar classification accuracy and faster training times.", "paperhash": "leemput|memcnn_a_framework_for_developing_memory_efficient_deep_invertible_networks", "keywords": ["MemCNN", "Memory efficient", "deep learning", "neural networks", "invertible networks", "reversible networks", "PyTorch", "framework"], "_bibtex": "@misc{\n  leemput2018memcnn:,\n  title={MemCNN: a Framework for Developing Memory Efficient Deep Invertible Networks},\n  author={Sil C. van de Leemput and Jonas Teuwen and Rashindra Manniesing},\n  year={2018},\n  url={https://openreview.net/forum?id=r1KzqK1wz}\n}", "authorids": ["silvandeleemput@gmail.com", "jonasteuwen@gmail.com", "rashindra@gmail.com"], "authors": ["Sil C. van de Leemput", "Jonas Teuwen", "Rashindra Manniesing"], "TL;DR": "A novel Framework for Developing Memory Efficient Deep Invertible Networks based on Reversible Operations.", "pdf": "/pdf/c6967a799c56756c1c1ac1f50bb12b39efc1a420.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582976475, "id": "ICLR.cc/2018/Workshop/-/Paper303/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper303/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper303/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper303/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper303/AnonReviewer2"], "reply": {"forum": "r1KzqK1wz", "replyto": "r1KzqK1wz", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper303/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper303/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582976475}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582745909, "tcdate": 1520658337188, "number": 2, "cdate": 1520658337188, "id": "B1YovkZFf", "invitation": "ICLR.cc/2018/Workshop/-/Paper303/Official_Review", "forum": "r1KzqK1wz", "replyto": "r1KzqK1wz", "signatures": ["ICLR.cc/2018/Workshop/Paper303/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper303/AnonReviewer3"], "content": {"title": "Less information about the framework", "rating": "4: Ok but not good enough - rejection", "review": "The paper describes a new framework on PyTorch to implement arbitrary reversible connections. It may become a useful functionality to implement those architectures. However, the paper describes only a bit information not enough to argue the advantage of the proposed framework.\nTable 1 has less information about differences in time consumption between TensorFlow and PyTorch implementations, because it also looks like the effect due to only the difference of the base framework. At least authors should add experiments about all models with \"naive implementations\" on PyTorch.\nThere is no information about the usage of the framework on both the paper and the repository, which is essentially important for this kind of papers. E.g., small (a few lines) sample codes, and descriptions of how the code works can greatly help potential users of the framework.\n\nOther comments:\n\n* The framework name \"MemCNN\" does not express the actual functionality: implementing reversible connections, and reconsidering the name is recommended for the community.\n* The order of equations about $x_1$ and $x_2$ in Eq. (2) should be reversed to maintain the actual order of the calculation (like what written in [Gomez et al., '17]).\n* Table 1 looks a bit confusing because results about datasets and acc/time appears alternately. Separating the table according to at least an axis (dataset or acc/time) can help readers to comprehend the results.\n\n", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "MemCNN: a Framework for Developing Memory Efficient Deep Invertible Networks", "abstract": "Reversible operations have recently been successfully applied to classification problems to reduce memory requirements during neural network training. This feature is accomplished by removing the need to store the input activation for computing the gradients at the backward pass and instead reconstruct them on demand. However, current approaches rely on custom implementations of backpropagation, which limits applicability and extendibility. We present MemCNN, a novel PyTorch framework which simplifies the application of reversible functions by removing the need for a customized backpropagation. The framework contains a set of practical generalized tools, which can wrap common operations like convolutions and batch normalization and which take care of the memory management. We validate the presented framework by reproducing state-of-the-art experiments comparing classification accuracy and training time on Cifar-10 and Cifar-100 with the existing state-of-the-art, achieving similar classification accuracy and faster training times.", "paperhash": "leemput|memcnn_a_framework_for_developing_memory_efficient_deep_invertible_networks", "keywords": ["MemCNN", "Memory efficient", "deep learning", "neural networks", "invertible networks", "reversible networks", "PyTorch", "framework"], "_bibtex": "@misc{\n  leemput2018memcnn:,\n  title={MemCNN: a Framework for Developing Memory Efficient Deep Invertible Networks},\n  author={Sil C. van de Leemput and Jonas Teuwen and Rashindra Manniesing},\n  year={2018},\n  url={https://openreview.net/forum?id=r1KzqK1wz}\n}", "authorids": ["silvandeleemput@gmail.com", "jonasteuwen@gmail.com", "rashindra@gmail.com"], "authors": ["Sil C. van de Leemput", "Jonas Teuwen", "Rashindra Manniesing"], "TL;DR": "A novel Framework for Developing Memory Efficient Deep Invertible Networks based on Reversible Operations.", "pdf": "/pdf/c6967a799c56756c1c1ac1f50bb12b39efc1a420.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582976475, "id": "ICLR.cc/2018/Workshop/-/Paper303/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper303/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper303/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper303/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper303/AnonReviewer2"], "reply": {"forum": "r1KzqK1wz", "replyto": "r1KzqK1wz", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper303/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper303/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582976475}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582614083, "tcdate": 1520876819411, "number": 3, "cdate": 1520876819411, "id": "HJoGpNVFf", "invitation": "ICLR.cc/2018/Workshop/-/Paper303/Official_Review", "forum": "r1KzqK1wz", "replyto": "r1KzqK1wz", "signatures": ["ICLR.cc/2018/Workshop/Paper303/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper303/AnonReviewer2"], "content": {"title": "Nice python framework implementation of reversible functions in CNN. Useful for DL practicioners.", "rating": "7: Good paper, accept", "review": "Before providing my review I need to add that this is not my area of expertise. As such I will mainly review this work regarding its quality, clarity and significance (the latter in a way explained in what follows).  \n\nReversible operations as part of a NN are shown to reduce memory requirements during training. As such, their application as part of the NN training design has attracted lots of attention. Current approaches implement the reversible functions using backpropagation which prohibits the application of novel invertible networks. The authors present the MemCNN, a PyTorch implementation which implements reversibility without the need for customized backpropagation. Their implementation is based on the equations of Gomez et all and Dinh et al. They successfully managed to incorporate the ideas into an efficient PyTorch framework. They support their implementation by providing results on two distinct classification tasks where the framework is compared to the state-of-the-art Tensor-Flow implementation. Both implementations seem to be comparable in accuracy. The MemCNN outperforms in training time though. \n\nI am pleased with the quality and the clarity of the work presented here. The authors did a good job presenting their contribution in a clear way by stating the motivation and plugging their implementation in the picture. I think their contribution is significant for the DL practitioners; a Python framework that allows for less training time is attractive. \n\n\n\n\n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "MemCNN: a Framework for Developing Memory Efficient Deep Invertible Networks", "abstract": "Reversible operations have recently been successfully applied to classification problems to reduce memory requirements during neural network training. This feature is accomplished by removing the need to store the input activation for computing the gradients at the backward pass and instead reconstruct them on demand. However, current approaches rely on custom implementations of backpropagation, which limits applicability and extendibility. We present MemCNN, a novel PyTorch framework which simplifies the application of reversible functions by removing the need for a customized backpropagation. The framework contains a set of practical generalized tools, which can wrap common operations like convolutions and batch normalization and which take care of the memory management. We validate the presented framework by reproducing state-of-the-art experiments comparing classification accuracy and training time on Cifar-10 and Cifar-100 with the existing state-of-the-art, achieving similar classification accuracy and faster training times.", "paperhash": "leemput|memcnn_a_framework_for_developing_memory_efficient_deep_invertible_networks", "keywords": ["MemCNN", "Memory efficient", "deep learning", "neural networks", "invertible networks", "reversible networks", "PyTorch", "framework"], "_bibtex": "@misc{\n  leemput2018memcnn:,\n  title={MemCNN: a Framework for Developing Memory Efficient Deep Invertible Networks},\n  author={Sil C. van de Leemput and Jonas Teuwen and Rashindra Manniesing},\n  year={2018},\n  url={https://openreview.net/forum?id=r1KzqK1wz}\n}", "authorids": ["silvandeleemput@gmail.com", "jonasteuwen@gmail.com", "rashindra@gmail.com"], "authors": ["Sil C. van de Leemput", "Jonas Teuwen", "Rashindra Manniesing"], "TL;DR": "A novel Framework for Developing Memory Efficient Deep Invertible Networks based on Reversible Operations.", "pdf": "/pdf/c6967a799c56756c1c1ac1f50bb12b39efc1a420.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582976475, "id": "ICLR.cc/2018/Workshop/-/Paper303/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper303/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper303/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper303/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper303/AnonReviewer2"], "reply": {"forum": "r1KzqK1wz", "replyto": "r1KzqK1wz", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper303/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper303/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582976475}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521573576324, "tcdate": 1521573576324, "number": 141, "cdate": 1521573575990, "id": "H1g0ACAKz", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "r1KzqK1wz", "replyto": "r1KzqK1wz", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Accept", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "Congratulations, your paper was accepted to the ICLR workshop."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "MemCNN: a Framework for Developing Memory Efficient Deep Invertible Networks", "abstract": "Reversible operations have recently been successfully applied to classification problems to reduce memory requirements during neural network training. This feature is accomplished by removing the need to store the input activation for computing the gradients at the backward pass and instead reconstruct them on demand. However, current approaches rely on custom implementations of backpropagation, which limits applicability and extendibility. We present MemCNN, a novel PyTorch framework which simplifies the application of reversible functions by removing the need for a customized backpropagation. The framework contains a set of practical generalized tools, which can wrap common operations like convolutions and batch normalization and which take care of the memory management. We validate the presented framework by reproducing state-of-the-art experiments comparing classification accuracy and training time on Cifar-10 and Cifar-100 with the existing state-of-the-art, achieving similar classification accuracy and faster training times.", "paperhash": "leemput|memcnn_a_framework_for_developing_memory_efficient_deep_invertible_networks", "keywords": ["MemCNN", "Memory efficient", "deep learning", "neural networks", "invertible networks", "reversible networks", "PyTorch", "framework"], "_bibtex": "@misc{\n  leemput2018memcnn:,\n  title={MemCNN: a Framework for Developing Memory Efficient Deep Invertible Networks},\n  author={Sil C. van de Leemput and Jonas Teuwen and Rashindra Manniesing},\n  year={2018},\n  url={https://openreview.net/forum?id=r1KzqK1wz}\n}", "authorids": ["silvandeleemput@gmail.com", "jonasteuwen@gmail.com", "rashindra@gmail.com"], "authors": ["Sil C. van de Leemput", "Jonas Teuwen", "Rashindra Manniesing"], "TL;DR": "A novel Framework for Developing Memory Efficient Deep Invertible Networks based on Reversible Operations.", "pdf": "/pdf/c6967a799c56756c1c1ac1f50bb12b39efc1a420.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}], "count": 5}