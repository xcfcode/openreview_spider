{"notes": [{"id": "u15gHPQViL", "original": "R3Ud0Hsgbj6", "number": 2412, "cdate": 1601308266223, "ddate": null, "tcdate": 1601308266223, "tmdate": 1614985756385, "tddate": null, "forum": "u15gHPQViL", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Zero-Shot Recognition through Image-Guided Semantic Classification", "authorids": ["~Mei-Chen_Yeh1", "~Fang_Li5", "a0917251699@gmail.com"], "authors": ["Mei-Chen Yeh", "Fang Li", "Bo-Heng Li"], "keywords": ["zero-shot learning", "visual-semantic embedding", "deep learning"], "abstract": "We present a new visual-semantic embedding method for generalized zero-shot learning. Existing embedding-based methods aim to learn the correspondence between an image classifier (visual representation) and its class prototype (semantic representation) for each class. Inspired by the binary relevance method for multi-label classification, we learn the mapping between an image and its semantic classifier. Given an input image, the proposed Image-Guided Semantic Classification (IGSC) method creates a label classifier, being applied to all label embeddings to determine whether a label belongs to the input image. Therefore, a semantic classifier is image conditioned and is generated during inference. We also show that IGSC is a unifying framework for two state-of-the-art deep-embedding methods. We validate our approach with four standard benchmark datasets.\n\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yeh|zeroshot_recognition_through_imageguided_semantic_classification", "pdf": "/pdf/1cbbbe8244990488eb4cd71454e5cbfc32b4646c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=_xQVLLzjyO", "_bibtex": "@misc{\nyeh2021zeroshot,\ntitle={Zero-Shot Recognition through Image-Guided Semantic Classification},\nauthor={Mei-Chen Yeh and Fang Li and Bo-Heng Li},\nyear={2021},\nurl={https://openreview.net/forum?id=u15gHPQViL}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "Tq-yoCdnRru", "original": null, "number": 1, "cdate": 1610040378329, "ddate": null, "tcdate": 1610040378329, "tmdate": 1610473970922, "tddate": null, "forum": "u15gHPQViL", "replyto": "u15gHPQViL", "invitation": "ICLR.cc/2021/Conference/Paper2412/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "This paper presents work on zero-shot learning.  The reviewers appreciated the simplicity of the method and its clear exposition.  However, concerns were raised over novelty, motivation, and empirical validation.  After reading the authors' response, the reviewers remained of the opinion that these concerns have not yet been addressed sufficiently.  Based on these points, the paper is not yet ready for publication."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Zero-Shot Recognition through Image-Guided Semantic Classification", "authorids": ["~Mei-Chen_Yeh1", "~Fang_Li5", "a0917251699@gmail.com"], "authors": ["Mei-Chen Yeh", "Fang Li", "Bo-Heng Li"], "keywords": ["zero-shot learning", "visual-semantic embedding", "deep learning"], "abstract": "We present a new visual-semantic embedding method for generalized zero-shot learning. Existing embedding-based methods aim to learn the correspondence between an image classifier (visual representation) and its class prototype (semantic representation) for each class. Inspired by the binary relevance method for multi-label classification, we learn the mapping between an image and its semantic classifier. Given an input image, the proposed Image-Guided Semantic Classification (IGSC) method creates a label classifier, being applied to all label embeddings to determine whether a label belongs to the input image. Therefore, a semantic classifier is image conditioned and is generated during inference. We also show that IGSC is a unifying framework for two state-of-the-art deep-embedding methods. We validate our approach with four standard benchmark datasets.\n\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yeh|zeroshot_recognition_through_imageguided_semantic_classification", "pdf": "/pdf/1cbbbe8244990488eb4cd71454e5cbfc32b4646c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=_xQVLLzjyO", "_bibtex": "@misc{\nyeh2021zeroshot,\ntitle={Zero-Shot Recognition through Image-Guided Semantic Classification},\nauthor={Mei-Chen Yeh and Fang Li and Bo-Heng Li},\nyear={2021},\nurl={https://openreview.net/forum?id=u15gHPQViL}\n}"}, "tags": [], "invitation": {"reply": {"forum": "u15gHPQViL", "replyto": "u15gHPQViL", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040378314, "tmdate": 1610473970906, "id": "ICLR.cc/2021/Conference/Paper2412/-/Decision"}}}, {"id": "RUpNdrCWu4i", "original": null, "number": 3, "cdate": 1603951440446, "ddate": null, "tcdate": 1603951440446, "tmdate": 1606762479217, "tddate": null, "forum": "u15gHPQViL", "replyto": "u15gHPQViL", "invitation": "ICLR.cc/2021/Conference/Paper2412/-/Official_Review", "content": {"title": "Good paper, easy to read with promising results", "review": "This paper proposes a visual-semantic embedding model useful for generalized zero-shot learning. The proposed model transforms an image into a label classifier, which is then used to predict the correct label in the semantic space. \n\nThe paper is well constructed and easy to read. It provides a good presentation of some related work and identifies the contributions as compared to existing approaches.\n\nThe experimental validation is performed on four popular public datasets and compares the performance to several state of the art approaches. The obtained performance shows similar/promising results as compared to the state of the art.\n\nFrom my perspective, the paper is missing some experimental analysis/comparison to some recent methods that are inductive only to samples and also some methods that are transductive for unseen class prototypes and unlabeled unseen test instances \n(for instance, papers mentioned in Section 4.3). First, that comparison will allow evaluating the performance of the proposed approach to more recent papers than the ones used in Section 4.2. Second, it seems that these methods, specifically the ones that are transductive for unseen class prototypes, achieve a much higher performance and it's important to evaluate the performance of the proposed method in that setting or to report on the performance loss when someone decides to use this approach in that specific setting (inductive to both unseen images and unseen semantic vectors).  A discussion that addresses the above questions/concerns could do it too.\n\nPost-Rebuttal Evaluation [FINAL] I would like to thank the authors for their response and for updating their paper based on the reviewers feedback. Following these updates, I'm changing my recommendation to Rejection for the following 2 reasons: 1- the technical novelty of this paper is moderate and there's a significant gap in the model performance as compared to state of the art, 2- the authors failed to provide convincing answers to many of the reviewers concerns, including motivation for not using semantic embeddings during the training process, and not comparing their approach to  transductive ZSL ones which achieve a higher performance.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2412/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2412/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Zero-Shot Recognition through Image-Guided Semantic Classification", "authorids": ["~Mei-Chen_Yeh1", "~Fang_Li5", "a0917251699@gmail.com"], "authors": ["Mei-Chen Yeh", "Fang Li", "Bo-Heng Li"], "keywords": ["zero-shot learning", "visual-semantic embedding", "deep learning"], "abstract": "We present a new visual-semantic embedding method for generalized zero-shot learning. Existing embedding-based methods aim to learn the correspondence between an image classifier (visual representation) and its class prototype (semantic representation) for each class. Inspired by the binary relevance method for multi-label classification, we learn the mapping between an image and its semantic classifier. Given an input image, the proposed Image-Guided Semantic Classification (IGSC) method creates a label classifier, being applied to all label embeddings to determine whether a label belongs to the input image. Therefore, a semantic classifier is image conditioned and is generated during inference. We also show that IGSC is a unifying framework for two state-of-the-art deep-embedding methods. We validate our approach with four standard benchmark datasets.\n\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yeh|zeroshot_recognition_through_imageguided_semantic_classification", "pdf": "/pdf/1cbbbe8244990488eb4cd71454e5cbfc32b4646c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=_xQVLLzjyO", "_bibtex": "@misc{\nyeh2021zeroshot,\ntitle={Zero-Shot Recognition through Image-Guided Semantic Classification},\nauthor={Mei-Chen Yeh and Fang Li and Bo-Heng Li},\nyear={2021},\nurl={https://openreview.net/forum?id=u15gHPQViL}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "u15gHPQViL", "replyto": "u15gHPQViL", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2412/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538096927, "tmdate": 1606915766373, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2412/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2412/-/Official_Review"}}}, {"id": "keueJPowQXP", "original": null, "number": 2, "cdate": 1603785239275, "ddate": null, "tcdate": 1603785239275, "tmdate": 1606751118773, "tddate": null, "forum": "u15gHPQViL", "replyto": "u15gHPQViL", "invitation": "ICLR.cc/2021/Conference/Paper2412/-/Official_Review", "content": {"title": "A reasonable ZSL formulation with weak experimental results", "review": "This paper describes zero-shot learning for image classification.  The proposed method is termed as Image-Guided Semantic Classification (IGSC) that learns image-specific label classifiers to achieve zero-shot classification. Experimental results on four standard datasets are reported.\n\nThe authors are suggested to address the following comments.\n1. The idea to learn image-specific classifiers (specified by the yielded parameters) resembles the conventional local learning, where a classifier is learned for each training sample. In the last paragraph of page 4, the authors mention that the proposed IGSC mechanism is similar to that of  Dynamic Filter Networks by Jia et al., but  also point out a fundamental difference that their method instead aims to learn model representations. However, similar techniques in this aspect have already been explored, e.g., in solving the VQA problem. Take, for example, the CVPR 2016 paper, entitled \u201cImage Question Answering using Convolutional Neural Network with Dynamic Parameter Prediction.\u201d The novelty of the proposed image-guided formulation seems to be limited. \n2. The math notations in the sentence right after equation (1) are incorrect. Y \\in Y_u should be corrected into Y = Y_u. Also, Y in Y_s \\cup Y_u should be Y = Y_s \\cup Y_u. \n3. Besides the technical novelty, my main concern about this work is that the experimental results are not convincing. For example, the previous technique AREN (Xie et al., 2019) is included in Table 4, but not in Table 3. However, the classification results achieved by AREN are considerably better than those yielded by the proposed technique.  For the ZSL results in Table 3, AREN achieves 60.6/71.8/67.9/39.2, while those by the proposed method are 58.3/56.9/62.1/35.2. For the GZSL results in Table 4, the acc_u result of SUN by AREN is 19%, not 9%. Furthermore, as Calibrated Stacking (CS) is also used in AREN, it is not reasonable that the comparison excludes the GZSL results of AREN+CS, which are significantly better than those by the proposed IGSC+CS. In particular, the respective harmonic means derived by AREN+CS are 35.9/66.0/64.7/36.9, while those by the proposed technique (IGSC+AC) are 34.9/48.7/39.3/33.2.\n4. The comparisons skip more recent ZSL techniques in that the proposed IGSC formulation does not consider semantic vectors of unseen classes. However, the semantic vectors of both seen and unseen classes are often made available in the training stage of ZSL and most recent ZSL techniques do consider exploring such information. It would be useful if the proposed IGSC can be generalized to take account of all the available semantic information so that comprehensive comparisons to SOTA ZSL techniques can be carried out.\n\nAfter author response: \nMy main concerns about this paper are technical novelty and weak experimental results. The authors did not make efforts to address the two aspects properly. For example, I have listed several issues in my comment 3 about the specific experimental results, but the authors did not try to address them at all. Their responses to Reviewer 4 about the transductive ZSL are not relevant to the weak experimental results of concern.  As most of my concerns are not resolved in the response, I see no evidence to upgrade my rating.\n\nI appreciate that the authors have revised their responses. However, the added response still does not address the two specific questions (the reasons why AREN is not included in Table 3, and AREN+CS not in Table 4) in my comment 3. My final evaluation about the paper is on the negative side in that its technical novelty is moderate and the experimental results are not convincing. ", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2412/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2412/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Zero-Shot Recognition through Image-Guided Semantic Classification", "authorids": ["~Mei-Chen_Yeh1", "~Fang_Li5", "a0917251699@gmail.com"], "authors": ["Mei-Chen Yeh", "Fang Li", "Bo-Heng Li"], "keywords": ["zero-shot learning", "visual-semantic embedding", "deep learning"], "abstract": "We present a new visual-semantic embedding method for generalized zero-shot learning. Existing embedding-based methods aim to learn the correspondence between an image classifier (visual representation) and its class prototype (semantic representation) for each class. Inspired by the binary relevance method for multi-label classification, we learn the mapping between an image and its semantic classifier. Given an input image, the proposed Image-Guided Semantic Classification (IGSC) method creates a label classifier, being applied to all label embeddings to determine whether a label belongs to the input image. Therefore, a semantic classifier is image conditioned and is generated during inference. We also show that IGSC is a unifying framework for two state-of-the-art deep-embedding methods. We validate our approach with four standard benchmark datasets.\n\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yeh|zeroshot_recognition_through_imageguided_semantic_classification", "pdf": "/pdf/1cbbbe8244990488eb4cd71454e5cbfc32b4646c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=_xQVLLzjyO", "_bibtex": "@misc{\nyeh2021zeroshot,\ntitle={Zero-Shot Recognition through Image-Guided Semantic Classification},\nauthor={Mei-Chen Yeh and Fang Li and Bo-Heng Li},\nyear={2021},\nurl={https://openreview.net/forum?id=u15gHPQViL}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "u15gHPQViL", "replyto": "u15gHPQViL", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2412/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538096927, "tmdate": 1606915766373, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2412/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2412/-/Official_Review"}}}, {"id": "thCRuYL2mn", "original": null, "number": 4, "cdate": 1603973581855, "ddate": null, "tcdate": 1603973581855, "tmdate": 1606741401595, "tddate": null, "forum": "u15gHPQViL", "replyto": "u15gHPQViL", "invitation": "ICLR.cc/2021/Conference/Paper2412/-/Official_Review", "content": {"title": "AnonReviewer2 [Finalized after authors' response]", "review": "*Summary*\nThe authors tackle the problem of zero-shot learning, that is, the recognition of classes and categories for which no visual data are available, but only semantic embedding, providing a description of the classes in terms of auxiliary textual descriptions. To this aim, authors propose a method dubbed  Image-Guided Semantic Classification in which a two-stream network (fed by either visual and semantic embedding) learns a compatibility function whose recognition performance is enhanced by means of calibrated stacking (Chao et al. 2016).  \n\n*Pros*\n* The method is simple and easy to understand. \n\n\n*Cons*\n* The computational pipeline is not novel and largely inspired by methods such as the ones proposed in [Yang et al., A Unified Perspective on Multi-Domain and Multi-Task Learning, ICLR 2015] or [Liu et al. Generalized Zero-Shot Learning with Deep\nCalibration Network, NeurIPS 2018] which are not even cited in the paper, unfortunately. Authors seem essential to add a calibration module to this kind of architectures: since the calibration module is inherited from prior works (Chao et al. 2016), I found the method quite incremental.\n\n* Within the experimental comparison, GAN-based methods are not reported although being a mainstream class of state-of-the-art methods. I am referring to works such as CLSWGAN [Xian et al. Features Generating Networks for Zero-Shot Learning, CVPR 2018], f-VAEGAN-D2 [Xian et al. A Feature Generating Framework for Any-Shot Learning, CVPR 2019],  CADA-VAE [Schonfeld et al., Generalized zero- and few-shot learning via aligned variational autoencoders, CVPR 2019], DLFZRL [Tong et al., Hierarchical disentanglement of discriminative latent features for zero-shot learning, CVPR 2019] or tf-VAEGAN [Narayan et al. Latent Embedding Feedback and Discriminative Features for Zero-Shot Classification, ECCV 2020]. Authors justified this approach by reducing the methods in comparison within the ones which use unseen semantic embeddings only at test time. However, the knowledge of semantic embeddings also for the unseen classes is something which is necessary in a zero-shot recognition paradigm: the authors themselves take advantage of them during the nearest neighbor search. Thus, at this point, using the semantic embeddings for the unseen classes is therefore legit and I do not see any added value in constraining the experiments.\n\n* The reported performance is highly suboptimal with respect to the state-of-the-art: invertible zero-shot recognition flows, recently proposed at ECCV 2020, greatly outperformed the proposed approach by margin: H=49.8 on aPY, H=54.8 on SUN, H=59.4 on CUB and H=68.0 on AWA2.  \n\n*Pre-Rebuttal Evaluation*\nI regret to register a substantial overlap with prior methods, thus undermining the novelty impact of the present submission. On the experimental side, I found a sharply gapped performance which is highly inferior with respect to the state-of-the-art, caused by reducing the approaches included in the comparison on the basis of which methods exploit unseen semantic embeddings for training (this claim is not convincing in my opinion, unseen semantic embeddings are used in any cases, why not exploiting them for feature generation purposes?). For those reasons I am afraid to discourage the acceptance of the manuscript\n\n*Post-Rebuttal Evaluation [FINAL]*\nI would like to thank the authors for their response and for the updated version of the manuscript, I appreciate their efforts. Unfortunately, I still believe that the paper lacking about original contribution and I am not fully convinced by the authors' comments on the relationship with [Yang et al., A Unified Perspective on Multi-Domain and Multi-Task Learning, ICLR 2015] and [Liu et al. Generalized Zero-Shot Learning with Deep Calibration Network, NeurIPS 2018] which I still judge highly overlapping with the current methodology.\nFurthermore, although authors clarified on the avoidance of using semantic embeddings for the training methodology, I do not see a sharp point in pursuing this approach given the high gap in performance with prior art (GAN-based). \nFor all these reasons, I regret to confirm my initial rejection score.\n\n", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper2412/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2412/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Zero-Shot Recognition through Image-Guided Semantic Classification", "authorids": ["~Mei-Chen_Yeh1", "~Fang_Li5", "a0917251699@gmail.com"], "authors": ["Mei-Chen Yeh", "Fang Li", "Bo-Heng Li"], "keywords": ["zero-shot learning", "visual-semantic embedding", "deep learning"], "abstract": "We present a new visual-semantic embedding method for generalized zero-shot learning. Existing embedding-based methods aim to learn the correspondence between an image classifier (visual representation) and its class prototype (semantic representation) for each class. Inspired by the binary relevance method for multi-label classification, we learn the mapping between an image and its semantic classifier. Given an input image, the proposed Image-Guided Semantic Classification (IGSC) method creates a label classifier, being applied to all label embeddings to determine whether a label belongs to the input image. Therefore, a semantic classifier is image conditioned and is generated during inference. We also show that IGSC is a unifying framework for two state-of-the-art deep-embedding methods. We validate our approach with four standard benchmark datasets.\n\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yeh|zeroshot_recognition_through_imageguided_semantic_classification", "pdf": "/pdf/1cbbbe8244990488eb4cd71454e5cbfc32b4646c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=_xQVLLzjyO", "_bibtex": "@misc{\nyeh2021zeroshot,\ntitle={Zero-Shot Recognition through Image-Guided Semantic Classification},\nauthor={Mei-Chen Yeh and Fang Li and Bo-Heng Li},\nyear={2021},\nurl={https://openreview.net/forum?id=u15gHPQViL}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "u15gHPQViL", "replyto": "u15gHPQViL", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2412/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538096927, "tmdate": 1606915766373, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2412/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2412/-/Official_Review"}}}, {"id": "MttQK45Spgf", "original": null, "number": 6, "cdate": 1606275987866, "ddate": null, "tcdate": 1606275987866, "tmdate": 1606275987866, "tddate": null, "forum": "u15gHPQViL", "replyto": "u15gHPQViL", "invitation": "ICLR.cc/2021/Conference/Paper2412/-/Official_Comment", "content": {"title": "Summary of changes", "comment": "We would like to thank all reviewers for their valuable and constructive comments, which help us improve the paper. \n1. We have added a paragraph in the experimental section discussing the comparison to GAN-based methods.\n2. We have added subsection 4.4 discussing the model flexibility and visualizing the generated classifier weights.\n3. We have fixed all errors mentioned in the comments. \n\nBelow please find our response to each comment."}, "signatures": ["ICLR.cc/2021/Conference/Paper2412/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2412/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Zero-Shot Recognition through Image-Guided Semantic Classification", "authorids": ["~Mei-Chen_Yeh1", "~Fang_Li5", "a0917251699@gmail.com"], "authors": ["Mei-Chen Yeh", "Fang Li", "Bo-Heng Li"], "keywords": ["zero-shot learning", "visual-semantic embedding", "deep learning"], "abstract": "We present a new visual-semantic embedding method for generalized zero-shot learning. Existing embedding-based methods aim to learn the correspondence between an image classifier (visual representation) and its class prototype (semantic representation) for each class. Inspired by the binary relevance method for multi-label classification, we learn the mapping between an image and its semantic classifier. Given an input image, the proposed Image-Guided Semantic Classification (IGSC) method creates a label classifier, being applied to all label embeddings to determine whether a label belongs to the input image. Therefore, a semantic classifier is image conditioned and is generated during inference. We also show that IGSC is a unifying framework for two state-of-the-art deep-embedding methods. We validate our approach with four standard benchmark datasets.\n\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yeh|zeroshot_recognition_through_imageguided_semantic_classification", "pdf": "/pdf/1cbbbe8244990488eb4cd71454e5cbfc32b4646c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=_xQVLLzjyO", "_bibtex": "@misc{\nyeh2021zeroshot,\ntitle={Zero-Shot Recognition through Image-Guided Semantic Classification},\nauthor={Mei-Chen Yeh and Fang Li and Bo-Heng Li},\nyear={2021},\nurl={https://openreview.net/forum?id=u15gHPQViL}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "u15gHPQViL", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2412/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2412/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2412/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2412/Authors|ICLR.cc/2021/Conference/Paper2412/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2412/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923848673, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2412/-/Official_Comment"}}}, {"id": "7G_H_hmOst0", "original": null, "number": 4, "cdate": 1606118419806, "ddate": null, "tcdate": 1606118419806, "tmdate": 1606275267672, "tddate": null, "forum": "u15gHPQViL", "replyto": "keueJPowQXP", "invitation": "ICLR.cc/2021/Conference/Paper2412/-/Official_Comment", "content": {"title": "Responses to AnonReviewer1", "comment": "1. Thank you for indicating the VQA work. Although a similar idea has been explored in solving the VQA problem, such a powerful idea has not been investigated for visual recognition, especially with the zero-shot learning setting. This technique is particularly useful to ZSL because the relationships of the visual evidences and the attributes---learned globally from seen classes---can help recognition involving unseen classes. Learning this correspondence is even suitable for solving image recognition because in VAQ one image usually has only a few questions while in our case one class has many images.\n\n2. Thank you for giving the comment. We used the same math notations as those in the SP-AEN paper (Chen et al., CVPR 2018). We have fixed it in the paper.\n\n3. AREN is the state-of-the-art embedding-based approach. It involves two branches of region attention mechanisms in the model design. We have corrected the acc_u value of SUN by AREN. Both approaches can benefit from the CS trick. If CS is not used, AREN achieved 25.5/52.1/26.7/16.4, while our results are 25.7/39.3/32.1/22.5. Except CUB, we outperformed AREN in all datasets.\n\n4. Please refer to the response to AnonReviewer4.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2412/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2412/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Zero-Shot Recognition through Image-Guided Semantic Classification", "authorids": ["~Mei-Chen_Yeh1", "~Fang_Li5", "a0917251699@gmail.com"], "authors": ["Mei-Chen Yeh", "Fang Li", "Bo-Heng Li"], "keywords": ["zero-shot learning", "visual-semantic embedding", "deep learning"], "abstract": "We present a new visual-semantic embedding method for generalized zero-shot learning. Existing embedding-based methods aim to learn the correspondence between an image classifier (visual representation) and its class prototype (semantic representation) for each class. Inspired by the binary relevance method for multi-label classification, we learn the mapping between an image and its semantic classifier. Given an input image, the proposed Image-Guided Semantic Classification (IGSC) method creates a label classifier, being applied to all label embeddings to determine whether a label belongs to the input image. Therefore, a semantic classifier is image conditioned and is generated during inference. We also show that IGSC is a unifying framework for two state-of-the-art deep-embedding methods. We validate our approach with four standard benchmark datasets.\n\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yeh|zeroshot_recognition_through_imageguided_semantic_classification", "pdf": "/pdf/1cbbbe8244990488eb4cd71454e5cbfc32b4646c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=_xQVLLzjyO", "_bibtex": "@misc{\nyeh2021zeroshot,\ntitle={Zero-Shot Recognition through Image-Guided Semantic Classification},\nauthor={Mei-Chen Yeh and Fang Li and Bo-Heng Li},\nyear={2021},\nurl={https://openreview.net/forum?id=u15gHPQViL}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "u15gHPQViL", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2412/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2412/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2412/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2412/Authors|ICLR.cc/2021/Conference/Paper2412/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2412/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923848673, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2412/-/Official_Comment"}}}, {"id": "29LfKikK6i", "original": null, "number": 2, "cdate": 1606118098090, "ddate": null, "tcdate": 1606118098090, "tmdate": 1606274205766, "tddate": null, "forum": "u15gHPQViL", "replyto": "thCRuYL2mn", "invitation": "ICLR.cc/2021/Conference/Paper2412/-/Official_Comment", "content": {"title": "Responses to AnonReviewer2 ", "comment": "1. Thank you for indicating the methods [ICLR 2015 and NeurIPS 2018]. Both methods project visual and semantic representations to a latent space and perform classification by the nearest prototype classifier (NPC), which in our view apply the conventional zero-shot learning paradigm as illustrated in Fig. 1 (a) and (b) in our paper. In the ICLR 2015 paper, $P$ and $Q$ are static during inference. Similarly, the prediction function $f$ in the NeurIPS 2018 paper is fixed once trained. We propose a completely different approach, in that the model (label classifier) construction is adaptive to the input image during test-time. We will cite the papers in our papers. \nIndeed, the calibration module is not the focus of our paper. We directly use this trick from Chao et al. 2016, as it additionally improves the performance.\n2. Thank you for referring GAN-based methods. The settings are different in that GAN-based methods require the semantic embeddings for the unseen classes but we do not. We are confused about the sentence about \"the authors themselves take advantage of them\" because, we neither use them (the semantic embeddings for the unseen classes) nor the nearest neighbor search (i.e. we did not perform label classification via nearest neighbor search). We agree with you and other reviewers that we should not restrict ourselves to the instance- and class-inductive setting. We agree that GAN-based methods provide a very strong data augmentation scheme and therefore should significant benefit our method for boosting the recognition performance.\n3. To achieve state-of-the-art performance, one method must have a powerful feature generating method (like GAN-based methods) and an effective classifier (like the focus of our paper). Most GAN-based methods focus on the former and use linear softmax classifiers, which we believe these STOA methods can be improved by using the proposed, adaptive classification approach. We have added a paragraph (the last paragraph in the experimental section) that compares our method to GAN-based methods.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2412/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2412/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Zero-Shot Recognition through Image-Guided Semantic Classification", "authorids": ["~Mei-Chen_Yeh1", "~Fang_Li5", "a0917251699@gmail.com"], "authors": ["Mei-Chen Yeh", "Fang Li", "Bo-Heng Li"], "keywords": ["zero-shot learning", "visual-semantic embedding", "deep learning"], "abstract": "We present a new visual-semantic embedding method for generalized zero-shot learning. Existing embedding-based methods aim to learn the correspondence between an image classifier (visual representation) and its class prototype (semantic representation) for each class. Inspired by the binary relevance method for multi-label classification, we learn the mapping between an image and its semantic classifier. Given an input image, the proposed Image-Guided Semantic Classification (IGSC) method creates a label classifier, being applied to all label embeddings to determine whether a label belongs to the input image. Therefore, a semantic classifier is image conditioned and is generated during inference. We also show that IGSC is a unifying framework for two state-of-the-art deep-embedding methods. We validate our approach with four standard benchmark datasets.\n\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yeh|zeroshot_recognition_through_imageguided_semantic_classification", "pdf": "/pdf/1cbbbe8244990488eb4cd71454e5cbfc32b4646c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=_xQVLLzjyO", "_bibtex": "@misc{\nyeh2021zeroshot,\ntitle={Zero-Shot Recognition through Image-Guided Semantic Classification},\nauthor={Mei-Chen Yeh and Fang Li and Bo-Heng Li},\nyear={2021},\nurl={https://openreview.net/forum?id=u15gHPQViL}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "u15gHPQViL", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2412/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2412/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2412/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2412/Authors|ICLR.cc/2021/Conference/Paper2412/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2412/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923848673, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2412/-/Official_Comment"}}}, {"id": "KS16KcuKPMi", "original": null, "number": 3, "cdate": 1606118282971, "ddate": null, "tcdate": 1606118282971, "tmdate": 1606118948237, "tddate": null, "forum": "u15gHPQViL", "replyto": "RUpNdrCWu4i", "invitation": "ICLR.cc/2021/Conference/Paper2412/-/Official_Comment", "content": {"title": "Response to AnonReviewer4", "comment": "We agree that it is important to evaluate the performance of our method in the transductive setting. Based on our survey, there are two possible solutions: (1) integrating GAN-based methods for generating synthetic features of unseen classes; (2) manipulating semantic vectors for introducing a reconstruction loss to regularize the model. We have been experimenting with GAN-based methods (i.e. a simplified VAEGAN similar to f-VAEGAN-D2 proposed in CVPR 2019) but have not been successful for training a reasonable feature generator by far. For the second solution, because we do not apply any transformation to the semantic vectors (they are solely used to evaluate the learned label classifier), we found it non-trivial to construct a reconstruction loss."}, "signatures": ["ICLR.cc/2021/Conference/Paper2412/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2412/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Zero-Shot Recognition through Image-Guided Semantic Classification", "authorids": ["~Mei-Chen_Yeh1", "~Fang_Li5", "a0917251699@gmail.com"], "authors": ["Mei-Chen Yeh", "Fang Li", "Bo-Heng Li"], "keywords": ["zero-shot learning", "visual-semantic embedding", "deep learning"], "abstract": "We present a new visual-semantic embedding method for generalized zero-shot learning. Existing embedding-based methods aim to learn the correspondence between an image classifier (visual representation) and its class prototype (semantic representation) for each class. Inspired by the binary relevance method for multi-label classification, we learn the mapping between an image and its semantic classifier. Given an input image, the proposed Image-Guided Semantic Classification (IGSC) method creates a label classifier, being applied to all label embeddings to determine whether a label belongs to the input image. Therefore, a semantic classifier is image conditioned and is generated during inference. We also show that IGSC is a unifying framework for two state-of-the-art deep-embedding methods. We validate our approach with four standard benchmark datasets.\n\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yeh|zeroshot_recognition_through_imageguided_semantic_classification", "pdf": "/pdf/1cbbbe8244990488eb4cd71454e5cbfc32b4646c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=_xQVLLzjyO", "_bibtex": "@misc{\nyeh2021zeroshot,\ntitle={Zero-Shot Recognition through Image-Guided Semantic Classification},\nauthor={Mei-Chen Yeh and Fang Li and Bo-Heng Li},\nyear={2021},\nurl={https://openreview.net/forum?id=u15gHPQViL}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "u15gHPQViL", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2412/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2412/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2412/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2412/Authors|ICLR.cc/2021/Conference/Paper2412/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2412/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923848673, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2412/-/Official_Comment"}}}, {"id": "uQi_d3WESi", "original": null, "number": 5, "cdate": 1606118718207, "ddate": null, "tcdate": 1606118718207, "tmdate": 1606118718207, "tddate": null, "forum": "u15gHPQViL", "replyto": "lZW_eBwJ7NH", "invitation": "ICLR.cc/2021/Conference/Paper2412/-/Official_Comment", "content": {"title": "Responses to AnonReviewer3", "comment": "1. Thank you for your suggestion. In the paper we have added Section 4.4 and Fig. 3, involving a visualization of the weight values that validates the motivation of learning image-adaptive classification weights. Two interesting observations are: (1) our method tends to generate similar weight values for images of the same class; (2) Although the proposed framework allows the learning of a \u201csingle\u201d label classifier for each class (class-level classifier), instance-level classifiers benefit the optimization of recognition accuracy (points are \u201cscattered\u201d but fall around).\n\n2. The cross-entropy loss in (8) is a reasonable choice. It is not specifically beneficial to the proposed method. We had experimented with the log-sum-exp pairwise loss function (a smooth version of the hinge loss) and achieved a similar performance but with a longer training time. For the non-linear label classifier, the hidden layer dimension $h$ was set to 30 in all experiments. Conceptually the nonlinear classifier can be very deep, say, we can use a Res-101 to generate image-adaptive Res101s. We had added one more layer but did not find improved performance. However, we were not sure if the performance saturated because we experimented with the ZSL benchmark datasets (not very large). \n\n3. We agree that we should provide the performance with the feature extractor fine-tuned, but it is sad to tell that the machine we can afford does not have sufficient memory to conduct this experiment. Instead, we had replaced the Res-101 features with Res-152 and showed the results in Section 4.4 and Table 6. Performance improvements were observed when we extracted visual features by using a deeper visual model.\n\n4. Please refer to our 3rd response to AnonReviewer2.\n \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2412/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2412/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Zero-Shot Recognition through Image-Guided Semantic Classification", "authorids": ["~Mei-Chen_Yeh1", "~Fang_Li5", "a0917251699@gmail.com"], "authors": ["Mei-Chen Yeh", "Fang Li", "Bo-Heng Li"], "keywords": ["zero-shot learning", "visual-semantic embedding", "deep learning"], "abstract": "We present a new visual-semantic embedding method for generalized zero-shot learning. Existing embedding-based methods aim to learn the correspondence between an image classifier (visual representation) and its class prototype (semantic representation) for each class. Inspired by the binary relevance method for multi-label classification, we learn the mapping between an image and its semantic classifier. Given an input image, the proposed Image-Guided Semantic Classification (IGSC) method creates a label classifier, being applied to all label embeddings to determine whether a label belongs to the input image. Therefore, a semantic classifier is image conditioned and is generated during inference. We also show that IGSC is a unifying framework for two state-of-the-art deep-embedding methods. We validate our approach with four standard benchmark datasets.\n\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yeh|zeroshot_recognition_through_imageguided_semantic_classification", "pdf": "/pdf/1cbbbe8244990488eb4cd71454e5cbfc32b4646c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=_xQVLLzjyO", "_bibtex": "@misc{\nyeh2021zeroshot,\ntitle={Zero-Shot Recognition through Image-Guided Semantic Classification},\nauthor={Mei-Chen Yeh and Fang Li and Bo-Heng Li},\nyear={2021},\nurl={https://openreview.net/forum?id=u15gHPQViL}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "u15gHPQViL", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2412/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2412/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2412/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2412/Authors|ICLR.cc/2021/Conference/Paper2412/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2412/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923848673, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2412/-/Official_Comment"}}}, {"id": "lZW_eBwJ7NH", "original": null, "number": 1, "cdate": 1603766418032, "ddate": null, "tcdate": 1603766418032, "tmdate": 1605024217139, "tddate": null, "forum": "u15gHPQViL", "replyto": "u15gHPQViL", "invitation": "ICLR.cc/2021/Conference/Paper2412/-/Official_Review", "content": {"title": "A simple yet effective method for zero-shot learning", "review": "------------\nSummary: This paper proposes a simple yet effective method for zero-shot learning. In the method, a network is learned to predict the compatibility function weight given the input of the image. The predicted weight is then applied to semantic attributes and the final class label is predicted by the maximum compatibility score. The method is evaluated on benchmark datasets and illustrates competitive performance.\n\n------------\n\nReason for score:\nThe paper is overall clear and easy to understand, and the proposed method is simple and achieves competitive results. But the paper lacks significance and contributions (detailed below), which makes me hard to judge its potential impact in the related area . So I don't find the paper meet the standard of ICLR. Hopefully the authors can address my concerns in the rebuttal period.\n\n------------\n\nPros:\n1. The authors propose a simple method for zero-shot learning motivated by learning instance-guided classifier weights.\n2. The method demonstrates competitive results on benchmark datasets with a very simple implementation.\n3. The paper is written clearly and easy to follow.\n\n------------\nCons:\n1. The paper fails to validate its motivation. The authors claim the method is proposed to learn an \"image-guided semantic classification model\" and the \"weight values depend on the input image\". Although the model architecture complies the motivation and the evaluation results are good, it is hard to tell if the results are achieved by the \"image-guided weight values\". The authors didn't provide either theoretical analysis or empirical evaluations to demonstrate that the weights learned by the model is image-sensitive and the overall performance benefits from it. For example, it will be helpful if the authors can visualize the \n2. Some implementation details are missing and some choices in the method lack explanations. For example, why the model is trained by a cross-entropy loss in (8)? Is it specifically beneficial to the proposed method in the zero-shot learning context? For the non-linear label classifier, what is the hidden layer dimension $h$ used in the experiments? What is the effect of extending the non-linear label classifier to deeper networks? Will it saturate? etc. The paper lacks this type of details or evaluations, which makes the readers hard to evaluate its potential effectiveness.\n3. The proposed method is a framework which can implemented in various ways, but the authors fail to provide enough analysis on its potential extensions. For example, in the experiments, for fair comparison the authors froze the feature extractor. But it will be helpful if the authors can also provide the performance with the feature extractor fine-tuned, so as to provide insight on how this framework will synergize with deep feature networks. In addition, I'd love to see the performance of the proposed method on large-scale recognition-in-the-wild dataset like ImageNet.\n4. Although the authors fix the feature extractor and only compare with the methods in the exact same setting (namely, feature extractor fixed, no unseen information in training), the baselines compared are rather weak. For example, the baseline models may not benefit from the Calibrated Stacking (CS),  and the proposed method without CS has less significant results. Some baseline models like SP-AEN have lower unseen and higher seen than the proposed method, so it is possible they can achieve better performance with CS. Also, like we discussed in 3, many feature-based methods achieve significantly better results and it will be great to show how the proposed method works if the entire model including the feature extractor is trained end-to-end. \n\nBased on the points above, I find the paper less significant and lack contributions, thus giving my score.", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper2412/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2412/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Zero-Shot Recognition through Image-Guided Semantic Classification", "authorids": ["~Mei-Chen_Yeh1", "~Fang_Li5", "a0917251699@gmail.com"], "authors": ["Mei-Chen Yeh", "Fang Li", "Bo-Heng Li"], "keywords": ["zero-shot learning", "visual-semantic embedding", "deep learning"], "abstract": "We present a new visual-semantic embedding method for generalized zero-shot learning. Existing embedding-based methods aim to learn the correspondence between an image classifier (visual representation) and its class prototype (semantic representation) for each class. Inspired by the binary relevance method for multi-label classification, we learn the mapping between an image and its semantic classifier. Given an input image, the proposed Image-Guided Semantic Classification (IGSC) method creates a label classifier, being applied to all label embeddings to determine whether a label belongs to the input image. Therefore, a semantic classifier is image conditioned and is generated during inference. We also show that IGSC is a unifying framework for two state-of-the-art deep-embedding methods. We validate our approach with four standard benchmark datasets.\n\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yeh|zeroshot_recognition_through_imageguided_semantic_classification", "pdf": "/pdf/1cbbbe8244990488eb4cd71454e5cbfc32b4646c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=_xQVLLzjyO", "_bibtex": "@misc{\nyeh2021zeroshot,\ntitle={Zero-Shot Recognition through Image-Guided Semantic Classification},\nauthor={Mei-Chen Yeh and Fang Li and Bo-Heng Li},\nyear={2021},\nurl={https://openreview.net/forum?id=u15gHPQViL}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "u15gHPQViL", "replyto": "u15gHPQViL", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2412/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538096927, "tmdate": 1606915766373, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2412/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2412/-/Official_Review"}}}], "count": 11}