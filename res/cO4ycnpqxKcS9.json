{"notes": [{"ddate": null, "legacy_migration": true, "tmdate": 1392699660000, "tcdate": 1392699660000, "number": 3, "id": "jrhLjNJlLRr45", "invitation": "ICLR.cc/2014/-/submission/workshop/review", "forum": "cO4ycnpqxKcS9", "replyto": "cO4ycnpqxKcS9", "signatures": ["Karen Simonyan"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "We thank the reviewers for their positive feedback.\r\n\r\nR1: Not impressed by the per-class canonical image generation ... given the work of Erhan et al.\r\n\r\nWe agree that our per class canonical image visualisation is based on that of Erhan [5], and thus not the most original part of the paper. However, it is worth including for two reasons:\r\n1) As noted by R2, we are the first to apply it to ImageNet classification ConvNets, and furthermore we visualise the activities in the final fully-connected layer (rather than the soft-max layer, which leads to less prominent visualisations).\r\n2) We establish a close connection between gradient-based visualisation techniques (such as canonical image generation and class saliency maps) and Deconvolutional networks of Zeiler and Fergus.\r\n\r\nR1: I think the saliency map method is quite interesting. In particular, the fact that it can be leveraged to obtain a decent object localizer, which is only partially supervised, seems impressive.\r\nR2: I found the weakly supervised object localization application the most impressive part of the paper.\r\n\r\nWe also feel that this is one of the most interesting aspects of our contribution. In particular, we were also impressed by the ability of the network to learn object segmentation in a *weakly supervised* setting (and produce object localisations, competitive with strongly supervised conventional object detectors). We believe that our image specific class saliency maps can be used in applications beyond GraphCut segmentation initialisation, and we plan to address them in future work."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps", "decision": "submitted, no decision", "abstract": "This paper addresses the visualisation of image classification models, learnt using deep Convolutional Networks (ConvNets). We consider two visualisation techniques, based on computing the gradient of the class score with respect to the input image. The first one generates an image, which maximises the class score [Erhan et al., 2009], thus visualising the notion of the class, captured by a ConvNet. The second technique computes a class saliency map, specific to a given image and class. We show that such maps can be employed for weakly supervised object segmentation using classification ConvNets. Finally, we establish the connection between the gradient-based ConvNet visualisation methods and deconvolutional networks [Zeiler et al., 2013].", "pdf": "https://arxiv.org/abs/1312.6034", "paperhash": "simonyan|deep_inside_convolutional_networks_visualising_image_classification_models_and_saliency_maps", "keywords": [], "conflicts": [], "authors": ["Karen Simonyan", "Andrea Vedaldi", "Andrew Zisserman"], "authorids": ["karen@robots.ox.ac.uk", "vedaldi@robots.ox.ac.uk", "az@robots.ox.ac.uk"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391887080000, "tcdate": 1391887080000, "number": 2, "id": "jKcYnfEAY_jL-", "invitation": "ICLR.cc/2014/-/submission/workshop/review", "forum": "cO4ycnpqxKcS9", "replyto": "cO4ycnpqxKcS9", "signatures": ["anonymous reviewer 9e94"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps", "review": "Deep convolutional neural networks (convnets) have achieved tremendous success lately in large-scale visual recognition. Their popularity has exploded after winning recent high-profile competitions. As more research groups begin to experiment with convnets, there is increasing interest into what is happening *inside* the convnet. A few papers have been published within the last year providing ways of visualizing what units inside the convnet represent, and also visualizing the spatial support of a particular class. This paper presents two methods for visualization of convnets: one, based on an approach by Erhan, simply backpropagates the gradient of the class score with respect to the image pixels to generate class appearance models. The other method, class-saliency maps, visualize spatial support for a class and are also used to perform weakly supervised object localization.\r\n\r\nThe authors are very clear about their contributions, mainly in producing understandable visualizations. In terms of novelty, the method by which one obtains the class appearance models have been used in the unsupervised learning context by Erhan et al., but this paper is the first to apply the technique to convnets. The method by which to obtain the class saliency maps is intuitive and produces reasonable visualizations. I found the weakly supervised object localization application the most impressive part of the paper. Although it does not perform nearly as well as methods that consider localization part of training, it's promising to see how localization can be learned without bounding boxes.\r\n\r\nPros:\r\n* Clear, simple\r\n* Provides a useful, practical tool for convnet practitioners\r\n* The discussion re: Zeiler and Fergus' Deconvolutional net method clears up any misunderstanding among the two methods which do seem pretty similar\r\n* Evaluated on large-scale data (ILSVRC-2013)\r\n\r\nCons\r\n* Though the similarity to the Deconvolutional net method is acknowledged, the technical contribution of this work is not a massive departure from the other work (though suitable for workshop track)\r\n\r\nOverall, I think this is a good workshop paper. The methodology does not depart far from previous work but the weakly supervised localization is interesting and will generate interest at the conference.\r\n\r\nComments\r\n========\r\n\r\nFurther insight/discussion on the implication of the change in treatment between the Deconvolutional net method and the proposed method is suggested."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps", "decision": "submitted, no decision", "abstract": "This paper addresses the visualisation of image classification models, learnt using deep Convolutional Networks (ConvNets). We consider two visualisation techniques, based on computing the gradient of the class score with respect to the input image. The first one generates an image, which maximises the class score [Erhan et al., 2009], thus visualising the notion of the class, captured by a ConvNet. The second technique computes a class saliency map, specific to a given image and class. We show that such maps can be employed for weakly supervised object segmentation using classification ConvNets. Finally, we establish the connection between the gradient-based ConvNet visualisation methods and deconvolutional networks [Zeiler et al., 2013].", "pdf": "https://arxiv.org/abs/1312.6034", "paperhash": "simonyan|deep_inside_convolutional_networks_visualising_image_classification_models_and_saliency_maps", "keywords": [], "conflicts": [], "authors": ["Karen Simonyan", "Andrea Vedaldi", "Andrew Zisserman"], "authorids": ["karen@robots.ox.ac.uk", "vedaldi@robots.ox.ac.uk", "az@robots.ox.ac.uk"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1390788960000, "tcdate": 1390788960000, "number": 1, "id": "XDxVTYb9VtT9N", "invitation": "ICLR.cc/2014/-/submission/workshop/review", "forum": "cO4ycnpqxKcS9", "replyto": "cO4ycnpqxKcS9", "signatures": ["anonymous reviewer e565"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps", "review": "This paper presents methods for visualizing the behaviour of an object recognition convolutional neural network. The first method generates a 'canonical image' for a given class that the network can recognize. The second generates a saliency map for a given input image and specified class, that illustrates the part of the image (pixels) that influence the most the given class's output probability. This can be used to seed a graphcut segmentation and localize objects of that class in the input image. Finally, a connection between the saliency map method and the work of Zeiler and Fergus on using deconvolutions to visualize deep networks is established.\r\n\r\nWhile I'm not impressed by the per-class canonical image generation (which isn't very original anyways, given the work of Erhan et al.), I think the saliency map method is quite interesting. In particular, the fact that it can be leveraged to obtain a decent object localizer, which is only partially supervised, seems impressive. This is probably the most interesting part of the paper. As for the connection with deconvolution, I think it's also a nice observation.\r\n\r\nAs for the cons of this paper, they are those you expect from a workshop paper, i.e. the experimental work could be stronger. Specifically, I feel like there is a lack of quantitative comparisons. I wonder whether other alternatives to the graphcut initialization could have served as baselines with which to compare quantitatively (but this isn't my expertise, so perhaps there aren't...). The fact that one of their previous systems (which was fully supervised for localization) actually performs worse than this partially supervised system is certainly impressive however!"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps", "decision": "submitted, no decision", "abstract": "This paper addresses the visualisation of image classification models, learnt using deep Convolutional Networks (ConvNets). We consider two visualisation techniques, based on computing the gradient of the class score with respect to the input image. The first one generates an image, which maximises the class score [Erhan et al., 2009], thus visualising the notion of the class, captured by a ConvNet. The second technique computes a class saliency map, specific to a given image and class. We show that such maps can be employed for weakly supervised object segmentation using classification ConvNets. Finally, we establish the connection between the gradient-based ConvNet visualisation methods and deconvolutional networks [Zeiler et al., 2013].", "pdf": "https://arxiv.org/abs/1312.6034", "paperhash": "simonyan|deep_inside_convolutional_networks_visualising_image_classification_models_and_saliency_maps", "keywords": [], "conflicts": [], "authors": ["Karen Simonyan", "Andrea Vedaldi", "Andrew Zisserman"], "authorids": ["karen@robots.ox.ac.uk", "vedaldi@robots.ox.ac.uk", "az@robots.ox.ac.uk"]}, "tags": [], "invitation": {}}}, {"replyto": null, "ddate": null, "legacy_migration": true, "tmdate": 1387779780000, "tcdate": 1387779780000, "number": 6, "id": "cO4ycnpqxKcS9", "invitation": "ICLR.cc/2014/workshop/-/submission", "forum": "cO4ycnpqxKcS9", "signatures": ["karen@robots.ox.ac.uk"], "readers": ["everyone"], "content": {"title": "Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps", "decision": "submitted, no decision", "abstract": "This paper addresses the visualisation of image classification models, learnt using deep Convolutional Networks (ConvNets). We consider two visualisation techniques, based on computing the gradient of the class score with respect to the input image. The first one generates an image, which maximises the class score [Erhan et al., 2009], thus visualising the notion of the class, captured by a ConvNet. The second technique computes a class saliency map, specific to a given image and class. We show that such maps can be employed for weakly supervised object segmentation using classification ConvNets. Finally, we establish the connection between the gradient-based ConvNet visualisation methods and deconvolutional networks [Zeiler et al., 2013].", "pdf": "https://arxiv.org/abs/1312.6034", "paperhash": "simonyan|deep_inside_convolutional_networks_visualising_image_classification_models_and_saliency_maps", "keywords": [], "conflicts": [], "authors": ["Karen Simonyan", "Andrea Vedaldi", "Andrew Zisserman"], "authorids": ["karen@robots.ox.ac.uk", "vedaldi@robots.ox.ac.uk", "az@robots.ox.ac.uk"]}, "writers": [], "details": {"replyCount": 3, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1369422751717, "tmdate": 1496674357014, "id": "ICLR.cc/2014/workshop/-/submission", "writers": ["ICLR.cc/2014"], "signatures": ["OpenReview.net"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": []}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1377198751717, "cdate": 1496674357014}}}], "count": 4}