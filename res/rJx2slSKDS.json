{"notes": [{"id": "rJx2slSKDS", "original": "HJeNa1-twr", "number": 2514, "cdate": 1569439907613, "ddate": null, "tcdate": 1569439907613, "tmdate": 1577168240175, "tddate": null, "forum": "rJx2slSKDS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["zhaodeli@gmail.com", "jengzhu0@gmail.com", "zhangbo@xiaomi.com"], "title": "Latent Variables on Spheres for Sampling and Inference", "authors": ["Deli Zhao", "Jiapeng Zhu", "Bo Zhang"], "pdf": "/pdf/cf09471639922342c6ed3485a723244ecece8726.pdf", "abstract": "Variational inference is a fundamental problem in Variational AutoEncoder (VAE). The optimization with lower bound of marginal log-likelihood results in the distribution of latent variables approximate to a given prior probability, which is the dilemma of employing VAE to solve real-world problems. By virtue of high-dimensional geometry, we propose a very simple algorithm completely different from existing ones to alleviate the variational inference in VAE. We analyze the unique characteristics of random variables on spheres in high dimensions and prove that Wasserstein distance between two arbitrary data sets randomly drawn from a sphere are nearly identical when the dimension is sufficiently large. Based on our theory, a novel algorithm for distribution-robust sampling is devised. Moreover, we reform the latent space of VAE by constraining latent variables on the sphere, thus freeing VAE from the approximate optimization of posterior probability via variational inference. The new algorithm is named Spherical AutoEncoder (SAE). Extensive experiments by sampling and inference tasks validate our theoretical analysis and the superiority of SAE.", "keywords": ["variational autoencoder", "generative adversarial network"], "paperhash": "zhao|latent_variables_on_spheres_for_sampling_and_inference", "original_pdf": "/attachment/6e124d9b9bf80c513483e35a90a68f3860623f0a.pdf", "_bibtex": "@misc{\nzhao2020latent,\ntitle={Latent Variables on Spheres for Sampling and Inference},\nauthor={Deli Zhao and Jiapeng Zhu and Bo Zhang},\nyear={2020},\nurl={https://openreview.net/forum?id=rJx2slSKDS}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "8l3nr14lO", "original": null, "number": 1, "cdate": 1576798750887, "ddate": null, "tcdate": 1576798750887, "tmdate": 1576800884824, "tddate": null, "forum": "rJx2slSKDS", "replyto": "rJx2slSKDS", "invitation": "ICLR.cc/2020/Conference/Paper2514/-/Decision", "content": {"decision": "Reject", "comment": "This paper proposes to improve VAE/GAN by performing variational inference with a constraint that the latent variables lie on a sphere. The reviewers find some technical issues with the paper (R3's comment regarding theorem 3). They also found that the method is not motivated well, and the paper is not convincing. Based on this feedback, I recommend to reject the paper.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zhaodeli@gmail.com", "jengzhu0@gmail.com", "zhangbo@xiaomi.com"], "title": "Latent Variables on Spheres for Sampling and Inference", "authors": ["Deli Zhao", "Jiapeng Zhu", "Bo Zhang"], "pdf": "/pdf/cf09471639922342c6ed3485a723244ecece8726.pdf", "abstract": "Variational inference is a fundamental problem in Variational AutoEncoder (VAE). The optimization with lower bound of marginal log-likelihood results in the distribution of latent variables approximate to a given prior probability, which is the dilemma of employing VAE to solve real-world problems. By virtue of high-dimensional geometry, we propose a very simple algorithm completely different from existing ones to alleviate the variational inference in VAE. We analyze the unique characteristics of random variables on spheres in high dimensions and prove that Wasserstein distance between two arbitrary data sets randomly drawn from a sphere are nearly identical when the dimension is sufficiently large. Based on our theory, a novel algorithm for distribution-robust sampling is devised. Moreover, we reform the latent space of VAE by constraining latent variables on the sphere, thus freeing VAE from the approximate optimization of posterior probability via variational inference. The new algorithm is named Spherical AutoEncoder (SAE). Extensive experiments by sampling and inference tasks validate our theoretical analysis and the superiority of SAE.", "keywords": ["variational autoencoder", "generative adversarial network"], "paperhash": "zhao|latent_variables_on_spheres_for_sampling_and_inference", "original_pdf": "/attachment/6e124d9b9bf80c513483e35a90a68f3860623f0a.pdf", "_bibtex": "@misc{\nzhao2020latent,\ntitle={Latent Variables on Spheres for Sampling and Inference},\nauthor={Deli Zhao and Jiapeng Zhu and Bo Zhang},\nyear={2020},\nurl={https://openreview.net/forum?id=rJx2slSKDS}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "rJx2slSKDS", "replyto": "rJx2slSKDS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795715792, "tmdate": 1576800265781, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2514/-/Decision"}}}, {"id": "Hketwk83jH", "original": null, "number": 7, "cdate": 1573834592654, "ddate": null, "tcdate": 1573834592654, "tmdate": 1573834592654, "tddate": null, "forum": "rJx2slSKDS", "replyto": "BJl_-qJEiH", "invitation": "ICLR.cc/2020/Conference/Paper2514/-/Official_Comment", "content": {"title": "Response to the rebuttal", "comment": "Thank you for your response.\n\nI went through the rebuttal and the revised version of the paper, and most of my original concerns remain unaddressed:\n\n- The positioning of the paper with respect to VAE, variational inference is confusing and even misleading.\n\n- Attributing generation issues in VAE to the fact that \u201cthe posterior q(z|x) is incapable of matching the prior distribution p(z) well\u201d is not correct. In VAE we are not expecting q(z|x) to match p(z) well as this would result in useless inference (q(z|x) ignoring x).  Please refer to my original comments for more details. In their answer A3, the authors mentioned about Wasserstein autoencoder (WAE). I would like to emphasize that in WAE the objective is to match the \u201caggregated\u201d posterior q(z) with the prior p(z), where  q(z) = \\int q(x)q(z|x)dx, with q(x) denoting the empirical data distribution.  Indeed, we want q(z), but not q(z|x), to perfectly match p(z). \n\n- Assumptions under which Theorem 2 holds should be stated clearly. The sampling at random assumption is not enough if z, z\u2019 are from empirical distributions on the sphere with overlapping supports. \n\n- While the result of Theorem 2 may justify \u201cdistribution robust-sampling\u201d, it is not clear how and why would this Theorem justify improvement in inference when projecting z onto a hypersphere. \n\n- The revised version includes qualitative results for VAE and the proposed SAE on a new dataset (CelebA), as well as for hyper-Spherical VAE on MNIST. However, quantitative experiments remain weak.\n\n- The writing has been revised making it better than the initial version, yet the paper is still hard to follow, and further improvements are necessary."}, "signatures": ["ICLR.cc/2020/Conference/Paper2514/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2514/AnonReviewer3", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zhaodeli@gmail.com", "jengzhu0@gmail.com", "zhangbo@xiaomi.com"], "title": "Latent Variables on Spheres for Sampling and Inference", "authors": ["Deli Zhao", "Jiapeng Zhu", "Bo Zhang"], "pdf": "/pdf/cf09471639922342c6ed3485a723244ecece8726.pdf", "abstract": "Variational inference is a fundamental problem in Variational AutoEncoder (VAE). The optimization with lower bound of marginal log-likelihood results in the distribution of latent variables approximate to a given prior probability, which is the dilemma of employing VAE to solve real-world problems. By virtue of high-dimensional geometry, we propose a very simple algorithm completely different from existing ones to alleviate the variational inference in VAE. We analyze the unique characteristics of random variables on spheres in high dimensions and prove that Wasserstein distance between two arbitrary data sets randomly drawn from a sphere are nearly identical when the dimension is sufficiently large. Based on our theory, a novel algorithm for distribution-robust sampling is devised. Moreover, we reform the latent space of VAE by constraining latent variables on the sphere, thus freeing VAE from the approximate optimization of posterior probability via variational inference. The new algorithm is named Spherical AutoEncoder (SAE). Extensive experiments by sampling and inference tasks validate our theoretical analysis and the superiority of SAE.", "keywords": ["variational autoencoder", "generative adversarial network"], "paperhash": "zhao|latent_variables_on_spheres_for_sampling_and_inference", "original_pdf": "/attachment/6e124d9b9bf80c513483e35a90a68f3860623f0a.pdf", "_bibtex": "@misc{\nzhao2020latent,\ntitle={Latent Variables on Spheres for Sampling and Inference},\nauthor={Deli Zhao and Jiapeng Zhu and Bo Zhang},\nyear={2020},\nurl={https://openreview.net/forum?id=rJx2slSKDS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJx2slSKDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2514/Authors", "ICLR.cc/2020/Conference/Paper2514/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2514/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2514/Reviewers", "ICLR.cc/2020/Conference/Paper2514/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2514/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2514/Authors|ICLR.cc/2020/Conference/Paper2514/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504140226, "tmdate": 1576860551097, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2514/Authors", "ICLR.cc/2020/Conference/Paper2514/Reviewers", "ICLR.cc/2020/Conference/Paper2514/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2514/-/Official_Comment"}}}, {"id": "BJl_-qJEiH", "original": null, "number": 4, "cdate": 1573284352289, "ddate": null, "tcdate": 1573284352289, "tmdate": 1573723823642, "tddate": null, "forum": "rJx2slSKDS", "replyto": "H1e_Hdw6tr", "invitation": "ICLR.cc/2020/Conference/Paper2514/-/Official_Comment", "content": {"title": "To Reviewer #3", "comment": "\nQ1: \u201cAn important claim in this paper is that the proposed approach \u201calleviates variational inference in VAE\u201d. However, this requires clarification as well as theoretical/empirical justifications\u201d\nQ2: \u201cMoreover, why and how would Theorem 2 justify improved inference when projecting latent samples onto a hypersphere?\u201d\nA1 and A2: These two questions and related comments might be due to our inappropriate use of \u201calleviate\u201d and the extensive meaning of inference beyond probability. Actually, there is no posterior inference and any priors involved in our SAE algorithm. It is the vanilla autoencoder subject to the spherical constraint shown in equation (10). So we said \u201cthus freeing VAE from the approximate optimization of posterior probability via variational inference\u201d and \u201cOur algorithm is geometric and free from posterior probability optimization\u201d. Indeed, \u201calleviates variational inference in VAE\u201d is an inappropriate use in this scenario. We will correct this in the revised version. \n\nBesides, we use \u201cinference\u201d to refer to inferring (obtaining) z from the encoder, not only for \u201cvariational\u201d inference or \u201cprobabilistic\u201d inference. This might cause misunderstanding with habitual thinking in this field. This misunderstanding might be avoided by using \u201cgeometric inference\u201d. We will note this meaning clearly in the revised version.\n\n\nQ3: \u201cHowever, in VAE we do not expect the posterior to match the prior perfectly, as this would result in useless data representations or inference.\u201d \nA3: We  understand your viewpoint about the model distribution and the prior distribution . \u201cmatch the prior perfectly\u201d does not mean the point-to-point correspondence. We refer to fitting distributions. The word \u201cmatch\u201d is also used in Wasserstein autoencoder (https://arxiv.org/abs/1711.01558), which is the same scenario to ours.\n\nQ4: \u201cTheorem 2 (on the convergence of the Wasserstein distance (W2) on high dimensional hyperspheres) does not seem to hold if, for instance, P and P\u2019 are empirical distributions with overlapping supports.\u201d\nQ5: \u201cFurther, even when the above Theorem holds, the W2 distance may be relatively high since it is proportional to the square root of the number of samples.\u201d\nA4: To make our theory much easier to understand, we directly gave the computational definition of Wasserstein distance in (8) and (9) rather than its original integral form. Thus, Theorem 2 is the direct result by substituting the conclusion of Lemma 1 into (8). It is very easy. About the correctness of Lemma 1, please refer to the elegant proof at http://faculty.madisoncollege.edu/alehnen/sphere/hypers.htm.\n\nMost theorems only hold under some conditions. Both Lemma 1 and Theorem 2 need a basic condition. The condition is that the points are drawn from spheres at RANDOM. To satisfy the condition, we use the operation of centerization in our SAE algorithm, which is motivated from central limit theorem in probability. \n\nIn fact, it is straightforward to design the case to deny Lemma 1 and Theorem 2 if we bypass the condition. For instance, let Z1 be the set sampled from the spherical part in the open positive orthant and Z2 sampled from the spherical part in the open negative orthant. The third set Z3 is derived from Z2 by the small perturbation. Both Lemma 1 and Theorem 2 do not hold for the dataset { Z1, Z2, Z3}. But such samping violates the randomness needed. For SAE, the centerization is used to prevent such cases. \n\nA5: \u201cthe W2 distance may be relatively high since it is proportional to the square root of the number of samples.\u201d is correct. However, it is logically wrong to use it to deny our theory, because all the W2 distances between two arbitrary random datasets still converge to be the same constant in Theorem 2 when the number of samples increases. The conclusion still holds in our paper.\n\nQ6: \u201cImprove experiments by including more datasets and baselines (e.g., hyperspherical VAE [1]), as well conduct more targeted experiments to give more insights regarding the effect of the L2 normalization on inference and generation. \u201d\nA6: We failed to get the convergent results of hyper-Spherical VAE (S-VAE) on FFHQ faces of size 128x128. So we did not compare it in the current version. We are now running it on MNIST. The results will be updated in the revised version within several days.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2514/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2514/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zhaodeli@gmail.com", "jengzhu0@gmail.com", "zhangbo@xiaomi.com"], "title": "Latent Variables on Spheres for Sampling and Inference", "authors": ["Deli Zhao", "Jiapeng Zhu", "Bo Zhang"], "pdf": "/pdf/cf09471639922342c6ed3485a723244ecece8726.pdf", "abstract": "Variational inference is a fundamental problem in Variational AutoEncoder (VAE). The optimization with lower bound of marginal log-likelihood results in the distribution of latent variables approximate to a given prior probability, which is the dilemma of employing VAE to solve real-world problems. By virtue of high-dimensional geometry, we propose a very simple algorithm completely different from existing ones to alleviate the variational inference in VAE. We analyze the unique characteristics of random variables on spheres in high dimensions and prove that Wasserstein distance between two arbitrary data sets randomly drawn from a sphere are nearly identical when the dimension is sufficiently large. Based on our theory, a novel algorithm for distribution-robust sampling is devised. Moreover, we reform the latent space of VAE by constraining latent variables on the sphere, thus freeing VAE from the approximate optimization of posterior probability via variational inference. The new algorithm is named Spherical AutoEncoder (SAE). Extensive experiments by sampling and inference tasks validate our theoretical analysis and the superiority of SAE.", "keywords": ["variational autoencoder", "generative adversarial network"], "paperhash": "zhao|latent_variables_on_spheres_for_sampling_and_inference", "original_pdf": "/attachment/6e124d9b9bf80c513483e35a90a68f3860623f0a.pdf", "_bibtex": "@misc{\nzhao2020latent,\ntitle={Latent Variables on Spheres for Sampling and Inference},\nauthor={Deli Zhao and Jiapeng Zhu and Bo Zhang},\nyear={2020},\nurl={https://openreview.net/forum?id=rJx2slSKDS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJx2slSKDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2514/Authors", "ICLR.cc/2020/Conference/Paper2514/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2514/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2514/Reviewers", "ICLR.cc/2020/Conference/Paper2514/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2514/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2514/Authors|ICLR.cc/2020/Conference/Paper2514/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504140226, "tmdate": 1576860551097, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2514/Authors", "ICLR.cc/2020/Conference/Paper2514/Reviewers", "ICLR.cc/2020/Conference/Paper2514/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2514/-/Official_Comment"}}}, {"id": "B1xPFFJ4jH", "original": null, "number": 3, "cdate": 1573284223276, "ddate": null, "tcdate": 1573284223276, "tmdate": 1573723581032, "tddate": null, "forum": "rJx2slSKDS", "replyto": "BJeMRX6atB", "invitation": "ICLR.cc/2020/Conference/Paper2514/-/Official_Comment", "content": {"title": "To Reviewer #2", "comment": "\nQ1: \u201cThen, to clarify the algorithm, it seems necessary to provide the formulation of objective functions.\u201d,  \u201cIs the objective still valid or reasonable even it is derived from the equation (10) without posterior inference?\u201d\nA1: The objective function will be provided in the revised version. It is the reconstruction loss || x - \\tilde{x} || subject to the spherical constraint on z (equation (10)). There are no posterior inference and no KL-divergence involved in our algorithm. It is very simple. \n\nQ2: \u201cHow does the objective change when centerization and spherization are applied to the GAN?\u201d\nA2:  There is no extra objective when applied to GANs. Only centerization and spherization are needed. \n\nQ3: \u201cCompared with using von Mises-Fisher distribution in the vanilla VAE, the advantage of the proposed method is not clear. To my understanding, the main difference seems to be whether using lower bound with posterior inference or deterministic framework without such approximation. However, there are no theoretical or empirical results to show the benefit of the proposed method.\u201d\nA3: This might be the misunderstanding caused by that we didn\u2019t explicitly write the objective function in the paper. We explain this in Q1. Our SAE algorithm is essentially different from S-VAE (hyper-Spherical VAE). The S-VAE is established on the principle of VAE. So, S-VAE has the drawbacks posed by VAE such as the approximation of posterior inference, the prior dependence, and the reparameterization trick for random variables. But SAE is distribution-agnostic with respect to Wasserstein distance, which is rigorously guaranteed by Theorem 2. \n\nActually, we failed to get the convergent results of S-VAE on FFHQ faces of size 128x128. We are now running it on MNIST. The results will be updated in the revised version within several days.\n\n\nQ4: \u201cCompare to ProGAN and StyleGAN, is the contribution of the paper to applying centerization to GAN and centerization and spherization to autoencoder?\u201d\nA4: Both GAN and autoencoder need to use centerization and spherization on random variables. For ProGAN and StyleGAN, the authors empirically applied spherization on z in their code, which motivated our work. We also made it clear in the context of equation (3).  \n\nQ4: \u201cWhat dimension do you use as latent dimension in the experiments?\u201d\nA4: We followed the experimental setting of StyleGAN. The 512-dimensional latent vectors are used for StyleGAN, VAE, and SAE on the face datasets including FFHQ and CelebA. For MNIST, we take the 10-dimensional latent codes. \n\nQ5: \u201cDoes the choice of prior distribution affect the experimental results? If so, is there any compatible reason with the intuition of SAE?\u201d\nA5: This question might be another misunderstanding caused by Q1. Actually, there are no any priors involved in SAE during training. We used different priors to test the robustness of SAE and VAE after training was completed. We will make it clear in the revised version.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2514/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2514/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zhaodeli@gmail.com", "jengzhu0@gmail.com", "zhangbo@xiaomi.com"], "title": "Latent Variables on Spheres for Sampling and Inference", "authors": ["Deli Zhao", "Jiapeng Zhu", "Bo Zhang"], "pdf": "/pdf/cf09471639922342c6ed3485a723244ecece8726.pdf", "abstract": "Variational inference is a fundamental problem in Variational AutoEncoder (VAE). The optimization with lower bound of marginal log-likelihood results in the distribution of latent variables approximate to a given prior probability, which is the dilemma of employing VAE to solve real-world problems. By virtue of high-dimensional geometry, we propose a very simple algorithm completely different from existing ones to alleviate the variational inference in VAE. We analyze the unique characteristics of random variables on spheres in high dimensions and prove that Wasserstein distance between two arbitrary data sets randomly drawn from a sphere are nearly identical when the dimension is sufficiently large. Based on our theory, a novel algorithm for distribution-robust sampling is devised. Moreover, we reform the latent space of VAE by constraining latent variables on the sphere, thus freeing VAE from the approximate optimization of posterior probability via variational inference. The new algorithm is named Spherical AutoEncoder (SAE). Extensive experiments by sampling and inference tasks validate our theoretical analysis and the superiority of SAE.", "keywords": ["variational autoencoder", "generative adversarial network"], "paperhash": "zhao|latent_variables_on_spheres_for_sampling_and_inference", "original_pdf": "/attachment/6e124d9b9bf80c513483e35a90a68f3860623f0a.pdf", "_bibtex": "@misc{\nzhao2020latent,\ntitle={Latent Variables on Spheres for Sampling and Inference},\nauthor={Deli Zhao and Jiapeng Zhu and Bo Zhang},\nyear={2020},\nurl={https://openreview.net/forum?id=rJx2slSKDS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJx2slSKDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2514/Authors", "ICLR.cc/2020/Conference/Paper2514/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2514/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2514/Reviewers", "ICLR.cc/2020/Conference/Paper2514/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2514/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2514/Authors|ICLR.cc/2020/Conference/Paper2514/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504140226, "tmdate": 1576860551097, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2514/Authors", "ICLR.cc/2020/Conference/Paper2514/Reviewers", "ICLR.cc/2020/Conference/Paper2514/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2514/-/Official_Comment"}}}, {"id": "Byek4o59or", "original": null, "number": 6, "cdate": 1573722919248, "ddate": null, "tcdate": 1573722919248, "tmdate": 1573723002286, "tddate": null, "forum": "rJx2slSKDS", "replyto": "rJx2slSKDS", "invitation": "ICLR.cc/2020/Conference/Paper2514/-/Official_Comment", "content": {"title": "About the revised version", "comment": "The revised version has been updated. We revised the submission from the following eight aspects according to Reviewers' advice.\n\n1. We explicitly wrote the objective function of SAE in equation (11). The reconstruction loss and the spherical constraint in equations (10) and (11) are all operations in SAE. There are no variational inference, no probabilistic optimization, and no priors involved in SAE during training.\n\n2. To make the meaning of the word \"inference\" clear, we named the inference in SAE as the spherical inference. As opposed to the variational inference, the spherical inference \u00a0is deterministic during training. But the decoder of SAE is rather robust to various priors for sampling after training. We made this expression clear in the paper to avoid misunderstanding with the variational inference.\n\n3. We added the discussion for Wasserstein autoencoder, adversarial autoencoder, and beta-VAE in the related work.\n\n4. We also compared VAE and SAE on CelebA. \u00a0We need to note that the quality of generated faces by VAE and SAE will be both improved if we use the face images of only cropping facial parts for the experiments. But such data are not sufficient to test the robustness of the algorithms against the variations of the entire facial features.\n\n5. We compared hyper-Spherical VAE (S-VAE) with SAE on MNIST using the official code at https://github.com/nicola-decao/s-vae-tf.\n\n6. We provided the visualization results of latent codes from VAE, S-VAE, and SAE on CelebA and MNIST. This visualization clearly shows the superiority of the spherical inference in SAE.\n\n7. We re-arranged images in Figure 5 to save more space for the new contents. The experimental results in this Figure are kept the same as the previous version.\n\n8. We corrected the typos, polished the writing, \u00a0and made the paper more readable.\n\nAll the complementary experimental results were attached in Appendix."}, "signatures": ["ICLR.cc/2020/Conference/Paper2514/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2514/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zhaodeli@gmail.com", "jengzhu0@gmail.com", "zhangbo@xiaomi.com"], "title": "Latent Variables on Spheres for Sampling and Inference", "authors": ["Deli Zhao", "Jiapeng Zhu", "Bo Zhang"], "pdf": "/pdf/cf09471639922342c6ed3485a723244ecece8726.pdf", "abstract": "Variational inference is a fundamental problem in Variational AutoEncoder (VAE). The optimization with lower bound of marginal log-likelihood results in the distribution of latent variables approximate to a given prior probability, which is the dilemma of employing VAE to solve real-world problems. By virtue of high-dimensional geometry, we propose a very simple algorithm completely different from existing ones to alleviate the variational inference in VAE. We analyze the unique characteristics of random variables on spheres in high dimensions and prove that Wasserstein distance between two arbitrary data sets randomly drawn from a sphere are nearly identical when the dimension is sufficiently large. Based on our theory, a novel algorithm for distribution-robust sampling is devised. Moreover, we reform the latent space of VAE by constraining latent variables on the sphere, thus freeing VAE from the approximate optimization of posterior probability via variational inference. The new algorithm is named Spherical AutoEncoder (SAE). Extensive experiments by sampling and inference tasks validate our theoretical analysis and the superiority of SAE.", "keywords": ["variational autoencoder", "generative adversarial network"], "paperhash": "zhao|latent_variables_on_spheres_for_sampling_and_inference", "original_pdf": "/attachment/6e124d9b9bf80c513483e35a90a68f3860623f0a.pdf", "_bibtex": "@misc{\nzhao2020latent,\ntitle={Latent Variables on Spheres for Sampling and Inference},\nauthor={Deli Zhao and Jiapeng Zhu and Bo Zhang},\nyear={2020},\nurl={https://openreview.net/forum?id=rJx2slSKDS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJx2slSKDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2514/Authors", "ICLR.cc/2020/Conference/Paper2514/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2514/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2514/Reviewers", "ICLR.cc/2020/Conference/Paper2514/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2514/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2514/Authors|ICLR.cc/2020/Conference/Paper2514/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504140226, "tmdate": 1576860551097, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2514/Authors", "ICLR.cc/2020/Conference/Paper2514/Reviewers", "ICLR.cc/2020/Conference/Paper2514/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2514/-/Official_Comment"}}}, {"id": "HylUhjk4iB", "original": null, "number": 5, "cdate": 1573284781579, "ddate": null, "tcdate": 1573284781579, "tmdate": 1573284781579, "tddate": null, "forum": "rJx2slSKDS", "replyto": "HJga0cRjYr", "invitation": "ICLR.cc/2020/Conference/Paper2514/-/Official_Comment", "content": {"title": "To Reviewer #1", "comment": "\nQ1: \u201cis this a variant of the Wasserstein auto-encoder?\u201d\nA1: Our SAE algorithm is not a variant of WAE proposed in the following paper.\n\nWasserstein Auto-Encoders\nhttps://arxiv.org/abs/1711.01558\n\nWAE minimized Wasserstein distance between the model distribution and the prior distribution. The algorithm reduces to adversarial learning via a discriminator in the latent space, which is similar to the following paper\n\nAdversarial Autoencoders\nhttps://arxiv.org/abs/1511.05644\n\nLike VAE, both Wasserstein autoencoder and adversarial autoencoder need a prior distribution to match. However, there is no loss imposed on the latent space to optimize for SAE. SAE does not need priors either. The object function is the reconstruction loss || x - \\tilde{x} || with the spherical  constraint shown in equation (10). It is much simpler than Wasserstein autoencoder.\n\nIn order to elucidate the unique property of random variables on spheres, we leveraged Wasserstein distance to derive Theorem 2. The Wasserstein distance here serves to establish a circumstance that the algorithm with the spherical constraint can be distribution-agnostic. We did not use Wasserstein distance for computation in SAE.  \n\nBoth Wasserstein autoencoder and adversarial autoencoder are very interesting and inspiring algorithms. We like these two works very much.\n\n\nQ2: \u201cthe image quality of VAE (CelebA) is not that bad in other VAE papers, maybe tuning the \\beta-VAE can also achieve the same quantitative and qualitative results.\u201d\nQ3: \u201cCan you visualize the latent space (z) for the CelebA dataset, also comparing with the results from VAE?\u201d\nA2: For data factors, the image quality of VAE depends on the image size and the image diversity. For face images, the large image size and more backgrounds in the image will make the data difficult to fit. We used the more challenging data of FFHQ available at  https://github.com/NVlabs/stylegan and the image size we used is 128x128. \n\nA3: We are conducting the experiment on CelebA of size 64x64 according to your advice. We will update the results when this complementary experiment is completed.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2514/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2514/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zhaodeli@gmail.com", "jengzhu0@gmail.com", "zhangbo@xiaomi.com"], "title": "Latent Variables on Spheres for Sampling and Inference", "authors": ["Deli Zhao", "Jiapeng Zhu", "Bo Zhang"], "pdf": "/pdf/cf09471639922342c6ed3485a723244ecece8726.pdf", "abstract": "Variational inference is a fundamental problem in Variational AutoEncoder (VAE). The optimization with lower bound of marginal log-likelihood results in the distribution of latent variables approximate to a given prior probability, which is the dilemma of employing VAE to solve real-world problems. By virtue of high-dimensional geometry, we propose a very simple algorithm completely different from existing ones to alleviate the variational inference in VAE. We analyze the unique characteristics of random variables on spheres in high dimensions and prove that Wasserstein distance between two arbitrary data sets randomly drawn from a sphere are nearly identical when the dimension is sufficiently large. Based on our theory, a novel algorithm for distribution-robust sampling is devised. Moreover, we reform the latent space of VAE by constraining latent variables on the sphere, thus freeing VAE from the approximate optimization of posterior probability via variational inference. The new algorithm is named Spherical AutoEncoder (SAE). Extensive experiments by sampling and inference tasks validate our theoretical analysis and the superiority of SAE.", "keywords": ["variational autoencoder", "generative adversarial network"], "paperhash": "zhao|latent_variables_on_spheres_for_sampling_and_inference", "original_pdf": "/attachment/6e124d9b9bf80c513483e35a90a68f3860623f0a.pdf", "_bibtex": "@misc{\nzhao2020latent,\ntitle={Latent Variables on Spheres for Sampling and Inference},\nauthor={Deli Zhao and Jiapeng Zhu and Bo Zhang},\nyear={2020},\nurl={https://openreview.net/forum?id=rJx2slSKDS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJx2slSKDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2514/Authors", "ICLR.cc/2020/Conference/Paper2514/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2514/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2514/Reviewers", "ICLR.cc/2020/Conference/Paper2514/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2514/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2514/Authors|ICLR.cc/2020/Conference/Paper2514/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504140226, "tmdate": 1576860551097, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2514/Authors", "ICLR.cc/2020/Conference/Paper2514/Reviewers", "ICLR.cc/2020/Conference/Paper2514/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2514/-/Official_Comment"}}}, {"id": "HJga0cRjYr", "original": null, "number": 1, "cdate": 1571707605009, "ddate": null, "tcdate": 1571707605009, "tmdate": 1572972328666, "tddate": null, "forum": "rJx2slSKDS", "replyto": "rJx2slSKDS", "invitation": "ICLR.cc/2020/Conference/Paper2514/-/Official_Review", "content": {"rating": "6: Weak Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper proposed an interesting idea that by regularizing the structure of the latent space into a sphere, we can free VAE from variantional inference framework.\n\nHowever, here are several concerns about this paper:\n1. is this a variant of the Wasserstein auto-encoder?\n2. the image quality of VAE (CelebA) is not that bad in other VAE papers, maybe tuning the \\beta-VAE can also achieve the same quantitative and qualitative results.\n3. Can you visualize the latent space (z) for the CelebA dataset, also comparing with the results from VAE?"}, "signatures": ["ICLR.cc/2020/Conference/Paper2514/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2514/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zhaodeli@gmail.com", "jengzhu0@gmail.com", "zhangbo@xiaomi.com"], "title": "Latent Variables on Spheres for Sampling and Inference", "authors": ["Deli Zhao", "Jiapeng Zhu", "Bo Zhang"], "pdf": "/pdf/cf09471639922342c6ed3485a723244ecece8726.pdf", "abstract": "Variational inference is a fundamental problem in Variational AutoEncoder (VAE). The optimization with lower bound of marginal log-likelihood results in the distribution of latent variables approximate to a given prior probability, which is the dilemma of employing VAE to solve real-world problems. By virtue of high-dimensional geometry, we propose a very simple algorithm completely different from existing ones to alleviate the variational inference in VAE. We analyze the unique characteristics of random variables on spheres in high dimensions and prove that Wasserstein distance between two arbitrary data sets randomly drawn from a sphere are nearly identical when the dimension is sufficiently large. Based on our theory, a novel algorithm for distribution-robust sampling is devised. Moreover, we reform the latent space of VAE by constraining latent variables on the sphere, thus freeing VAE from the approximate optimization of posterior probability via variational inference. The new algorithm is named Spherical AutoEncoder (SAE). Extensive experiments by sampling and inference tasks validate our theoretical analysis and the superiority of SAE.", "keywords": ["variational autoencoder", "generative adversarial network"], "paperhash": "zhao|latent_variables_on_spheres_for_sampling_and_inference", "original_pdf": "/attachment/6e124d9b9bf80c513483e35a90a68f3860623f0a.pdf", "_bibtex": "@misc{\nzhao2020latent,\ntitle={Latent Variables on Spheres for Sampling and Inference},\nauthor={Deli Zhao and Jiapeng Zhu and Bo Zhang},\nyear={2020},\nurl={https://openreview.net/forum?id=rJx2slSKDS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rJx2slSKDS", "replyto": "rJx2slSKDS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2514/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2514/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575484772019, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2514/Reviewers"], "noninvitees": [], "tcdate": 1570237721777, "tmdate": 1575484772033, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2514/-/Official_Review"}}}, {"id": "H1e_Hdw6tr", "original": null, "number": 2, "cdate": 1571809344264, "ddate": null, "tcdate": 1571809344264, "tmdate": 1572972328627, "tddate": null, "forum": "rJx2slSKDS", "replyto": "rJx2slSKDS", "invitation": "ICLR.cc/2020/Conference/Paper2514/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary\n\nThis paper considers the L2 normalization of samples \u201cz\u201d from a given prior p(z) in Generative Adversarial Netowks (GAN) and autoencoders. The L2 normalization corresponds to projecting samples onto the surface of a unit-hypersphere. Hence, to attempt to justify this normalization, the authors rely on some already established results regarding high dimensional hyperspheres. In particular, the focus is on the fact that, the Euclidean distance between any given point on a hypersphere and another randomly sampled point on the hypersphere tends to a constant, when the number of dimensions goes to infinity. This result is then used to show that the Wasserstein distance between two arbitrary distributions on a hypersphere converges to a constant when the number of dimensions grows. Based on this result, the authors claim that projecting the latent samples onto the surface of a hypersphere would make GAN less sensitive to the choice of the prior distribution. Moreover, they claim that such normalization would also benefits inference, and that it addresses the issue of variational inference in VAE.\n\nMain comments. \n\nThis paper is hard to follow and requires substantial improvements in terms of writing, owing to several grammatical and semantic issues. Moreover, there is a lack rigor; some important claims are supported neither by experiments nor by theoretical analysis. Experiments in the main paper are also weak. I can therefore not recommend acceptance. My detailed comments are below.\n\n- An important claim in this paper is that the proposed approach \u201calleviates variational inference in VAE\u201d. However, this requires clarification as well as theoretical/empirical justifications.\n- In the introduction, it is stated that generated samples from VAE may deviate from real data samples, because \u201cthe posterior q(z|x) cannot match  the prior p(z) perfectly\u201d. However, in VAE we do not expect the posterior to match the prior perfectly, as this would result in useless data representations or inference. Generation issues in VAE may rather be explained by the fact that, in this context we optimize a lower bound on the KL-divergence between the empirical data distribution and the model distribution. The latter objective does not penalize the model distribution if it puts some of its mass in regions where the empirical data distribution is very low or even zero.\n- Theorem 2 (on the convergence of the Wasserstein distance (W2) on high dimensional hyperspheres) does not seem to hold if, for instance, P and P\u2019 are empirical distributions with overlapping supports. Further, even when the above Theorem holds, the W2 distance may be relatively high since it is proportional to the square root of the number of samples.\n- Moreover, why and how would Theorem 2 justify improved inference when projecting latent samples onto a hypersphere?\n- Please consider revising the following statement in the introduction: \u201cThe encoder f in VAE approximates the posterior q(z|x)\u201d. The encoder \u201cf\u201d in VAE parametrizes the variational posterior.\n- Some typos,\n\t- Abstract, \u201c\u2026 by sampling and inference tasks\u201d  -- \u201con sampling \u2026\u201d\n\t- Introduction second paragraph after eq 2. \u201c\u2026 it also causes the new problems\u201d \u2013 \u201c \u2026 causes new problems\u201d\n\t- Section 2.1, \u201cFor convenient analysis \u2026\u201d \u2013 \u201cFor a convenient \u2026\u201d \n\t- Second paragraph after Theorem 1. \u201c\u2026 perform probabilistic optimizations \u2026 \u201d \u2013 \u201c\u2026 optimization \u2026\u201d \n\t- Section 5.2, second paragraph. Is it Figure 9?\n\nThe main recommendations I would make are as follows.\n- Consider revising the paper to improve its writing.\n- Provide rigorous theoretical analysis and discussions to support the main claims. \n- Improve experiments by including more datasets and baselines (e.g., hyperspherical VAE [1]), as well conduct more targeted experiments to give more insights regarding the effect of the L2 normalization on inference and generation. \n\n[1] Davidson, Tim R., et al. \"Hyperspherical variational auto-encoders.\" UAI, 2018.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2514/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2514/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zhaodeli@gmail.com", "jengzhu0@gmail.com", "zhangbo@xiaomi.com"], "title": "Latent Variables on Spheres for Sampling and Inference", "authors": ["Deli Zhao", "Jiapeng Zhu", "Bo Zhang"], "pdf": "/pdf/cf09471639922342c6ed3485a723244ecece8726.pdf", "abstract": "Variational inference is a fundamental problem in Variational AutoEncoder (VAE). The optimization with lower bound of marginal log-likelihood results in the distribution of latent variables approximate to a given prior probability, which is the dilemma of employing VAE to solve real-world problems. By virtue of high-dimensional geometry, we propose a very simple algorithm completely different from existing ones to alleviate the variational inference in VAE. We analyze the unique characteristics of random variables on spheres in high dimensions and prove that Wasserstein distance between two arbitrary data sets randomly drawn from a sphere are nearly identical when the dimension is sufficiently large. Based on our theory, a novel algorithm for distribution-robust sampling is devised. Moreover, we reform the latent space of VAE by constraining latent variables on the sphere, thus freeing VAE from the approximate optimization of posterior probability via variational inference. The new algorithm is named Spherical AutoEncoder (SAE). Extensive experiments by sampling and inference tasks validate our theoretical analysis and the superiority of SAE.", "keywords": ["variational autoencoder", "generative adversarial network"], "paperhash": "zhao|latent_variables_on_spheres_for_sampling_and_inference", "original_pdf": "/attachment/6e124d9b9bf80c513483e35a90a68f3860623f0a.pdf", "_bibtex": "@misc{\nzhao2020latent,\ntitle={Latent Variables on Spheres for Sampling and Inference},\nauthor={Deli Zhao and Jiapeng Zhu and Bo Zhang},\nyear={2020},\nurl={https://openreview.net/forum?id=rJx2slSKDS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rJx2slSKDS", "replyto": "rJx2slSKDS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2514/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2514/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575484772019, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2514/Reviewers"], "noninvitees": [], "tcdate": 1570237721777, "tmdate": 1575484772033, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2514/-/Official_Review"}}}, {"id": "BJeMRX6atB", "original": null, "number": 3, "cdate": 1571832778138, "ddate": null, "tcdate": 1571832778138, "tmdate": 1572972328562, "tddate": null, "forum": "rJx2slSKDS", "replyto": "rJx2slSKDS", "invitation": "ICLR.cc/2020/Conference/Paper2514/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a novel autoencoder algorithm, named Spherical AutoEncoder (SAE). In this paper, the authors argue that the sphere structure has good properties in high-dimensional. To leverage the properties, proposed algorithm centerizes latent variables and projects them onto unit sphere. To show the empirical performance of the proposed approach, the authors perform image reconstruction and generation using FFHQ dataset and MNIST dataset.\n\nComments:\nI think the proposed approach, using spherical latent space, is interesting and make sense.\n\n- As mentioned in section 3.2, the proposed algorithm is reduced to standard autoencoder since it is free from posterior inference. Then, to clarify the algorithm, it seems necessary to provide the formulation of objective functions.\n- Is the objective still valid or reasonable even it is derived from the equation (10) without posterior inference?\n- How does the objective change when centerization and spherization are applied to the GAN?\n- Compared with using von Mises-Fisher distribution in the vanilla VAE, the advantage of the proposed method is not clear. To my understanding, the main difference seems to be whether using lower bound with posterior inference or deterministic framework without such approximation. However, there are no theoretical or empirical results to show the benefit of the proposed method. If theoretical or empirical results with reasonable intuition is provided, it will make the proposed algorithm more valuable.\n\nQuestions:\n- Compare to ProGAN and StyleGAN, is the contribution of the paper to applying centerization to GAN and centerization and spherization to autoencoder?\n- What dimension do you use as latent dimension in the experiments?\n- Does the choice of prior distribution affect the experimental results? If so, is there any compatible reason with the intuition of SAE?\n\nTypo:\nUnder equation (10) in page 5: \\tilde{z} should be \\hat{z}.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2514/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2514/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zhaodeli@gmail.com", "jengzhu0@gmail.com", "zhangbo@xiaomi.com"], "title": "Latent Variables on Spheres for Sampling and Inference", "authors": ["Deli Zhao", "Jiapeng Zhu", "Bo Zhang"], "pdf": "/pdf/cf09471639922342c6ed3485a723244ecece8726.pdf", "abstract": "Variational inference is a fundamental problem in Variational AutoEncoder (VAE). The optimization with lower bound of marginal log-likelihood results in the distribution of latent variables approximate to a given prior probability, which is the dilemma of employing VAE to solve real-world problems. By virtue of high-dimensional geometry, we propose a very simple algorithm completely different from existing ones to alleviate the variational inference in VAE. We analyze the unique characteristics of random variables on spheres in high dimensions and prove that Wasserstein distance between two arbitrary data sets randomly drawn from a sphere are nearly identical when the dimension is sufficiently large. Based on our theory, a novel algorithm for distribution-robust sampling is devised. Moreover, we reform the latent space of VAE by constraining latent variables on the sphere, thus freeing VAE from the approximate optimization of posterior probability via variational inference. The new algorithm is named Spherical AutoEncoder (SAE). Extensive experiments by sampling and inference tasks validate our theoretical analysis and the superiority of SAE.", "keywords": ["variational autoencoder", "generative adversarial network"], "paperhash": "zhao|latent_variables_on_spheres_for_sampling_and_inference", "original_pdf": "/attachment/6e124d9b9bf80c513483e35a90a68f3860623f0a.pdf", "_bibtex": "@misc{\nzhao2020latent,\ntitle={Latent Variables on Spheres for Sampling and Inference},\nauthor={Deli Zhao and Jiapeng Zhu and Bo Zhang},\nyear={2020},\nurl={https://openreview.net/forum?id=rJx2slSKDS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rJx2slSKDS", "replyto": "rJx2slSKDS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2514/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2514/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575484772019, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2514/Reviewers"], "noninvitees": [], "tcdate": 1570237721777, "tmdate": 1575484772033, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2514/-/Official_Review"}}}, {"id": "r1lf20oUFB", "original": null, "number": 2, "cdate": 1571368618368, "ddate": null, "tcdate": 1571368618368, "tmdate": 1571368618368, "tddate": null, "forum": "rJx2slSKDS", "replyto": "S1gC86mHYr", "invitation": "ICLR.cc/2020/Conference/Paper2514/-/Official_Comment", "content": {"comment": "Thanks for your very insightful comments, Alex.\n\n1) About covering\n\nThe case you raised is really challenging. We choose the vector centerization to afford randomness on the sphere. But how well this operation enforces z_i to cover the spherical surface as uniformly as possible is an important topic to study for spherical autoencoder (SAE).\n\nThe vector centerization presented in our paper does have flaw. For example, the points on the spherical surface falling into the open positive orthant (z_i > 0) cannot be sampled. Considering that there are all 2^512 orthants in R^512, however, the open positive orthant only takes 1/2^512 part of the whole sphere. So, the negative effect is nearly trivial.\n\nWe also figured out another way of sampling on the sphere to circumvent this problem. For any {z_1,...,z_i,...,z_n} drawn from arbitrary distributions, we can first project them on the sphere by z_i <-- z_i/norm(z_i). The projected points probably lie on some specific regions on the sphere.  Then we can randomly rotate these points on the sphere by a series of orthogonal matrices that are obtained by orthogonalizing random matrices via Gram\u2013Schmidt process. In this way, we can get {z_1,...,z_i,...,z_n} that distributes randomly on the sphere as long as the rotation manipulations are sufficient.\nHowever, this method is not friendly to end-to-end learning for autoencoder. We do not use it in this paper.\n\nThe vector centerization and spherization is the simplest way we get to realize our idea, even though it is not perfect. What is most important is that it is very easy to use in the end-to-end architecture of autoencoder.\n\n2) About inductive bias\n\nTheorem 2 tells that SAE is distribution-agnostic with respect to Wasserstein distance. In other words, it has distributional inductive bias. However, it is very inspiring about your conjecture \"Perhaps the inductive bias of the neural network makes this type of issue unlikely\".\n\nActually, your conjecture leads to the connection between random variables on the sphere and universal approximation theorem (UAT).  We also think that this is an alternative way of further exposing the deep reason why the simple spherical constraint can outperform the traditional variational inference. You may refer to the following paper, if interested.\n\nSpherical approximate identity neural networks are universal approximators\n Zarita Zainuddin, Saeed Panahian Fard\nICNC, 2014\n\nYour thoughts are quite inspiring. We will consider the topics you raised seriously for our future work.", "title": "about covering and inductive bias"}, "signatures": ["ICLR.cc/2020/Conference/Paper2514/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2514/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zhaodeli@gmail.com", "jengzhu0@gmail.com", "zhangbo@xiaomi.com"], "title": "Latent Variables on Spheres for Sampling and Inference", "authors": ["Deli Zhao", "Jiapeng Zhu", "Bo Zhang"], "pdf": "/pdf/cf09471639922342c6ed3485a723244ecece8726.pdf", "abstract": "Variational inference is a fundamental problem in Variational AutoEncoder (VAE). The optimization with lower bound of marginal log-likelihood results in the distribution of latent variables approximate to a given prior probability, which is the dilemma of employing VAE to solve real-world problems. By virtue of high-dimensional geometry, we propose a very simple algorithm completely different from existing ones to alleviate the variational inference in VAE. We analyze the unique characteristics of random variables on spheres in high dimensions and prove that Wasserstein distance between two arbitrary data sets randomly drawn from a sphere are nearly identical when the dimension is sufficiently large. Based on our theory, a novel algorithm for distribution-robust sampling is devised. Moreover, we reform the latent space of VAE by constraining latent variables on the sphere, thus freeing VAE from the approximate optimization of posterior probability via variational inference. The new algorithm is named Spherical AutoEncoder (SAE). Extensive experiments by sampling and inference tasks validate our theoretical analysis and the superiority of SAE.", "keywords": ["variational autoencoder", "generative adversarial network"], "paperhash": "zhao|latent_variables_on_spheres_for_sampling_and_inference", "original_pdf": "/attachment/6e124d9b9bf80c513483e35a90a68f3860623f0a.pdf", "_bibtex": "@misc{\nzhao2020latent,\ntitle={Latent Variables on Spheres for Sampling and Inference},\nauthor={Deli Zhao and Jiapeng Zhu and Bo Zhang},\nyear={2020},\nurl={https://openreview.net/forum?id=rJx2slSKDS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJx2slSKDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2514/Authors", "ICLR.cc/2020/Conference/Paper2514/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2514/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2514/Reviewers", "ICLR.cc/2020/Conference/Paper2514/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2514/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2514/Authors|ICLR.cc/2020/Conference/Paper2514/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504140226, "tmdate": 1576860551097, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2514/Authors", "ICLR.cc/2020/Conference/Paper2514/Reviewers", "ICLR.cc/2020/Conference/Paper2514/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2514/-/Official_Comment"}}}, {"id": "S1gC86mHYr", "original": null, "number": 1, "cdate": 1571269973875, "ddate": null, "tcdate": 1571269973875, "tmdate": 1571269973875, "tddate": null, "forum": "rJx2slSKDS", "replyto": "rJx2slSKDS", "invitation": "ICLR.cc/2020/Conference/Paper2514/-/Public_Comment", "content": {"comment": "I think this is an interesting paper and I was quite impressed by the results and elegance of the approach.  I have some thoughts about it from a conceptual point of view though.  \n\n1.  I'm curious if the SAE loss going to zero guarantees good samples in a theoretical sense.  I'm not sure if this is the case because during training the z's are always projected onto the sphere, but there is no requirement that they cover all of the points on the sphere.  So I could imagine a way of packing all of the z's seen during training into a small region on part of the sphere, having these points decode well, and then having all of the other regions decode to bad points.  \n\nPerhaps the inductive bias of the neural network makes this type of issue unlikely - in either case it makes it interesting that it seems to work so well.  (if it's the case it reminds me a bit of the cyclegan, where there is technically a way for the model to do something bad, but it doesn't happen as a result of the inductive bias from the architecture).  \n\nI think I have a particular construction that you might find interesting.  Let\u2019s say that each real data point is binary, for example x in N^784 (as with binary MNIST).  I can encode this digit in a single number xb by laying out the digits: 0.0011010\u20261 (with each decimal point corresponding to that pixel position).  \n\nNow let c = sqrt((1 - 2*xb^2) / 2)\n\nThen let\u2019s say z = [xb, -xb, c, -c].  \n\nRegardless of xb, so long as it is between (-sqrt(0.5), sqrt(0.5)), this z will be centered and on the sphere.  I realize that this is extremely unlikely to be learnable by a NN, especially due to smoothness and its inductive biases.  However I still think it's at least interesting to think about.  \n\n2.  I think one reason SAE might work so well in practice is due to the asymmetry in the KL-divergence in the VAE objective, where you have KL(q(z|x) || p(z)).  It becomes unbounded and large if q(z|x) ever has support but p(z) doesn't have support.  By pushing q(z | x) onto the sphere, and because samples from p(z) are essentially always on the sphere, you guarantee that the KL is at least bounded.  In practice this might be enough to make KL(q(z|x)||p(z)) sufficiently small, especially because the SAE doesn\u2019t have any incentive to concentrate the encoded points in particular parts of z-space, even though it hypothetically could.  ", "title": "Some questions related to theory for spherical autoencoder"}, "signatures": ["~Alex_Matthew_Lamb1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Alex_Matthew_Lamb1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zhaodeli@gmail.com", "jengzhu0@gmail.com", "zhangbo@xiaomi.com"], "title": "Latent Variables on Spheres for Sampling and Inference", "authors": ["Deli Zhao", "Jiapeng Zhu", "Bo Zhang"], "pdf": "/pdf/cf09471639922342c6ed3485a723244ecece8726.pdf", "abstract": "Variational inference is a fundamental problem in Variational AutoEncoder (VAE). The optimization with lower bound of marginal log-likelihood results in the distribution of latent variables approximate to a given prior probability, which is the dilemma of employing VAE to solve real-world problems. By virtue of high-dimensional geometry, we propose a very simple algorithm completely different from existing ones to alleviate the variational inference in VAE. We analyze the unique characteristics of random variables on spheres in high dimensions and prove that Wasserstein distance between two arbitrary data sets randomly drawn from a sphere are nearly identical when the dimension is sufficiently large. Based on our theory, a novel algorithm for distribution-robust sampling is devised. Moreover, we reform the latent space of VAE by constraining latent variables on the sphere, thus freeing VAE from the approximate optimization of posterior probability via variational inference. The new algorithm is named Spherical AutoEncoder (SAE). Extensive experiments by sampling and inference tasks validate our theoretical analysis and the superiority of SAE.", "keywords": ["variational autoencoder", "generative adversarial network"], "paperhash": "zhao|latent_variables_on_spheres_for_sampling_and_inference", "original_pdf": "/attachment/6e124d9b9bf80c513483e35a90a68f3860623f0a.pdf", "_bibtex": "@misc{\nzhao2020latent,\ntitle={Latent Variables on Spheres for Sampling and Inference},\nauthor={Deli Zhao and Jiapeng Zhu and Bo Zhang},\nyear={2020},\nurl={https://openreview.net/forum?id=rJx2slSKDS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJx2slSKDS", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504179201, "tmdate": 1576860584355, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper2514/Authors", "ICLR.cc/2020/Conference/Paper2514/Reviewers", "ICLR.cc/2020/Conference/Paper2514/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2514/-/Public_Comment"}}}], "count": 12}