{"notes": [{"id": "B1GHJ3R9tQ", "original": "rke2P-p5Fm", "number": 984, "cdate": 1538087901277, "ddate": null, "tcdate": 1538087901277, "tmdate": 1545355391180, "tddate": null, "forum": "B1GHJ3R9tQ", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "HyperGAN:  Exploring the Manifold of Neural Networks", "abstract": "We introduce HyperGAN, a generative network that learns to generate all the weight parameters of deep neural networks. HyperGAN first transforms low dimensional noise into a latent space, which can be sampled from to obtain diverse, performant sets of parameters for a target architecture. We utilize an architecture that bears resemblance to generative adversarial networks, but we evaluate the likelihood of samples with a classification loss. This is equivalent to minimizing the KL-divergence between the generated network parameter distribution and an unknown true parameter distribution. We apply HyperGAN to classification, showing that HyperGAN can learn to generate parameters which solve the MNIST and CIFAR-10 datasets with competitive performance to fully supervised learning while learning a rich distribution of effective parameters. We also show that HyperGAN can also provide better uncertainty than standard ensembles. This is evaluated by the ability of HyperGAN-generated ensembles to detect out of distribution data as well as adversarial examples. We see that in addition to being highly accurate on inlier data, HyperGAN can provide reasonable uncertainty estimates. ", "keywords": ["hypernetworks", "generative adversarial networks", "anomaly detection"], "authorids": ["ratzlafn@oregonstate.edu", "lif@oregonstate.edu"], "authors": ["Neale Ratzlaff", "Li  Fuxin"], "TL;DR": "We use a GAN to generate parameters of a neural network in one forward pass.", "pdf": "/pdf/bc0e4ee7bd22131225d76785340113d7baa00692.pdf", "paperhash": "ratzlaff|hypergan_exploring_the_manifold_of_neural_networks", "_bibtex": "@misc{\nratzlaff2019hypergan,\ntitle={Hyper{GAN}:  Exploring the Manifold of Neural Networks},\nauthor={Neale Ratzlaff and Li  Fuxin},\nyear={2019},\nurl={https://openreview.net/forum?id=B1GHJ3R9tQ},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "HygoKO3elE", "original": null, "number": 1, "cdate": 1544763522828, "ddate": null, "tcdate": 1544763522828, "tmdate": 1545354519774, "tddate": null, "forum": "B1GHJ3R9tQ", "replyto": "B1GHJ3R9tQ", "invitation": "ICLR.cc/2019/Conference/-/Paper984/Meta_Review", "content": {"metareview": "All of the reviewers find this paper to contain interesting ideas. Originally, clarity was a major issue, although a few issues remain (see the comments of reviewer 3). The reviewers believe that the paper has been substantially improved from its original form, however there is still room for improvement: more comprehensive comparisons to existing work (reviewer 1), careful ablations (reviewer 3), etc. With a little bit of polish, this paper is likely to be accepted at another venue.\n\nI am certainly not penalizing you for anonymously sharing your code on Github, as this was specifically requested by reviewer 1.\n", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Reject", "title": "Interesting work that requires a bit of fine tuning."}, "signatures": ["ICLR.cc/2019/Conference/Paper984/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper984/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "HyperGAN:  Exploring the Manifold of Neural Networks", "abstract": "We introduce HyperGAN, a generative network that learns to generate all the weight parameters of deep neural networks. HyperGAN first transforms low dimensional noise into a latent space, which can be sampled from to obtain diverse, performant sets of parameters for a target architecture. We utilize an architecture that bears resemblance to generative adversarial networks, but we evaluate the likelihood of samples with a classification loss. This is equivalent to minimizing the KL-divergence between the generated network parameter distribution and an unknown true parameter distribution. We apply HyperGAN to classification, showing that HyperGAN can learn to generate parameters which solve the MNIST and CIFAR-10 datasets with competitive performance to fully supervised learning while learning a rich distribution of effective parameters. We also show that HyperGAN can also provide better uncertainty than standard ensembles. This is evaluated by the ability of HyperGAN-generated ensembles to detect out of distribution data as well as adversarial examples. We see that in addition to being highly accurate on inlier data, HyperGAN can provide reasonable uncertainty estimates. ", "keywords": ["hypernetworks", "generative adversarial networks", "anomaly detection"], "authorids": ["ratzlafn@oregonstate.edu", "lif@oregonstate.edu"], "authors": ["Neale Ratzlaff", "Li  Fuxin"], "TL;DR": "We use a GAN to generate parameters of a neural network in one forward pass.", "pdf": "/pdf/bc0e4ee7bd22131225d76785340113d7baa00692.pdf", "paperhash": "ratzlaff|hypergan_exploring_the_manifold_of_neural_networks", "_bibtex": "@misc{\nratzlaff2019hypergan,\ntitle={Hyper{GAN}:  Exploring the Manifold of Neural Networks},\nauthor={Neale Ratzlaff and Li  Fuxin},\nyear={2019},\nurl={https://openreview.net/forum?id=B1GHJ3R9tQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper984/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353011144, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1GHJ3R9tQ", "replyto": "B1GHJ3R9tQ", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper984/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper984/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper984/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353011144}}}, {"id": "SJee6VaBhQ", "original": null, "number": 2, "cdate": 1540900023822, "ddate": null, "tcdate": 1540900023822, "tmdate": 1543870720258, "tddate": null, "forum": "B1GHJ3R9tQ", "replyto": "B1GHJ3R9tQ", "invitation": "ICLR.cc/2019/Conference/-/Paper984/Official_Review", "content": {"title": "Good experimental results but lacking rigour", "review": "This works propose a new approach to learn to sample (or generate) the parameters of a deep neural networks to solve a task. They propose a new architecture inspired by hyper networks and adversarial auto-encoders, where the parameters of the networks are generated from a low dimensional latent space. By using an ensemble of networks sampled with their approach they're able to get state of the art results on uncertainty estimation.\n\nThe notations are confusing and the paper contains several mistakes. In particular:\n- P_z is used to represent different distributions. It sometimes refers to the distribution of the latent variables and sometimes to the prior over the weight embeddings. Different notation should be used to represent different quantity.\n- D_z sometimes refers to the regularization term or to the discriminator.\n- Eq 2. I believe there is a bug in the equation, the expectation is over Q(z) but it should be P_z (distribution of the latent variable z), otherwise it doesn't make much sense.\n- The equation for the cross entropy is wrong. If y_i are the true labels and F(x_i, theta) is the prediction then it should be y_i*log(F(x_i, theta)).\n- It's not clear if the loss of the discriminator should be maximized for the parameters of the discriminator and minimized with respect to the parameters of the encoder. Furthermore it would be interesting to study what is the impact of this particular choice of loss for the discriminator. In particular I invite the author to compare the loss proposed to the loss in [1].\nFixing these, would make the paper much easier to understand.\n\nThe authors motivates their approach by drawing a link with wasserstein (WAE) and adversarial auto-encoders. While this could be interesting I think this link should be made more formal. \nIndeed, the WAE is derived from the wasserstein distance between the true data distribution and the distribution of the model. However it's not clear if the approach proposed can still be derived from such a principle. I would invite the author to make the link between wasserstein distance minimization and their approach more explicit.\n\nTo my knowledge the method proposed is novel, however using implicit posterior to learn the weights is not novel and several other works have looked at it. In particular I think [1,2] should be discussed in the related work. \nThe difference with traditional bayesian approach such as variational inference should also be discussed, since the approach is really close to approximating the posterior with an implicit distribution and computing the KL term using a GAN (like in [3,4]).\n\nI think one interesting novelty that needs to be emphasized is that the model has both: parameters that are point estimates (the parameters of the generators) and parameters that are sampled from a posterior distribution (the weight embeddings). \n\nPros:\n- Good and promising experimental results.\n\nCons:\n- The paper combines several tricks and ideas but it's not really clear what is important and why such an approach works. For example how important is the latent space and the encoder ? Could we just sample directly the weight embeddings from a gaussian and remove the regularization ?\n- The other points mentioned above about the clarity of the paper.\n\nOthers:\n- The title is misleading, the manifold is not really explored... If the author really want to explore the manifold some interesting questions are:  what happens if we try to interpolate between two latent variables ? What do the latent variables represent ? what's the influence of the dimension of the latent space ?\n- In the experiments: what is the number of networks used for the other methods ?\n- It would be nice to have a plot showing the accuracy as a function of the perturbation in section 4.5.\n\nConclusion:\nThe experimental results seem promising however the motivation for the approach is not clear. I think fixing some of the points mentioned above could greatly improve the clarity of the paper and make it a stronger submission. In the current state I don't believe the paper is rigorous enough to be accepted.\n\nReferences:\n[1] Pawlowski, N., Rajchl, M., & Glocker, B. (2017). Implicit weight uncertainty in neural networks.\u00a0arXiv:1711.01297.\n[2] Wang, K. C., Vicol, P., Lucas, J., Gu, L., Grosse, R., & Zemel, R. (2018, July). Adversarial Distillation of Bayesian Neural Network Posteriors. ICML\n[3] Mescheder, L., Nowozin, S., & Geiger, A. (2017, July). Adversarial Variational Bayes: Unifying Variational Autoencoders and Generative Adversarial Networks. ICML\n[4] Husz\u00e1r, F. (2017). Variational inference using implicit distributions.\u00a0arXiv:1702.08235.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper984/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "HyperGAN:  Exploring the Manifold of Neural Networks", "abstract": "We introduce HyperGAN, a generative network that learns to generate all the weight parameters of deep neural networks. HyperGAN first transforms low dimensional noise into a latent space, which can be sampled from to obtain diverse, performant sets of parameters for a target architecture. We utilize an architecture that bears resemblance to generative adversarial networks, but we evaluate the likelihood of samples with a classification loss. This is equivalent to minimizing the KL-divergence between the generated network parameter distribution and an unknown true parameter distribution. We apply HyperGAN to classification, showing that HyperGAN can learn to generate parameters which solve the MNIST and CIFAR-10 datasets with competitive performance to fully supervised learning while learning a rich distribution of effective parameters. We also show that HyperGAN can also provide better uncertainty than standard ensembles. This is evaluated by the ability of HyperGAN-generated ensembles to detect out of distribution data as well as adversarial examples. We see that in addition to being highly accurate on inlier data, HyperGAN can provide reasonable uncertainty estimates. ", "keywords": ["hypernetworks", "generative adversarial networks", "anomaly detection"], "authorids": ["ratzlafn@oregonstate.edu", "lif@oregonstate.edu"], "authors": ["Neale Ratzlaff", "Li  Fuxin"], "TL;DR": "We use a GAN to generate parameters of a neural network in one forward pass.", "pdf": "/pdf/bc0e4ee7bd22131225d76785340113d7baa00692.pdf", "paperhash": "ratzlaff|hypergan_exploring_the_manifold_of_neural_networks", "_bibtex": "@misc{\nratzlaff2019hypergan,\ntitle={Hyper{GAN}:  Exploring the Manifold of Neural Networks},\nauthor={Neale Ratzlaff and Li  Fuxin},\nyear={2019},\nurl={https://openreview.net/forum?id=B1GHJ3R9tQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper984/Official_Review", "cdate": 1542234332768, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "B1GHJ3R9tQ", "replyto": "B1GHJ3R9tQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper984/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335846962, "tmdate": 1552335846962, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper984/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "B1e3U8GQJ4", "original": null, "number": 7, "cdate": 1543870036176, "ddate": null, "tcdate": 1543870036176, "tmdate": 1543870036176, "tddate": null, "forum": "B1GHJ3R9tQ", "replyto": "rJxTVwkqAm", "invitation": "ICLR.cc/2019/Conference/-/Paper984/Official_Comment", "content": {"title": "Reply", "comment": "Thanks for the clarifications, the section 3 has been greatly improved (thus I slightly increased my score) but there is still room for improvement.\n\n1. I think the section 3.1 is unclear and potentially unnecessary. It is well known that minimizing the KL between the true data distribution and the model is equivalent to maximizing the log likelihood of the model. Plus there is a mistake in equation 6 the expectation should be taken over p(x|\\theta).\n2. I'm not sure to understand table 3 & 4, do you sample the q_n directly from P ? If so I don't understand why this would \"collapse\" ? since you actually argue in section 3 that \"This constraint makes it closer to the generated parameters and ensures that Q(s) itself does not collapse to always outputting the same latent code\".\n\nAs a note I recommend for next time that you don't share your code through a GitHub link as this can compromise anonymity."}, "signatures": ["ICLR.cc/2019/Conference/Paper984/AnonReviewer3"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper984/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper984/AnonReviewer3", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "HyperGAN:  Exploring the Manifold of Neural Networks", "abstract": "We introduce HyperGAN, a generative network that learns to generate all the weight parameters of deep neural networks. HyperGAN first transforms low dimensional noise into a latent space, which can be sampled from to obtain diverse, performant sets of parameters for a target architecture. We utilize an architecture that bears resemblance to generative adversarial networks, but we evaluate the likelihood of samples with a classification loss. This is equivalent to minimizing the KL-divergence between the generated network parameter distribution and an unknown true parameter distribution. We apply HyperGAN to classification, showing that HyperGAN can learn to generate parameters which solve the MNIST and CIFAR-10 datasets with competitive performance to fully supervised learning while learning a rich distribution of effective parameters. We also show that HyperGAN can also provide better uncertainty than standard ensembles. This is evaluated by the ability of HyperGAN-generated ensembles to detect out of distribution data as well as adversarial examples. We see that in addition to being highly accurate on inlier data, HyperGAN can provide reasonable uncertainty estimates. ", "keywords": ["hypernetworks", "generative adversarial networks", "anomaly detection"], "authorids": ["ratzlafn@oregonstate.edu", "lif@oregonstate.edu"], "authors": ["Neale Ratzlaff", "Li  Fuxin"], "TL;DR": "We use a GAN to generate parameters of a neural network in one forward pass.", "pdf": "/pdf/bc0e4ee7bd22131225d76785340113d7baa00692.pdf", "paperhash": "ratzlaff|hypergan_exploring_the_manifold_of_neural_networks", "_bibtex": "@misc{\nratzlaff2019hypergan,\ntitle={Hyper{GAN}:  Exploring the Manifold of Neural Networks},\nauthor={Neale Ratzlaff and Li  Fuxin},\nyear={2019},\nurl={https://openreview.net/forum?id=B1GHJ3R9tQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper984/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609652, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1GHJ3R9tQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper984/Authors", "ICLR.cc/2019/Conference/Paper984/Reviewers", "ICLR.cc/2019/Conference/Paper984/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper984/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper984/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper984/Authors|ICLR.cc/2019/Conference/Paper984/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper984/Reviewers", "ICLR.cc/2019/Conference/Paper984/Authors", "ICLR.cc/2019/Conference/Paper984/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609652}}}, {"id": "BkgFAhVU3m", "original": null, "number": 3, "cdate": 1540930769487, "ddate": null, "tcdate": 1540930769487, "tmdate": 1543508051136, "tddate": null, "forum": "B1GHJ3R9tQ", "replyto": "B1GHJ3R9tQ", "invitation": "ICLR.cc/2019/Conference/-/Paper984/Official_Review", "content": {"title": "Review", "review": "This paper proposes a technique for learning a distribution over parameters of a neural network such that samples from the distribution correspond to performant networks. The approach effectively encourages sampled parameters to have low loss on the training set, and also uses an adversarial loss to encourage the distribution of parameters to be Gaussian distributed. This approach can improve performance slightly by using ensembling and can be useful for uncertainty estimates for out-of-distribution examples. The approach is tested on a few simple problems and is shown to work well.\n\nI am definitely in favor of exploring adversarial divergences (using a critic as a differentiable loss to compare two distributions) in unusual settings, and this paper certainly does this. The idea of transforming samples from a prior such that the transformed sample corresponds to useful network parameters is interesting. The results also seem promising. However, currently the mathematical description of this method is completely unclear and ridden with many errors. I can understand at a reasonable level what the approach is doing from Figure 1, but the definitions and equations given in Equation 3 are at times nearly incomprehensible \"mathiness\". I'm giving the paper a borderline accept because the idea is interesting and the results are OK; I will raise my score if Section 3 is dramatically improved. I give some specific examples of issues with Section 3 in my specific comments below. I'd also note that the paper does a somewhat poor job comparing to existing work - only section 4.2 includes a comparison to existing \"uncertainty\" methods. This should also be improved - the authors should implement the existing methods and use them as a point of comparison in all of their experiments. As a final high-level note, the approach is described at various points as an \"autoencoder\" particularly in reference to the adversarial autoencoder. However, the approach does not \"autoencode\" anything - there is no reconstruction term, or input apart from the noise samples. The only thing it has in common with the adversarial autoencoder is the use of a critic to enforce a distributional constraint. Calling it, or comparing it to, an autoencoder is confusing and misleading.\n\nSpecific comments:\n\n- You mention fast weights in related work. I believe Hinton and Plaut were the first to propose fast weights in \"Using Fast Weights to Deblur Old Memories\", and I'd also suggest mentioning \"Using Fast Weights to Attend to the Recent Past\" which is a more recent demonstration that fast weights can be useful on modern problems.\n- The are some issues with your description of Equation 1: First, I don't believe you define G(z) (I assume it is the \"decoder\" network; please define it). Second, in practice I don't believe you actually use JSD or MMD for D_z; you use a critic architecture which in some limit approximates some statistical divergence but in practice they typically don't (see e.g. Arjovsky and Bottou 2017; Fedus et al. 2017; Rosca et al. 2018). Third, writing Q_z \\sim Q(z | x) seems strange to me - Q_z is a distribution, and I don't believe that Q(z | x) is a distribution over distributions, so how are you sampling a distribution (Q_z) from Q(z | x) as suggested by the use of the \\sim notation? I think you simply mean that Q_z is Q(z | x) approximately marginalized over x.\n- Equation 2 is also not clear. First, the sentence before starts \"Suppose the real parameters \\theta^* \\sim \\Theta...\" The equation itself does not include \\theta^* or \\Theta so I don't see what this is referring to. Second, the expression for an m-dimensional is written \\mathcal{N}(0, \\sigma^2, I_m). It's not clear why there is a comma before I_m, and I_m is not defined (though I assume it is the m \\times m identity matrix) - did you mean to multiply I_m by \\sigma^2? Third, it looks like you actually define P_z twice, once as \"an m-dimensional isotropic Gaussian\" and again as \"a Kd-dimensional isotropic Gaussian\"; am I to infer that m = Kd? Why use both? Fourth, you mention the joint P(x, y) but the expectation is taken over P_x and P(y | x). Why call it P_x and not P(x)? And why not compute the expectation over P(x, y)? Fifth, you write \"Here the encoder...\" -- you never define that Q(z) or G is \"the encoder\", I assume Q(z). It is strange to take the expectation over Q(z) (I assume sampling z \\sim Q(z)) but then have the term Q(z) appear in (2). How are Q(z) and Q_z related? On that note, I don't see how (2) is an autoencoder, since there is no Q(z | x) term. It appears instead that you are sampling z from Q(z) which doesn't condition on x. So what is being autoencoded? Related, you write \"all the q_k (that will generate different layers) will be correlated, unlike dimensions of z which are drawn to be independent from each other.\" But if Q(z) = [q_1, ..., q_K] then doesn't the secont term in (2) suggest that they are being enforced to be similar to the prior P_z, and therefore uncorrelated? Note that you also say later on \"The job of the regularizer D_z(P_z, Q_z) is to force each embedding q_n to approximate P_z.\" Frankly at this point I will stop pointing out issues with this equation and discussion since they are so widespread.\n- In your definition of your ensemble scoring rule, you are taking the sum over N + 1 elements (n = 0 to N) but dividing by N.\n- In 4.2, do you use the same model architecture/training/regularization etc. as in previous studies? If not I think comparing the different methods will be conflated by differences in training procedures. Since you do not report results in many experimental settings, I assume you don't.\n- In Figure 3, why not plot the true standard deviation around the true function? It appears you are only plotting +/- 3 stndard deviations for the learned function.\n- Why not include 100 models L2 on Figure 3?\n- It's not clear to me why you define your \"disagreement d\" when it appears the same as the entropy score you used in 4.4.\n- A stronger and more convincing attack would be to attack the ensemble of models, instead of attacking a single model and testing on the ensemble.", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper984/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "HyperGAN:  Exploring the Manifold of Neural Networks", "abstract": "We introduce HyperGAN, a generative network that learns to generate all the weight parameters of deep neural networks. HyperGAN first transforms low dimensional noise into a latent space, which can be sampled from to obtain diverse, performant sets of parameters for a target architecture. We utilize an architecture that bears resemblance to generative adversarial networks, but we evaluate the likelihood of samples with a classification loss. This is equivalent to minimizing the KL-divergence between the generated network parameter distribution and an unknown true parameter distribution. We apply HyperGAN to classification, showing that HyperGAN can learn to generate parameters which solve the MNIST and CIFAR-10 datasets with competitive performance to fully supervised learning while learning a rich distribution of effective parameters. We also show that HyperGAN can also provide better uncertainty than standard ensembles. This is evaluated by the ability of HyperGAN-generated ensembles to detect out of distribution data as well as adversarial examples. We see that in addition to being highly accurate on inlier data, HyperGAN can provide reasonable uncertainty estimates. ", "keywords": ["hypernetworks", "generative adversarial networks", "anomaly detection"], "authorids": ["ratzlafn@oregonstate.edu", "lif@oregonstate.edu"], "authors": ["Neale Ratzlaff", "Li  Fuxin"], "TL;DR": "We use a GAN to generate parameters of a neural network in one forward pass.", "pdf": "/pdf/bc0e4ee7bd22131225d76785340113d7baa00692.pdf", "paperhash": "ratzlaff|hypergan_exploring_the_manifold_of_neural_networks", "_bibtex": "@misc{\nratzlaff2019hypergan,\ntitle={Hyper{GAN}:  Exploring the Manifold of Neural Networks},\nauthor={Neale Ratzlaff and Li  Fuxin},\nyear={2019},\nurl={https://openreview.net/forum?id=B1GHJ3R9tQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper984/Official_Review", "cdate": 1542234332768, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "B1GHJ3R9tQ", "replyto": "B1GHJ3R9tQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper984/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335846962, "tmdate": 1552335846962, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper984/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "S1xNMg5pRX", "original": null, "number": 6, "cdate": 1543507980328, "ddate": null, "tcdate": 1543507980328, "tmdate": 1543507980328, "tddate": null, "forum": "B1GHJ3R9tQ", "replyto": "SJxY6Iy50Q", "invitation": "ICLR.cc/2019/Conference/-/Paper984/Official_Comment", "content": {"title": "Re: Reply", "comment": "Thank you for addressing my comments and for your updates. Section 3 is clearly improved; I believe it is more clear what the model is now although since I spent so much time trying to figure out what was going on in the first draft I may have an easier time understanding it than someone who is being exposed to the idea for the first time. The paper remains somewhat weak in terms of comparing to existing work, though I appreciate the inclusion of [1] as a baseline. I have raised my score accordingly but I would suggest the authors continue to rework the paper, improve the baselines, and resubmit it to another conference if they are interested in it being published.\n\nA remaining low-level question (no need to answer explicitly here): Why are so many entries in Table 2 empty? Did the authors reimplement the baselines they are comparing against? If you are just quoting numbers from previous papers, I bet that your implementation of the model and their implementation is quite different and the results cannot be reliably compared. I would think you would need to reimplement all of these baselines in the same framework/model implementation/training scheme/etc. to get a reliable comparison."}, "signatures": ["ICLR.cc/2019/Conference/Paper984/AnonReviewer2"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper984/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper984/AnonReviewer2", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "HyperGAN:  Exploring the Manifold of Neural Networks", "abstract": "We introduce HyperGAN, a generative network that learns to generate all the weight parameters of deep neural networks. HyperGAN first transforms low dimensional noise into a latent space, which can be sampled from to obtain diverse, performant sets of parameters for a target architecture. We utilize an architecture that bears resemblance to generative adversarial networks, but we evaluate the likelihood of samples with a classification loss. This is equivalent to minimizing the KL-divergence between the generated network parameter distribution and an unknown true parameter distribution. We apply HyperGAN to classification, showing that HyperGAN can learn to generate parameters which solve the MNIST and CIFAR-10 datasets with competitive performance to fully supervised learning while learning a rich distribution of effective parameters. We also show that HyperGAN can also provide better uncertainty than standard ensembles. This is evaluated by the ability of HyperGAN-generated ensembles to detect out of distribution data as well as adversarial examples. We see that in addition to being highly accurate on inlier data, HyperGAN can provide reasonable uncertainty estimates. ", "keywords": ["hypernetworks", "generative adversarial networks", "anomaly detection"], "authorids": ["ratzlafn@oregonstate.edu", "lif@oregonstate.edu"], "authors": ["Neale Ratzlaff", "Li  Fuxin"], "TL;DR": "We use a GAN to generate parameters of a neural network in one forward pass.", "pdf": "/pdf/bc0e4ee7bd22131225d76785340113d7baa00692.pdf", "paperhash": "ratzlaff|hypergan_exploring_the_manifold_of_neural_networks", "_bibtex": "@misc{\nratzlaff2019hypergan,\ntitle={Hyper{GAN}:  Exploring the Manifold of Neural Networks},\nauthor={Neale Ratzlaff and Li  Fuxin},\nyear={2019},\nurl={https://openreview.net/forum?id=B1GHJ3R9tQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper984/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609652, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1GHJ3R9tQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper984/Authors", "ICLR.cc/2019/Conference/Paper984/Reviewers", "ICLR.cc/2019/Conference/Paper984/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper984/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper984/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper984/Authors|ICLR.cc/2019/Conference/Paper984/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper984/Reviewers", "ICLR.cc/2019/Conference/Paper984/Authors", "ICLR.cc/2019/Conference/Paper984/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609652}}}, {"id": "H1lJ5P1cRQ", "original": null, "number": 4, "cdate": 1543268231260, "ddate": null, "tcdate": 1543268231260, "tmdate": 1543268231260, "tddate": null, "forum": "B1GHJ3R9tQ", "replyto": "rkgVi_vVhQ", "invitation": "ICLR.cc/2019/Conference/-/Paper984/Official_Comment", "content": {"title": "Reply", "comment": "We have updated the references and replaced them with conference proceedings where applicable. We have updated section 3 to make the technical description more clear. \n\nQuestion: Connection and difference with Variational inference\nHyperGAN looks on the surface similar to Bayesian methods like variational inference, in implicit generative models like VAE, VI is done by minimizing the KL divergence between the prior and the latent space. This is done by assuming a Gaussian encoder and prior so that the KL term can be computed. Instead of using a closed-form distribution as in VI, we enforce the distributional constraint on Q by using a discriminator trained adversarially. We have removed the language stating that our method is not Bayesian. It\u2019s more accurate that we don\u2019t make use of fully probabilistic models as in our model there is no encoder that goes from network parameters to the latent sample, and as in other VAE/GAN-based approaches, we can\u2019t compute the probability of generating a particular set of parameters.  Our rewritten Section 3 makes this more clear. \n\nQuestion: What\u2019s expected out of the 1D regression task?\nThe 1D toy regression task is a simple test which shows that HyperGAN can learn more than just the mean function of the training data. In any model which can measure uncertainty, we would expect the model to give a wider distribution of predictions as we move farther from the training data. We can see that this happens reliably with HyperGAN-generated ensembles of different sizes. \n\nQuestion: Title is inaccurate\nWe have edited the title to reflect that we are generating diverse neural networks, instead of a whole manifold. \n\nQuestion: Code is not available\nThe code is now available on Github at : https://github.com/ICLR19HyperGAN/HyperGAN\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper984/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper984/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper984/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "HyperGAN:  Exploring the Manifold of Neural Networks", "abstract": "We introduce HyperGAN, a generative network that learns to generate all the weight parameters of deep neural networks. HyperGAN first transforms low dimensional noise into a latent space, which can be sampled from to obtain diverse, performant sets of parameters for a target architecture. We utilize an architecture that bears resemblance to generative adversarial networks, but we evaluate the likelihood of samples with a classification loss. This is equivalent to minimizing the KL-divergence between the generated network parameter distribution and an unknown true parameter distribution. We apply HyperGAN to classification, showing that HyperGAN can learn to generate parameters which solve the MNIST and CIFAR-10 datasets with competitive performance to fully supervised learning while learning a rich distribution of effective parameters. We also show that HyperGAN can also provide better uncertainty than standard ensembles. This is evaluated by the ability of HyperGAN-generated ensembles to detect out of distribution data as well as adversarial examples. We see that in addition to being highly accurate on inlier data, HyperGAN can provide reasonable uncertainty estimates. ", "keywords": ["hypernetworks", "generative adversarial networks", "anomaly detection"], "authorids": ["ratzlafn@oregonstate.edu", "lif@oregonstate.edu"], "authors": ["Neale Ratzlaff", "Li  Fuxin"], "TL;DR": "We use a GAN to generate parameters of a neural network in one forward pass.", "pdf": "/pdf/bc0e4ee7bd22131225d76785340113d7baa00692.pdf", "paperhash": "ratzlaff|hypergan_exploring_the_manifold_of_neural_networks", "_bibtex": "@misc{\nratzlaff2019hypergan,\ntitle={Hyper{GAN}:  Exploring the Manifold of Neural Networks},\nauthor={Neale Ratzlaff and Li  Fuxin},\nyear={2019},\nurl={https://openreview.net/forum?id=B1GHJ3R9tQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper984/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609652, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1GHJ3R9tQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper984/Authors", "ICLR.cc/2019/Conference/Paper984/Reviewers", "ICLR.cc/2019/Conference/Paper984/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper984/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper984/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper984/Authors|ICLR.cc/2019/Conference/Paper984/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper984/Reviewers", "ICLR.cc/2019/Conference/Paper984/Authors", "ICLR.cc/2019/Conference/Paper984/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609652}}}, {"id": "rJxTVwkqAm", "original": null, "number": 3, "cdate": 1543268149437, "ddate": null, "tcdate": 1543268149437, "tmdate": 1543268149437, "tddate": null, "forum": "B1GHJ3R9tQ", "replyto": "SJee6VaBhQ", "invitation": "ICLR.cc/2019/Conference/-/Paper984/Official_Comment", "content": {"title": "Reply", "comment": "We have revised the manuscript and addressed the notation and terminology concerns as well as the comments made by other reviewers. We have improved these to better describe our method. We would really appreciate if you can read the new Section 3 and give us a new evaluation and further feedback. With the revision, we still want to answer the questions laid out here.\n\nQuestion: The difference between a traditional Bayesian approach such as variational inference should also be discussed. It would be interesting to study what is the impact of this particular choice of loss for the discriminator. In particular, I invite the author to compare the loss proposed to the loss in [1].\nAnswer:  From the new description one can see that there is a significant difference between our approach and variational Bayesian approach in that we never explicitly model the KL-divergence term that comes from p(z|\\theta), because we do not have explicit theta samples nor had an encoder.\nIn Bayes by Hypernetwork (BbH) [1], we note two differences. First, the prior matching step treats each weight independently. This is different from HyperGAN where we perform the prior matching step between the prior and the continuous mixture Q(s) using the adversarial loss. Second, BbH uses independent noise samples as input to the generators. We found that this configuration hurt the diversity of our generated networks, which is why we use the mixer Q to introduce correlations to our single noise sample. In Table 3 and 4 we compare against using independent generators and find that we lose significant diversity in our generated ensembles. \n\nQuestion: What is the number of networks used for other methods?\nAnswer: Each other (non-HyperGAN) method in Table 2 uses the mean of 100 samples, which corresponds to our strongest considered ensemble of 100 networks. We only used 10 networks for the L2 (standard) ensembles in the adversarial detection experiments in Sec. 4.5, because of the prohibitive cost of training 100 neural networks on each task. \n\nQuestion: It would be nice to have a plot showing the accuracy as a function of the perturbation in section 4.5.\nAnswer: We tested only on adversarial examples which succeeded in fooling our ensemble. This means the accuracy of the ensemble predictions under all perturbation levels is 0, so we chose not to plot it.\n\nQuestion: Title is inaccurate \nWe have also edited the title to reflect that we are generating diverse neural networks, instead of a whole manifold. \n\n[1] Pawlowski, Nick, et al. \"Implicit weight uncertainty in neural networks.\" arXiv preprint arXiv:1711.01297 (2017).\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper984/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper984/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper984/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "HyperGAN:  Exploring the Manifold of Neural Networks", "abstract": "We introduce HyperGAN, a generative network that learns to generate all the weight parameters of deep neural networks. HyperGAN first transforms low dimensional noise into a latent space, which can be sampled from to obtain diverse, performant sets of parameters for a target architecture. We utilize an architecture that bears resemblance to generative adversarial networks, but we evaluate the likelihood of samples with a classification loss. This is equivalent to minimizing the KL-divergence between the generated network parameter distribution and an unknown true parameter distribution. We apply HyperGAN to classification, showing that HyperGAN can learn to generate parameters which solve the MNIST and CIFAR-10 datasets with competitive performance to fully supervised learning while learning a rich distribution of effective parameters. We also show that HyperGAN can also provide better uncertainty than standard ensembles. This is evaluated by the ability of HyperGAN-generated ensembles to detect out of distribution data as well as adversarial examples. We see that in addition to being highly accurate on inlier data, HyperGAN can provide reasonable uncertainty estimates. ", "keywords": ["hypernetworks", "generative adversarial networks", "anomaly detection"], "authorids": ["ratzlafn@oregonstate.edu", "lif@oregonstate.edu"], "authors": ["Neale Ratzlaff", "Li  Fuxin"], "TL;DR": "We use a GAN to generate parameters of a neural network in one forward pass.", "pdf": "/pdf/bc0e4ee7bd22131225d76785340113d7baa00692.pdf", "paperhash": "ratzlaff|hypergan_exploring_the_manifold_of_neural_networks", "_bibtex": "@misc{\nratzlaff2019hypergan,\ntitle={Hyper{GAN}:  Exploring the Manifold of Neural Networks},\nauthor={Neale Ratzlaff and Li  Fuxin},\nyear={2019},\nurl={https://openreview.net/forum?id=B1GHJ3R9tQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper984/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609652, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1GHJ3R9tQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper984/Authors", "ICLR.cc/2019/Conference/Paper984/Reviewers", "ICLR.cc/2019/Conference/Paper984/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper984/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper984/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper984/Authors|ICLR.cc/2019/Conference/Paper984/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper984/Reviewers", "ICLR.cc/2019/Conference/Paper984/Authors", "ICLR.cc/2019/Conference/Paper984/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609652}}}, {"id": "SJxY6Iy50Q", "original": null, "number": 2, "cdate": 1543268033086, "ddate": null, "tcdate": 1543268033086, "tmdate": 1543268033086, "tddate": null, "forum": "B1GHJ3R9tQ", "replyto": "BkgFAhVU3m", "invitation": "ICLR.cc/2019/Conference/-/Paper984/Official_Comment", "content": {"title": "Reply", "comment": "Thanks a lot for your valuable comments. \nWe have thoroughly reworked the manuscript, especially section 3, to address these concerns as well as those had by other reviewers. We would really appreciate if you can read the new Section 3 and give us a new evaluation and further feedback. \nWe want to address some specific issues here:\n\n-We have added comparisons to [1], as well as discussion of the results. We conducted these experiments using model architectures identical to our own. We will add [2] and [3] in the next version of the paper,\n\n-We have added the suggested citations to our related work section\n\n-We have reworked our treatment of \u201cauto-encoders\u201d. We have re-written Section 3 to point out the similarity and differences between HyperGAN and conventional GANs. Indeed, HyperGAN is not an auto-encoder since nothing was encoded explicitly. Hence we changed all the \u201cencoder\u201d text in the manuscript to a new term called mixer. The mixer serves to introduce dependencies in the latent samples, which turned out to be crucial for the diversity of the networks generated by HyperGAN (Table 3, 4). We have added a section in the appendix which thoroughly explains these methods and how HyperGAN differs from both. \n\n-We did not use ensembles of 100 (non-HyperGAN) models for our anomaly detection because it becomes computationally prohibitive for us to train so many models, especially for CIFAR-10. One of the main advantages of HyperGAN is to generate hundreds and thousands of models effortlessly from the generator without additional training, while it is time-consuming to generate conventional L2 ensembles with more than a handful of models. We note that there is a drastic difference between HyperGAN and L2 ensembles with 10 models (Fig. 3 and Fig. 5). \n\n-In the adversarial attack experiment (section 4.5), we indeed attacked our ensembles (both standard and HyperGAN) until we have adversarial examples which fooled the ensemble for Fig. 5. There was a typo in the previous paragraph where we mentioned fooling just one network and test it on the whole ensemble, which should refer to Fig. 4 and is a separate experiment that we did to show the validity of using the uncertainty to detect adversarial examples. We have revised the explanation in the manuscript to make this more clear. \n\n[1] Louizos, Christos, and Max Welling. \"Multiplicative normalizing flows for variational bayesian neural networks.\" arXiv preprint arXiv:1703.01961 (2017).\n[2] Wang, K. C., Vicol, P., Lucas, J., Gu, L., Grosse, R., & Zemel, R. (2018, July). Adversarial Distillation of Bayesian Neural Network Posteriors. ICML\n[3] Gal, Yarin, and Zoubin Ghahramani. \"Dropout as a Bayesian approximation: Representing model uncertainty in deep learning.\" international conference on machine learning. 2016.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper984/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper984/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper984/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "HyperGAN:  Exploring the Manifold of Neural Networks", "abstract": "We introduce HyperGAN, a generative network that learns to generate all the weight parameters of deep neural networks. HyperGAN first transforms low dimensional noise into a latent space, which can be sampled from to obtain diverse, performant sets of parameters for a target architecture. We utilize an architecture that bears resemblance to generative adversarial networks, but we evaluate the likelihood of samples with a classification loss. This is equivalent to minimizing the KL-divergence between the generated network parameter distribution and an unknown true parameter distribution. We apply HyperGAN to classification, showing that HyperGAN can learn to generate parameters which solve the MNIST and CIFAR-10 datasets with competitive performance to fully supervised learning while learning a rich distribution of effective parameters. We also show that HyperGAN can also provide better uncertainty than standard ensembles. This is evaluated by the ability of HyperGAN-generated ensembles to detect out of distribution data as well as adversarial examples. We see that in addition to being highly accurate on inlier data, HyperGAN can provide reasonable uncertainty estimates. ", "keywords": ["hypernetworks", "generative adversarial networks", "anomaly detection"], "authorids": ["ratzlafn@oregonstate.edu", "lif@oregonstate.edu"], "authors": ["Neale Ratzlaff", "Li  Fuxin"], "TL;DR": "We use a GAN to generate parameters of a neural network in one forward pass.", "pdf": "/pdf/bc0e4ee7bd22131225d76785340113d7baa00692.pdf", "paperhash": "ratzlaff|hypergan_exploring_the_manifold_of_neural_networks", "_bibtex": "@misc{\nratzlaff2019hypergan,\ntitle={Hyper{GAN}:  Exploring the Manifold of Neural Networks},\nauthor={Neale Ratzlaff and Li  Fuxin},\nyear={2019},\nurl={https://openreview.net/forum?id=B1GHJ3R9tQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper984/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609652, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1GHJ3R9tQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper984/Authors", "ICLR.cc/2019/Conference/Paper984/Reviewers", "ICLR.cc/2019/Conference/Paper984/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper984/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper984/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper984/Authors|ICLR.cc/2019/Conference/Paper984/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper984/Reviewers", "ICLR.cc/2019/Conference/Paper984/Authors", "ICLR.cc/2019/Conference/Paper984/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609652}}}, {"id": "rkgVi_vVhQ", "original": null, "number": 1, "cdate": 1540810907899, "ddate": null, "tcdate": 1540810907899, "tmdate": 1541533521021, "tddate": null, "forum": "B1GHJ3R9tQ", "replyto": "B1GHJ3R9tQ", "invitation": "ICLR.cc/2019/Conference/-/Paper984/Official_Review", "content": {"title": "Nice idea", "review": "TL;DR. I find the manuscript to contain interesting ideas, yet I believe there is room for improvement.\n\n* Summary\n\nFor any given specific network architecture, the manuscript aims at learning a distribution over the weights (rather than point-wise estimates of the weights). This is achieved through using a two-steps procedure, in which a \"hypernetwork\" is trained to output weights for the network of interest, and a GAN is then used to (adversarially) generate samples from a distribution $Q$ which is assumed not too far (in a KL sense) from a Gaussian prior $P$.\n\n* Major issues\n\nI find the central idea to be of interest to the ICLR community. However I have found a number of shortcomings to be addressed before I could recommend acceptance. The following list is in no particular order.\n\n- References: 20 out of 22 (!) references are preprints, about half of which are 3+ years old. Most of them are now published in proceedings and I strongly feel proper credit should be given to authors by replacing outdated preprints with correct citations.\n- Links with Bayesian deep learning: I feel this should be more carefully discussed in the manuscript. The sentence \"We have proposed a generative, non-Bayesian solution [...]\" should be explained, as from what I gather HyperGAN samples (GAN-like) weights (i.e., networks) from a distribution $Q$ which is deemed not too far (in the KL sense) from a prior distribution $P$. How is that not Bayesian?\n- Numerical experiments. Table 4: what are the numbers reported? If single evaluation, I do not believe any conclusion may be drawn. If averages over multiple repetitions, no conclusion can be drawn without reporting standard deviations. In addition, I do not quite grasp the purpose of the 1D toy example.\n- Overall, I think the authors should try and make their contributions and method clearer. For example, a pseudo-code of the whole procedure might help readers understand the gist. \n- Architecture specific: I find the claim that HyperGAN explores the manifold of neural nets too strong. As the whole procedure is architecture-specific, I would find more appropriate to change that claim to \"exploring the weights distribution for a specific architecture\".\n- Code availability: the scope of the paper is diminished by the fact that no code is available by the time of review. A toolbox (not disclosing the authors' identities) should be made available to support the manuscript claims. Last sentence (page 8) is likely to be outdated and should be removed. \n\n* Minor issues\n\n- some typos: architecture (caption figure 1), $G(Q(z))$ (missing parenthesis, page 3), sum index $n$ not used in the last equation (page 7).\n- \"Perhaps the first proposed method...\" (page 2). Such imprecise statements must be avoided.", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper984/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "HyperGAN:  Exploring the Manifold of Neural Networks", "abstract": "We introduce HyperGAN, a generative network that learns to generate all the weight parameters of deep neural networks. HyperGAN first transforms low dimensional noise into a latent space, which can be sampled from to obtain diverse, performant sets of parameters for a target architecture. We utilize an architecture that bears resemblance to generative adversarial networks, but we evaluate the likelihood of samples with a classification loss. This is equivalent to minimizing the KL-divergence between the generated network parameter distribution and an unknown true parameter distribution. We apply HyperGAN to classification, showing that HyperGAN can learn to generate parameters which solve the MNIST and CIFAR-10 datasets with competitive performance to fully supervised learning while learning a rich distribution of effective parameters. We also show that HyperGAN can also provide better uncertainty than standard ensembles. This is evaluated by the ability of HyperGAN-generated ensembles to detect out of distribution data as well as adversarial examples. We see that in addition to being highly accurate on inlier data, HyperGAN can provide reasonable uncertainty estimates. ", "keywords": ["hypernetworks", "generative adversarial networks", "anomaly detection"], "authorids": ["ratzlafn@oregonstate.edu", "lif@oregonstate.edu"], "authors": ["Neale Ratzlaff", "Li  Fuxin"], "TL;DR": "We use a GAN to generate parameters of a neural network in one forward pass.", "pdf": "/pdf/bc0e4ee7bd22131225d76785340113d7baa00692.pdf", "paperhash": "ratzlaff|hypergan_exploring_the_manifold_of_neural_networks", "_bibtex": "@misc{\nratzlaff2019hypergan,\ntitle={Hyper{GAN}:  Exploring the Manifold of Neural Networks},\nauthor={Neale Ratzlaff and Li  Fuxin},\nyear={2019},\nurl={https://openreview.net/forum?id=B1GHJ3R9tQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper984/Official_Review", "cdate": 1542234332768, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "B1GHJ3R9tQ", "replyto": "B1GHJ3R9tQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper984/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335846962, "tmdate": 1552335846962, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper984/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "HklazP-Oc7", "original": null, "number": 1, "cdate": 1538950932751, "ddate": null, "tcdate": 1538950932751, "tmdate": 1539293443758, "tddate": null, "forum": "B1GHJ3R9tQ", "replyto": "r1gR6GG7cm", "invitation": "ICLR.cc/2019/Conference/-/Paper984/Official_Comment", "content": {"title": "Thanks for the input about related work", "comment": "Thank you for alerting us of this recent work we missed. We will be sure to cite and compare against it in the final paper. \nOur paper is, although, quite different from the aforementioned reference [1]. The offline setting in [1] is the approach we considered and didn't take (page 3) - to first train many networks and train a GAN on them. In the online setting in [1], a GAN is trained from samples taken from a single training procedure, with noise added from MCMC. The MCMC samples and GAN updates are interleaved. With this approach, the parameters that are used to train the GAN are highly correlated with each other and the diversity of the networks that the trained GANs can generate is questionable.\n\nIn contrast, HyperGAN generates networks purely from random noise samples. Our approach doesn't use correlated examples in training and hence we believe it can generate more diverse networks (Table 3). We also show that HyperGAN can achieve higher classification accuracy on testing data and ensembles of models help significantly, showcasing the diversity of generated networks. \n\n[1] Wang, K.-C., Vicol, P., Lucas, J., Gu, L., Grosse, R., and Zemel, R. Adversarial Distillation of Bayesian Neural\nNetwork Posteriors. In Proceedings of the 35th International Conference on Machine Learning, 2018"}, "signatures": ["ICLR.cc/2019/Conference/Paper984/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper984/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper984/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "HyperGAN:  Exploring the Manifold of Neural Networks", "abstract": "We introduce HyperGAN, a generative network that learns to generate all the weight parameters of deep neural networks. HyperGAN first transforms low dimensional noise into a latent space, which can be sampled from to obtain diverse, performant sets of parameters for a target architecture. We utilize an architecture that bears resemblance to generative adversarial networks, but we evaluate the likelihood of samples with a classification loss. This is equivalent to minimizing the KL-divergence between the generated network parameter distribution and an unknown true parameter distribution. We apply HyperGAN to classification, showing that HyperGAN can learn to generate parameters which solve the MNIST and CIFAR-10 datasets with competitive performance to fully supervised learning while learning a rich distribution of effective parameters. We also show that HyperGAN can also provide better uncertainty than standard ensembles. This is evaluated by the ability of HyperGAN-generated ensembles to detect out of distribution data as well as adversarial examples. We see that in addition to being highly accurate on inlier data, HyperGAN can provide reasonable uncertainty estimates. ", "keywords": ["hypernetworks", "generative adversarial networks", "anomaly detection"], "authorids": ["ratzlafn@oregonstate.edu", "lif@oregonstate.edu"], "authors": ["Neale Ratzlaff", "Li  Fuxin"], "TL;DR": "We use a GAN to generate parameters of a neural network in one forward pass.", "pdf": "/pdf/bc0e4ee7bd22131225d76785340113d7baa00692.pdf", "paperhash": "ratzlaff|hypergan_exploring_the_manifold_of_neural_networks", "_bibtex": "@misc{\nratzlaff2019hypergan,\ntitle={Hyper{GAN}:  Exploring the Manifold of Neural Networks},\nauthor={Neale Ratzlaff and Li  Fuxin},\nyear={2019},\nurl={https://openreview.net/forum?id=B1GHJ3R9tQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper984/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609652, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1GHJ3R9tQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper984/Authors", "ICLR.cc/2019/Conference/Paper984/Reviewers", "ICLR.cc/2019/Conference/Paper984/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper984/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper984/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper984/Authors|ICLR.cc/2019/Conference/Paper984/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper984/Reviewers", "ICLR.cc/2019/Conference/Paper984/Authors", "ICLR.cc/2019/Conference/Paper984/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609652}}}, {"id": "r1gR6GG7cm", "original": null, "number": 1, "cdate": 1538626245848, "ddate": null, "tcdate": 1538626245848, "tmdate": 1538626255669, "tddate": null, "forum": "B1GHJ3R9tQ", "replyto": "B1GHJ3R9tQ", "invitation": "ICLR.cc/2019/Conference/-/Paper984/Public_Comment", "content": {"comment": "Previous work has addressed parameter generation using GANs. Please check out this paper: http://proceedings.mlr.press/v80/wang18i.html\n\nIn my opinion, this work still has novelty but a discussion/comparison seems due.", "title": "A closely related paper"}, "signatures": ["~James_Lucas1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper984/Reviewers/Unsubmitted"], "writers": ["~James_Lucas1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "HyperGAN:  Exploring the Manifold of Neural Networks", "abstract": "We introduce HyperGAN, a generative network that learns to generate all the weight parameters of deep neural networks. HyperGAN first transforms low dimensional noise into a latent space, which can be sampled from to obtain diverse, performant sets of parameters for a target architecture. We utilize an architecture that bears resemblance to generative adversarial networks, but we evaluate the likelihood of samples with a classification loss. This is equivalent to minimizing the KL-divergence between the generated network parameter distribution and an unknown true parameter distribution. We apply HyperGAN to classification, showing that HyperGAN can learn to generate parameters which solve the MNIST and CIFAR-10 datasets with competitive performance to fully supervised learning while learning a rich distribution of effective parameters. We also show that HyperGAN can also provide better uncertainty than standard ensembles. This is evaluated by the ability of HyperGAN-generated ensembles to detect out of distribution data as well as adversarial examples. We see that in addition to being highly accurate on inlier data, HyperGAN can provide reasonable uncertainty estimates. ", "keywords": ["hypernetworks", "generative adversarial networks", "anomaly detection"], "authorids": ["ratzlafn@oregonstate.edu", "lif@oregonstate.edu"], "authors": ["Neale Ratzlaff", "Li  Fuxin"], "TL;DR": "We use a GAN to generate parameters of a neural network in one forward pass.", "pdf": "/pdf/bc0e4ee7bd22131225d76785340113d7baa00692.pdf", "paperhash": "ratzlaff|hypergan_exploring_the_manifold_of_neural_networks", "_bibtex": "@misc{\nratzlaff2019hypergan,\ntitle={Hyper{GAN}:  Exploring the Manifold of Neural Networks},\nauthor={Neale Ratzlaff and Li  Fuxin},\nyear={2019},\nurl={https://openreview.net/forum?id=B1GHJ3R9tQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper984/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311706289, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "B1GHJ3R9tQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper984/Authors", "ICLR.cc/2019/Conference/Paper984/Reviewers", "ICLR.cc/2019/Conference/Paper984/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper984/Authors", "ICLR.cc/2019/Conference/Paper984/Reviewers", "ICLR.cc/2019/Conference/Paper984/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311706289}}}], "count": 12}