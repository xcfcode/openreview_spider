{"notes": [{"id": "Bke61krFvS", "original": "B1eEyxi_DH", "number": 1486, "cdate": 1569439461123, "ddate": null, "tcdate": 1569439461123, "tmdate": 1583912053577, "tddate": null, "forum": "Bke61krFvS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Learning representations for binary-classification without backpropagation", "authors": ["Mathias Lechner"], "authorids": ["mathias.lechner@ist.ac.at"], "keywords": ["feedback alignment", "alternatives to backpropagation", "biologically motivated learning algorithms"], "TL;DR": "First feedback alignment algorithm with provable learning guarantees for networks with single output neuron", "abstract": "The family of feedback alignment (FA) algorithms aims to provide a more biologically motivated alternative to backpropagation (BP), by substituting the computations that are unrealistic to be implemented in physical brains.\nWhile FA algorithms have been shown to work well in practice, there is a lack of rigorous theory proofing their learning capabilities.\t\t\nHere we introduce the first feedback alignment algorithm with provable learning guarantees. In contrast to existing work, we do not require any assumption about the size or depth of the network except that it has a single output neuron, i.e., such as for binary classification tasks.\nWe show that our FA algorithm can deliver its theoretical promises in practice, surpassing the learning performance of existing FA methods and matching backpropagation in binary classification tasks.\nFinally, we demonstrate the limits of our FA variant when the number of output neurons grows beyond a certain quantity.", "pdf": "/pdf/7deda89ac18d2f3fd6786ec9f448aacefd7bc0a4.pdf", "code": "https://github.com/mlech26l/iclr_paper_mdfa", "paperhash": "lechner|learning_representations_for_binaryclassification_without_backpropagation", "_bibtex": "@inproceedings{\nLechner2020Learning,\ntitle={Learning representations for binary-classification without backpropagation},\nauthor={Mathias Lechner},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Bke61krFvS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/52131900815b24b50d7682ac5ea470f5ce24786c.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "9WcmfWTJKn", "original": null, "number": 1, "cdate": 1576798724500, "ddate": null, "tcdate": 1576798724500, "tmdate": 1576800912008, "tddate": null, "forum": "Bke61krFvS", "replyto": "Bke61krFvS", "invitation": "ICLR.cc/2020/Conference/Paper1486/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "This paper provides a rigorous analysis of feedback alignment under two restrictions 1) that all, except the first, layers are constrained to realize monotone functions and 2) the task is binary classification. Overall, all reviewers agree that this is an interesting submission providing important results on the topic and as such all agree that it should feature at the ICLR program. Thus, I recommend acceptance. However, I ask the authors to take into account the reviewers' concerns and include a discussion about limitations (and general applicability) of this work.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning representations for binary-classification without backpropagation", "authors": ["Mathias Lechner"], "authorids": ["mathias.lechner@ist.ac.at"], "keywords": ["feedback alignment", "alternatives to backpropagation", "biologically motivated learning algorithms"], "TL;DR": "First feedback alignment algorithm with provable learning guarantees for networks with single output neuron", "abstract": "The family of feedback alignment (FA) algorithms aims to provide a more biologically motivated alternative to backpropagation (BP), by substituting the computations that are unrealistic to be implemented in physical brains.\nWhile FA algorithms have been shown to work well in practice, there is a lack of rigorous theory proofing their learning capabilities.\t\t\nHere we introduce the first feedback alignment algorithm with provable learning guarantees. In contrast to existing work, we do not require any assumption about the size or depth of the network except that it has a single output neuron, i.e., such as for binary classification tasks.\nWe show that our FA algorithm can deliver its theoretical promises in practice, surpassing the learning performance of existing FA methods and matching backpropagation in binary classification tasks.\nFinally, we demonstrate the limits of our FA variant when the number of output neurons grows beyond a certain quantity.", "pdf": "/pdf/7deda89ac18d2f3fd6786ec9f448aacefd7bc0a4.pdf", "code": "https://github.com/mlech26l/iclr_paper_mdfa", "paperhash": "lechner|learning_representations_for_binaryclassification_without_backpropagation", "_bibtex": "@inproceedings{\nLechner2020Learning,\ntitle={Learning representations for binary-classification without backpropagation},\nauthor={Mathias Lechner},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Bke61krFvS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/52131900815b24b50d7682ac5ea470f5ce24786c.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "Bke61krFvS", "replyto": "Bke61krFvS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795723620, "tmdate": 1576800275128, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1486/-/Decision"}}}, {"id": "HJlPbgIsKB", "original": null, "number": 1, "cdate": 1571672063004, "ddate": null, "tcdate": 1571672063004, "tmdate": 1574694105424, "tddate": null, "forum": "Bke61krFvS", "replyto": "Bke61krFvS", "invitation": "ICLR.cc/2020/Conference/Paper1486/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #3", "review": "------update------\n\nAfter talking to other reviews and reading the rebuttal, I am convinced that the paper contributes sufficiently to the theoretical understanding of the FA algorithm and should be accepted as a conference paper. \n\nI hope that, in the next revision, the authors could include more about the limitation of their work and potential alternatives to improve the generosity of the proposed method.\n\n-------end of the update------\n\nThe paper presented a mono-net which has only positive weights and monotonically increasing functions in between layers except for the first layer, and it can be shown that the proposed mono-net is capable of modelling any continuous functions. The modified Direct Feedback Alignment (DFA) is applied where only signs are used to update the weights. There are several issues and concerns that leads me to the following comments and concerns:\n\n(A) Why is it necessary to construct a monotonic layer which is constrained to only be able to approximate monotonic functions? Please elaborate. If the concern is about the gradient update, then there is better way of constructing such a network with limiting it being only capable of modelling monotonic functions.\n\n(B) If, given the proof in appendix, that the proposed mono-net is indeed a universal function approximator, then why do all the layers on top of the first layer have to contain only positive weights?\n\nFollowing the proof, given a neural network with only one hidden layer and hyperbolic tanh activation functions, as long as the weights in the layer that is after the tanh functions are all non-negative or non-positive, the neural network is also a universal function approximator.\n\nSince the chosen activation function monotonically increases in the input domain, the sign of the update calculated by DFA is the same as the gradient calculated by the chain rule. \n\nThe aforementioned way of constructing a neural network allows it to be able to model to change the monotonicity of the function in between layers when the two sets of weights in consecutive two layers have opposite signs. Also, the update gives the sign of the gradient calculated by backpropagation.   \n\n(C) What is the different between the proposed network along with the update rule and a network with only non-negative weights in the layers above the first layer trained with RPROP? They look exactly the same to me. \n\nIf so, another perspective of the story is that the paper emposes a non-negative constraint on the weights in a neural network, and this could be used as a baseline.\n\n(D) The proof that gives the universal approximation theorem explicitly defines the squashing function/the activation function to be bounded and this paper follows the setting, which is reflected in the experiments where the proposed network with ReLU activation functions don't work well. \n\nHowever, the expressiveness of ReLU networks has been shown to be strong and they are also universal approximators. I'd believe that with (B), it might lift the constraints brought by ReLU in the settings proposed by the paper.\n\n(E) I am not sure why for now the number of output units matters that much. The training algorithm can easily ignore other output units when samples of a specific class is presented, then after learning, the algorithm can rank the output units to make predictions. ", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1486/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1486/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning representations for binary-classification without backpropagation", "authors": ["Mathias Lechner"], "authorids": ["mathias.lechner@ist.ac.at"], "keywords": ["feedback alignment", "alternatives to backpropagation", "biologically motivated learning algorithms"], "TL;DR": "First feedback alignment algorithm with provable learning guarantees for networks with single output neuron", "abstract": "The family of feedback alignment (FA) algorithms aims to provide a more biologically motivated alternative to backpropagation (BP), by substituting the computations that are unrealistic to be implemented in physical brains.\nWhile FA algorithms have been shown to work well in practice, there is a lack of rigorous theory proofing their learning capabilities.\t\t\nHere we introduce the first feedback alignment algorithm with provable learning guarantees. In contrast to existing work, we do not require any assumption about the size or depth of the network except that it has a single output neuron, i.e., such as for binary classification tasks.\nWe show that our FA algorithm can deliver its theoretical promises in practice, surpassing the learning performance of existing FA methods and matching backpropagation in binary classification tasks.\nFinally, we demonstrate the limits of our FA variant when the number of output neurons grows beyond a certain quantity.", "pdf": "/pdf/7deda89ac18d2f3fd6786ec9f448aacefd7bc0a4.pdf", "code": "https://github.com/mlech26l/iclr_paper_mdfa", "paperhash": "lechner|learning_representations_for_binaryclassification_without_backpropagation", "_bibtex": "@inproceedings{\nLechner2020Learning,\ntitle={Learning representations for binary-classification without backpropagation},\nauthor={Mathias Lechner},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Bke61krFvS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/52131900815b24b50d7682ac5ea470f5ce24786c.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Bke61krFvS", "replyto": "Bke61krFvS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1486/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1486/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576095846222, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1486/Reviewers"], "noninvitees": [], "tcdate": 1570237736682, "tmdate": 1576095846235, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1486/-/Official_Review"}}}, {"id": "H1gC5r7mjS", "original": null, "number": 3, "cdate": 1573234069857, "ddate": null, "tcdate": 1573234069857, "tmdate": 1573234069857, "tddate": null, "forum": "Bke61krFvS", "replyto": "HJlPbgIsKB", "invitation": "ICLR.cc/2020/Conference/Paper1486/-/Official_Comment", "content": {"title": "Thank you for your comments, we hope we can address your concerns", "comment": "We thank the reviewer for their careful analysis of our manuscript.\nNotably, we appreciate the reviewer's thoughts on the methodological approach and the legibility of the paper.\n\nFirst of all, we want to clarify the statement made by the reviewer \"The modified Direct Feedback Alignment (DFA) is applied where only signs are used to update the weights.\", which is not 100% correct.\nThe mDFA update does not use the signs. Instead, the signs of the mDFA update equal the signs of the true gradient (at least for networks with a single output neuron), i.e., the angle between the update vector and the gradient stay within 90 degrees of each other.\n\nSecondly, we want to recapitulate that the premise of this paper is to seek alternatives to backpropagation for training multi-layer neural networks. Existing research on that topic consists primarily of empirical studies or requires strong assumptions to give any mathematical learning guarantee.\n\nFinally, we want to respond to the detailed concerns that the reviewer made:\n(A) The monotonic layers are necessary for the DFA update (not to be confused with the \"gradient\" update). We want the DFA update vector to point in roughly the same direction as the gradient update. \n(B) We want to develop a model that, I: is a universal approximator, and II: can be provably trained without backpropagation.\nWe were able to achieve both, I and II, by constraining all layers on top of the first layer to non-negative weight values.\n(C) Our target was particularly that the mDFA update rule and the RPROP update should be the same. The difference is that mDFA does not rely on any weight sharing and only employs a weakened form of a reciprocal error transport. Both are biological implausibilities of the backpropagation-of-error algorithm.\n(D) Indeed, it has been shown that unconstrained networks perform specifically well with ReLU activation [1]. However, we assessed in the paper, that having only non-negative weights (our constraint) and non-negative activations (ReLU), hurts the expressiveness and learning of the network. We agree that resolving this limitation is an interesting problem; nevertheless, it is not the main focus of this paper.\n(E)  Generally, the training performance of a network with backpropagation is not affected by the size of the output layer. However, as we have shown empirically and theoretically, it does matter when training the network with mDFA. For details, see supplementary materials A.3.\n\n\nWe hope that we could address most of the reviewer's concerns, and we hope the reviewer reconsiders this paper regarding their recommendation on acceptance.\n\n\n[1] Glorot, Xavier and Bordes, Antoine and Bengio, Yoshua. \"Deep sparse rectifier neural networks\". AISTAT 2011."}, "signatures": ["ICLR.cc/2020/Conference/Paper1486/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1486/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning representations for binary-classification without backpropagation", "authors": ["Mathias Lechner"], "authorids": ["mathias.lechner@ist.ac.at"], "keywords": ["feedback alignment", "alternatives to backpropagation", "biologically motivated learning algorithms"], "TL;DR": "First feedback alignment algorithm with provable learning guarantees for networks with single output neuron", "abstract": "The family of feedback alignment (FA) algorithms aims to provide a more biologically motivated alternative to backpropagation (BP), by substituting the computations that are unrealistic to be implemented in physical brains.\nWhile FA algorithms have been shown to work well in practice, there is a lack of rigorous theory proofing their learning capabilities.\t\t\nHere we introduce the first feedback alignment algorithm with provable learning guarantees. In contrast to existing work, we do not require any assumption about the size or depth of the network except that it has a single output neuron, i.e., such as for binary classification tasks.\nWe show that our FA algorithm can deliver its theoretical promises in practice, surpassing the learning performance of existing FA methods and matching backpropagation in binary classification tasks.\nFinally, we demonstrate the limits of our FA variant when the number of output neurons grows beyond a certain quantity.", "pdf": "/pdf/7deda89ac18d2f3fd6786ec9f448aacefd7bc0a4.pdf", "code": "https://github.com/mlech26l/iclr_paper_mdfa", "paperhash": "lechner|learning_representations_for_binaryclassification_without_backpropagation", "_bibtex": "@inproceedings{\nLechner2020Learning,\ntitle={Learning representations for binary-classification without backpropagation},\nauthor={Mathias Lechner},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Bke61krFvS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/52131900815b24b50d7682ac5ea470f5ce24786c.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Bke61krFvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1486/Authors", "ICLR.cc/2020/Conference/Paper1486/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1486/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1486/Reviewers", "ICLR.cc/2020/Conference/Paper1486/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1486/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1486/Authors|ICLR.cc/2020/Conference/Paper1486/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504155301, "tmdate": 1576860547589, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1486/Authors", "ICLR.cc/2020/Conference/Paper1486/Reviewers", "ICLR.cc/2020/Conference/Paper1486/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1486/-/Official_Comment"}}}, {"id": "SkxYmEmQoS", "original": null, "number": 2, "cdate": 1573233697385, "ddate": null, "tcdate": 1573233697385, "tmdate": 1573233697385, "tddate": null, "forum": "Bke61krFvS", "replyto": "BylesNeRKr", "invitation": "ICLR.cc/2020/Conference/Paper1486/-/Official_Comment", "content": {"title": "Thanks for your review, we included your suggestions into the paper", "comment": "We thank the reviewer for their thoughtful comments on our paper, especially for providing valuable related works, which we incorporated into our manuscript.\n\nWe want to respond to two points that the reviewer made:\n1. We have added a sub-section (2.3 \"Sign-symmetry algorithms\"), in which we briefly discuss the papers that the reviewer mentioned [1-3]\n2. We have added a discussion, \"How does mDFA relate to the non-negative matrix factorization?\" (top of page 6), which provides a comparison to research on non-negative matrix factorization algorithms.\nWe have updated the manuscript accordingly, and hope the reviewer supports the contributions of this work on biologically motivated learning algorithms.\n\n\n[1] Liao, Qianli and Leibo, Joel Z and Poggio, Tomaso. \"How important is weight symmetry in backpropagation?\". AAAI 2016\n[2] Moskovitz, Theodore H and Litwin-Kumar, Ashok and Abbott, LF. \"Feedback alignment in deep convolutional networks\". arXiv preprint 2018.\n[3] Xiao, Will and Chen, Honglin and Liao, Qianli and Poggio, Tomaso. \"Biologically-plausible learning algorithms can scale to large datasets\". ICLR 2019"}, "signatures": ["ICLR.cc/2020/Conference/Paper1486/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1486/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning representations for binary-classification without backpropagation", "authors": ["Mathias Lechner"], "authorids": ["mathias.lechner@ist.ac.at"], "keywords": ["feedback alignment", "alternatives to backpropagation", "biologically motivated learning algorithms"], "TL;DR": "First feedback alignment algorithm with provable learning guarantees for networks with single output neuron", "abstract": "The family of feedback alignment (FA) algorithms aims to provide a more biologically motivated alternative to backpropagation (BP), by substituting the computations that are unrealistic to be implemented in physical brains.\nWhile FA algorithms have been shown to work well in practice, there is a lack of rigorous theory proofing their learning capabilities.\t\t\nHere we introduce the first feedback alignment algorithm with provable learning guarantees. In contrast to existing work, we do not require any assumption about the size or depth of the network except that it has a single output neuron, i.e., such as for binary classification tasks.\nWe show that our FA algorithm can deliver its theoretical promises in practice, surpassing the learning performance of existing FA methods and matching backpropagation in binary classification tasks.\nFinally, we demonstrate the limits of our FA variant when the number of output neurons grows beyond a certain quantity.", "pdf": "/pdf/7deda89ac18d2f3fd6786ec9f448aacefd7bc0a4.pdf", "code": "https://github.com/mlech26l/iclr_paper_mdfa", "paperhash": "lechner|learning_representations_for_binaryclassification_without_backpropagation", "_bibtex": "@inproceedings{\nLechner2020Learning,\ntitle={Learning representations for binary-classification without backpropagation},\nauthor={Mathias Lechner},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Bke61krFvS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/52131900815b24b50d7682ac5ea470f5ce24786c.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Bke61krFvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1486/Authors", "ICLR.cc/2020/Conference/Paper1486/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1486/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1486/Reviewers", "ICLR.cc/2020/Conference/Paper1486/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1486/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1486/Authors|ICLR.cc/2020/Conference/Paper1486/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504155301, "tmdate": 1576860547589, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1486/Authors", "ICLR.cc/2020/Conference/Paper1486/Reviewers", "ICLR.cc/2020/Conference/Paper1486/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1486/-/Official_Comment"}}}, {"id": "BylAPQmQjr", "original": null, "number": 1, "cdate": 1573233510492, "ddate": null, "tcdate": 1573233510492, "tmdate": 1573233510492, "tddate": null, "forum": "Bke61krFvS", "replyto": "rJxE1TVAFH", "invitation": "ICLR.cc/2020/Conference/Paper1486/-/Official_Comment", "content": {"title": "Thanks for your review, we incorporated your feedback.", "comment": "We thank the reviewer for their thorough review of our paper, and their strong support on this research topic.\n\nWe want to respond to the three suggestions the reviewer made:\n1) We agree that the cited papers provide background and context to our work. Consequently, we have added the sub-section 2.3 \"Sign-symmetry algorithms\" to our revised submission, where we briefly discuss the papers [1-3]\n\n2) Agree. We added a statement about it at the end of the discussion **What about networks with more than one output neuron?** (bottom of page 5).\n\n3) The general trend observed in our experiments approximately aligns with the results of Bartunov et al. (2018), e.g., the accuracies achieved by the fully-connected network trained with BP on CIFAR-10.\nHowever, several subtle differences explain the discrepancies between our results and the ones reported by Bartunov et al.:\n- We evaluated 10-class subsampled CIFAR-100, whereas Bartunov et al. used standard CIFAR-10 (explains high standard deviation in our results)\n- We performed a proper training-validation-test split, whereas Bartunov et al. reported the best-achieved test accuracy.\n- The Fully-connected network of Bartunov et al. has three hidden layers (ours has only two)\n- The CNN of Bartunov et al. is larger, i.e., 256 filters in the last layer (ours has only 96) and has an additional fully-connected layer between the last convolutional layer and the output layer (ours is an \"all-convolutional-net\")\n- As the main contribution of Bartunov et al. was to benchmark various biologically inspired learning algorithms, they performed a more extensive hyperparameter search.\nThis difference is further amplified for FA and DFA, because of the observation reported in Bartunov et al. (also confirmed in our experiments) that all FA variants are relatively sensitive to hyperparameter choice.\nIn case there remain any concerns, the code to reproduce our results is publicly available.\n\nThe paper has been updated according to points 1 and 2.\n\nWe want to thank the reviewer for their time, and we hope that we could address the concerns of the reviewer.\n\nRemark: Reference to Lillicrap et al. has been updated.\n\n[1] Liao, Qianli and Leibo, Joel Z and Poggio, Tomaso. \"How important is weight symmetry in backpropagation?\". AAAI 2016\n[2] Moskovitz, Theodore H and Litwin-Kumar, Ashok and Abbott, LF. \"Feedback alignment in deep convolutional networks\". arXiv preprint 2018.\n[3] Xiao, Will and Chen, Honglin and Liao, Qianli and Poggio, Tomaso. \"Biologically-plausible learning algorithms can scale to large datasets\". ICLR 2019"}, "signatures": ["ICLR.cc/2020/Conference/Paper1486/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1486/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning representations for binary-classification without backpropagation", "authors": ["Mathias Lechner"], "authorids": ["mathias.lechner@ist.ac.at"], "keywords": ["feedback alignment", "alternatives to backpropagation", "biologically motivated learning algorithms"], "TL;DR": "First feedback alignment algorithm with provable learning guarantees for networks with single output neuron", "abstract": "The family of feedback alignment (FA) algorithms aims to provide a more biologically motivated alternative to backpropagation (BP), by substituting the computations that are unrealistic to be implemented in physical brains.\nWhile FA algorithms have been shown to work well in practice, there is a lack of rigorous theory proofing their learning capabilities.\t\t\nHere we introduce the first feedback alignment algorithm with provable learning guarantees. In contrast to existing work, we do not require any assumption about the size or depth of the network except that it has a single output neuron, i.e., such as for binary classification tasks.\nWe show that our FA algorithm can deliver its theoretical promises in practice, surpassing the learning performance of existing FA methods and matching backpropagation in binary classification tasks.\nFinally, we demonstrate the limits of our FA variant when the number of output neurons grows beyond a certain quantity.", "pdf": "/pdf/7deda89ac18d2f3fd6786ec9f448aacefd7bc0a4.pdf", "code": "https://github.com/mlech26l/iclr_paper_mdfa", "paperhash": "lechner|learning_representations_for_binaryclassification_without_backpropagation", "_bibtex": "@inproceedings{\nLechner2020Learning,\ntitle={Learning representations for binary-classification without backpropagation},\nauthor={Mathias Lechner},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Bke61krFvS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/52131900815b24b50d7682ac5ea470f5ce24786c.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Bke61krFvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1486/Authors", "ICLR.cc/2020/Conference/Paper1486/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1486/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1486/Reviewers", "ICLR.cc/2020/Conference/Paper1486/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1486/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1486/Authors|ICLR.cc/2020/Conference/Paper1486/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504155301, "tmdate": 1576860547589, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1486/Authors", "ICLR.cc/2020/Conference/Paper1486/Reviewers", "ICLR.cc/2020/Conference/Paper1486/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1486/-/Official_Comment"}}}, {"id": "BylesNeRKr", "original": null, "number": 2, "cdate": 1571845272179, "ddate": null, "tcdate": 1571845272179, "tmdate": 1572972462368, "tddate": null, "forum": "Bke61krFvS", "replyto": "Bke61krFvS", "invitation": "ICLR.cc/2020/Conference/Paper1486/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper presents an approach towards extending the capabilities of feedback alignment algorithms, that in essence replace the error backpropagation weights with random matrices.  The authors propose a particular type of network where all weights are constraint to positive values except the first layers, a monotonically increasing activation function, and where a single output neuron exists (i.e., for binary classification - empirical evidence for more output neurons is presented but not theoretically supported).  This is to enforce that the backpropagation of the (scalar) error signal to affect the magnitude of the error rather than the sign, while preserving universal approximation.  The authors also provide provable learning capabilities, and several experiments that show good performance, while also pointing out limitations in case of using multiple output neurons.\n\nThe strong point of the paper and main contribution is in terms of proposing the specific network architecture to facilitate scalar error propagation, as well as the proofs and insights on the topic.  The proposed network affects only magnitude rather than sign, and the authors demonstrate that it can do better than current FA and match BP performance.  This seems inspired from earlier work [1,2] - where e.g., in [2] improvements are observed when feedback weights share the sign but not the magnitude of feedforward nets.\n\nSummarizing, I believe that this research is interesting, and can lead to improvements in FA algorithms that could potentially be more biologically plausible, and offer advantages such as full weight update parallelization (although this is more related to the fixed weights rather than the method per-se given my understanding).  However, this also seems - at the moment - to be of limited applicability.\n\n===\nFurthermore, the introduction of the network with positive weights in the 2nd layer and on is remiscent of non-negative matrix factorization algorithms.  Can the authors establish a link to these methods, where variants with backprop have also been proposed?\n\n\n\n[1] Xiao W. et al. Biologically-Plausible Learning Algorithms Can Scale to Large Datasets, 2018\n[2] Qianli Liao, Joel Z Leibo, and Tomaso Poggio.  How important is weight symmetry in backprop-agation, AAAI 2016"}, "signatures": ["ICLR.cc/2020/Conference/Paper1486/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1486/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning representations for binary-classification without backpropagation", "authors": ["Mathias Lechner"], "authorids": ["mathias.lechner@ist.ac.at"], "keywords": ["feedback alignment", "alternatives to backpropagation", "biologically motivated learning algorithms"], "TL;DR": "First feedback alignment algorithm with provable learning guarantees for networks with single output neuron", "abstract": "The family of feedback alignment (FA) algorithms aims to provide a more biologically motivated alternative to backpropagation (BP), by substituting the computations that are unrealistic to be implemented in physical brains.\nWhile FA algorithms have been shown to work well in practice, there is a lack of rigorous theory proofing their learning capabilities.\t\t\nHere we introduce the first feedback alignment algorithm with provable learning guarantees. In contrast to existing work, we do not require any assumption about the size or depth of the network except that it has a single output neuron, i.e., such as for binary classification tasks.\nWe show that our FA algorithm can deliver its theoretical promises in practice, surpassing the learning performance of existing FA methods and matching backpropagation in binary classification tasks.\nFinally, we demonstrate the limits of our FA variant when the number of output neurons grows beyond a certain quantity.", "pdf": "/pdf/7deda89ac18d2f3fd6786ec9f448aacefd7bc0a4.pdf", "code": "https://github.com/mlech26l/iclr_paper_mdfa", "paperhash": "lechner|learning_representations_for_binaryclassification_without_backpropagation", "_bibtex": "@inproceedings{\nLechner2020Learning,\ntitle={Learning representations for binary-classification without backpropagation},\nauthor={Mathias Lechner},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Bke61krFvS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/52131900815b24b50d7682ac5ea470f5ce24786c.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Bke61krFvS", "replyto": "Bke61krFvS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1486/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1486/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576095846222, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1486/Reviewers"], "noninvitees": [], "tcdate": 1570237736682, "tmdate": 1576095846235, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1486/-/Official_Review"}}}, {"id": "rJxE1TVAFH", "original": null, "number": 3, "cdate": 1571863772009, "ddate": null, "tcdate": 1571863772009, "tmdate": 1572972462323, "tddate": null, "forum": "Bke61krFvS", "replyto": "Bke61krFvS", "invitation": "ICLR.cc/2020/Conference/Paper1486/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper examines the question of learning in neural networks with random, fixed feedback weights, a technique known as \u201cfeedback alignment\u201d. Feedback alignment was originally discovered by Lillicrap et al. (2016; Nature Communications, 7, 13276) when they were exploring potential means of solving the \u201cweight transport problem\u201d for neural networks. Essentially, the weight transport problem refers to the fact that the backpropagation-of-error algorithm requires feedback pathways for communicating errors that have synaptic weights that are symmetric to the feedforward pathway, which is biologically questionable. Feedback alignment is one approach to solving the weight transport problem, which as stated above, relies on the use of random, fixed weights for communicating the error backwards. It has been shown that in some cases, feedback alignment converges to weight updates that are reasonably well-aligned to the true gradient. Though initially considered a good potential solution for biologically realistic learning, feedback alignment both has not scaled up to difficult datasets and has no theoretical guarantees that it converges to the true gradient. This paper addresses both these issues.\n\nTo address these issues, the authors introduce two restrictions on the networks: (1) They enforce \u201cmonotone\u201d networks, meaning that following the first layer, all synaptic weights are positive. This also holds for the feedback weights. (2) They require that the task in question be a binary classification task. The authors demonstrate analytically that with these restrictions, direct feedback alignment (where the errors are communicated directly to each hidden layer by separate feedback weights) is guaranteed to follow the sign of the gradient.  (Importantly, they also show that monotone nets are universal approximators.) Empirically, they back up their analysis by demonstrating that in fully connected networks that obey these two restrictions they can get nearly as good performance as back propagation on training sets, and even better performance on tests sets sometimes. However, they also demonstrate (empirically and analytically) that violating the second requirement (by introducing more classes) leads to divergence from the gradient and major impairments in performance relative to backpropagation.\n\nUltimately, I think this is a great paper, and I think it should be accepted at ICLR. It provides some of the first rigorous analysis of feedback alignment since the original paper came out, and unlike those original analyses, it is not restricted to linear networks (which are certainly not universal function approximators). I have looked over the proofs, and they seem to all be correct. As well, I found the paper easy to read, which was nice. However, there are a few things that could be done to clarify the contributions and situate the work within the field of biological learning algorithms better:\n\n1) Though they do not include rigorous analyses, two previous papers have demonstrated empirically that feedback alignment works extremely well as long as the feedback weights share the same sign as the feedforward weights (see: Moskovitz, Theodore H., Ashok Litwin-Kumar, and L. F. Abbott. \"Feedback alignment in deep convolutional networks.\" arXiv preprint arXiv:1812.06488 (2018) and Liao, Qianli, Joel Z. Leibo, and Tomaso Poggio. \"How important is weight symmetry in backpropagation?.\" In Thirtieth AAAI Conference on Artificial Intelligence. 2016). Due to the requirement for monotone networks, this work is also providing a guarantee that the sign of feedforward and feedback weights are the same. That does not subtract substantially from the contributions of this paper, as the provision of the analytical guarantees is important. But, it is important for the authors to consider how their work relates to this past work. For example, could their analytical approach work equally well with nothing more than a sign symmetry guarantee? This should at least be discussed.\n\n2) It should be admitted somewhere in the paper that the second requirement on the networks for binary tasks is deeply unbiological. As such, it should be recognized in discussion that this paper provides some important contributions to our understanding of feedback alignment, but does not ultimately move the question of biologically realistic learning forward all that much. Indeed, the discussion at the end about applications notably ignores biology. But, rather than just ignoring it, the biological mismatch should be openly admitted.\n\n3) The results with the test sets are a little strange, at least for the tests with larger numbers of categories. In Bartunov et al. (2016), they reported not only better training set results with backprop, but also better test set results generally, than feedback alignment. Are the authors sure that their results, in say, Table 4, are not indicative of insufficient hyperparameter optimization?\n\nSmall notes:\n\n- Lillicrap et al.\u2019s paper was eventually published in Nature Communications (see citation above), and the reference should be changed to reflect this.\n\n- The discussion on the impact of convolutions could be beefed up a little bit. In particular, it could be discussed relative to the results of Moskovitz et al. (above) who show that convnets work fine with nothing but guaranteed sign symmetry."}, "signatures": ["ICLR.cc/2020/Conference/Paper1486/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1486/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning representations for binary-classification without backpropagation", "authors": ["Mathias Lechner"], "authorids": ["mathias.lechner@ist.ac.at"], "keywords": ["feedback alignment", "alternatives to backpropagation", "biologically motivated learning algorithms"], "TL;DR": "First feedback alignment algorithm with provable learning guarantees for networks with single output neuron", "abstract": "The family of feedback alignment (FA) algorithms aims to provide a more biologically motivated alternative to backpropagation (BP), by substituting the computations that are unrealistic to be implemented in physical brains.\nWhile FA algorithms have been shown to work well in practice, there is a lack of rigorous theory proofing their learning capabilities.\t\t\nHere we introduce the first feedback alignment algorithm with provable learning guarantees. In contrast to existing work, we do not require any assumption about the size or depth of the network except that it has a single output neuron, i.e., such as for binary classification tasks.\nWe show that our FA algorithm can deliver its theoretical promises in practice, surpassing the learning performance of existing FA methods and matching backpropagation in binary classification tasks.\nFinally, we demonstrate the limits of our FA variant when the number of output neurons grows beyond a certain quantity.", "pdf": "/pdf/7deda89ac18d2f3fd6786ec9f448aacefd7bc0a4.pdf", "code": "https://github.com/mlech26l/iclr_paper_mdfa", "paperhash": "lechner|learning_representations_for_binaryclassification_without_backpropagation", "_bibtex": "@inproceedings{\nLechner2020Learning,\ntitle={Learning representations for binary-classification without backpropagation},\nauthor={Mathias Lechner},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Bke61krFvS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/52131900815b24b50d7682ac5ea470f5ce24786c.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Bke61krFvS", "replyto": "Bke61krFvS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1486/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1486/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576095846222, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1486/Reviewers"], "noninvitees": [], "tcdate": 1570237736682, "tmdate": 1576095846235, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1486/-/Official_Review"}}}], "count": 8}