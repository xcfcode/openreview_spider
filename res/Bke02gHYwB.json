{"notes": [{"id": "Bke02gHYwB", "original": "HyeFagZKDH", "number": 2557, "cdate": 1569439926235, "ddate": null, "tcdate": 1569439926235, "tmdate": 1577168247893, "tddate": null, "forum": "Bke02gHYwB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Learn Interpretable Word Embeddings Efficiently with von Mises-Fisher Distribution", "authors": ["Minghong Yao", "Liansheng Zhuang", "Houqiang Li", "Jian Yang", "Shafei Wang"], "authorids": ["mhyao1@mail.ustc.edu.cn", "lszhuang@ustc.edu.cn", "lihq@ustc.edu.cn", "nanwuyaoshi@163.com", "rockingsandstorm@163.com"], "keywords": ["word embedding", "natural language processing"], "TL;DR": "Learn Interpretable Word Embeddings Efficiently with von Mises-Fisher Distribution", "abstract": "Word embedding plays a key role in various tasks of natural language processing. However, the dominant word embedding models don't explain what information is carried with the resulting embeddings. To generate interpretable word embeddings we intend to replace the word vector with a probability density distribution. The insight here is that if we regularize the mixture distribution of all words to be uniform, then we can prove that the inner product between word embeddings represent the point-wise mutual information between words. Moreover, our model can also handle polysemy. Each word's probability density distribution will generate different vectors for its various meanings. We have evaluated our model in several word similarity tasks. Results show that our model can outperform the dominant models consistently in these tasks.", "pdf": "/pdf/8a91761b4af124d99dbfc0080b285952021ef089.pdf", "paperhash": "yao|learn_interpretable_word_embeddings_efficiently_with_von_misesfisher_distribution", "original_pdf": "/attachment/8a91761b4af124d99dbfc0080b285952021ef089.pdf", "_bibtex": "@misc{\nyao2020learn,\ntitle={Learn Interpretable Word Embeddings Efficiently with von Mises-Fisher Distribution},\nauthor={Minghong Yao and Liansheng Zhuang and Houqiang Li and Jian Yang and Shafei Wang},\nyear={2020},\nurl={https://openreview.net/forum?id=Bke02gHYwB}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "g-YhRcFdh", "original": null, "number": 1, "cdate": 1576798752074, "ddate": null, "tcdate": 1576798752074, "tmdate": 1576800883567, "tddate": null, "forum": "Bke02gHYwB", "replyto": "Bke02gHYwB", "invitation": "ICLR.cc/2020/Conference/Paper2557/-/Decision", "content": {"decision": "Reject", "comment": "The paper presents an approach to learning interpretable word embeddings. The reviewers put this in the lower half of the submissions. One reason seems to be the size of the training corpora used in the experiments, as well as the limited number of experiments; another that the claim of interpretability seems over-stated. There's also a lack of comparison to related work. I also think it would be interesting to move beyond the standard benchmarks - and either use word embeddings downstream or learn word embeddings for multiple languages [you should do this, regardless] and use Procrustes analysis or the like to learn a mapping: A good embedding algorithm should induce more linearly alignable embedding spaces. \n\nNB: While the authors cite other work by these authors, [0] seems relevant, too. Other related work: [1-4]. \n\n[0] https://www.aclweb.org/anthology/Q15-1016.pdf\n[1] https://www.aclweb.org/anthology/Q16-1020.pdf\n[2] https://www.aclweb.org/anthology/W19-4329.pdf\n[3] https://www.aclweb.org/anthology/D17-1198/\n[4]\u00a0https://www.aclweb.org/anthology/D15-1183.pdf", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learn Interpretable Word Embeddings Efficiently with von Mises-Fisher Distribution", "authors": ["Minghong Yao", "Liansheng Zhuang", "Houqiang Li", "Jian Yang", "Shafei Wang"], "authorids": ["mhyao1@mail.ustc.edu.cn", "lszhuang@ustc.edu.cn", "lihq@ustc.edu.cn", "nanwuyaoshi@163.com", "rockingsandstorm@163.com"], "keywords": ["word embedding", "natural language processing"], "TL;DR": "Learn Interpretable Word Embeddings Efficiently with von Mises-Fisher Distribution", "abstract": "Word embedding plays a key role in various tasks of natural language processing. However, the dominant word embedding models don't explain what information is carried with the resulting embeddings. To generate interpretable word embeddings we intend to replace the word vector with a probability density distribution. The insight here is that if we regularize the mixture distribution of all words to be uniform, then we can prove that the inner product between word embeddings represent the point-wise mutual information between words. Moreover, our model can also handle polysemy. Each word's probability density distribution will generate different vectors for its various meanings. We have evaluated our model in several word similarity tasks. Results show that our model can outperform the dominant models consistently in these tasks.", "pdf": "/pdf/8a91761b4af124d99dbfc0080b285952021ef089.pdf", "paperhash": "yao|learn_interpretable_word_embeddings_efficiently_with_von_misesfisher_distribution", "original_pdf": "/attachment/8a91761b4af124d99dbfc0080b285952021ef089.pdf", "_bibtex": "@misc{\nyao2020learn,\ntitle={Learn Interpretable Word Embeddings Efficiently with von Mises-Fisher Distribution},\nauthor={Minghong Yao and Liansheng Zhuang and Houqiang Li and Jian Yang and Shafei Wang},\nyear={2020},\nurl={https://openreview.net/forum?id=Bke02gHYwB}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "Bke02gHYwB", "replyto": "Bke02gHYwB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795724527, "tmdate": 1576800276188, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2557/-/Decision"}}}, {"id": "rkxk7rD6tS", "original": null, "number": 2, "cdate": 1571808534616, "ddate": null, "tcdate": 1571808534616, "tmdate": 1572972322793, "tddate": null, "forum": "Bke02gHYwB", "replyto": "Bke02gHYwB", "invitation": "ICLR.cc/2020/Conference/Paper2557/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper addresses the problem the problem in word embeddings where \"word-word\" or \"context-context\" inner products are relied upon in practice when the embeddings are usually only optimized for good properties of \"word-context\" inner products. The new approach to training word embeddings addresses interpretability in the sense of having a theoretically grounded interpretation for the good properties that the inner products should possess -- namely that they capture the PMI between words. By changing the embedding training procedure (to include a step that samples vectors from each word's distribution representation) this interpretation is made possible without problematic theoretical assumptions. Now trained for the purpose they are intended to be used for, the approach yields strong SOTA results on the non-small challenge datasets.\n\nThe word is well exceptionally well motivated (we should directly optimize for the properties we want!) and situated in the literature. The opposition of Arora 2016 (getting at \"interpretable\" word embeddings with a latent variable model) and Mimno & Thompson 2017 (problematic assumptions in skip-gram embeddings) is particularly convincing. The vMF distribution is underexamined as tool for analyzing high-dimensional semantic vector distributions, and it is an excellent fit for the purposes of this project.\n\nTo back up claims of theoretical interpretability, a derivation (building on Ma 2017) proceeds without problematic leap thanks to a property introduced by a carefully selected (if straightforward) regularization term. This reviewer did not read the appendix (not sure where this would be located -- first time reviewing with OpenReview here), but the intuition doesn't seem to be much different than Ma.\n\nTo back up claims of applied performance, appropriate experiments are conducted on multiple evaluation datasets. The results seem to be fairly interpreted.\n\nThis reviewer decides to accept this paper for its balance of theoretical and empirical contributions and for the role it might play in reducing dependence on mysticisms in word embeddings (relying on accidental / uncharacterized properties of previous embedding strategies).\n\nSuggestions for improvement:\n- Run a spell checker. It will catch a large number of problems that weren't ever big enough to hurt my appreciation of the paper.\n- Consider the creation of an adversarial dataset (of ~3 words in ~3 contexts) where techniques that optimize for the wrong thing will catastrophically fail but the proposed approach succeeds.\n- Write one or two more sentences about the fate of \\kappa_c -- is it ever updated or am I just missing something? Making a bigger point about leaving \\kappa un-optimized shows there is room for additional depth in this line of thinking. (If you start learning concentration parameters, check out the Kent distribution for an even more expressive distribution on the unit hypersphere: https://en.wikipedia.org/wiki/Kent_distribution)\n- Figure out what happened to your appendix.\n- Watch out that readers/reviewers of similar work may try to read the word \"interpretable\" in the title as a reference to the much broader topic of interpretability in ML related to explaining a model's behavior. Is there a synonym for \"interpretable\" that won't raise this false link?"}, "signatures": ["ICLR.cc/2020/Conference/Paper2557/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2557/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learn Interpretable Word Embeddings Efficiently with von Mises-Fisher Distribution", "authors": ["Minghong Yao", "Liansheng Zhuang", "Houqiang Li", "Jian Yang", "Shafei Wang"], "authorids": ["mhyao1@mail.ustc.edu.cn", "lszhuang@ustc.edu.cn", "lihq@ustc.edu.cn", "nanwuyaoshi@163.com", "rockingsandstorm@163.com"], "keywords": ["word embedding", "natural language processing"], "TL;DR": "Learn Interpretable Word Embeddings Efficiently with von Mises-Fisher Distribution", "abstract": "Word embedding plays a key role in various tasks of natural language processing. However, the dominant word embedding models don't explain what information is carried with the resulting embeddings. To generate interpretable word embeddings we intend to replace the word vector with a probability density distribution. The insight here is that if we regularize the mixture distribution of all words to be uniform, then we can prove that the inner product between word embeddings represent the point-wise mutual information between words. Moreover, our model can also handle polysemy. Each word's probability density distribution will generate different vectors for its various meanings. We have evaluated our model in several word similarity tasks. Results show that our model can outperform the dominant models consistently in these tasks.", "pdf": "/pdf/8a91761b4af124d99dbfc0080b285952021ef089.pdf", "paperhash": "yao|learn_interpretable_word_embeddings_efficiently_with_von_misesfisher_distribution", "original_pdf": "/attachment/8a91761b4af124d99dbfc0080b285952021ef089.pdf", "_bibtex": "@misc{\nyao2020learn,\ntitle={Learn Interpretable Word Embeddings Efficiently with von Mises-Fisher Distribution},\nauthor={Minghong Yao and Liansheng Zhuang and Houqiang Li and Jian Yang and Shafei Wang},\nyear={2020},\nurl={https://openreview.net/forum?id=Bke02gHYwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Bke02gHYwB", "replyto": "Bke02gHYwB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2557/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2557/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575626662834, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2557/Reviewers"], "noninvitees": [], "tcdate": 1570237721145, "tmdate": 1575626662849, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2557/-/Official_Review"}}}, {"id": "ryg1XGKOcB", "original": null, "number": 3, "cdate": 1572536855299, "ddate": null, "tcdate": 1572536855299, "tmdate": 1572972322749, "tddate": null, "forum": "Bke02gHYwB", "replyto": "Bke02gHYwB", "invitation": "ICLR.cc/2020/Conference/Paper2557/-/Official_Review", "content": {"rating": "1: Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper is about learning word embeddings. In contrast to previous work (word2vec's Skipgram) where a word is represented by a fixed word and context vector, here each word is represented by a von Mises-Fisher distribution and a context vector. \nThe paper claims that the resulting embeddings are more interpretable, and that the inner product of two context vectors represents their point-wise mutual information.\n\nMy main concerns are the following:\n\n- Representing words by distributions is not a novel idea; it was previously done by Brazinskas et al. (2017): Embedding words as distributions with a bayesian skip-gram model (BSG). They however used Gaussian distributions and not von Mises-Fisher. BSG is acknowledged in the paper, but only small scale comparisons are performed (15 million) while the BSG paper uses a lot larger data sets. There is therefore not a meaningful comparison to the most relevant previous work.\n- Word similarity experiments are not enough to justify this approach. BSG at least showed the strength of using distributions to represent words by showing that different samples could constitute different word senses/meanings. There is no such analysis here.\n- The claim that the resulting representations are more \"interpretable\" is not backed up by any evidence at all, even though the word \"interpretable\" is in the title and the list of contributions.\n\nGenerally this paper could benefit from proof reading and editing."}, "signatures": ["ICLR.cc/2020/Conference/Paper2557/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2557/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learn Interpretable Word Embeddings Efficiently with von Mises-Fisher Distribution", "authors": ["Minghong Yao", "Liansheng Zhuang", "Houqiang Li", "Jian Yang", "Shafei Wang"], "authorids": ["mhyao1@mail.ustc.edu.cn", "lszhuang@ustc.edu.cn", "lihq@ustc.edu.cn", "nanwuyaoshi@163.com", "rockingsandstorm@163.com"], "keywords": ["word embedding", "natural language processing"], "TL;DR": "Learn Interpretable Word Embeddings Efficiently with von Mises-Fisher Distribution", "abstract": "Word embedding plays a key role in various tasks of natural language processing. However, the dominant word embedding models don't explain what information is carried with the resulting embeddings. To generate interpretable word embeddings we intend to replace the word vector with a probability density distribution. The insight here is that if we regularize the mixture distribution of all words to be uniform, then we can prove that the inner product between word embeddings represent the point-wise mutual information between words. Moreover, our model can also handle polysemy. Each word's probability density distribution will generate different vectors for its various meanings. We have evaluated our model in several word similarity tasks. Results show that our model can outperform the dominant models consistently in these tasks.", "pdf": "/pdf/8a91761b4af124d99dbfc0080b285952021ef089.pdf", "paperhash": "yao|learn_interpretable_word_embeddings_efficiently_with_von_misesfisher_distribution", "original_pdf": "/attachment/8a91761b4af124d99dbfc0080b285952021ef089.pdf", "_bibtex": "@misc{\nyao2020learn,\ntitle={Learn Interpretable Word Embeddings Efficiently with von Mises-Fisher Distribution},\nauthor={Minghong Yao and Liansheng Zhuang and Houqiang Li and Jian Yang and Shafei Wang},\nyear={2020},\nurl={https://openreview.net/forum?id=Bke02gHYwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Bke02gHYwB", "replyto": "Bke02gHYwB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2557/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2557/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575626662834, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2557/Reviewers"], "noninvitees": [], "tcdate": 1570237721145, "tmdate": 1575626662849, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2557/-/Official_Review"}}}, {"id": "Byg0bMQ5YS", "original": null, "number": 1, "cdate": 1571594758132, "ddate": null, "tcdate": 1571594758132, "tmdate": 1572972322702, "tddate": null, "forum": "Bke02gHYwB", "replyto": "Bke02gHYwB", "invitation": "ICLR.cc/2020/Conference/Paper2557/-/Official_Review", "content": {"rating": "1: Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "Summary: This paper proposed a variational word embedding method, by using vMF distribution as prior and adding an entropy regularization term.\n\nStrengths:\n[+] In the experiment parts, the authors describe the experiment settings in detail.\n[+] Detailed descriptions for each equation is given.\n\nWeaknesses:\n[-] Template: It seems that the authors use the wrong template, which is not the template for ICLR2020.\n[-] Appendix: Although the author mentioned 'appendix' many times, I cannot see the appendix.\n[-] Motivation: The connection between motivation and proposed method seems weak. The authors argue 'interpretable' in their title and abstract, but their method and experiments do not show this point explicitly. \n\nQuestions\n[.] The experiments seem a little weak. The vocabulary size is 10K and the corpus is not so big, and I wonder whether the performance of the proposed method will be better for large corpus and longer training time.\n[.] For Equation 8, we should have a guarantee on the concentration of partition functions. Is it still true for vMF distribution? \n[.] What is the advantage of vMF distribution? "}, "signatures": ["ICLR.cc/2020/Conference/Paper2557/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2557/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learn Interpretable Word Embeddings Efficiently with von Mises-Fisher Distribution", "authors": ["Minghong Yao", "Liansheng Zhuang", "Houqiang Li", "Jian Yang", "Shafei Wang"], "authorids": ["mhyao1@mail.ustc.edu.cn", "lszhuang@ustc.edu.cn", "lihq@ustc.edu.cn", "nanwuyaoshi@163.com", "rockingsandstorm@163.com"], "keywords": ["word embedding", "natural language processing"], "TL;DR": "Learn Interpretable Word Embeddings Efficiently with von Mises-Fisher Distribution", "abstract": "Word embedding plays a key role in various tasks of natural language processing. However, the dominant word embedding models don't explain what information is carried with the resulting embeddings. To generate interpretable word embeddings we intend to replace the word vector with a probability density distribution. The insight here is that if we regularize the mixture distribution of all words to be uniform, then we can prove that the inner product between word embeddings represent the point-wise mutual information between words. Moreover, our model can also handle polysemy. Each word's probability density distribution will generate different vectors for its various meanings. We have evaluated our model in several word similarity tasks. Results show that our model can outperform the dominant models consistently in these tasks.", "pdf": "/pdf/8a91761b4af124d99dbfc0080b285952021ef089.pdf", "paperhash": "yao|learn_interpretable_word_embeddings_efficiently_with_von_misesfisher_distribution", "original_pdf": "/attachment/8a91761b4af124d99dbfc0080b285952021ef089.pdf", "_bibtex": "@misc{\nyao2020learn,\ntitle={Learn Interpretable Word Embeddings Efficiently with von Mises-Fisher Distribution},\nauthor={Minghong Yao and Liansheng Zhuang and Houqiang Li and Jian Yang and Shafei Wang},\nyear={2020},\nurl={https://openreview.net/forum?id=Bke02gHYwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Bke02gHYwB", "replyto": "Bke02gHYwB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2557/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2557/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575626662834, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2557/Reviewers"], "noninvitees": [], "tcdate": 1570237721145, "tmdate": 1575626662849, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2557/-/Official_Review"}}}], "count": 5}