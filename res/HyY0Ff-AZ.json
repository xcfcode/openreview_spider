{"notes": [{"tddate": null, "ddate": null, "tmdate": 1518730161643, "tcdate": 1509136851803, "number": 895, "cdate": 1518730161633, "id": "HyY0Ff-AZ", "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "forum": "HyY0Ff-AZ", "original": "HJN2FGZAb", "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference"], "content": {"title": "Representing Entropy : A short proof of the equivalence between soft Q-learning and policy gradients", "abstract": "Two main families of reinforcement learning algorithms, Q-learning and policy gradients, have recently been proven to be equivalent when using a softmax relaxation on one part, and an entropic regularization on the other. We relate this result to the well-known convex duality of Shannon entropy and the softmax function. Such a result is also known as the Donsker-Varadhan formula. This provides a short proof of the equivalence. We then interpret this duality further, and use ideas of convex analysis to prove a new policy inequality relative to soft Q-learning.", "pdf": "/pdf/d8235f3f5648e3f3f3f2a8ea7fd28d1e533f1d1e.pdf", "TL;DR": "A short proof of the equivalence of soft Q-learning and policy gradients.", "paperhash": "richemond|representing_entropy_a_short_proof_of_the_equivalence_between_soft_qlearning_and_policy_gradients", "_bibtex": "@misc{\nh.2018representing,\ntitle={Representing Entropy : A short proof of the equivalence between soft Q-learning and policy gradients},\nauthor={Pierre H. Richemond and Brendan Maginnis},\nyear={2018},\nurl={https://openreview.net/forum?id=HyY0Ff-AZ},\n}", "keywords": ["soft Q-learning", "policy gradients", "entropy", "Legendre transformation", "duality", "convex analysis", "Donsker-Varadhan"], "authors": ["Pierre H. Richemond", "Brendan Maginnis"], "authorids": ["phr17@imperial.ac.uk", "b.maginnis@imperial.ac.uk"]}, "nonreaders": [], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1506717071958, "id": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Conference"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference"]}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"authors": {"required": false, "order": 1, "values-regex": ".*", "description": "Comma separated list of author names, as they appear in the paper."}, "authorids": {"required": false, "order": 2, "values-regex": ".*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "cdate": 1506717071958}}, "tauthor": "ICLR.cc/2018/Conference"}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1517260085527, "tcdate": 1517249859042, "number": 563, "cdate": 1517249859020, "id": "SJoHB1aSG", "invitation": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "forum": "HyY0Ff-AZ", "replyto": "HyY0Ff-AZ", "signatures": ["ICLR.cc/2018/Conference/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Program_Chairs"], "content": {"decision": "Reject", "title": "ICLR 2018 Conference Acceptance Decision", "comment": "The reviewers point out that this is a well known result and is not novel."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Representing Entropy : A short proof of the equivalence between soft Q-learning and policy gradients", "abstract": "Two main families of reinforcement learning algorithms, Q-learning and policy gradients, have recently been proven to be equivalent when using a softmax relaxation on one part, and an entropic regularization on the other. We relate this result to the well-known convex duality of Shannon entropy and the softmax function. Such a result is also known as the Donsker-Varadhan formula. This provides a short proof of the equivalence. We then interpret this duality further, and use ideas of convex analysis to prove a new policy inequality relative to soft Q-learning.", "pdf": "/pdf/d8235f3f5648e3f3f3f2a8ea7fd28d1e533f1d1e.pdf", "TL;DR": "A short proof of the equivalence of soft Q-learning and policy gradients.", "paperhash": "richemond|representing_entropy_a_short_proof_of_the_equivalence_between_soft_qlearning_and_policy_gradients", "_bibtex": "@misc{\nh.2018representing,\ntitle={Representing Entropy : A short proof of the equivalence between soft Q-learning and policy gradients},\nauthor={Pierre H. Richemond and Brendan Maginnis},\nyear={2018},\nurl={https://openreview.net/forum?id=HyY0Ff-AZ},\n}", "keywords": ["soft Q-learning", "policy gradients", "entropy", "Legendre transformation", "duality", "convex analysis", "Donsker-Varadhan"], "authors": ["Pierre H. Richemond", "Brendan Maginnis"], "authorids": ["phr17@imperial.ac.uk", "b.maginnis@imperial.ac.uk"]}, "tags": [], "invitation": {"id": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "rdate": null, "ddate": null, "expdate": 1541175629000, "duedate": null, "tmdate": 1541177635767, "tddate": null, "super": null, "final": null, "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": {"values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Conference/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Conference Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": [], "noninvitees": [], "writers": ["ICLR.cc/2018/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1541177635767}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642528313, "tcdate": 1511763616931, "number": 1, "cdate": 1511763616931, "id": "r1FjCQKxG", "invitation": "ICLR.cc/2018/Conference/-/Paper895/Official_Review", "forum": "HyY0Ff-AZ", "replyto": "HyY0Ff-AZ", "signatures": ["ICLR.cc/2018/Conference/Paper895/AnonReviewer1"], "readers": ["everyone"], "content": {"title": "main results are already known in the context of risk-sensitive control", "rating": "5: Marginally below acceptance threshold", "review": "This paper uses a well-known variational representation of the relative entropy (the so-called Donsker-Varadhan formula) to derive an expression for the Bellman error with entropy regularization in terms of a certain log-partition function. This is stated in Equation (13) in the paper. However, this precise representation of the Bellman error (with costs instead of rewards and with minimization instead of maximization) has been known in the literature on risk-sensitive control, see, e.g., P. D. Pra, L. Meneghini, and W. J. Runggaldier, \u201cConnections between stochastic control and dynamic games,\u201d Math. Control Signals Systems, vol. 9, pp. 303\u2013326, 1996. The same applies to contraction results for the \"softmax\" Bellman operator -- these results are not novel at all, see, e.g., D. Hernandez-Hernandez and S. I. Marcus, \u201cRisk sensitive control of Markov processes in countable state space,\u201d Systems and Control Letters, vol. 29, pp. 147\u2013155, 1996.\n\nAlso, there are some errors in the paper: for example, the functional of $\\pi(a|s)$ in Eq. (2) is concave, not convex, since the expression for the Shannon entropy in Eq. (3) has the wrong sign.", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Representing Entropy : A short proof of the equivalence between soft Q-learning and policy gradients", "abstract": "Two main families of reinforcement learning algorithms, Q-learning and policy gradients, have recently been proven to be equivalent when using a softmax relaxation on one part, and an entropic regularization on the other. We relate this result to the well-known convex duality of Shannon entropy and the softmax function. Such a result is also known as the Donsker-Varadhan formula. This provides a short proof of the equivalence. We then interpret this duality further, and use ideas of convex analysis to prove a new policy inequality relative to soft Q-learning.", "pdf": "/pdf/d8235f3f5648e3f3f3f2a8ea7fd28d1e533f1d1e.pdf", "TL;DR": "A short proof of the equivalence of soft Q-learning and policy gradients.", "paperhash": "richemond|representing_entropy_a_short_proof_of_the_equivalence_between_soft_qlearning_and_policy_gradients", "_bibtex": "@misc{\nh.2018representing,\ntitle={Representing Entropy : A short proof of the equivalence between soft Q-learning and policy gradients},\nauthor={Pierre H. Richemond and Brendan Maginnis},\nyear={2018},\nurl={https://openreview.net/forum?id=HyY0Ff-AZ},\n}", "keywords": ["soft Q-learning", "policy gradients", "entropy", "Legendre transformation", "duality", "convex analysis", "Donsker-Varadhan"], "authors": ["Pierre H. Richemond", "Brendan Maginnis"], "authorids": ["phr17@imperial.ac.uk", "b.maginnis@imperial.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642528203, "id": "ICLR.cc/2018/Conference/-/Paper895/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper895/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper895/AnonReviewer1", "ICLR.cc/2018/Conference/Paper895/AnonReviewer3", "ICLR.cc/2018/Conference/Paper895/AnonReviewer2"], "reply": {"forum": "HyY0Ff-AZ", "replyto": "HyY0Ff-AZ", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper895/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642528203}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642528261, "tcdate": 1511771458020, "number": 2, "cdate": 1511771458020, "id": "rJ9BTHFez", "invitation": "ICLR.cc/2018/Conference/-/Paper895/Official_Review", "forum": "HyY0Ff-AZ", "replyto": "HyY0Ff-AZ", "signatures": ["ICLR.cc/2018/Conference/Paper895/AnonReviewer3"], "readers": ["everyone"], "content": {"title": "Empty paper.", "rating": "2: Strong rejection", "review": "Summary\n*******\nThe paper provides a collection of existing results in statistics.\n\nComments\n********\nPage 1: references to Q-learning and Policy-gradients look awkwardly recent, given that these have been around for several decades.\n\nI dont get what is the novelty in this paper. There is no doubt that all the tools that are detailed here are extremely useful and powerful results in mathematical statistics. But they are all known.\n\nThe Gibbs variational principle is folklore, Proposition 1,2 are available in all good text books on the topic, \nand Proposition 4 is nothing but a transportation Lemma.\nNow, Proposition 3 is about soft-Bellman operators. This perhaps is less standard because contraction property of soft-Bellman operator in infinite norm is more recent than for Bellman operators.\nBut as mentioned by the authors, this is not new either. \nAlso I don't really see the point of providing the proofs of these results in the main material, and not for instance in appendix, as there is no novelty either in the proof techniques.\n\nI don't get the sentence \"we have restricted so far the proof in the bandit setting\": bandits are not even mentioned earlier.\n\nDecision\n********\nI am sorry but unless I missed something (that then should be clarified) this seems to be an empty paper: Strong reject.", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Representing Entropy : A short proof of the equivalence between soft Q-learning and policy gradients", "abstract": "Two main families of reinforcement learning algorithms, Q-learning and policy gradients, have recently been proven to be equivalent when using a softmax relaxation on one part, and an entropic regularization on the other. We relate this result to the well-known convex duality of Shannon entropy and the softmax function. Such a result is also known as the Donsker-Varadhan formula. This provides a short proof of the equivalence. We then interpret this duality further, and use ideas of convex analysis to prove a new policy inequality relative to soft Q-learning.", "pdf": "/pdf/d8235f3f5648e3f3f3f2a8ea7fd28d1e533f1d1e.pdf", "TL;DR": "A short proof of the equivalence of soft Q-learning and policy gradients.", "paperhash": "richemond|representing_entropy_a_short_proof_of_the_equivalence_between_soft_qlearning_and_policy_gradients", "_bibtex": "@misc{\nh.2018representing,\ntitle={Representing Entropy : A short proof of the equivalence between soft Q-learning and policy gradients},\nauthor={Pierre H. Richemond and Brendan Maginnis},\nyear={2018},\nurl={https://openreview.net/forum?id=HyY0Ff-AZ},\n}", "keywords": ["soft Q-learning", "policy gradients", "entropy", "Legendre transformation", "duality", "convex analysis", "Donsker-Varadhan"], "authors": ["Pierre H. Richemond", "Brendan Maginnis"], "authorids": ["phr17@imperial.ac.uk", "b.maginnis@imperial.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642528203, "id": "ICLR.cc/2018/Conference/-/Paper895/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper895/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper895/AnonReviewer1", "ICLR.cc/2018/Conference/Paper895/AnonReviewer3", "ICLR.cc/2018/Conference/Paper895/AnonReviewer2"], "reply": {"forum": "HyY0Ff-AZ", "replyto": "HyY0Ff-AZ", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper895/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642528203}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642528225, "tcdate": 1512158015632, "number": 3, "cdate": 1512158015632, "id": "B1_rmNyZz", "invitation": "ICLR.cc/2018/Conference/-/Paper895/Official_Review", "forum": "HyY0Ff-AZ", "replyto": "HyY0Ff-AZ", "signatures": ["ICLR.cc/2018/Conference/Paper895/AnonReviewer2"], "readers": ["everyone"], "content": {"title": "The paper presents a concise proof of the equivalence between entropy-regularised policy gradients and soft-max Q learning. It first shows the results in the bandit-case and extend it to the MDP case. The presentation is a little bit more general and related to works outside the field of machine learning. However, the results are not new and are a survey of previous works (e.g., Neu 2017).", "rating": "5: Marginally below acceptance threshold", "review": "Clarity: The paper is easy to follow and presents quite well the equivalence. \n\nOriginality: The results presented are well known and there is no clear contribution algorithmic-wise to the field of RL. The originality comes from the conciseness of the proof and how it relates to other works outside ML. Thus, this contribution seems minor and out of the scope of the conference which focus on representation learning for ML and RL.\n\nSuggestion: I strongly suggest the authors to work on a more detailed proof for the RL case explaining for instance the minimal conditions (on the reward, on the ergodicity of the MDP) in which the equivalence holds and submit it to a more theoretically oriented conference such as COLT or NIPS.   \n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Representing Entropy : A short proof of the equivalence between soft Q-learning and policy gradients", "abstract": "Two main families of reinforcement learning algorithms, Q-learning and policy gradients, have recently been proven to be equivalent when using a softmax relaxation on one part, and an entropic regularization on the other. We relate this result to the well-known convex duality of Shannon entropy and the softmax function. Such a result is also known as the Donsker-Varadhan formula. This provides a short proof of the equivalence. We then interpret this duality further, and use ideas of convex analysis to prove a new policy inequality relative to soft Q-learning.", "pdf": "/pdf/d8235f3f5648e3f3f3f2a8ea7fd28d1e533f1d1e.pdf", "TL;DR": "A short proof of the equivalence of soft Q-learning and policy gradients.", "paperhash": "richemond|representing_entropy_a_short_proof_of_the_equivalence_between_soft_qlearning_and_policy_gradients", "_bibtex": "@misc{\nh.2018representing,\ntitle={Representing Entropy : A short proof of the equivalence between soft Q-learning and policy gradients},\nauthor={Pierre H. Richemond and Brendan Maginnis},\nyear={2018},\nurl={https://openreview.net/forum?id=HyY0Ff-AZ},\n}", "keywords": ["soft Q-learning", "policy gradients", "entropy", "Legendre transformation", "duality", "convex analysis", "Donsker-Varadhan"], "authors": ["Pierre H. Richemond", "Brendan Maginnis"], "authorids": ["phr17@imperial.ac.uk", "b.maginnis@imperial.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642528203, "id": "ICLR.cc/2018/Conference/-/Paper895/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper895/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper895/AnonReviewer1", "ICLR.cc/2018/Conference/Paper895/AnonReviewer3", "ICLR.cc/2018/Conference/Paper895/AnonReviewer2"], "reply": {"forum": "HyY0Ff-AZ", "replyto": "HyY0Ff-AZ", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper895/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642528203}}}, {"tddate": null, "ddate": null, "tmdate": 1515186297526, "tcdate": 1515186297526, "number": 3, "cdate": 1515186297526, "id": "S1-FOw67M", "invitation": "ICLR.cc/2018/Conference/-/Paper895/Official_Comment", "forum": "HyY0Ff-AZ", "replyto": "rJ9BTHFez", "signatures": ["ICLR.cc/2018/Conference/Paper895/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper895/Authors"], "content": {"title": "The method of the proof", "comment": "We thank the reviewer for their evaluation, and acknowledge it. However, we are not in full agreement with the specific concern voiced here - our shorter proof indeed uses fairly well-known statistical tools, for instance in the Legendre transform of the log-Laplace. But of the several papers highlighting the equivalence of soft Q-learning and entropy regularized policy gradients published this year (at least three, see for instance Nachum et al.'s https://arxiv.org/abs/1702.08892 or Schulman et al.'s https://arxiv.org/abs/1704.06440, or the anonymous submission https://openreview.net/pdf?id=HJjvxl-Cb), none to our knowledge used this representation formula that expedites the proof singularly. The technique gives very intuitively soft Q-learning as a Cramer-Chernoff transform, and could be applied to other regularizers ; furthermore, the paper highlights a connection with large deviations that could be helpful in future work, for instance by applying relevant changes of measure.\n\nThe sentence 'we have restricted so far the proof in the bandit setting' refers to applying the representation formula in the one-step return case for clarity ; this is terminology used for instance in Schulman et al.'s article https://arxiv.org/abs/1704.06440.\n\nWe do agree - and state in the abstract -  that the proof of equivalence of soft Q-learning and entropy regularization is not a novelty of our article."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Representing Entropy : A short proof of the equivalence between soft Q-learning and policy gradients", "abstract": "Two main families of reinforcement learning algorithms, Q-learning and policy gradients, have recently been proven to be equivalent when using a softmax relaxation on one part, and an entropic regularization on the other. We relate this result to the well-known convex duality of Shannon entropy and the softmax function. Such a result is also known as the Donsker-Varadhan formula. This provides a short proof of the equivalence. We then interpret this duality further, and use ideas of convex analysis to prove a new policy inequality relative to soft Q-learning.", "pdf": "/pdf/d8235f3f5648e3f3f3f2a8ea7fd28d1e533f1d1e.pdf", "TL;DR": "A short proof of the equivalence of soft Q-learning and policy gradients.", "paperhash": "richemond|representing_entropy_a_short_proof_of_the_equivalence_between_soft_qlearning_and_policy_gradients", "_bibtex": "@misc{\nh.2018representing,\ntitle={Representing Entropy : A short proof of the equivalence between soft Q-learning and policy gradients},\nauthor={Pierre H. Richemond and Brendan Maginnis},\nyear={2018},\nurl={https://openreview.net/forum?id=HyY0Ff-AZ},\n}", "keywords": ["soft Q-learning", "policy gradients", "entropy", "Legendre transformation", "duality", "convex analysis", "Donsker-Varadhan"], "authors": ["Pierre H. Richemond", "Brendan Maginnis"], "authorids": ["phr17@imperial.ac.uk", "b.maginnis@imperial.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825726004, "id": "ICLR.cc/2018/Conference/-/Paper895/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "HyY0Ff-AZ", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper895/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper895/Authors|ICLR.cc/2018/Conference/Paper895/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper895/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper895/Authors|ICLR.cc/2018/Conference/Paper895/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper895/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper895/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper895/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper895/Reviewers", "ICLR.cc/2018/Conference/Paper895/Authors", "ICLR.cc/2018/Conference/Paper895/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825726004}}}, {"tddate": null, "ddate": null, "tmdate": 1515185623280, "tcdate": 1515185623280, "number": 2, "cdate": 1515185623280, "id": "Sk1y8waXf", "invitation": "ICLR.cc/2018/Conference/-/Paper895/Official_Comment", "forum": "HyY0Ff-AZ", "replyto": "r1FjCQKxG", "signatures": ["ICLR.cc/2018/Conference/Paper895/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper895/Authors"], "content": {"title": "RE : Risk sensitive control", "comment": "We take due note of the fact that these duality results are already known in reinforcement learning - we were not aware of either 1996-dated paper, and want to respectfully thank the reviewer for their mention, and sharing their extensive knowledge of the field.\n\nThe sign typo regarding the J functional is a valid point that has been corrected."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Representing Entropy : A short proof of the equivalence between soft Q-learning and policy gradients", "abstract": "Two main families of reinforcement learning algorithms, Q-learning and policy gradients, have recently been proven to be equivalent when using a softmax relaxation on one part, and an entropic regularization on the other. We relate this result to the well-known convex duality of Shannon entropy and the softmax function. Such a result is also known as the Donsker-Varadhan formula. This provides a short proof of the equivalence. We then interpret this duality further, and use ideas of convex analysis to prove a new policy inequality relative to soft Q-learning.", "pdf": "/pdf/d8235f3f5648e3f3f3f2a8ea7fd28d1e533f1d1e.pdf", "TL;DR": "A short proof of the equivalence of soft Q-learning and policy gradients.", "paperhash": "richemond|representing_entropy_a_short_proof_of_the_equivalence_between_soft_qlearning_and_policy_gradients", "_bibtex": "@misc{\nh.2018representing,\ntitle={Representing Entropy : A short proof of the equivalence between soft Q-learning and policy gradients},\nauthor={Pierre H. Richemond and Brendan Maginnis},\nyear={2018},\nurl={https://openreview.net/forum?id=HyY0Ff-AZ},\n}", "keywords": ["soft Q-learning", "policy gradients", "entropy", "Legendre transformation", "duality", "convex analysis", "Donsker-Varadhan"], "authors": ["Pierre H. Richemond", "Brendan Maginnis"], "authorids": ["phr17@imperial.ac.uk", "b.maginnis@imperial.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825726004, "id": "ICLR.cc/2018/Conference/-/Paper895/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "HyY0Ff-AZ", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper895/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper895/Authors|ICLR.cc/2018/Conference/Paper895/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper895/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper895/Authors|ICLR.cc/2018/Conference/Paper895/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper895/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper895/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper895/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper895/Reviewers", "ICLR.cc/2018/Conference/Paper895/Authors", "ICLR.cc/2018/Conference/Paper895/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825726004}}}, {"tddate": null, "ddate": null, "tmdate": 1515185262822, "tcdate": 1515185262822, "number": 1, "cdate": 1515185262822, "id": "SyDO4PpXf", "invitation": "ICLR.cc/2018/Conference/-/Paper895/Official_Comment", "forum": "HyY0Ff-AZ", "replyto": "B1_rmNyZz", "signatures": ["ICLR.cc/2018/Conference/Paper895/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper895/Authors"], "content": {"title": "Thank you for your suggestions !", "comment": "We respectfully acknowledge the reviewer's comments, and will indeed endeavour to take a more theoretical angle on minimal conditions for the proof in further work. This is an extremely helpful suggestion. We are thankful for comments on clarity/writing style.\n\nWe want to sincerely thank the reviewer for their insight and their time."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Representing Entropy : A short proof of the equivalence between soft Q-learning and policy gradients", "abstract": "Two main families of reinforcement learning algorithms, Q-learning and policy gradients, have recently been proven to be equivalent when using a softmax relaxation on one part, and an entropic regularization on the other. We relate this result to the well-known convex duality of Shannon entropy and the softmax function. Such a result is also known as the Donsker-Varadhan formula. This provides a short proof of the equivalence. We then interpret this duality further, and use ideas of convex analysis to prove a new policy inequality relative to soft Q-learning.", "pdf": "/pdf/d8235f3f5648e3f3f3f2a8ea7fd28d1e533f1d1e.pdf", "TL;DR": "A short proof of the equivalence of soft Q-learning and policy gradients.", "paperhash": "richemond|representing_entropy_a_short_proof_of_the_equivalence_between_soft_qlearning_and_policy_gradients", "_bibtex": "@misc{\nh.2018representing,\ntitle={Representing Entropy : A short proof of the equivalence between soft Q-learning and policy gradients},\nauthor={Pierre H. Richemond and Brendan Maginnis},\nyear={2018},\nurl={https://openreview.net/forum?id=HyY0Ff-AZ},\n}", "keywords": ["soft Q-learning", "policy gradients", "entropy", "Legendre transformation", "duality", "convex analysis", "Donsker-Varadhan"], "authors": ["Pierre H. Richemond", "Brendan Maginnis"], "authorids": ["phr17@imperial.ac.uk", "b.maginnis@imperial.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825726004, "id": "ICLR.cc/2018/Conference/-/Paper895/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "HyY0Ff-AZ", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper895/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper895/Authors|ICLR.cc/2018/Conference/Paper895/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper895/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper895/Authors|ICLR.cc/2018/Conference/Paper895/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper895/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper895/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper895/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper895/Reviewers", "ICLR.cc/2018/Conference/Paper895/Authors", "ICLR.cc/2018/Conference/Paper895/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825726004}}}], "count": 8}