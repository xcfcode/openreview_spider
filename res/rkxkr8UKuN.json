{"notes": [{"id": "rkxkr8UKuN", "original": "H1lYOxw_OV", "number": 41, "cdate": 1553716791270, "ddate": null, "tcdate": 1553716791270, "tmdate": 1562083044361, "tddate": null, "forum": "rkxkr8UKuN", "replyto": null, "invitation": "ICLR.cc/2019/Workshop/DeepGenStruct/-/Blind_Submission", "content": {"title": "Perceptual Generative Autoencoders", "authors": ["Zijun Zhang", "Ruixiang Zhang", "Zongpeng Li", "Yoshua Bengio", "Liam Paull"], "authorids": ["zijun.zhang@ucalgary.ca", "sodabeta7@gmail.com", "zongpeng@whu.edu.cn", "yoshua.bengio@mila.quebec", "paulll@iro.umontreal.ca"], "keywords": [], "TL;DR": "A framework for training autoencoder-based generative models, with non-adversarial losses and unrestricted neural network architectures.", "abstract": "Modern generative models are usually designed to match target distributions directly in the data space, where the intrinsic dimensionality of data can be much lower than the ambient dimensionality. We argue that this discrepancy may contribute to the difficulties in training generative models. We therefore propose to map both the generated and target distributions to the latent space using the encoder of a standard autoencoder, and train the generator (or decoder) to match the target distribution in the latent space. The resulting method, perceptual generative autoencoder (PGA), is then incorporated with maximum likelihood or variational autoencoder (VAE) objective to train the generative model. With maximum likelihood, PGA generalizes the idea of reversible generative models to unrestricted neural network architectures and arbitrary latent dimensionalities. When combined with VAE, PGA can generate sharper samples than vanilla VAE.", "pdf": "/pdf/2edd5fbd4a482e5442a28549848c474ed532d7ed.pdf", "paperhash": "zhang|perceptual_generative_autoencoders"}, "signatures": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "details": {"replyCount": 3, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/DeepGenStruct/-/Blind_Submission", "cdate": 1547567085825, "reply": {"forum": null, "replyto": null, "readers": {"values-regex": [".*"]}, "writers": {"values": ["ICLR.cc/2019/Workshop/DeepGenStruct"]}, "signatures": {"values": ["ICLR.cc/2019/Workshop/DeepGenStruct"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}}}, "tcdate": 1547567085825, "tmdate": 1555704438520, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "invitees": ["~"], "signatures": ["ICLR.cc/2019/Workshop/DeepGenStruct"]}}, "tauthor": "OpenReview.net"}, {"id": "SygwfBqM9E", "original": null, "number": 2, "cdate": 1555371278995, "ddate": null, "tcdate": 1555371278995, "tmdate": 1556906129115, "tddate": null, "forum": "rkxkr8UKuN", "replyto": "rkxkr8UKuN", "invitation": "ICLR.cc/2019/Workshop/DeepGenStruct/-/Paper41/Official_Review", "content": {"title": "perceptual generative autoencoders", "review": "This work proposes to use autoencoders to learn perceptually meaningful spaces in which to train generative models. Two variants of the framework are introduced, using maximum likelihood training and using a variational approach. This is a fresh take on autoencoders which uses ideas from invertible neural networks to enable training of generative models in latent space.\n\nOne of the main strengths of adversarial models seems to be their ability to incorporate strong inductive biases in the loss function (i.e. the convolutional architecture of the discriminator), and this work brings that ability to other types of generative models without requiring any adversarial components (and thus neatly avoiding the instability they bring).\n\nThe manuscript would benefit from a slightly extended exposition, and possibly a few diagrams, as Section 2 was a bit hard to follow. As a reader familiar with various different autoencoder paradigms, I had a particular prior about what each component represents, and this work uses these components in new and (initially) counterintuitive ways. Emphasising these differences with prior work, and demonstrating visually where and how each component is used, would improve readability.\n\nIt would also be interesting to describe in more detail how alpha, beta, gamma are tuned, what their optimal values tend to look like, and what this means. Ablations would also be interesting. For example, what happens when beta=0? The paragraph before formula (3) provides some motivation for this particular component of the loss function, but it would be interesting to see what happens in practice.\n\nStating that the results in Fig. 1 are competitive with GANs is perhaps a bit of a stretch, but overall the results are promising nevertheless, and I am curious to see this idea explored further. It would especially be interesting to see if this helps with scaling up e.g. likelihood-based models and other alternatives to adversarial models.", "rating": "4: Top 50% of accepted papers, clear accept", "confidence": "2: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Workshop/DeepGenStruct/Paper41/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/DeepGenStruct/Paper41/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Perceptual Generative Autoencoders", "authors": ["Zijun Zhang", "Ruixiang Zhang", "Zongpeng Li", "Yoshua Bengio", "Liam Paull"], "authorids": ["zijun.zhang@ucalgary.ca", "sodabeta7@gmail.com", "zongpeng@whu.edu.cn", "yoshua.bengio@mila.quebec", "paulll@iro.umontreal.ca"], "keywords": [], "TL;DR": "A framework for training autoencoder-based generative models, with non-adversarial losses and unrestricted neural network architectures.", "abstract": "Modern generative models are usually designed to match target distributions directly in the data space, where the intrinsic dimensionality of data can be much lower than the ambient dimensionality. We argue that this discrepancy may contribute to the difficulties in training generative models. We therefore propose to map both the generated and target distributions to the latent space using the encoder of a standard autoencoder, and train the generator (or decoder) to match the target distribution in the latent space. The resulting method, perceptual generative autoencoder (PGA), is then incorporated with maximum likelihood or variational autoencoder (VAE) objective to train the generative model. With maximum likelihood, PGA generalizes the idea of reversible generative models to unrestricted neural network architectures and arbitrary latent dimensionalities. When combined with VAE, PGA can generate sharper samples than vanilla VAE.", "pdf": "/pdf/2edd5fbd4a482e5442a28549848c474ed532d7ed.pdf", "paperhash": "zhang|perceptual_generative_autoencoders"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/DeepGenStruct/-/Paper41/Official_Review", "cdate": 1554234172062, "reply": {"forum": "rkxkr8UKuN", "replyto": "rkxkr8UKuN", "readers": [".*"], "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2019/Workshop/DeepGenStruct/Paper41/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2019/Workshop/DeepGenStruct/Paper41/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["5: Top 15% of accepted papers, strong accept", "4: Top 50% of accepted papers, clear accept", "3: Marginally above acceptance threshold", "2: Marginally below acceptance threshold", "1: Strong rejection"], "required": true}, "confidence": {"order": 4, "value-radio": ["3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "2: The reviewer is fairly confident that the evaluation is correct", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "tcdate": 1554234172062, "tmdate": 1556906092198, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "invitees": ["ICLR.cc/2019/Workshop/DeepGenStruct/Paper41/Reviewers"], "signatures": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "details": {"writable": true}}}}, {"id": "rJefg9sVF4", "original": null, "number": 1, "cdate": 1554459113556, "ddate": null, "tcdate": 1554459113556, "tmdate": 1556906128899, "tddate": null, "forum": "rkxkr8UKuN", "replyto": "rkxkr8UKuN", "invitation": "ICLR.cc/2019/Workshop/DeepGenStruct/-/Paper41/Official_Review", "content": {"title": "Convoluted, but an interesting idea", "review": "This paper relies on autoencoders in order to to distribution matching in high dimensional spaces, with the following recipe:\n   1) build an autoencoder of the data g(f(x)) - eq 1.\n   2)  build an autoencoder in latent space - ensure that f(g(z)) can reconstruct both z samples from N(0, 1) - eq 2 and f(g(f(x))) - eq 3.\n   3) Show (under assumptions) that if  eq 2 is minimized, for  z samples from N(0, 1), if f(g(z)) = f(g(f(x)) for some x in the original data space, then g(z) belongs to the reconstruction distribution. This entails that if the conditions of the theorem are satisfied (namely if f(g(z)) = f(g(f(x)) for some x in the original data space). sample quality will match reconstruction quality. \n  4) Use 3) to justify that  f(g(z)) should have high likelihood in the distribution induced by f(g(x)). Achieve that either by maximum likelihood (approximate - since there is no guarantee that h is invertible) or by variational inference. \n\nEquation 5: equation 5 follows from a change of variable and then using that f(x) for x in the data will be normally distributed. What ensures this? Minimizing equation (2) ensures that f(g(z)) will be normally distribution, with z sampled from N(0, 1). \n\nCons:\n  * The paper is very convoluted to read. Notation is missing and the discussion is missing important aspects which is making following the correctness of the paper difficult. I urge the authors to add further discussions and figures.\n  * Parts of the loss function used are rather ad hoc.\n * The method seems dependent on 3 important hyperparameters. The sensitivity to hyperparameters is not discussed.\n\nPros:\n  * good empirical results\n  * code is open sourced\n\nCitations: I would add a citation to VEEGAN[1] which also uses distribution matching in latent space.\n\n[1] Srivastava, Akash, et al. \"Veegan: Reducing mode collapse in gans using implicit variational learning.\" Advances in Neural Information Processing Systems. 2017.\n", "rating": "3: Marginally above acceptance threshold", "confidence": "3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Workshop/DeepGenStruct/Paper41/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/DeepGenStruct/Paper41/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Perceptual Generative Autoencoders", "authors": ["Zijun Zhang", "Ruixiang Zhang", "Zongpeng Li", "Yoshua Bengio", "Liam Paull"], "authorids": ["zijun.zhang@ucalgary.ca", "sodabeta7@gmail.com", "zongpeng@whu.edu.cn", "yoshua.bengio@mila.quebec", "paulll@iro.umontreal.ca"], "keywords": [], "TL;DR": "A framework for training autoencoder-based generative models, with non-adversarial losses and unrestricted neural network architectures.", "abstract": "Modern generative models are usually designed to match target distributions directly in the data space, where the intrinsic dimensionality of data can be much lower than the ambient dimensionality. We argue that this discrepancy may contribute to the difficulties in training generative models. We therefore propose to map both the generated and target distributions to the latent space using the encoder of a standard autoencoder, and train the generator (or decoder) to match the target distribution in the latent space. The resulting method, perceptual generative autoencoder (PGA), is then incorporated with maximum likelihood or variational autoencoder (VAE) objective to train the generative model. With maximum likelihood, PGA generalizes the idea of reversible generative models to unrestricted neural network architectures and arbitrary latent dimensionalities. When combined with VAE, PGA can generate sharper samples than vanilla VAE.", "pdf": "/pdf/2edd5fbd4a482e5442a28549848c474ed532d7ed.pdf", "paperhash": "zhang|perceptual_generative_autoencoders"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/DeepGenStruct/-/Paper41/Official_Review", "cdate": 1554234172062, "reply": {"forum": "rkxkr8UKuN", "replyto": "rkxkr8UKuN", "readers": [".*"], "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2019/Workshop/DeepGenStruct/Paper41/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2019/Workshop/DeepGenStruct/Paper41/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["5: Top 15% of accepted papers, strong accept", "4: Top 50% of accepted papers, clear accept", "3: Marginally above acceptance threshold", "2: Marginally below acceptance threshold", "1: Strong rejection"], "required": true}, "confidence": {"order": 4, "value-radio": ["3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "2: The reviewer is fairly confident that the evaluation is correct", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "tcdate": 1554234172062, "tmdate": 1556906092198, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "invitees": ["ICLR.cc/2019/Workshop/DeepGenStruct/Paper41/Reviewers"], "signatures": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "details": {"writable": true}}}}, {"id": "HkxrS4dD9V", "original": null, "number": 1, "cdate": 1555690556618, "ddate": null, "tcdate": 1555690556618, "tmdate": 1556906128681, "tddate": null, "forum": "rkxkr8UKuN", "replyto": "rkxkr8UKuN", "invitation": "ICLR.cc/2019/Workshop/DeepGenStruct/-/Paper41/Decision", "content": {"title": "Acceptance Decision", "decision": "Accept"}, "signatures": ["ICLR.cc/2019/Workshop/DeepGenStruct/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/DeepGenStruct/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Perceptual Generative Autoencoders", "authors": ["Zijun Zhang", "Ruixiang Zhang", "Zongpeng Li", "Yoshua Bengio", "Liam Paull"], "authorids": ["zijun.zhang@ucalgary.ca", "sodabeta7@gmail.com", "zongpeng@whu.edu.cn", "yoshua.bengio@mila.quebec", "paulll@iro.umontreal.ca"], "keywords": [], "TL;DR": "A framework for training autoencoder-based generative models, with non-adversarial losses and unrestricted neural network architectures.", "abstract": "Modern generative models are usually designed to match target distributions directly in the data space, where the intrinsic dimensionality of data can be much lower than the ambient dimensionality. We argue that this discrepancy may contribute to the difficulties in training generative models. We therefore propose to map both the generated and target distributions to the latent space using the encoder of a standard autoencoder, and train the generator (or decoder) to match the target distribution in the latent space. The resulting method, perceptual generative autoencoder (PGA), is then incorporated with maximum likelihood or variational autoencoder (VAE) objective to train the generative model. With maximum likelihood, PGA generalizes the idea of reversible generative models to unrestricted neural network architectures and arbitrary latent dimensionalities. When combined with VAE, PGA can generate sharper samples than vanilla VAE.", "pdf": "/pdf/2edd5fbd4a482e5442a28549848c474ed532d7ed.pdf", "paperhash": "zhang|perceptual_generative_autoencoders"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/DeepGenStruct/-/Paper41/Decision", "cdate": 1554814603168, "reply": {"forum": "rkxkr8UKuN", "replyto": "rkxkr8UKuN", "readers": [".*"], "nonreaders": {"values": []}, "writers": {"values-regex": ["ICLR.cc/2019/Workshop/DeepGenStruct/Program_Chairs"], "description": "How your identity will be displayed."}, "signatures": {"values": ["ICLR.cc/2019/Workshop/DeepGenStruct/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"title": {"order": 1, "required": true, "value": "Acceptance Decision"}, "decision": {"order": 2, "required": true, "value-radio": ["Accept", "Reject"], "description": "Acceptance decision"}, "comment": {"order": 3, "required": false, "value-regex": "[\\S\\s]{0,5000}", "description": ""}}}, "tcdate": 1554814603168, "tmdate": 1556906101890, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "invitees": ["ICLR.cc/2019/Workshop/DeepGenStruct/Program_Chairs"], "signatures": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "details": {"writable": true}}}}], "count": 4}