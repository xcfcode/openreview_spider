{"notes": [{"id": "rJlUt0EYwS", "original": "S1xSAmdOwS", "number": 1248, "cdate": 1569439357920, "ddate": null, "tcdate": 1569439357920, "tmdate": 1583912038945, "tddate": null, "forum": "rJlUt0EYwS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["ziqi-wan16@mails.tsinghua.edu.cn", "qinyj16@mails.tsinghua.edu.cn", "zhouwenx@usc.edu", "yanjun@usc.edu", "qinyuany@usc.edu", "lneves@snap.com", "liuzy@tsinghua.edu.cn", "xiangren@usc.edu"], "title": "Learning from Explanations with Neural Execution Tree", "authors": ["Ziqi Wang*", "Yujia Qin*", "Wenxuan Zhou", "Jun Yan", "Qinyuan Ye", "Leonardo Neves", "Zhiyuan Liu", "Xiang Ren"], "pdf": "/pdf/384b941e7fb13502fee54b50ee3a59950f4beae9.pdf", "abstract": "While deep neural networks have achieved impressive performance on a range of NLP tasks, these data-hungry models heavily rely on labeled data, which restricts their applications in scenarios where data annotation is expensive. Natural language (NL) explanations have been demonstrated very useful additional supervision, which can provide sufficient domain knowledge for generating more labeled data over new instances, while the annotation time only doubles. However, directly applying them for augmenting model learning encounters two challenges: (1) NL explanations are unstructured and inherently compositional, which asks for a modularized model to represent their semantics, (2) NL explanations often have large numbers of linguistic variants, resulting in low recall and limited generalization ability. In this paper, we propose a novel Neural Execution Tree (NExT) framework to augment training data for text classification using NL explanations. After transforming NL explanations into executable logical forms by semantic parsing, NExT generalizes different types of actions specified by the logical forms for labeling data instances, which substantially increases the coverage of each NL explanation. Experiments on two NLP tasks (relation extraction and sentiment analysis) demonstrate its superiority over baseline methods. Its extension to multi-hop question answering achieves performance gain with light annotation effort.", "code": "https://www.dropbox.com/sh/zkp19yr44yr8idt/AABpjFN3r2COIOub33L7DtfLa?dl=0", "keywords": [], "paperhash": "wang|learning_from_explanations_with_neural_execution_tree", "_bibtex": "@inproceedings{\nWang*2020Learning,\ntitle={Learning from Explanations with Neural Execution Tree},\nauthor={Ziqi Wang* and Yujia Qin* and Wenxuan Zhou and Jun Yan and Qinyuan Ye and Leonardo Neves and Zhiyuan Liu and Xiang Ren},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rJlUt0EYwS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/ecb996e12720b878c35679607709088e29d1933f.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "xv68sTToQT", "original": null, "number": 1, "cdate": 1576798718546, "ddate": null, "tcdate": 1576798718546, "tmdate": 1576800918019, "tddate": null, "forum": "rJlUt0EYwS", "replyto": "rJlUt0EYwS", "invitation": "ICLR.cc/2020/Conference/Paper1248/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "This paper proposing a framework for augmenting classification systems with explanations was very well received by two reviewers, and on reviewer labeling themselves as \"perfectly neutral\". I see no reason not to recommend acceptance.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ziqi-wan16@mails.tsinghua.edu.cn", "qinyj16@mails.tsinghua.edu.cn", "zhouwenx@usc.edu", "yanjun@usc.edu", "qinyuany@usc.edu", "lneves@snap.com", "liuzy@tsinghua.edu.cn", "xiangren@usc.edu"], "title": "Learning from Explanations with Neural Execution Tree", "authors": ["Ziqi Wang*", "Yujia Qin*", "Wenxuan Zhou", "Jun Yan", "Qinyuan Ye", "Leonardo Neves", "Zhiyuan Liu", "Xiang Ren"], "pdf": "/pdf/384b941e7fb13502fee54b50ee3a59950f4beae9.pdf", "abstract": "While deep neural networks have achieved impressive performance on a range of NLP tasks, these data-hungry models heavily rely on labeled data, which restricts their applications in scenarios where data annotation is expensive. Natural language (NL) explanations have been demonstrated very useful additional supervision, which can provide sufficient domain knowledge for generating more labeled data over new instances, while the annotation time only doubles. However, directly applying them for augmenting model learning encounters two challenges: (1) NL explanations are unstructured and inherently compositional, which asks for a modularized model to represent their semantics, (2) NL explanations often have large numbers of linguistic variants, resulting in low recall and limited generalization ability. In this paper, we propose a novel Neural Execution Tree (NExT) framework to augment training data for text classification using NL explanations. After transforming NL explanations into executable logical forms by semantic parsing, NExT generalizes different types of actions specified by the logical forms for labeling data instances, which substantially increases the coverage of each NL explanation. Experiments on two NLP tasks (relation extraction and sentiment analysis) demonstrate its superiority over baseline methods. Its extension to multi-hop question answering achieves performance gain with light annotation effort.", "code": "https://www.dropbox.com/sh/zkp19yr44yr8idt/AABpjFN3r2COIOub33L7DtfLa?dl=0", "keywords": [], "paperhash": "wang|learning_from_explanations_with_neural_execution_tree", "_bibtex": "@inproceedings{\nWang*2020Learning,\ntitle={Learning from Explanations with Neural Execution Tree},\nauthor={Ziqi Wang* and Yujia Qin* and Wenxuan Zhou and Jun Yan and Qinyuan Ye and Leonardo Neves and Zhiyuan Liu and Xiang Ren},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rJlUt0EYwS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/ecb996e12720b878c35679607709088e29d1933f.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "rJlUt0EYwS", "replyto": "rJlUt0EYwS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795718438, "tmdate": 1576800268920, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1248/-/Decision"}}}, {"id": "S1e1GcC2Fr", "original": null, "number": 1, "cdate": 1571772935073, "ddate": null, "tcdate": 1571772935073, "tmdate": 1574479792816, "tddate": null, "forum": "rJlUt0EYwS", "replyto": "rJlUt0EYwS", "invitation": "ICLR.cc/2020/Conference/Paper1248/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #1", "review": "\nThis paper explores using natural language explanations as auxiliary training data for NLP tasks. It first transforms natural language expressions into a logical form through CCG, and then use a neural module network architecture to label data instances. Experimental analyses are conducted on two tasks -- relation extraction and sentiment analysis, showing that the proposed approach outperforms previous work that incorporates explanations as training data.\n\nOverall, the paper addresses an important issue of how to utilize human explanations as additional supervision source for NLP tasks and shows promising results. Hence, I believe the paper is above the acceptance threshold, and recommend for weak acceptance.\n\nI had concerns on the cost of collecting human explanations, since they are non-trivial to collect. However, the authors provided convincing arguments regarding the data annotation cost in their response, so I do not think this is a major limitation of the method.\n\nHowever, I would also like to note that the paper has a few limitations. The proposed is based on semantic parsing of the natural language explanations into logical forms and is therefore inherently limited by the representation power of symbolic and logical representations. In the two examples shown in Figure 1, the human explanations are very simple and have limited variety, so it is relatively easy to represent them in logical forms. However, on many NLP tasks (such as question answering), the human explanations (in natural language) may often be complicated and difficult to be represented in CCG. Therefore, it is unclear whether the proposed approach can be easily generalized to other tasks.\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"}, "signatures": ["ICLR.cc/2020/Conference/Paper1248/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1248/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ziqi-wan16@mails.tsinghua.edu.cn", "qinyj16@mails.tsinghua.edu.cn", "zhouwenx@usc.edu", "yanjun@usc.edu", "qinyuany@usc.edu", "lneves@snap.com", "liuzy@tsinghua.edu.cn", "xiangren@usc.edu"], "title": "Learning from Explanations with Neural Execution Tree", "authors": ["Ziqi Wang*", "Yujia Qin*", "Wenxuan Zhou", "Jun Yan", "Qinyuan Ye", "Leonardo Neves", "Zhiyuan Liu", "Xiang Ren"], "pdf": "/pdf/384b941e7fb13502fee54b50ee3a59950f4beae9.pdf", "abstract": "While deep neural networks have achieved impressive performance on a range of NLP tasks, these data-hungry models heavily rely on labeled data, which restricts their applications in scenarios where data annotation is expensive. Natural language (NL) explanations have been demonstrated very useful additional supervision, which can provide sufficient domain knowledge for generating more labeled data over new instances, while the annotation time only doubles. However, directly applying them for augmenting model learning encounters two challenges: (1) NL explanations are unstructured and inherently compositional, which asks for a modularized model to represent their semantics, (2) NL explanations often have large numbers of linguistic variants, resulting in low recall and limited generalization ability. In this paper, we propose a novel Neural Execution Tree (NExT) framework to augment training data for text classification using NL explanations. After transforming NL explanations into executable logical forms by semantic parsing, NExT generalizes different types of actions specified by the logical forms for labeling data instances, which substantially increases the coverage of each NL explanation. Experiments on two NLP tasks (relation extraction and sentiment analysis) demonstrate its superiority over baseline methods. Its extension to multi-hop question answering achieves performance gain with light annotation effort.", "code": "https://www.dropbox.com/sh/zkp19yr44yr8idt/AABpjFN3r2COIOub33L7DtfLa?dl=0", "keywords": [], "paperhash": "wang|learning_from_explanations_with_neural_execution_tree", "_bibtex": "@inproceedings{\nWang*2020Learning,\ntitle={Learning from Explanations with Neural Execution Tree},\nauthor={Ziqi Wang* and Yujia Qin* and Wenxuan Zhou and Jun Yan and Qinyuan Ye and Leonardo Neves and Zhiyuan Liu and Xiang Ren},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rJlUt0EYwS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/ecb996e12720b878c35679607709088e29d1933f.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rJlUt0EYwS", "replyto": "rJlUt0EYwS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1248/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1248/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575653739146, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1248/Reviewers"], "noninvitees": [], "tcdate": 1570237740148, "tmdate": 1575653739165, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1248/-/Official_Review"}}}, {"id": "S1lktlSYjB", "original": null, "number": 7, "cdate": 1573634167241, "ddate": null, "tcdate": 1573634167241, "tmdate": 1573707099665, "tddate": null, "forum": "rJlUt0EYwS", "replyto": "rJlUt0EYwS", "invitation": "ICLR.cc/2020/Conference/Paper1248/-/Official_Comment", "content": {"title": "Paper Revised", "comment": "We would like to thank all the reviewers for their efforts and valuable comments. We have revised the paper to address the questions from reviewers. The major updates are:\n\n- Improved clarity in section 3.1 and 4.3, in response to reviewer #2 and #3.\n- Scrape the method summary and get straight into the Explanation Parsing section in response to reviewer #2.\n- Correct all typos and grammatical errors and add one related work in response to reviewer #2."}, "signatures": ["ICLR.cc/2020/Conference/Paper1248/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1248/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ziqi-wan16@mails.tsinghua.edu.cn", "qinyj16@mails.tsinghua.edu.cn", "zhouwenx@usc.edu", "yanjun@usc.edu", "qinyuany@usc.edu", "lneves@snap.com", "liuzy@tsinghua.edu.cn", "xiangren@usc.edu"], "title": "Learning from Explanations with Neural Execution Tree", "authors": ["Ziqi Wang*", "Yujia Qin*", "Wenxuan Zhou", "Jun Yan", "Qinyuan Ye", "Leonardo Neves", "Zhiyuan Liu", "Xiang Ren"], "pdf": "/pdf/384b941e7fb13502fee54b50ee3a59950f4beae9.pdf", "abstract": "While deep neural networks have achieved impressive performance on a range of NLP tasks, these data-hungry models heavily rely on labeled data, which restricts their applications in scenarios where data annotation is expensive. Natural language (NL) explanations have been demonstrated very useful additional supervision, which can provide sufficient domain knowledge for generating more labeled data over new instances, while the annotation time only doubles. However, directly applying them for augmenting model learning encounters two challenges: (1) NL explanations are unstructured and inherently compositional, which asks for a modularized model to represent their semantics, (2) NL explanations often have large numbers of linguistic variants, resulting in low recall and limited generalization ability. In this paper, we propose a novel Neural Execution Tree (NExT) framework to augment training data for text classification using NL explanations. After transforming NL explanations into executable logical forms by semantic parsing, NExT generalizes different types of actions specified by the logical forms for labeling data instances, which substantially increases the coverage of each NL explanation. Experiments on two NLP tasks (relation extraction and sentiment analysis) demonstrate its superiority over baseline methods. Its extension to multi-hop question answering achieves performance gain with light annotation effort.", "code": "https://www.dropbox.com/sh/zkp19yr44yr8idt/AABpjFN3r2COIOub33L7DtfLa?dl=0", "keywords": [], "paperhash": "wang|learning_from_explanations_with_neural_execution_tree", "_bibtex": "@inproceedings{\nWang*2020Learning,\ntitle={Learning from Explanations with Neural Execution Tree},\nauthor={Ziqi Wang* and Yujia Qin* and Wenxuan Zhou and Jun Yan and Qinyuan Ye and Leonardo Neves and Zhiyuan Liu and Xiang Ren},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rJlUt0EYwS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/ecb996e12720b878c35679607709088e29d1933f.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJlUt0EYwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1248/Authors", "ICLR.cc/2020/Conference/Paper1248/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1248/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1248/Reviewers", "ICLR.cc/2020/Conference/Paper1248/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1248/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1248/Authors|ICLR.cc/2020/Conference/Paper1248/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504158933, "tmdate": 1576860551144, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1248/Authors", "ICLR.cc/2020/Conference/Paper1248/Reviewers", "ICLR.cc/2020/Conference/Paper1248/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1248/-/Official_Comment"}}}, {"id": "H1xOK0EKjB", "original": null, "number": 4, "cdate": 1573633664500, "ddate": null, "tcdate": 1573633664500, "tmdate": 1573658562996, "tddate": null, "forum": "rJlUt0EYwS", "replyto": "Skli-hQx9B", "invitation": "ICLR.cc/2020/Conference/Paper1248/-/Official_Comment", "content": {"title": "Response to reviewer 2 [2/3]", "comment": "Q4: \"For keyword query q, we directly encode it into vector z_q by bi-LSTM and attention.\" Can you elaborate on how z_q is constructed?\n\nA4: We first use a BiLSTM to generate contextual embedding {h_i}, then all hidden layers are summarized by an attention layer as follows: [1]\n\ns_t = v_h^T \\tanh (W_h h_t)\na_t = \\frac{\\exp (s_t)}{\\sum_{i=1}^n \\exp (s_i)}\nc = \\sum_{t=1}^n a_t h_t\n\nWe\u2019ll clarify this in detail in our paper.\n\n[1] Translation by Jointly Learning to Align and Translate   ICLR 2015\n\n-------------------------------\nQ5: I find the first paragraph of section 4.3 quite abstruse and bare in its explanation of the method used.\n\nA5: We apologize for not explaining it in detail. We\u2019ve changed the first paragraph of section 4.3 as follows:\n\nTo further test the capability of NMET in downstream tasks, we apply it to WikiHop country task by fusing NMET-matched facts into baseline model NLProlog. For a brief introduction, WikiHop country task requires a model to select the correct candidate ENT-Y for question ``Which country is ENT-X in?'' given a list of support sentences. As part of dataset design, the correct answer can only be found by reasoning over multiple support sentences.\n\nNLProlog is a model proposed for WikiHop. It first extracts triples from support sentences and treats the masked sentence as the relation between two entities. For example, ``Socrate was born in Athens'' is converted to (Socrate, ``ENT1 was born in ENT2'', Athens), where ``ENT1 was born in ENT2'' is later embedded by Sent2Vec to represent the relation between ENT1 and ENT2. Triples extracted from supporting sentences are fed into a Prolog reasoner which will do backward chaining and reasoning to arrive at the target statement country(ENT-X, ENT-Y). We refer readers to [1] for in-depth introduction of NLProlog.\n\n[1] NLProlog: Reasoning with Weak Unification for Question Answering in Natural Language, ACL 2019\n\n-------------------------------\nQ6: I am surprised that none of the modules or compared methods include any architecture that use a Transformer or a form of contextualized word vectors (McCann et al. 2017, Peters et al. 2017, Devlin et al. 2018). Is there an explanation for this?\n\nA6: The aim of our NMET framework is to leverage NL explanations so that models can learn to automatically annotate unlabeled data. The framework is model-agnostic as it can be integrated with any trainable base model (or downstream classifier). In our experiments, we choose BiLSTM+attention and ATAE-LSTM, which are generally used as base model for relation extraction and aspect level sentiment analysis respectively. And we demonstrate that NMET is significantly better than semi-supervised methods and data programming with these two base models.\n\nTo address your concern, we also added BERT into Sentiment Analysis task (Restaurant) and the results are as follows:\n\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014-\nNumber of explanations   \t    |        45     |       75        |\nMean_Teacher (+BERT)           |      79.3    |      79.8      |\nPseudo_Labeling (+BERT)\t    |       78.7   |      81.0      |\nSelf_Training (+BERT)              |       80.9   |      81.1      |\nNMET (+BERT)                          |       81.4   |      82.0      |\n\nThe above results show that our model still outperforms baseline methods when BERT is incorporated. The gain is not as remarkable as in our paper, because the performance is approaching the upper bound (around 85, we get this score by feeding all data to BERT and conduct supervised training). Results of NMET is close to this upper bound, with only 75 explanations, which again demonstrates the annotation efficiency with NMET. The results align with our conclusions and we\u2019ll add it in our paper."}, "signatures": ["ICLR.cc/2020/Conference/Paper1248/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1248/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ziqi-wan16@mails.tsinghua.edu.cn", "qinyj16@mails.tsinghua.edu.cn", "zhouwenx@usc.edu", "yanjun@usc.edu", "qinyuany@usc.edu", "lneves@snap.com", "liuzy@tsinghua.edu.cn", "xiangren@usc.edu"], "title": "Learning from Explanations with Neural Execution Tree", "authors": ["Ziqi Wang*", "Yujia Qin*", "Wenxuan Zhou", "Jun Yan", "Qinyuan Ye", "Leonardo Neves", "Zhiyuan Liu", "Xiang Ren"], "pdf": "/pdf/384b941e7fb13502fee54b50ee3a59950f4beae9.pdf", "abstract": "While deep neural networks have achieved impressive performance on a range of NLP tasks, these data-hungry models heavily rely on labeled data, which restricts their applications in scenarios where data annotation is expensive. Natural language (NL) explanations have been demonstrated very useful additional supervision, which can provide sufficient domain knowledge for generating more labeled data over new instances, while the annotation time only doubles. However, directly applying them for augmenting model learning encounters two challenges: (1) NL explanations are unstructured and inherently compositional, which asks for a modularized model to represent their semantics, (2) NL explanations often have large numbers of linguistic variants, resulting in low recall and limited generalization ability. In this paper, we propose a novel Neural Execution Tree (NExT) framework to augment training data for text classification using NL explanations. After transforming NL explanations into executable logical forms by semantic parsing, NExT generalizes different types of actions specified by the logical forms for labeling data instances, which substantially increases the coverage of each NL explanation. Experiments on two NLP tasks (relation extraction and sentiment analysis) demonstrate its superiority over baseline methods. Its extension to multi-hop question answering achieves performance gain with light annotation effort.", "code": "https://www.dropbox.com/sh/zkp19yr44yr8idt/AABpjFN3r2COIOub33L7DtfLa?dl=0", "keywords": [], "paperhash": "wang|learning_from_explanations_with_neural_execution_tree", "_bibtex": "@inproceedings{\nWang*2020Learning,\ntitle={Learning from Explanations with Neural Execution Tree},\nauthor={Ziqi Wang* and Yujia Qin* and Wenxuan Zhou and Jun Yan and Qinyuan Ye and Leonardo Neves and Zhiyuan Liu and Xiang Ren},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rJlUt0EYwS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/ecb996e12720b878c35679607709088e29d1933f.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJlUt0EYwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1248/Authors", "ICLR.cc/2020/Conference/Paper1248/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1248/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1248/Reviewers", "ICLR.cc/2020/Conference/Paper1248/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1248/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1248/Authors|ICLR.cc/2020/Conference/Paper1248/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504158933, "tmdate": 1576860551144, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1248/Authors", "ICLR.cc/2020/Conference/Paper1248/Reviewers", "ICLR.cc/2020/Conference/Paper1248/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1248/-/Official_Comment"}}}, {"id": "HJxfj1iFsr", "original": null, "number": 8, "cdate": 1573658522455, "ddate": null, "tcdate": 1573658522455, "tmdate": 1573658522455, "tddate": null, "forum": "rJlUt0EYwS", "replyto": "Skli-hQx9B", "invitation": "ICLR.cc/2020/Conference/Paper1248/-/Official_Comment", "content": {"title": "Response to reviewer 2 [1/3] ", "comment": "Thank you for your suggestions for improving the presentation of our work! Our response is as follows:\n\n-------------------------------\nQ1: One recent work that comes to mind from ACL 2019: Leveraging Language Models for Commonsense Reasoning (Rajani et al. 2019). And it seems like a reasonable reference to contrast the more structured approach to using explanations like Srivasta et al. (2017), Hancock et al. (2018), and this work.\n\nA1: Thank you for pointing out this line of work and we\u2019ll add it into reference. We would like to emphasize that both methods (language model and structured approach) leverage explanations but with different focus and motivation. \n\nRajani et al. (2019) explore an implicit and data-hungry method (language models) to leverage natural language explanations. They focus on the importance of NL explanations as *supplementary supervision*, when *labeled data is abundant*. \n\nOur work (also [1] and [2]) proposes a more structured and explicit approach. We focus on leveraging NL explanations to *efficiently annotate unlabeled data*, when *annotation is expensive*. Our setting is most similar to Stanford Snorkel [3], where labeling training data is the largest bottleneck (especially when domain expertise is required). In these settings, users write labeling functions (in our case, it is NL explanations) that express arbitrary heuristics to create training data rapidly and efficiently.\n\n[1] Training Classifiers with Natural Language Explanations   ACL 2018\n[2] Joint concept learning and semantic parsing from natural language explanations   EMNLP 2017\n[3] Snorkel: Rapid training data creation with weak supervision   VLDB Endowment 2017\n\n-------------------------------\nQ2: Why is it so essential that you study the setting in which explanations are low-resources? I'm curious to see what would happen with more explanations.\n\nA2: As is stated in A1, our setting is most similar to *Snorkel*. We focus on how to leverage NL explanations *efficiently* when annotation is expensive, and we demonstrate *efficiency* by showing promising results with only *a few* NL explanations in our experiments. This setting is very realistic in certain domains that require expertise (e.g. medical domain), where large-scale conventional annotation is not practical due to time and budget limit. \n\nOur experiments in Sec 4.2 demonstrate our model benefits from more explanations (as shown in Fig. 5 on page 9 or the table below). In domains where annotation is not expensive, we can certainly collect more explanations for improvement. But this kind of setting is not the focus of our study. We agree that there are interesting topics like if there is a \u201csaturation point\u201d for using explanations as supervision and will leave them as future work.\n\nSentiment Analysis (Restaurant):\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014-\u2014-\u2014\u2014-\u2014\nNumber of explanations   |  10   |  20   |  30   |  45   |  60   |  75   |\nPseudo Labeling                 | 61.0 | 71.2 | 70.3 | 70.9 | 70.8 | 70.7 |\nLSTM+ATT                            | 64.7 | 71.2 | 70.8 | 71.1 | 71.2 | 70.8 |\nMeanTeacher                      | 51.2 | 70.5 | 72.2 | 72.0 | 72.2 | 76.0 |\nNMET                                    | 72.1 | 72.3 | 74.6 | 75.8 | 76.2 | 76.9 |\n\nRelation Extraction (TACRED):\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014-\u2014\u2014-\u2014\nNumber of explanations   | 100  | 120  | 140  | 170  | 200  | 230  |\nPseudo Labeling                 | 37.5 | 37.1 | 40.6 | 41.5 | 42.1 | 42.1 |\nATAE-LSTM                           | 36.8 | 37.8 | 40.4 | 41.4 | 41.0 | 41.9 |\nMeanTeacher                      | 36.5 | 36.9 | 40.9 | 40.8 | 41.7 | 42.5 |\nNMET                                    | 40.9 | 42.1 | 43.0 | 45.6 | 45.1 | 45.8 |\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014-\u2014-\u2014\u2014-\u2014\n\n-------------------------------\nQ3: Can you elaborate on why LF(E) is so dominant on precision in Table 6 and 7 of the Appendix, but low on recall, rather than just saying this is expected in section 4.1? \n\nA3: As is found in our collected dataset, people tend to use detailed and specific constraints in an NL explanation to ensure they cover all aspects of the instance. As a result, those instances that satisfy the constraints are correctly labeled in most cases, and thus the precision is high.  \n\nMeanwhile, generalization ability is compromised. As discussed in the introduction, linguistic variants are common, which makes it difficult to generalize NL explanations for matching sentences that are semantically equivalent but having different word usage, e.g. \u201cis founded by\u201d in a rule can not be strictly matched with \u201cis created by\u201d though they express the same meaning. In this case, those \u201cis created by\u201d instances cannot be matched, and thus the recall is low. We will clarify this in detail in our paper."}, "signatures": ["ICLR.cc/2020/Conference/Paper1248/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1248/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ziqi-wan16@mails.tsinghua.edu.cn", "qinyj16@mails.tsinghua.edu.cn", "zhouwenx@usc.edu", "yanjun@usc.edu", "qinyuany@usc.edu", "lneves@snap.com", "liuzy@tsinghua.edu.cn", "xiangren@usc.edu"], "title": "Learning from Explanations with Neural Execution Tree", "authors": ["Ziqi Wang*", "Yujia Qin*", "Wenxuan Zhou", "Jun Yan", "Qinyuan Ye", "Leonardo Neves", "Zhiyuan Liu", "Xiang Ren"], "pdf": "/pdf/384b941e7fb13502fee54b50ee3a59950f4beae9.pdf", "abstract": "While deep neural networks have achieved impressive performance on a range of NLP tasks, these data-hungry models heavily rely on labeled data, which restricts their applications in scenarios where data annotation is expensive. Natural language (NL) explanations have been demonstrated very useful additional supervision, which can provide sufficient domain knowledge for generating more labeled data over new instances, while the annotation time only doubles. However, directly applying them for augmenting model learning encounters two challenges: (1) NL explanations are unstructured and inherently compositional, which asks for a modularized model to represent their semantics, (2) NL explanations often have large numbers of linguistic variants, resulting in low recall and limited generalization ability. In this paper, we propose a novel Neural Execution Tree (NExT) framework to augment training data for text classification using NL explanations. After transforming NL explanations into executable logical forms by semantic parsing, NExT generalizes different types of actions specified by the logical forms for labeling data instances, which substantially increases the coverage of each NL explanation. Experiments on two NLP tasks (relation extraction and sentiment analysis) demonstrate its superiority over baseline methods. Its extension to multi-hop question answering achieves performance gain with light annotation effort.", "code": "https://www.dropbox.com/sh/zkp19yr44yr8idt/AABpjFN3r2COIOub33L7DtfLa?dl=0", "keywords": [], "paperhash": "wang|learning_from_explanations_with_neural_execution_tree", "_bibtex": "@inproceedings{\nWang*2020Learning,\ntitle={Learning from Explanations with Neural Execution Tree},\nauthor={Ziqi Wang* and Yujia Qin* and Wenxuan Zhou and Jun Yan and Qinyuan Ye and Leonardo Neves and Zhiyuan Liu and Xiang Ren},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rJlUt0EYwS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/ecb996e12720b878c35679607709088e29d1933f.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJlUt0EYwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1248/Authors", "ICLR.cc/2020/Conference/Paper1248/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1248/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1248/Reviewers", "ICLR.cc/2020/Conference/Paper1248/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1248/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1248/Authors|ICLR.cc/2020/Conference/Paper1248/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504158933, "tmdate": 1576860551144, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1248/Authors", "ICLR.cc/2020/Conference/Paper1248/Reviewers", "ICLR.cc/2020/Conference/Paper1248/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1248/-/Official_Comment"}}}, {"id": "BygR22VKjH", "original": null, "number": 3, "cdate": 1573633206167, "ddate": null, "tcdate": 1573633206167, "tmdate": 1573658434895, "tddate": null, "forum": "rJlUt0EYwS", "replyto": "Skli-hQx9B", "invitation": "ICLR.cc/2020/Conference/Paper1248/-/Official_Comment", "content": {"title": "Response to reviewer 2 [3/3]", "comment": "Q7: Would any other tasks from benchmarks like GLUE or SuperGLUE be amenable to your approach?\n\nA7: We believe generalizing our framework to other tasks is a promising direction to explore. We want to describe how to generalize our framework to more complex tasks, such as Question Answering.\n\nIn this paper, we've made some attempts by applying NMET to a multi-hop multiple-choice Question Answering task (WikiHop in section 4.3) and it's making promising improvements. From the results, simply adding 100+ strictly-matched facts is not making a notable improvement, however, with the help of NMET, external knowledge from only 21 NL explanations and 5 rules help recognize a large number of structured facts and improves the performance.\n\nExtending our framework to span-based QA (e.g. SQuAD) is a research problem that we\u2019re recently tackling, and we would like to show the following example.\n\nQuestion: How is packet switching characterized?\nContext: Circuit switching is characterized by a fee per unit of connection time, even when no data is transferred, while packet switching may be characterized by a fee per unit of information transmitted, \nAnswer: by a fee per unit of information transmitted.\n\nA good explanation for such a dataset should cover three things: (1) What are the keywords in the question or context? (2) How is this type of question usually answered? (3) How do we locate the answer? For the above example, human annotators can write an NL explanation as follows: In the question, the words \"packet switching\" and \"characterized\" are important phrases. The question starts with \"How is\", so the answer probably starts with \"by\". The answer is right after \"characterized\" and the phrase \"may be\" is between \"packet switching\" and \"characterized\".\n\nWe can easily adapt our CCG parser and conduct semantic parsing on the above explanation. It can be parsed into the following logical form (a little different from what we defined in this paper):\n\n- Q=Question, S=Sentence\n- Variables: X(NP), Y(VBZ), ANS(PP)\n- Rules1\n@Is(Q, @StartWith(\u201cHow is\u201d))\n@Is(X, @Direct(@Right(\u201cHow is\u201d)))\n@Is(Y, @Direct(@Right(X)))\n@Is(Q, @EndWith(Y))\n- Rules2\n@Is(X, @AppearIn(S))\n@Is(Y, @AppearIn(S))\n@Is(\u201cmay be\u201d, @Between(X,Y))\n- Rules3\n@Is(ANS, @StartWith(\u201cBy\u201d))\n- Rules4\n@Is(ANS, @Direct(@Right(Y))\n\nThis logical form can then be applied to answer the following question:\n\nQuestion: How is cheese made?\nContext: Cheese is made the same way \u2014 by curdling milk \u2014 except the milk is curdled on purpose. Most cheese is made in factories. After milk is poured into big vats, a \u201cstarter culture\u201d of bacteria is added to convert the lactose into lactic acid.\n\nTo answer this question with the above rule, we first pre-process the question/context into chunks to fill the variables (X, Y) with candidate chunks in the question (e.g. X=cheese, Y=made). We score all candidates with Rule1. If the question matches with Rule1 with high confidence, we further retrieve relevant context with Rule2 in context. We later fill the variable ANS with selected sentence with Rule3. In the end we evaluate the candidate ANS with Rule4. The evaluation of candidates with Rule1-4 can be done with the proposed NMET framework. The answer to the above instance, \"by curdling milk\" can be correctly selected.\n\nAlternatively, we can leverage this LF to match context first (with Rule2) and then generate question (with Rule1) and answer (with Rule3-4).\n\n-------------------------------\nIn addition, thanks for all the suggestions for improving the writing. We will correct all typos and grammatical errors.  Also, we\u2019ll scrape the method summary and get straight into the Explanation Parsing section as suggested. We\u2019ll explain LF(E) explicitly, which denotes applying logical forms generated by explanations directly onto instances.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1248/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1248/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ziqi-wan16@mails.tsinghua.edu.cn", "qinyj16@mails.tsinghua.edu.cn", "zhouwenx@usc.edu", "yanjun@usc.edu", "qinyuany@usc.edu", "lneves@snap.com", "liuzy@tsinghua.edu.cn", "xiangren@usc.edu"], "title": "Learning from Explanations with Neural Execution Tree", "authors": ["Ziqi Wang*", "Yujia Qin*", "Wenxuan Zhou", "Jun Yan", "Qinyuan Ye", "Leonardo Neves", "Zhiyuan Liu", "Xiang Ren"], "pdf": "/pdf/384b941e7fb13502fee54b50ee3a59950f4beae9.pdf", "abstract": "While deep neural networks have achieved impressive performance on a range of NLP tasks, these data-hungry models heavily rely on labeled data, which restricts their applications in scenarios where data annotation is expensive. Natural language (NL) explanations have been demonstrated very useful additional supervision, which can provide sufficient domain knowledge for generating more labeled data over new instances, while the annotation time only doubles. However, directly applying them for augmenting model learning encounters two challenges: (1) NL explanations are unstructured and inherently compositional, which asks for a modularized model to represent their semantics, (2) NL explanations often have large numbers of linguistic variants, resulting in low recall and limited generalization ability. In this paper, we propose a novel Neural Execution Tree (NExT) framework to augment training data for text classification using NL explanations. After transforming NL explanations into executable logical forms by semantic parsing, NExT generalizes different types of actions specified by the logical forms for labeling data instances, which substantially increases the coverage of each NL explanation. Experiments on two NLP tasks (relation extraction and sentiment analysis) demonstrate its superiority over baseline methods. Its extension to multi-hop question answering achieves performance gain with light annotation effort.", "code": "https://www.dropbox.com/sh/zkp19yr44yr8idt/AABpjFN3r2COIOub33L7DtfLa?dl=0", "keywords": [], "paperhash": "wang|learning_from_explanations_with_neural_execution_tree", "_bibtex": "@inproceedings{\nWang*2020Learning,\ntitle={Learning from Explanations with Neural Execution Tree},\nauthor={Ziqi Wang* and Yujia Qin* and Wenxuan Zhou and Jun Yan and Qinyuan Ye and Leonardo Neves and Zhiyuan Liu and Xiang Ren},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rJlUt0EYwS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/ecb996e12720b878c35679607709088e29d1933f.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJlUt0EYwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1248/Authors", "ICLR.cc/2020/Conference/Paper1248/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1248/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1248/Reviewers", "ICLR.cc/2020/Conference/Paper1248/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1248/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1248/Authors|ICLR.cc/2020/Conference/Paper1248/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504158933, "tmdate": 1576860551144, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1248/Authors", "ICLR.cc/2020/Conference/Paper1248/Reviewers", "ICLR.cc/2020/Conference/Paper1248/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1248/-/Official_Comment"}}}, {"id": "S1x9js4tsS", "original": null, "number": 2, "cdate": 1573632930236, "ddate": null, "tcdate": 1573632930236, "tmdate": 1573634623714, "tddate": null, "forum": "rJlUt0EYwS", "replyto": "S1e1GcC2Fr", "invitation": "ICLR.cc/2020/Conference/Paper1248/-/Official_Comment", "content": {"title": "Response to reviewer 1", "comment": "We really appreciate your comments and valuable suggestions for improving this work!\n\n-------------------------------\nQ1: It is unclear whether the proposed methods can be generalized to other tasks (such as question answering):\n\nA1: Indeed, it is challenging to generalize our framework to more complex tasks such as Question Answering. Still, we believe this is a promising direction to explore.\n\nIn this paper, we've made some attempts by applying NMET to a multi-hop multiple-choice Question Answering task (WikiHop in section 4.3) and it's making promising improvements. From the results, simply adding 100+ strictly-matched facts is not making a notable improvement, however, with the help of NMET, external knowledge from only 21 NL explanations and 5 rules help recognize a large number of structured facts and improves the performance.\n\nExtending our framework to span-based QA (e.g. SQuAD) is a research problem that we\u2019re recently tackling, and we would like to show the following example.\n\nQuestion: How is packet switching characterized?\nContext: Circuit switching is characterized by a fee per unit of connection time, even when no data is transferred, while packet switching may be characterized by a fee per unit of information transmitted, \nAnswer: by a fee per unit of information transmitted.\n\nA good explanation for such a dataset should cover three things: (1) What are the keywords in the question or context? (2) How is this type of question usually answered? (3) How do we locate the answer? For the above example, human annotators can write an NL explanation as follows: In the question, the words \"packet switching\" and \"characterized\" are important phrases. The question starts with \"How is\", so the answer probably starts with \"by\". The answer is right after \"characterized\" and the phrase \"may be\" is between \"packet switching\" and \"characterized\".\n\nWe can easily adapt our CCG parser and conduct semantic parsing on the above explanation. It can be parsed into the following logical form (a little different from what we defined in this paper):\n\n- Q=Question, S=Sentence\n- Variables: X(NP), Y(VBZ), ANS(PP)\n- Rules1\n@Is(Q, @StartWith(\u201cHow is\u201d))\n@Is(X, @Direct(@Right(\u201cHow is\u201d)))\n@Is(Y, @Direct(@Right(X)))\n@Is(Q, @EndWith(Y))\n- Rules2\n@Is(X, @AppearIn(S))\n@Is(Y, @AppearIn(S))\n@Is(\u201cmay be\u201d, @Between(X,Y))\n- Rules3\n@Is(ANS, @StartWith(\u201cBy\u201d))\n- Rules4\n@Is(ANS, @Direct(@Right(Y))\n\nThis logical form can then be applied to answer the following question:\n\nQuestion: How is cheese made?\nContext: Cheese is made the same way \u2014 by curdling milk \u2014 except the milk is curdled on purpose. Most cheese is made in factories. After milk is poured into big vats, a \u201cstarter culture\u201d of bacteria is added to convert the lactose into lactic acid.\n\nTo answer this question with the above rule, we first pre-process the question/context into chunks to fill the variables (X, Y) with candidate chunks in the question (e.g. X=cheese, Y=made). We score all candidates with Rule1. If the question matches with Rule1 with high confidence, we further retrieve relevant context with Rule2 in context. We later fill the variable ANS with selected sentence with Rule3. In the end, we evaluate the candidate ANS with Rule4. The evaluation of candidates with Rule1-4 can be done with the proposed NMET framework. The answer to the above instance, \"by curdling milk\" can be correctly selected.\n\nAlternatively, we can leverage this LF to match context first (with Rule2) and then generate question (with Rule1) and answer (with Rule3-4).\n\n-------------------------------\nQ2: Besides, it is also non-trivial to collect human explanations.\n\nA2: Given that an annotator has viewed the whole instance and gives a label, it should be easy for them to write a corresponding NL explanation explaining how the decision is made. It has been found in previous work that, on average it took the same amount of time to collect 30 explanations as 60 labels ([1]) in relation extraction task. Previous work also found that collecting annotator rationales in the form of highlighted substrings from the sentence only doubles annotation time ([2]). From our experiments, we find that for skilled annotators, the average ratio of (A) the time for only giving a label and (B) the time for giving both a label and an explanation in RE task is 1:2.01 for TACRED and 1:1.99 for SemEval, while in SA task it is 1:2.22 for Laptop and 1:2.30 for Restaurant. \n\nAs is discussed in section 4.2, compared with giving labels only, NL explanations with our NMET achieves higher data annotation efficiency (considering both model performance and annotation time).\n\n[1] Training Classifiers with Natural Language Explanations   ACL 2018\n[2] Modeling Annotators: A Generative Approach to Learning from Annotator Rationales   EMNLP 2008"}, "signatures": ["ICLR.cc/2020/Conference/Paper1248/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1248/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ziqi-wan16@mails.tsinghua.edu.cn", "qinyj16@mails.tsinghua.edu.cn", "zhouwenx@usc.edu", "yanjun@usc.edu", "qinyuany@usc.edu", "lneves@snap.com", "liuzy@tsinghua.edu.cn", "xiangren@usc.edu"], "title": "Learning from Explanations with Neural Execution Tree", "authors": ["Ziqi Wang*", "Yujia Qin*", "Wenxuan Zhou", "Jun Yan", "Qinyuan Ye", "Leonardo Neves", "Zhiyuan Liu", "Xiang Ren"], "pdf": "/pdf/384b941e7fb13502fee54b50ee3a59950f4beae9.pdf", "abstract": "While deep neural networks have achieved impressive performance on a range of NLP tasks, these data-hungry models heavily rely on labeled data, which restricts their applications in scenarios where data annotation is expensive. Natural language (NL) explanations have been demonstrated very useful additional supervision, which can provide sufficient domain knowledge for generating more labeled data over new instances, while the annotation time only doubles. However, directly applying them for augmenting model learning encounters two challenges: (1) NL explanations are unstructured and inherently compositional, which asks for a modularized model to represent their semantics, (2) NL explanations often have large numbers of linguistic variants, resulting in low recall and limited generalization ability. In this paper, we propose a novel Neural Execution Tree (NExT) framework to augment training data for text classification using NL explanations. After transforming NL explanations into executable logical forms by semantic parsing, NExT generalizes different types of actions specified by the logical forms for labeling data instances, which substantially increases the coverage of each NL explanation. Experiments on two NLP tasks (relation extraction and sentiment analysis) demonstrate its superiority over baseline methods. Its extension to multi-hop question answering achieves performance gain with light annotation effort.", "code": "https://www.dropbox.com/sh/zkp19yr44yr8idt/AABpjFN3r2COIOub33L7DtfLa?dl=0", "keywords": [], "paperhash": "wang|learning_from_explanations_with_neural_execution_tree", "_bibtex": "@inproceedings{\nWang*2020Learning,\ntitle={Learning from Explanations with Neural Execution Tree},\nauthor={Ziqi Wang* and Yujia Qin* and Wenxuan Zhou and Jun Yan and Qinyuan Ye and Leonardo Neves and Zhiyuan Liu and Xiang Ren},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rJlUt0EYwS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/ecb996e12720b878c35679607709088e29d1933f.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJlUt0EYwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1248/Authors", "ICLR.cc/2020/Conference/Paper1248/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1248/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1248/Reviewers", "ICLR.cc/2020/Conference/Paper1248/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1248/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1248/Authors|ICLR.cc/2020/Conference/Paper1248/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504158933, "tmdate": 1576860551144, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1248/Authors", "ICLR.cc/2020/Conference/Paper1248/Reviewers", "ICLR.cc/2020/Conference/Paper1248/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1248/-/Official_Comment"}}}, {"id": "SJxnxjNtiH", "original": null, "number": 1, "cdate": 1573632755764, "ddate": null, "tcdate": 1573632755764, "tmdate": 1573632755764, "tddate": null, "forum": "rJlUt0EYwS", "replyto": "SkgwWtPCFS", "invitation": "ICLR.cc/2020/Conference/Paper1248/-/Official_Comment", "content": {"title": "Response to reviewer 3", "comment": "We really appreciate your detailed comments and valuable feedback!\n\n> The Deterministic Function Module is not explained very well.\n\nWe are sorry for the confusion and unclear explanation about the Deterministic Function Module. Some predicates defined by human annotators, like \u2018The phrase *** occurs\u2019, can be softened by our NMET framework to improve the generalization ability. However, there are other predicates that are deterministic, which means they can only be exactly matched, like \u2018Left\u2019 and \u2018Right\u2019. A string is either right or left of an anchor word. Therefore, the probability it outputs should be either 0 or 1. The Deterministic Function Module deals with all these predicates and outputs a mask sequence, which is fed into the tree structure and combined with other information. We\u2019ll discuss it in detail in our paper."}, "signatures": ["ICLR.cc/2020/Conference/Paper1248/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1248/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ziqi-wan16@mails.tsinghua.edu.cn", "qinyj16@mails.tsinghua.edu.cn", "zhouwenx@usc.edu", "yanjun@usc.edu", "qinyuany@usc.edu", "lneves@snap.com", "liuzy@tsinghua.edu.cn", "xiangren@usc.edu"], "title": "Learning from Explanations with Neural Execution Tree", "authors": ["Ziqi Wang*", "Yujia Qin*", "Wenxuan Zhou", "Jun Yan", "Qinyuan Ye", "Leonardo Neves", "Zhiyuan Liu", "Xiang Ren"], "pdf": "/pdf/384b941e7fb13502fee54b50ee3a59950f4beae9.pdf", "abstract": "While deep neural networks have achieved impressive performance on a range of NLP tasks, these data-hungry models heavily rely on labeled data, which restricts their applications in scenarios where data annotation is expensive. Natural language (NL) explanations have been demonstrated very useful additional supervision, which can provide sufficient domain knowledge for generating more labeled data over new instances, while the annotation time only doubles. However, directly applying them for augmenting model learning encounters two challenges: (1) NL explanations are unstructured and inherently compositional, which asks for a modularized model to represent their semantics, (2) NL explanations often have large numbers of linguistic variants, resulting in low recall and limited generalization ability. In this paper, we propose a novel Neural Execution Tree (NExT) framework to augment training data for text classification using NL explanations. After transforming NL explanations into executable logical forms by semantic parsing, NExT generalizes different types of actions specified by the logical forms for labeling data instances, which substantially increases the coverage of each NL explanation. Experiments on two NLP tasks (relation extraction and sentiment analysis) demonstrate its superiority over baseline methods. Its extension to multi-hop question answering achieves performance gain with light annotation effort.", "code": "https://www.dropbox.com/sh/zkp19yr44yr8idt/AABpjFN3r2COIOub33L7DtfLa?dl=0", "keywords": [], "paperhash": "wang|learning_from_explanations_with_neural_execution_tree", "_bibtex": "@inproceedings{\nWang*2020Learning,\ntitle={Learning from Explanations with Neural Execution Tree},\nauthor={Ziqi Wang* and Yujia Qin* and Wenxuan Zhou and Jun Yan and Qinyuan Ye and Leonardo Neves and Zhiyuan Liu and Xiang Ren},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rJlUt0EYwS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/ecb996e12720b878c35679607709088e29d1933f.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJlUt0EYwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1248/Authors", "ICLR.cc/2020/Conference/Paper1248/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1248/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1248/Reviewers", "ICLR.cc/2020/Conference/Paper1248/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1248/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1248/Authors|ICLR.cc/2020/Conference/Paper1248/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504158933, "tmdate": 1576860551144, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1248/Authors", "ICLR.cc/2020/Conference/Paper1248/Reviewers", "ICLR.cc/2020/Conference/Paper1248/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1248/-/Official_Comment"}}}, {"id": "SkgwWtPCFS", "original": null, "number": 2, "cdate": 1571875071300, "ddate": null, "tcdate": 1571875071300, "tmdate": 1572972493580, "tddate": null, "forum": "rJlUt0EYwS", "replyto": "rJlUt0EYwS", "invitation": "ICLR.cc/2020/Conference/Paper1248/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "Labeling sentences for NLP requires a lot of human effort. In order to tackle this problem, the system proposed in this paper, called NMET, aims at labeling sentences by exploiting the explanations given by humans. First it converts the explanations into logical formulas. This logical formulas are then exploited for partitioning the dataset into two datasets: labeled dataset and unlabeled dataset. Then, NMET relaxes the logical formulas for labeling unlabeled examples by exploiting a neural architecture that uses four modules to deal with different types of predicates. \n\nThe paper is pretty clear and well-written. It can be understood even by non-experts. The proposed approach seems technically sound and pretty novel. Moreover, the experimental results show that the proposed system achieves better performances than traditional label-only supervised models.\nThe only concern that I see in the paper is that the Deterministic Function Module is not explained very well. It is not clear to me its purpose.\n\n[Minor]\nPage 3\n\u201cthe the logical form\u201d\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1248/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1248/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ziqi-wan16@mails.tsinghua.edu.cn", "qinyj16@mails.tsinghua.edu.cn", "zhouwenx@usc.edu", "yanjun@usc.edu", "qinyuany@usc.edu", "lneves@snap.com", "liuzy@tsinghua.edu.cn", "xiangren@usc.edu"], "title": "Learning from Explanations with Neural Execution Tree", "authors": ["Ziqi Wang*", "Yujia Qin*", "Wenxuan Zhou", "Jun Yan", "Qinyuan Ye", "Leonardo Neves", "Zhiyuan Liu", "Xiang Ren"], "pdf": "/pdf/384b941e7fb13502fee54b50ee3a59950f4beae9.pdf", "abstract": "While deep neural networks have achieved impressive performance on a range of NLP tasks, these data-hungry models heavily rely on labeled data, which restricts their applications in scenarios where data annotation is expensive. Natural language (NL) explanations have been demonstrated very useful additional supervision, which can provide sufficient domain knowledge for generating more labeled data over new instances, while the annotation time only doubles. However, directly applying them for augmenting model learning encounters two challenges: (1) NL explanations are unstructured and inherently compositional, which asks for a modularized model to represent their semantics, (2) NL explanations often have large numbers of linguistic variants, resulting in low recall and limited generalization ability. In this paper, we propose a novel Neural Execution Tree (NExT) framework to augment training data for text classification using NL explanations. After transforming NL explanations into executable logical forms by semantic parsing, NExT generalizes different types of actions specified by the logical forms for labeling data instances, which substantially increases the coverage of each NL explanation. Experiments on two NLP tasks (relation extraction and sentiment analysis) demonstrate its superiority over baseline methods. Its extension to multi-hop question answering achieves performance gain with light annotation effort.", "code": "https://www.dropbox.com/sh/zkp19yr44yr8idt/AABpjFN3r2COIOub33L7DtfLa?dl=0", "keywords": [], "paperhash": "wang|learning_from_explanations_with_neural_execution_tree", "_bibtex": "@inproceedings{\nWang*2020Learning,\ntitle={Learning from Explanations with Neural Execution Tree},\nauthor={Ziqi Wang* and Yujia Qin* and Wenxuan Zhou and Jun Yan and Qinyuan Ye and Leonardo Neves and Zhiyuan Liu and Xiang Ren},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rJlUt0EYwS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/ecb996e12720b878c35679607709088e29d1933f.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rJlUt0EYwS", "replyto": "rJlUt0EYwS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1248/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1248/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575653739146, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1248/Reviewers"], "noninvitees": [], "tcdate": 1570237740148, "tmdate": 1575653739165, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1248/-/Official_Review"}}}, {"id": "Skli-hQx9B", "original": null, "number": 3, "cdate": 1571990531211, "ddate": null, "tcdate": 1571990531211, "tmdate": 1572972493407, "tddate": null, "forum": "rJlUt0EYwS", "replyto": "rJlUt0EYwS", "invitation": "ICLR.cc/2020/Conference/Paper1248/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "One recent work that comes to mind from ACL 2019: Leveraging Language Models for Commonsense Reasoning (Rajani et al 2019). In that work, they also have human annotators provide explanations (extending the CommonsenseQA dataset), and they show that by training with these explanations, inference is improved even without them. They also train a language model to generate the explanations, and they show that the language model generated explanations improve performance further at inference time. Seems like a reasonable reference to contrast the more structured approach to using explanations like Srivasta et al (2017), Hancock et al (2018), and this work.\n\nI find the method summary beginning with \"Human explanations are first converted to machine-actionable logical forms by a semantic parser\" until the end of that paragraph to be unnecessary. Actually, as I read it, I find myself asking a lot of questions that get answered below. So I would prefer scrapping that method summary and just getting straight into the Explanation Parsing.\n\n\"indicates the the logical form matches\" redundant 'the'\n\nI can't find a definition for LF(E) anywhere, and yet LF(E) is present in many tables. I see that E is mentioned to be the explanations, but this is only in the caption of Table 2 even though the symbol is first used in the first paragraph of Section 4.1. I'm assuming LF logical forms applied directly to explanations, but this should be stated explicitly. Can you elaborate on why it is so dominant on precision In Table 6 and 7 of the Appendix, but low on recall, rather than just saying this is expected in Section 4.1? \n\n\"For keyword query q, we directly encode it into vector z_q by bi-LSTM and attention.\" Can you elaborate on how z_q is constructed? Is it the final state of a forward LSTM concatenated with the final state of a backward LSTM?\n\nWhy is it so essential that you study the setting in which explanations are low-resource? I'm curious to see what would happen with more explanations.\n\nI am surprised that none of the modules or compared methods include any architecture that use a Transformer or a form of contextualized word vectors (McCann et al 2017, Peters et al 2017, Devlin et al 2018). Is there an explanation for this?\n\nI would prefer to see a larger suite of tested tasks given that each of these datasets is quite small. Would any other tasks from benchmarks like GLUE or SuperGLUE be amenable to your approach? The tasks you've chosen limit the scope of this work and leaves the question of whether it would generally improve across a greater variety of tasks, especially tasks that have seen significant improvement using new methods. Your claim would be much stronger if the explanations were shown to be helpful even to pretrained models like BERT when fine-tuned for a specific task. In particular, it would be interesting to see how the benefits of explanations vary for different kinds of tasks and for different training set sizes. \n\n\"In the real world, a more realistic problem is that, with limited human-power, should we just annotate more labels or spend time explaining existing annotations.\"\" I think that you mean \"should we\" makes it sound like this is a question, but there is no question mark, and it makes more sense as a statement. I propose \"...human-power, there is a question of whether it is more valuable to ...\"\n\n\"Explanations prove to be an very efficient form for data annotation.\" should be \"a very\" not \"an very\".\n\n\" a vital rule\" should be \"a vital role\"\n\nI find the first paragraph of Section 4.3 quite abstruse and bare in its explanation of the method used."}, "signatures": ["ICLR.cc/2020/Conference/Paper1248/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1248/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ziqi-wan16@mails.tsinghua.edu.cn", "qinyj16@mails.tsinghua.edu.cn", "zhouwenx@usc.edu", "yanjun@usc.edu", "qinyuany@usc.edu", "lneves@snap.com", "liuzy@tsinghua.edu.cn", "xiangren@usc.edu"], "title": "Learning from Explanations with Neural Execution Tree", "authors": ["Ziqi Wang*", "Yujia Qin*", "Wenxuan Zhou", "Jun Yan", "Qinyuan Ye", "Leonardo Neves", "Zhiyuan Liu", "Xiang Ren"], "pdf": "/pdf/384b941e7fb13502fee54b50ee3a59950f4beae9.pdf", "abstract": "While deep neural networks have achieved impressive performance on a range of NLP tasks, these data-hungry models heavily rely on labeled data, which restricts their applications in scenarios where data annotation is expensive. Natural language (NL) explanations have been demonstrated very useful additional supervision, which can provide sufficient domain knowledge for generating more labeled data over new instances, while the annotation time only doubles. However, directly applying them for augmenting model learning encounters two challenges: (1) NL explanations are unstructured and inherently compositional, which asks for a modularized model to represent their semantics, (2) NL explanations often have large numbers of linguistic variants, resulting in low recall and limited generalization ability. In this paper, we propose a novel Neural Execution Tree (NExT) framework to augment training data for text classification using NL explanations. After transforming NL explanations into executable logical forms by semantic parsing, NExT generalizes different types of actions specified by the logical forms for labeling data instances, which substantially increases the coverage of each NL explanation. Experiments on two NLP tasks (relation extraction and sentiment analysis) demonstrate its superiority over baseline methods. Its extension to multi-hop question answering achieves performance gain with light annotation effort.", "code": "https://www.dropbox.com/sh/zkp19yr44yr8idt/AABpjFN3r2COIOub33L7DtfLa?dl=0", "keywords": [], "paperhash": "wang|learning_from_explanations_with_neural_execution_tree", "_bibtex": "@inproceedings{\nWang*2020Learning,\ntitle={Learning from Explanations with Neural Execution Tree},\nauthor={Ziqi Wang* and Yujia Qin* and Wenxuan Zhou and Jun Yan and Qinyuan Ye and Leonardo Neves and Zhiyuan Liu and Xiang Ren},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rJlUt0EYwS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/ecb996e12720b878c35679607709088e29d1933f.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rJlUt0EYwS", "replyto": "rJlUt0EYwS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1248/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1248/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575653739146, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1248/Reviewers"], "noninvitees": [], "tcdate": 1570237740148, "tmdate": 1575653739165, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1248/-/Official_Review"}}}], "count": 11}