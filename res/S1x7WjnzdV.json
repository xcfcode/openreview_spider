{"notes": [{"id": "S1x7WjnzdV", "original": "rklkHnmWOV", "number": 35, "cdate": 1553283835124, "ddate": null, "tcdate": 1553283835124, "tmdate": 1562082117808, "tddate": null, "forum": "S1x7WjnzdV", "replyto": null, "invitation": "ICLR.cc/2019/Workshop/LLD/-/Blind_Submission", "content": {"title": "Spatial Broadcast Decoder: A Simple Architecture for Disentangled Representations in VAEs", "authors": ["Nick Watters", "Loic Matthey", "Chris P. Burgess", "Alexander Lerchner"], "authorids": ["nwatters@google.com", "lmatthey@google.com", "cpburgess@google.com", "lerchner@google.com"], "keywords": ["disentangling", "VAE", "coordconv", "representation learning", "untangling"], "TL;DR": "We introduce a neural rendering architecture that helps VAEs learn disentangled latent representations.", "abstract": "We present a neural rendering architecture that helps variational autoencoders (VAEs) learn disentangled representations. Instead of the deconvolutional network typically used in the decoder of VAEs, we tile (broadcast) the latent vector across space, concatenate fixed X- and Y-\u201ccoordinate\u201d channels, and apply a fully convolutional network with 1x1 stride. This provides an architectural prior for dissociating positional from non-positional features in the latent space, yet without providing any explicit supervision to this effect. We show that this architecture, which we term the Spatial Broadcast decoder, improves disentangling, reconstruction accuracy, and generalization to held-out regions in data space.  We show the Spatial Broadcast Decoder is complementary to state-of-the-art (SOTA) disentangling techniques and when incorporated improves their performance.", "pdf": "/pdf/f471caec5f6625e9b2c62dd31cc197472e4f3b56.pdf", "paperhash": "watters|spatial_broadcast_decoder_a_simple_architecture_for_disentangled_representations_in_vaes"}, "signatures": ["ICLR.cc/2019/Workshop/LLD"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/LLD"], "details": {"replyCount": 3, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/LLD/-/Blind_Submission", "cdate": 1548689671889, "reply": {"forum": null, "replyto": null, "readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2019/Workshop/LLD"]}, "signatures": {"values": ["ICLR.cc/2019/Workshop/LLD"]}, "content": {"authors": {"values-regex": ".*"}, "authorids": {"values-regex": ".*"}}}, "tcdate": 1548689671889, "tmdate": 1557933709646, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/LLD"], "invitees": ["~"], "signatures": ["ICLR.cc/2019/Workshop/LLD"], "details": {"writable": true}}}, "tauthor": "OpenReview.net"}, {"id": "S1gWju1DFV", "original": null, "number": 1, "cdate": 1554606233352, "ddate": null, "tcdate": 1554606233352, "tmdate": 1555512021762, "tddate": null, "forum": "S1x7WjnzdV", "replyto": "S1x7WjnzdV", "invitation": "ICLR.cc/2019/Workshop/LLD/-/Paper35/Official_Review", "content": {"title": "An Interesting Analysis of Coordinate Tiled VAE Decoders", "review": "Summary: the authors present a simple extension of VAEs (and CoordConv VAEs) and demonstrate through a variety of experiments that the proposed tiling and (1x1) coord-conv solution produces a more disentangled representation. The presentation of detailed ablation studies is helpful in understanding exactly what benefits are brought by 1x1 convolutions vs. upsampling The empirical results are strong and promising, but a few points should be addressed in the final version. \n\nMajor:\n  - The results comparing Spatial Broadcast VAEs to CoordConv VAEs is a pretty critical result and should be moved into the main text from the appendix. Note that this should be present for all experiments, including the ones demonstrating the rate-distortion curves. In addition it would be interesting to contrast the CoordConv VAE with a few upsample layers, followed by 1x1 convolutions (as in the Spatial Broadcast VAE) to see if the effect is mainly from tiling or from the lack of upsampling blocks.\n  - A simple evaluation of disentanglement would be to use a linear classifier on the (mean) posterior sample after the training of the VAE. This would provide a more informative evaluation of (linear) separation in the latent space. This has been done in Associative Compressive Networks by Alex Graves for example. \n\nMinor:\n  - Figure labeling (i.e. a, b) missing on figure 3.\n  - Consistency between letter figure labeling and left/right.\n  - A4: what is the condition for termination of training? Early Stopping? If so what are the hyper-parameters used there?", "rating": "3: Marginally above acceptance threshold", "confidence": "3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Workshop/LLD/Paper35/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/LLD/Paper35/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Spatial Broadcast Decoder: A Simple Architecture for Disentangled Representations in VAEs", "authors": ["Nick Watters", "Loic Matthey", "Chris P. Burgess", "Alexander Lerchner"], "authorids": ["nwatters@google.com", "lmatthey@google.com", "cpburgess@google.com", "lerchner@google.com"], "keywords": ["disentangling", "VAE", "coordconv", "representation learning", "untangling"], "TL;DR": "We introduce a neural rendering architecture that helps VAEs learn disentangled latent representations.", "abstract": "We present a neural rendering architecture that helps variational autoencoders (VAEs) learn disentangled representations. Instead of the deconvolutional network typically used in the decoder of VAEs, we tile (broadcast) the latent vector across space, concatenate fixed X- and Y-\u201ccoordinate\u201d channels, and apply a fully convolutional network with 1x1 stride. This provides an architectural prior for dissociating positional from non-positional features in the latent space, yet without providing any explicit supervision to this effect. We show that this architecture, which we term the Spatial Broadcast decoder, improves disentangling, reconstruction accuracy, and generalization to held-out regions in data space.  We show the Spatial Broadcast Decoder is complementary to state-of-the-art (SOTA) disentangling techniques and when incorporated improves their performance.", "pdf": "/pdf/f471caec5f6625e9b2c62dd31cc197472e4f3b56.pdf", "paperhash": "watters|spatial_broadcast_decoder_a_simple_architecture_for_disentangled_representations_in_vaes"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/LLD/-/Paper35/Official_Review", "cdate": 1553713416126, "expdate": 1555718400000, "duedate": 1554681600000, "reply": {"forum": "S1x7WjnzdV", "replyto": "S1x7WjnzdV", "writers": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2019/Workshop/LLD/Paper35/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2019/Workshop/LLD/Paper35/AnonReviewer[0-9]+"}, "readers": {"values": ["everyone"], "description": "The users who will be allowed to read the above content."}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["5: Top 15% of accepted papers, strong accept", "4: Top 50% of accepted papers, clear accept", "3: Marginally above acceptance threshold", "2: Marginally below acceptance threshold", "1: Strong rejection"], "required": true}, "confidence": {"order": 4, "value-radio": ["3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "2: The reviewer is fairly confident that the evaluation is correct", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "tcdate": 1553713416126, "tmdate": 1555511826362, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/LLD"], "invitees": ["ICLR.cc/2019/Workshop/LLD/Paper35/Reviewers"], "signatures": ["ICLR.cc/2019/Workshop/LLD"]}}}, {"id": "r1eHYs0wFE", "original": null, "number": 2, "cdate": 1554668412871, "ddate": null, "tcdate": 1554668412871, "tmdate": 1555512016842, "tddate": null, "forum": "S1x7WjnzdV", "replyto": "S1x7WjnzdV", "invitation": "ICLR.cc/2019/Workshop/LLD/-/Paper35/Official_Review", "content": {"title": "Good work and write-up but unlcear about fit with this workshop ", "review": "Pros:\n- extensive and thorough experimentation\n- interesting and original idea\n- proposed an approach that is complimentary to previous approaches and helps improve SOTA results\n- comprehensive supplementary\n\nCons:\n- not immediately clear how this work relates to the limited labels setting", "rating": "4: Top 50% of accepted papers, clear accept", "confidence": "1: The reviewer's evaluation is an educated guess"}, "signatures": ["ICLR.cc/2019/Workshop/LLD/Paper35/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/LLD/Paper35/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Spatial Broadcast Decoder: A Simple Architecture for Disentangled Representations in VAEs", "authors": ["Nick Watters", "Loic Matthey", "Chris P. Burgess", "Alexander Lerchner"], "authorids": ["nwatters@google.com", "lmatthey@google.com", "cpburgess@google.com", "lerchner@google.com"], "keywords": ["disentangling", "VAE", "coordconv", "representation learning", "untangling"], "TL;DR": "We introduce a neural rendering architecture that helps VAEs learn disentangled latent representations.", "abstract": "We present a neural rendering architecture that helps variational autoencoders (VAEs) learn disentangled representations. Instead of the deconvolutional network typically used in the decoder of VAEs, we tile (broadcast) the latent vector across space, concatenate fixed X- and Y-\u201ccoordinate\u201d channels, and apply a fully convolutional network with 1x1 stride. This provides an architectural prior for dissociating positional from non-positional features in the latent space, yet without providing any explicit supervision to this effect. We show that this architecture, which we term the Spatial Broadcast decoder, improves disentangling, reconstruction accuracy, and generalization to held-out regions in data space.  We show the Spatial Broadcast Decoder is complementary to state-of-the-art (SOTA) disentangling techniques and when incorporated improves their performance.", "pdf": "/pdf/f471caec5f6625e9b2c62dd31cc197472e4f3b56.pdf", "paperhash": "watters|spatial_broadcast_decoder_a_simple_architecture_for_disentangled_representations_in_vaes"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/LLD/-/Paper35/Official_Review", "cdate": 1553713416126, "expdate": 1555718400000, "duedate": 1554681600000, "reply": {"forum": "S1x7WjnzdV", "replyto": "S1x7WjnzdV", "writers": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2019/Workshop/LLD/Paper35/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2019/Workshop/LLD/Paper35/AnonReviewer[0-9]+"}, "readers": {"values": ["everyone"], "description": "The users who will be allowed to read the above content."}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["5: Top 15% of accepted papers, strong accept", "4: Top 50% of accepted papers, clear accept", "3: Marginally above acceptance threshold", "2: Marginally below acceptance threshold", "1: Strong rejection"], "required": true}, "confidence": {"order": 4, "value-radio": ["3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "2: The reviewer is fairly confident that the evaluation is correct", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "tcdate": 1553713416126, "tmdate": 1555511826362, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/LLD"], "invitees": ["ICLR.cc/2019/Workshop/LLD/Paper35/Reviewers"], "signatures": ["ICLR.cc/2019/Workshop/LLD"]}}}, {"id": "r1xI7wJtF4", "original": null, "number": 1, "cdate": 1554736926271, "ddate": null, "tcdate": 1554736926271, "tmdate": 1555510986510, "tddate": null, "forum": "S1x7WjnzdV", "replyto": "S1x7WjnzdV", "invitation": "ICLR.cc/2019/Workshop/LLD/-/Paper35/Decision", "content": {"title": "Acceptance Decision", "decision": "Accept"}, "signatures": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Spatial Broadcast Decoder: A Simple Architecture for Disentangled Representations in VAEs", "authors": ["Nick Watters", "Loic Matthey", "Chris P. Burgess", "Alexander Lerchner"], "authorids": ["nwatters@google.com", "lmatthey@google.com", "cpburgess@google.com", "lerchner@google.com"], "keywords": ["disentangling", "VAE", "coordconv", "representation learning", "untangling"], "TL;DR": "We introduce a neural rendering architecture that helps VAEs learn disentangled latent representations.", "abstract": "We present a neural rendering architecture that helps variational autoencoders (VAEs) learn disentangled representations. Instead of the deconvolutional network typically used in the decoder of VAEs, we tile (broadcast) the latent vector across space, concatenate fixed X- and Y-\u201ccoordinate\u201d channels, and apply a fully convolutional network with 1x1 stride. This provides an architectural prior for dissociating positional from non-positional features in the latent space, yet without providing any explicit supervision to this effect. We show that this architecture, which we term the Spatial Broadcast decoder, improves disentangling, reconstruction accuracy, and generalization to held-out regions in data space.  We show the Spatial Broadcast Decoder is complementary to state-of-the-art (SOTA) disentangling techniques and when incorporated improves their performance.", "pdf": "/pdf/f471caec5f6625e9b2c62dd31cc197472e4f3b56.pdf", "paperhash": "watters|spatial_broadcast_decoder_a_simple_architecture_for_disentangled_representations_in_vaes"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/LLD/-/Paper35/Decision", "cdate": 1554736077594, "reply": {"forum": "S1x7WjnzdV", "replyto": "S1x7WjnzdV", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-regex": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "description": "How your identity will be displayed."}, "signatures": {"values": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"title": {"order": 1, "required": true, "value": "Acceptance Decision"}, "decision": {"order": 2, "required": true, "value-radio": ["Accept", "Reject"], "description": "Acceptance decision"}, "comment": {"order": 3, "required": false, "value-regex": "[\\S\\s]{0,5000}", "description": ""}}}, "tcdate": 1554736077594, "tmdate": 1555510961261, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/LLD"], "invitees": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "signatures": ["ICLR.cc/2019/Workshop/LLD"]}}}], "count": 4}