{"notes": [{"id": "r1l7E1HFPH", "original": "S1gMIsnOvr", "number": 1648, "cdate": 1569439530956, "ddate": null, "tcdate": 1569439530956, "tmdate": 1577168264969, "tddate": null, "forum": "r1l7E1HFPH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Multi-step Greedy Policies in Model-Free Deep Reinforcement Learning", "authors": ["Yonathan Efroni", "Manan Tomar", "Mohammad Ghavamzadeh"], "authorids": ["jonathan.efroni@gmail.com", "manan.tomar@gmail.com", "mgh@fb.com"], "keywords": ["Reinforcement Learning", "Multi-step greedy policies", "Model free Reinforcement Learning"], "TL;DR": "Use model free algorithms like DQN/TRPO to solve short horizon problems (model free) iteratively in a Policy/Value Iteration fashion.", "abstract": "Multi-step greedy policies have been extensively used in model-based Reinforcement Learning (RL) and in the case when a model of the environment is available (e.g., in the game of Go). In this work, we explore the benefits of multi-step greedy policies in model-free RL when employed in the framework of multi-step Dynamic Programming (DP): multi-step Policy and Value Iteration. These algorithms iteratively solve short-horizon decision problems and converge to the optimal solution of the original one. By using model-free algorithms as solvers of the short-horizon problems we derive fully model-free algorithms which are instances of the multi-step DP framework. As model-free algorithms are prone to instabilities w.r.t. the decision problem horizon, this simple approach can help in mitigating these instabilities and results in an improved model-free algorithms. We test this approach and show results on both discrete and continuous control problems.", "pdf": "/pdf/7609d0324668ea44a1e7893ff8de39f72889aa79.pdf", "paperhash": "efroni|multistep_greedy_policies_in_modelfree_deep_reinforcement_learning", "original_pdf": "/attachment/3bbb637f5515b6060f4563d4455b11f17f6c4975.pdf", "_bibtex": "@misc{\nefroni2020multistep,\ntitle={Multi-step Greedy Policies in Model-Free Deep Reinforcement Learning},\nauthor={Yonathan Efroni and Manan Tomar and Mohammad Ghavamzadeh},\nyear={2020},\nurl={https://openreview.net/forum?id=r1l7E1HFPH}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "EnItGAmxLd", "original": null, "number": 1, "cdate": 1576798728756, "ddate": null, "tcdate": 1576798728756, "tmdate": 1576800907771, "tddate": null, "forum": "r1l7E1HFPH", "replyto": "r1l7E1HFPH", "invitation": "ICLR.cc/2020/Conference/Paper1648/-/Decision", "content": {"decision": "Reject", "comment": "This paper extends recent multi-step dynamic programming algorithms to reinforcement learning with function approximation.  In particular, the paper extends h-step optimal Bellman operators (and associated k-PI and k-VI algorithms) to deep reinforcement learning.  The paper describes new extensions to DQN and TRPO algorithms.  This approach is claimed to reduce the instability of model-free algorithms, and the approach is tested on Atari and Mujoco domains. \n\nThe reviewers noticed several limitations of the work.  The reviewers found little theoretical contribution in this work and they were unsatisfied with the empirical contributions.  The reviewers were unconvinced of the strength and clarity of the empirical results with the Atari and Mujoco domains along with the deep learning network architectures.  The reviewers suggested that simpler domains with a simpler function approximation scheme could enable more through experiments and more conclusive results.  The claim in the abstract of addressing the instabilities was also not adequately studied in the paper.\n\nThis paper is not ready for publication.  The primary contribution of this work is the empirical evaluation, and the evaluation is not sufficiently clear for the reviewers.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Multi-step Greedy Policies in Model-Free Deep Reinforcement Learning", "authors": ["Yonathan Efroni", "Manan Tomar", "Mohammad Ghavamzadeh"], "authorids": ["jonathan.efroni@gmail.com", "manan.tomar@gmail.com", "mgh@fb.com"], "keywords": ["Reinforcement Learning", "Multi-step greedy policies", "Model free Reinforcement Learning"], "TL;DR": "Use model free algorithms like DQN/TRPO to solve short horizon problems (model free) iteratively in a Policy/Value Iteration fashion.", "abstract": "Multi-step greedy policies have been extensively used in model-based Reinforcement Learning (RL) and in the case when a model of the environment is available (e.g., in the game of Go). In this work, we explore the benefits of multi-step greedy policies in model-free RL when employed in the framework of multi-step Dynamic Programming (DP): multi-step Policy and Value Iteration. These algorithms iteratively solve short-horizon decision problems and converge to the optimal solution of the original one. By using model-free algorithms as solvers of the short-horizon problems we derive fully model-free algorithms which are instances of the multi-step DP framework. As model-free algorithms are prone to instabilities w.r.t. the decision problem horizon, this simple approach can help in mitigating these instabilities and results in an improved model-free algorithms. We test this approach and show results on both discrete and continuous control problems.", "pdf": "/pdf/7609d0324668ea44a1e7893ff8de39f72889aa79.pdf", "paperhash": "efroni|multistep_greedy_policies_in_modelfree_deep_reinforcement_learning", "original_pdf": "/attachment/3bbb637f5515b6060f4563d4455b11f17f6c4975.pdf", "_bibtex": "@misc{\nefroni2020multistep,\ntitle={Multi-step Greedy Policies in Model-Free Deep Reinforcement Learning},\nauthor={Yonathan Efroni and Manan Tomar and Mohammad Ghavamzadeh},\nyear={2020},\nurl={https://openreview.net/forum?id=r1l7E1HFPH}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "r1l7E1HFPH", "replyto": "r1l7E1HFPH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795710511, "tmdate": 1576800259534, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1648/-/Decision"}}}, {"id": "rJlPyxFhor", "original": null, "number": 9, "cdate": 1573847007448, "ddate": null, "tcdate": 1573847007448, "tmdate": 1573847007448, "tddate": null, "forum": "r1l7E1HFPH", "replyto": "S1gcu3dnjH", "invitation": "ICLR.cc/2020/Conference/Paper1648/-/Official_Comment", "content": {"title": "Updated review (2/2)", "comment": "Finally, about my rating of the paper, I have decided to keep it the same: weak accept. However, there is still some work to do on the paper, so I would not mind if the current iteration of the paper was rejected. \n\nAs reviewer #1 mentioned, the empirical evaluations in the main body of the paper are hard to read because of the overlap in the graphs. Moreover, although the results show a clear improvement over DQN and TRPO, this fact is lost in the amount of information presented in each graph. Personally, I liked the results in mountain car and pendulum because they are a lot cleaner than the results in Atari and Mujoco. You could also consider using less configuration of the parameters with a bigger spread to more clearly emphasize the results."}, "signatures": ["ICLR.cc/2020/Conference/Paper1648/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1648/AnonReviewer3", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Multi-step Greedy Policies in Model-Free Deep Reinforcement Learning", "authors": ["Yonathan Efroni", "Manan Tomar", "Mohammad Ghavamzadeh"], "authorids": ["jonathan.efroni@gmail.com", "manan.tomar@gmail.com", "mgh@fb.com"], "keywords": ["Reinforcement Learning", "Multi-step greedy policies", "Model free Reinforcement Learning"], "TL;DR": "Use model free algorithms like DQN/TRPO to solve short horizon problems (model free) iteratively in a Policy/Value Iteration fashion.", "abstract": "Multi-step greedy policies have been extensively used in model-based Reinforcement Learning (RL) and in the case when a model of the environment is available (e.g., in the game of Go). In this work, we explore the benefits of multi-step greedy policies in model-free RL when employed in the framework of multi-step Dynamic Programming (DP): multi-step Policy and Value Iteration. These algorithms iteratively solve short-horizon decision problems and converge to the optimal solution of the original one. By using model-free algorithms as solvers of the short-horizon problems we derive fully model-free algorithms which are instances of the multi-step DP framework. As model-free algorithms are prone to instabilities w.r.t. the decision problem horizon, this simple approach can help in mitigating these instabilities and results in an improved model-free algorithms. We test this approach and show results on both discrete and continuous control problems.", "pdf": "/pdf/7609d0324668ea44a1e7893ff8de39f72889aa79.pdf", "paperhash": "efroni|multistep_greedy_policies_in_modelfree_deep_reinforcement_learning", "original_pdf": "/attachment/3bbb637f5515b6060f4563d4455b11f17f6c4975.pdf", "_bibtex": "@misc{\nefroni2020multistep,\ntitle={Multi-step Greedy Policies in Model-Free Deep Reinforcement Learning},\nauthor={Yonathan Efroni and Manan Tomar and Mohammad Ghavamzadeh},\nyear={2020},\nurl={https://openreview.net/forum?id=r1l7E1HFPH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1l7E1HFPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1648/Authors", "ICLR.cc/2020/Conference/Paper1648/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1648/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1648/Reviewers", "ICLR.cc/2020/Conference/Paper1648/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1648/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1648/Authors|ICLR.cc/2020/Conference/Paper1648/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504152919, "tmdate": 1576860541123, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1648/Authors", "ICLR.cc/2020/Conference/Paper1648/Reviewers", "ICLR.cc/2020/Conference/Paper1648/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1648/-/Official_Comment"}}}, {"id": "S1gcu3dnjH", "original": null, "number": 8, "cdate": 1573846130464, "ddate": null, "tcdate": 1573846130464, "tmdate": 1573846130464, "tddate": null, "forum": "r1l7E1HFPH", "replyto": "B1l5akg2jB", "invitation": "ICLR.cc/2020/Conference/Paper1648/-/Official_Comment", "content": {"title": "Updated review (1/2)", "comment": "Thank you for your reply and for addressing my comments. However, I still have a few concerns.\n\n- About the simple domains and architectures\nMy  main concern was less about the big domains and more about  the complicated architectures used for k-VI/PI DQN and TRPO. With big deep neural network architectures there are too many hyperparameters to tune and control for if one wants to study the effect of one particular part of the algorithm. Thus, I suggested to use a less complex function approximator such as a tilecoder (see Sutton & Barto 2018), which only depends on a few architectural choices and eliminates the representation learning problem from the task. Instead, the authors provided experiments in simpler domains such as cartpole and mountain car. I apologize for the confusion. Nevertheless, the results do shed some clarity on the performance of the algorithms, so I still consider it an improvement. I still have a few questions about the results in mountain car.\n\nFirst, what implementation of mountain car are you using? Often the first few episodes in mountain car implemented as in Sutton and Barto (2018) have a cumulative reward below -100, but the plots indicate that the lowest cumulative reward obtained is more than -50. Additionally, with respect to the y-axis of Figure 19 (top left pane), is iterations the same as episodes? If you're using a different implementation from the one in Sutton and Barto (2018) please cite your sources.\n\nSecond, what method was used to smooth the plot? From my experience with this environment, even when averaging over 100 runs, the plots still appear very noisy, so it seems surprising to me that the plots are this smooth with a sample size of 10. \n\nAdditionally, it would be useful to also provide results about the behaviour of the algorithms under different values of CFA. I understand that CFA is related to the final accuracy of V_pi. However, since the V_pi is being approximated anyway, perhaps there may be an interesting trade-off between using a lot of samples to compute an accurate estimate or accept a higher error while moving on to the next policy improvement iteration with a slightly better, yet inaccurate, estimate than before.\n\n- About the size of the network\nThe size of the network was less important to me than the rationale behind choosing to deviate further from the original k-PI and k-VI algorithms. In Remark 1, it is mentioned that \\tilde{ Q }_\\theta, a target network that remains unchanged during the policy improvement step, should be used for \\pi_{i-1}; however, because of space complexity Q_\\theta', which may have changed during the policy improvement step,  is used instead. My concern about this is that it further deviates from the original algorithms and introduces more confounding factors. Since the purpose of the paper is to study the performance of these algorithms when using deep neural networks to estimate V_pi and Q_pi, then I think it would be better if it was as closed to the original algorithms as possible, which seems doable since the sizes of the networks were not exorbitantly big. \n\n- Expected Sarsa Update\nLine 19 resembles more the update for Sarsa more than Expected Sarsa (see Equation 6.7 of Sutton and Barto , 2018 and compare it to Equation 5 from Van Seijen et. al., 2009). Moreover, neither of those two should be called TD(0) since they are different algorithms. Finally, it is not clear how this is an off-policy update if there is no correction between the current policy and policy used to sample the action at the time it was stored in the experience replay buffer. The bottom line is that the statement that the update in Line 19 corresponds to off-policy TD(0) is false and should be corrected. \n\n- About the contradictory claims\nMy main point about that comment was that the claim \"[Table 1 and Figure 1] ... lead to a clear degradation of final performance,\" is an overstatement. In Seaquest, Enduro, Beam Rider, and Qbert the confidence intervals of the performance of k-Pi DQN (k best) and N(k) = T overlap. It is true that with more samples the confidence interval will tend to shrink; however, they will shrink around the true mean, not the observed sample average. That means that if the confidence intervals overlap, there exists a chance that the true mean of N(k) = T will be higher than the k-PI DQN. Since there is a chance that N(k) = T has better performance than k-PI DQN (k best), saying that N(k) = T shows a clear degradation of performance seems exaggerated.\n\n== More typos == \n- Paragraph above Equation (1), 4th line from the bottom, \"The algorithms by which an is be solved....\" It feels like there is a word missing. \n\n== References ==\n1. Sutton, R. S., Barto, A. G. (2018 ). Reinforcement Learning: An Introduction. The MIT Press.\n\n2.  Van Seijen, Harm, et al. \"A theoretical and empirical analysis of Expected Sarsa.\" 2009 IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning. IEEE, 2009.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1648/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1648/AnonReviewer3", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Multi-step Greedy Policies in Model-Free Deep Reinforcement Learning", "authors": ["Yonathan Efroni", "Manan Tomar", "Mohammad Ghavamzadeh"], "authorids": ["jonathan.efroni@gmail.com", "manan.tomar@gmail.com", "mgh@fb.com"], "keywords": ["Reinforcement Learning", "Multi-step greedy policies", "Model free Reinforcement Learning"], "TL;DR": "Use model free algorithms like DQN/TRPO to solve short horizon problems (model free) iteratively in a Policy/Value Iteration fashion.", "abstract": "Multi-step greedy policies have been extensively used in model-based Reinforcement Learning (RL) and in the case when a model of the environment is available (e.g., in the game of Go). In this work, we explore the benefits of multi-step greedy policies in model-free RL when employed in the framework of multi-step Dynamic Programming (DP): multi-step Policy and Value Iteration. These algorithms iteratively solve short-horizon decision problems and converge to the optimal solution of the original one. By using model-free algorithms as solvers of the short-horizon problems we derive fully model-free algorithms which are instances of the multi-step DP framework. As model-free algorithms are prone to instabilities w.r.t. the decision problem horizon, this simple approach can help in mitigating these instabilities and results in an improved model-free algorithms. We test this approach and show results on both discrete and continuous control problems.", "pdf": "/pdf/7609d0324668ea44a1e7893ff8de39f72889aa79.pdf", "paperhash": "efroni|multistep_greedy_policies_in_modelfree_deep_reinforcement_learning", "original_pdf": "/attachment/3bbb637f5515b6060f4563d4455b11f17f6c4975.pdf", "_bibtex": "@misc{\nefroni2020multistep,\ntitle={Multi-step Greedy Policies in Model-Free Deep Reinforcement Learning},\nauthor={Yonathan Efroni and Manan Tomar and Mohammad Ghavamzadeh},\nyear={2020},\nurl={https://openreview.net/forum?id=r1l7E1HFPH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1l7E1HFPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1648/Authors", "ICLR.cc/2020/Conference/Paper1648/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1648/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1648/Reviewers", "ICLR.cc/2020/Conference/Paper1648/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1648/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1648/Authors|ICLR.cc/2020/Conference/Paper1648/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504152919, "tmdate": 1576860541123, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1648/Authors", "ICLR.cc/2020/Conference/Paper1648/Reviewers", "ICLR.cc/2020/Conference/Paper1648/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1648/-/Official_Comment"}}}, {"id": "HJe7nfqXiH", "original": null, "number": 1, "cdate": 1573261995058, "ddate": null, "tcdate": 1573261995058, "tmdate": 1573810661503, "tddate": null, "forum": "r1l7E1HFPH", "replyto": "H1geTaI6KS", "invitation": "ICLR.cc/2020/Conference/Paper1648/-/Official_Comment", "content": {"title": "Response to AnonReviewer3", "comment": "We would like to thank the reviewer for the detailed review and useful comments. \n\n\u201cusing simpler domains to better explain the algorithms\u201d\nWe agree with the reviewer that it would be better to describe very new algorithms using small/simple domains. The reason that we did not initially include our experiments in simple domains is that the difference between the performance of the algorithms is not very clear in such problems. To address the reviewer\u2019s concern, we have now added a new section to the paper (Appendix C), where we report results on simpler environments, such as CartPole and Mountain Car. For the experiments on CartPole, we focus on the $\\kappa$-PI TRPO algorithm solely, since the results for the other versions ($\\kappa$-VI TRPO, DQN, and $\\kappa$-PI DQN) follow similarly. As pointed out by the reviewer, the purpose of this section is to gather a more intuitive understanding of the algorithms. Please refer to Appendix C in the updated paper for a more detailed discussion.\n\n\u201ca simpler way to emphasize this is to show a plot of the cumulative reward ...\u201d\nWe have added a couple of bar plots to address this in Appendix C.4. These correspond to the $\\kappa$-PI training plots for HalfCheetah and Ant domains. It is clear from the bar plots that the performance is smooth in $\\kappa$. \n\n\u201c1. Why choose 50% confidence intervals?\u201d\nThe results reported in Table 1 and 2 are the empirical mean $\\pm$ the empirical standard deviation, which for the sample size of 4 or 5 runs is roughly equal to the 95% confidence interval bound. Moreover, in the plots, we show results describing the empirical mean $\\pm$ 0.5 * empirical standard deviation, which is, again for the sample size of 4 or 5 runs, roughly equal to a 60% to 70% confidence interval. This is done so that there is less overlap in the graphs and they are more readable. The 50% value actually corresponds to these plots. We apologize for the lack of clarity here and have updated the paper with the correct confidence values. All conclusions are made with respect to the Table data eventually, which remains unchanged and still corresponds to the 95% confidence bound.\n\n\u201c2. How big were the networks that you used for k-PI DQN?\u201d\nThe DQN network sizes are the same as used in Mnih et al. [1], i.e., 3 convolutional layers followed by 2 fully connected layers. \n\n\u201c3. Line 19 of Algorithm 5 in Appendix A.1 \u2026\u201d\nThe update in Line 9 resembles the expected SARSA update in Van Seijen et al. [2]. Also, this is the exact update as in DDPG (Equation 5 in Lillicrap et al. [3]).\n\n\u201cContradictory Claims in the Results\u201d\nOur claim is essentially saying that the mean values of the best performing $\\kappa$ are consistently better than the mean values of the N_kappa = T baseline. Since the data here corresponds to the 95% confidence interval bound for 4-5 sample runs, increasing the number of sample runs would decrease the width of the 95% confidence interval, which essentially would ensure no overlap between the upper confidence limit of the baseline and the lower confidence limit of the best $\\kappa$ value. For example, comparing the two versions for 10 sample runs in the Ant domain, results in the lower confidence limit of the best $\\kappa$ value to be around 1230, while the upper confidence limit of the baseline to be around 1180, hence ensuring no overlap. Please note that due to the inherent variability in final training performance because of random seeding, the mean values for both cases, although relatively consistent, are also slightly changed. However, taking more samples always ensures that any x% confidence bound is narrowed.\n\n\u201cTypo in the last column of Table 1\u201d\nThank you for pointing this out. We have fixed this in the updated version.\n\n\u201cLinear convergence of PI and VI\u201d\nWe apologize for the confusion here. Many of the works in the optimization literature refer to such an exponential rate as linear in the parameter N, and we borrowed the same definition. We have fixed this in the updated version.\n\n\nReferences:\n\n1. Mnih, Volodymyr, et al. \"Human-level control through deep reinforcement learning.\" Nature 518.7540 (2015): 529.\n2. Van Seijen, Harm, et al. \"A theoretical and empirical analysis of Expected Sarsa.\" 2009 IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning. IEEE, 2009.\n3. Lillicrap, Timothy P., et al. \"Continuous control with deep reinforcement learning.\" arXiv preprint arXiv:1509.02971 (2015).\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1648/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1648/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Multi-step Greedy Policies in Model-Free Deep Reinforcement Learning", "authors": ["Yonathan Efroni", "Manan Tomar", "Mohammad Ghavamzadeh"], "authorids": ["jonathan.efroni@gmail.com", "manan.tomar@gmail.com", "mgh@fb.com"], "keywords": ["Reinforcement Learning", "Multi-step greedy policies", "Model free Reinforcement Learning"], "TL;DR": "Use model free algorithms like DQN/TRPO to solve short horizon problems (model free) iteratively in a Policy/Value Iteration fashion.", "abstract": "Multi-step greedy policies have been extensively used in model-based Reinforcement Learning (RL) and in the case when a model of the environment is available (e.g., in the game of Go). In this work, we explore the benefits of multi-step greedy policies in model-free RL when employed in the framework of multi-step Dynamic Programming (DP): multi-step Policy and Value Iteration. These algorithms iteratively solve short-horizon decision problems and converge to the optimal solution of the original one. By using model-free algorithms as solvers of the short-horizon problems we derive fully model-free algorithms which are instances of the multi-step DP framework. As model-free algorithms are prone to instabilities w.r.t. the decision problem horizon, this simple approach can help in mitigating these instabilities and results in an improved model-free algorithms. We test this approach and show results on both discrete and continuous control problems.", "pdf": "/pdf/7609d0324668ea44a1e7893ff8de39f72889aa79.pdf", "paperhash": "efroni|multistep_greedy_policies_in_modelfree_deep_reinforcement_learning", "original_pdf": "/attachment/3bbb637f5515b6060f4563d4455b11f17f6c4975.pdf", "_bibtex": "@misc{\nefroni2020multistep,\ntitle={Multi-step Greedy Policies in Model-Free Deep Reinforcement Learning},\nauthor={Yonathan Efroni and Manan Tomar and Mohammad Ghavamzadeh},\nyear={2020},\nurl={https://openreview.net/forum?id=r1l7E1HFPH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1l7E1HFPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1648/Authors", "ICLR.cc/2020/Conference/Paper1648/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1648/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1648/Reviewers", "ICLR.cc/2020/Conference/Paper1648/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1648/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1648/Authors|ICLR.cc/2020/Conference/Paper1648/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504152919, "tmdate": 1576860541123, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1648/Authors", "ICLR.cc/2020/Conference/Paper1648/Reviewers", "ICLR.cc/2020/Conference/Paper1648/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1648/-/Official_Comment"}}}, {"id": "B1l5akg2jB", "original": null, "number": 7, "cdate": 1573810114487, "ddate": null, "tcdate": 1573810114487, "tmdate": 1573810114487, "tddate": null, "forum": "r1l7E1HFPH", "replyto": "HJe7nfqXiH", "invitation": "ICLR.cc/2020/Conference/Paper1648/-/Official_Comment", "content": {"title": "Update to Appendix C", "comment": "For completeness, we have added more results (on the Pendulum domain) in the Appendix C section. Moreover, we have tried to summarize our intuition on why $\\kappa$-PI and VI work well in different domains at the end of this section. We believe that this addresses the concerns about intuitive understanding using simpler domains, but would welcome additional suggestions for the final version."}, "signatures": ["ICLR.cc/2020/Conference/Paper1648/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1648/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Multi-step Greedy Policies in Model-Free Deep Reinforcement Learning", "authors": ["Yonathan Efroni", "Manan Tomar", "Mohammad Ghavamzadeh"], "authorids": ["jonathan.efroni@gmail.com", "manan.tomar@gmail.com", "mgh@fb.com"], "keywords": ["Reinforcement Learning", "Multi-step greedy policies", "Model free Reinforcement Learning"], "TL;DR": "Use model free algorithms like DQN/TRPO to solve short horizon problems (model free) iteratively in a Policy/Value Iteration fashion.", "abstract": "Multi-step greedy policies have been extensively used in model-based Reinforcement Learning (RL) and in the case when a model of the environment is available (e.g., in the game of Go). In this work, we explore the benefits of multi-step greedy policies in model-free RL when employed in the framework of multi-step Dynamic Programming (DP): multi-step Policy and Value Iteration. These algorithms iteratively solve short-horizon decision problems and converge to the optimal solution of the original one. By using model-free algorithms as solvers of the short-horizon problems we derive fully model-free algorithms which are instances of the multi-step DP framework. As model-free algorithms are prone to instabilities w.r.t. the decision problem horizon, this simple approach can help in mitigating these instabilities and results in an improved model-free algorithms. We test this approach and show results on both discrete and continuous control problems.", "pdf": "/pdf/7609d0324668ea44a1e7893ff8de39f72889aa79.pdf", "paperhash": "efroni|multistep_greedy_policies_in_modelfree_deep_reinforcement_learning", "original_pdf": "/attachment/3bbb637f5515b6060f4563d4455b11f17f6c4975.pdf", "_bibtex": "@misc{\nefroni2020multistep,\ntitle={Multi-step Greedy Policies in Model-Free Deep Reinforcement Learning},\nauthor={Yonathan Efroni and Manan Tomar and Mohammad Ghavamzadeh},\nyear={2020},\nurl={https://openreview.net/forum?id=r1l7E1HFPH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1l7E1HFPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1648/Authors", "ICLR.cc/2020/Conference/Paper1648/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1648/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1648/Reviewers", "ICLR.cc/2020/Conference/Paper1648/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1648/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1648/Authors|ICLR.cc/2020/Conference/Paper1648/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504152919, "tmdate": 1576860541123, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1648/Authors", "ICLR.cc/2020/Conference/Paper1648/Reviewers", "ICLR.cc/2020/Conference/Paper1648/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1648/-/Official_Comment"}}}, {"id": "SJeiUS9miB", "original": null, "number": 5, "cdate": 1573262674672, "ddate": null, "tcdate": 1573262674672, "tmdate": 1573281774932, "tddate": null, "forum": "r1l7E1HFPH", "replyto": "BklUIEo6_r", "invitation": "ICLR.cc/2020/Conference/Paper1648/-/Official_Comment", "content": {"title": "Response to AnonReviewer1 (2/2)", "comment": "*****Response to the Reviewer\u2019s Comments*****\nAt the beginning, we would like to bring it to the reviewer\u2019s attention that $\\kappa$ is a parameter in the range [0,1] and cannot go to infinity. $\\kappa$=0 corresponds to 1-step greedy and $\\kappa$=1 corresponds to solving the entire MDP. This fact makes the resulting algorithms easy to implement, unlike an approach that uses finite lookahead policies.  \n\nWe now provide a summary of our experiments and the lessons one can learn from them. Hope this helps the reviewer with reading the experiments, and clarifies the messages we would like to deliver in this work. \n\nThe goal behind our experiments is to compare against the DQN and TRPO baselines, which are special cases of our algorithm by setting $\\kappa$=1. \n\nThe first takeaway message is that there are non-trivial $\\kappa$ values for which we could observe better performance than DQN and TRPO. These $\\kappa$ values are different for different environments. Here are some results revealed through our work:\n \n    - We can categorize each environment with a certain range of \u2018ideal\u2019 $\\kappa$ values, e.g., either lower or higher $\\kappa$ values.\n\n    - Our results also show that in TRPO, although previous work, such as GAE, concluded to have a fixed $\\lambda$ parameter across all environments, this is certainly not true. A $\\kappa$ or a $\\lambda$ value that works well for one environment is not guaranteed to be working well for another. Therefore, the natural next step, that we are currently working on,  is to build methods that can adapt the value of $\\kappa$ based on the problem at hand.\n\nSecondly, since our methods have been derived from Policy/Value Iteration schemes, it makes sense to check how well they work when the policy evaluation and improvement steps are separated, i.e., improving for multiple time steps before evaluating the policy. We do this through the \u2018naive\u2019 baseline comparison which improves the policy for a single time-step. The results consistently show that doing a multi-step update is better. This is the second takeaway message from our work. \n\nThirdly, one can also wonder what effect does lowering the discount factor have on the problem, since the $\\kappa$-PI algorithm advocates for solving a more discounted MDP (i.e., the $\\gamma\\kappa$ MDP, instead of the $\\gamma$ MDP, at each time step). Our results show that the comparison is non-trivial, as we achieve consistently better performance with $\\kappa$ PI/VI, while lowering the discount factor actually hurts the baseline performance in most cases. This forms the third take-away of our work.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1648/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1648/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Multi-step Greedy Policies in Model-Free Deep Reinforcement Learning", "authors": ["Yonathan Efroni", "Manan Tomar", "Mohammad Ghavamzadeh"], "authorids": ["jonathan.efroni@gmail.com", "manan.tomar@gmail.com", "mgh@fb.com"], "keywords": ["Reinforcement Learning", "Multi-step greedy policies", "Model free Reinforcement Learning"], "TL;DR": "Use model free algorithms like DQN/TRPO to solve short horizon problems (model free) iteratively in a Policy/Value Iteration fashion.", "abstract": "Multi-step greedy policies have been extensively used in model-based Reinforcement Learning (RL) and in the case when a model of the environment is available (e.g., in the game of Go). In this work, we explore the benefits of multi-step greedy policies in model-free RL when employed in the framework of multi-step Dynamic Programming (DP): multi-step Policy and Value Iteration. These algorithms iteratively solve short-horizon decision problems and converge to the optimal solution of the original one. By using model-free algorithms as solvers of the short-horizon problems we derive fully model-free algorithms which are instances of the multi-step DP framework. As model-free algorithms are prone to instabilities w.r.t. the decision problem horizon, this simple approach can help in mitigating these instabilities and results in an improved model-free algorithms. We test this approach and show results on both discrete and continuous control problems.", "pdf": "/pdf/7609d0324668ea44a1e7893ff8de39f72889aa79.pdf", "paperhash": "efroni|multistep_greedy_policies_in_modelfree_deep_reinforcement_learning", "original_pdf": "/attachment/3bbb637f5515b6060f4563d4455b11f17f6c4975.pdf", "_bibtex": "@misc{\nefroni2020multistep,\ntitle={Multi-step Greedy Policies in Model-Free Deep Reinforcement Learning},\nauthor={Yonathan Efroni and Manan Tomar and Mohammad Ghavamzadeh},\nyear={2020},\nurl={https://openreview.net/forum?id=r1l7E1HFPH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1l7E1HFPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1648/Authors", "ICLR.cc/2020/Conference/Paper1648/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1648/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1648/Reviewers", "ICLR.cc/2020/Conference/Paper1648/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1648/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1648/Authors|ICLR.cc/2020/Conference/Paper1648/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504152919, "tmdate": 1576860541123, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1648/Authors", "ICLR.cc/2020/Conference/Paper1648/Reviewers", "ICLR.cc/2020/Conference/Paper1648/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1648/-/Official_Comment"}}}, {"id": "r1eXcBqXsr", "original": null, "number": 6, "cdate": 1573262730921, "ddate": null, "tcdate": 1573262730921, "tmdate": 1573262730921, "tddate": null, "forum": "r1l7E1HFPH", "replyto": "BklUIEo6_r", "invitation": "ICLR.cc/2020/Conference/Paper1648/-/Official_Comment", "content": {"title": "Response to AnonReviewer1 (1/2) ", "comment": "We would like to thank the reviewer for the comments. \n\nBefore addressing the reviewer\u2019s comments, we would like to summarize the contributions of our work.\n \n*****Summary*****\nThe advantage of lowering the discount factor in RL has been investigated in several prior work, e.g., Petrik and Scherrer (2009) and Jiang et al. (2015). However, they have shown that lowering the discount factor introduces bias, and as we empirically demonstrate in Section 5.3, this bias can lead to a deterioration in the performance.\n \nIn our work, instead of lowering the discount factor, we follow a different route that was theoretically formulated in Efroni et al. (2018a). They introduced the notion of $\\kappa$ greedy policy, from which they derived $\\kappa$-Policy Iteration ($\\kappa$-PI) and $\\kappa$-Value Iteration ($\\kappa$-VI) algorithms. In their previous work (Efroni et al., 2018a and 2018b), they only theoretically analyzed these algorithms and empirically evaluated their convergence speed in problems with small number of states (100 states at most) and where the model of the environment is known (planning setting, not learning setting). It is not obvious if their theoretical results apply when complex function approximations, such as deep neural networks, are used to solve the problem in the model-free setting (learning, not planning). In this work, we investigate extending the algorithms proposed by Efroni et al (2018a,b) to model-free and function approximation settings with both discrete and continuous actions, and show that this extension is non-trivial (as also pointed out by Reviewer 3) and care should be taken in deriving the practical versions of these algorithms (e.g., the importance of the C_{FA} parameter). Furthermore, we demonstrate the generality of the framework by showing that popular algorithms, DQN and TRPO, are special cases of our multi-step greedy framework for $\\kappa$ = 1. \n \nWe show the advantage of using $\\kappa$-PI and $\\kappa$-VI algorithms over lowering the discount factor for both value (DQN) and policy (TRPO) based algorithms, when neural networks are used as function approximator (as mentioned in the first paragraph). Our results (in Section 5.3) indicate that while the performance of DQN and TRPO degrades with lowering the discount factor, our multi-step greedy algorithms improve over DQN and TRPO.\n \nFurthermore, we test some of the consequences of the theory in Efroni et al. (2018a,b) on multi-step greedy dynamic programming. In particular, we show the advantage of using \u2018hard\u2019 updates over \u2018soft\u2019 updates, which was shown to be problematic (theoretically) by Efroni et al. (2018b). By hard and soft updates (terms used in Efroni et al., 2018b), we refer to fully solving the $\\gamma\\kappa$ MDP in a model-free manner (hard) versus changing the policy at each iteration (soft). In the \u2018hard\u2019 setting, the policy improvement and evaluation steps are separated, while in the \u2018soft\u2019 setting, they are concurrent (each policy improvement step is followed by a policy evaluation step).\n \nWe believe much more is left to be understood in applying multi-step greedy policies to RL. We consider our work as a first step towards this goal as well as showing such an approach is empirically beneficial.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1648/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1648/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Multi-step Greedy Policies in Model-Free Deep Reinforcement Learning", "authors": ["Yonathan Efroni", "Manan Tomar", "Mohammad Ghavamzadeh"], "authorids": ["jonathan.efroni@gmail.com", "manan.tomar@gmail.com", "mgh@fb.com"], "keywords": ["Reinforcement Learning", "Multi-step greedy policies", "Model free Reinforcement Learning"], "TL;DR": "Use model free algorithms like DQN/TRPO to solve short horizon problems (model free) iteratively in a Policy/Value Iteration fashion.", "abstract": "Multi-step greedy policies have been extensively used in model-based Reinforcement Learning (RL) and in the case when a model of the environment is available (e.g., in the game of Go). In this work, we explore the benefits of multi-step greedy policies in model-free RL when employed in the framework of multi-step Dynamic Programming (DP): multi-step Policy and Value Iteration. These algorithms iteratively solve short-horizon decision problems and converge to the optimal solution of the original one. By using model-free algorithms as solvers of the short-horizon problems we derive fully model-free algorithms which are instances of the multi-step DP framework. As model-free algorithms are prone to instabilities w.r.t. the decision problem horizon, this simple approach can help in mitigating these instabilities and results in an improved model-free algorithms. We test this approach and show results on both discrete and continuous control problems.", "pdf": "/pdf/7609d0324668ea44a1e7893ff8de39f72889aa79.pdf", "paperhash": "efroni|multistep_greedy_policies_in_modelfree_deep_reinforcement_learning", "original_pdf": "/attachment/3bbb637f5515b6060f4563d4455b11f17f6c4975.pdf", "_bibtex": "@misc{\nefroni2020multistep,\ntitle={Multi-step Greedy Policies in Model-Free Deep Reinforcement Learning},\nauthor={Yonathan Efroni and Manan Tomar and Mohammad Ghavamzadeh},\nyear={2020},\nurl={https://openreview.net/forum?id=r1l7E1HFPH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1l7E1HFPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1648/Authors", "ICLR.cc/2020/Conference/Paper1648/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1648/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1648/Reviewers", "ICLR.cc/2020/Conference/Paper1648/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1648/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1648/Authors|ICLR.cc/2020/Conference/Paper1648/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504152919, "tmdate": 1576860541123, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1648/Authors", "ICLR.cc/2020/Conference/Paper1648/Reviewers", "ICLR.cc/2020/Conference/Paper1648/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1648/-/Official_Comment"}}}, {"id": "BylDpXqQoH", "original": null, "number": 2, "cdate": 1573262271216, "ddate": null, "tcdate": 1573262271216, "tmdate": 1573262507713, "tddate": null, "forum": "r1l7E1HFPH", "replyto": "BJeblVM6tr", "invitation": "ICLR.cc/2020/Conference/Paper1648/-/Official_Comment", "content": {"title": "Response to AnonReviewer2 (2/2)", "comment": "\u201cno mention of our TRPO work in the review\u201d\nThe review only mentions our DQN work and does not talk about our TRPO algorithms and experiments. We are sorry if we did not present this part of our work clearly enough. We will improve the presentation of this part in the final version of the paper. To clarify, we experimented with both DQN and TRPO extensions of our approach. As stated in the paper, the TRPO extension resembles the practically used GAE (Generalized Advantage Estimation) algorithm, with a crucial difference that in GAE the value and policy are concurrently updated (each policy improvement step is followed by a policy evaluation step), while in our work, we emphasize the need to do multiple step improvement before evaluating the policy. The theoretical results of Efroni et al. (2018b) suggest that the concurrent update approach used by GAE does not necessarily result in an improving algorithm, which hints that using this approach might be problematic. We conjecture that the reason that this issue does not lead to significant performance deterioration in GAE is that most MuJoCo continuous control tasks are inherently of short horizon. In fact, our experiments show that in the Atari domains concurrently learning the policy and value leads to inferior performance."}, "signatures": ["ICLR.cc/2020/Conference/Paper1648/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1648/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Multi-step Greedy Policies in Model-Free Deep Reinforcement Learning", "authors": ["Yonathan Efroni", "Manan Tomar", "Mohammad Ghavamzadeh"], "authorids": ["jonathan.efroni@gmail.com", "manan.tomar@gmail.com", "mgh@fb.com"], "keywords": ["Reinforcement Learning", "Multi-step greedy policies", "Model free Reinforcement Learning"], "TL;DR": "Use model free algorithms like DQN/TRPO to solve short horizon problems (model free) iteratively in a Policy/Value Iteration fashion.", "abstract": "Multi-step greedy policies have been extensively used in model-based Reinforcement Learning (RL) and in the case when a model of the environment is available (e.g., in the game of Go). In this work, we explore the benefits of multi-step greedy policies in model-free RL when employed in the framework of multi-step Dynamic Programming (DP): multi-step Policy and Value Iteration. These algorithms iteratively solve short-horizon decision problems and converge to the optimal solution of the original one. By using model-free algorithms as solvers of the short-horizon problems we derive fully model-free algorithms which are instances of the multi-step DP framework. As model-free algorithms are prone to instabilities w.r.t. the decision problem horizon, this simple approach can help in mitigating these instabilities and results in an improved model-free algorithms. We test this approach and show results on both discrete and continuous control problems.", "pdf": "/pdf/7609d0324668ea44a1e7893ff8de39f72889aa79.pdf", "paperhash": "efroni|multistep_greedy_policies_in_modelfree_deep_reinforcement_learning", "original_pdf": "/attachment/3bbb637f5515b6060f4563d4455b11f17f6c4975.pdf", "_bibtex": "@misc{\nefroni2020multistep,\ntitle={Multi-step Greedy Policies in Model-Free Deep Reinforcement Learning},\nauthor={Yonathan Efroni and Manan Tomar and Mohammad Ghavamzadeh},\nyear={2020},\nurl={https://openreview.net/forum?id=r1l7E1HFPH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1l7E1HFPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1648/Authors", "ICLR.cc/2020/Conference/Paper1648/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1648/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1648/Reviewers", "ICLR.cc/2020/Conference/Paper1648/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1648/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1648/Authors|ICLR.cc/2020/Conference/Paper1648/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504152919, "tmdate": 1576860541123, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1648/Authors", "ICLR.cc/2020/Conference/Paper1648/Reviewers", "ICLR.cc/2020/Conference/Paper1648/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1648/-/Official_Comment"}}}, {"id": "SJguBE9QsH", "original": null, "number": 4, "cdate": 1573262400391, "ddate": null, "tcdate": 1573262400391, "tmdate": 1573262471293, "tddate": null, "forum": "r1l7E1HFPH", "replyto": "BJeblVM6tr", "invitation": "ICLR.cc/2020/Conference/Paper1648/-/Official_Comment", "content": {"title": "Response to AnonReviewer2 (1/2) ", "comment": "We would like to thank the reviewer for the comments. \n\nBefore addressing the reviewer\u2019s comments, we would like to summarize the contributions of our work.\n \n*****Summary*****\nThe advantage of lowering the discount factor in RL has been investigated in several prior work, e.g., Petrik and Scherrer (2009) and Jiang et al. (2015). However, they have shown that lowering the discount factor introduces bias, and as we empirically demonstrate in Section 5.3, this bias can lead to a deterioration in the performance.\n \nIn our work, instead of lowering the discount factor, we follow a different route that was theoretically formulated in Efroni et al. (2018a). They introduced the notion of $\\kappa$ greedy policy, from which they derived $\\kappa$-Policy Iteration ($\\kappa$-PI) and $\\kappa$-Value Iteration ($\\kappa$-VI) algorithms. In their previous work (Efroni et al., 2018a and 2018b), they only theoretically analyzed these algorithms and empirically evaluated their convergence speed in problems with small number of states (100 states at most) and where the model of the environment is known (planning setting, not learning setting). It is not obvious if their theoretical results apply when complex function approximations, such as deep neural networks, are used to solve the problem in the model-free setting (learning, not planning). In this work, we investigate extending the algorithms proposed by Efroni et al (2018a,b) to model-free and function approximation settings with both discrete and continuous actions, and show that this extension is non-trivial (as also pointed out by Reviewer 3) and care should be taken in deriving the practical versions of these algorithms (e.g., the importance of the C_{FA} parameter). Furthermore, we demonstrate the generality of the framework by showing that popular algorithms, DQN and TRPO, are special cases of our multi-step greedy framework for $\\kappa$ = 1. \n \nWe show the advantage of using $\\kappa$-PI and $\\kappa$-VI algorithms over lowering the discount factor for both value (DQN) and policy (TRPO) based algorithms, when neural networks are used as function approximator (as mentioned in the first paragraph). Our results (in Section 5.3) indicate that while the performance of DQN and TRPO degrades with lowering the discount factor, our multi-step greedy algorithms improve over DQN and TRPO.\n \nFurthermore, we test some of the consequences of the theory in Efroni et al. (2018a,b) on multi-step greedy dynamic programming. In particular, we show the advantage of using \u2018hard\u2019 updates over \u2018soft\u2019 updates, which was shown to be problematic (theoretically) by Efroni et al. (2018b). By hard and soft updates (terms used in Efroni et al., 2018b), we refer to fully solving the $\\gamma\\kappa$ MDP in a model-free manner (hard) versus changing the policy at each iteration (soft). In the \u2018hard\u2019 setting, the policy improvement and evaluation steps are separated, while in the \u2018soft\u2019 setting, they are concurrent (each policy improvement step is followed by a policy evaluation step).\n \nWe believe much more is left to be understood in applying multi-step greedy policies to RL. We consider our work as a first step towards this goal as well as showing such an approach is empirically beneficial.\n\n*****Response to the Reviewer\u2019s Comments*****\n\u201cthis work extends Efroni et al. (2018b) from tabular to function approximation\u201d\nThis reviewer\u2019s statement is not completely accurate. The work of Efroni et al (2018a) and (2018b) focused on theoretical analysis of $\\kappa$ PI/VI algorithms, and their experiments are in small (tabular) problems, where the model is given (the setting is planning, not learning). Our work is the first one to use multi-step greedy policies with neural networks as function approximator, in model-free RL (learning and not planning setting)."}, "signatures": ["ICLR.cc/2020/Conference/Paper1648/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1648/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Multi-step Greedy Policies in Model-Free Deep Reinforcement Learning", "authors": ["Yonathan Efroni", "Manan Tomar", "Mohammad Ghavamzadeh"], "authorids": ["jonathan.efroni@gmail.com", "manan.tomar@gmail.com", "mgh@fb.com"], "keywords": ["Reinforcement Learning", "Multi-step greedy policies", "Model free Reinforcement Learning"], "TL;DR": "Use model free algorithms like DQN/TRPO to solve short horizon problems (model free) iteratively in a Policy/Value Iteration fashion.", "abstract": "Multi-step greedy policies have been extensively used in model-based Reinforcement Learning (RL) and in the case when a model of the environment is available (e.g., in the game of Go). In this work, we explore the benefits of multi-step greedy policies in model-free RL when employed in the framework of multi-step Dynamic Programming (DP): multi-step Policy and Value Iteration. These algorithms iteratively solve short-horizon decision problems and converge to the optimal solution of the original one. By using model-free algorithms as solvers of the short-horizon problems we derive fully model-free algorithms which are instances of the multi-step DP framework. As model-free algorithms are prone to instabilities w.r.t. the decision problem horizon, this simple approach can help in mitigating these instabilities and results in an improved model-free algorithms. We test this approach and show results on both discrete and continuous control problems.", "pdf": "/pdf/7609d0324668ea44a1e7893ff8de39f72889aa79.pdf", "paperhash": "efroni|multistep_greedy_policies_in_modelfree_deep_reinforcement_learning", "original_pdf": "/attachment/3bbb637f5515b6060f4563d4455b11f17f6c4975.pdf", "_bibtex": "@misc{\nefroni2020multistep,\ntitle={Multi-step Greedy Policies in Model-Free Deep Reinforcement Learning},\nauthor={Yonathan Efroni and Manan Tomar and Mohammad Ghavamzadeh},\nyear={2020},\nurl={https://openreview.net/forum?id=r1l7E1HFPH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1l7E1HFPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1648/Authors", "ICLR.cc/2020/Conference/Paper1648/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1648/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1648/Reviewers", "ICLR.cc/2020/Conference/Paper1648/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1648/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1648/Authors|ICLR.cc/2020/Conference/Paper1648/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504152919, "tmdate": 1576860541123, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1648/Authors", "ICLR.cc/2020/Conference/Paper1648/Reviewers", "ICLR.cc/2020/Conference/Paper1648/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1648/-/Official_Comment"}}}, {"id": "BklUIEo6_r", "original": null, "number": 1, "cdate": 1570776141826, "ddate": null, "tcdate": 1570776141826, "tmdate": 1572972441234, "tddate": null, "forum": "r1l7E1HFPH", "replyto": "r1l7E1HFPH", "invitation": "ICLR.cc/2020/Conference/Paper1648/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper focuses on the implementation and some empirical evaluations of a class of algorithms designed to find optimal strategies/values of large MDP.\n\nThe basic idea of these algorithm (called \\kappa-PI or \\kappa-VI) is to combine two type of classical approaches:\n- policy/value iteration \n- k-step ahead computation (instead of just 1-ahead, and actually, k should be quite big or even infinite with an auxiliary appropriate discount rate).\n\nThe theoretical formulation of \\kappa-PI and \\kappa-VI involves solving, at each iteration, another auxiliary MDP problem (where the discount rate is of order \\kappa\\gamma). This is basically what this paper suggests to do, and implements. \n\nThe experiments are a bit difficult for me to read, as the baselines (\\kappa=0 and =1, say) are compared with \"the best \\kappa\" which seems to be problem dependent, so I do not know if there is a clear message."}, "signatures": ["ICLR.cc/2020/Conference/Paper1648/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1648/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Multi-step Greedy Policies in Model-Free Deep Reinforcement Learning", "authors": ["Yonathan Efroni", "Manan Tomar", "Mohammad Ghavamzadeh"], "authorids": ["jonathan.efroni@gmail.com", "manan.tomar@gmail.com", "mgh@fb.com"], "keywords": ["Reinforcement Learning", "Multi-step greedy policies", "Model free Reinforcement Learning"], "TL;DR": "Use model free algorithms like DQN/TRPO to solve short horizon problems (model free) iteratively in a Policy/Value Iteration fashion.", "abstract": "Multi-step greedy policies have been extensively used in model-based Reinforcement Learning (RL) and in the case when a model of the environment is available (e.g., in the game of Go). In this work, we explore the benefits of multi-step greedy policies in model-free RL when employed in the framework of multi-step Dynamic Programming (DP): multi-step Policy and Value Iteration. These algorithms iteratively solve short-horizon decision problems and converge to the optimal solution of the original one. By using model-free algorithms as solvers of the short-horizon problems we derive fully model-free algorithms which are instances of the multi-step DP framework. As model-free algorithms are prone to instabilities w.r.t. the decision problem horizon, this simple approach can help in mitigating these instabilities and results in an improved model-free algorithms. We test this approach and show results on both discrete and continuous control problems.", "pdf": "/pdf/7609d0324668ea44a1e7893ff8de39f72889aa79.pdf", "paperhash": "efroni|multistep_greedy_policies_in_modelfree_deep_reinforcement_learning", "original_pdf": "/attachment/3bbb637f5515b6060f4563d4455b11f17f6c4975.pdf", "_bibtex": "@misc{\nefroni2020multistep,\ntitle={Multi-step Greedy Policies in Model-Free Deep Reinforcement Learning},\nauthor={Yonathan Efroni and Manan Tomar and Mohammad Ghavamzadeh},\nyear={2020},\nurl={https://openreview.net/forum?id=r1l7E1HFPH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "r1l7E1HFPH", "replyto": "r1l7E1HFPH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1648/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1648/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575497393542, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1648/Reviewers"], "noninvitees": [], "tcdate": 1570237734316, "tmdate": 1575497393556, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1648/-/Official_Review"}}}, {"id": "BJeblVM6tr", "original": null, "number": 2, "cdate": 1571787753220, "ddate": null, "tcdate": 1571787753220, "tmdate": 1572972441201, "tddate": null, "forum": "r1l7E1HFPH", "replyto": "r1l7E1HFPH", "invitation": "ICLR.cc/2020/Conference/Paper1648/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The main contributions of this paper are k-PI-DQN and k-VI-DQN, which are model-free versions of dynamic programming (DP) methods k-PI and k-VI from another paper (Efroni et al., 2018).  The deep architecture of the two algorithms follows that of DQN.  Efroni et al. (2018b) already gave a stochastic online (model-free) version of k-PI in the tabular setting.  Although this paper is going one step further extending from tabular to function approximation, I feel that the paper just combined known results, the shaped reward from Efroni et al (2018a) and DQN.  The extension seems straightforward.  Mentioning previous results from Efroni et al (2018a) and (2018b) does not justify the extension would possess the same property or behaviour.   The experiments were only comparing their methods with different hyperparameters, with only a brief comparison to DQN.  "}, "signatures": ["ICLR.cc/2020/Conference/Paper1648/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1648/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Multi-step Greedy Policies in Model-Free Deep Reinforcement Learning", "authors": ["Yonathan Efroni", "Manan Tomar", "Mohammad Ghavamzadeh"], "authorids": ["jonathan.efroni@gmail.com", "manan.tomar@gmail.com", "mgh@fb.com"], "keywords": ["Reinforcement Learning", "Multi-step greedy policies", "Model free Reinforcement Learning"], "TL;DR": "Use model free algorithms like DQN/TRPO to solve short horizon problems (model free) iteratively in a Policy/Value Iteration fashion.", "abstract": "Multi-step greedy policies have been extensively used in model-based Reinforcement Learning (RL) and in the case when a model of the environment is available (e.g., in the game of Go). In this work, we explore the benefits of multi-step greedy policies in model-free RL when employed in the framework of multi-step Dynamic Programming (DP): multi-step Policy and Value Iteration. These algorithms iteratively solve short-horizon decision problems and converge to the optimal solution of the original one. By using model-free algorithms as solvers of the short-horizon problems we derive fully model-free algorithms which are instances of the multi-step DP framework. As model-free algorithms are prone to instabilities w.r.t. the decision problem horizon, this simple approach can help in mitigating these instabilities and results in an improved model-free algorithms. We test this approach and show results on both discrete and continuous control problems.", "pdf": "/pdf/7609d0324668ea44a1e7893ff8de39f72889aa79.pdf", "paperhash": "efroni|multistep_greedy_policies_in_modelfree_deep_reinforcement_learning", "original_pdf": "/attachment/3bbb637f5515b6060f4563d4455b11f17f6c4975.pdf", "_bibtex": "@misc{\nefroni2020multistep,\ntitle={Multi-step Greedy Policies in Model-Free Deep Reinforcement Learning},\nauthor={Yonathan Efroni and Manan Tomar and Mohammad Ghavamzadeh},\nyear={2020},\nurl={https://openreview.net/forum?id=r1l7E1HFPH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "r1l7E1HFPH", "replyto": "r1l7E1HFPH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1648/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1648/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575497393542, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1648/Reviewers"], "noninvitees": [], "tcdate": 1570237734316, "tmdate": 1575497393556, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1648/-/Official_Review"}}}, {"id": "H1geTaI6KS", "original": null, "number": 3, "cdate": 1571806647832, "ddate": null, "tcdate": 1571806647832, "tmdate": 1572972441157, "tddate": null, "forum": "r1l7E1HFPH", "replyto": "r1l7E1HFPH", "invitation": "ICLR.cc/2020/Conference/Paper1648/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "===== Summary =====\nThe paper proposes an extension of multi-step dynamic programming algorithms from Efroni, Dalal, Scherrer, and Mannor (2018a, 2018b) to the reinforcement learning setting with function approximation. The multi-step dynamic programming algorithms proposed by Efroni et. al. (2018a)  find the solution of the h-step optimal Bellman operator, which applies the maximum over the next h sequence of actions. Moreover, Efroni et. al. (2018a) also showed an equivalence between h-step optimal Bellman operators and k-Policy Iteration (k-PI) and k-Value Iteration (k-VI) algorithms, which, similar to TD( \ud835\udf06 ) but for policy improvement, take a geometric average of all future h-step returns weighted by k. The paper extends the work from Efroni et. al. (2018a, 2018b) to the deep reinforcement learning setting by proposing an approximate k-PI and k-VI algorithm based on DQN and TRPO. Finally, the paper provides empirical evaluations of k-PI and k-VI with DQN in several Atari games and of k-PI and k-VI with TRPO in several MuJoCo environments with continuous actions paces. \n\nContributions:\n1. The paper proposes a non-trivial extension for k-PI and k-VI to use function approximation via the DQN algorithm. \n2. Similarly, the paper proposes a non-trivial extension for k-PI and k-VI to use function approximation with continuous action spaces via the TRPO algorithm. \n3. The paper provides empirical evaluations of the four proposed algorithms and, at least for the k-PI algorithm with DQN and TRPO, demonstrates an improvement over the baselines. \n\n===== Decision =====\nThe paper represents a natural next step to the work of Efroni et. al. (2018a, 2018b). The paper extends the applicability of multi-step greedy policies to more complex environments and shows a statistically significant improvement in performance compared to the methods that it builds upon. Additionally, the ideas are presented clearly and incrementally throughout the paper, which makes it flow nicely until the part where k-PI and k-VI DQN and TRPO are introduced. This is my main complaint about the paper, the lack of simple and intuitive understanding about k-PI and k-VI with function approximation due to the complicated architectures associated with DQN and TRPO. For this reason, my rating of the paper is weak accept.\n\n===== Detailed Comments about Decision =====\nAll of these are comments for which I would consider increasing my score if they were addressed. \n\n=== Empirical Evaluations ===\nFirst, my main complaint is the complicated architectures and complex domains used to gain insights about k-PI and k-VI with function approximation. Big demonstrations in Atari and MuJoCo are important, but in the case of very new algorithms such as these ones, I consider it to be more important to gain insight through small domains that allow us to dig deep into the algorithms. Any small domain that would allow for big sample sizes for ablation and parameter studies would be more insightful than big demonstrations with very small sample sizes. I do not mean to be dismissive about what has been done in the paper, but it would be a great source of insight and a big improvement to what has already been done if a simple demonstration was presented in the paper. \n\nMy suggestion would be to use a simple approximation method, such as Tile Coding with linear function approximation, in small a domain such as mountain car. This would allow for a bigger sample size and a parameter study that could provide more insight about the role of the parameters k and C_{FA} on the performance of k-PI and k-VI. \n\nAdditionally, one of the claims in the conclusions was never emphasized in the results: \u201cimportantly, the performance of the algorithms was shown to be \u2018smooth\u2019  in the parameter k.\u201d This was not completely obvious until I spent some time looking closely at the graph. It eventually became clear, but I think a simpler way to emphasize this is to show a plot of the cumulative reward over the whole training period with the values of k on the x-axis. Based on the top right pane of FIgure 1, this type of plot would show a smooth increase from k=0.99 to k=0.68 followed by a smooth decrease from k=0.68 to k=0. \n\nFinally, I have some questions about some of the choices made in the experiments and results sections:\n\n1. Why choose 50% confidence intervals? 50% confidence intervals with a sample size of 4 in the case of DQN and 5 in the case of TRPO is equivalent to multiplying the standard error by a factor of approximately 0.7, which is narrower than using the standard error on its own. Thus, it seems that some of the conclusions would change based on using a 95% confidence interval compared to a 50% confidence interval in Tables 1 and 2. I insist in showing the performance in a small domain with a simple form of function approximation. This would complement the Atari and MuJoCo experiments by showing improvements in performance with a higher confidence. \n\n2. In remark one, it is pointed out that another target network \\tilde Q should be used to obtain \\pi_{t-1}, but this was not done to reduce the space complexity of the algorithm. How big were the networks that you used for k-PI DQN? If the network was not prohibitively big, why not implement \\tilde Q instead of using an alternative that further deviates from the original k-PI algorithm? \n\n3. Line 19 of Algorithm 5 in Appendix A.1 is supposed to be the off-policy TD(0) update. However, it is not clear how this update is off-policy TD(0) since it based on Q and it does not have any importance sampling to correct for the difference in policies. Am I missing something? It seems that it should be off-policy Sarsa(0), but even then it would still be missing an importance sampling term (see Sutton & Barto, 2018, Equation 7.11, or Algorithm 1 of Precup, Sutton, and Singh, 2000, for more information).\n\n=== Contradictory Claims in the Results ===\nThere are a few claims that contradict with what is shown in Table 1 and 2.\n\nIn the last paragraph of Section 5.1.1 it says that \u201c[the table 1] show[s] that setting N(k) = T leads to a clear degradation of the final training performance on all the domains except Enduro.\u201d This is only true in two out of four games presented in Table 1. In Seaquest the lower confidence bound of the performance of k-PI with k=0.68 is 4643, whereas the upper confidence bound of the performance of k-PI with N(k) = T is 4837; the intervals clearly overlap. Similarly, in the game of Enduro, where k-PI with N(k) = T is said to have better performance, the lower confidence bound of k-PI with N(k) =T is 530, whereas for k-PI with k=0.84 the upper confidence bound is 575; again, the confidence intervals overlap. Hence, neither of these two claims are fully justified, and it is certainly not a \u201cclear degradation of the final training performance.\u201d\n\nSimilarly, in Section 5.2.2, k-PI is said to have a better performance than N(k) = T based on the results of Table 2. However, similar calculations show that this is only true for the Ant domain.\n\n===== Minor Comments =====\n1. I believe there is a typo in the last column of Table 1, it should be a \\kappa instead of a \ud835\udf06.\n\n2. In the second paragraph above Equation 7, the convergence of PI and VI are said to converge to the optimal value with linear rate, but the rate of convergence is O( \\gamma^N ), i.e., exponential. Similarly, for the k-PI and k-VI their rate of convergence is O( \\ksi ( \\kappa )^{N( \\kappa )} ), which is also exponential. \n\n===== References =====\nPrecup, Doina; Sutton, Richard S.; and Singh, Satinder, \"Eligibility Traces for Off-Policy Policy Evaluation\" (2000).ICML '00 Proceedings of the Seventeenth International Conference on Machine Learning. 80.Retrieved fromhttps://scholarworks.umass.edu/cs_faculty_pubs/80\n\nR. Sutton and A. Barto. Reinforcement learning: An introduction. 2018.\n\nY. Efroni, G. Dalal, B. Scherrer, and S. Mannor. Beyond the one step greedy approach in reinforcement learning. In Proceedings of the 35th International Conference on Machine Learning, 2018a.\n\nY. Efroni, G. Dalal, B. Scherrer, and S. Mannor. Multiple-step greedy policies in approximate and online reinforcement learning. In Advances in Neural Information Processing Systems, pp. 5238\u20135247, 2018b.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1648/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1648/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Multi-step Greedy Policies in Model-Free Deep Reinforcement Learning", "authors": ["Yonathan Efroni", "Manan Tomar", "Mohammad Ghavamzadeh"], "authorids": ["jonathan.efroni@gmail.com", "manan.tomar@gmail.com", "mgh@fb.com"], "keywords": ["Reinforcement Learning", "Multi-step greedy policies", "Model free Reinforcement Learning"], "TL;DR": "Use model free algorithms like DQN/TRPO to solve short horizon problems (model free) iteratively in a Policy/Value Iteration fashion.", "abstract": "Multi-step greedy policies have been extensively used in model-based Reinforcement Learning (RL) and in the case when a model of the environment is available (e.g., in the game of Go). In this work, we explore the benefits of multi-step greedy policies in model-free RL when employed in the framework of multi-step Dynamic Programming (DP): multi-step Policy and Value Iteration. These algorithms iteratively solve short-horizon decision problems and converge to the optimal solution of the original one. By using model-free algorithms as solvers of the short-horizon problems we derive fully model-free algorithms which are instances of the multi-step DP framework. As model-free algorithms are prone to instabilities w.r.t. the decision problem horizon, this simple approach can help in mitigating these instabilities and results in an improved model-free algorithms. We test this approach and show results on both discrete and continuous control problems.", "pdf": "/pdf/7609d0324668ea44a1e7893ff8de39f72889aa79.pdf", "paperhash": "efroni|multistep_greedy_policies_in_modelfree_deep_reinforcement_learning", "original_pdf": "/attachment/3bbb637f5515b6060f4563d4455b11f17f6c4975.pdf", "_bibtex": "@misc{\nefroni2020multistep,\ntitle={Multi-step Greedy Policies in Model-Free Deep Reinforcement Learning},\nauthor={Yonathan Efroni and Manan Tomar and Mohammad Ghavamzadeh},\nyear={2020},\nurl={https://openreview.net/forum?id=r1l7E1HFPH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "r1l7E1HFPH", "replyto": "r1l7E1HFPH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1648/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1648/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575497393542, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1648/Reviewers"], "noninvitees": [], "tcdate": 1570237734316, "tmdate": 1575497393556, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1648/-/Official_Review"}}}], "count": 13}