{"notes": [{"id": "r1lnigSFDr", "original": "S1lYaJbKDB", "number": 2516, "cdate": 1569439908498, "ddate": null, "tcdate": 1569439908498, "tmdate": 1577168237996, "tddate": null, "forum": "r1lnigSFDr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["gua@google.com", "caglarg@google.com", "mwhoffman@google.com", "razp@google.com"], "title": "Improving the Gating Mechanism of Recurrent Neural Networks", "authors": ["Albert Gua", "Caglar Gulcehre", "Tom le Paine", "Razvan Pascanu", "Matt Hoffman"], "pdf": "/pdf/ee6aa8f1d0d1d8ec5841207e08caae0971cc7fa6.pdf", "TL;DR": "Improving the gating mechanisms of recurrent neural networks by addressing the initialization of the biases and the saturation problem of sigmoid.", "abstract": "In this work, we revisit the gating mechanisms widely used in various recurrent and feedforward networks such as LSTMs, GRUs, or highway networks. These gates are meant to control information flow, allowing gradients to better propagate back in time for recurrent models. However, to propagate gradients over very long temporal windows, they need to operate close to their saturation regime. We propose two independent and synergistic modifications to the standard gating mechanism that are easy to implement, introduce no additional hyper-parameters, and are aimed at improving learnability of the gates when they are close to saturation. Our proposals are theoretically justified, and we show a generic framework that encompasses other recently proposed gating mechanisms such as chrono-initialization and master gates . We perform systematic analyses and ablation studies on the proposed improvements and evaluate our method on a wide range of applications including synthetic memorization tasks, sequential image classification, language modeling, and reinforcement learning.  Empirically, our proposed gating mechanisms robustly increase the performance of recurrent models such as LSTMs, especially on tasks requiring long temporal dependencies.", "keywords": ["recurrent neural networks", "LSTM", "GRUs", "gating mechanisms", "deep learning", "reinforcement learning"], "paperhash": "gua|improving_the_gating_mechanism_of_recurrent_neural_networks", "original_pdf": "/attachment/f9ddc5df9b4c1f1864a74ab1b88ebd1a6c6de21e.pdf", "_bibtex": "@misc{\ngua2020improving,\ntitle={Improving the Gating Mechanism of Recurrent Neural Networks},\nauthor={Albert Gua and Caglar Gulcehre and Tom le Paine and Razvan Pascanu and Matt Hoffman},\nyear={2020},\nurl={https://openreview.net/forum?id=r1lnigSFDr}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "seWRbvb0vy", "original": null, "number": 1, "cdate": 1576798750945, "ddate": null, "tcdate": 1576798750945, "tmdate": 1576800884759, "tddate": null, "forum": "r1lnigSFDr", "replyto": "r1lnigSFDr", "invitation": "ICLR.cc/2020/Conference/Paper2516/-/Decision", "content": {"decision": "Reject", "comment": "This submission proposes a new gating mechanism to improve gradient information propagation during back-propagation when training recurrent neural networks.\n\nStrengths:\n-The problem is interesting and important.\n-The proposed method is novel.\n\nWeaknesses:\n-The justification and motivation of the UGI mechanism was not clear and/or convincing.\n-The experimental validation is sometimes hard to interpret and the proposed improvements of the gating mechanism are not well-reflected in the quantitative results.\n-The submission was hard to read and some images were initially illegible.\n\nThe authors improved several of the weaknesses but not to the desired level.\n\nAC agrees with the majority recommendation to reject.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["gua@google.com", "caglarg@google.com", "mwhoffman@google.com", "razp@google.com"], "title": "Improving the Gating Mechanism of Recurrent Neural Networks", "authors": ["Albert Gua", "Caglar Gulcehre", "Tom le Paine", "Razvan Pascanu", "Matt Hoffman"], "pdf": "/pdf/ee6aa8f1d0d1d8ec5841207e08caae0971cc7fa6.pdf", "TL;DR": "Improving the gating mechanisms of recurrent neural networks by addressing the initialization of the biases and the saturation problem of sigmoid.", "abstract": "In this work, we revisit the gating mechanisms widely used in various recurrent and feedforward networks such as LSTMs, GRUs, or highway networks. These gates are meant to control information flow, allowing gradients to better propagate back in time for recurrent models. However, to propagate gradients over very long temporal windows, they need to operate close to their saturation regime. We propose two independent and synergistic modifications to the standard gating mechanism that are easy to implement, introduce no additional hyper-parameters, and are aimed at improving learnability of the gates when they are close to saturation. Our proposals are theoretically justified, and we show a generic framework that encompasses other recently proposed gating mechanisms such as chrono-initialization and master gates . We perform systematic analyses and ablation studies on the proposed improvements and evaluate our method on a wide range of applications including synthetic memorization tasks, sequential image classification, language modeling, and reinforcement learning.  Empirically, our proposed gating mechanisms robustly increase the performance of recurrent models such as LSTMs, especially on tasks requiring long temporal dependencies.", "keywords": ["recurrent neural networks", "LSTM", "GRUs", "gating mechanisms", "deep learning", "reinforcement learning"], "paperhash": "gua|improving_the_gating_mechanism_of_recurrent_neural_networks", "original_pdf": "/attachment/f9ddc5df9b4c1f1864a74ab1b88ebd1a6c6de21e.pdf", "_bibtex": "@misc{\ngua2020improving,\ntitle={Improving the Gating Mechanism of Recurrent Neural Networks},\nauthor={Albert Gua and Caglar Gulcehre and Tom le Paine and Razvan Pascanu and Matt Hoffman},\nyear={2020},\nurl={https://openreview.net/forum?id=r1lnigSFDr}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "r1lnigSFDr", "replyto": "r1lnigSFDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795723950, "tmdate": 1576800275515, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2516/-/Decision"}}}, {"id": "r1la3UHzoB", "original": null, "number": 8, "cdate": 1573177012520, "ddate": null, "tcdate": 1573177012520, "tmdate": 1573177012520, "tddate": null, "forum": "r1lnigSFDr", "replyto": "rkl3__O0tr", "invitation": "ICLR.cc/2020/Conference/Paper2516/-/Official_Comment", "content": {"title": "Thank you for your feedback", "comment": "We thank the reviewer for pointing out potentially confusing aspects of the submission, which we address below.\n\n>>>   a. In Figure 3(a), where are the other baselines? Are they performing too badly so that they can not show up in the figure? It needs more explanation.\n     b.  In Figure 3(b), actually a lot of methods are performing similar, while some methods converge similarly. What is the reason?\n\nSome baselines seem to not show up because their curves are actually overlapping. This is because the data in our experiments were controlled between methods (i.e., every method saw the same training and testing minibatches for a given seed) to reduce potential variance. Consequently, models that were unable to make any progress on this task showed identical performance. We have added these details in the caption and in Appendix [].\n\nWe have additionally re-plotted these results on a log scale, which distinguishes the learning curves better.\n\n\n>>> There are several parts in the experiment that are not very convincing.\nAside from details in Figure 3, could you point to other experiments that could be clarified?\n\n\n>>> It is not defined why the uniform gate initialization works.\nPlease see our shared response to all reviewers.\n\n\n>>> The proposed results actually not always perform the best. For instance, in Table 3, purely using the UR-LSTM only achieve good results on sMNIST. What is the reason? The proposed method seems not very general.\n\nWe respectfully disagree with this conclusion. In response to Table 3:\n    -  First, there was a typo that has been fixed: the \u201cGRU + Zoneout\u201d method that achieved SOTA on sCIFAR is actually our method \u201cUR-GRU + Zoneout\u201d. We apologize for the confusion this may have caused\n    -  On sCIFAR, Table 3 shows that the UR-LSTM by itself improves on a vanilla LSTM or Transformer by almost 10% accuracy. It is also the best recurrent model on pMNIST already\n    -  As mentioned in that section, our gate modifications synergize with other methods such as regularization and auxiliary losses. Table 3 shows how it synergizes with a regularization technique to achieve SOTA on sCIFAR, and we expect that the auxiliary loss of the r-LSTM (Trinh et al. 2018) can be applied to the UR-LSTM for further improvements on all datasets\n    -  We show non-recurrent models for completeness, but they are not directly comparable to ours. For example, they should be expected to be better on permuted datasets due to having global receptive field\n\nOverall, we believe that our method is very general and robust, which is shown throughout the Experiments section, where it improves on multiple types of recurrent models across many different domains.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2516/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2516/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["gua@google.com", "caglarg@google.com", "mwhoffman@google.com", "razp@google.com"], "title": "Improving the Gating Mechanism of Recurrent Neural Networks", "authors": ["Albert Gua", "Caglar Gulcehre", "Tom le Paine", "Razvan Pascanu", "Matt Hoffman"], "pdf": "/pdf/ee6aa8f1d0d1d8ec5841207e08caae0971cc7fa6.pdf", "TL;DR": "Improving the gating mechanisms of recurrent neural networks by addressing the initialization of the biases and the saturation problem of sigmoid.", "abstract": "In this work, we revisit the gating mechanisms widely used in various recurrent and feedforward networks such as LSTMs, GRUs, or highway networks. These gates are meant to control information flow, allowing gradients to better propagate back in time for recurrent models. However, to propagate gradients over very long temporal windows, they need to operate close to their saturation regime. We propose two independent and synergistic modifications to the standard gating mechanism that are easy to implement, introduce no additional hyper-parameters, and are aimed at improving learnability of the gates when they are close to saturation. Our proposals are theoretically justified, and we show a generic framework that encompasses other recently proposed gating mechanisms such as chrono-initialization and master gates . We perform systematic analyses and ablation studies on the proposed improvements and evaluate our method on a wide range of applications including synthetic memorization tasks, sequential image classification, language modeling, and reinforcement learning.  Empirically, our proposed gating mechanisms robustly increase the performance of recurrent models such as LSTMs, especially on tasks requiring long temporal dependencies.", "keywords": ["recurrent neural networks", "LSTM", "GRUs", "gating mechanisms", "deep learning", "reinforcement learning"], "paperhash": "gua|improving_the_gating_mechanism_of_recurrent_neural_networks", "original_pdf": "/attachment/f9ddc5df9b4c1f1864a74ab1b88ebd1a6c6de21e.pdf", "_bibtex": "@misc{\ngua2020improving,\ntitle={Improving the Gating Mechanism of Recurrent Neural Networks},\nauthor={Albert Gua and Caglar Gulcehre and Tom le Paine and Razvan Pascanu and Matt Hoffman},\nyear={2020},\nurl={https://openreview.net/forum?id=r1lnigSFDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1lnigSFDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2516/Authors", "ICLR.cc/2020/Conference/Paper2516/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2516/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2516/Reviewers", "ICLR.cc/2020/Conference/Paper2516/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2516/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2516/Authors|ICLR.cc/2020/Conference/Paper2516/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504140198, "tmdate": 1576860552030, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2516/Authors", "ICLR.cc/2020/Conference/Paper2516/Reviewers", "ICLR.cc/2020/Conference/Paper2516/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2516/-/Official_Comment"}}}, {"id": "rkewrIrMsS", "original": null, "number": 7, "cdate": 1573176894548, "ddate": null, "tcdate": 1573176894548, "tmdate": 1573176894548, "tddate": null, "forum": "r1lnigSFDr", "replyto": "rJxC9rZ2FH", "invitation": "ICLR.cc/2020/Conference/Paper2516/-/Official_Comment", "content": {"title": "Thank you for your feedback", "comment": "We thank the reviewer for the helpful feedback and suggestions, which we respond to below.\n\n>>> The writing in the description of the UGI and refine fate is not clear.\n\nWe have improved the motivation and description of UGI and the refine gate in the revision. Further feedback on how they can be further clarified is appreciated.\n\n>>> The authors compares UGI to standard initialization but where is the standard initialization?\n\nStandard initialization was stated to be initializing the bias term to a constant; for example linear models usually initialize the bias to 0 (Section 2.2). In our experiments, the vanilla LSTM used 1.0 bias initialization, which has become the standard for LSTMs (Gers et al.). This detail has been clarified in Section 2.2 and 3.\n\n>>> I am not convinced how the UGI gate help avoid decaying of the input\n\nPlease see our shared response to all reviewers.\n\n\n>>> On propositions\n\nThe central principle of gated RNNs is that the values of gate activations control the timescales that the model can address. Propositions 2 and 3 formalize how our gate modifications affect the timescales of the recurrent model. However, we agree that Propositions 2 and 3 are somewhat tangential to the main points of the paper and have moved them to Appendix B.2.\n\n\n>>> Even though the title of the paper is \"improving the gating mechanism of recurrent neural networks\", the authors try to solve signal propagation problems. It is unclear why \"gate\" is important.\n\nThe above principle means that the performance of gated RNNs (which are the dominant form of RNN, for reasons outlined in Section 1 and 2) is closely tied to the activations and learnability of their gates. Thus our improvements to the gates improve recurrent models at large. We hope that our revised introduction and background sections have clarified this motivation.\n\n\n>>> Minors\n\nThanks for pointing out several mistakes which have now been fixed. In particular, we have used the suggestion to use the Hadamard multiplication symbol to emphasize the elementwise multiplication, for example in Equation (1).\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2516/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2516/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["gua@google.com", "caglarg@google.com", "mwhoffman@google.com", "razp@google.com"], "title": "Improving the Gating Mechanism of Recurrent Neural Networks", "authors": ["Albert Gua", "Caglar Gulcehre", "Tom le Paine", "Razvan Pascanu", "Matt Hoffman"], "pdf": "/pdf/ee6aa8f1d0d1d8ec5841207e08caae0971cc7fa6.pdf", "TL;DR": "Improving the gating mechanisms of recurrent neural networks by addressing the initialization of the biases and the saturation problem of sigmoid.", "abstract": "In this work, we revisit the gating mechanisms widely used in various recurrent and feedforward networks such as LSTMs, GRUs, or highway networks. These gates are meant to control information flow, allowing gradients to better propagate back in time for recurrent models. However, to propagate gradients over very long temporal windows, they need to operate close to their saturation regime. We propose two independent and synergistic modifications to the standard gating mechanism that are easy to implement, introduce no additional hyper-parameters, and are aimed at improving learnability of the gates when they are close to saturation. Our proposals are theoretically justified, and we show a generic framework that encompasses other recently proposed gating mechanisms such as chrono-initialization and master gates . We perform systematic analyses and ablation studies on the proposed improvements and evaluate our method on a wide range of applications including synthetic memorization tasks, sequential image classification, language modeling, and reinforcement learning.  Empirically, our proposed gating mechanisms robustly increase the performance of recurrent models such as LSTMs, especially on tasks requiring long temporal dependencies.", "keywords": ["recurrent neural networks", "LSTM", "GRUs", "gating mechanisms", "deep learning", "reinforcement learning"], "paperhash": "gua|improving_the_gating_mechanism_of_recurrent_neural_networks", "original_pdf": "/attachment/f9ddc5df9b4c1f1864a74ab1b88ebd1a6c6de21e.pdf", "_bibtex": "@misc{\ngua2020improving,\ntitle={Improving the Gating Mechanism of Recurrent Neural Networks},\nauthor={Albert Gua and Caglar Gulcehre and Tom le Paine and Razvan Pascanu and Matt Hoffman},\nyear={2020},\nurl={https://openreview.net/forum?id=r1lnigSFDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1lnigSFDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2516/Authors", "ICLR.cc/2020/Conference/Paper2516/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2516/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2516/Reviewers", "ICLR.cc/2020/Conference/Paper2516/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2516/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2516/Authors|ICLR.cc/2020/Conference/Paper2516/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504140198, "tmdate": 1576860552030, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2516/Authors", "ICLR.cc/2020/Conference/Paper2516/Reviewers", "ICLR.cc/2020/Conference/Paper2516/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2516/-/Official_Comment"}}}, {"id": "BJxBlUSfsr", "original": null, "number": 6, "cdate": 1573176812644, "ddate": null, "tcdate": 1573176812644, "tmdate": 1573176812644, "tddate": null, "forum": "r1lnigSFDr", "replyto": "S1llO2HCKH", "invitation": "ICLR.cc/2020/Conference/Paper2516/-/Official_Comment", "content": {"title": "Thank you for your feedback", "comment": "We appreciate the reviewer\u2019s detailed reading of the paper and thoughtful comments, suggestions, and questions. We have responded to these below.\n\n\n>>> This manuscript already quite long and has several formatting issues... \n\nWe agree with the suggestions and have improved the labels and captions of many figures. We have updated Figure 3 on a log scale which helps distinguish the curves. Figure 5 conveys information that a table of numbers may not be able to; for example, it shows that the UR-LSTM converges both faster and to a higher maximum than other methods, and accounts for confidence intervals. We will continue revising Figure 2 for readability.\n\n\n>>> I think that for this approach to work, two conditions need to be satisfied (a) there must be foreseeable improvements in the use of a forget gate that can reach values close to 0/1 for the task at hand and (b) r_t needs to function well despite not being too close to 0 or 1 (lest its parameters suffer from gradient flow issues)\n  * Was there any visualizations done on whether (a) happened? i.e. for the URLSTMs that performed well, were the values of the forget gate closer to 0/1 than the baselines?\n\nWe agree with these insights. We believe that Figure 4 and Figure 9 show clear empirical support for these principles, in particular that\n  *  How close the forget gate activations are to 1.0 affect whether the models are able to solve this memory task\n  *  The addition of a refine gate improves the ability of the model to learn more extremal activations, enabling it to solve the task\n\n\n>>> What were typical values of r_t, did the models need the refine gate to reach values close to 0 or 1 for the overall approach to work?\n\nWe note that as long as one of the gates is not saturated (either the original gate or the refine gate), the model can still learn.\nWe did not find any scenarios empirically in which the refine gate saturated and prevented the overall approach from working.\n\n\n>>> While I'm not entirely convinced about the proposed initialization scheme...\n\nWe believe that in addition to the refine gate, UGI is also quite motivated and shows clear empirical benefits, which we summarized in the shared response to all reviewers.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2516/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2516/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["gua@google.com", "caglarg@google.com", "mwhoffman@google.com", "razp@google.com"], "title": "Improving the Gating Mechanism of Recurrent Neural Networks", "authors": ["Albert Gua", "Caglar Gulcehre", "Tom le Paine", "Razvan Pascanu", "Matt Hoffman"], "pdf": "/pdf/ee6aa8f1d0d1d8ec5841207e08caae0971cc7fa6.pdf", "TL;DR": "Improving the gating mechanisms of recurrent neural networks by addressing the initialization of the biases and the saturation problem of sigmoid.", "abstract": "In this work, we revisit the gating mechanisms widely used in various recurrent and feedforward networks such as LSTMs, GRUs, or highway networks. These gates are meant to control information flow, allowing gradients to better propagate back in time for recurrent models. However, to propagate gradients over very long temporal windows, they need to operate close to their saturation regime. We propose two independent and synergistic modifications to the standard gating mechanism that are easy to implement, introduce no additional hyper-parameters, and are aimed at improving learnability of the gates when they are close to saturation. Our proposals are theoretically justified, and we show a generic framework that encompasses other recently proposed gating mechanisms such as chrono-initialization and master gates . We perform systematic analyses and ablation studies on the proposed improvements and evaluate our method on a wide range of applications including synthetic memorization tasks, sequential image classification, language modeling, and reinforcement learning.  Empirically, our proposed gating mechanisms robustly increase the performance of recurrent models such as LSTMs, especially on tasks requiring long temporal dependencies.", "keywords": ["recurrent neural networks", "LSTM", "GRUs", "gating mechanisms", "deep learning", "reinforcement learning"], "paperhash": "gua|improving_the_gating_mechanism_of_recurrent_neural_networks", "original_pdf": "/attachment/f9ddc5df9b4c1f1864a74ab1b88ebd1a6c6de21e.pdf", "_bibtex": "@misc{\ngua2020improving,\ntitle={Improving the Gating Mechanism of Recurrent Neural Networks},\nauthor={Albert Gua and Caglar Gulcehre and Tom le Paine and Razvan Pascanu and Matt Hoffman},\nyear={2020},\nurl={https://openreview.net/forum?id=r1lnigSFDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1lnigSFDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2516/Authors", "ICLR.cc/2020/Conference/Paper2516/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2516/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2516/Reviewers", "ICLR.cc/2020/Conference/Paper2516/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2516/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2516/Authors|ICLR.cc/2020/Conference/Paper2516/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504140198, "tmdate": 1576860552030, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2516/Authors", "ICLR.cc/2020/Conference/Paper2516/Reviewers", "ICLR.cc/2020/Conference/Paper2516/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2516/-/Official_Comment"}}}, {"id": "rye_5HHzsr", "original": null, "number": 5, "cdate": 1573176720355, "ddate": null, "tcdate": 1573176720355, "tmdate": 1573176720355, "tddate": null, "forum": "r1lnigSFDr", "replyto": "r1lnigSFDr", "invitation": "ICLR.cc/2020/Conference/Paper2516/-/Official_Comment", "content": {"title": "Updated Paper and Response to Reviews", "comment": "We thank all the reviewers for their comments and feedback. We have uploaded a revised draft of the paper which we believe substantially improves the clarity of the submission and addresses the concerns that the reviewers raised. We highlight the most important changes below, as well as address shared feedback among the reviewers.\n\n\n*Presentation and Formatting*\n\nWe apologize for the presentation issues and have fixed them to improve readability, including\n    -  Increased line spacing and header separation and larger table fonts\n    -  Larger and more readable figure labels\n    -  Improved notation for gates\n\n\n*Exposition*\n\n    -  We have expanded on the motivation and description of our methods in Sections 1 and 2, and added a subsection (2.1) that summarizes the overall model with explicit equations for the UR-LSTM.\n    -  We have added a brief discussion about the overall effectiveness of our methods (Section 3.5). In summary, we believe that the robust empirical improvement of our simple modification over the time-tested LSTM is an important contribution in itself.\n\n\n*Uniform gate initialization*\n\nWe have clarified the motivation and description of UGI, which can be seen as a hyperparameter-free heuristic for improving initialization of the gates by letting neurons forget at different rates.\n\nEmpirically, we point out that UGI shows improvements on many experiments\n    -  On the Copy task, the refine gate alone (R-LSTM) is stuck at baseline. Simply changing the initialization to UGI (UR-LSTM) solves the task very quickly\n    -  On sequential image classification, we have added the R-LSTM ablation, which performs worse than the U-LSTM ablation (Figure 5). The isolated initialization change (e.g. from LSTM to U-LSTM, or R-LSTM to UR-LSTM) provides substantial improvements on these tasks\n    -  On language modeling, the U-LSTM alone improves over the SOTA LSTM baseline and matched the more specialized ON-LSTM baseline (Figure 6)\n\nTheoretically, we reiterate here why UGI intuitively helps\n    -  The central principle of gated RNNs is that the values of gate activations control the timescales that the model can address. Section 2.2 defines the \u201ccharacteristic timescale\u201d, which implies that forget gate activations near 1.0 are necessary for long-term memory. Similar observations have been made before (Tallec et al. 2018).\n    -  This phenomenon is also empirically supported in Figure 4, which shows that the methods which are able to solve the difficult memory task have more forget gate values near 1\n    -  By initializing the activations uniformly throughout [0,1], UGI removes the bias hyperparameter and addresses a wider range of timescales including long-term dependencies. This explains the empirical improvements previously noted\n    -  A more thorough discussion of the theoretical properties including how it affects the timescale distribution has been moved to Appendix B.2 and B.3\n\n\n*Contributions*\n    -  Even disregarding UGI, we emphasize that the refine gate is a novel and theoretically justified contribution that can improve any gated model\n    -  We have added pseudocode snippets of the proposed methods in Appendix A, which consist of only modifying 2 lines of code each. We again highlight that these small changes translate to broad empirical improvements\n    -  Overall, due to the simplicity and principled nature of these modifications, and the ubiquity of \u201cgates\u201d in machine learning models, we believe that our contributions can be a valuable tool for practitioners.\n\n\nWe have responded to other comments individually. We encourage the reviewers to look at the improved draft, and look forward to hearing further feedback on the submission.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2516/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2516/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["gua@google.com", "caglarg@google.com", "mwhoffman@google.com", "razp@google.com"], "title": "Improving the Gating Mechanism of Recurrent Neural Networks", "authors": ["Albert Gua", "Caglar Gulcehre", "Tom le Paine", "Razvan Pascanu", "Matt Hoffman"], "pdf": "/pdf/ee6aa8f1d0d1d8ec5841207e08caae0971cc7fa6.pdf", "TL;DR": "Improving the gating mechanisms of recurrent neural networks by addressing the initialization of the biases and the saturation problem of sigmoid.", "abstract": "In this work, we revisit the gating mechanisms widely used in various recurrent and feedforward networks such as LSTMs, GRUs, or highway networks. These gates are meant to control information flow, allowing gradients to better propagate back in time for recurrent models. However, to propagate gradients over very long temporal windows, they need to operate close to their saturation regime. We propose two independent and synergistic modifications to the standard gating mechanism that are easy to implement, introduce no additional hyper-parameters, and are aimed at improving learnability of the gates when they are close to saturation. Our proposals are theoretically justified, and we show a generic framework that encompasses other recently proposed gating mechanisms such as chrono-initialization and master gates . We perform systematic analyses and ablation studies on the proposed improvements and evaluate our method on a wide range of applications including synthetic memorization tasks, sequential image classification, language modeling, and reinforcement learning.  Empirically, our proposed gating mechanisms robustly increase the performance of recurrent models such as LSTMs, especially on tasks requiring long temporal dependencies.", "keywords": ["recurrent neural networks", "LSTM", "GRUs", "gating mechanisms", "deep learning", "reinforcement learning"], "paperhash": "gua|improving_the_gating_mechanism_of_recurrent_neural_networks", "original_pdf": "/attachment/f9ddc5df9b4c1f1864a74ab1b88ebd1a6c6de21e.pdf", "_bibtex": "@misc{\ngua2020improving,\ntitle={Improving the Gating Mechanism of Recurrent Neural Networks},\nauthor={Albert Gua and Caglar Gulcehre and Tom le Paine and Razvan Pascanu and Matt Hoffman},\nyear={2020},\nurl={https://openreview.net/forum?id=r1lnigSFDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1lnigSFDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2516/Authors", "ICLR.cc/2020/Conference/Paper2516/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2516/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2516/Reviewers", "ICLR.cc/2020/Conference/Paper2516/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2516/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2516/Authors|ICLR.cc/2020/Conference/Paper2516/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504140198, "tmdate": 1576860552030, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2516/Authors", "ICLR.cc/2020/Conference/Paper2516/Reviewers", "ICLR.cc/2020/Conference/Paper2516/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2516/-/Official_Comment"}}}, {"id": "S1llO2HCKH", "original": null, "number": 2, "cdate": 1571867752309, "ddate": null, "tcdate": 1571867752309, "tmdate": 1572972315820, "tddate": null, "forum": "r1lnigSFDr", "replyto": "r1lnigSFDr", "invitation": "ICLR.cc/2020/Conference/Paper2516/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "\nThis paper introduces and studies modifications to gating mechanisms in RNNs.\nThe first modification is uniform gate initialization. Here the biases in the forget and input gate bias are sampled such that after the application of the sigmoid the values are in the range (1/d, 1-1/d) where d is the dimensionality of the hidden space for the bias. The second modification is the introduction of a refine gating mechanism with a view to allow for gradients to flow when the forget gates f_t in the LSTM updates are near saturation. The idea is to create an effective gate g = f_t +/- phi(f_t, x_t). The paper proposes using phi (f_t, x_t) = f_t(1-f_t) * (2*r_t-1) where (r_t is between 0 or 1). The effect of such a change is that g_t can reach values of 0.99 when the value of f_t is 0.9 allowing gradients to flow more freely through the parameters that constitute the forget gate. Overall the change corresponds to improving gradient flow for the forget gate by interpolating between f_t^2 and (1-f_t)^2. i.e. the authors note that the result of these changes is that it corresponds to sampling biases from a heavier tailed distribution while the refine gate (by allowing the forget gate to reach values close to 0 and 1), allows for capturing information on a much longer time scale.\n\n\nThe paper studies various combinations of the two changes proposed to gating architectures. Other baselines include a vanilla LSTM, a Chrono initialized LSTM, and an ordered Neuron LSTM. The models are trained on several synthetic and real world tasks. On the copy and add tasks, the LSTMs that contain the refine gate converge the fastest. A similar story is observed on the task of pixel by pixel image classification. The refine gate was also adapted to memory architectures such as the DNC and RMA where it was found to improve performance on two different tasks.\n\nOverall, the paper is written well, I like the (second) idea of the refine gate and the contributions are explained in an accessible manner. While I'm not entirely convinced about the proposed initialization scheme but across the many different tasks tried, the use of the refine gate does appear to give performance improvements that lead me to conclude that this aspect of the work is a solid contribution to the literature.\n\nQuestions and comments:\n* This manuscript already quite long and has several formatting issues. Several of the figures are unreadable when printed. For example, every piece of text on Figure 2(d) is unreadable on paper. Figure 3 and 5 are difficult to read; they contain too many alternatives with a colour scheme that makes it difficult to distinguish between them -- consider displaying a subset of the options via a plot and using a table to display (# steps to convergence) as a metric instead. It also appears as if the caption for Table 6 is deleted?\n* I think that for this approach to work, two conditions need to be satisfied (a) there must be foreseeable improvements in the use of a forget gate that can reach values close to 0/1 for the task at hand and (b) r_t needs to function well despite not being too close to 0 or 1 (lest its parameters suffer from gradient flow issues).\n  * Was there any visualizations done on whether (a) happened? i.e. for the URLSTMs that performed well, were the values of the forget gate closer to 0/1 than the baselines?\n  * What were typical values of r_t, did the models need the refine gate to reach values close to 0 or 1 for the overall approach to work?"}, "signatures": ["ICLR.cc/2020/Conference/Paper2516/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2516/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["gua@google.com", "caglarg@google.com", "mwhoffman@google.com", "razp@google.com"], "title": "Improving the Gating Mechanism of Recurrent Neural Networks", "authors": ["Albert Gua", "Caglar Gulcehre", "Tom le Paine", "Razvan Pascanu", "Matt Hoffman"], "pdf": "/pdf/ee6aa8f1d0d1d8ec5841207e08caae0971cc7fa6.pdf", "TL;DR": "Improving the gating mechanisms of recurrent neural networks by addressing the initialization of the biases and the saturation problem of sigmoid.", "abstract": "In this work, we revisit the gating mechanisms widely used in various recurrent and feedforward networks such as LSTMs, GRUs, or highway networks. These gates are meant to control information flow, allowing gradients to better propagate back in time for recurrent models. However, to propagate gradients over very long temporal windows, they need to operate close to their saturation regime. We propose two independent and synergistic modifications to the standard gating mechanism that are easy to implement, introduce no additional hyper-parameters, and are aimed at improving learnability of the gates when they are close to saturation. Our proposals are theoretically justified, and we show a generic framework that encompasses other recently proposed gating mechanisms such as chrono-initialization and master gates . We perform systematic analyses and ablation studies on the proposed improvements and evaluate our method on a wide range of applications including synthetic memorization tasks, sequential image classification, language modeling, and reinforcement learning.  Empirically, our proposed gating mechanisms robustly increase the performance of recurrent models such as LSTMs, especially on tasks requiring long temporal dependencies.", "keywords": ["recurrent neural networks", "LSTM", "GRUs", "gating mechanisms", "deep learning", "reinforcement learning"], "paperhash": "gua|improving_the_gating_mechanism_of_recurrent_neural_networks", "original_pdf": "/attachment/f9ddc5df9b4c1f1864a74ab1b88ebd1a6c6de21e.pdf", "_bibtex": "@misc{\ngua2020improving,\ntitle={Improving the Gating Mechanism of Recurrent Neural Networks},\nauthor={Albert Gua and Caglar Gulcehre and Tom le Paine and Razvan Pascanu and Matt Hoffman},\nyear={2020},\nurl={https://openreview.net/forum?id=r1lnigSFDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "r1lnigSFDr", "replyto": "r1lnigSFDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2516/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2516/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575564020900, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2516/Reviewers"], "noninvitees": [], "tcdate": 1570237721750, "tmdate": 1575564020916, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2516/-/Official_Review"}}}, {"id": "rJxC9rZ2FH", "original": null, "number": 1, "cdate": 1571718550478, "ddate": null, "tcdate": 1571718550478, "tmdate": 1572972315751, "tddate": null, "forum": "r1lnigSFDr", "replyto": "r1lnigSFDr", "invitation": "ICLR.cc/2020/Conference/Paper2516/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper introduces two novel techniques to help long term signal propagation in RNNs. One is an initialization strategy which uses inverse sigmoid function to avoid the decay of the contribution of the input earlier in time and another is a new design of a refine gate which pushes the value of the gate closer to 0 or 1.  The authors conduct exhaustive ablation and empirical studies on copy task, sequential MNIST, language modeling and reinforcement learning. \n\nThough the experiment section is solid, I still vote for rejection for the following reasons:\n\n1. The writing in the description of the UGI and refine fate is not clear.\na. The authors compares UGI to standard initialization but where is the standard initialization? I do not see \"standard initialization\" clearly defined in the paper.\nb. I am not convinced how the UGI gate help avoid decaying of the input. There is a proposition 2 trying to explain some part of the mechanism of UGI. But the proposition is never proved anywhere and I am not sure why this proposition is important. More explanations are needed. Also this proposition is far away from the place the authors introduce the UGI. The authors may want to refer it in the place introducing UGI.\nc. Similar to proposition 2, proposition 3 is not explained and proved in the paper. It is hard for me to analyze the importance of these two propositions. Overall, propositions 2 and 3 look isolated in the section.\nd. Proposition 1 looks like a definition. Not sure why the authors name it as a proposition.\n\n2. Even though the title of the paper is \"improving the gating mechanism of recurrent neural networks\", the authors try to solve signal propagation problems. It is unclear why \"gate\" is important. Maybe other designs of the recurrent neural network can satisfy better the desiderata the authors want. Based on my limited knowledge, the initialization the authors mention (saturation) is exactly from the need of using a sigmoid gate. The importance of using \"gate\" should be discussed.\n\n3. The authors shrink the space before and after headings in the paper. I think this is not allowed in ICLR. It would be better that the authors correct the spacing in the revised version.\n\n\nMinors:\n1. page 1 second paragraph: repeated \u201cproven\u201d\n2. page 1 second last paragraph: \u201cdue\u201d -> \u201cdue to\u201d \n3. page 2 second last paragraph: repeated \u201cbe\u201d\n4. page 2 Equation (4) and (5): using some symbols like \\odot for element wise multiplication will be good for the readers.\n\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2516/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2516/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["gua@google.com", "caglarg@google.com", "mwhoffman@google.com", "razp@google.com"], "title": "Improving the Gating Mechanism of Recurrent Neural Networks", "authors": ["Albert Gua", "Caglar Gulcehre", "Tom le Paine", "Razvan Pascanu", "Matt Hoffman"], "pdf": "/pdf/ee6aa8f1d0d1d8ec5841207e08caae0971cc7fa6.pdf", "TL;DR": "Improving the gating mechanisms of recurrent neural networks by addressing the initialization of the biases and the saturation problem of sigmoid.", "abstract": "In this work, we revisit the gating mechanisms widely used in various recurrent and feedforward networks such as LSTMs, GRUs, or highway networks. These gates are meant to control information flow, allowing gradients to better propagate back in time for recurrent models. However, to propagate gradients over very long temporal windows, they need to operate close to their saturation regime. We propose two independent and synergistic modifications to the standard gating mechanism that are easy to implement, introduce no additional hyper-parameters, and are aimed at improving learnability of the gates when they are close to saturation. Our proposals are theoretically justified, and we show a generic framework that encompasses other recently proposed gating mechanisms such as chrono-initialization and master gates . We perform systematic analyses and ablation studies on the proposed improvements and evaluate our method on a wide range of applications including synthetic memorization tasks, sequential image classification, language modeling, and reinforcement learning.  Empirically, our proposed gating mechanisms robustly increase the performance of recurrent models such as LSTMs, especially on tasks requiring long temporal dependencies.", "keywords": ["recurrent neural networks", "LSTM", "GRUs", "gating mechanisms", "deep learning", "reinforcement learning"], "paperhash": "gua|improving_the_gating_mechanism_of_recurrent_neural_networks", "original_pdf": "/attachment/f9ddc5df9b4c1f1864a74ab1b88ebd1a6c6de21e.pdf", "_bibtex": "@misc{\ngua2020improving,\ntitle={Improving the Gating Mechanism of Recurrent Neural Networks},\nauthor={Albert Gua and Caglar Gulcehre and Tom le Paine and Razvan Pascanu and Matt Hoffman},\nyear={2020},\nurl={https://openreview.net/forum?id=r1lnigSFDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "r1lnigSFDr", "replyto": "r1lnigSFDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2516/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2516/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575564020900, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2516/Reviewers"], "noninvitees": [], "tcdate": 1570237721750, "tmdate": 1575564020916, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2516/-/Official_Review"}}}, {"id": "rkl3__O0tr", "original": null, "number": 3, "cdate": 1571879027535, "ddate": null, "tcdate": 1571879027535, "tmdate": 1572972315662, "tddate": null, "forum": "r1lnigSFDr", "replyto": "r1lnigSFDr", "invitation": "ICLR.cc/2020/Conference/Paper2516/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes to improve the learnability of the gating mechanism in RNN by two modifications on the standard RNN structure, uniform gate initialization and refine gate. The authors give some propositions to show that the refine gate can maintain an effective forget effect within a larger range of timescale. The authors conduct experiments on four different tasks and compare the proposed modification with baseline methods.\n\nStrong points:\n1. The authors propose a new refine structure that seems to have a longer \"memory\".\n2. The authors designed a good synthetic experiment to demonstrate whether the proposed refine structure can help to remember information in longer sequence.\n\nWeak points:\n1. There are several parts in the experiment that are not very convincing.\n     a. In Figure 3(a), where are the other baselines? Are they performing too badly so that they can not show up in the figure? It needs more explanation.\n     b.  In Figure 3(b), actually a lot of methods are performing similar, while some methods converge similarly. What is the reason?\n2. It is not defined why the uniform gate initialization works.\n3. The proposed results actually not always perform the best. For instance, in Table 3, purely using the UR-LSTM only achieve good results on sMNIST. What is the reason? The proposed method seems not very general.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2516/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2516/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["gua@google.com", "caglarg@google.com", "mwhoffman@google.com", "razp@google.com"], "title": "Improving the Gating Mechanism of Recurrent Neural Networks", "authors": ["Albert Gua", "Caglar Gulcehre", "Tom le Paine", "Razvan Pascanu", "Matt Hoffman"], "pdf": "/pdf/ee6aa8f1d0d1d8ec5841207e08caae0971cc7fa6.pdf", "TL;DR": "Improving the gating mechanisms of recurrent neural networks by addressing the initialization of the biases and the saturation problem of sigmoid.", "abstract": "In this work, we revisit the gating mechanisms widely used in various recurrent and feedforward networks such as LSTMs, GRUs, or highway networks. These gates are meant to control information flow, allowing gradients to better propagate back in time for recurrent models. However, to propagate gradients over very long temporal windows, they need to operate close to their saturation regime. We propose two independent and synergistic modifications to the standard gating mechanism that are easy to implement, introduce no additional hyper-parameters, and are aimed at improving learnability of the gates when they are close to saturation. Our proposals are theoretically justified, and we show a generic framework that encompasses other recently proposed gating mechanisms such as chrono-initialization and master gates . We perform systematic analyses and ablation studies on the proposed improvements and evaluate our method on a wide range of applications including synthetic memorization tasks, sequential image classification, language modeling, and reinforcement learning.  Empirically, our proposed gating mechanisms robustly increase the performance of recurrent models such as LSTMs, especially on tasks requiring long temporal dependencies.", "keywords": ["recurrent neural networks", "LSTM", "GRUs", "gating mechanisms", "deep learning", "reinforcement learning"], "paperhash": "gua|improving_the_gating_mechanism_of_recurrent_neural_networks", "original_pdf": "/attachment/f9ddc5df9b4c1f1864a74ab1b88ebd1a6c6de21e.pdf", "_bibtex": "@misc{\ngua2020improving,\ntitle={Improving the Gating Mechanism of Recurrent Neural Networks},\nauthor={Albert Gua and Caglar Gulcehre and Tom le Paine and Razvan Pascanu and Matt Hoffman},\nyear={2020},\nurl={https://openreview.net/forum?id=r1lnigSFDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "r1lnigSFDr", "replyto": "r1lnigSFDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2516/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2516/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575564020900, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2516/Reviewers"], "noninvitees": [], "tcdate": 1570237721750, "tmdate": 1575564020916, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2516/-/Official_Review"}}}, {"id": "HkgP_s1RYr", "original": null, "number": 1, "cdate": 1571842926849, "ddate": null, "tcdate": 1571842926849, "tmdate": 1571842926849, "tddate": null, "forum": "r1lnigSFDr", "replyto": "r1lnigSFDr", "invitation": "ICLR.cc/2020/Conference/Paper2516/-/Public_Comment", "content": {"comment": "This paper was previously desk-rejected by mistake. It has been placed back in the submission pool.\n\nBest,\n\nOpenReview Team.", "title": "Paper modification date updated"}, "signatures": ["~Super_User1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Super_User1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["gua@google.com", "caglarg@google.com", "mwhoffman@google.com", "razp@google.com"], "title": "Improving the Gating Mechanism of Recurrent Neural Networks", "authors": ["Albert Gua", "Caglar Gulcehre", "Tom le Paine", "Razvan Pascanu", "Matt Hoffman"], "pdf": "/pdf/ee6aa8f1d0d1d8ec5841207e08caae0971cc7fa6.pdf", "TL;DR": "Improving the gating mechanisms of recurrent neural networks by addressing the initialization of the biases and the saturation problem of sigmoid.", "abstract": "In this work, we revisit the gating mechanisms widely used in various recurrent and feedforward networks such as LSTMs, GRUs, or highway networks. These gates are meant to control information flow, allowing gradients to better propagate back in time for recurrent models. However, to propagate gradients over very long temporal windows, they need to operate close to their saturation regime. We propose two independent and synergistic modifications to the standard gating mechanism that are easy to implement, introduce no additional hyper-parameters, and are aimed at improving learnability of the gates when they are close to saturation. Our proposals are theoretically justified, and we show a generic framework that encompasses other recently proposed gating mechanisms such as chrono-initialization and master gates . We perform systematic analyses and ablation studies on the proposed improvements and evaluate our method on a wide range of applications including synthetic memorization tasks, sequential image classification, language modeling, and reinforcement learning.  Empirically, our proposed gating mechanisms robustly increase the performance of recurrent models such as LSTMs, especially on tasks requiring long temporal dependencies.", "keywords": ["recurrent neural networks", "LSTM", "GRUs", "gating mechanisms", "deep learning", "reinforcement learning"], "paperhash": "gua|improving_the_gating_mechanism_of_recurrent_neural_networks", "original_pdf": "/attachment/f9ddc5df9b4c1f1864a74ab1b88ebd1a6c6de21e.pdf", "_bibtex": "@misc{\ngua2020improving,\ntitle={Improving the Gating Mechanism of Recurrent Neural Networks},\nauthor={Albert Gua and Caglar Gulcehre and Tom le Paine and Razvan Pascanu and Matt Hoffman},\nyear={2020},\nurl={https://openreview.net/forum?id=r1lnigSFDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1lnigSFDr", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504179172, "tmdate": 1576860585224, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper2516/Authors", "ICLR.cc/2020/Conference/Paper2516/Reviewers", "ICLR.cc/2020/Conference/Paper2516/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2516/-/Public_Comment"}}}], "count": 10}