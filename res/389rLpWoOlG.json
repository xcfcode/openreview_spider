{"notes": [{"id": "389rLpWoOlG", "original": "9Ov03wA29yO", "number": 3219, "cdate": 1601308357631, "ddate": null, "tcdate": 1601308357631, "tmdate": 1614985646812, "tddate": null, "forum": "389rLpWoOlG", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Machine Learning Algorithms for Data Labeling: An Empirical Evaluation", "authorids": ["~Teodor_Anders_Fredriksson1", "davidis@chalmers.se", "jan.bosch@chalmers.se", "helena.holmstrom.olsson@mau.se"], "authors": ["Teodor Anders Fredriksson", "David Issa Mattos", "Jan Bosch", "Helena Holmstr\u00f6m Olsson"], "keywords": ["Data Labeling", "Empirical Evaluation", "Active Machine Learning", "Semi-Supervised Learning"], "abstract": "The lack of labeled data is a major problem in both research and industrial settings since obtaining labels is often an expensive and time-consuming activity. In the past years, several machine learning algorithms were developed to assist and perform automated labeling in partially labeled datasets. While many of these algorithms are available in open-source packages, there is no research that investigates how these algorithms compare to each other in different types of datasets and with different percentages of available labels. To address this problem, this paper empirically evaluates and compares seven algorithms for automated labeling in terms of accuracy. We investigate how these algorithms perform in six different and well-known datasets with three different types of data, images, texts, and numerical values. We evaluate these algorithms under two different experimental conditions, with 10\\% and 50\\% labels of available labels in the dataset. Each algorithm, in each dataset for each experimental condition, is evaluated independently ten times with different random seeds. The results are analyzed and the algorithms are compared utilizing a Bayesian Bradley-Terry model. The results indicate that while the algorithms label spreading with K-nearest neighbors perform better in the aggregated results, the active learning algorithms query by instance QBC and query instance uncertainty sample perform better when there is only 10\\% of labels available. These results can help machine learning practitioners in choosing optimal machine learning algorithms to label their data.", "one-sentence_summary": "This paper provides an empirical evaluation of automatic labeling methods based on machine learning.", "pdf": "/pdf/d266ae12758df4ddd0f30d71e6236ea9cd097cc8.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "fredriksson|machine_learning_algorithms_for_data_labeling_an_empirical_evaluation", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=_Al4gzKd_i", "_bibtex": "@misc{\nfredriksson2021machine,\ntitle={Machine Learning Algorithms for Data Labeling: An Empirical Evaluation},\nauthor={Teodor Anders Fredriksson and David Issa Mattos and Jan Bosch and Helena Holmstr{\\\"o}m Olsson},\nyear={2021},\nurl={https://openreview.net/forum?id=389rLpWoOlG}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "8A9GQOZhX_C", "original": null, "number": 1, "cdate": 1610040515505, "ddate": null, "tcdate": 1610040515505, "tmdate": 1610474123711, "tddate": null, "forum": "389rLpWoOlG", "replyto": "389rLpWoOlG", "invitation": "ICLR.cc/2021/Conference/Paper3219/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "The paper evaluates several different strategies for labeling of missing data, and recommend the best strategy in practice based on the empirical results on six data sets.\n\nThe reviewers agree that empirical evaluation is important for providing a good guideline for this practical problem. The concerns of the reviewers include the lack of motivation on the chosen strategies, the lack of novelty (the tools in the strategies are all pretty standard in the literature), the lack of reproducibility (by referring to the authors' own anonymous work for parameters), and the lack of breadth (e.g #data sets) and depth (e.g. metrics explored) in the experiments.\n"}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Machine Learning Algorithms for Data Labeling: An Empirical Evaluation", "authorids": ["~Teodor_Anders_Fredriksson1", "davidis@chalmers.se", "jan.bosch@chalmers.se", "helena.holmstrom.olsson@mau.se"], "authors": ["Teodor Anders Fredriksson", "David Issa Mattos", "Jan Bosch", "Helena Holmstr\u00f6m Olsson"], "keywords": ["Data Labeling", "Empirical Evaluation", "Active Machine Learning", "Semi-Supervised Learning"], "abstract": "The lack of labeled data is a major problem in both research and industrial settings since obtaining labels is often an expensive and time-consuming activity. In the past years, several machine learning algorithms were developed to assist and perform automated labeling in partially labeled datasets. While many of these algorithms are available in open-source packages, there is no research that investigates how these algorithms compare to each other in different types of datasets and with different percentages of available labels. To address this problem, this paper empirically evaluates and compares seven algorithms for automated labeling in terms of accuracy. We investigate how these algorithms perform in six different and well-known datasets with three different types of data, images, texts, and numerical values. We evaluate these algorithms under two different experimental conditions, with 10\\% and 50\\% labels of available labels in the dataset. Each algorithm, in each dataset for each experimental condition, is evaluated independently ten times with different random seeds. The results are analyzed and the algorithms are compared utilizing a Bayesian Bradley-Terry model. The results indicate that while the algorithms label spreading with K-nearest neighbors perform better in the aggregated results, the active learning algorithms query by instance QBC and query instance uncertainty sample perform better when there is only 10\\% of labels available. These results can help machine learning practitioners in choosing optimal machine learning algorithms to label their data.", "one-sentence_summary": "This paper provides an empirical evaluation of automatic labeling methods based on machine learning.", "pdf": "/pdf/d266ae12758df4ddd0f30d71e6236ea9cd097cc8.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "fredriksson|machine_learning_algorithms_for_data_labeling_an_empirical_evaluation", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=_Al4gzKd_i", "_bibtex": "@misc{\nfredriksson2021machine,\ntitle={Machine Learning Algorithms for Data Labeling: An Empirical Evaluation},\nauthor={Teodor Anders Fredriksson and David Issa Mattos and Jan Bosch and Helena Holmstr{\\\"o}m Olsson},\nyear={2021},\nurl={https://openreview.net/forum?id=389rLpWoOlG}\n}"}, "tags": [], "invitation": {"reply": {"forum": "389rLpWoOlG", "replyto": "389rLpWoOlG", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040515492, "tmdate": 1610474123693, "id": "ICLR.cc/2021/Conference/Paper3219/-/Decision"}}}, {"id": "xUHwg1oV2Jy", "original": null, "number": 1, "cdate": 1603386954861, "ddate": null, "tcdate": 1603386954861, "tmdate": 1605024044386, "tddate": null, "forum": "389rLpWoOlG", "replyto": "389rLpWoOlG", "invitation": "ICLR.cc/2021/Conference/Paper3219/-/Official_Review", "content": {"title": " The paper compares several approaches for labelling data, spanning active learning and semi-supervised strategies. The problem and choice of algorithms are not sufficiently motivated. In particular, it is unclear why the authors compare semi-supervised techniques with active learning techniques. The latter requires an oracle while the former doesn't. There is a range of further largely unjustified ad-hoc choices. This work is not sufficiently significant to warrant publication. ", "review": "\n\n## Detailed Comments\n- \"There are problems with supervised learning and machine learning in general.\" - This statement is so general, it is essentially vacuous.\n- \"machine learning requires huge amounts of data\" - unclear what this means. and whatever it means it's not true in general.\n- \"severe labeling issues were found\" - what issues?\n- \"we provide the an overview\"\n- \"(AL),\" - missing space. this happens at several places in the text\n- \"continue iterative\" - iteratively\n- \"other stopping criteria\" -> \"criterion\". also, why \"other\"? there was no stopping criterion mentioned so far. \n- \"If a learner does not choose his strategy\" -> their strategy\n- eq at bottom of page 2: what is \"u\"?\n\n", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3219/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3219/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Machine Learning Algorithms for Data Labeling: An Empirical Evaluation", "authorids": ["~Teodor_Anders_Fredriksson1", "davidis@chalmers.se", "jan.bosch@chalmers.se", "helena.holmstrom.olsson@mau.se"], "authors": ["Teodor Anders Fredriksson", "David Issa Mattos", "Jan Bosch", "Helena Holmstr\u00f6m Olsson"], "keywords": ["Data Labeling", "Empirical Evaluation", "Active Machine Learning", "Semi-Supervised Learning"], "abstract": "The lack of labeled data is a major problem in both research and industrial settings since obtaining labels is often an expensive and time-consuming activity. In the past years, several machine learning algorithms were developed to assist and perform automated labeling in partially labeled datasets. While many of these algorithms are available in open-source packages, there is no research that investigates how these algorithms compare to each other in different types of datasets and with different percentages of available labels. To address this problem, this paper empirically evaluates and compares seven algorithms for automated labeling in terms of accuracy. We investigate how these algorithms perform in six different and well-known datasets with three different types of data, images, texts, and numerical values. We evaluate these algorithms under two different experimental conditions, with 10\\% and 50\\% labels of available labels in the dataset. Each algorithm, in each dataset for each experimental condition, is evaluated independently ten times with different random seeds. The results are analyzed and the algorithms are compared utilizing a Bayesian Bradley-Terry model. The results indicate that while the algorithms label spreading with K-nearest neighbors perform better in the aggregated results, the active learning algorithms query by instance QBC and query instance uncertainty sample perform better when there is only 10\\% of labels available. These results can help machine learning practitioners in choosing optimal machine learning algorithms to label their data.", "one-sentence_summary": "This paper provides an empirical evaluation of automatic labeling methods based on machine learning.", "pdf": "/pdf/d266ae12758df4ddd0f30d71e6236ea9cd097cc8.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "fredriksson|machine_learning_algorithms_for_data_labeling_an_empirical_evaluation", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=_Al4gzKd_i", "_bibtex": "@misc{\nfredriksson2021machine,\ntitle={Machine Learning Algorithms for Data Labeling: An Empirical Evaluation},\nauthor={Teodor Anders Fredriksson and David Issa Mattos and Jan Bosch and Helena Holmstr{\\\"o}m Olsson},\nyear={2021},\nurl={https://openreview.net/forum?id=389rLpWoOlG}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "389rLpWoOlG", "replyto": "389rLpWoOlG", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3219/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538079853, "tmdate": 1606915804464, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3219/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3219/-/Official_Review"}}}, {"id": "KRL6cnsc2nA", "original": null, "number": 2, "cdate": 1603806309806, "ddate": null, "tcdate": 1603806309806, "tmdate": 1605024044321, "tddate": null, "forum": "389rLpWoOlG", "replyto": "389rLpWoOlG", "invitation": "ICLR.cc/2021/Conference/Paper3219/-/Official_Review", "content": {"title": "An important topic (empirical evaluation) but it might require a major revision", "review": "In this paper, the authors present an empirical analysis of seven machine learning algorithms based on six benchmark datasets: Four graph-based semi-supervised learning algorithms and three active learning algorithms have been evaluated on image data sets (CIFAR10, Digits), texts (Fake and true news, 20news), and other data types (Iris and Wine). Based on the empirical performances of these algorithms, the authors provided a ranked list of the studied algorithms.\n\nEmpirical comparisons of multiple machine learning algorithms on real-world datasets are important as they help understand the strengths and weaknesses of different algorithms when they are deployed in real-world environments. However, I think this paper will benefit significantly from a major revision addressing the concerns listed below:\n1) Algorithm choices: all algorithms considered in the current paper were already extensively studied. Focusing on state of the art approaches (including recent deep learning-based approach) could significantly strengthen the practical relevance and impact of experiments and conclusions reported here.\n2) Data set choices: more challenging datasets can be considered: The datasets used in the current paper (including classical Iris and Wine) have up to only 20 categories.\n3) Presentation: I found it challenging to comprehend the exact experimental settings.\n- Section 3.1 states that 80% of data are allocated for training while the remaining 20% are used in testing. How does this setting apply for active learning algorithms? Did they actively select data points to label until when 80% are labeled or use only `50 instances\u2019?\n- How is 80/20% decomposition applied to semi-supervised learning algorithms? Did they use the entire dataset with 80% of the entities labeled? In typical application scenarios of semi-supervised learning, only small portions of data instances are labeled. I was not sure if the experimental setting prepared in the current paper reflects well the real-world application scenarios of semi-supervised learning.\n- Also, in general, semi-supervised learning and active learning are different problems. I was not sure how the results reported in the current paper including Table 1 should be interpreted: I guess this does not suggest that semi-supervised learning algorithms are better than active learning approaches.\n4) Without having access to two `anonymous\u2019 papers cited in this submission, it is hard to properly assess the contributions of this paper.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3219/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3219/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Machine Learning Algorithms for Data Labeling: An Empirical Evaluation", "authorids": ["~Teodor_Anders_Fredriksson1", "davidis@chalmers.se", "jan.bosch@chalmers.se", "helena.holmstrom.olsson@mau.se"], "authors": ["Teodor Anders Fredriksson", "David Issa Mattos", "Jan Bosch", "Helena Holmstr\u00f6m Olsson"], "keywords": ["Data Labeling", "Empirical Evaluation", "Active Machine Learning", "Semi-Supervised Learning"], "abstract": "The lack of labeled data is a major problem in both research and industrial settings since obtaining labels is often an expensive and time-consuming activity. In the past years, several machine learning algorithms were developed to assist and perform automated labeling in partially labeled datasets. While many of these algorithms are available in open-source packages, there is no research that investigates how these algorithms compare to each other in different types of datasets and with different percentages of available labels. To address this problem, this paper empirically evaluates and compares seven algorithms for automated labeling in terms of accuracy. We investigate how these algorithms perform in six different and well-known datasets with three different types of data, images, texts, and numerical values. We evaluate these algorithms under two different experimental conditions, with 10\\% and 50\\% labels of available labels in the dataset. Each algorithm, in each dataset for each experimental condition, is evaluated independently ten times with different random seeds. The results are analyzed and the algorithms are compared utilizing a Bayesian Bradley-Terry model. The results indicate that while the algorithms label spreading with K-nearest neighbors perform better in the aggregated results, the active learning algorithms query by instance QBC and query instance uncertainty sample perform better when there is only 10\\% of labels available. These results can help machine learning practitioners in choosing optimal machine learning algorithms to label their data.", "one-sentence_summary": "This paper provides an empirical evaluation of automatic labeling methods based on machine learning.", "pdf": "/pdf/d266ae12758df4ddd0f30d71e6236ea9cd097cc8.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "fredriksson|machine_learning_algorithms_for_data_labeling_an_empirical_evaluation", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=_Al4gzKd_i", "_bibtex": "@misc{\nfredriksson2021machine,\ntitle={Machine Learning Algorithms for Data Labeling: An Empirical Evaluation},\nauthor={Teodor Anders Fredriksson and David Issa Mattos and Jan Bosch and Helena Holmstr{\\\"o}m Olsson},\nyear={2021},\nurl={https://openreview.net/forum?id=389rLpWoOlG}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "389rLpWoOlG", "replyto": "389rLpWoOlG", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3219/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538079853, "tmdate": 1606915804464, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3219/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3219/-/Official_Review"}}}, {"id": "iTGQtmUBFBA", "original": null, "number": 3, "cdate": 1603824172935, "ddate": null, "tcdate": 1603824172935, "tmdate": 1605024044243, "tddate": null, "forum": "389rLpWoOlG", "replyto": "389rLpWoOlG", "invitation": "ICLR.cc/2021/Conference/Paper3219/-/Official_Review", "content": {"title": "Good idea, but flawed execution", "review": "The paper presents an empirical comparison of different approaches for data\nlabeling. The authors describe their experimental setup and findings, making\nrecommendations for when to use what approach in practice.\n\nThe authors reference their own anonymous work throughout the paper as\njustification for the presented investigation and its parameters. This is\nproblematic as the reviewers are now unable to confirm that the presented\ninvestigation is well-grounded.\n\nThe authors evaluate their approaches on only six datasets. It is unclear to\nwhat extent the results generalize, in particular as no detailed results per\ndataset are given. There could be significant differences between the different\ntypes of datasets, but not enough data is presented to judge. This matters in\nparticular with respect to the recommendations the authors make at the end of\nthe paper.\n\nSome details of the experimental setup are unclear. The authors say that they\nmeasure F1 score, but then refer to accuracy (e.g. in Figure 1). Which measure\nwas used? The experimental setup describes six datasets, but the results text\nrefers to seven. The results presented in Table 1 and Figure 1 seem to disagree\nwith Table 2 -- LabelSpreadingKNN is the highest-ranked algorithm, but\nUncertaintySampling performs better in terms of all the statistics presented in\nTable 1. The same is true for the second set of experiments (Tables 3 and 4).\nFor the first set of experiments it is unclear what fraction of labels were\nmissing.\n\nIt is unclear why the Bradley-Terry model was used here to compare outcomes.\nThere are multiple other methods to judge how and whether paired distributions\ndiffer. It appears that only ranks were used for this comparison and not the\nactual performance numbers.\n\nFinally, all methods evaluated by the authors have hyperparameters that need to\nbe set. It is unclear how the authors chose the particular values they used in\nthe experiments, and tuning them for best performance may have a major impact on\ntheir performance and the rankings. Conclusions from untuned methods are\nunlikely to generalize.\n\nThere are numerous typos and grammatical mistakes throughout the paper.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3219/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3219/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Machine Learning Algorithms for Data Labeling: An Empirical Evaluation", "authorids": ["~Teodor_Anders_Fredriksson1", "davidis@chalmers.se", "jan.bosch@chalmers.se", "helena.holmstrom.olsson@mau.se"], "authors": ["Teodor Anders Fredriksson", "David Issa Mattos", "Jan Bosch", "Helena Holmstr\u00f6m Olsson"], "keywords": ["Data Labeling", "Empirical Evaluation", "Active Machine Learning", "Semi-Supervised Learning"], "abstract": "The lack of labeled data is a major problem in both research and industrial settings since obtaining labels is often an expensive and time-consuming activity. In the past years, several machine learning algorithms were developed to assist and perform automated labeling in partially labeled datasets. While many of these algorithms are available in open-source packages, there is no research that investigates how these algorithms compare to each other in different types of datasets and with different percentages of available labels. To address this problem, this paper empirically evaluates and compares seven algorithms for automated labeling in terms of accuracy. We investigate how these algorithms perform in six different and well-known datasets with three different types of data, images, texts, and numerical values. We evaluate these algorithms under two different experimental conditions, with 10\\% and 50\\% labels of available labels in the dataset. Each algorithm, in each dataset for each experimental condition, is evaluated independently ten times with different random seeds. The results are analyzed and the algorithms are compared utilizing a Bayesian Bradley-Terry model. The results indicate that while the algorithms label spreading with K-nearest neighbors perform better in the aggregated results, the active learning algorithms query by instance QBC and query instance uncertainty sample perform better when there is only 10\\% of labels available. These results can help machine learning practitioners in choosing optimal machine learning algorithms to label their data.", "one-sentence_summary": "This paper provides an empirical evaluation of automatic labeling methods based on machine learning.", "pdf": "/pdf/d266ae12758df4ddd0f30d71e6236ea9cd097cc8.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "fredriksson|machine_learning_algorithms_for_data_labeling_an_empirical_evaluation", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=_Al4gzKd_i", "_bibtex": "@misc{\nfredriksson2021machine,\ntitle={Machine Learning Algorithms for Data Labeling: An Empirical Evaluation},\nauthor={Teodor Anders Fredriksson and David Issa Mattos and Jan Bosch and Helena Holmstr{\\\"o}m Olsson},\nyear={2021},\nurl={https://openreview.net/forum?id=389rLpWoOlG}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "389rLpWoOlG", "replyto": "389rLpWoOlG", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3219/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538079853, "tmdate": 1606915804464, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3219/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3219/-/Official_Review"}}}, {"id": "Hs2Nn8mjM3f", "original": null, "number": 4, "cdate": 1603892308567, "ddate": null, "tcdate": 1603892308567, "tmdate": 1605024044159, "tddate": null, "forum": "389rLpWoOlG", "replyto": "389rLpWoOlG", "invitation": "ICLR.cc/2021/Conference/Paper3219/-/Official_Review", "content": {"title": "A weak empirical evaluation", "review": "This paper aims to evaluate the performance of seven automated labeling algorithms in terms of accuracy. The authors conducted a set of experiments on six datasets from different domains under two typical settings where 10% and 50%of labels in the datasets are available. Experimental results show that the algorithms label spreading with KNN perform better in the aggregated results, the active learning algorithms  QBC and query instance uncertainty sample perform better when 10% of labels available.\n\nOverall, this paper cannot meet the high-quality requirements of ICLR.  First, active learning algorithms such as QBC and uncertainty sampling is not automated labeling algorithms. They are only strategies for the selection of unlabeled instances. The selected instance either can be labeled by human experts or automated labeling algorithms. Second, when evaluating an automated labeling algorithm, merely using accuracy is not enough. For example, when the underlying class distributions are imbalanced, the accuracy is not sufficient to characterize the generalization performance of a learning algorithm. Third, two settings of 10% and 50% of labels available are also insufficient. Many papers of the empirical study investigated the performance under more complicated settings. Finally, the number of investigated methods is two small and the paper should cover more state-of-the-art algorithms.", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper3219/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3219/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Machine Learning Algorithms for Data Labeling: An Empirical Evaluation", "authorids": ["~Teodor_Anders_Fredriksson1", "davidis@chalmers.se", "jan.bosch@chalmers.se", "helena.holmstrom.olsson@mau.se"], "authors": ["Teodor Anders Fredriksson", "David Issa Mattos", "Jan Bosch", "Helena Holmstr\u00f6m Olsson"], "keywords": ["Data Labeling", "Empirical Evaluation", "Active Machine Learning", "Semi-Supervised Learning"], "abstract": "The lack of labeled data is a major problem in both research and industrial settings since obtaining labels is often an expensive and time-consuming activity. In the past years, several machine learning algorithms were developed to assist and perform automated labeling in partially labeled datasets. While many of these algorithms are available in open-source packages, there is no research that investigates how these algorithms compare to each other in different types of datasets and with different percentages of available labels. To address this problem, this paper empirically evaluates and compares seven algorithms for automated labeling in terms of accuracy. We investigate how these algorithms perform in six different and well-known datasets with three different types of data, images, texts, and numerical values. We evaluate these algorithms under two different experimental conditions, with 10\\% and 50\\% labels of available labels in the dataset. Each algorithm, in each dataset for each experimental condition, is evaluated independently ten times with different random seeds. The results are analyzed and the algorithms are compared utilizing a Bayesian Bradley-Terry model. The results indicate that while the algorithms label spreading with K-nearest neighbors perform better in the aggregated results, the active learning algorithms query by instance QBC and query instance uncertainty sample perform better when there is only 10\\% of labels available. These results can help machine learning practitioners in choosing optimal machine learning algorithms to label their data.", "one-sentence_summary": "This paper provides an empirical evaluation of automatic labeling methods based on machine learning.", "pdf": "/pdf/d266ae12758df4ddd0f30d71e6236ea9cd097cc8.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "fredriksson|machine_learning_algorithms_for_data_labeling_an_empirical_evaluation", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=_Al4gzKd_i", "_bibtex": "@misc{\nfredriksson2021machine,\ntitle={Machine Learning Algorithms for Data Labeling: An Empirical Evaluation},\nauthor={Teodor Anders Fredriksson and David Issa Mattos and Jan Bosch and Helena Holmstr{\\\"o}m Olsson},\nyear={2021},\nurl={https://openreview.net/forum?id=389rLpWoOlG}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "389rLpWoOlG", "replyto": "389rLpWoOlG", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3219/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538079853, "tmdate": 1606915804464, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3219/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3219/-/Official_Review"}}}], "count": 6}