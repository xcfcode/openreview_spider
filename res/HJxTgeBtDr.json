{"notes": [{"id": "HJxTgeBtDr", "original": "Hye54AyKDH", "number": 2116, "cdate": 1569439733134, "ddate": null, "tcdate": 1569439733134, "tmdate": 1577168278435, "tddate": null, "forum": "HJxTgeBtDr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["fujl16@fudan.edu.cn", "pfliu14@fudan.edu.cn", "xjhuang@fudan.edu.cn"], "title": "Towards Interpretable Evaluations: A Case Study of Named Entity Recognition", "authors": ["Jinlan Fu", "Pengfei Liu", "Xuanjing Huang"], "pdf": "/pdf/657da0da10c50375c849beb311118aecc12cf382.pdf", "TL;DR": "We propose a generalized evaluation methodology to interpret model biases, dataset biases, and their correlation.", "abstract": "    With the proliferation of models for natural language processing (NLP) tasks, it is even harder to understand the differences between models and their relative merits. Simply looking at differences between holistic metrics such as accuracy, BLEU, or F1 do not tell us \\emph{why} or \\emph{how} a particular method is better and how dataset biases influence the choices of model design.\n    In this paper, we present a general methodology for {\\emph{interpretable}} evaluation of NLP systems and choose the task of named entity recognition (NER) as a case study, which is a core task of identifying people, places, or organizations in text. The proposed evaluation method enables us to interpret the \\textit{model biases}, \\textit{dataset biases}, and how the \\emph{differences in the datasets} affect the design of the models, identifying the strengths and weaknesses of current approaches. By making our {analysis} tool available, we make it easy for future researchers to run similar analyses and drive the progress in this area.", "keywords": ["interpretable evaluation", "dataset biases", "model biases", "NER"], "paperhash": "fu|towards_interpretable_evaluations_a_case_study_of_named_entity_recognition", "original_pdf": "/attachment/8c3cd5119ed242865846d17f2be72125c10a8de1.pdf", "_bibtex": "@misc{\nfu2020towards,\ntitle={Towards Interpretable Evaluations: A Case Study of Named Entity Recognition},\nauthor={Jinlan Fu and Pengfei Liu and Xuanjing Huang},\nyear={2020},\nurl={https://openreview.net/forum?id=HJxTgeBtDr}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "hYlQMrsvmf", "original": null, "number": 1, "cdate": 1576798740943, "ddate": null, "tcdate": 1576798740943, "tmdate": 1576800895278, "tddate": null, "forum": "HJxTgeBtDr", "replyto": "HJxTgeBtDr", "invitation": "ICLR.cc/2020/Conference/Paper2116/-/Decision", "content": {"decision": "Reject", "comment": "The paper diligently setup and conducted multiple experiments to validate their approach - bucketizating attributions of data and analyze them accordingly to discover deeper insights eg biases. However, reviewers pointed out that such bucketing is tailored to tasks where attributions are easily observed, such as the one of the focus in this paper -NER. While manuscript proposes this approach as \u2018general\u2019, reviewers failed to seem this point. Another reviewer recommended this manuscript to become a journal item rather than conference, due to the length of the page in appendix (17). There were some confusions around writings as well, pointed out by some reviewers. We highly recommend authors to carefully reflect on reviewers both pros and cons of the paper to improve the paper for your future submission. \n", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["fujl16@fudan.edu.cn", "pfliu14@fudan.edu.cn", "xjhuang@fudan.edu.cn"], "title": "Towards Interpretable Evaluations: A Case Study of Named Entity Recognition", "authors": ["Jinlan Fu", "Pengfei Liu", "Xuanjing Huang"], "pdf": "/pdf/657da0da10c50375c849beb311118aecc12cf382.pdf", "TL;DR": "We propose a generalized evaluation methodology to interpret model biases, dataset biases, and their correlation.", "abstract": "    With the proliferation of models for natural language processing (NLP) tasks, it is even harder to understand the differences between models and their relative merits. Simply looking at differences between holistic metrics such as accuracy, BLEU, or F1 do not tell us \\emph{why} or \\emph{how} a particular method is better and how dataset biases influence the choices of model design.\n    In this paper, we present a general methodology for {\\emph{interpretable}} evaluation of NLP systems and choose the task of named entity recognition (NER) as a case study, which is a core task of identifying people, places, or organizations in text. The proposed evaluation method enables us to interpret the \\textit{model biases}, \\textit{dataset biases}, and how the \\emph{differences in the datasets} affect the design of the models, identifying the strengths and weaknesses of current approaches. By making our {analysis} tool available, we make it easy for future researchers to run similar analyses and drive the progress in this area.", "keywords": ["interpretable evaluation", "dataset biases", "model biases", "NER"], "paperhash": "fu|towards_interpretable_evaluations_a_case_study_of_named_entity_recognition", "original_pdf": "/attachment/8c3cd5119ed242865846d17f2be72125c10a8de1.pdf", "_bibtex": "@misc{\nfu2020towards,\ntitle={Towards Interpretable Evaluations: A Case Study of Named Entity Recognition},\nauthor={Jinlan Fu and Pengfei Liu and Xuanjing Huang},\nyear={2020},\nurl={https://openreview.net/forum?id=HJxTgeBtDr}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "HJxTgeBtDr", "replyto": "HJxTgeBtDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795714882, "tmdate": 1576800264670, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2116/-/Decision"}}}, {"id": "Bye062bisS", "original": null, "number": 8, "cdate": 1573752006448, "ddate": null, "tcdate": 1573752006448, "tmdate": 1573752006448, "tddate": null, "forum": "HJxTgeBtDr", "replyto": "Skgm6QnaYr", "invitation": "ICLR.cc/2020/Conference/Paper2116/-/Official_Comment", "content": {"title": "Response to Review #1-Part3", "comment": "For other detailed suggestions, we have refined our paper based on your feedback:\n1)\u00a0\u00a0\u00a0\u00a0Clarify the description of \u201cSupplementary exam\u201d\n2) \u00a0Re-organize the Sec.2.2 and remove some repetition in methodological\nperspective\n3)\u00a0\u00a0\u00a0\u00a0Merge Sec.3 into Sec.2 \n4)\u00a0\u00a0\u00a0\u00a0Add a more intuitive explanation of the measures defined in Sec.4.3 (3.3 in new version)\n\nHope we address your concern correctly and look forward to your feedback again.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2116/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2116/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["fujl16@fudan.edu.cn", "pfliu14@fudan.edu.cn", "xjhuang@fudan.edu.cn"], "title": "Towards Interpretable Evaluations: A Case Study of Named Entity Recognition", "authors": ["Jinlan Fu", "Pengfei Liu", "Xuanjing Huang"], "pdf": "/pdf/657da0da10c50375c849beb311118aecc12cf382.pdf", "TL;DR": "We propose a generalized evaluation methodology to interpret model biases, dataset biases, and their correlation.", "abstract": "    With the proliferation of models for natural language processing (NLP) tasks, it is even harder to understand the differences between models and their relative merits. Simply looking at differences between holistic metrics such as accuracy, BLEU, or F1 do not tell us \\emph{why} or \\emph{how} a particular method is better and how dataset biases influence the choices of model design.\n    In this paper, we present a general methodology for {\\emph{interpretable}} evaluation of NLP systems and choose the task of named entity recognition (NER) as a case study, which is a core task of identifying people, places, or organizations in text. The proposed evaluation method enables us to interpret the \\textit{model biases}, \\textit{dataset biases}, and how the \\emph{differences in the datasets} affect the design of the models, identifying the strengths and weaknesses of current approaches. By making our {analysis} tool available, we make it easy for future researchers to run similar analyses and drive the progress in this area.", "keywords": ["interpretable evaluation", "dataset biases", "model biases", "NER"], "paperhash": "fu|towards_interpretable_evaluations_a_case_study_of_named_entity_recognition", "original_pdf": "/attachment/8c3cd5119ed242865846d17f2be72125c10a8de1.pdf", "_bibtex": "@misc{\nfu2020towards,\ntitle={Towards Interpretable Evaluations: A Case Study of Named Entity Recognition},\nauthor={Jinlan Fu and Pengfei Liu and Xuanjing Huang},\nyear={2020},\nurl={https://openreview.net/forum?id=HJxTgeBtDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJxTgeBtDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2116/Authors", "ICLR.cc/2020/Conference/Paper2116/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2116/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2116/Reviewers", "ICLR.cc/2020/Conference/Paper2116/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2116/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2116/Authors|ICLR.cc/2020/Conference/Paper2116/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504146094, "tmdate": 1576860536137, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2116/Authors", "ICLR.cc/2020/Conference/Paper2116/Reviewers", "ICLR.cc/2020/Conference/Paper2116/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2116/-/Official_Comment"}}}, {"id": "rkxnw3-isS", "original": null, "number": 7, "cdate": 1573751907621, "ddate": null, "tcdate": 1573751907621, "tmdate": 1573751907621, "tddate": null, "forum": "HJxTgeBtDr", "replyto": "Skgm6QnaYr", "invitation": "ICLR.cc/2020/Conference/Paper2116/-/Official_Comment", "content": {"title": "Response to Review #1-Part2", "comment": "We appreciate your thorough review and helpful suggestions. We will try to address your questions below.\n\nQ1: \u201cThese metrics are given in section 4 but here again only from a formal point of view: it is very difficult for the reader to understand how to interpret them and how to use them for a practical case.\u201d\nA1: Thanks for your feedback. Here, it is the generality of the methodology that allows us to\ndescribe it from a formal point of view. In a practical case, this method could be adapted to other tasks easily. Below, we would like to give a more specific explanation of how to use them for a practical case.\nAs shown in the Table (Part1), for a given NLP task, once we determine related attributes and Bucket Strategy, we could calculate proposed metrics in Sec.4.2 (Sec.3.2 in new version) and make similar analyses.\n\nQ2: \u201cIn the end, the proposition is a formalisation of the simple error analysis which is commonly\ndone when trying to improve a machine learning system. The advantage of the\nmethod could be to introduce some metrics to make the error analysis more\nautomatic.\u201d\nA2: Our method shares some common properties with error analysis, but beyond it:\n\u00a0\u00a0\u00a0 1) Regarding model analysis, (automatic) error analysis suffers from the confirmation\nbias (Tab.1) problem while our method doesn\u2019t.\nMoreover, the development of error analysis stopped at focusing solely on a\nsingle dataset [1][2][3]. Many challenges will come when we take the multi-dataset setting into account. This work takes a step towards diagnosing the strengths and weaknesses of different models under different datasets.\n\u00a0\u00a0\u00a0 2) Regarding dataset analysis, the proposed methodology enables us to quantify the data biases, knowing more about the characteristics of each dataset, which is beyond the grasp of error analysis.\n\n[1] Joke Daems, Lieve Macken, and Sonia Vandepitte. On the origin of errors: A fine-grained analysis of mt and pe errors and their relationship\n[2] Jonathan K. Kummerfeld and Dan Klein. Error-driven analysis of challenges in coreference resolution\n[3] Jonathan K. Kummerfeld, David Hall, James R. Curran, and Dan Klein. Parser showdown at the wall street corral: An empirical investigation of error types in parser output\n\n\nQ3: \u201cfamiliarity : test/train distribution should be the same.\u201c\nA3: We\u2019re sorry for not understanding your statement clearly \u201ctest/train distribution should be the\nsame.\u201d Do you mean if the calculation of the familiarity requires that test/train\ndistribution should be the same?\u00a0 If, in that case, the answer is the calculation of the familiarity doesn\u2019t require that. \u00a0\n\nQ4: \u201cFk computer on train set because it is bigger ?\u201d\nA4: Do you mean if we calculate F_k on the train set because it is bigger?\nIf, in that case, the answer is no. The F_k is defined over training set is why we called it as #familiarity#: it could reflect how the statistics in training set of an attribute influence the test performance.\n\nQ5: \u201cit allows to study the impact of the number of occurrences in the training set. Is it more interesting than a learning curve ?\u201d\nA5: The main contribution of this paper is not only to study how the occurrences of some attributes in training set influence different models, but also to investigate how different datasets are sensitive to occurrences of some attributes in the training set. \u00a0A learning curve is far from this goal.\n\nQ6: \u201cmulti attribute familiarity : risk of metric explosion ? how to select the attributes ?\u201d\nA6: Multi-attribute familiarity may be a risk of metric explosion, but at the same time, it could\nencourage us to explore new meaningful measures. For example, \u201cMF-et\u201d could\nquantify the category ambiguity phenomenon. (Analogously, deep neural networks\nachieve impressive at the cost of architecture engineering.).  We would like to search for new combinations of attributes, which could be as our future work but is out of scope for this paper.\n\u00a0\nQ7: \u201cThe encoding of the model name is not clear\u201d\nA7: Although we have tried our best to name these 11 models more intuitive and more precise (by\nhighlighted sub-words, detailed model choices), yet we still think we could do it better. We have added more explanation in our revised version.\n\nQ8: \u201ca metric on all the dataset for each model could be computed to decide which one is the best\noverall\u201d\nA8: We are not sure if we have understood your meaning of the term \u201cmetric\u201d: Do you suggest we evaluate each model based on solely one attribute and find another best overall? Here, it would make no sense since different attributes just provide more fine-grained results, which will not lead to a new best overall performance.\n\nQ9: \u201canalysis of Fig 4 : R-eLen does not existe (R-Ele). what is eta ?\u201d\nA9: Thanks for catching the typos. We have corrected them: \u201cR-Ele, R-eLen -> R-eLen\u201d, eta -> zeta\n\nQ10: \u201cfigure 2 : where are the links to levels ?\u201d\nA10: Fig.2 is used to aid the understanding of the \u201cAttribute Definition\u201d (Sec.3.1) and \u201cBucketization Strategy\u201d (Sec.3.2).\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2116/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2116/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["fujl16@fudan.edu.cn", "pfliu14@fudan.edu.cn", "xjhuang@fudan.edu.cn"], "title": "Towards Interpretable Evaluations: A Case Study of Named Entity Recognition", "authors": ["Jinlan Fu", "Pengfei Liu", "Xuanjing Huang"], "pdf": "/pdf/657da0da10c50375c849beb311118aecc12cf382.pdf", "TL;DR": "We propose a generalized evaluation methodology to interpret model biases, dataset biases, and their correlation.", "abstract": "    With the proliferation of models for natural language processing (NLP) tasks, it is even harder to understand the differences between models and their relative merits. Simply looking at differences between holistic metrics such as accuracy, BLEU, or F1 do not tell us \\emph{why} or \\emph{how} a particular method is better and how dataset biases influence the choices of model design.\n    In this paper, we present a general methodology for {\\emph{interpretable}} evaluation of NLP systems and choose the task of named entity recognition (NER) as a case study, which is a core task of identifying people, places, or organizations in text. The proposed evaluation method enables us to interpret the \\textit{model biases}, \\textit{dataset biases}, and how the \\emph{differences in the datasets} affect the design of the models, identifying the strengths and weaknesses of current approaches. By making our {analysis} tool available, we make it easy for future researchers to run similar analyses and drive the progress in this area.", "keywords": ["interpretable evaluation", "dataset biases", "model biases", "NER"], "paperhash": "fu|towards_interpretable_evaluations_a_case_study_of_named_entity_recognition", "original_pdf": "/attachment/8c3cd5119ed242865846d17f2be72125c10a8de1.pdf", "_bibtex": "@misc{\nfu2020towards,\ntitle={Towards Interpretable Evaluations: A Case Study of Named Entity Recognition},\nauthor={Jinlan Fu and Pengfei Liu and Xuanjing Huang},\nyear={2020},\nurl={https://openreview.net/forum?id=HJxTgeBtDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJxTgeBtDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2116/Authors", "ICLR.cc/2020/Conference/Paper2116/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2116/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2116/Reviewers", "ICLR.cc/2020/Conference/Paper2116/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2116/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2116/Authors|ICLR.cc/2020/Conference/Paper2116/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504146094, "tmdate": 1576860536137, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2116/Authors", "ICLR.cc/2020/Conference/Paper2116/Reviewers", "ICLR.cc/2020/Conference/Paper2116/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2116/-/Official_Comment"}}}, {"id": "Syg_KsbijH", "original": null, "number": 6, "cdate": 1573751680452, "ddate": null, "tcdate": 1573751680452, "tmdate": 1573751680452, "tddate": null, "forum": "HJxTgeBtDr", "replyto": "Skgm6QnaYr", "invitation": "ICLR.cc/2020/Conference/Paper2116/-/Official_Comment", "content": {"title": "Response to Review #1-Part1", "comment": "*****************************************************************************************\nTasks\t\t\t                        Attributes\t\t\t                     Measures\t\tBucket Strategy\n*****************************************************************************************\nMachine Translation\t               sentence length\t\t\t             Bleu\t\t              R-Buck\nMachine Translation\t               word (or N-gram) frequency\t     Accuracy*\t\t      R-Buck\n\t\t\t                               in the training set.\t \nMachine Translation\t               word POS-tag in the training set    Accuracy*\t\t       R-Buck\nMachine Translation\t               words in reference file\t\t             Word likelihood      R-Buck\n------------------------------------------------------------------------------------------------------------------------\nSummarization (Ext. or Abs.)\tsentence length\t\t\t             Rouge\t\t        R-Buck\nSummarization (Ext. or Abs.)\tcompression of summary\t\t     Rouge\t\t        R or F-Buck\nSummarization (Ext. or Abs.)\tdensity of summary\t\t             Rouge\t\t        R or F-Buck\nSummarization (Ext. or Abs.)\tvolume overlap\t\t\t             Rouge\t\t        R-Buck\n------------------------------------------------------------------------------------------------------------------------\nSummarization (Ext.)\t                 position of each sentence\t\t     Rouge/Accuracy     F-Buck\nSummarization (Ext.)\t                 OOV rate of sentence\t\t     Rouge\t\t        R-Buck\n------------------------------------------------------------------------------------------------------------------------\nSemantic Matching\t\t         length of sent1 or sent2\t             Accuracy\t\t        R-Buck\nSemantic Matching\t\t         Func(sent1, sent2)\t                     Accuracy\t                R-Buck\nSemantic Matching\t\t         OOV\t\t\t\t                     Accuracy\t\t        R-Buck\n------------------------------------------------------------------------------------------------------------------------\nQA\t\t\t                                 answer length, type, position\t    Matching F1\t         F-Buck\nQA\t\t\t                                 document length\t\t\t    Matching F1\t         R-Buck\nQA\t\t\t                                 query type\t\t\t    Matching F1\t         F-Buck\n------------------------------------------------------------------------------------------------------------------------\nText Classification\t\t         sentence/word length\t\t     Accuracy\t\t          R-Buck\nText Classification\t\t         OOV\t\t\t\t                     Accuracy\t\t          R-Buck\nText Classification\t\t         sentence familiarity\t\t\t     Accuracy\t\t          F-Buck\n------------------------------------------------------------------------------------------------------------------------\nSequence labeling\t\t         Similar to this work\t\n------------------------------------------------------------------------------------------------------------------------\n\nSimilar to the Table mentioned in R3, the \u201cTasks\u201d column shows different types of tasks.\n\u201cAttributes\u201d denotes the criterion that we use to divide the test set, and \u201cMeasures\u201d represents the measure we use to evaluate each divided sub-set. \u201cBucket Strategy\u201d shows which types of bucketization methods could be adopted. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2116/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2116/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["fujl16@fudan.edu.cn", "pfliu14@fudan.edu.cn", "xjhuang@fudan.edu.cn"], "title": "Towards Interpretable Evaluations: A Case Study of Named Entity Recognition", "authors": ["Jinlan Fu", "Pengfei Liu", "Xuanjing Huang"], "pdf": "/pdf/657da0da10c50375c849beb311118aecc12cf382.pdf", "TL;DR": "We propose a generalized evaluation methodology to interpret model biases, dataset biases, and their correlation.", "abstract": "    With the proliferation of models for natural language processing (NLP) tasks, it is even harder to understand the differences between models and their relative merits. Simply looking at differences between holistic metrics such as accuracy, BLEU, or F1 do not tell us \\emph{why} or \\emph{how} a particular method is better and how dataset biases influence the choices of model design.\n    In this paper, we present a general methodology for {\\emph{interpretable}} evaluation of NLP systems and choose the task of named entity recognition (NER) as a case study, which is a core task of identifying people, places, or organizations in text. The proposed evaluation method enables us to interpret the \\textit{model biases}, \\textit{dataset biases}, and how the \\emph{differences in the datasets} affect the design of the models, identifying the strengths and weaknesses of current approaches. By making our {analysis} tool available, we make it easy for future researchers to run similar analyses and drive the progress in this area.", "keywords": ["interpretable evaluation", "dataset biases", "model biases", "NER"], "paperhash": "fu|towards_interpretable_evaluations_a_case_study_of_named_entity_recognition", "original_pdf": "/attachment/8c3cd5119ed242865846d17f2be72125c10a8de1.pdf", "_bibtex": "@misc{\nfu2020towards,\ntitle={Towards Interpretable Evaluations: A Case Study of Named Entity Recognition},\nauthor={Jinlan Fu and Pengfei Liu and Xuanjing Huang},\nyear={2020},\nurl={https://openreview.net/forum?id=HJxTgeBtDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJxTgeBtDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2116/Authors", "ICLR.cc/2020/Conference/Paper2116/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2116/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2116/Reviewers", "ICLR.cc/2020/Conference/Paper2116/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2116/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2116/Authors|ICLR.cc/2020/Conference/Paper2116/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504146094, "tmdate": 1576860536137, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2116/Authors", "ICLR.cc/2020/Conference/Paper2116/Reviewers", "ICLR.cc/2020/Conference/Paper2116/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2116/-/Official_Comment"}}}, {"id": "SklmjObioH", "original": null, "number": 5, "cdate": 1573750938666, "ddate": null, "tcdate": 1573750938666, "tmdate": 1573750938666, "tddate": null, "forum": "HJxTgeBtDr", "replyto": "ByljmRZSqH", "invitation": "ICLR.cc/2020/Conference/Paper2116/-/Official_Comment", "content": {"title": "Response to Review #2", "comment": "Thank you for your encouraging review. We will continue to improve the draft in the revised version.\n\nQ1: \u201cThe bucketization idea is not something out of the park novel. It is probably something already being used in practice. However, delineating the procedure and suggesting quantifiable statistics and designing experiments to illustrate how these can be used to draw qualitative conclusions is something that is very interesting and useful to the community as a whole.\u201d\nA1: We\u2019re quite excited that you have pointed out the most challenging part of our work.\nYes, when we would like to take multiple attributes, models, datasets all together, the most\nchallenging thing is how to derive specific conclusions based on these tremendous results. In this paper, we overcome the difficulty by designing several meaningful measures, which can help us understand the relative merits between models quantitatively. This work also would like to show:\nwhen multiple datasets, models are ready, the time is ripe for us to shift the data-driven\nlearning to data-driven analyzing (conduct an analysis over plenty of experimental data with the help of meaning measures)\n\nQ2: \u201cWhile the authors have tried to state that the method is \"general\" and goes beyond NER, I am not sure if that is the case. The creation of attribute buckets is vital for any further analysis, its\nnot clear how the method can be adapted to more general settings unless such\nattributes and buckets can be created easily (e.g. using domain knowledge).\u201d\nA2: We try to address your concern by presenting a detailed description of general settings on other tasks. You could refer to our first answer to R3.\n\nQ3: \u201cthe paper is well-written and easy to understand, albeit some of the related work seems a little unrelated to the task at hand\u201d\nA3: Thanks for your suggestion and we have made it revised in our new version.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2116/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2116/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["fujl16@fudan.edu.cn", "pfliu14@fudan.edu.cn", "xjhuang@fudan.edu.cn"], "title": "Towards Interpretable Evaluations: A Case Study of Named Entity Recognition", "authors": ["Jinlan Fu", "Pengfei Liu", "Xuanjing Huang"], "pdf": "/pdf/657da0da10c50375c849beb311118aecc12cf382.pdf", "TL;DR": "We propose a generalized evaluation methodology to interpret model biases, dataset biases, and their correlation.", "abstract": "    With the proliferation of models for natural language processing (NLP) tasks, it is even harder to understand the differences between models and their relative merits. Simply looking at differences between holistic metrics such as accuracy, BLEU, or F1 do not tell us \\emph{why} or \\emph{how} a particular method is better and how dataset biases influence the choices of model design.\n    In this paper, we present a general methodology for {\\emph{interpretable}} evaluation of NLP systems and choose the task of named entity recognition (NER) as a case study, which is a core task of identifying people, places, or organizations in text. The proposed evaluation method enables us to interpret the \\textit{model biases}, \\textit{dataset biases}, and how the \\emph{differences in the datasets} affect the design of the models, identifying the strengths and weaknesses of current approaches. By making our {analysis} tool available, we make it easy for future researchers to run similar analyses and drive the progress in this area.", "keywords": ["interpretable evaluation", "dataset biases", "model biases", "NER"], "paperhash": "fu|towards_interpretable_evaluations_a_case_study_of_named_entity_recognition", "original_pdf": "/attachment/8c3cd5119ed242865846d17f2be72125c10a8de1.pdf", "_bibtex": "@misc{\nfu2020towards,\ntitle={Towards Interpretable Evaluations: A Case Study of Named Entity Recognition},\nauthor={Jinlan Fu and Pengfei Liu and Xuanjing Huang},\nyear={2020},\nurl={https://openreview.net/forum?id=HJxTgeBtDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJxTgeBtDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2116/Authors", "ICLR.cc/2020/Conference/Paper2116/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2116/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2116/Reviewers", "ICLR.cc/2020/Conference/Paper2116/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2116/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2116/Authors|ICLR.cc/2020/Conference/Paper2116/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504146094, "tmdate": 1576860536137, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2116/Authors", "ICLR.cc/2020/Conference/Paper2116/Reviewers", "ICLR.cc/2020/Conference/Paper2116/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2116/-/Official_Comment"}}}, {"id": "r1gOzubiir", "original": null, "number": 4, "cdate": 1573750800396, "ddate": null, "tcdate": 1573750800396, "tmdate": 1573750800396, "tddate": null, "forum": "HJxTgeBtDr", "replyto": "rkxmuiLnYH", "invitation": "ICLR.cc/2020/Conference/Paper2116/-/Official_Comment", "content": {"title": "Response to Review #3-Part2", "comment": "Q1: \u201cHowever, it seems to be somewhat tailored to the NER task. My question is: How well the proposed method generalizes to other NLP tasks without attributes? Similarly, how well the\nproposed bucketization strategies generalize beyond the NER task?\u201d\nA1: To adapt this methodology to other tasks, we honestly admit that the process of attribute definition usually requires some domain knowledge. However, we would like to show the process is not complicated since many task-agnostic attributes could be applied, such as oov, sentence length. Importantly, we believe each domain-specific expert should take responsibility for driving the development of the domain based on their understanding of the task, and hopefully, this work could provide such a methodology in which domain knowledge from different tasks could be utilized.\u00a0\nAs a preliminary summary in the  table (Part1), we share some task-specific definition of attributes, where \u201cTasks\u201d represents different types of tasks; \u201cAttributes\u201d denotes the criterion that we use to divide the test set, and \u201cMeasures\u201d represents the measure we use to evaluate each divided sub-set. \u201cRelated ref.\u201d shows the corresponding papers that have adopted the attribute for fine-grained evaluation.\n\n\nQ2: In Section 4.2, for the R-Bucket strategy it is stated as having the requirement of discrete and finite attributes. Based on the equations of the other two strategies (R-bucket and F-bucket), it seems that they also have the requirement of having discrete attributes. Is this indeed the case? if so, it should be explicitly indicated. Having said that, this raises another question: Is this protocol exclusive to tasks/problems with explicit discrete attributes?\nA2: We are sorry for not making it clear, and we have refined the description of the bucket strategy more clearly. Specifically, R- and F- Bucket strategies could be applied to\nattributes with discrete value and continuous value. For example, the \u201centity density\u201d attribute we used in this paper. Although its value is continuous, we could discretize the value into different ranges (i.e. low, medium, high), and then we could adopt the R- or F-Bucket strategies.\n\n\nQ3: \u201cLast paragraph of Section 4.2 summarizes ideas that were just presented. It feels somewhat\nredundant. I suggest removing in in favor of extending the existing discussions\nand analysis.\u201d\nA3: Thanks for your granular suggestions, and you can see the modification in our revised version.\n\nQ4: \"something very desirable for every evaluation. As such, in my opinion, the \"interpretable\" tag associate to the proposed method is somewhat out of place. Having said that, I would recommend removing the \"interpretable\" tag and stress the contribution of this manuscript as\nan evaluation protocol.\"\nA4: Thanks for your constructive suggestion, and we have carefully considered it. However, we have not taken it yet in our revised version, and we would like to share our reasons:\n\u00a0\u00a01) In this work, we aim to interpret the model biases, dataset biases, and their correlation. Some of the previous work also involves the \"bucketize-then-evaluate\" idea (As we have listed in the above table and mentioned in the introduction section) while they are without a quantitative process to analyze these biases. \n\u00a0 2) We also would like to show that this attribute-aided evaluation method could be a way for us to understand our black-box models and datasets.\n\nThanks again for your insightful comments! We have already refined the paper, please check the latest version. Hope we answer your questions correctly and look forward to your feedback again!"}, "signatures": ["ICLR.cc/2020/Conference/Paper2116/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2116/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["fujl16@fudan.edu.cn", "pfliu14@fudan.edu.cn", "xjhuang@fudan.edu.cn"], "title": "Towards Interpretable Evaluations: A Case Study of Named Entity Recognition", "authors": ["Jinlan Fu", "Pengfei Liu", "Xuanjing Huang"], "pdf": "/pdf/657da0da10c50375c849beb311118aecc12cf382.pdf", "TL;DR": "We propose a generalized evaluation methodology to interpret model biases, dataset biases, and their correlation.", "abstract": "    With the proliferation of models for natural language processing (NLP) tasks, it is even harder to understand the differences between models and their relative merits. Simply looking at differences between holistic metrics such as accuracy, BLEU, or F1 do not tell us \\emph{why} or \\emph{how} a particular method is better and how dataset biases influence the choices of model design.\n    In this paper, we present a general methodology for {\\emph{interpretable}} evaluation of NLP systems and choose the task of named entity recognition (NER) as a case study, which is a core task of identifying people, places, or organizations in text. The proposed evaluation method enables us to interpret the \\textit{model biases}, \\textit{dataset biases}, and how the \\emph{differences in the datasets} affect the design of the models, identifying the strengths and weaknesses of current approaches. By making our {analysis} tool available, we make it easy for future researchers to run similar analyses and drive the progress in this area.", "keywords": ["interpretable evaluation", "dataset biases", "model biases", "NER"], "paperhash": "fu|towards_interpretable_evaluations_a_case_study_of_named_entity_recognition", "original_pdf": "/attachment/8c3cd5119ed242865846d17f2be72125c10a8de1.pdf", "_bibtex": "@misc{\nfu2020towards,\ntitle={Towards Interpretable Evaluations: A Case Study of Named Entity Recognition},\nauthor={Jinlan Fu and Pengfei Liu and Xuanjing Huang},\nyear={2020},\nurl={https://openreview.net/forum?id=HJxTgeBtDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJxTgeBtDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2116/Authors", "ICLR.cc/2020/Conference/Paper2116/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2116/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2116/Reviewers", "ICLR.cc/2020/Conference/Paper2116/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2116/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2116/Authors|ICLR.cc/2020/Conference/Paper2116/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504146094, "tmdate": 1576860536137, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2116/Authors", "ICLR.cc/2020/Conference/Paper2116/Reviewers", "ICLR.cc/2020/Conference/Paper2116/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2116/-/Official_Comment"}}}, {"id": "BJlGfDZssr", "original": null, "number": 3, "cdate": 1573750537972, "ddate": null, "tcdate": 1573750537972, "tmdate": 1573750537972, "tddate": null, "forum": "HJxTgeBtDr", "replyto": "rkxmuiLnYH", "invitation": "ICLR.cc/2020/Conference/Paper2116/-/Official_Comment", "content": {"title": "Response to Review #3-Part1", "comment": "\n*****************************************************************************************\nTasks\t\t\t                        Attributes\t\t\t                     Measures\t\tRelated Ref.\n*****************************************************************************************\nMachine Translation\t               sentence length\t\t\t             Bleu\t\t              [1]\nMachine Translation\t               word (or N-gram) frequency\t     Accuracy*\t\t      [2]\n\t\t\t                               in the training set.\t \nMachine Translation\t               word POS-tag in the training set    Accuracy*\t\t       [3]\nMachine Translation\t               words in reference file\t\t             Word likelihood      [3]\n------------------------------------------------------------------------------------------------------------------------\nSummarization (Ext. or Abs.)\tsentence length\t\t\t             Rouge\t\t        -\nSummarization (Ext. or Abs.)\tcompression of summary\t\t     Rouge\t\t       [6]\nSummarization (Ext. or Abs.)\tdensity of summary\t\t             Rouge\t\t       [6]\nSummarization (Ext. or Abs.)\tvolume overlap\t\t\t             Rouge\t\t       [5]\n------------------------------------------------------------------------------------------------------------------------\nSummarization (Ext.)\t                 position of each sentence\t\t     Rouge/Accuracy     [4] [5]\nSummarization (Ext.)\t                 OOV rate of sentence\t\t     Rouge\t\t         -\n------------------------------------------------------------------------------------------------------------------------\nSemantic Matching\t\t         length of sent1 or sent2\t             Accuracy\t\t        [7]\nSemantic Matching\t\t         Func(sent1, sent2)\t                     Accuracy\t                  -\nSemantic Matching\t\t         OOV\t\t\t\t                     Accuracy\t\t          -\n------------------------------------------------------------------------------------------------------------------------\nQA\t\t\t                                 answer length, type, position\t    Matching F1\t         [8]\nQA\t\t\t                                 document length\t\t\t    Matching F1\t         [12]\nQA\t\t\t                                 query length, type\t\t\t    Matching F1\t         [9]\n------------------------------------------------------------------------------------------------------------------------\nText Classification\t\t         sentence/word length\t\t     Accuracy\t\t          [11]\nText Classification\t\t         OOV\t\t\t\t                     Accuracy\t\t          [10]\nText Classification\t\t         sentence familiarity\t\t\t     Accuracy\t\t           -\n------------------------------------------------------------------------------------------------------------------------\nSequence labeling\t\t         Similar to this work\t\n------------------------------------------------------------------------------------------------------------------------\n\n\u3010Footnotes\u3011\n\"Accuracy*\" :  whether generated words appeared in the gold reference\n\u201cFunc\u201d can be used to compute sentence length difference.\n\u201cSentence familiarity\u201d: we could quantify the degree to which the test sentence has been seen in the training set (based on n-gram calculation).\n\n\u3010References\u3011\n[1] Effective Approaches to Attention-based Neural Machine Translation, Minh-Thang Luong Hieu Pham\nChristopher D. Manning\n[2] Von misesfisher loss for training sequence to sequence, Sachin Kumar and Yulia Tsvetkov. \n[3] Compare-mt: A Tool for Holistic Comparison of Language Generation Systems, Graham Neubig,\nZi-Yi Dou, Junjie Hu, Paul Michel, Danish Pruthi, Xinyi Wang, John Wieting\n[4] Text Summarization with Pretrained Encoders, Yang Liu,\u00a0 Mirella Lapata\n[5] Earlier Isn\u2019t Always Better: Sub-aspect Analysis on Corpus and System Biases in Summarization,\nTaehee Jung, Dongyeop Kang, Lucas Mentch, Eduard Hovy\n[6] A Closer Look at Data Bias in Neural Extractive Summarization Models, Ming Zhong, Danqing Wang,\nPengfei Liu, Xipeng Qiu, Xuanjing Huang\n[7] Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks, Kai Sheng\nTai, Richard Socher, Christopher D. Manning\n[8] Bidirectional Attention Flow for Machine Comprehension, Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, Hannaneh Hajishirzi\n[9] A Thorough Examination of the CNN/Daily Mail Reading Comprehension Task, Danqi Chen, Jason Bolton, Christopher D. Manning\n[10] Learning Semantic Representations of Users and Products for Document Level Sentiment Classification, Duyu Tang, Bing Qin, Ting Liu\n[11] The Relationship of Word Length and Sentence Length: The Inter-Textual Perspective, Peter Grzybek, Ernst\nStadlober, Emmerich Kelih\n[12] TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension, Mandar Joshi, Eunsol Choi, Daniel S. Weld, Luke Zettlemoyer"}, "signatures": ["ICLR.cc/2020/Conference/Paper2116/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2116/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["fujl16@fudan.edu.cn", "pfliu14@fudan.edu.cn", "xjhuang@fudan.edu.cn"], "title": "Towards Interpretable Evaluations: A Case Study of Named Entity Recognition", "authors": ["Jinlan Fu", "Pengfei Liu", "Xuanjing Huang"], "pdf": "/pdf/657da0da10c50375c849beb311118aecc12cf382.pdf", "TL;DR": "We propose a generalized evaluation methodology to interpret model biases, dataset biases, and their correlation.", "abstract": "    With the proliferation of models for natural language processing (NLP) tasks, it is even harder to understand the differences between models and their relative merits. Simply looking at differences between holistic metrics such as accuracy, BLEU, or F1 do not tell us \\emph{why} or \\emph{how} a particular method is better and how dataset biases influence the choices of model design.\n    In this paper, we present a general methodology for {\\emph{interpretable}} evaluation of NLP systems and choose the task of named entity recognition (NER) as a case study, which is a core task of identifying people, places, or organizations in text. The proposed evaluation method enables us to interpret the \\textit{model biases}, \\textit{dataset biases}, and how the \\emph{differences in the datasets} affect the design of the models, identifying the strengths and weaknesses of current approaches. By making our {analysis} tool available, we make it easy for future researchers to run similar analyses and drive the progress in this area.", "keywords": ["interpretable evaluation", "dataset biases", "model biases", "NER"], "paperhash": "fu|towards_interpretable_evaluations_a_case_study_of_named_entity_recognition", "original_pdf": "/attachment/8c3cd5119ed242865846d17f2be72125c10a8de1.pdf", "_bibtex": "@misc{\nfu2020towards,\ntitle={Towards Interpretable Evaluations: A Case Study of Named Entity Recognition},\nauthor={Jinlan Fu and Pengfei Liu and Xuanjing Huang},\nyear={2020},\nurl={https://openreview.net/forum?id=HJxTgeBtDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJxTgeBtDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2116/Authors", "ICLR.cc/2020/Conference/Paper2116/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2116/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2116/Reviewers", "ICLR.cc/2020/Conference/Paper2116/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2116/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2116/Authors|ICLR.cc/2020/Conference/Paper2116/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504146094, "tmdate": 1576860536137, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2116/Authors", "ICLR.cc/2020/Conference/Paper2116/Reviewers", "ICLR.cc/2020/Conference/Paper2116/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2116/-/Official_Comment"}}}, {"id": "H1xyV2ljsH", "original": null, "number": 2, "cdate": 1573747751023, "ddate": null, "tcdate": 1573747751023, "tmdate": 1573747751023, "tddate": null, "forum": "HJxTgeBtDr", "replyto": "HJxTgeBtDr", "invitation": "ICLR.cc/2020/Conference/Paper2116/-/Official_Comment", "content": {"title": "Updated version of the paper (Version 1.0)", "comment": "We thank all reviewers for their comments. They are extremely insightful and help us to make our paper better. We have been refining our paper based on their suggestions, and a new version is uploaded.\nBelow is a summary of the major changes:\n1) We re-organize Section2,3 in the last version and merge them as \"Preliminaries\" Section to summarize the properties of evaluation methods (of related work) and describe the NER task and its current evaluation strategy. We remove some redundant description. (To address R2 and R3's concern)\n2) We give more explanation of the \"supplementary exam\" to address R1's concern.\n3) We refine Section3.2 to make the description of Fig.2 and Tab.2 more clear.\n4) We make the introduction of R-Bucket more clear and give a concrete example.  Additionally, we have removed the last paragraph in Section4.2 (now it is Section3.2) (To address R3's concern)\n5) We add an intuitive explanation for each measure defined in Section3.3. (To address R1's concern)\n6) We add a more detailed explanation for the names of 11models and give an example. (To address R1's concern)\n7) We refine our introduction section, providing detailed examples to show how to adapt to the proposed methodology to other types of NLP tasks."}, "signatures": ["ICLR.cc/2020/Conference/Paper2116/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2116/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["fujl16@fudan.edu.cn", "pfliu14@fudan.edu.cn", "xjhuang@fudan.edu.cn"], "title": "Towards Interpretable Evaluations: A Case Study of Named Entity Recognition", "authors": ["Jinlan Fu", "Pengfei Liu", "Xuanjing Huang"], "pdf": "/pdf/657da0da10c50375c849beb311118aecc12cf382.pdf", "TL;DR": "We propose a generalized evaluation methodology to interpret model biases, dataset biases, and their correlation.", "abstract": "    With the proliferation of models for natural language processing (NLP) tasks, it is even harder to understand the differences between models and their relative merits. Simply looking at differences between holistic metrics such as accuracy, BLEU, or F1 do not tell us \\emph{why} or \\emph{how} a particular method is better and how dataset biases influence the choices of model design.\n    In this paper, we present a general methodology for {\\emph{interpretable}} evaluation of NLP systems and choose the task of named entity recognition (NER) as a case study, which is a core task of identifying people, places, or organizations in text. The proposed evaluation method enables us to interpret the \\textit{model biases}, \\textit{dataset biases}, and how the \\emph{differences in the datasets} affect the design of the models, identifying the strengths and weaknesses of current approaches. By making our {analysis} tool available, we make it easy for future researchers to run similar analyses and drive the progress in this area.", "keywords": ["interpretable evaluation", "dataset biases", "model biases", "NER"], "paperhash": "fu|towards_interpretable_evaluations_a_case_study_of_named_entity_recognition", "original_pdf": "/attachment/8c3cd5119ed242865846d17f2be72125c10a8de1.pdf", "_bibtex": "@misc{\nfu2020towards,\ntitle={Towards Interpretable Evaluations: A Case Study of Named Entity Recognition},\nauthor={Jinlan Fu and Pengfei Liu and Xuanjing Huang},\nyear={2020},\nurl={https://openreview.net/forum?id=HJxTgeBtDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJxTgeBtDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2116/Authors", "ICLR.cc/2020/Conference/Paper2116/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2116/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2116/Reviewers", "ICLR.cc/2020/Conference/Paper2116/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2116/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2116/Authors|ICLR.cc/2020/Conference/Paper2116/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504146094, "tmdate": 1576860536137, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2116/Authors", "ICLR.cc/2020/Conference/Paper2116/Reviewers", "ICLR.cc/2020/Conference/Paper2116/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2116/-/Official_Comment"}}}, {"id": "rkxmuiLnYH", "original": null, "number": 1, "cdate": 1571740523133, "ddate": null, "tcdate": 1571740523133, "tmdate": 1572972381037, "tddate": null, "forum": "HJxTgeBtDr", "replyto": "HJxTgeBtDr", "invitation": "ICLR.cc/2020/Conference/Paper2116/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "\nThe manuscript proposes an evaluation methodology to obtain deeper insights regarding the strength and weaknesses of different methods on different datasets. The method considers a set of methods addressing the task of Named Entity Recognition (NER) as case study. In addition, it proposes a set of attribute-based criteria, i.e. bucketization strategies, under which the dataset can be divided and analyzed in order to highlight different properties of the evaluated methods.\n\nAs said earlier, the manuscript proposes an evaluation methodology to obtain deeper insights regarding the strength and weaknesses of different methods on different datasets. The characteristic of being able to provided deeper insights on strength/weaknesses and relevant factors on the inner-workings of a given method is \nsomething very desirable for every evaluation. As such, in my opinion, the \"interpretable\" tag associate to the proposed method is somewhat out of place. Having said that, I would recommend removing the \"interpretable\" tag and stress the contribution of this manuscript as an evaluation protocol. \n\nIn Section 4.2, for the R-Bucket strategy it is stated as having the requirement of discrete and finite attributes. Based on the equations of the other two strategies (R-bucket and F-bucket), it seems that they also have the requirement of having discrete attributes. Is this indeed the case? if so, it should be explicitly indicated. \nHaving said that, this raises another question: Is this protocol exclusive to tasks/problems with explicit discrete attributes?\n\nThe goal of this manuscript is to propose a general evaluation protocol for NLP tasks.\nHowever, it seems to be somewhat tailored to the NER task. My question is: How well the proposed method generalizes to other NLP tasks without attributes? Similarly, how well the proposed bucketization strategies generalize beyond the NER task? Perhaps the generalization characteristics and limitations of the proposed evaluation methodology should be explicitly discussed in the manuscript.\n\nLast paragraph of Section 4.2 summarizes ideas that were just presented. It feels somewhat redundant. I suggest removing in in favor of extending the existing discussions and analysis.\n\nI may consider upgrading my initial rating based on on the feedback given to my questions/doubts.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2116/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2116/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["fujl16@fudan.edu.cn", "pfliu14@fudan.edu.cn", "xjhuang@fudan.edu.cn"], "title": "Towards Interpretable Evaluations: A Case Study of Named Entity Recognition", "authors": ["Jinlan Fu", "Pengfei Liu", "Xuanjing Huang"], "pdf": "/pdf/657da0da10c50375c849beb311118aecc12cf382.pdf", "TL;DR": "We propose a generalized evaluation methodology to interpret model biases, dataset biases, and their correlation.", "abstract": "    With the proliferation of models for natural language processing (NLP) tasks, it is even harder to understand the differences between models and their relative merits. Simply looking at differences between holistic metrics such as accuracy, BLEU, or F1 do not tell us \\emph{why} or \\emph{how} a particular method is better and how dataset biases influence the choices of model design.\n    In this paper, we present a general methodology for {\\emph{interpretable}} evaluation of NLP systems and choose the task of named entity recognition (NER) as a case study, which is a core task of identifying people, places, or organizations in text. The proposed evaluation method enables us to interpret the \\textit{model biases}, \\textit{dataset biases}, and how the \\emph{differences in the datasets} affect the design of the models, identifying the strengths and weaknesses of current approaches. By making our {analysis} tool available, we make it easy for future researchers to run similar analyses and drive the progress in this area.", "keywords": ["interpretable evaluation", "dataset biases", "model biases", "NER"], "paperhash": "fu|towards_interpretable_evaluations_a_case_study_of_named_entity_recognition", "original_pdf": "/attachment/8c3cd5119ed242865846d17f2be72125c10a8de1.pdf", "_bibtex": "@misc{\nfu2020towards,\ntitle={Towards Interpretable Evaluations: A Case Study of Named Entity Recognition},\nauthor={Jinlan Fu and Pengfei Liu and Xuanjing Huang},\nyear={2020},\nurl={https://openreview.net/forum?id=HJxTgeBtDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJxTgeBtDr", "replyto": "HJxTgeBtDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2116/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2116/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575392523226, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2116/Reviewers"], "noninvitees": [], "tcdate": 1570237727482, "tmdate": 1575392523242, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2116/-/Official_Review"}}}, {"id": "Skgm6QnaYr", "original": null, "number": 2, "cdate": 1571828666690, "ddate": null, "tcdate": 1571828666690, "tmdate": 1572972381001, "tddate": null, "forum": "HJxTgeBtDr", "replyto": "HJxTgeBtDr", "invitation": "ICLR.cc/2020/Conference/Paper2116/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "TOWARDS INTERPRETABLE EVALUATIONS A CASE STUDY OF NAMED ENTITY RECOGNITION\n\n\n\nThe authors propose an evaluation methodology to study the relations between datasets and machine learning models. This methodology introduces the notion of attributes which describes different aspects of the samples and buckets which group samples according to the attributes. The goal is to give a better understanding of the strengths and weaknesses of an algorithm on a specific dataset according to the attributes, as shown on Fig4.\n\nThe article is very dense and the author chose to present the method from an abstract and generic point of view which makes the reading of the article difficult. In the end, the proposition is a formalisation of the simple error analysis which is commonly done when trying to improve a machine learning system. The advantage of the method could be to introduce some metrics to make the error analysis more automatic. These metrics are given in section 4 but here again only from a formal point of view : it is very difficult for the reader to understand how to interpret them and how to use them for a practical case. \n\nThe paper is 17 pages long with the annex : it would better fit a journal publication or the author should select some of the main results to present them in a conference paper. The aspects of the paper related to learning relations is no put forward enough. \n\n2. Related work\n\n2.1 :\n -supplementary exam : unclear\n\n2.2 : \n- methodological perspective : a bit a repetition of introduction\n- task perspective : not very clear, is the main message  \"it important to understand what in the dataset make the model work ?\"\n\n3 Task\n\nSection is too small to be a level 1 title\n\n4 Attributes\n\nfigure 2 : where are the links to levels ?\n\n4.2 :\n\n  * familiarity : test/train distribution should be the same. Fk computer on train set because it is bigger ? it allows to study the impact of the number of occurrences in the training set. Is it more interesting than a learning curve ?\n\n  * multi attribute familiarity : risk of metric explosion ? how to select the attributes ?\n\n  * eq 3 : spearman not defined\n\n* 4.3\n\n  * metric are defined by formula but it is difficult to understand what is the rationale behind each of them and therefore figure out how to interpret them\n\n  * \"Usually where a, b represent two different models and usually model a has a higher performance (by dataset-level metric)\" : unclear\n\n5 Experimental setting\n\nTable3 : \nthe encoding of the model name is not clear\na metric on all the dataset for each model could be computed to decide which one is the best overall\nhow did you choose the tested combinaisons ?\n\n6\\.2\n\nanalysis of Fig 4 : R-eLen does not existe (R-Ele). what is eta ?\n\ntable 4 : spearman\\**r*\\* ?\n\n6\\.4\n\n* CRF vs MLP : \"... a major factor for the choices of CRF and MLP: **if** a dataset with higher \u03b6MF\u2212et, in which longer entities can benefit more from CRF-based models.\" > missing words ?\n\nWriting :\n\n* \"Concretely\" isn't very natural at the beginning of sentences, same thing with \"Formally\", 'Intuitively' \u2026\n\n* in 4.1 : \"We refer to E, P, K as the sets of entities (i.e. New York), entity attributes (i.e. entity length) and attributes values (i.e. 2).\" => \"We refer to the sets of entities (i.e. New York) as E, entity attributes (i.e. entity length) as P and attributes values (i.e. 2) as K\" would be better\n\n* same thing in 4.3 \"we refer to M = m1,\u00b7\u00b7\u00b7 ,m|M| as a set of **models** and P = p1,\u00b7\u00b7\u00b7 ,p|P| as a set of **attributes**\" doesn't really work, \"M = m1,\u00b7\u00b7\u00b7 ,m|M| is a set of **models** and P = p1,\u00b7\u00b7\u00b7 ,p|P| is a set of **attributes**\" maybe\n\n* in 4.2 page 5 : \"the familiarity Fk (p1 , p2 ) is a measure with intriguing explanation \u2026\" : not clear\n\n* 6.3 (3) \"Only using character-level CNN is apt to overfit the feature of capital letters.\" **apt** doesnt work here"}, "signatures": ["ICLR.cc/2020/Conference/Paper2116/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2116/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["fujl16@fudan.edu.cn", "pfliu14@fudan.edu.cn", "xjhuang@fudan.edu.cn"], "title": "Towards Interpretable Evaluations: A Case Study of Named Entity Recognition", "authors": ["Jinlan Fu", "Pengfei Liu", "Xuanjing Huang"], "pdf": "/pdf/657da0da10c50375c849beb311118aecc12cf382.pdf", "TL;DR": "We propose a generalized evaluation methodology to interpret model biases, dataset biases, and their correlation.", "abstract": "    With the proliferation of models for natural language processing (NLP) tasks, it is even harder to understand the differences between models and their relative merits. Simply looking at differences between holistic metrics such as accuracy, BLEU, or F1 do not tell us \\emph{why} or \\emph{how} a particular method is better and how dataset biases influence the choices of model design.\n    In this paper, we present a general methodology for {\\emph{interpretable}} evaluation of NLP systems and choose the task of named entity recognition (NER) as a case study, which is a core task of identifying people, places, or organizations in text. The proposed evaluation method enables us to interpret the \\textit{model biases}, \\textit{dataset biases}, and how the \\emph{differences in the datasets} affect the design of the models, identifying the strengths and weaknesses of current approaches. By making our {analysis} tool available, we make it easy for future researchers to run similar analyses and drive the progress in this area.", "keywords": ["interpretable evaluation", "dataset biases", "model biases", "NER"], "paperhash": "fu|towards_interpretable_evaluations_a_case_study_of_named_entity_recognition", "original_pdf": "/attachment/8c3cd5119ed242865846d17f2be72125c10a8de1.pdf", "_bibtex": "@misc{\nfu2020towards,\ntitle={Towards Interpretable Evaluations: A Case Study of Named Entity Recognition},\nauthor={Jinlan Fu and Pengfei Liu and Xuanjing Huang},\nyear={2020},\nurl={https://openreview.net/forum?id=HJxTgeBtDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJxTgeBtDr", "replyto": "HJxTgeBtDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2116/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2116/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575392523226, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2116/Reviewers"], "noninvitees": [], "tcdate": 1570237727482, "tmdate": 1575392523242, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2116/-/Official_Review"}}}, {"id": "ByljmRZSqH", "original": null, "number": 3, "cdate": 1572310563340, "ddate": null, "tcdate": 1572310563340, "tmdate": 1572972380953, "tddate": null, "forum": "HJxTgeBtDr", "replyto": "HJxTgeBtDr", "invitation": "ICLR.cc/2020/Conference/Paper2116/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper discusses a methodology to interpret models and model outputs for Named Entity Recognition (NER) based on assigned attributes. The key idea is to bucketize the test data based on characteristics of attributes and then comment on effect of the attribute on the model, the task itself or the dataset bias. \n\nThe empirical evaluation is impressive. The authors have constructed a series of experiments to make their case. The paper is well-written and easy to understand, albeit some of the related work seems a little unrelated to the task at hand. While the authors have tried to state that the method is \"general\" and goes beyond NER, I am not sure if that is the case. The creation of attribute buckets is vital for any further analysis, its not clear how the method can be adapted to more general settings unless such attributes and buckets can be created easily (e.g. using domain knowledge). Furthermore, there is only one problem setting considered (i.e. NER), and for the paper is make claim to more general settings, I would expect evaluations on atleast one more problem setting. I would suggest the authors modify the claims accordingly. This is not to diminish from their contributions in the NER. \n\nThe bucketization idea is not something out of the park novel. It is probably something already being used in practice. However, delineating the procedure and suggesting quantifiable statistics and designing experiments to illustrate how these can be used to draw qualitative conclusions is something that is very  interesting and useful to the community as a whole. The strongest part of this paper is the empirical evaluation that allows drawing interesting conclusions, and suggests a methodology to reach that conclusion. While some of the claims made (e.g. regarding dataset biases) probably require further and deeper analysis, this is a good first step that should foster further research and discussion. "}, "signatures": ["ICLR.cc/2020/Conference/Paper2116/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2116/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["fujl16@fudan.edu.cn", "pfliu14@fudan.edu.cn", "xjhuang@fudan.edu.cn"], "title": "Towards Interpretable Evaluations: A Case Study of Named Entity Recognition", "authors": ["Jinlan Fu", "Pengfei Liu", "Xuanjing Huang"], "pdf": "/pdf/657da0da10c50375c849beb311118aecc12cf382.pdf", "TL;DR": "We propose a generalized evaluation methodology to interpret model biases, dataset biases, and their correlation.", "abstract": "    With the proliferation of models for natural language processing (NLP) tasks, it is even harder to understand the differences between models and their relative merits. Simply looking at differences between holistic metrics such as accuracy, BLEU, or F1 do not tell us \\emph{why} or \\emph{how} a particular method is better and how dataset biases influence the choices of model design.\n    In this paper, we present a general methodology for {\\emph{interpretable}} evaluation of NLP systems and choose the task of named entity recognition (NER) as a case study, which is a core task of identifying people, places, or organizations in text. The proposed evaluation method enables us to interpret the \\textit{model biases}, \\textit{dataset biases}, and how the \\emph{differences in the datasets} affect the design of the models, identifying the strengths and weaknesses of current approaches. By making our {analysis} tool available, we make it easy for future researchers to run similar analyses and drive the progress in this area.", "keywords": ["interpretable evaluation", "dataset biases", "model biases", "NER"], "paperhash": "fu|towards_interpretable_evaluations_a_case_study_of_named_entity_recognition", "original_pdf": "/attachment/8c3cd5119ed242865846d17f2be72125c10a8de1.pdf", "_bibtex": "@misc{\nfu2020towards,\ntitle={Towards Interpretable Evaluations: A Case Study of Named Entity Recognition},\nauthor={Jinlan Fu and Pengfei Liu and Xuanjing Huang},\nyear={2020},\nurl={https://openreview.net/forum?id=HJxTgeBtDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJxTgeBtDr", "replyto": "HJxTgeBtDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2116/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2116/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575392523226, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2116/Reviewers"], "noninvitees": [], "tcdate": 1570237727482, "tmdate": 1575392523242, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2116/-/Official_Review"}}}], "count": 12}