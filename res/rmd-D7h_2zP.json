{"notes": [{"id": "rmd-D7h_2zP", "original": "RWeRfHWmwhH", "number": 1611, "cdate": 1601308178574, "ddate": null, "tcdate": 1601308178574, "tmdate": 1614985767170, "tddate": null, "forum": "rmd-D7h_2zP", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Domain-slot Relationship Modeling using  a Pre-trained Language Encoder for Multi-Domain Dialogue State Tracking", "authorids": ["~Jinwon_An1", "~Misuk_Kim1", "~Sungzoon_Cho1", "hjbang21pp@etri.re.kr"], "authors": ["Jinwon An", "Misuk Kim", "Sungzoon Cho", "Junseong Bang"], "keywords": ["Dialogue State Tracking", "Domain-slot Relationship Modeling", "Pre-trained Language Encoder"], "abstract": "Dialogue state tracking for multi-domain dialogues is challenging because the model should be able to track dialogue states across multiple domains and slots. Past studies had its limitations in that they did not factor in the relationship among different domain-slot pairs. Although recent approaches did support relationship modeling among the domain-slot pairs, they did not leverage a pre-trained language model, which has improved the performance of numerous natural language tasks, in the encoding process. Our approach fills the gap between these previous studies. We propose a model for multi-domain dialogue state tracking that effectively models the relationship among domain-slot pairs using a pre-trained language encoder. Inspired by the way the special $[CLS]$ token in BERT is used to aggregate the information of the whole sequence, we use multiple special tokens for each domain-slot pair that encodes information corresponding to its domain and slot. The special tokens are run together with the dialogue context through the pre-trained language encoder, which effectively models the relationship among different domain-slot pairs. Our experimental results show that our model achieves state-of-the-art performance on the MultiWOZ-2.1 and MultiWOZ-2.2 dataset.", "one-sentence_summary": "We propose a model for multi-domain dialogue state tracking that effectively models the relationship among domain-slot pairs using a pre-trained language encoder.", "pdf": "/pdf/dbc08fab9f86385bde6ddee9efe0881fad8dff11.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "an|domainslot_relationship_modeling_using_a_pretrained_language_encoder_for_multidomain_dialogue_state_tracking", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=y4VuzDVdFf", "_bibtex": "@misc{\nan2021domainslot,\ntitle={Domain-slot Relationship Modeling using  a Pre-trained Language Encoder for Multi-Domain Dialogue State Tracking},\nauthor={Jinwon An and Misuk Kim and Sungzoon Cho and Junseong Bang},\nyear={2021},\nurl={https://openreview.net/forum?id=rmd-D7h_2zP}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "Al1aSkgJgo", "original": null, "number": 1, "cdate": 1612163259806, "ddate": null, "tcdate": 1612163259806, "tmdate": 1612163259806, "tddate": null, "forum": "rmd-D7h_2zP", "replyto": "rmd-D7h_2zP", "invitation": "ICLR.cc/2021/Conference/Paper1611/-/Comment", "content": {"title": "withdrawal of paper", "comment": "We withdraw the current version of our work. We found issues with the evaluation of the dataset with large models. Thus, the current version of our paper is invalid."}, "signatures": ["~Jinwon_An1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "~Jinwon_An1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Domain-slot Relationship Modeling using  a Pre-trained Language Encoder for Multi-Domain Dialogue State Tracking", "authorids": ["~Jinwon_An1", "~Misuk_Kim1", "~Sungzoon_Cho1", "hjbang21pp@etri.re.kr"], "authors": ["Jinwon An", "Misuk Kim", "Sungzoon Cho", "Junseong Bang"], "keywords": ["Dialogue State Tracking", "Domain-slot Relationship Modeling", "Pre-trained Language Encoder"], "abstract": "Dialogue state tracking for multi-domain dialogues is challenging because the model should be able to track dialogue states across multiple domains and slots. Past studies had its limitations in that they did not factor in the relationship among different domain-slot pairs. Although recent approaches did support relationship modeling among the domain-slot pairs, they did not leverage a pre-trained language model, which has improved the performance of numerous natural language tasks, in the encoding process. Our approach fills the gap between these previous studies. We propose a model for multi-domain dialogue state tracking that effectively models the relationship among domain-slot pairs using a pre-trained language encoder. Inspired by the way the special $[CLS]$ token in BERT is used to aggregate the information of the whole sequence, we use multiple special tokens for each domain-slot pair that encodes information corresponding to its domain and slot. The special tokens are run together with the dialogue context through the pre-trained language encoder, which effectively models the relationship among different domain-slot pairs. Our experimental results show that our model achieves state-of-the-art performance on the MultiWOZ-2.1 and MultiWOZ-2.2 dataset.", "one-sentence_summary": "We propose a model for multi-domain dialogue state tracking that effectively models the relationship among domain-slot pairs using a pre-trained language encoder.", "pdf": "/pdf/dbc08fab9f86385bde6ddee9efe0881fad8dff11.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "an|domainslot_relationship_modeling_using_a_pretrained_language_encoder_for_multidomain_dialogue_state_tracking", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=y4VuzDVdFf", "_bibtex": "@misc{\nan2021domainslot,\ntitle={Domain-slot Relationship Modeling using  a Pre-trained Language Encoder for Multi-Domain Dialogue State Tracking},\nauthor={Jinwon An and Misuk Kim and Sungzoon Cho and Junseong Bang},\nyear={2021},\nurl={https://openreview.net/forum?id=rmd-D7h_2zP}\n}"}, "tags": [], "invitation": {"reply": {"forum": "rmd-D7h_2zP", "readers": {"values": ["everyone"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2021/Conference/Paper1611/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1611/Authors|ICLR.cc/2021/Conference/Paper1611/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs"}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}}, "multiReply": true, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["everyone"], "tcdate": 1610649444145, "tmdate": 1610649444145, "id": "ICLR.cc/2021/Conference/Paper1611/-/Comment"}}}, {"id": "AZuvyQm-7Q", "original": null, "number": 1, "cdate": 1610040366025, "ddate": null, "tcdate": 1610040366025, "tmdate": 1610473956655, "tddate": null, "forum": "rmd-D7h_2zP", "replyto": "rmd-D7h_2zP", "invitation": "ICLR.cc/2021/Conference/Paper1611/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "The authors explore modeling the relationship between domain-slot pairs in multi-domain dialogue state tracking via use of special tokens in pre-trained contextualized word embeddings (i.e., one special token for each domain-slot pair or special tokens for the domain and the slot that are merged). Beyond this, the basic architecture is very similar to the TRADE architecture (and papers that build on this general slot-gate + slot-value classifier) for the fixed vocabulary setting. Experiments are conducted on the MultiWOZ 2.1/2.2 datasets, demonstrating impressive improvements over recent results.\n\n== Pros ==\n+ They demonstrate that domain-slot interdependencies can be modeled through special tokens for use with pre-trained embeddings.\n+ The top-line empirical results are impressive.\n\n== Cons ==\n- Lack of a deep dive on the empirical analysis to show precisely why/where the proposed method is working better than existing work.\n- The methodological advance is minimal beyond using better pre-trained embeddings.\n- Only one dataset when others exist and this is largely an empirical paper.\n- The writing is rushed and reads like a 'late-breaking' paper.\n\nEvaluating along the specified dimensions:\n* Quality: The quality of the work was the primary concern of the reviewers. Specifically, this reads like a 'late breaking' paper where the table of results is impressive, but there isn't significant examination of the empirical results showing why/when it works relative to competing methods. Focusing just on Tables 2 & 3, much of the improvement is ostensibly really due to the more powerful embeddings. Contextualizing this wrt {SimpleTOD, TRADE, DSTQA, Picklist}, this appears a minor methodological innovation centered around the input embeddings. The empirical results are impressive, but may very well be a result of the more powerful pre-trained embeddings -- additional empirical analysis and discussion might be able to convince the reader otherwise, but is lacking here.\n* Clarity: This is a very simple idea, so it should be easily understood by most familiar with the research area. That being said, the paper seems very rushed in general.\n* Originality: This applies ideas used in many NLP applications to the dialogue-state tracking problem. As previously stated, the architecture is similar to several existing DST formulations -- where the core idea is to model slot-value interdependencies through the contextualized embeddings using special token. While not a trivial idea, it also is something that many could/would have put together. Until it is abundantly clear that this isn't really a study of how to apply larger pre-trained embeddings to DST problems, it isn't clear that this is a significant dialogue systems advance beyond the strong performance.\n* Significance: As stated, this isn't a significant methodological advance. However, the empirical results appear very impressive -- although the reviewers expressed some concerns regarding the evaluation. Since this is largely empirical, one of the reviewers pointed out that additional relevant datasets now exist, which would significantly strengthen the case.\n\nIn summary, the empirical results appear impressive, ostensibly setting the SoTA. However, there were several concerns regarding the novelty of the approach, if it is actually working better due to the reasons stated, sufficient analysis of the empirical results, amongst other things. Thus, despite the impressive results, the consensus evaluation was that this work is not ready for publication in its current form (even if the top-line results should be disseminated).\n"}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Domain-slot Relationship Modeling using  a Pre-trained Language Encoder for Multi-Domain Dialogue State Tracking", "authorids": ["~Jinwon_An1", "~Misuk_Kim1", "~Sungzoon_Cho1", "hjbang21pp@etri.re.kr"], "authors": ["Jinwon An", "Misuk Kim", "Sungzoon Cho", "Junseong Bang"], "keywords": ["Dialogue State Tracking", "Domain-slot Relationship Modeling", "Pre-trained Language Encoder"], "abstract": "Dialogue state tracking for multi-domain dialogues is challenging because the model should be able to track dialogue states across multiple domains and slots. Past studies had its limitations in that they did not factor in the relationship among different domain-slot pairs. Although recent approaches did support relationship modeling among the domain-slot pairs, they did not leverage a pre-trained language model, which has improved the performance of numerous natural language tasks, in the encoding process. Our approach fills the gap between these previous studies. We propose a model for multi-domain dialogue state tracking that effectively models the relationship among domain-slot pairs using a pre-trained language encoder. Inspired by the way the special $[CLS]$ token in BERT is used to aggregate the information of the whole sequence, we use multiple special tokens for each domain-slot pair that encodes information corresponding to its domain and slot. The special tokens are run together with the dialogue context through the pre-trained language encoder, which effectively models the relationship among different domain-slot pairs. Our experimental results show that our model achieves state-of-the-art performance on the MultiWOZ-2.1 and MultiWOZ-2.2 dataset.", "one-sentence_summary": "We propose a model for multi-domain dialogue state tracking that effectively models the relationship among domain-slot pairs using a pre-trained language encoder.", "pdf": "/pdf/dbc08fab9f86385bde6ddee9efe0881fad8dff11.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "an|domainslot_relationship_modeling_using_a_pretrained_language_encoder_for_multidomain_dialogue_state_tracking", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=y4VuzDVdFf", "_bibtex": "@misc{\nan2021domainslot,\ntitle={Domain-slot Relationship Modeling using  a Pre-trained Language Encoder for Multi-Domain Dialogue State Tracking},\nauthor={Jinwon An and Misuk Kim and Sungzoon Cho and Junseong Bang},\nyear={2021},\nurl={https://openreview.net/forum?id=rmd-D7h_2zP}\n}"}, "tags": [], "invitation": {"reply": {"forum": "rmd-D7h_2zP", "replyto": "rmd-D7h_2zP", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040366011, "tmdate": 1610473956634, "id": "ICLR.cc/2021/Conference/Paper1611/-/Decision"}}}, {"id": "l2wbpyOZt7H", "original": null, "number": 3, "cdate": 1603932956237, "ddate": null, "tcdate": 1603932956237, "tmdate": 1607121495554, "tddate": null, "forum": "rmd-D7h_2zP", "replyto": "rmd-D7h_2zP", "invitation": "ICLR.cc/2021/Conference/Paper1611/-/Official_Review", "content": {"title": "limited analysis ", "review": "Summary:\n\nThis paper proposed a new approach for modeling multi-domain dialogue state tracking by incorporating domain-slot relationship using a pre-trained language encoder. The proposed approach are based on using special tokens to mode l such relationship. Two kinds of special tokens are proposed to represent domain-slot pair, DS_merge token for each specific pair, and tokens for every domain and slots separately \n\n\n\nComments:\n\n1- what is the role of segment embedding (Fig3) for DST tank? Are two different segment used in the pretraining of model?\n\n2- Table 1:\n                  2-1 what type of label cleaning is used to compute joint goal accuracy? From SimpleTOD paper, each baseline has used different label cleaning\n                  2-2: DS-split is bolded, while it is lower than SimpleTOD\n\n3- Table 2: the results indicate that DST performance drops in total by blocking attention across different DS tokens. However, it is not clear how much of the performance drop belongs to turns with cross-domain related slots. The figure 4 only present one example of this case, which might not be correct for all wrong predictions. Also, it is helpful to report DST for single domain and to evaluate the importance of proposed approach. \n\n4- Since the proposed approach can be used on any pretrained encoder, the evaluation on BERT and/or Roberta is helpful to understand the robustness of approach to the choice of pretrained encoder.\n\n\n\n--------------------------------------------------------------\nPost rebuttal: \n\nTable 3:\nthe results indicates that only Albert-xxlarge achieve very high performance (222M). however, comparable models to other approaches, such as Roberta-base or albert-xlarge achieved around ~57% performance which is within margin of previous arts. for example, SimpleTOD with gpt2-base (124M) achieved 55.7% and ConvBert achieved 58%.\nTherefor, it is unclear why Albert-xxlarge get so much higher performance compared to other encoders, since same tokenization and domain-slot relation is used. \nBased on results in Table 3, there is inconsistency in which domain-slot relation does not always results in better performance,  and it depends on the choice of encoder too. \n\nOverall, the proposed architecture is very similar to TRADE model, in terms of using an encoder for dialogue history, slot-gate and slot-value classifier. The only difference is in using a much powerful pretrained encoder. ", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper1611/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1611/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Domain-slot Relationship Modeling using  a Pre-trained Language Encoder for Multi-Domain Dialogue State Tracking", "authorids": ["~Jinwon_An1", "~Misuk_Kim1", "~Sungzoon_Cho1", "hjbang21pp@etri.re.kr"], "authors": ["Jinwon An", "Misuk Kim", "Sungzoon Cho", "Junseong Bang"], "keywords": ["Dialogue State Tracking", "Domain-slot Relationship Modeling", "Pre-trained Language Encoder"], "abstract": "Dialogue state tracking for multi-domain dialogues is challenging because the model should be able to track dialogue states across multiple domains and slots. Past studies had its limitations in that they did not factor in the relationship among different domain-slot pairs. Although recent approaches did support relationship modeling among the domain-slot pairs, they did not leverage a pre-trained language model, which has improved the performance of numerous natural language tasks, in the encoding process. Our approach fills the gap between these previous studies. We propose a model for multi-domain dialogue state tracking that effectively models the relationship among domain-slot pairs using a pre-trained language encoder. Inspired by the way the special $[CLS]$ token in BERT is used to aggregate the information of the whole sequence, we use multiple special tokens for each domain-slot pair that encodes information corresponding to its domain and slot. The special tokens are run together with the dialogue context through the pre-trained language encoder, which effectively models the relationship among different domain-slot pairs. Our experimental results show that our model achieves state-of-the-art performance on the MultiWOZ-2.1 and MultiWOZ-2.2 dataset.", "one-sentence_summary": "We propose a model for multi-domain dialogue state tracking that effectively models the relationship among domain-slot pairs using a pre-trained language encoder.", "pdf": "/pdf/dbc08fab9f86385bde6ddee9efe0881fad8dff11.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "an|domainslot_relationship_modeling_using_a_pretrained_language_encoder_for_multidomain_dialogue_state_tracking", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=y4VuzDVdFf", "_bibtex": "@misc{\nan2021domainslot,\ntitle={Domain-slot Relationship Modeling using  a Pre-trained Language Encoder for Multi-Domain Dialogue State Tracking},\nauthor={Jinwon An and Misuk Kim and Sungzoon Cho and Junseong Bang},\nyear={2021},\nurl={https://openreview.net/forum?id=rmd-D7h_2zP}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "rmd-D7h_2zP", "replyto": "rmd-D7h_2zP", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1611/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538114790, "tmdate": 1606915762918, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1611/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1611/-/Official_Review"}}}, {"id": "n0_EQ7h4Kkh", "original": null, "number": 2, "cdate": 1603786789772, "ddate": null, "tcdate": 1603786789772, "tmdate": 1606657393853, "tddate": null, "forum": "rmd-D7h_2zP", "replyto": "rmd-D7h_2zP", "invitation": "ICLR.cc/2021/Conference/Paper1611/-/Official_Review", "content": {"title": "Interesting approach using pre-trained representations to encode domain-slot pairs for DST", "review": "Summary:\nThis paper showcases how pre-training can help with Dialogue State Tracking. The authors explicitly model\nthe relationship between domain-slot pairs. With their encoding and using strong pre-trained initializations\nthey are able to improve the joint goal accuracy by almost 1.5 points which is impressive.\n\nReasons for score: \nThis is a very well written paper and will be a good resource for people working on the task of Dialogue State Tracking.\nThe authors show how they can model relationships between domain-slot pairs and how they can encode them effectively\nusing pre-trained representations.\nI am hoping that the authors can address some of the cons during the rebuttal period.\n\nPros:\n1. Good dialogue representation which helps with the task of state tracking\n2. Simple model consisting of encoders and 2 classifiers which are well explained.\n3. Clear ablation study showing the value of 1) pre-training and 2) modeling relationship between domain-slot values\n\n\nCons:\n1. This approach, like other popular approaches, suffers from the problem of having a fixed output vocabulary for slot values - hence limiting its scalability. While this cannot be addressed in this work, this is a drawback of this approach.\n2. Some of the design decisions are stated but not well explained\n- Only one pre-training method compared\n- Authors mention they drop \"dontcare\" from slot gating but don't show the affect with or without it.\n- Not much details on the setup and how it was trained.\n3. Not much qualitative analysis.\n\nPlease address and clarify the cons above \n\nTypos/Areas for improvement:\n1. Section 3.2 and 3.3 can be shortened a lot. I would suggest showing more analysis.\n- More examples of type of  mistakes fixed.\n- Which turn in the dialogue does the error decrease the most.\n- How much is the training time/ accuracy tradeoff\n2. Adding another layer to make DS-split work should be trivial, there is no reason to leave that to future work.\nCould you show how the results look with that?\n\n------\n\nUpdating score based on authors' response.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1611/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1611/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Domain-slot Relationship Modeling using  a Pre-trained Language Encoder for Multi-Domain Dialogue State Tracking", "authorids": ["~Jinwon_An1", "~Misuk_Kim1", "~Sungzoon_Cho1", "hjbang21pp@etri.re.kr"], "authors": ["Jinwon An", "Misuk Kim", "Sungzoon Cho", "Junseong Bang"], "keywords": ["Dialogue State Tracking", "Domain-slot Relationship Modeling", "Pre-trained Language Encoder"], "abstract": "Dialogue state tracking for multi-domain dialogues is challenging because the model should be able to track dialogue states across multiple domains and slots. Past studies had its limitations in that they did not factor in the relationship among different domain-slot pairs. Although recent approaches did support relationship modeling among the domain-slot pairs, they did not leverage a pre-trained language model, which has improved the performance of numerous natural language tasks, in the encoding process. Our approach fills the gap between these previous studies. We propose a model for multi-domain dialogue state tracking that effectively models the relationship among domain-slot pairs using a pre-trained language encoder. Inspired by the way the special $[CLS]$ token in BERT is used to aggregate the information of the whole sequence, we use multiple special tokens for each domain-slot pair that encodes information corresponding to its domain and slot. The special tokens are run together with the dialogue context through the pre-trained language encoder, which effectively models the relationship among different domain-slot pairs. Our experimental results show that our model achieves state-of-the-art performance on the MultiWOZ-2.1 and MultiWOZ-2.2 dataset.", "one-sentence_summary": "We propose a model for multi-domain dialogue state tracking that effectively models the relationship among domain-slot pairs using a pre-trained language encoder.", "pdf": "/pdf/dbc08fab9f86385bde6ddee9efe0881fad8dff11.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "an|domainslot_relationship_modeling_using_a_pretrained_language_encoder_for_multidomain_dialogue_state_tracking", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=y4VuzDVdFf", "_bibtex": "@misc{\nan2021domainslot,\ntitle={Domain-slot Relationship Modeling using  a Pre-trained Language Encoder for Multi-Domain Dialogue State Tracking},\nauthor={Jinwon An and Misuk Kim and Sungzoon Cho and Junseong Bang},\nyear={2021},\nurl={https://openreview.net/forum?id=rmd-D7h_2zP}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "rmd-D7h_2zP", "replyto": "rmd-D7h_2zP", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1611/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538114790, "tmdate": 1606915762918, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1611/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1611/-/Official_Review"}}}, {"id": "99MJfohQxiW", "original": null, "number": 5, "cdate": 1605269469422, "ddate": null, "tcdate": 1605269469422, "tmdate": 1605797607575, "tddate": null, "forum": "rmd-D7h_2zP", "replyto": "JGicoZ1Rsmx", "invitation": "ICLR.cc/2021/Conference/Paper1611/-/Official_Comment", "content": {"title": "Author Response to AnonReviewer2", "comment": "Thank you very much for your review of our paper. We initially updated our paper with an updated result using *xxlarge-v2* of ALBERT, which showed the state-of-the-art result. We are running additional experiments to respond to the reviewers comment, as soon as the experiments are done we will update the manuscript right away.\n\nNow, we will try to explain each point that you have mentioned.\n\n1. The idea of using domain-pairs as input to a large pre-trained model is not novel (Wu et al., 2019; Zhang et al., 2019; Lee et al., 2019), as also pointed out by the authors, but the authors do not explicitly clarify this in the methodology section, leading the reader to believe that the domain-pairs is their own contribution. Same for the slot-gate (Wu et al., 2019).\n\n$\\rightarrow$ Thank you for pointing out our unclear explanation. However, we think that our model is different from the models that you have mentioned in different ways. In all the models that you have mentioned (Wu et al., 2019; Zhang et al., 2019; Lee et al., 2019), relationship modeling among the different domain-slot pairs is not modeled. These models take input each individual domain-slot pair and predict the slot value for each domain-slot pair independently. Our model on the other hand, takes input all the domain-slot pairs which is represented as $\\{DS\\}$ tokens. As mentioned in section 2 related works, we believe that (Le et al., 2019) and (Chen et al., 2020)  are the two studies that directly models the relationship among different domain-slot pairs. However, our model differs from these models in that we used a pre-trained language encoder for predicting slot values.(Le et al., 2019) uses an independent Transformer based encoder. (Chen et al., 2020) uses a graph-attention network. We believe that by using the state-of-the-art pre-trained language encoder, along with the input representation we proposed using $\\{DS\\}$ tokens, enabled our model to achieve the state-of-the-art result in the MultiWOZ 2.1 dataset. For the slot-gate, we will mention that we did not invent the idea, but borrowed it from (Wu et al., 2019). Thank you for you suggestion to improve our article. We explicitly added this in section 3 Proposed method:\n\n> The adopted the concept of the slot-gate classifier from (Wu et al., 2019) and made adjustments to apply to our model.\n\n\n\n2. The authors claim to learn relations between slots, but the analysis section is very thin and it just shows an ablation by masking the attention between the slot. Two points: why not just removing the [CLS] token instead of removing the attention, and why just using on ALBERTA large. For instance, the authors said \"For this experiment, we used the ALBERT configuration of large-v2, for faster experimentation\" which is contradictory since large-v2 is the slowest to run I guess. Can the authors show this ablation for all the model size?\n\n$\\rightarrow$ Thank you for suggesting an ablation study to improve our manuscript. Would it be possible for you to make some clarifications on the ablation study? Does removing the $CLS$ token mean removing the $\\{DS\\}$ tokens and use the $CLS$ token only for the slot-value prediction? From our humble understanding, we think that removing the $\\{DS\\}$ tokens will make sure to prove that the model indeed need the $\\{DS\\}$ tokens to perform well. The experiment excluding $\\{DS\\}$ tokens is still running, but early results show that their performance is far inferior to the model witih $\\{DS\\}$ tokens (joint slot accuracy of 36% as of 6k steps of training). Please correct us if we misunderstood your ablation study suggestion.\n\nThe reason why we experimented on ALBERT *large-v2* was due to lack of computational power. We were not able to run the model with *xlarge-v2*, which is the main model that is shown in Table 1.,  before the submission time. Since *large-v2* is smaller than *xlarge-v2*, we only ran the model with it. Currently, we are running the *xxlarge-v2* version for the ablation study of disabling attention among $\\{DS\\}$ tokens.\n\nDue to limitations to computing power, we expect that we would be unable to run ablation experiments for all configurations, but we will try our best.\n\n3. Although, MWoZ is the current benchmark for DST in ToDs, there are also other datasets for this task that can be considered (e.g., Schema Guided Dialogue (SGD) (Rastogi et.al. 2019))\n\n$\\rightarrow$ We agree that additional analysis on more datasets will prove the generalizability of our model. We will try to add experiments to those dataset if possible. However, since the SGD dataset is bigger than MultiWOZ 2.1, we unfortunately expect that it would be very hard to train the model on the SGD dataset."}, "signatures": ["ICLR.cc/2021/Conference/Paper1611/Authors"], "readers": ["everyone", "ICLR.cc/2021/Conference/Paper1611/Reviewers"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1611/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Domain-slot Relationship Modeling using  a Pre-trained Language Encoder for Multi-Domain Dialogue State Tracking", "authorids": ["~Jinwon_An1", "~Misuk_Kim1", "~Sungzoon_Cho1", "hjbang21pp@etri.re.kr"], "authors": ["Jinwon An", "Misuk Kim", "Sungzoon Cho", "Junseong Bang"], "keywords": ["Dialogue State Tracking", "Domain-slot Relationship Modeling", "Pre-trained Language Encoder"], "abstract": "Dialogue state tracking for multi-domain dialogues is challenging because the model should be able to track dialogue states across multiple domains and slots. Past studies had its limitations in that they did not factor in the relationship among different domain-slot pairs. Although recent approaches did support relationship modeling among the domain-slot pairs, they did not leverage a pre-trained language model, which has improved the performance of numerous natural language tasks, in the encoding process. Our approach fills the gap between these previous studies. We propose a model for multi-domain dialogue state tracking that effectively models the relationship among domain-slot pairs using a pre-trained language encoder. Inspired by the way the special $[CLS]$ token in BERT is used to aggregate the information of the whole sequence, we use multiple special tokens for each domain-slot pair that encodes information corresponding to its domain and slot. The special tokens are run together with the dialogue context through the pre-trained language encoder, which effectively models the relationship among different domain-slot pairs. Our experimental results show that our model achieves state-of-the-art performance on the MultiWOZ-2.1 and MultiWOZ-2.2 dataset.", "one-sentence_summary": "We propose a model for multi-domain dialogue state tracking that effectively models the relationship among domain-slot pairs using a pre-trained language encoder.", "pdf": "/pdf/dbc08fab9f86385bde6ddee9efe0881fad8dff11.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "an|domainslot_relationship_modeling_using_a_pretrained_language_encoder_for_multidomain_dialogue_state_tracking", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=y4VuzDVdFf", "_bibtex": "@misc{\nan2021domainslot,\ntitle={Domain-slot Relationship Modeling using  a Pre-trained Language Encoder for Multi-Domain Dialogue State Tracking},\nauthor={Jinwon An and Misuk Kim and Sungzoon Cho and Junseong Bang},\nyear={2021},\nurl={https://openreview.net/forum?id=rmd-D7h_2zP}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "rmd-D7h_2zP", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1611/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1611/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1611/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1611/Authors|ICLR.cc/2021/Conference/Paper1611/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1611/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923857774, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1611/-/Official_Comment"}}}, {"id": "BfYdwdsEVx", "original": null, "number": 6, "cdate": 1605270122243, "ddate": null, "tcdate": 1605270122243, "tmdate": 1605797596417, "tddate": null, "forum": "rmd-D7h_2zP", "replyto": "JGicoZ1Rsmx", "invitation": "ICLR.cc/2021/Conference/Paper1611/-/Official_Comment", "content": {"title": "Author Response to AnonReviewer2 ", "comment": "Continued from previous comment.\n\n4. [Reason to Reject] The main contribution of this paper is very thin, adding the [CLS] token as input, and the main technical contribution is not well explored (missing an in-depth ablation).\n\n$\\rightarrow$ We believe that our contribution is adding $\\{DS\\}$ tokens (which work as domain-slot specific $CLS$ tokens) for the input representation and also using a pre-trained language encoder to encode the information using those  $\\{DS\\}$ tokens. We think that this idea is novel since as far as our knowledge, there has not been models running multiple $CLS$ tokens to represent different domain-slot pairs. Previous models that use pre-trained language encoders used only a single $CLS$ token by taking each single domain-slot pair information as the input and running the model multiple times. Our model can predict all domain-slot pair values in a single run, while modeling the dependencies among them. This results in a state-of-the-art result, which is achieved by using the *xxlarge-v2* version of ALBERT. \n\nHowever, we agree that our qualitative analysis is not comprehensive. We are running the ablation experiment that you have suggested, which we believe will prove that our proposed $\\{DS\\}$ does work very well. We will provide additional examples such as Fig 4.. Also, we will provide attention heat maps to infer how our model works.\n\n5. [Reason to Accept] State-of-the-art performance at the submission time. To be noted, (Mehri et.al. 2020) reported better performance in MWoZ and other datasets, but this paper was released after the ICLR submission deadline.\n\n$\\rightarrow$ Is the title of the (Mehri et.al. 2020) paper :Unsupervised Evaluation of Interactive Dialog with DialoGPT? From google scholar this paper was written by Mehri et al., and is related to dialog state tracking. However, the paper did not run experiments on the MultiWOZ 2.1 dataset. If I have misunderstood the paper you are mentioning please let us know.\n\n5. Can the authors show the ablation for all the model size?\n\n$\\rightarrow$ As mentioned in 2. of our comment, due to limitations to computing power, we expect that we would be unable to run ablation experiments for all configurations, but we will try our best.\n\n6. Figure 4 is very hard to read. I suggest to better format the dialogue.\n\n$\\rightarrow$ We agree that Fig 4. should be presented in a better way. We will add more examples and change the presentation of the analysis as well. Thank you very much for your suggestion to improve our manuscript.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1611/Authors"], "readers": ["everyone", "ICLR.cc/2021/Conference/Paper1611/Reviewers"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1611/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Domain-slot Relationship Modeling using  a Pre-trained Language Encoder for Multi-Domain Dialogue State Tracking", "authorids": ["~Jinwon_An1", "~Misuk_Kim1", "~Sungzoon_Cho1", "hjbang21pp@etri.re.kr"], "authors": ["Jinwon An", "Misuk Kim", "Sungzoon Cho", "Junseong Bang"], "keywords": ["Dialogue State Tracking", "Domain-slot Relationship Modeling", "Pre-trained Language Encoder"], "abstract": "Dialogue state tracking for multi-domain dialogues is challenging because the model should be able to track dialogue states across multiple domains and slots. Past studies had its limitations in that they did not factor in the relationship among different domain-slot pairs. Although recent approaches did support relationship modeling among the domain-slot pairs, they did not leverage a pre-trained language model, which has improved the performance of numerous natural language tasks, in the encoding process. Our approach fills the gap between these previous studies. We propose a model for multi-domain dialogue state tracking that effectively models the relationship among domain-slot pairs using a pre-trained language encoder. Inspired by the way the special $[CLS]$ token in BERT is used to aggregate the information of the whole sequence, we use multiple special tokens for each domain-slot pair that encodes information corresponding to its domain and slot. The special tokens are run together with the dialogue context through the pre-trained language encoder, which effectively models the relationship among different domain-slot pairs. Our experimental results show that our model achieves state-of-the-art performance on the MultiWOZ-2.1 and MultiWOZ-2.2 dataset.", "one-sentence_summary": "We propose a model for multi-domain dialogue state tracking that effectively models the relationship among domain-slot pairs using a pre-trained language encoder.", "pdf": "/pdf/dbc08fab9f86385bde6ddee9efe0881fad8dff11.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "an|domainslot_relationship_modeling_using_a_pretrained_language_encoder_for_multidomain_dialogue_state_tracking", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=y4VuzDVdFf", "_bibtex": "@misc{\nan2021domainslot,\ntitle={Domain-slot Relationship Modeling using  a Pre-trained Language Encoder for Multi-Domain Dialogue State Tracking},\nauthor={Jinwon An and Misuk Kim and Sungzoon Cho and Junseong Bang},\nyear={2021},\nurl={https://openreview.net/forum?id=rmd-D7h_2zP}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "rmd-D7h_2zP", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1611/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1611/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1611/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1611/Authors|ICLR.cc/2021/Conference/Paper1611/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1611/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923857774, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1611/-/Official_Comment"}}}, {"id": "VRGar3qWmZf", "original": null, "number": 4, "cdate": 1605263232371, "ddate": null, "tcdate": 1605263232371, "tmdate": 1605797572441, "tddate": null, "forum": "rmd-D7h_2zP", "replyto": "n0_EQ7h4Kkh", "invitation": "ICLR.cc/2021/Conference/Paper1611/-/Official_Comment", "content": {"title": "Author Response to AnonReviewer3", "comment": "Thank you very much for your review of our paper. We initially updated our paper with an updated result using *xxlarge-v2* of ALBERT, which showed the state-of-the-art result. We are running additional experiments to respond to the reviewers comment, as soon as the experiments are done we will update the manuscript right away.\n\n1. This approach, like other popular approaches, suffers from the problem of having a fixed output vocabulary for slot values - hence limiting its scalability. While this cannot be addressed in this work, this is a drawback of this approach.\n\n$\\rightarrow$ Thank you for pointing out the fundamental limitations of our approach. Yes we agree with this. This paper is using the fixed-vocabulary approach. However our **xxlarge-v2** version showed much better performance that any other model. We would like to think our model as the stepping stone to the generative models with open vocabulary. We currently working on how to deal with the open-vocabulary approach on the next paper.\n\n2. Some of the design decisions are stated but not well explained: Only one pre-training method compared\n\n$\\rightarrow$ For additional pre-trained encoders, we will try to run both BERT and Roberta. However due to the tight deadline, we think we would be possible to run only Roberta for the new pretrained encoder. We will update our manuscript as soon as the results come up.\n\n3. Some of the design decisions are stated but not well explained: Authors mention they drop \"dontcare\" from slot gating but don't show the affect with or without it.\n$\\rightarrow$ We will mention the precision/recall of the dontcare slot gating. In our preliminary models with ALBERT \\textit{large-v2}, the prediction and recall for $dontcare$ was $48.87$% and $17.21$%, respectively. The precision and recall for $none$ showed $98.91$%, $99.45$% and $prediction$ $96.16$%, $94.93$%, respectively. We will mention this in the manuscript Section 3.2 Slot-gate classifier\n\n> In our preliminary models with ALBERT *large-v2*, the prediction and recall for dontcare was 48.87% and 17.21%, respectively. The precision and recall for none showed 98.91%, 99.45% and prediction 96.16%, 94.93%, respectively.\n\n\n4. Some of the design decisions are stated but not well explained: Not much details on the setup and how it was trained.\n\n$\\rightarrow$ We will describe in more detail on the experimental setup in section 4.2 Setup on the effective batch size and the GPU we used for training. We will provide more details to the setup if there are other details that you would like to know.\n\n5. Not much qualitative analysis.\n$\\rightarrow$ We agree that our analysis is not comprehensive. We are running additional experiments with only the $CLS$ token involved, without any $\\{DS\\}$ tokens, which was recommended by AnonReviewer2. This can show that involving  tokens is an effective solution. Early results show that the model without  tokens show very poor performance. As soon as the results are available, we will update our manuscript. We will also update our manuscript with additional examples of such improvements as well as some attention heat maps which can be used to infer how the model process.\n\n6. Section 3.2 and 3.3 can be shortened a lot. I would suggest showing more analysis.\n$\\rightarrow$ We will try to fit our paper in the 9 page limit with more analysis as mentioned above. We will try to shorten the Section that you have mentioned.\n\n7. Which turn in the dialogue does the error decrease the most.\n$\\rightarrow$ Could you please clarify what you mean by this comment? Does it mean to find a specific turn $t$ for some dialogue example and show qualitatively why that turn improved the error for that specific dialogue? We will run the analysis as you tell us to do.\n\n8. How much is the training time/ accuracy tradeoff\n$\\rightarrow$ We will provide the training time in terms of training steps and its corresponding accuracy. We did not record the results for early experiments with small models, so we are running them again. As soon as they are finished we will add the detail in the manuscript. For the *xxlarge-v2* version, in 20k steps 59.62%, in 25k steps: 63.46% 150k steps: $69.23$%.\n\n9. Adding another layer to make DS-split work should be trivial, there is no reason to leave that to future work. Could you show how the results look with that?\nWe were able to run the results with the smaller *base-v2* version. The results show that adding additional linear layers after concatenation did not improve the results. However, since this is a preliminary result, we will run the experiments on at least the *large-v2* version for confirmation. Note that in the *xxlarge-v2* version, the $DS_{split}$ worked better than the $DS_{merge}$ version. We think that this suggests that what matters is the encoding process of self-attention, not mere linear layers, which is in line with our preliminary results with *base-v2*.\n\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1611/Authors"], "readers": ["everyone", "ICLR.cc/2021/Conference/Paper1611/Reviewers"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1611/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Domain-slot Relationship Modeling using  a Pre-trained Language Encoder for Multi-Domain Dialogue State Tracking", "authorids": ["~Jinwon_An1", "~Misuk_Kim1", "~Sungzoon_Cho1", "hjbang21pp@etri.re.kr"], "authors": ["Jinwon An", "Misuk Kim", "Sungzoon Cho", "Junseong Bang"], "keywords": ["Dialogue State Tracking", "Domain-slot Relationship Modeling", "Pre-trained Language Encoder"], "abstract": "Dialogue state tracking for multi-domain dialogues is challenging because the model should be able to track dialogue states across multiple domains and slots. Past studies had its limitations in that they did not factor in the relationship among different domain-slot pairs. Although recent approaches did support relationship modeling among the domain-slot pairs, they did not leverage a pre-trained language model, which has improved the performance of numerous natural language tasks, in the encoding process. Our approach fills the gap between these previous studies. We propose a model for multi-domain dialogue state tracking that effectively models the relationship among domain-slot pairs using a pre-trained language encoder. Inspired by the way the special $[CLS]$ token in BERT is used to aggregate the information of the whole sequence, we use multiple special tokens for each domain-slot pair that encodes information corresponding to its domain and slot. The special tokens are run together with the dialogue context through the pre-trained language encoder, which effectively models the relationship among different domain-slot pairs. Our experimental results show that our model achieves state-of-the-art performance on the MultiWOZ-2.1 and MultiWOZ-2.2 dataset.", "one-sentence_summary": "We propose a model for multi-domain dialogue state tracking that effectively models the relationship among domain-slot pairs using a pre-trained language encoder.", "pdf": "/pdf/dbc08fab9f86385bde6ddee9efe0881fad8dff11.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "an|domainslot_relationship_modeling_using_a_pretrained_language_encoder_for_multidomain_dialogue_state_tracking", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=y4VuzDVdFf", "_bibtex": "@misc{\nan2021domainslot,\ntitle={Domain-slot Relationship Modeling using  a Pre-trained Language Encoder for Multi-Domain Dialogue State Tracking},\nauthor={Jinwon An and Misuk Kim and Sungzoon Cho and Junseong Bang},\nyear={2021},\nurl={https://openreview.net/forum?id=rmd-D7h_2zP}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "rmd-D7h_2zP", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1611/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1611/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1611/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1611/Authors|ICLR.cc/2021/Conference/Paper1611/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1611/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923857774, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1611/-/Official_Comment"}}}, {"id": "5kP5_OQMW8v", "original": null, "number": 3, "cdate": 1605261966746, "ddate": null, "tcdate": 1605261966746, "tmdate": 1605797549140, "tddate": null, "forum": "rmd-D7h_2zP", "replyto": "l2wbpyOZt7H", "invitation": "ICLR.cc/2021/Conference/Paper1611/-/Official_Comment", "content": {"title": "Author Response to AnonReviewer1", "comment": "Thank you very much for your review of our paper. We initially updated our paper with an updated result using *xx-large-v2* of ALBERT, which showed the state-of-the-art result. We are running additional experiments to respond to the reviewers comment, as soon as the experiments are done we will update the manuscript right away. \n\nNow, we will try to explain each point that you have mentioned.\n\n1. What is the role of segment embedding (Fig3) for DST tank? Are two different segment used in the pretraining of model?\n\n$\\rightarrow$ The role of the segment embedding is the default segment embedding for ALBERT as is described in the original paper (Lan, Zhenzhong, et al. 2019). During the pre-training of ALBERT, two segment embeddings are used, which is used to differentiate the two segments of sentences. Two segment embeddings are used because the order of two sentence segments can be continuous or random. In our model, we use a single segment embedding for the whole $\\{DS\\}$ tokens and $CLS$ and dialog context tokens. This is because as the $CLS$ token is subject to the first segment in the original ALBERT, $\\{DS\\}$ also serve as special $CLS$ tokens for each domain-slot pair.\n\n2. Table 1: 2-1 what type of label cleaning is used to compute joint goal accuracy? From SimpleTOD paper, each baseline has used different label cleaning 2-2: DS-split is bolded, while it is lower than SimpleTOD\n\n$\\rightarrow$ For label cleaning we used the method of TRADE, which is the standard that most models follow. It is described in section 4.1 Dataset as\n> We follow the pre-processing explained in (Wu et al., 2019).  \n\nIn Table 1, the SimpleTOD results are for the version using the label cleaning of TRADE. We will erase the bold of DS-split for the xlarge case. However, since our xx-large version achieved the new state-of-the-art result, we are changing Table 1. accordingly.\n\n\n3. Table 2: the results indicate that DST performance drops in total by blocking attention across different DS tokens. However, it is not clear how much of the performance drop belongs to turns with cross-domain related slots. The figure 4 only present one example of this case, which might not be correct for all wrong predictions. Also, it is helpful to report DST for single domain and to evaluate the importance of proposed approach.\n\n$\\rightarrow$ We understand that our analysis is not fully comprehensive. We assumed that the performance drop was due to the blocking of attention because the only different point of the model is that we disabled the attention across different DS tokens. Other model configurations were the same.  We are running additional experiments with only the $CLS$ token involved, without any $\\{DS\\}$ tokens, which was recommended by AnonReviewer2. This can show that involving $\\{DS\\}$ tokens is an effective solution. Early results show that the model without $\\{DS\\}$ tokens show very poor performance. As soon as the results are available, we will update our manuscript. \nWe will also update our manuscript with additional examples of such improvements as well as some attention heat maps which can be used to infer how the model process. \nDoes reporting DST for single domain mean that we should report the accuracy for each domain in the multi-domain setting? Or does it mean that we should train our model for a single domain, for example, the restaurant domain, and evaluate it? We will run the experiments as you tell us to do.\n\n3.  In the domain split/merge method, does the domain/slot used as vocabulary tag or word embedding? \n\n$\\rightarrow$ No, the \\$\\{DS\\}$ tokens initialized randomly without a vocabulary tag or word embedding. We will look into methods to effectively incorporate word embeddings without harming computation costs.\n\n4. Since the proposed approach can be used on any pretrained encoder, the evaluation on BERT and/or Roberta is helpful to understand the robustness of approach to the choice of pretrained encoder.\n\n$\\rightarrow$ We will try to run both BERT and Roberta. However due to the tight deadline, we think we would be possible to run only Roberta for the new pretrained encoder. We will update our manuscript as soon as the results come up."}, "signatures": ["ICLR.cc/2021/Conference/Paper1611/Authors"], "readers": ["everyone", "ICLR.cc/2021/Conference/Paper1611/Reviewers"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1611/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Domain-slot Relationship Modeling using  a Pre-trained Language Encoder for Multi-Domain Dialogue State Tracking", "authorids": ["~Jinwon_An1", "~Misuk_Kim1", "~Sungzoon_Cho1", "hjbang21pp@etri.re.kr"], "authors": ["Jinwon An", "Misuk Kim", "Sungzoon Cho", "Junseong Bang"], "keywords": ["Dialogue State Tracking", "Domain-slot Relationship Modeling", "Pre-trained Language Encoder"], "abstract": "Dialogue state tracking for multi-domain dialogues is challenging because the model should be able to track dialogue states across multiple domains and slots. Past studies had its limitations in that they did not factor in the relationship among different domain-slot pairs. Although recent approaches did support relationship modeling among the domain-slot pairs, they did not leverage a pre-trained language model, which has improved the performance of numerous natural language tasks, in the encoding process. Our approach fills the gap between these previous studies. We propose a model for multi-domain dialogue state tracking that effectively models the relationship among domain-slot pairs using a pre-trained language encoder. Inspired by the way the special $[CLS]$ token in BERT is used to aggregate the information of the whole sequence, we use multiple special tokens for each domain-slot pair that encodes information corresponding to its domain and slot. The special tokens are run together with the dialogue context through the pre-trained language encoder, which effectively models the relationship among different domain-slot pairs. Our experimental results show that our model achieves state-of-the-art performance on the MultiWOZ-2.1 and MultiWOZ-2.2 dataset.", "one-sentence_summary": "We propose a model for multi-domain dialogue state tracking that effectively models the relationship among domain-slot pairs using a pre-trained language encoder.", "pdf": "/pdf/dbc08fab9f86385bde6ddee9efe0881fad8dff11.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "an|domainslot_relationship_modeling_using_a_pretrained_language_encoder_for_multidomain_dialogue_state_tracking", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=y4VuzDVdFf", "_bibtex": "@misc{\nan2021domainslot,\ntitle={Domain-slot Relationship Modeling using  a Pre-trained Language Encoder for Multi-Domain Dialogue State Tracking},\nauthor={Jinwon An and Misuk Kim and Sungzoon Cho and Junseong Bang},\nyear={2021},\nurl={https://openreview.net/forum?id=rmd-D7h_2zP}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "rmd-D7h_2zP", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1611/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1611/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1611/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1611/Authors|ICLR.cc/2021/Conference/Paper1611/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1611/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923857774, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1611/-/Official_Comment"}}}, {"id": "WJoSNtwjN7", "original": null, "number": 2, "cdate": 1605255245265, "ddate": null, "tcdate": 1605255245265, "tmdate": 1605797538103, "tddate": null, "forum": "rmd-D7h_2zP", "replyto": "A49FqJHZEG", "invitation": "ICLR.cc/2021/Conference/Paper1611/-/Official_Comment", "content": {"title": "Author Response to AnonReviewer4", "comment": "Thank you very much for your review of our paper. We initially updated our paper with an updated result using *xx-large-v2* of ALBERT, which showed the state-of-the-art result. We are running additional experiments to respond to the reviewers comment, as soon as the experiments are done we will update the manuscript right away. \n\nNow, we will try to explain each point that you have mentioned.\n\n1. It\u2019s better to experiment on more dataset to prove the method\u2019s effectiveness.\n\n$\\rightarrow$ We agree that additional analysis on more datasets will prove the generalizability of our model. We know that there are other datasets such as Schema Guided Dialogue (SGD) (Rastogi et.al. 2019). We will try to add experiments to those dataset if possible.\n\n2. This paper https://arxiv.org/abs/2006.01554 seems get better performance on the same dataset. \n\n$\\rightarrow$ Thank you for recommeding the paper with better performance on the dataset. We were able to run the experiment with a bigger version of ALBERT *xxlarge-v2*, which enabled our model to achieve the state-of-the-art result as of now. We added the result of the paper you've mentioned in Table 1.\n\n3.  In the domain split/merge method, does the domain/slot used as vocabulary tag or word embedding? \n\n$\\rightarrow$ No, the \\$\\{DS\\}$ tokens initialized randomly without a vocabulary tag or word embedding. We will look into methods to effectively incorporate word embeddings without harming computation costs.\n\n4. The slot-gate classifier\u2019s output is fed into the slot-value classifier, how does this affect the performance?\n\n$\\rightarrow$The input of the slot-gate classifier are the  \\$\\{DS\\}$ tokens as shown in equation (7) and (8). The input for the slot-value classifier are the  \\$\\{DS\\}$ with the $CLS$ token as shown in equation (10) and (11). The input is partially shared, but the output of the slot-gate classifier is not used in the slot-value classifier.  If the domain-slot is predicted to $none$, the output of the slot-value classifier is changed into $none$ regardless of the prediction of the slot-value classifier. We will add this detail at the end of Section 3.2 slot-gate classifier."}, "signatures": ["ICLR.cc/2021/Conference/Paper1611/Authors"], "readers": ["everyone", "ICLR.cc/2021/Conference/Paper1611/Reviewers"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1611/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Domain-slot Relationship Modeling using  a Pre-trained Language Encoder for Multi-Domain Dialogue State Tracking", "authorids": ["~Jinwon_An1", "~Misuk_Kim1", "~Sungzoon_Cho1", "hjbang21pp@etri.re.kr"], "authors": ["Jinwon An", "Misuk Kim", "Sungzoon Cho", "Junseong Bang"], "keywords": ["Dialogue State Tracking", "Domain-slot Relationship Modeling", "Pre-trained Language Encoder"], "abstract": "Dialogue state tracking for multi-domain dialogues is challenging because the model should be able to track dialogue states across multiple domains and slots. Past studies had its limitations in that they did not factor in the relationship among different domain-slot pairs. Although recent approaches did support relationship modeling among the domain-slot pairs, they did not leverage a pre-trained language model, which has improved the performance of numerous natural language tasks, in the encoding process. Our approach fills the gap between these previous studies. We propose a model for multi-domain dialogue state tracking that effectively models the relationship among domain-slot pairs using a pre-trained language encoder. Inspired by the way the special $[CLS]$ token in BERT is used to aggregate the information of the whole sequence, we use multiple special tokens for each domain-slot pair that encodes information corresponding to its domain and slot. The special tokens are run together with the dialogue context through the pre-trained language encoder, which effectively models the relationship among different domain-slot pairs. Our experimental results show that our model achieves state-of-the-art performance on the MultiWOZ-2.1 and MultiWOZ-2.2 dataset.", "one-sentence_summary": "We propose a model for multi-domain dialogue state tracking that effectively models the relationship among domain-slot pairs using a pre-trained language encoder.", "pdf": "/pdf/dbc08fab9f86385bde6ddee9efe0881fad8dff11.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "an|domainslot_relationship_modeling_using_a_pretrained_language_encoder_for_multidomain_dialogue_state_tracking", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=y4VuzDVdFf", "_bibtex": "@misc{\nan2021domainslot,\ntitle={Domain-slot Relationship Modeling using  a Pre-trained Language Encoder for Multi-Domain Dialogue State Tracking},\nauthor={Jinwon An and Misuk Kim and Sungzoon Cho and Junseong Bang},\nyear={2021},\nurl={https://openreview.net/forum?id=rmd-D7h_2zP}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "rmd-D7h_2zP", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1611/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1611/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1611/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1611/Authors|ICLR.cc/2021/Conference/Paper1611/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1611/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923857774, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1611/-/Official_Comment"}}}, {"id": "Xj23Im28EIE", "original": null, "number": 7, "cdate": 1605797519764, "ddate": null, "tcdate": 1605797519764, "tmdate": 1605797519764, "tddate": null, "forum": "rmd-D7h_2zP", "replyto": "rmd-D7h_2zP", "invitation": "ICLR.cc/2021/Conference/Paper1611/-/Official_Comment", "content": {"title": "Revision adding more experimental results for ablation studies", "comment": "We added additional experimental results for ablation studies.\n\nSpecifically, we ran more experiments for proving that our domain-slot relationship modeling is effective. In addition to the blocking of attention among domain-slots, we ran a model without any $\\{DS\\}$ tokens at all, with only a $CLS$ token. We also ran the experiment for $ALBERT-xxlarge-v2$, in addition to the results from the $ALBERT-large-v2$ version.\n\nWe also ran another pretrained language model, RoBERTa. Specifically, RoBERTa-base and RoBERTa-large was added. Both models showed strong results.\n\nWe are currently running the model on MultiWOZ 2.2 (Zang et. al., 2020), which improved the MultiWOZ 2.1 dataset. We will update the manuscript as soon as the experiments are finished.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1611/Authors"], "readers": ["everyone", "ICLR.cc/2021/Conference/Paper1611/Reviewers", "ICLR.cc/2021/Conference/Paper1611/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1611/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Domain-slot Relationship Modeling using  a Pre-trained Language Encoder for Multi-Domain Dialogue State Tracking", "authorids": ["~Jinwon_An1", "~Misuk_Kim1", "~Sungzoon_Cho1", "hjbang21pp@etri.re.kr"], "authors": ["Jinwon An", "Misuk Kim", "Sungzoon Cho", "Junseong Bang"], "keywords": ["Dialogue State Tracking", "Domain-slot Relationship Modeling", "Pre-trained Language Encoder"], "abstract": "Dialogue state tracking for multi-domain dialogues is challenging because the model should be able to track dialogue states across multiple domains and slots. Past studies had its limitations in that they did not factor in the relationship among different domain-slot pairs. Although recent approaches did support relationship modeling among the domain-slot pairs, they did not leverage a pre-trained language model, which has improved the performance of numerous natural language tasks, in the encoding process. Our approach fills the gap between these previous studies. We propose a model for multi-domain dialogue state tracking that effectively models the relationship among domain-slot pairs using a pre-trained language encoder. Inspired by the way the special $[CLS]$ token in BERT is used to aggregate the information of the whole sequence, we use multiple special tokens for each domain-slot pair that encodes information corresponding to its domain and slot. The special tokens are run together with the dialogue context through the pre-trained language encoder, which effectively models the relationship among different domain-slot pairs. Our experimental results show that our model achieves state-of-the-art performance on the MultiWOZ-2.1 and MultiWOZ-2.2 dataset.", "one-sentence_summary": "We propose a model for multi-domain dialogue state tracking that effectively models the relationship among domain-slot pairs using a pre-trained language encoder.", "pdf": "/pdf/dbc08fab9f86385bde6ddee9efe0881fad8dff11.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "an|domainslot_relationship_modeling_using_a_pretrained_language_encoder_for_multidomain_dialogue_state_tracking", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=y4VuzDVdFf", "_bibtex": "@misc{\nan2021domainslot,\ntitle={Domain-slot Relationship Modeling using  a Pre-trained Language Encoder for Multi-Domain Dialogue State Tracking},\nauthor={Jinwon An and Misuk Kim and Sungzoon Cho and Junseong Bang},\nyear={2021},\nurl={https://openreview.net/forum?id=rmd-D7h_2zP}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "rmd-D7h_2zP", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1611/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1611/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1611/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1611/Authors|ICLR.cc/2021/Conference/Paper1611/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1611/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923857774, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1611/-/Official_Comment"}}}, {"id": "JGicoZ1Rsmx", "original": null, "number": 1, "cdate": 1603496515731, "ddate": null, "tcdate": 1603496515731, "tmdate": 1605024402435, "tddate": null, "forum": "rmd-D7h_2zP", "replyto": "rmd-D7h_2zP", "invitation": "ICLR.cc/2021/Conference/Paper1611/-/Official_Review", "content": {"title": "Review", "review": "[Summary]\nIn this paper, the authors proposed a multidomain state-tracking model that leverages the relationship among different domain-slot pairs. This is done by leveraging the full-attention step over the [CLS] special token and by providing all the domain-slot pairs as a special token to a pre-trained language model (Figure 2 is very clear). To predict the value of the slot $D_{i,j}$, the author concatenates the representation of the [CLS] token, share among all the domain-slots, and the $D_{i,j}$, provided as input, and use a gating mechanism, by only using $D_{i,j}$ representation, to decide whether require as value (i.e., prediction) or not (e.g. None). \\\n\nThe authors experimented using ALBERT (Lan et al., 2019) as a pre-trained language model, on the well-known benchmark MultiWoZ 2.0  (Budzianowski et al., 2018)  2.1 (Eric et al., 2019). The authors studied different format to represent $D_{i,j}$ DS-merge (i.e., one token per domain-slot) and DS-split (i.e., one token per slot and one per domain, thus more scalable). The reported performance is state-of-the-art at the time of submission. \n\n[Pros]\n- The paper reads well and it is easy to follow for people working on Task-Oriented dialogue.\n- The proposed method is simple and effective, and it would be easy to reproduce. \n\n[Cons]\n- The idea of using domain-pairs as input to a large pre-trained model is not novel (Wu et al., 2019; Zhang et al., 2019; Lee et al., 2019), as also pointed out by the authors, but the authors do not explicitly clarify this in the methodology section, leading the reader to believe that the domain-pairs is their own contribution. Same for the slot-gate (Wu et al., 2019)\n- The authors claim to learn relations between slots, but the analysis section is very thin and it just shows an ablation by masking the attention between the slot. Two points: why not just removing the [CLS] token instead of removing the attention, and why just using on ALBERTA large. For instance, the authors said \"For this experiment, we used the ALBERT configuration of large-v2,\nfor faster experimentation\" which is contradictory since large-v2 is the slowest to run I guess. Can the authors show this ablation for all the model size? \n- Although, MWoZ is the current benchmark for DST in ToDs, there are also other datasets for this task that can be considered (e.g., Schema Guided Dialogue (SGD) (Rastogi et.al. 2019))\n\n\n[Reason to Reject]\nThe main contribution of this paper is very thin, adding the [CLS] token as input, and the main technical contribution is not well explored (missing an in-depth ablation). \n\n[Reason to Accept]\nState-of-the-art performance at the submission time. To be noted, (Mehri et.al. 2020) reported better performance in MWoZ and other datasets, but this paper was released after the ICLR submission deadline.\n\n[Question]\n-  Can the authors show the ablation for all the model size? \n\n[Suggestion]\n- Figure 4 is very hard to read. I suggest to better format the dialogue. \n\n", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper1611/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1611/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Domain-slot Relationship Modeling using  a Pre-trained Language Encoder for Multi-Domain Dialogue State Tracking", "authorids": ["~Jinwon_An1", "~Misuk_Kim1", "~Sungzoon_Cho1", "hjbang21pp@etri.re.kr"], "authors": ["Jinwon An", "Misuk Kim", "Sungzoon Cho", "Junseong Bang"], "keywords": ["Dialogue State Tracking", "Domain-slot Relationship Modeling", "Pre-trained Language Encoder"], "abstract": "Dialogue state tracking for multi-domain dialogues is challenging because the model should be able to track dialogue states across multiple domains and slots. Past studies had its limitations in that they did not factor in the relationship among different domain-slot pairs. Although recent approaches did support relationship modeling among the domain-slot pairs, they did not leverage a pre-trained language model, which has improved the performance of numerous natural language tasks, in the encoding process. Our approach fills the gap between these previous studies. We propose a model for multi-domain dialogue state tracking that effectively models the relationship among domain-slot pairs using a pre-trained language encoder. Inspired by the way the special $[CLS]$ token in BERT is used to aggregate the information of the whole sequence, we use multiple special tokens for each domain-slot pair that encodes information corresponding to its domain and slot. The special tokens are run together with the dialogue context through the pre-trained language encoder, which effectively models the relationship among different domain-slot pairs. Our experimental results show that our model achieves state-of-the-art performance on the MultiWOZ-2.1 and MultiWOZ-2.2 dataset.", "one-sentence_summary": "We propose a model for multi-domain dialogue state tracking that effectively models the relationship among domain-slot pairs using a pre-trained language encoder.", "pdf": "/pdf/dbc08fab9f86385bde6ddee9efe0881fad8dff11.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "an|domainslot_relationship_modeling_using_a_pretrained_language_encoder_for_multidomain_dialogue_state_tracking", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=y4VuzDVdFf", "_bibtex": "@misc{\nan2021domainslot,\ntitle={Domain-slot Relationship Modeling using  a Pre-trained Language Encoder for Multi-Domain Dialogue State Tracking},\nauthor={Jinwon An and Misuk Kim and Sungzoon Cho and Junseong Bang},\nyear={2021},\nurl={https://openreview.net/forum?id=rmd-D7h_2zP}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "rmd-D7h_2zP", "replyto": "rmd-D7h_2zP", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1611/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538114790, "tmdate": 1606915762918, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1611/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1611/-/Official_Review"}}}, {"id": "A49FqJHZEG", "original": null, "number": 4, "cdate": 1604019192978, "ddate": null, "tcdate": 1604019192978, "tmdate": 1605024402232, "tddate": null, "forum": "rmd-D7h_2zP", "replyto": "rmd-D7h_2zP", "invitation": "ICLR.cc/2021/Conference/Paper1611/-/Official_Review", "content": {"title": "This paper studies the problem of multi-domain dialogue state tracking which is challenging.  It proposes a bert based model for multi-domain dialogue that effectively models the relationship among domain-slot pairs.  The experimental results show the model achieve sota performance on the MultiWOZ-2.1 dataset.", "review": "Pros\n\u2022\tThis paper incorporates the multi-domain domain-slot pairs into the bert input so that the relations between sentences, domain-slot are modeled.\nCons\n\u2022\tIt\u2019s better to experiment on more dataset to prove the method\u2019s effectiveness.\nComments\n\u2022\tThis paper   https://arxiv.org/abs/2006.01554 seems get better performance on the same dataset.\n\u2022\tIn the domain split/merge method, does the domain/slot used as vocabulary tag or word embedding?\n\u2022\tThe slot-gate classifier\u2019s output is fed into the slot-value classifier, how does this affect the performance?\n", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1611/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1611/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Domain-slot Relationship Modeling using  a Pre-trained Language Encoder for Multi-Domain Dialogue State Tracking", "authorids": ["~Jinwon_An1", "~Misuk_Kim1", "~Sungzoon_Cho1", "hjbang21pp@etri.re.kr"], "authors": ["Jinwon An", "Misuk Kim", "Sungzoon Cho", "Junseong Bang"], "keywords": ["Dialogue State Tracking", "Domain-slot Relationship Modeling", "Pre-trained Language Encoder"], "abstract": "Dialogue state tracking for multi-domain dialogues is challenging because the model should be able to track dialogue states across multiple domains and slots. Past studies had its limitations in that they did not factor in the relationship among different domain-slot pairs. Although recent approaches did support relationship modeling among the domain-slot pairs, they did not leverage a pre-trained language model, which has improved the performance of numerous natural language tasks, in the encoding process. Our approach fills the gap between these previous studies. We propose a model for multi-domain dialogue state tracking that effectively models the relationship among domain-slot pairs using a pre-trained language encoder. Inspired by the way the special $[CLS]$ token in BERT is used to aggregate the information of the whole sequence, we use multiple special tokens for each domain-slot pair that encodes information corresponding to its domain and slot. The special tokens are run together with the dialogue context through the pre-trained language encoder, which effectively models the relationship among different domain-slot pairs. Our experimental results show that our model achieves state-of-the-art performance on the MultiWOZ-2.1 and MultiWOZ-2.2 dataset.", "one-sentence_summary": "We propose a model for multi-domain dialogue state tracking that effectively models the relationship among domain-slot pairs using a pre-trained language encoder.", "pdf": "/pdf/dbc08fab9f86385bde6ddee9efe0881fad8dff11.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "an|domainslot_relationship_modeling_using_a_pretrained_language_encoder_for_multidomain_dialogue_state_tracking", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=y4VuzDVdFf", "_bibtex": "@misc{\nan2021domainslot,\ntitle={Domain-slot Relationship Modeling using  a Pre-trained Language Encoder for Multi-Domain Dialogue State Tracking},\nauthor={Jinwon An and Misuk Kim and Sungzoon Cho and Junseong Bang},\nyear={2021},\nurl={https://openreview.net/forum?id=rmd-D7h_2zP}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "rmd-D7h_2zP", "replyto": "rmd-D7h_2zP", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1611/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538114790, "tmdate": 1606915762918, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1611/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1611/-/Official_Review"}}}], "count": 13}