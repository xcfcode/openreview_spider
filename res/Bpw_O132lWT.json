{"notes": [{"id": "Bpw_O132lWT", "original": "_DcrDHj6X9", "number": 1418, "cdate": 1601308158003, "ddate": null, "tcdate": 1601308158003, "tmdate": 1614985685818, "tddate": null, "forum": "Bpw_O132lWT", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Dynamic of Stochastic Gradient Descent with State-dependent Noise", "authorids": ["~Qi_Meng1", "~Shiqi_Gong1", "~Wei_Chen1", "~Zhi-Ming_Ma1", "~Tie-Yan_Liu1"], "authors": ["Qi Meng", "Shiqi Gong", "Wei Chen", "Zhi-Ming Ma", "Tie-Yan Liu"], "keywords": ["state-dependent noise", "power-law dynamic", "stochastic gradient descent", "generalization", "deep neural network", "heavy-tailed", "escape time"], "abstract": "Stochastic gradient descent (SGD) and its variants are mainstream methods to train deep neural networks. Since neural networks are non-convex, more and more works study the dynamic behavior of SGD and its impact to generalization, especially the escaping efficiency from local minima. However, these works make the over-simplified assumption that the distribution of gradient noise is state-independent, although it is state-dependent. In this work, we propose a novel power-law dynamic with state-dependent diffusion to approximate the dynamic of SGD. Then, we prove that the stationary distribution of power-law dynamic is heavy-tailed, which matches the existing empirical observations. Next, we study the escaping efficiency from local minimum of power-law dynamic and prove that the mean escaping time is in polynomial order of the barrier height of the basin, much faster than exponential order of previous dynamics. It indicates that SGD can escape deep sharp minima efficiently and tends to stop at flat minima that have lower generalization error. Finally, we conduct experiments to compare SGD and power-law dynamic, and the results verify our theoretical findings.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "meng|dynamic_of_stochastic_gradient_descent_with_statedependent_noise", "one-sentence_summary": "We propose a novel power-law dynamic with state-dependent diffusion to approximate the dynamic of SGD, and analyze escaping efficiency and PAC-Bayesian generalization bound for it.", "pdf": "/pdf/178eaecb04429fe8847d55f837b1b008857148df.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=ryrEpL90e", "_bibtex": "@misc{\nmeng2021dynamic,\ntitle={Dynamic of Stochastic Gradient Descent with State-dependent Noise},\nauthor={Qi Meng and Shiqi Gong and Wei Chen and Zhi-Ming Ma and Tie-Yan Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=Bpw_O132lWT}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "2_9RqYISfX7", "original": null, "number": 1, "cdate": 1610040465821, "ddate": null, "tcdate": 1610040465821, "tmdate": 1610474069384, "tddate": null, "forum": "Bpw_O132lWT", "replyto": "Bpw_O132lWT", "invitation": "ICLR.cc/2021/Conference/Paper1418/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "The average review rating is 5.5 which means it\u2019s somewhat borderline. One of the reviewers planned to increase the score but apparently didn\u2019t do so formally. A subset of the main pros and cons the reviewers pointed out are: \n\nPros: \n\u201cSome empirical support is provided for the theory.\u201d\n\u201c It is particularly interesting that the authors show that the second order effect of the SGD noise in the Hessian induces a power law distribution over the iterates.\u201d\n\nCons: \n\u201cThe escaping efficiency of the power-law dynamic is only analyzed in low-dimension case. ...\u201d The author responded that Theorem 7 proves the multi-dimensional case. But the AC noted that it\u2019s very likely that escaping time is exponential in dimension (because kappa needs to be larger than d as the author noted and the det() might also be exponential in d. The author did say in the revision that the dimension should be considered as the effective dimension of the hessian, but the AC couldn\u2019t find a formal argument about it.)\n\u201cThe assumptions made are somewhat strong and may not hold in some cases...\u201d\n\nThe reviewers also had a few clarity questions which the author addressed in revisions with re-organized writing. The AC weighed the pros and cons and found that the unclarity and potential exponential escaping time in the multi-dimensional case outweigh the pros.   \n\n"}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic of Stochastic Gradient Descent with State-dependent Noise", "authorids": ["~Qi_Meng1", "~Shiqi_Gong1", "~Wei_Chen1", "~Zhi-Ming_Ma1", "~Tie-Yan_Liu1"], "authors": ["Qi Meng", "Shiqi Gong", "Wei Chen", "Zhi-Ming Ma", "Tie-Yan Liu"], "keywords": ["state-dependent noise", "power-law dynamic", "stochastic gradient descent", "generalization", "deep neural network", "heavy-tailed", "escape time"], "abstract": "Stochastic gradient descent (SGD) and its variants are mainstream methods to train deep neural networks. Since neural networks are non-convex, more and more works study the dynamic behavior of SGD and its impact to generalization, especially the escaping efficiency from local minima. However, these works make the over-simplified assumption that the distribution of gradient noise is state-independent, although it is state-dependent. In this work, we propose a novel power-law dynamic with state-dependent diffusion to approximate the dynamic of SGD. Then, we prove that the stationary distribution of power-law dynamic is heavy-tailed, which matches the existing empirical observations. Next, we study the escaping efficiency from local minimum of power-law dynamic and prove that the mean escaping time is in polynomial order of the barrier height of the basin, much faster than exponential order of previous dynamics. It indicates that SGD can escape deep sharp minima efficiently and tends to stop at flat minima that have lower generalization error. Finally, we conduct experiments to compare SGD and power-law dynamic, and the results verify our theoretical findings.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "meng|dynamic_of_stochastic_gradient_descent_with_statedependent_noise", "one-sentence_summary": "We propose a novel power-law dynamic with state-dependent diffusion to approximate the dynamic of SGD, and analyze escaping efficiency and PAC-Bayesian generalization bound for it.", "pdf": "/pdf/178eaecb04429fe8847d55f837b1b008857148df.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=ryrEpL90e", "_bibtex": "@misc{\nmeng2021dynamic,\ntitle={Dynamic of Stochastic Gradient Descent with State-dependent Noise},\nauthor={Qi Meng and Shiqi Gong and Wei Chen and Zhi-Ming Ma and Tie-Yan Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=Bpw_O132lWT}\n}"}, "tags": [], "invitation": {"reply": {"forum": "Bpw_O132lWT", "replyto": "Bpw_O132lWT", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040465807, "tmdate": 1610474069368, "id": "ICLR.cc/2021/Conference/Paper1418/-/Decision"}}}, {"id": "uB0smUihduB", "original": null, "number": 9, "cdate": 1606188162363, "ddate": null, "tcdate": 1606188162363, "tmdate": 1606188162363, "tddate": null, "forum": "Bpw_O132lWT", "replyto": "B3FmtfikWvp", "invitation": "ICLR.cc/2021/Conference/Paper1418/-/Official_Comment", "content": {"title": "Thank you for raising the score", "comment": "Thank you for raising the score. We\u2019re glad to see that our response has addressed your concerns. We will continue to polish the paper as you suggested."}, "signatures": ["ICLR.cc/2021/Conference/Paper1418/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1418/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic of Stochastic Gradient Descent with State-dependent Noise", "authorids": ["~Qi_Meng1", "~Shiqi_Gong1", "~Wei_Chen1", "~Zhi-Ming_Ma1", "~Tie-Yan_Liu1"], "authors": ["Qi Meng", "Shiqi Gong", "Wei Chen", "Zhi-Ming Ma", "Tie-Yan Liu"], "keywords": ["state-dependent noise", "power-law dynamic", "stochastic gradient descent", "generalization", "deep neural network", "heavy-tailed", "escape time"], "abstract": "Stochastic gradient descent (SGD) and its variants are mainstream methods to train deep neural networks. Since neural networks are non-convex, more and more works study the dynamic behavior of SGD and its impact to generalization, especially the escaping efficiency from local minima. However, these works make the over-simplified assumption that the distribution of gradient noise is state-independent, although it is state-dependent. In this work, we propose a novel power-law dynamic with state-dependent diffusion to approximate the dynamic of SGD. Then, we prove that the stationary distribution of power-law dynamic is heavy-tailed, which matches the existing empirical observations. Next, we study the escaping efficiency from local minimum of power-law dynamic and prove that the mean escaping time is in polynomial order of the barrier height of the basin, much faster than exponential order of previous dynamics. It indicates that SGD can escape deep sharp minima efficiently and tends to stop at flat minima that have lower generalization error. Finally, we conduct experiments to compare SGD and power-law dynamic, and the results verify our theoretical findings.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "meng|dynamic_of_stochastic_gradient_descent_with_statedependent_noise", "one-sentence_summary": "We propose a novel power-law dynamic with state-dependent diffusion to approximate the dynamic of SGD, and analyze escaping efficiency and PAC-Bayesian generalization bound for it.", "pdf": "/pdf/178eaecb04429fe8847d55f837b1b008857148df.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=ryrEpL90e", "_bibtex": "@misc{\nmeng2021dynamic,\ntitle={Dynamic of Stochastic Gradient Descent with State-dependent Noise},\nauthor={Qi Meng and Shiqi Gong and Wei Chen and Zhi-Ming Ma and Tie-Yan Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=Bpw_O132lWT}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Bpw_O132lWT", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1418/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1418/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1418/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1418/Authors|ICLR.cc/2021/Conference/Paper1418/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1418/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923859942, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1418/-/Official_Comment"}}}, {"id": "FL6HlaYlCU4", "original": null, "number": 2, "cdate": 1605867450436, "ddate": null, "tcdate": 1605867450436, "tmdate": 1606186772390, "tddate": null, "forum": "Bpw_O132lWT", "replyto": "XaiXgknkrs5", "invitation": "ICLR.cc/2021/Conference/Paper1418/-/Official_Comment", "content": {"title": "Response to Reviewer #4", "comment": "Thank you very much for the valuable comments. Please check the proof of Theorem 2 in Appendix 7.1 in the updated version. \nHere are our responses to your two concerns.\n1. \u201cIt is not clear how w* is selected considering there are multiple local minima. It does not make sense to me if w* is fixed when taking w-->\\infty\u201d\n\nOur analyses are established in the local region of the local minima and thus w^* can be any fixed local minima. In the updated version, instead of taking $w\\rightarrow\\infty$, we discuss about the decreasing rate of p(w) in the region $[w^*-\\epsilon, w^*+\\epsilon]$, which indicates power-law distribution is less concentrated in the quadratic basin and heavy-tailed. \n\n2. \u201cThe escaping efficiency of the power-law dynamic is only analyzed in low-dimension case. Does it provide more benefits than Langevin/alpha-stable dynamic in the expense of calculating sigma_g and sigma_H.\"\n\na) Please refer Theorem 7 in our submission for our results in multi-dimensional case. The result for multi-dimensional case also inherits the benefit of power-law dynamic in 1-dimensional case. The only difference is that dimension d appears in the denominator and kappa needs to be larger than d/2 in multi-dimensional case. As discussed in the remark after theorem 7, H is observed to be highly degenerate in deep learning [1]. Thus, d can be replaced by the number of dimensions whose corresponding eigenvalues are significantly larger than zero, which is much smaller. \n\nb) As Langevin dynamic with constant diffusion does not generalize well (shown in Figure 1 in [2]), in practice, our theory motivates us to inject state-dependent noise to GD or large batch SGD to improve its generalization. Although calculating the exact value of \\sigma_H may be costly, we will investigate efficient approximation of it in future work. \n\n$\\textbf{For your minor comment}$, although the work [Li et al., 2017] formulates the dynamic of SGD as SDE with state-dependent diffusion, all the theoretical analysis including \u201coptimal control of learning rate and momentum\u201d are derived under the assumption that C(w) is locally constant. We cite and discuss this paper in related work in the updated version. \n\n[1] Levent Sagun, Leon Bottou, Yann LeCun. 2017 Eigenvalues of the Hessian in Deep Learning: Singularity and Beyond\n[2] Zhu, Zhanxing, Wu, Jingfeng, Yu, Bing, Wu, Lei, & Ma, Jinwen. 2019.  The anisotropic noise instochastic gradient descent: Its behavior of escaping from sharp minima and regularization effects.Pages 7654\u20137663 of: Proceedings of International Conference on Machine Learning.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1418/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1418/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic of Stochastic Gradient Descent with State-dependent Noise", "authorids": ["~Qi_Meng1", "~Shiqi_Gong1", "~Wei_Chen1", "~Zhi-Ming_Ma1", "~Tie-Yan_Liu1"], "authors": ["Qi Meng", "Shiqi Gong", "Wei Chen", "Zhi-Ming Ma", "Tie-Yan Liu"], "keywords": ["state-dependent noise", "power-law dynamic", "stochastic gradient descent", "generalization", "deep neural network", "heavy-tailed", "escape time"], "abstract": "Stochastic gradient descent (SGD) and its variants are mainstream methods to train deep neural networks. Since neural networks are non-convex, more and more works study the dynamic behavior of SGD and its impact to generalization, especially the escaping efficiency from local minima. However, these works make the over-simplified assumption that the distribution of gradient noise is state-independent, although it is state-dependent. In this work, we propose a novel power-law dynamic with state-dependent diffusion to approximate the dynamic of SGD. Then, we prove that the stationary distribution of power-law dynamic is heavy-tailed, which matches the existing empirical observations. Next, we study the escaping efficiency from local minimum of power-law dynamic and prove that the mean escaping time is in polynomial order of the barrier height of the basin, much faster than exponential order of previous dynamics. It indicates that SGD can escape deep sharp minima efficiently and tends to stop at flat minima that have lower generalization error. Finally, we conduct experiments to compare SGD and power-law dynamic, and the results verify our theoretical findings.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "meng|dynamic_of_stochastic_gradient_descent_with_statedependent_noise", "one-sentence_summary": "We propose a novel power-law dynamic with state-dependent diffusion to approximate the dynamic of SGD, and analyze escaping efficiency and PAC-Bayesian generalization bound for it.", "pdf": "/pdf/178eaecb04429fe8847d55f837b1b008857148df.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=ryrEpL90e", "_bibtex": "@misc{\nmeng2021dynamic,\ntitle={Dynamic of Stochastic Gradient Descent with State-dependent Noise},\nauthor={Qi Meng and Shiqi Gong and Wei Chen and Zhi-Ming Ma and Tie-Yan Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=Bpw_O132lWT}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Bpw_O132lWT", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1418/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1418/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1418/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1418/Authors|ICLR.cc/2021/Conference/Paper1418/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1418/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923859942, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1418/-/Official_Comment"}}}, {"id": "juO-JoFhad", "original": null, "number": 2, "cdate": 1603848975709, "ddate": null, "tcdate": 1603848975709, "tmdate": 1606169297224, "tddate": null, "forum": "Bpw_O132lWT", "replyto": "Bpw_O132lWT", "invitation": "ICLR.cc/2021/Conference/Paper1418/-/Official_Review", "content": {"title": "Interesting theoretical results, though presentation could be improved", "review": "Summary: the paper studies the effect of SGD noise near a local minimum of the loss by using a novel Taylor expansion to estimate the distribution of gradient noise in the neighborhood of that minimum. They use this to derive closed-form equations describing the distribution of the iterates, which they use to characterize properties such escaping times and generalization. \n\nPros: \n- To my knowledge, the mathematical analysis appears to be quite novel and insightful. It is particularly interesting that the authors show that the second order effect of the SGD noise in the Hessian induces a power law distribution over the iterates. \n- Some empirical support is provided for the theory.\n\nCons:\n- In general, a clearer statement (and justification) of the assumptions is required. For example, it appears to be implicit throughout the paper that we only consider the neighborhood of a local minimum, so the analysis is essentially for a quadratic in this neighborhood. This should be stated more explicitly. \n- I also have some concerns about mathematical precision in the theorem statements. It is sometimes unclear which computations are rigorous equalities and which are not - for example, in Lemma 6 about escaping times, exact equality is used. However, the proof relies on Taylor expansion and uses approximate equalities in the steps. This is potentially misleading. \n\nIn general, the results seem interesting, and it is understandable that certain assumptions/heuristics must be used because this area of research is technically challenging. However, I would like to see the clarity of the presentation improved before recommending acceptance. \n\nI have more specific questions regarding the details in the paper below:\n- Could the authors elaborate on the relationship between kappa and generalization? From the paper my understanding was that smaller kappa meant flatter curvature and better generalization, but this doesn't seem to be supported by Figure 3. \n- How is the value of kappa contained in Figure 3? Is it computed via computing the Hessian and its covariance or chosen to best fit the histograms in the figure?\n- Eqn 6: It seems like this closed form computation is specifically for the case when the function is quadratic (e.g. we take 2nd order Taylor approximation around a local min. Can the authors confirm?) If this is the case, what happens to the dependency on w - w^* and why is there no such explicit term in eqn 6? It would appear that g(w) should depend on (w - w^*). \n- In the overparameterized regime, it would appear that \\sigma_g could go to 0 if each training example is overfit by the model. It appears that plugging in \\sigma_g = 0 would introduce some degeneracy in equation 6 and 7, however. Can the authors comment on this?\n- Intuition on the term \\sigma_H: what do we expect this to look like in practice and do the authors have a sense on whether this term only matters around local minimum?\n- The definition of \\Sigma_H in the multivariate case: in the first paragraph of section 3, the definition on the LHS has no mention of i, j but the RHS does.\n- Assuming the signal to noise ratio of \\tilde{H} can be characterized by a scalar - why is this assumption reasonable?\n\n*********\nEDIT: Changed my score from 5 to 6 after the author response/revision.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1418/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1418/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic of Stochastic Gradient Descent with State-dependent Noise", "authorids": ["~Qi_Meng1", "~Shiqi_Gong1", "~Wei_Chen1", "~Zhi-Ming_Ma1", "~Tie-Yan_Liu1"], "authors": ["Qi Meng", "Shiqi Gong", "Wei Chen", "Zhi-Ming Ma", "Tie-Yan Liu"], "keywords": ["state-dependent noise", "power-law dynamic", "stochastic gradient descent", "generalization", "deep neural network", "heavy-tailed", "escape time"], "abstract": "Stochastic gradient descent (SGD) and its variants are mainstream methods to train deep neural networks. Since neural networks are non-convex, more and more works study the dynamic behavior of SGD and its impact to generalization, especially the escaping efficiency from local minima. However, these works make the over-simplified assumption that the distribution of gradient noise is state-independent, although it is state-dependent. In this work, we propose a novel power-law dynamic with state-dependent diffusion to approximate the dynamic of SGD. Then, we prove that the stationary distribution of power-law dynamic is heavy-tailed, which matches the existing empirical observations. Next, we study the escaping efficiency from local minimum of power-law dynamic and prove that the mean escaping time is in polynomial order of the barrier height of the basin, much faster than exponential order of previous dynamics. It indicates that SGD can escape deep sharp minima efficiently and tends to stop at flat minima that have lower generalization error. Finally, we conduct experiments to compare SGD and power-law dynamic, and the results verify our theoretical findings.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "meng|dynamic_of_stochastic_gradient_descent_with_statedependent_noise", "one-sentence_summary": "We propose a novel power-law dynamic with state-dependent diffusion to approximate the dynamic of SGD, and analyze escaping efficiency and PAC-Bayesian generalization bound for it.", "pdf": "/pdf/178eaecb04429fe8847d55f837b1b008857148df.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=ryrEpL90e", "_bibtex": "@misc{\nmeng2021dynamic,\ntitle={Dynamic of Stochastic Gradient Descent with State-dependent Noise},\nauthor={Qi Meng and Shiqi Gong and Wei Chen and Zhi-Ming Ma and Tie-Yan Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=Bpw_O132lWT}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Bpw_O132lWT", "replyto": "Bpw_O132lWT", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1418/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538119120, "tmdate": 1606915790914, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1418/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1418/-/Official_Review"}}}, {"id": "B3FmtfikWvp", "original": null, "number": 8, "cdate": 1606169147849, "ddate": null, "tcdate": 1606169147849, "tmdate": 1606169147849, "tddate": null, "forum": "Bpw_O132lWT", "replyto": "XFvOdsAWR0R", "invitation": "ICLR.cc/2021/Conference/Paper1418/-/Official_Comment", "content": {"title": "Thanks for the clarification and revisions", "comment": "Thank you for the clarifications and revisions. As the revision has improved the clarity of the results in the paper, I will increase my score slightly. I suggest that for future revisions, the authors continue to improve the clarity in their presentation of the results."}, "signatures": ["ICLR.cc/2021/Conference/Paper1418/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1418/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic of Stochastic Gradient Descent with State-dependent Noise", "authorids": ["~Qi_Meng1", "~Shiqi_Gong1", "~Wei_Chen1", "~Zhi-Ming_Ma1", "~Tie-Yan_Liu1"], "authors": ["Qi Meng", "Shiqi Gong", "Wei Chen", "Zhi-Ming Ma", "Tie-Yan Liu"], "keywords": ["state-dependent noise", "power-law dynamic", "stochastic gradient descent", "generalization", "deep neural network", "heavy-tailed", "escape time"], "abstract": "Stochastic gradient descent (SGD) and its variants are mainstream methods to train deep neural networks. Since neural networks are non-convex, more and more works study the dynamic behavior of SGD and its impact to generalization, especially the escaping efficiency from local minima. However, these works make the over-simplified assumption that the distribution of gradient noise is state-independent, although it is state-dependent. In this work, we propose a novel power-law dynamic with state-dependent diffusion to approximate the dynamic of SGD. Then, we prove that the stationary distribution of power-law dynamic is heavy-tailed, which matches the existing empirical observations. Next, we study the escaping efficiency from local minimum of power-law dynamic and prove that the mean escaping time is in polynomial order of the barrier height of the basin, much faster than exponential order of previous dynamics. It indicates that SGD can escape deep sharp minima efficiently and tends to stop at flat minima that have lower generalization error. Finally, we conduct experiments to compare SGD and power-law dynamic, and the results verify our theoretical findings.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "meng|dynamic_of_stochastic_gradient_descent_with_statedependent_noise", "one-sentence_summary": "We propose a novel power-law dynamic with state-dependent diffusion to approximate the dynamic of SGD, and analyze escaping efficiency and PAC-Bayesian generalization bound for it.", "pdf": "/pdf/178eaecb04429fe8847d55f837b1b008857148df.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=ryrEpL90e", "_bibtex": "@misc{\nmeng2021dynamic,\ntitle={Dynamic of Stochastic Gradient Descent with State-dependent Noise},\nauthor={Qi Meng and Shiqi Gong and Wei Chen and Zhi-Ming Ma and Tie-Yan Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=Bpw_O132lWT}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Bpw_O132lWT", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1418/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1418/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1418/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1418/Authors|ICLR.cc/2021/Conference/Paper1418/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1418/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923859942, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1418/-/Official_Comment"}}}, {"id": "XFvOdsAWR0R", "original": null, "number": 5, "cdate": 1605869079311, "ddate": null, "tcdate": 1605869079311, "tmdate": 1605872830392, "tddate": null, "forum": "Bpw_O132lWT", "replyto": "juO-JoFhad", "invitation": "ICLR.cc/2021/Conference/Paper1418/-/Official_Comment", "content": {"title": "Response to Reviewer #2", "comment": "Thank you very much for the constructive comments for us to improve the mathematical precision and the recognition for the novelty and the insights of our work. We carefully revised the paper according to your comments. The revisions mainly include the followings:\na) To make a clearer statement of the assumptions, we reorganized all the assumptions for escaping time and move them to the beginning of section 4. Based on the assumptions, equality in Lemma 6 is established.  b) We gave the expression of $C(w)$ in the theorems to make them self-contained.  c) We unified the notation of $\\Sigma_H$.  d) We provided more justifications about the assumption on signal to noise ratio of \\tilde{H} in Appendix 7.2. \n\nRegarding the specific questions, we have the following responses.\n1.\t\"Relationship between kappa and generalization. From the paper my understanding was that smaller kappa meant flatter curvature and better generalization, but this doesn't seem to be supported by Figure 3.\"\n\nIn Figure3(b), when kappa=1.08 with small batch size 64,  training accuracy is lower because of high magnitude of noise. To remove the influence of the training and better illustrate the relationship between kappa and generalization, we report the gap between test error and the training error in the updated version.  All the results are consistent with our theory.    \n\n2.\tHow is the value of kappa contained in Figure 3?\n\nThe values of kappa in Figure 3 is chosen to best fit the histograms in the figure.\n\n3.\twhat happens to the dependency on $w - w^*$ and Why is there no such explicit term in eqn6?\n\nAs shown by the notations defined in the end of Sec 3.1, $C(w)$ depends on $w-w^*$. To make Theorem 2 self-contained, we have added the expression of $C(w)$ after eqn6 in the updated version. \n\n4.\tIn the overparameterized regime, it would appear that $\\sigma_g$ could go to 0 if each training example is overfit by the model. \n\nThanks for raising this interesting question. In overparameterized regime, $\\sigma_g$ could go to 0 when the value of loss goes to 0. At the same time, $\\sigma_H$ goes to zero and $C(w)$ goes to zero. The dynamic will be static and cannot escape from the local minima if $C(w)$ equals to zero.  We added a footnote for this case in the updated version. \n\n5.\tIntuition on the term $\\sigma_H$: what do we expect this to look like in practice and do the authors have a sense on whether this term only matters around local minimum? \n\nIntuitively, $\\sigma_H$ measures the variance of the flatness. Both the averaged flatness (i.e., $H$) and the variance of the flatness (i.e., $\\sigma_H$) will influence the dynamic behavior. In our current theoretical analyses, $\\sigma_H$ mainly makes sense around local minima or saddle points.\n\nAlthough the theory is mainly focused on local minimum [1], in practice, it inspires us to inject state-dependent noise (instead of constant noise) into the algorithm on its whole optimization path. That is to say, $\\sigma_H$ can be leveraged to help optimization algorithm escape critical points including local minima and saddle points efficiently. For example, when GD or large batch SGD stops, we can inject noise with variance $\\sigma_g+\\sigma_H(w-w^*)^2$ in them. We have done an experiment to demonstrate its efficiency. Please refer Appendix 7.5.4.   \n\n6.\tAssumption on the signal to noise ratio of \\tilde{H} \u2013 can be characterized by a scalar. \n\nThis assumption is made for ease of theoretical analyses. Similar assumption is adopt to analyze high dimensional Hessian in [2] where they only consider the fluctuation of the largest eigenvalue of Hessian. \nIn practice, we expect that the fluctuation of \\tilde{H} follows: for small eigenvalues in $H$, their variances are also small. It results in that parameters move only in a low-dimensional space as discussion in section 5 in [3]. So, we use a scalar to characterize the ratio of $\\Sigma_H$ and $H$ to roughly reflect the positive correlation between $H$ and $\\Sigma_H$. Besides this assumption, we provide results based on much weaker assumption in proposition 12 in Appendix 7.2.\n\nMaking this assumption also has practical benefit, i.e., we can use H and the scalar \\kappa to   characterize the dynamic without calculating $\\Sigma_H$. The analyses on escaping time and generalization for this dynamic show that this dynamic can generalize better than Langevin.\n\n\n[1]  Zhu, Zhanxing, Wu, Jingfeng, Yu, Bing, Wu, Lei, & Ma, Jinwen. 2019.  The anisotropic noise instochastic gradient descent: Its behavior of escaping from sharp minima and regularization effects. ICML2019\n\n[2] Wu et.al 2018. How sgd selects the global minima in over-parameterized learning: A dynamical stability perspective. Neurips2018\n\n[3] Xie, Zeke, Sato, Issei, & Sugiyama, Masashi. 2020. A Diffusion Theory for Deep Learning Dynamics:\nStochastic Gradient Descent Escapes From Sharp Minima Exponentially Fast. arXiv preprint\narXiv:2002.03495.\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1418/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1418/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic of Stochastic Gradient Descent with State-dependent Noise", "authorids": ["~Qi_Meng1", "~Shiqi_Gong1", "~Wei_Chen1", "~Zhi-Ming_Ma1", "~Tie-Yan_Liu1"], "authors": ["Qi Meng", "Shiqi Gong", "Wei Chen", "Zhi-Ming Ma", "Tie-Yan Liu"], "keywords": ["state-dependent noise", "power-law dynamic", "stochastic gradient descent", "generalization", "deep neural network", "heavy-tailed", "escape time"], "abstract": "Stochastic gradient descent (SGD) and its variants are mainstream methods to train deep neural networks. Since neural networks are non-convex, more and more works study the dynamic behavior of SGD and its impact to generalization, especially the escaping efficiency from local minima. However, these works make the over-simplified assumption that the distribution of gradient noise is state-independent, although it is state-dependent. In this work, we propose a novel power-law dynamic with state-dependent diffusion to approximate the dynamic of SGD. Then, we prove that the stationary distribution of power-law dynamic is heavy-tailed, which matches the existing empirical observations. Next, we study the escaping efficiency from local minimum of power-law dynamic and prove that the mean escaping time is in polynomial order of the barrier height of the basin, much faster than exponential order of previous dynamics. It indicates that SGD can escape deep sharp minima efficiently and tends to stop at flat minima that have lower generalization error. Finally, we conduct experiments to compare SGD and power-law dynamic, and the results verify our theoretical findings.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "meng|dynamic_of_stochastic_gradient_descent_with_statedependent_noise", "one-sentence_summary": "We propose a novel power-law dynamic with state-dependent diffusion to approximate the dynamic of SGD, and analyze escaping efficiency and PAC-Bayesian generalization bound for it.", "pdf": "/pdf/178eaecb04429fe8847d55f837b1b008857148df.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=ryrEpL90e", "_bibtex": "@misc{\nmeng2021dynamic,\ntitle={Dynamic of Stochastic Gradient Descent with State-dependent Noise},\nauthor={Qi Meng and Shiqi Gong and Wei Chen and Zhi-Ming Ma and Tie-Yan Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=Bpw_O132lWT}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Bpw_O132lWT", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1418/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1418/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1418/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1418/Authors|ICLR.cc/2021/Conference/Paper1418/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1418/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923859942, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1418/-/Official_Comment"}}}, {"id": "NF2JyUng6Py", "original": null, "number": 6, "cdate": 1605870202474, "ddate": null, "tcdate": 1605870202474, "tmdate": 1605871012603, "tddate": null, "forum": "Bpw_O132lWT", "replyto": "qevyo7b-1p", "invitation": "ICLR.cc/2021/Conference/Paper1418/-/Official_Comment", "content": {"title": "Response to Reviewer #1", "comment": "Thank you very much for your valuable comments and constructive suggestions on experiments. The following are our responses. \n1.\t$C(w)$ is diagonally dominant according to empirical observation. Does it have any theoretical justifications, or even in simplified cases?\n\n\u201cDiagonally dominant\u201d indicates that the variance of each dimension of stochastic gradient $\\tilde{g}(w)$ is larger than the covariance of two different dimensions of stochastic $\\tilde{g}(w)$. As you suggested, we have added a simplified example of deep neural network at the initialization to justify this assumption in Appendix 7.2. The example shows that, the independence of different dimensions of stochastic gradient comes from the random initialization of non-overlapped parameters. Please kindly check Appendix 7.2 for the details.\n\n2.\tHowever, the first term also includes the Hessian and might also affect generalization bound. \nWhat are the differences between the power-law dynamics and Langevin dynamics in terms of generalization?\n\na.\tWe are not sure that what is the first term you refer to. We guess that you mean the expected training loss term $\\mathbb{E}_{w\\sim p(w)}L(w)$. \n\nIn Theorem 8, the generalization error means the difference between the training and test loss, i.e.,  $\\mathbb{E}_{w,x}\\ell(w,x)-{\\mathbb{E}_w}L(w)$. Therefore, only the term contained KL divergence at the right side of the inequality is the upper bound of generalization error. \n\nIndeed, the Hessian will also influence the expected training loss (i.e., $\\mathbb{E}_{w\\sim p(w)}L(w)$), however, many optimization algorithms have enough ability to achieve low training loss in the over-parameterized regime. An example in Figure 3 in [1] shows that both Langevin dynamic with constant diffusion coefficient and SGD can achieve low training loss, but the test accuracy of Langevin dynamic is lower than SGD. Therefore, researchers mainly care about the generalization error [2].\n\nb.\t Our generalization bound shows that the generalization error of power-law dynamic is smaller than that of Langevin dynamic. As discussed in paragraph after Theorem 8, when kappa goes to infinity, the result goes to that for Langevin dynamic, which is consistent to the results in [2]. \n\n3.\tAt least, from visual observation, there are many other alternatives besides power-law distribution to fit, as Fig 3 shows.\n\nWe have added the Q-Q plot (quantile-quantile plot) for power-law distribution and Gaussian distribution in Figure 8 in Appendix. Figure 8 shows that power-law distribution fits distribution of parameters better than Gaussian distribution. \n\n4.\tAbout comparing the escaping efficiency, the result only shows the success rate, and the evidence about the polynomial and exponential difference should be provided. Also, practical networks and datasets should also be considered to provide more strong evidence.\n\na.\tAs you suggested, we conduct experiments to compare the escaping time with respect to different heights of barrier for power-law dynamic. The results are added in Appendix 7.5.5. Results in Figure 10(b) can support the theoretical results about mean escaping time of power-law dynamic and Langevin dynamic with respect to different barrier height.\n\nb.\tWe add the experiments on the escaping efficiency on practical networks in Appendix 7.5.4. We first use GD to train the network till it converges. Then the stopped point can be regarded as a local minimum. Then we use simulations of Langevin dynamic and power-law dynamic to continue the training. Results show that power-law dynamic can stopped at a flatter minimum and achieve higher test accuracy than Langevin dynamic.\n\n5.\tThank you for recommending the paper [Wu et.al 2018]. We cite this reference in related works in the updated version.\n\n[1] Zhu, Zhanxing, Wu, Jingfeng, Yu, Bing, Wu, Lei, & Ma, Jinwen. 2019.  The anisotropic noise instochastic gradient descent: Its behavior of escaping from sharp minima and regularization effects.Pages 7654\u20137663 of: Proceedings of International Conference on Machine Learning.\n\n[2] He, Fengxiang, Liu, Tongliang, & Tao, Dacheng. 2019a. Control Batch Size and Learning Rate to Generalize Well: Theoretical and Empirical Evidence.Pages 1141\u20131150 of: Advances in NeuralInformation Processing Systems.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1418/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1418/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic of Stochastic Gradient Descent with State-dependent Noise", "authorids": ["~Qi_Meng1", "~Shiqi_Gong1", "~Wei_Chen1", "~Zhi-Ming_Ma1", "~Tie-Yan_Liu1"], "authors": ["Qi Meng", "Shiqi Gong", "Wei Chen", "Zhi-Ming Ma", "Tie-Yan Liu"], "keywords": ["state-dependent noise", "power-law dynamic", "stochastic gradient descent", "generalization", "deep neural network", "heavy-tailed", "escape time"], "abstract": "Stochastic gradient descent (SGD) and its variants are mainstream methods to train deep neural networks. Since neural networks are non-convex, more and more works study the dynamic behavior of SGD and its impact to generalization, especially the escaping efficiency from local minima. However, these works make the over-simplified assumption that the distribution of gradient noise is state-independent, although it is state-dependent. In this work, we propose a novel power-law dynamic with state-dependent diffusion to approximate the dynamic of SGD. Then, we prove that the stationary distribution of power-law dynamic is heavy-tailed, which matches the existing empirical observations. Next, we study the escaping efficiency from local minimum of power-law dynamic and prove that the mean escaping time is in polynomial order of the barrier height of the basin, much faster than exponential order of previous dynamics. It indicates that SGD can escape deep sharp minima efficiently and tends to stop at flat minima that have lower generalization error. Finally, we conduct experiments to compare SGD and power-law dynamic, and the results verify our theoretical findings.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "meng|dynamic_of_stochastic_gradient_descent_with_statedependent_noise", "one-sentence_summary": "We propose a novel power-law dynamic with state-dependent diffusion to approximate the dynamic of SGD, and analyze escaping efficiency and PAC-Bayesian generalization bound for it.", "pdf": "/pdf/178eaecb04429fe8847d55f837b1b008857148df.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=ryrEpL90e", "_bibtex": "@misc{\nmeng2021dynamic,\ntitle={Dynamic of Stochastic Gradient Descent with State-dependent Noise},\nauthor={Qi Meng and Shiqi Gong and Wei Chen and Zhi-Ming Ma and Tie-Yan Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=Bpw_O132lWT}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Bpw_O132lWT", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1418/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1418/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1418/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1418/Authors|ICLR.cc/2021/Conference/Paper1418/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1418/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923859942, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1418/-/Official_Comment"}}}, {"id": "R6VTScHsT1", "original": null, "number": 4, "cdate": 1605868340153, "ddate": null, "tcdate": 1605868340153, "tmdate": 1605868531683, "tddate": null, "forum": "Bpw_O132lWT", "replyto": "0KhY94zUnkY", "invitation": "ICLR.cc/2021/Conference/Paper1418/-/Official_Comment", "content": {"title": "Response to Reviewer #3", "comment": "Thank you very much for your supportive review and constructive comments. Especially thanks for recognizing our work along this research direction. Our results are established by assuming that the loss is equal to quadratic function near the critical points. To make this explicit, we re-organize all the assumptions for escaping time and move them to the beginning of section 4. Thanks for pointing out the typos. We have revised them in the updated version. Here are our responses to your other questions about the paper. If any further confusing, please feel free to let us know.\n1. \u201cThe solution of Langevin equation is Gaussian distribution.\u201d What does it mean?\n\nThanks for your careful reading. As defined in Eqn (1), the diffusion term in Langevin dynamic (investigated in machine learning) is constant. The solution of the (specific) Langevin equation with constant diffusion is Gaussian process. Its stationary distribution is Gaussian distribution. We revised this claim to be accurate in the updated version. \n\n2. In Theorem 4, it is not stated that $H=H(w^*)$. This is because we are talking about an $\\epsilon$-ball around $w^*$ and tending $w\\rightarrow\\infty$ has no meaning.... Also, it seems that the distribution is defined only for positive w.\n\na. We have added \u201c$H=H(w^*)$\u201d in Theorem 4 in the updated version. \n\nb. In the updated version, instead of taking $w\\rightarrow\\infty$, we discuss about the decreasing rate of p(w) in the region $[w^*-\\epsilon, w^*+\\epsilon]$, which also indicates power-law distribution is less concentrated in the quadratic basin and heavy-tailed. \n\nc. The distribution in Eqn (9) is defined for $w\\in[w^*-\\epsilon, w^*+\\epsilon]$, not only for positive w. It is symmetric and positive with respect to $(w-w^*)$. \n\n3. \u201cit is assumed that the basin is quadratic and stays quadratic, even when it reaches the saddle point.\u201d\n\nWe assume that the loss surface near critical points (including local points and saddle points) can be approximated by the second order Taylor expansion. For saddle points, the Hessian has one negative eigenvalue. Thus, the loss surface along the whole escaping path from a local point to the saddle point near it is not quadratic. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1418/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1418/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic of Stochastic Gradient Descent with State-dependent Noise", "authorids": ["~Qi_Meng1", "~Shiqi_Gong1", "~Wei_Chen1", "~Zhi-Ming_Ma1", "~Tie-Yan_Liu1"], "authors": ["Qi Meng", "Shiqi Gong", "Wei Chen", "Zhi-Ming Ma", "Tie-Yan Liu"], "keywords": ["state-dependent noise", "power-law dynamic", "stochastic gradient descent", "generalization", "deep neural network", "heavy-tailed", "escape time"], "abstract": "Stochastic gradient descent (SGD) and its variants are mainstream methods to train deep neural networks. Since neural networks are non-convex, more and more works study the dynamic behavior of SGD and its impact to generalization, especially the escaping efficiency from local minima. However, these works make the over-simplified assumption that the distribution of gradient noise is state-independent, although it is state-dependent. In this work, we propose a novel power-law dynamic with state-dependent diffusion to approximate the dynamic of SGD. Then, we prove that the stationary distribution of power-law dynamic is heavy-tailed, which matches the existing empirical observations. Next, we study the escaping efficiency from local minimum of power-law dynamic and prove that the mean escaping time is in polynomial order of the barrier height of the basin, much faster than exponential order of previous dynamics. It indicates that SGD can escape deep sharp minima efficiently and tends to stop at flat minima that have lower generalization error. Finally, we conduct experiments to compare SGD and power-law dynamic, and the results verify our theoretical findings.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "meng|dynamic_of_stochastic_gradient_descent_with_statedependent_noise", "one-sentence_summary": "We propose a novel power-law dynamic with state-dependent diffusion to approximate the dynamic of SGD, and analyze escaping efficiency and PAC-Bayesian generalization bound for it.", "pdf": "/pdf/178eaecb04429fe8847d55f837b1b008857148df.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=ryrEpL90e", "_bibtex": "@misc{\nmeng2021dynamic,\ntitle={Dynamic of Stochastic Gradient Descent with State-dependent Noise},\nauthor={Qi Meng and Shiqi Gong and Wei Chen and Zhi-Ming Ma and Tie-Yan Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=Bpw_O132lWT}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Bpw_O132lWT", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1418/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1418/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1418/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1418/Authors|ICLR.cc/2021/Conference/Paper1418/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1418/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923859942, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1418/-/Official_Comment"}}}, {"id": "79ZK_mPsaLM", "original": null, "number": 3, "cdate": 1605867950711, "ddate": null, "tcdate": 1605867950711, "tmdate": 1605867950711, "tddate": null, "forum": "Bpw_O132lWT", "replyto": "Bpw_O132lWT", "invitation": "ICLR.cc/2021/Conference/Paper1418/-/Official_Comment", "content": {"title": "Paper Revision", "comment": "We appreciate all the reviewers for their great efforts on our paper and the constructive comments to us. \nWe have updated our manuscript according to reviewers\u2019 comments. The changes we made mainly include the followings. \n1. Assumptions: we reorganize all the assumptions for escaping time and move them to the beginning of section 4. \n2. Theorem 2: we give the expression of C(w) in the theorem to make it self-contained and put its proof in Appendix 7.1. We discuss the decreasing rate of p(w) in the local region of $w^*$ instead of taking $w\\rightarrow\\infty$, which indicates power-law distribution is less concentrated in the quadratic basin and heavy-tailed.\n3. Multi-dimensional case: We provide more explanations about the results for multi-dimensional case in remark after Theorem 7 and the assumption on $\\kappa$ in Appendix 7.2.  \n4. Experiments: we add the discussion for Figure 3(b) in Sec 5.1. We illustrate that power-law distribution can better fit the parameter distribution than Gaussian in Figure 8 in Appendix 7.5.2. We add the experiments on the escaping efficiency on practical networks in Appendix 7.5.4.\n5. References and typos: we modify the citations for Langevin dynamic and cite [Wu, et al, 2018]. We correct the typos.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1418/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1418/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic of Stochastic Gradient Descent with State-dependent Noise", "authorids": ["~Qi_Meng1", "~Shiqi_Gong1", "~Wei_Chen1", "~Zhi-Ming_Ma1", "~Tie-Yan_Liu1"], "authors": ["Qi Meng", "Shiqi Gong", "Wei Chen", "Zhi-Ming Ma", "Tie-Yan Liu"], "keywords": ["state-dependent noise", "power-law dynamic", "stochastic gradient descent", "generalization", "deep neural network", "heavy-tailed", "escape time"], "abstract": "Stochastic gradient descent (SGD) and its variants are mainstream methods to train deep neural networks. Since neural networks are non-convex, more and more works study the dynamic behavior of SGD and its impact to generalization, especially the escaping efficiency from local minima. However, these works make the over-simplified assumption that the distribution of gradient noise is state-independent, although it is state-dependent. In this work, we propose a novel power-law dynamic with state-dependent diffusion to approximate the dynamic of SGD. Then, we prove that the stationary distribution of power-law dynamic is heavy-tailed, which matches the existing empirical observations. Next, we study the escaping efficiency from local minimum of power-law dynamic and prove that the mean escaping time is in polynomial order of the barrier height of the basin, much faster than exponential order of previous dynamics. It indicates that SGD can escape deep sharp minima efficiently and tends to stop at flat minima that have lower generalization error. Finally, we conduct experiments to compare SGD and power-law dynamic, and the results verify our theoretical findings.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "meng|dynamic_of_stochastic_gradient_descent_with_statedependent_noise", "one-sentence_summary": "We propose a novel power-law dynamic with state-dependent diffusion to approximate the dynamic of SGD, and analyze escaping efficiency and PAC-Bayesian generalization bound for it.", "pdf": "/pdf/178eaecb04429fe8847d55f837b1b008857148df.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=ryrEpL90e", "_bibtex": "@misc{\nmeng2021dynamic,\ntitle={Dynamic of Stochastic Gradient Descent with State-dependent Noise},\nauthor={Qi Meng and Shiqi Gong and Wei Chen and Zhi-Ming Ma and Tie-Yan Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=Bpw_O132lWT}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Bpw_O132lWT", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1418/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1418/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1418/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1418/Authors|ICLR.cc/2021/Conference/Paper1418/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1418/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923859942, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1418/-/Official_Comment"}}}, {"id": "qevyo7b-1p", "original": null, "number": 1, "cdate": 1602572678481, "ddate": null, "tcdate": 1602572678481, "tmdate": 1605024450724, "tddate": null, "forum": "Bpw_O132lWT", "replyto": "Bpw_O132lWT", "invitation": "ICLR.cc/2021/Conference/Paper1418/-/Official_Review", "content": {"title": "Novel perspective, but left with much ambiguous analysis and insufficient empirical justifications", "review": "This paper proposes to use power-law dynamics to approximate the state-dependent gradient noise in SGD, and analyses its escaping efficiency compared with previous dynamics. \n\nStrength:\n1.\tTo the best of my knowledge, it is novel to use power-law dynamics to analyze the state-dependent noise in SGD. \n2.\tStill with strong assumptions on covariance structure, the analytical results based on power-dynamics are interesting. For example, it indicates that so-called kappa distribution highly depends on the fluctuations to the curvature over the training data. This is consistent with following work. So I suggest authors provide some discussion with the following work.\nWu et.al 2018. How sgd selects the global minima in over-parameterized learning: A dynamical stability perspective. In Advances in Neural Information Processing Systems (pp. 8279-8288).\n\nWeakness & Issues \n1.\tThe analytical results seem that they strongly depend on the covariance structure assumption, i.e. C(w) is diagonally dominant according to empirical observation. Does it have any theoretical justifications, or even in simplified cases? \n2.\tThe delivered PAC generalization bound and the followed analysis are a little ambiguous.  Firstly, in current deep learning theory community, the relationship between flatness (even how to define a proper flatness) and generalization is still mysterious and controversial, which depends many factors. This work uses one type of flatness measure, the determinant of H, and shows that flatter minima generalize better by only considering the KL term. However, the first term also includes the Hessian and might also affect generalization bound. Thus, the conclusion appears a little problematic. \nThe authors said that generalization error will decrease w.r.t. kappa\u2019s increase and infinite kappa results in Langevin dynamics. Then the question is what are the difference between the power-law dynamics and Langevin dynamics in term of generalization? \nMy view on the ambiguous analysis is that the authors attempt to answer extremely challenging questions but left with many questionable concerns. \n3.\tThe experiments might not be sufficient. \nI don\u2019t think fitting the parameter distribution according to limited empirical observations is an appropriate way to make justifications. At least, from visual observation, there are many other alternatives besides power-law distribution to fit, as Fig 3 shows. \nAbout comparing the escaping efficiency, the result only shows the success rate, and the evidence about the polynomial and exponential difference should be provided. Also, practical networks and datasets should also be considered to provide more strong evidence.\n\nIf the authors can resolve these issues carefully, I would raise the score. \n\nTypos\n\u201cEq. 4\u201d should be \u201cEq.3\u201d below equation 3\n", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper1418/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1418/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic of Stochastic Gradient Descent with State-dependent Noise", "authorids": ["~Qi_Meng1", "~Shiqi_Gong1", "~Wei_Chen1", "~Zhi-Ming_Ma1", "~Tie-Yan_Liu1"], "authors": ["Qi Meng", "Shiqi Gong", "Wei Chen", "Zhi-Ming Ma", "Tie-Yan Liu"], "keywords": ["state-dependent noise", "power-law dynamic", "stochastic gradient descent", "generalization", "deep neural network", "heavy-tailed", "escape time"], "abstract": "Stochastic gradient descent (SGD) and its variants are mainstream methods to train deep neural networks. Since neural networks are non-convex, more and more works study the dynamic behavior of SGD and its impact to generalization, especially the escaping efficiency from local minima. However, these works make the over-simplified assumption that the distribution of gradient noise is state-independent, although it is state-dependent. In this work, we propose a novel power-law dynamic with state-dependent diffusion to approximate the dynamic of SGD. Then, we prove that the stationary distribution of power-law dynamic is heavy-tailed, which matches the existing empirical observations. Next, we study the escaping efficiency from local minimum of power-law dynamic and prove that the mean escaping time is in polynomial order of the barrier height of the basin, much faster than exponential order of previous dynamics. It indicates that SGD can escape deep sharp minima efficiently and tends to stop at flat minima that have lower generalization error. Finally, we conduct experiments to compare SGD and power-law dynamic, and the results verify our theoretical findings.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "meng|dynamic_of_stochastic_gradient_descent_with_statedependent_noise", "one-sentence_summary": "We propose a novel power-law dynamic with state-dependent diffusion to approximate the dynamic of SGD, and analyze escaping efficiency and PAC-Bayesian generalization bound for it.", "pdf": "/pdf/178eaecb04429fe8847d55f837b1b008857148df.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=ryrEpL90e", "_bibtex": "@misc{\nmeng2021dynamic,\ntitle={Dynamic of Stochastic Gradient Descent with State-dependent Noise},\nauthor={Qi Meng and Shiqi Gong and Wei Chen and Zhi-Ming Ma and Tie-Yan Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=Bpw_O132lWT}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Bpw_O132lWT", "replyto": "Bpw_O132lWT", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1418/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538119120, "tmdate": 1606915790914, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1418/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1418/-/Official_Review"}}}, {"id": "0KhY94zUnkY", "original": null, "number": 3, "cdate": 1603893124705, "ddate": null, "tcdate": 1603893124705, "tmdate": 1605024450555, "tddate": null, "forum": "Bpw_O132lWT", "replyto": "Bpw_O132lWT", "invitation": "ICLR.cc/2021/Conference/Paper1418/-/Official_Review", "content": {"title": "Official Review of Reviewer #3", "review": "### Summary\nThis paper proposes an analysis for an approximate dynamic of SGD which captures the heavy-tailed noise distributions seen practically at local minima. The authors derive this new dynamic (which they call Power-law dynamic) using basic principles and the assumption that the noise variance depends on the state. The dynamics becomes a modified Langevin equation. They prove also that the expected time to escape a barrier is polynomial in the parameters, as well as a generalization error bound.\n\n### Strong/Weak points\n- The paper is built up on simple principles\n- It first gives a one-dimensional analysis then generalize, helping the reader to understand\n- Except a few points, in general the paper is well-written.\n- The assumptions made are somewhat strong and may not hold in some cases, see below.\n\nIn general I have a tendency to accept this paper. Even though there are crucial assumptions that are made, it can be considered as a first step towards a more rigorous and general argument.  \n\nHere are a few points that I have problems with in the paper:\n- On page 3, paragraph 2, it is written that the solution of Langevin equation is Gaussian distribution. What does it mean? The solution of a SDE is a Markov process, and considering the distribution of the process at time $t$, it is not necessarily Gaussian; the Fokker-Planck equation governs the change of distribution, having the Gibbs distribution as its stationary distribution, which is not Gaussian in general.\n- The whole argument is made through assuming that near the basin, everything is quadratic (not approximately, equal!). This is completely reflected in Proposition 1 and the further analysis.\n- In Theorem 4, it is not stated that $H = H(w^*)$, and I don't see why should one be interested when $w\\to\\infty$? This is because we are talking about an $\\epsilon$ ball around $w^*$ and tending $w \\to \\infty$ has no meaning.... Maybe I am missing something here? Also, it seems that the distribution is defined only for positive $w$.\n- In the argument for Section 4 (escaping), it is assumed that the basin is quadratic and __stays__ quadratic, even when it reaches the saddle point. I find this assumption flawed, or I am missing something.\n- At the bottom of page 3, it is said \"in this case, .... is satisfied\", while I think it should be \"not satisfied\".\n- On page 4, the notion $\\rightarrow_p$ is used for convergence in distribution, which is not usual and is reserved for convergence in probability.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1418/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1418/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic of Stochastic Gradient Descent with State-dependent Noise", "authorids": ["~Qi_Meng1", "~Shiqi_Gong1", "~Wei_Chen1", "~Zhi-Ming_Ma1", "~Tie-Yan_Liu1"], "authors": ["Qi Meng", "Shiqi Gong", "Wei Chen", "Zhi-Ming Ma", "Tie-Yan Liu"], "keywords": ["state-dependent noise", "power-law dynamic", "stochastic gradient descent", "generalization", "deep neural network", "heavy-tailed", "escape time"], "abstract": "Stochastic gradient descent (SGD) and its variants are mainstream methods to train deep neural networks. Since neural networks are non-convex, more and more works study the dynamic behavior of SGD and its impact to generalization, especially the escaping efficiency from local minima. However, these works make the over-simplified assumption that the distribution of gradient noise is state-independent, although it is state-dependent. In this work, we propose a novel power-law dynamic with state-dependent diffusion to approximate the dynamic of SGD. Then, we prove that the stationary distribution of power-law dynamic is heavy-tailed, which matches the existing empirical observations. Next, we study the escaping efficiency from local minimum of power-law dynamic and prove that the mean escaping time is in polynomial order of the barrier height of the basin, much faster than exponential order of previous dynamics. It indicates that SGD can escape deep sharp minima efficiently and tends to stop at flat minima that have lower generalization error. Finally, we conduct experiments to compare SGD and power-law dynamic, and the results verify our theoretical findings.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "meng|dynamic_of_stochastic_gradient_descent_with_statedependent_noise", "one-sentence_summary": "We propose a novel power-law dynamic with state-dependent diffusion to approximate the dynamic of SGD, and analyze escaping efficiency and PAC-Bayesian generalization bound for it.", "pdf": "/pdf/178eaecb04429fe8847d55f837b1b008857148df.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=ryrEpL90e", "_bibtex": "@misc{\nmeng2021dynamic,\ntitle={Dynamic of Stochastic Gradient Descent with State-dependent Noise},\nauthor={Qi Meng and Shiqi Gong and Wei Chen and Zhi-Ming Ma and Tie-Yan Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=Bpw_O132lWT}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Bpw_O132lWT", "replyto": "Bpw_O132lWT", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1418/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538119120, "tmdate": 1606915790914, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1418/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1418/-/Official_Review"}}}, {"id": "XaiXgknkrs5", "original": null, "number": 4, "cdate": 1604265187131, "ddate": null, "tcdate": 1604265187131, "tmdate": 1605024450482, "tddate": null, "forum": "Bpw_O132lWT", "replyto": "Bpw_O132lWT", "invitation": "ICLR.cc/2021/Conference/Paper1418/-/Official_Review", "content": {"title": "Review of Dynamic of Stochastic Gradient Descent with State-dependent Noise ", "review": "This paper proposes power-law dynamic of SGD which considers state-dependent noise. The power-law distributed derived from this  dynamic explains the heavy-tailed distribution of parameters trained by SGD. Besides, this dynamic also shows efficiency of escaping local minima.\n\nConcerns:\n1. The proof of theorem 2 is not provided in the appendix. But I doubt if C(w) is well-defined. It is not clear how w* is selected considering there are multiple local minima. It does not make sense to me if w* is fixed when taking x-->\\infty, as the quadratic approximation should be used in the neighborhood of w*. \n\n2. The escaping efficiency of the power-law dynamic is only analyzed in low-dimension case. I wonder how if performs in high-dimensional space. Does it provide more benefits than Langevin/alpha-stable dynamic in the expense of calculating sigma_g and sigma_H.\n\nMinor comments:\n1. I think  [Li et al., 2017] also proposed state-dependent noise in Theorem 1.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1418/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1418/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic of Stochastic Gradient Descent with State-dependent Noise", "authorids": ["~Qi_Meng1", "~Shiqi_Gong1", "~Wei_Chen1", "~Zhi-Ming_Ma1", "~Tie-Yan_Liu1"], "authors": ["Qi Meng", "Shiqi Gong", "Wei Chen", "Zhi-Ming Ma", "Tie-Yan Liu"], "keywords": ["state-dependent noise", "power-law dynamic", "stochastic gradient descent", "generalization", "deep neural network", "heavy-tailed", "escape time"], "abstract": "Stochastic gradient descent (SGD) and its variants are mainstream methods to train deep neural networks. Since neural networks are non-convex, more and more works study the dynamic behavior of SGD and its impact to generalization, especially the escaping efficiency from local minima. However, these works make the over-simplified assumption that the distribution of gradient noise is state-independent, although it is state-dependent. In this work, we propose a novel power-law dynamic with state-dependent diffusion to approximate the dynamic of SGD. Then, we prove that the stationary distribution of power-law dynamic is heavy-tailed, which matches the existing empirical observations. Next, we study the escaping efficiency from local minimum of power-law dynamic and prove that the mean escaping time is in polynomial order of the barrier height of the basin, much faster than exponential order of previous dynamics. It indicates that SGD can escape deep sharp minima efficiently and tends to stop at flat minima that have lower generalization error. Finally, we conduct experiments to compare SGD and power-law dynamic, and the results verify our theoretical findings.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "meng|dynamic_of_stochastic_gradient_descent_with_statedependent_noise", "one-sentence_summary": "We propose a novel power-law dynamic with state-dependent diffusion to approximate the dynamic of SGD, and analyze escaping efficiency and PAC-Bayesian generalization bound for it.", "pdf": "/pdf/178eaecb04429fe8847d55f837b1b008857148df.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=ryrEpL90e", "_bibtex": "@misc{\nmeng2021dynamic,\ntitle={Dynamic of Stochastic Gradient Descent with State-dependent Noise},\nauthor={Qi Meng and Shiqi Gong and Wei Chen and Zhi-Ming Ma and Tie-Yan Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=Bpw_O132lWT}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Bpw_O132lWT", "replyto": "Bpw_O132lWT", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1418/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538119120, "tmdate": 1606915790914, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1418/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1418/-/Official_Review"}}}], "count": 13}