{"notes": [{"id": "q-cnWaaoUTH", "original": "sp6wrF0rCkA", "number": 636, "cdate": 1601308075782, "ddate": null, "tcdate": 1601308075782, "tmdate": 1614583934867, "tddate": null, "forum": "q-cnWaaoUTH", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Conformation-Guided Molecular Representation with Hamiltonian Neural Networks", "authorids": ["~Ziyao_Li1", "swyang@pku.edu.cn", "~Guojie_Song1", "cailingsheng@pku.edu.cn"], "authors": ["Ziyao Li", "Shuwen Yang", "Guojie Song", "Lingsheng Cai"], "keywords": ["Molecular Representation", "Neural Physics Engines", "Molecular Dynamics", "Graph Neural Networks"], "abstract": "Well-designed molecular representations (fingerprints) are vital to combine medical chemistry and deep learning. Whereas incorporating 3D geometry of molecules (i.e. conformations) in their representations seems beneficial, current 3D algorithms are still in infancy. In this paper, we propose a novel molecular representation algorithm which preserves 3D conformations of molecules with a Molecular Hamiltonian Network (HamNet). In HamNet, implicit positions and momentums of atoms in a molecule interact in the Hamiltonian Engine following the discretized Hamiltonian equations. These implicit coordinations are supervised with real conformations with translation- & rotation-invariant losses, and further used as inputs to the Fingerprint Generator, a message-passing neural network. Experiments show that the Hamiltonian Engine can well preserve molecular conformations, and that the fingerprints generated by HamNet achieve state-of-the-art performances on MoleculeNet, a standard molecular machine learning benchmark.", "one-sentence_summary": "We propose a molecular representation algorithm, which preserves molecular conformations with a neural physics engine and generates fingerprints with an MPNN.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|conformationguided_molecular_representation_with_hamiltonian_neural_networks", "supplementary_material": "/attachment/c3a6d96b83a7cf2111b0f63066d54dbb42fb1009.zip", "pdf": "/pdf/7a5a25fdbe36c7b0286d17dafd233f47bf7dd30c.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nli2021conformationguided,\ntitle={Conformation-Guided Molecular Representation with Hamiltonian Neural Networks},\nauthor={Ziyao Li and Shuwen Yang and Guojie Song and Lingsheng Cai},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=q-cnWaaoUTH}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "c_JrvGYIeSR", "original": null, "number": 1, "cdate": 1614581799875, "ddate": null, "tcdate": 1614581799875, "tmdate": 1614581799875, "tddate": null, "forum": "q-cnWaaoUTH", "replyto": "q-cnWaaoUTH", "invitation": "ICLR.cc/2021/Conference/Paper636/-/Comment", "content": {"title": "Fixed experimental results.", "comment": "We fixed some bugs in the experiments of Table 2, namely those of FreeSolv and ESOL.\n\nAs a result, some figures in Table 2 were altered, while the advantage of HamNet was still corroborated.\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper636/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper636/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Conformation-Guided Molecular Representation with Hamiltonian Neural Networks", "authorids": ["~Ziyao_Li1", "swyang@pku.edu.cn", "~Guojie_Song1", "cailingsheng@pku.edu.cn"], "authors": ["Ziyao Li", "Shuwen Yang", "Guojie Song", "Lingsheng Cai"], "keywords": ["Molecular Representation", "Neural Physics Engines", "Molecular Dynamics", "Graph Neural Networks"], "abstract": "Well-designed molecular representations (fingerprints) are vital to combine medical chemistry and deep learning. Whereas incorporating 3D geometry of molecules (i.e. conformations) in their representations seems beneficial, current 3D algorithms are still in infancy. In this paper, we propose a novel molecular representation algorithm which preserves 3D conformations of molecules with a Molecular Hamiltonian Network (HamNet). In HamNet, implicit positions and momentums of atoms in a molecule interact in the Hamiltonian Engine following the discretized Hamiltonian equations. These implicit coordinations are supervised with real conformations with translation- & rotation-invariant losses, and further used as inputs to the Fingerprint Generator, a message-passing neural network. Experiments show that the Hamiltonian Engine can well preserve molecular conformations, and that the fingerprints generated by HamNet achieve state-of-the-art performances on MoleculeNet, a standard molecular machine learning benchmark.", "one-sentence_summary": "We propose a molecular representation algorithm, which preserves molecular conformations with a neural physics engine and generates fingerprints with an MPNN.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|conformationguided_molecular_representation_with_hamiltonian_neural_networks", "supplementary_material": "/attachment/c3a6d96b83a7cf2111b0f63066d54dbb42fb1009.zip", "pdf": "/pdf/7a5a25fdbe36c7b0286d17dafd233f47bf7dd30c.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nli2021conformationguided,\ntitle={Conformation-Guided Molecular Representation with Hamiltonian Neural Networks},\nauthor={Ziyao Li and Shuwen Yang and Guojie Song and Lingsheng Cai},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=q-cnWaaoUTH}\n}"}, "tags": [], "invitation": {"reply": {"forum": "q-cnWaaoUTH", "readers": {"values": ["everyone"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2021/Conference/Paper636/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper636/Authors|ICLR.cc/2021/Conference/Paper636/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs"}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}}, "multiReply": true, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["everyone"], "tcdate": 1610649458089, "tmdate": 1610649458089, "id": "ICLR.cc/2021/Conference/Paper636/-/Comment"}}}, {"id": "dM3gW71GQQg", "original": null, "number": 1, "cdate": 1610040449801, "ddate": null, "tcdate": 1610040449801, "tmdate": 1610474051906, "tddate": null, "forum": "q-cnWaaoUTH", "replyto": "q-cnWaaoUTH", "invitation": "ICLR.cc/2021/Conference/Paper636/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Poster)", "comment": "The paper proposes to lear molecular descriptors that account for the 3D structure of molecules. This is done by using first a \"Hamiltonian Engine\" that runs a brief simulation, predicting the structure of the small molecule by minimizing a learned potential energy, and second, a message passing algorithm that uses the predicted structure as input. The reported experimental results show state-of-the-art performance.\n\nStrengths:\n\n1 - Relevant contribution through the Hamiltonian Engine.\n\n2 - Strong empirical results.\n\nWeaknesses:\n\n3 - Some reviewers mentioned that the readability of the paper could be improved.\n\nI recommend the authors to also take into account the concerns of AnonReviewer1 to\nimprove the paper."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Conformation-Guided Molecular Representation with Hamiltonian Neural Networks", "authorids": ["~Ziyao_Li1", "swyang@pku.edu.cn", "~Guojie_Song1", "cailingsheng@pku.edu.cn"], "authors": ["Ziyao Li", "Shuwen Yang", "Guojie Song", "Lingsheng Cai"], "keywords": ["Molecular Representation", "Neural Physics Engines", "Molecular Dynamics", "Graph Neural Networks"], "abstract": "Well-designed molecular representations (fingerprints) are vital to combine medical chemistry and deep learning. Whereas incorporating 3D geometry of molecules (i.e. conformations) in their representations seems beneficial, current 3D algorithms are still in infancy. In this paper, we propose a novel molecular representation algorithm which preserves 3D conformations of molecules with a Molecular Hamiltonian Network (HamNet). In HamNet, implicit positions and momentums of atoms in a molecule interact in the Hamiltonian Engine following the discretized Hamiltonian equations. These implicit coordinations are supervised with real conformations with translation- & rotation-invariant losses, and further used as inputs to the Fingerprint Generator, a message-passing neural network. Experiments show that the Hamiltonian Engine can well preserve molecular conformations, and that the fingerprints generated by HamNet achieve state-of-the-art performances on MoleculeNet, a standard molecular machine learning benchmark.", "one-sentence_summary": "We propose a molecular representation algorithm, which preserves molecular conformations with a neural physics engine and generates fingerprints with an MPNN.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|conformationguided_molecular_representation_with_hamiltonian_neural_networks", "supplementary_material": "/attachment/c3a6d96b83a7cf2111b0f63066d54dbb42fb1009.zip", "pdf": "/pdf/7a5a25fdbe36c7b0286d17dafd233f47bf7dd30c.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nli2021conformationguided,\ntitle={Conformation-Guided Molecular Representation with Hamiltonian Neural Networks},\nauthor={Ziyao Li and Shuwen Yang and Guojie Song and Lingsheng Cai},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=q-cnWaaoUTH}\n}"}, "tags": [], "invitation": {"reply": {"forum": "q-cnWaaoUTH", "replyto": "q-cnWaaoUTH", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040449788, "tmdate": 1610474051890, "id": "ICLR.cc/2021/Conference/Paper636/-/Decision"}}}, {"id": "H1-zhj8otJi", "original": null, "number": 3, "cdate": 1603889918913, "ddate": null, "tcdate": 1603889918913, "tmdate": 1606741788836, "tddate": null, "forum": "q-cnWaaoUTH", "replyto": "q-cnWaaoUTH", "invitation": "ICLR.cc/2021/Conference/Paper636/-/Official_Review", "content": {"title": "Interesting approach for incorporate 3d inforamtion in molecular fingerprints, but lacking results", "review": "This paper proposes to use 3d conformations for learning molecular fingerprints by 1) training a generative model to predict the 3d coordinates and 2) use those to train a \"fingerprint generator\" to obtain fingerprints by learning to predict molecular properties. The paper is well structured and clearly written. The intuition behind the Hamiltonian engine is nicely explained in the discussion section. The idea of a two step procedure to include 3d structure information in the training even when it is not available at test time is interesting and original. \n\nUnfortunately, some of the design decisions did not became quite clear to me: Why is the relaxation modeled with an MD-like dynamics approach instead of structure optimization, i.e. following the gradient of the energy? Although, here is an ablation study for leaving out the dynamics that performs worse, this might rather indicate that the initial guess of the positions is not good enough to find the correct equilibrium structure. Given the construction of the network that generates the initial positions, this seems quite likely, since it uses some canonical ordering (no permutation invariance) and does not include some notion of rotational symmetry. In contrast, there is previous work that demonstrates that this is possible with autoregressive models (Mansimov et al, Sci Rep, 2019; Gebauer et al, NeurIPS 2019). These methods also achieve much lower RMSDs than the presented approach on QM9 data (both <0.5 A RMSD). Another baseline would be to find a good positional guess, e.g. with RDkit, and then optimize with an ML force field that has already been used for MD (Schuett et al, J Chem Phys, 2019) or relaxation in crystal structure prediction (Podryabinkin, Phys. Rev. B, 2019). \nMoreover, since the datasets without 3d positions are labeled with RDkit (which i guess uses the same method as in the RDKit baseline of Table 1?), it would be interesting to see whether these structures would also be sufficient for the property prediction of QM9 in Tab. 2. Currently, I am not convinced that the more accurate positions (w.r.t. DFT equilibrium) obtained by HamNet actually improve the prediction that much. In particular, since the improvement compared to prediction without conformations in Table 2 is not significant according to the reported error bars.\n\nPros\n------\n+ Interesting idea, clearly written paper\n+ Physically motivated approach\n+ Improvements against simple baselines and ablations\n\nCons\n-------\n- Missing of important baselines (see above)\n- Only \"multi-mae\" reported for QM9. This makes it hard to judge the quality of the prediction and compare to predictions of individual properties in many other papers (DimeNet, MPNN, SchNet, HIP-NN, DTNN, PhysNet, etc). Please list also individual metrics and compare to previous work.\n- Datasets without 3d positions are labeled with a computationally cheap method from RDKit.  This does not make much sense: Why then not directly use these positions?\n- Timings for the prediction should be added to judge computational cost compared to a fast force-field or semi-empirical method.\n\nUpdate:\nThe responses cleared up some aspects of the Hamiltonian engine and I adjusted my score accordingly. Still, additional baselines would improve the paper answer some open questions and validate some of the claims: Is the Ham. Eng. indeed faster than optimizing with a cheap force field? Is there then an advantage compared to featurization of the RDKit coordinates with one of the many MPNNs for 3d coordinates? This would be interesting to analyze.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper636/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper636/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Conformation-Guided Molecular Representation with Hamiltonian Neural Networks", "authorids": ["~Ziyao_Li1", "swyang@pku.edu.cn", "~Guojie_Song1", "cailingsheng@pku.edu.cn"], "authors": ["Ziyao Li", "Shuwen Yang", "Guojie Song", "Lingsheng Cai"], "keywords": ["Molecular Representation", "Neural Physics Engines", "Molecular Dynamics", "Graph Neural Networks"], "abstract": "Well-designed molecular representations (fingerprints) are vital to combine medical chemistry and deep learning. Whereas incorporating 3D geometry of molecules (i.e. conformations) in their representations seems beneficial, current 3D algorithms are still in infancy. In this paper, we propose a novel molecular representation algorithm which preserves 3D conformations of molecules with a Molecular Hamiltonian Network (HamNet). In HamNet, implicit positions and momentums of atoms in a molecule interact in the Hamiltonian Engine following the discretized Hamiltonian equations. These implicit coordinations are supervised with real conformations with translation- & rotation-invariant losses, and further used as inputs to the Fingerprint Generator, a message-passing neural network. Experiments show that the Hamiltonian Engine can well preserve molecular conformations, and that the fingerprints generated by HamNet achieve state-of-the-art performances on MoleculeNet, a standard molecular machine learning benchmark.", "one-sentence_summary": "We propose a molecular representation algorithm, which preserves molecular conformations with a neural physics engine and generates fingerprints with an MPNN.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|conformationguided_molecular_representation_with_hamiltonian_neural_networks", "supplementary_material": "/attachment/c3a6d96b83a7cf2111b0f63066d54dbb42fb1009.zip", "pdf": "/pdf/7a5a25fdbe36c7b0286d17dafd233f47bf7dd30c.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nli2021conformationguided,\ntitle={Conformation-Guided Molecular Representation with Hamiltonian Neural Networks},\nauthor={Ziyao Li and Shuwen Yang and Guojie Song and Lingsheng Cai},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=q-cnWaaoUTH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "q-cnWaaoUTH", "replyto": "q-cnWaaoUTH", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper636/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538138701, "tmdate": 1606915786671, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper636/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper636/-/Official_Review"}}}, {"id": "jOcmiQCC7EW", "original": null, "number": 4, "cdate": 1606057599591, "ddate": null, "tcdate": 1606057599591, "tmdate": 1606058013969, "tddate": null, "forum": "q-cnWaaoUTH", "replyto": "H1-zhj8otJi", "invitation": "ICLR.cc/2021/Conference/Paper636/-/Official_Comment", "content": {"title": "Official reply to Reviewer 1", "comment": "We thank you for the insightful comments. For the major comments, below are our responses:\n\n1. **Why MD instead of structure optimization?**\n\nFirstly, as is discussed in Section 3.3, the MD-like dynamics is essentially an implicit process of structure  optimization: in fact, the Hamiltonian equations depict exactly the *gradient descending with momentum* (MGD) process. The process of MGD of optimizing $f(q)$ against $q$ follows\n$$\nq_t = q_{t-1} + \\eta_t p_{t},\n$$\n$$\np_t = p_{t-1} + \\epsilon_t \\nabla_q f(q).\n$$\nThese descending steps are exactly the same as what Hamiltonian equations depict when $H(q; p)=T(p) + U(q)$ (let $f(q)=U(q)$) and $T(p)$ is the quadratic form of $p$ (so that $\\partial H / \\partial p \\propto p$). Note that MGD is generally a more robust and faster optimization algorithm than simple GD.\n\nSecondly, different with current PES (potential energy surface) approaches , HamNet do not learn parameters in the potential functions from known (labeled) potential energies or forces. Instead, the output positions are directly supervised. This requires that the entire optimization process to be differentiable. Therefore, we implement the MD process, which is differentiable w.r.t the parameters in function $T$ and $U$. Also, another desired byproduct of using MD, however, is the better interpretability of the model.\n\n2. **Why Hamiltonian Engine at all?**\n\nTo address your concerns on 1) missing baselines of recent conformation generation algorithms (e.g. ETKDG, or CVGAE, G-SchNet et al as you mentioned); 2) (potentially) worse results than these baselines on conformation prediction tasks, we would like to emphasize that the ultimate goal of HamNet is to **derive good molecular fingerprints** rather than to *accurately predict the conformation in equilibrium*. Showing the results of Ham. Eng. vs RDKit on conformation prediction is to prove that the model correctly captures the conformation in $q,p \\in R^{d_f}$, instead of arguing that Ham. Eng. is a state-of-the-art conformation prediction approach.\n\n**We believe to better understand the role of Ham. Eng., one may regard Ham. Eng. as a featurization approach from a encoder-decoder view**: the encoder calculates the hidden representations ($q^{(T)}, p^{(T)}$) from molecular graphs, and the decoder supervises $q^{(T)}$ with labeled conformations. The reason we need the Ham. Eng. is to better combine the structural and featural information (encoded in the molecular graph representations) with the spatial information (the labeled conformations) in the generalized space $R^{d_f}$. \n\nWe also show by the experiment *HamNet (real conf.)* the benefits of using Ham. Eng.: even on QM9 where the real conformations are used, *HamNet (ours)* still outperforms *HamNet (real conf.)* (note that the dimensionality issue is addressed in the comparison, as real coordinations of atoms are expanded into $2 \\times d_f$ so that two inputs have the same dimensionality). **That is, it is not the accuracy of conformations, but the featurization approach itself, that matters more.** This indicates the necessity of using Ham. Eng. to obtain the optimal results of fingerprints.\n\nAnother potential benefits of using implicit $q,p$s compared with using 3D positions (say $x$s), is that the former enjoys better spatial invariance: i) the input of Ham. Eng. is spatial invariant (although the LSTM to some degree compromised the permutation invariance); ii) the output of Ham. Eng. is supervised with spatial invariant metrics (K-RMSD, ADJ-k). Therefore, as long as the initialization encoders (GCN+LSTM) are properly regularized, the Ham. Eng. learns a canonical space which enjoys spatial invariance. \n\n3. **Why not using RDKit to generate conformations?**\n\nWe do use RDKit conformations when the datasets lack real ones, but they are calculated *only for the training set*. Actually, in real scenarios, conformations of training sets are generally available; if not, at least the calculation is once-and-for-all. What HamNet saves is the time of conformation calculation in the *inference stage*, where labeled confs are NOT used. In the scenarios including drug candidate generation & large-scale virtual screening, this improvement is crucial.\n\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper636/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper636/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Conformation-Guided Molecular Representation with Hamiltonian Neural Networks", "authorids": ["~Ziyao_Li1", "swyang@pku.edu.cn", "~Guojie_Song1", "cailingsheng@pku.edu.cn"], "authors": ["Ziyao Li", "Shuwen Yang", "Guojie Song", "Lingsheng Cai"], "keywords": ["Molecular Representation", "Neural Physics Engines", "Molecular Dynamics", "Graph Neural Networks"], "abstract": "Well-designed molecular representations (fingerprints) are vital to combine medical chemistry and deep learning. Whereas incorporating 3D geometry of molecules (i.e. conformations) in their representations seems beneficial, current 3D algorithms are still in infancy. In this paper, we propose a novel molecular representation algorithm which preserves 3D conformations of molecules with a Molecular Hamiltonian Network (HamNet). In HamNet, implicit positions and momentums of atoms in a molecule interact in the Hamiltonian Engine following the discretized Hamiltonian equations. These implicit coordinations are supervised with real conformations with translation- & rotation-invariant losses, and further used as inputs to the Fingerprint Generator, a message-passing neural network. Experiments show that the Hamiltonian Engine can well preserve molecular conformations, and that the fingerprints generated by HamNet achieve state-of-the-art performances on MoleculeNet, a standard molecular machine learning benchmark.", "one-sentence_summary": "We propose a molecular representation algorithm, which preserves molecular conformations with a neural physics engine and generates fingerprints with an MPNN.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|conformationguided_molecular_representation_with_hamiltonian_neural_networks", "supplementary_material": "/attachment/c3a6d96b83a7cf2111b0f63066d54dbb42fb1009.zip", "pdf": "/pdf/7a5a25fdbe36c7b0286d17dafd233f47bf7dd30c.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nli2021conformationguided,\ntitle={Conformation-Guided Molecular Representation with Hamiltonian Neural Networks},\nauthor={Ziyao Li and Shuwen Yang and Guojie Song and Lingsheng Cai},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=q-cnWaaoUTH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "q-cnWaaoUTH", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper636/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper636/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper636/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper636/Authors|ICLR.cc/2021/Conference/Paper636/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper636/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923868839, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper636/-/Official_Comment"}}}, {"id": "8PSRF72GXv5", "original": null, "number": 2, "cdate": 1606032900531, "ddate": null, "tcdate": 1606032900531, "tmdate": 1606043209791, "tddate": null, "forum": "q-cnWaaoUTH", "replyto": "WFogNcgqoMC", "invitation": "ICLR.cc/2021/Conference/Paper636/-/Official_Comment", "content": {"title": "Official reply to Reviewer 3", "comment": "We truly appreciate your recommendation for our paper, and all the valuable comments and detailed suggestions. With your help, we revised some figures and sentences in our paper. To conclude:\n\n1. We improved Figure 1 to better demonstrate the in/output of the two modules and the data flow in between;\n\n2. We used \"reconstruct\" instead of \"preserve\" to clear the confusion;\n\n3. We rephrased some of the typos / confusing statements follow your \"Minor comments\" section.\n\nWith regard to your questions, we would like to clarify:\n\n4. **Initialization** (why use GNN(+LSTM) to initialize and not start with real confs; why Step 0 looks compact; why a higher dimensional space is in need; and eventually, why Ham. Eng. at all?)\n\nFirstly, as we claimed in the paper, real conf (or RDKit simulated ones) are not always available. We do use RDKit confs when the datasets lack real ones, but they are calculated *only for the training set*. Actually, in real scenarios, confs of training sets are generally available; if not, at least the calculation is once-and-for-all. What HamNet saves is the time of the conf calculation in the *inference stage*, where labeled confs are NOT used. In the scenarios including drug candidate generation & large-scale virtual screening, this improvement is crucial.\n\nSecondly, we would like to stress that preserving the conformations is not the ultimate goal for HamNet, but to *derive good representations* is. Therefore, the desired properties of the initializations are not quite the same as classical MD. Starting from 2D structures & atom / edge features with GNN encoders enables the Ham. Eng. to be not only *spatial-aware* (supervised with real confs), but also *structural- & featural-aware*. In another word, the role Ham. Eng. plays is not only to derive good conformations, but also to merge the structural & featural information (input) with the spatial information (output) in the generalized space. We believe this also answers the question **why a higher dimensional space is in need**.\n\nExperimental results also support the advantage of this setup: in the experiments *HamNet (real conf.)* (Table 2), we tested the results of directly using real confs. as inputs (3D coordinations of atoms mapped with MLP to $2 \\times d$ dimensional to eliminate the effect of dimensionality), while no better results than *HamNet (ours)* are obtained. We believe this proves the effects of structural- & featural info in the generalized space.\n\nInitialization looking compact is thus comprehendable: they are encoded from atom / edge features directly, and as the GNN+LSTM is regularized, the outputs of them approaching 0 is in the expectation. Note that we only poses conformation supervision on the final outputs, and the initializations are not explicitly scaled: it is done by the Ham. Eng.. The process inside the Ham. Eng. attempts to \"stretch\" the feature-based conformation initializations to correct conformations. In this process, the mergence of two types of information is done.\n\n5. **Why simplifying the LJ potential?** \n\nThe function $r^{-12}-r^{-6}$ is indeed calculatable, however, given the circumstances that they appear in a (somehow complicated) neural network, using high orders of polynomials may lead to computational issues such as gradient explosion / vanishing et al, and experimentally, it does not work well. The polynomial $r^{-4} - r^{-2}$ has a similar landscape with lower order, so we made the simplification.\n\n6. **Normalization of mass**\n\nNormalization of mass is for computational convenience. Note that in quadratic functions $T(p)$ and $U(q)$, linear scales can be adaptively adjusted by the *singular values* of the weight matrics $W_T,W_U$, e.g.\n$$\n\\frac{p^TW_T^TW_Tp}{2 \\alpha m} = \\frac{p^T(\\alpha^{-1/2}W_T)^T(\\alpha^{-1/2}W_T)p}{2m} := \\frac{p^T\\tilde{W}_T^T\\tilde{W}_Tp}{2m}.\n$$\nTherefore, the scale of mass is not that important, as long as it is computational convenient. Using $m=M/50$ is empirically appropriate as $m \\in (0.1, 1)$ (heavy atoms).\n\n7. **Initialization of the momenta**\n\nWe initialize the momenta in the same way as the positions, i.e. using GNN+LSTM. Indeed, this does not lead to direct interpretability as drawing from a Boltzmann distribution does; however, as all the interaction happens in a generalized space, we leave the neural networks to do their job.\n\n\n\n\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper636/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper636/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Conformation-Guided Molecular Representation with Hamiltonian Neural Networks", "authorids": ["~Ziyao_Li1", "swyang@pku.edu.cn", "~Guojie_Song1", "cailingsheng@pku.edu.cn"], "authors": ["Ziyao Li", "Shuwen Yang", "Guojie Song", "Lingsheng Cai"], "keywords": ["Molecular Representation", "Neural Physics Engines", "Molecular Dynamics", "Graph Neural Networks"], "abstract": "Well-designed molecular representations (fingerprints) are vital to combine medical chemistry and deep learning. Whereas incorporating 3D geometry of molecules (i.e. conformations) in their representations seems beneficial, current 3D algorithms are still in infancy. In this paper, we propose a novel molecular representation algorithm which preserves 3D conformations of molecules with a Molecular Hamiltonian Network (HamNet). In HamNet, implicit positions and momentums of atoms in a molecule interact in the Hamiltonian Engine following the discretized Hamiltonian equations. These implicit coordinations are supervised with real conformations with translation- & rotation-invariant losses, and further used as inputs to the Fingerprint Generator, a message-passing neural network. Experiments show that the Hamiltonian Engine can well preserve molecular conformations, and that the fingerprints generated by HamNet achieve state-of-the-art performances on MoleculeNet, a standard molecular machine learning benchmark.", "one-sentence_summary": "We propose a molecular representation algorithm, which preserves molecular conformations with a neural physics engine and generates fingerprints with an MPNN.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|conformationguided_molecular_representation_with_hamiltonian_neural_networks", "supplementary_material": "/attachment/c3a6d96b83a7cf2111b0f63066d54dbb42fb1009.zip", "pdf": "/pdf/7a5a25fdbe36c7b0286d17dafd233f47bf7dd30c.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nli2021conformationguided,\ntitle={Conformation-Guided Molecular Representation with Hamiltonian Neural Networks},\nauthor={Ziyao Li and Shuwen Yang and Guojie Song and Lingsheng Cai},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=q-cnWaaoUTH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "q-cnWaaoUTH", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper636/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper636/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper636/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper636/Authors|ICLR.cc/2021/Conference/Paper636/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper636/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923868839, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper636/-/Official_Comment"}}}, {"id": "o_ZKZomVYJg", "original": null, "number": 3, "cdate": 1606043171865, "ddate": null, "tcdate": 1606043171865, "tmdate": 1606043171865, "tddate": null, "forum": "q-cnWaaoUTH", "replyto": "EDYZzkegBDH", "invitation": "ICLR.cc/2021/Conference/Paper636/-/Official_Comment", "content": {"title": "Official reply to Reviewer 2", "comment": "We truly appreciate your recommendation for our paper. Meanwhile, we are sorry to cause the confusions that you noted in the comments. To address to your concerns, we would like to clarify:\n\n1. **The training of HamNet**\n\nWe are sorry to miss out the training details in the mainbody of the paper. With your help, we revised the paper and addressed the training scheme of HamNet in both Section 3.2 (FG) and Section 4.1 (implementation). Currently, we adopt a separate training strategy which first pretrain HE with loss $L=L_{k-rmsd} + 5 L_{adj-3}$, and then train the FG with outputs from HE. As for the training losses of FG, we use MAE for QM9, MSE for other regression tasks, and cross entropy for the classification tasks. \n\nWe also tried to conduct joint training (i.e. train FG+HE for specific tasks), while no significant improvement was observed, the parameters in HE is hardly altered, and the complexity is rather high. Therefore, we report only the separate training strategy in our paper.\n\n2. **Improving Figure 1**\n\nIndeed it is hard to plan the figure, and we thank you for the consideration. What we would like to show via Figure 1 is the major data flow in the model, while listing all parameters & details as DimeNet does in Figure 1 is too difficult, especially considering there are complicated architectures including GRUs and (modified) GATs. \n\nWe revised Figure 1 and used different colors to mark out 1) the data interaction between two modules (red); 2) all layers with learnable parameters (orange); 3) inputs / outputs (labels) (green); 4) all other hidden representations (blue). We hope you'll find it clearer.\n\n3. **Ablation study details**\n\nDue to the limitation of space, we did not provide detailed explanations in the mainbody of the paper. We hope that Section B in the Appendix helps to comprehend the setups. We also revised this part to become more clear about the  implementation.\n\n4. **Experiments with $d_f = 3$**\n\nThe results of conformation prediction of Ham. Eng. with $d_f = 3$ is available in Figure 3 (d), and we see that it is far from sufficient in capturing the molecular conformations. Therefore, we did not do experiments on HamNet with $d_f=3$. We would anticipate that HamNet ($d_f=3$) behaves similarly to HamNet (w/o conf), as $q,p$ fails to capture valuable information in the labeled conformations.\n\n5. **More citations on ML (DL) potentials & neural physics engines**\n\nWe sincerely agree that some discussion on ML potentials and neural physics engines is beneficial. In fact, HamNet is highly motivated by these works. However, due to the limitation of space, we could not display these discussions in our first submission (the paper is *information-dense* already). As the page limitation is relaxed in the rebuttal stage, we revised our submission and added corresponding contents in Section 5.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper636/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper636/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Conformation-Guided Molecular Representation with Hamiltonian Neural Networks", "authorids": ["~Ziyao_Li1", "swyang@pku.edu.cn", "~Guojie_Song1", "cailingsheng@pku.edu.cn"], "authors": ["Ziyao Li", "Shuwen Yang", "Guojie Song", "Lingsheng Cai"], "keywords": ["Molecular Representation", "Neural Physics Engines", "Molecular Dynamics", "Graph Neural Networks"], "abstract": "Well-designed molecular representations (fingerprints) are vital to combine medical chemistry and deep learning. Whereas incorporating 3D geometry of molecules (i.e. conformations) in their representations seems beneficial, current 3D algorithms are still in infancy. In this paper, we propose a novel molecular representation algorithm which preserves 3D conformations of molecules with a Molecular Hamiltonian Network (HamNet). In HamNet, implicit positions and momentums of atoms in a molecule interact in the Hamiltonian Engine following the discretized Hamiltonian equations. These implicit coordinations are supervised with real conformations with translation- & rotation-invariant losses, and further used as inputs to the Fingerprint Generator, a message-passing neural network. Experiments show that the Hamiltonian Engine can well preserve molecular conformations, and that the fingerprints generated by HamNet achieve state-of-the-art performances on MoleculeNet, a standard molecular machine learning benchmark.", "one-sentence_summary": "We propose a molecular representation algorithm, which preserves molecular conformations with a neural physics engine and generates fingerprints with an MPNN.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|conformationguided_molecular_representation_with_hamiltonian_neural_networks", "supplementary_material": "/attachment/c3a6d96b83a7cf2111b0f63066d54dbb42fb1009.zip", "pdf": "/pdf/7a5a25fdbe36c7b0286d17dafd233f47bf7dd30c.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nli2021conformationguided,\ntitle={Conformation-Guided Molecular Representation with Hamiltonian Neural Networks},\nauthor={Ziyao Li and Shuwen Yang and Guojie Song and Lingsheng Cai},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=q-cnWaaoUTH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "q-cnWaaoUTH", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper636/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper636/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper636/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper636/Authors|ICLR.cc/2021/Conference/Paper636/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper636/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923868839, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper636/-/Official_Comment"}}}, {"id": "WFogNcgqoMC", "original": null, "number": 1, "cdate": 1603846040590, "ddate": null, "tcdate": 1603846040590, "tmdate": 1605024642133, "tddate": null, "forum": "q-cnWaaoUTH", "replyto": "q-cnWaaoUTH", "invitation": "ICLR.cc/2021/Conference/Paper636/-/Official_Review", "content": {"title": "Official Blind Review #3", "review": "**Summary**\nThe paper proposes a new method for generating fingerprints for small molecules. It is based on two components: a \"Hamiltonian Engine\" that runs a brief simulation, predicting the structure of the small molecule by minimizing a learned potential energy, and 2) a message passing algorithm that uses the predicted structure as input. The reported experimental results demonstrate state-of-the-art performance.\n\n**Strengths**\n o The manuscript addresses an important problem\n o The Hamiltonian Engine is a non-trivial contribution, which should be of broad interest.\n o The reported results seem to significantly outperform the current state of the art\n\n**Weaknesses**\nThe paper is information-dense and in some places difficult to read. \n\n**Recommendation**\nI recommend this paper be accepted. To my knowledge, the presented method is novel, and the results are convincing. Furthermore, it is an interesting contribution on the interface between physics and machine learning, which should be of interest even beyond the small-molecule ML subfield. The manuscript would however benefit from a slightly clearer presentation (see below).\n\n**Detailed feedback**\n\nAlthough I'm generally positive about the article, there are several places where I would recommend sentences could be improved to more clearly communicate the central ideas in the paper:\n\nPage 1. The last paragraph on page 1 should make it clearer what the role of the Hamiltonian Engine is. In my opinion, the choice of the word \"preserve\", is particularly confusing - it seems to me more like a \"prediction\" or a \"reconstruction\". The work \"preserve\" made me think that the positions from the original molecule were used to initialize the positions and momenta of the simulator (\"preserve\" thus meaning the ability of the forcefield to stabilize a given structure). Figure 2 cleared this up for me, but that does not appear until page 7. Also, the authors write that HamNet \"simulates the process of molecular dynamics\", but it is not clear why  a Hamiltonian Engine is needed at all - couldn't you simply have used a short MD simulation with an established molecular force field? \n\nFigure 1. Could anything be done visually to show how the Fingerprint Generator uses the output from the Hamiltonian Engine? Currently, the two methodological components of the paper seem a bit disconnected in the figure.\n\nPage 4. \"Graph-based neural networks are used to initialize these spatial quantities.\" It would be informative if you could add a few lines to motivate this choice. Couldn't you have used some idealized positions as a starting point for your simulation? Later on in the paper you mention that you use RDKit to create positions when they are not available in the data - couldn't you then just always use RDKit to create an initial structure - or would this be less accurate or too slow?\n\nPage 5. \"conformations always converge to a local minimum\" Perhaps you could comment on the nature of the learned energy landscape.  Do we expect it to be a highly multimodal landscape, with risk for convergence to suboptimal local optima? Is this why you place so much emphasis on the initialization of your simulations?\n\n**Clarifying questions to the authors**\nPage 4. \"we simplify the Lennard Jones potential\"\nWhy do you make the simplification to r^{-4} - r^{-2}? It seems to me that you could have calculated the standard r^{-12} - r{^-6} equally well from the r^2 that you have available.\n\nPage 4. \"we empirically normalize the relative atomic mass\".\nWhat is the reason for this normalization? It it just to improve the training behavior of the model? Is this a hyperparameter that needs to be optimized?\n\nFigure 2. Does the Step 0 correspond to an initialization produced by the GCN+LSTM? The positions look very compact - wouldn't you have expected them to look more realistic?\n\nPage 6. \"tuned with the Universal Force Field\" Could you have done a distance geometry initialization but using your learned forcefield for subsequent fine-tuning?. Would this be superior to using the Universal Force Field?\n\nPage 7, \"where the Hamiltonian Engine is removed and all q,ps are transformed from real coordinations\"\nIf you are only given structures, how to you obtain the momenta?\n\nPage 8. I found it interesting that it was necessary to map the positions and momenta to a higher dimensional space for optimal performance. Do you know if this overparamterization is necessary purely to facilitate gradient-based training, or whether there are more fundamental reasons for modelling them this way?\n\n**Minor comments**\nPage 1. \"For examples, Attentive Fingerprints have yet become the de facto state-of-the-art approach\".\nTo a reader not intimitely familiar with these references it is not clear what you mean by this sentence. Is Attentive FP a 2D method? I think there is also a \"to\" missing before \"become\".\n\nPage 4. \"but atoms with coincide positions\". \"coincide\" -> \"coinciding\"\n\nPage 4, \"we do not explicitly require \\hat Q to conform to labeled coordinations of atoms\". This was a bit difficult to read. I assume you mean that you only predict Q up to a rotation and a translation. Perhaps rephrase.\n\nPage 5. \"Speaking for detailly\". Rephrase.\n", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper636/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper636/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Conformation-Guided Molecular Representation with Hamiltonian Neural Networks", "authorids": ["~Ziyao_Li1", "swyang@pku.edu.cn", "~Guojie_Song1", "cailingsheng@pku.edu.cn"], "authors": ["Ziyao Li", "Shuwen Yang", "Guojie Song", "Lingsheng Cai"], "keywords": ["Molecular Representation", "Neural Physics Engines", "Molecular Dynamics", "Graph Neural Networks"], "abstract": "Well-designed molecular representations (fingerprints) are vital to combine medical chemistry and deep learning. Whereas incorporating 3D geometry of molecules (i.e. conformations) in their representations seems beneficial, current 3D algorithms are still in infancy. In this paper, we propose a novel molecular representation algorithm which preserves 3D conformations of molecules with a Molecular Hamiltonian Network (HamNet). In HamNet, implicit positions and momentums of atoms in a molecule interact in the Hamiltonian Engine following the discretized Hamiltonian equations. These implicit coordinations are supervised with real conformations with translation- & rotation-invariant losses, and further used as inputs to the Fingerprint Generator, a message-passing neural network. Experiments show that the Hamiltonian Engine can well preserve molecular conformations, and that the fingerprints generated by HamNet achieve state-of-the-art performances on MoleculeNet, a standard molecular machine learning benchmark.", "one-sentence_summary": "We propose a molecular representation algorithm, which preserves molecular conformations with a neural physics engine and generates fingerprints with an MPNN.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|conformationguided_molecular_representation_with_hamiltonian_neural_networks", "supplementary_material": "/attachment/c3a6d96b83a7cf2111b0f63066d54dbb42fb1009.zip", "pdf": "/pdf/7a5a25fdbe36c7b0286d17dafd233f47bf7dd30c.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nli2021conformationguided,\ntitle={Conformation-Guided Molecular Representation with Hamiltonian Neural Networks},\nauthor={Ziyao Li and Shuwen Yang and Guojie Song and Lingsheng Cai},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=q-cnWaaoUTH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "q-cnWaaoUTH", "replyto": "q-cnWaaoUTH", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper636/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538138701, "tmdate": 1606915786671, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper636/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper636/-/Official_Review"}}}, {"id": "EDYZzkegBDH", "original": null, "number": 2, "cdate": 1603870646086, "ddate": null, "tcdate": 1603870646086, "tmdate": 1605024642066, "tddate": null, "forum": "q-cnWaaoUTH", "replyto": "q-cnWaaoUTH", "invitation": "ICLR.cc/2021/Conference/Paper636/-/Official_Review", "content": {"title": "A very interesting work, but with a lot of questions from the unclear presentation.", "review": "Summary: This paper presents a novel neural network module called Hamiltonian Neural Networks to learn representation of molecules. The module consists of two main components: 1) Hamiltonian Engine (HE), and 2) Fingerprint Generator (FG). The HE 1) takes a molecular graph with atom and bond features as inputs, and first generate \"generalized\" positions p and momentums q via GNN+LSTM. These ps and qs are fed into a discrete Hamiltonian system with dissipation, and produces (generalized) \"conformations\" of molecules. The FG also takes a molecular graph with atom and bond features + the generalized positions and momentums from 1) as inputs to generate the final vector embedding of the input molecule. HE is trained to fit the input 3D conformation with a weighted combination of two types lossses (K-RMSD + 5 * ADJ-k loss). Posing this \"Hamiltonian\" inductive bias to the model, the paper demonstrated that the prediction performance over multiple molecular tasks from MoleculeNet are all improved. Also, the analysis of HE module or the hyperparamter sensitivity analysis are provided.\n\nComments:\nThis is a very interesting work in the light of molecular representation learning. The 3D geometries of molecules, i.e. conformations, are actually not rigid-body-like, rather has a degree of freedom varied according to the physical rule as we see them in molecular dynamics. It would be reasonable to see that this inductive bias explitly as \"Hamiltonian system with learnable parameters\" actually brings empirical performance improvements. \n\nI enjoyed the paper and core ideas, but the description and presentation of the (complicated) method's inner workings would need to be improved. In particular, it'll be nice to make clear the following points:\n\n- There are no description on the classifier or regressor head for downstream tasks of MoleculeNet, and how to train the entire network. This should be explicitly included in the paper. The paper only provides how to get the fingerprint, but seemingly there are no explanations or description on how to train this FG (or the entire module of HE+FG). We'll need to add some task-specifc heads after HE -> FG for MoleculeNet tasks. Then, is HE first trained with K-RMSD + 5 * ADJ-k loss, and the entire HE + FG fitted with the task-specific supervision loss? Or the entire HE + FG fitted with K-RMSD + 5 * ADJ-k loss + alpha * task-specific supervision loss (with some alpha)?\n\n- The model desciprion of Figure 1 is unclear and rough, and it was quite painstaking to get the information on what are learnable parameters and what are just states or intermediates. Also, the p and q from HE to FG would be also hard to capture at first glance. I understand that the module is quite complicated and not easy to depict it in one figure, but I'll appreciate if the authors can include all learnable parameters in Figure 1. (For example, like the model figure, Fig.4, of the DimeNet paper).\n\n- Related to the above problem, some \"ablation study\" would be quite unclear. For example, \"HamNet (real conf.)\" in Table 2 is to tweak HE somehow, or just feed the real 3D conformations into FG with momentums of 0s (with removing the entire HE processing)?? In the first place, the HE used 32-dimensional vector for ps and qs. Is it fair if this means to use 3-dimensional vector instead (at least, need to pass it to MLP for converting the same dimension?). What if we use 3-dimension for d_f??\n\n- \"HamNet (w/o conf.)\", \"Ham. Eng. (w/o LSTM)\", \"Ham. Eng. (w/o dyn.)\" are also quite unclear.\n\n- It is quite interesting to see \"HamNet (ours)\" was better than \"HamNet (real conf)\" even for QM9, and moreover, given that for most tasks, the 3D input conformation is generated by RDKit distance geometry with UFF. But the internal conformation of HamNet is actually \"d_f\"-dimensional (32-dimensional), and this difference could come from this expressiveness. To confirm this, it would be nice to include the performance of \"HamNet (ours) with d_f = 3\". (By the way, since the 2018.09 release of the RDKit, ETKDG is the default conformation generation method. What if we use this instead of the distance geometry...?)\n\n- It would be very nice to add some remarks on related work for readers. The name and method would definitely remind related work such as \"Hamiltonian Neural Networks (Greydanus et al, NeurIPS 2019)\" , Machine-learning potential for MD (see [a][b] below, for example), and and \"learning to simulate\" methods. See the \u201cdelta graph network\u201d (DeltaGN) baseline of the cited workshop paper by Sanchez-Gonzalez et al, 2019. \n\n[a] Chmiela et al, Machine learning of accurate energy-conserving molecular force fields (Science Advances, 2017)\n[b] Chmiela et al, Towards exact molecular dynamics simulations with machine-learned force fields (Nature Communications, 2018)\n", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper636/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper636/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Conformation-Guided Molecular Representation with Hamiltonian Neural Networks", "authorids": ["~Ziyao_Li1", "swyang@pku.edu.cn", "~Guojie_Song1", "cailingsheng@pku.edu.cn"], "authors": ["Ziyao Li", "Shuwen Yang", "Guojie Song", "Lingsheng Cai"], "keywords": ["Molecular Representation", "Neural Physics Engines", "Molecular Dynamics", "Graph Neural Networks"], "abstract": "Well-designed molecular representations (fingerprints) are vital to combine medical chemistry and deep learning. Whereas incorporating 3D geometry of molecules (i.e. conformations) in their representations seems beneficial, current 3D algorithms are still in infancy. In this paper, we propose a novel molecular representation algorithm which preserves 3D conformations of molecules with a Molecular Hamiltonian Network (HamNet). In HamNet, implicit positions and momentums of atoms in a molecule interact in the Hamiltonian Engine following the discretized Hamiltonian equations. These implicit coordinations are supervised with real conformations with translation- & rotation-invariant losses, and further used as inputs to the Fingerprint Generator, a message-passing neural network. Experiments show that the Hamiltonian Engine can well preserve molecular conformations, and that the fingerprints generated by HamNet achieve state-of-the-art performances on MoleculeNet, a standard molecular machine learning benchmark.", "one-sentence_summary": "We propose a molecular representation algorithm, which preserves molecular conformations with a neural physics engine and generates fingerprints with an MPNN.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|conformationguided_molecular_representation_with_hamiltonian_neural_networks", "supplementary_material": "/attachment/c3a6d96b83a7cf2111b0f63066d54dbb42fb1009.zip", "pdf": "/pdf/7a5a25fdbe36c7b0286d17dafd233f47bf7dd30c.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nli2021conformationguided,\ntitle={Conformation-Guided Molecular Representation with Hamiltonian Neural Networks},\nauthor={Ziyao Li and Shuwen Yang and Guojie Song and Lingsheng Cai},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=q-cnWaaoUTH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "q-cnWaaoUTH", "replyto": "q-cnWaaoUTH", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper636/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538138701, "tmdate": 1606915786671, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper636/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper636/-/Official_Review"}}}], "count": 9}