{"notes": [{"id": "PsdsEbzxZWr", "original": "5TKuw-Ap8xq", "number": 927, "cdate": 1601308105496, "ddate": null, "tcdate": 1601308105496, "tmdate": 1614985752259, "tddate": null, "forum": "PsdsEbzxZWr", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Analyzing and Improving Generative Adversarial Training for Generative Modeling and Out-of-Distribution Detection", "authorids": ["~Xuwang_Yin2", "sl8jx@virginia.edu", "~Gustavo_Rohde1"], "authors": ["Xuwang Yin", "Shiying li", "Gustavo Rohde"], "keywords": ["Adversarial Training", "Generative Modeling", "Out-of-Distribution Detection", "GANs", "Generative adversarial networks"], "abstract": "Generative adversarial training (GAT) is a recently introduced adversarial defense method. Previous works have focused on empirical evaluations of its application to training robust predictive models. In this paper we focus on theoretical understanding of the GAT method and extending its application to generative modeling and out-of-distribution detection. We analyze the optimal solutions of the maximin formulation employed by the GAT objective, and make a comparative analysis of the minimax formulation employed by GANs. We use theoretical analysis and 2D simulations to understand the convergence property of the training algorithm. Based on these results, we develop an unconstrained GAT algorithm, and conduct comprehensive evaluations of the algorithm's application to image generation and adversarial out-of-distribution detection. Our results suggest that generative adversarial training is a promising new direction for the above applications.", "one-sentence_summary": "Theoretical understanding of the generative adversarial training method and extending its application to generative modeling and out-of-distribution detection", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yin|analyzing_and_improving_generative_adversarial_training_for_generative_modeling_and_outofdistribution_detection", "pdf": "/pdf/e3fbd7afb933742c99772e6059c83bbd775c75ef.pdf", "supplementary_material": "/attachment/7fc7331eb1d17022753419a67c9826c7319a6fce.zip", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=6Z09ydXbEv", "_bibtex": "@misc{\nyin2021analyzing,\ntitle={Analyzing and Improving Generative Adversarial Training for Generative Modeling and Out-of-Distribution Detection},\nauthor={Xuwang Yin and Shiying li and Gustavo Rohde},\nyear={2021},\nurl={https://openreview.net/forum?id=PsdsEbzxZWr}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 14, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "sIDO7B-T7Hz", "original": null, "number": 1, "cdate": 1610040382871, "ddate": null, "tcdate": 1610040382871, "tmdate": 1610473976046, "tddate": null, "forum": "PsdsEbzxZWr", "replyto": "PsdsEbzxZWr", "invitation": "ICLR.cc/2021/Conference/Paper927/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "This paper conducts a theoretical and empirical analysis of the Generative Adversarial Training method (GAT). Although many comments have been addressed in the rebuttal, the reviewers still have few (but important) concerns, including the memorization effects and the lack of comparisons. "}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Analyzing and Improving Generative Adversarial Training for Generative Modeling and Out-of-Distribution Detection", "authorids": ["~Xuwang_Yin2", "sl8jx@virginia.edu", "~Gustavo_Rohde1"], "authors": ["Xuwang Yin", "Shiying li", "Gustavo Rohde"], "keywords": ["Adversarial Training", "Generative Modeling", "Out-of-Distribution Detection", "GANs", "Generative adversarial networks"], "abstract": "Generative adversarial training (GAT) is a recently introduced adversarial defense method. Previous works have focused on empirical evaluations of its application to training robust predictive models. In this paper we focus on theoretical understanding of the GAT method and extending its application to generative modeling and out-of-distribution detection. We analyze the optimal solutions of the maximin formulation employed by the GAT objective, and make a comparative analysis of the minimax formulation employed by GANs. We use theoretical analysis and 2D simulations to understand the convergence property of the training algorithm. Based on these results, we develop an unconstrained GAT algorithm, and conduct comprehensive evaluations of the algorithm's application to image generation and adversarial out-of-distribution detection. Our results suggest that generative adversarial training is a promising new direction for the above applications.", "one-sentence_summary": "Theoretical understanding of the generative adversarial training method and extending its application to generative modeling and out-of-distribution detection", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yin|analyzing_and_improving_generative_adversarial_training_for_generative_modeling_and_outofdistribution_detection", "pdf": "/pdf/e3fbd7afb933742c99772e6059c83bbd775c75ef.pdf", "supplementary_material": "/attachment/7fc7331eb1d17022753419a67c9826c7319a6fce.zip", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=6Z09ydXbEv", "_bibtex": "@misc{\nyin2021analyzing,\ntitle={Analyzing and Improving Generative Adversarial Training for Generative Modeling and Out-of-Distribution Detection},\nauthor={Xuwang Yin and Shiying li and Gustavo Rohde},\nyear={2021},\nurl={https://openreview.net/forum?id=PsdsEbzxZWr}\n}"}, "tags": [], "invitation": {"reply": {"forum": "PsdsEbzxZWr", "replyto": "PsdsEbzxZWr", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040382857, "tmdate": 1610473976028, "id": "ICLR.cc/2021/Conference/Paper927/-/Decision"}}}, {"id": "iAkiqxK2V2x", "original": null, "number": 3, "cdate": 1603987217271, "ddate": null, "tcdate": 1603987217271, "tmdate": 1606502091815, "tddate": null, "forum": "PsdsEbzxZWr", "replyto": "PsdsEbzxZWr", "invitation": "ICLR.cc/2021/Conference/Paper927/-/Official_Review", "content": {"title": "Review", "review": "In this paper the authors provide a theoretical and empirical analysis of the Generative Adversarial Training method (GAT) which is used to train models for OOD and adversarial example detection.\n\nThe GAT method is analyzed from a game theoretical prespective, focusing on the differences with GAN training, which are sometime conflated with it in the literature. The authors show that GAT and GAN have different training problems (maximin vs minimax) which have different optimal solutions.\n\nThe authors also propose a variant of GAT called Unconstraned GAT, which replaces the PGD attack in the inner optimization loop with an unconstrained steepest ascent update. They propose this training algorithm for both OOD detection, adversarial OOD detection and generative sampling.\nThey discuss the sensitivity of the proposed algorithm on the step size, which seems to critically depend on the model architecture and dataset. Experiments are performed on standard image datasets, using ImageNet as a source of known OOD examples.\n\nOverall the work is interesting, however I find some issues:\n- The key point of the theoretical anaysis comparing GAT with GAN is only made at the end. It would be better to mention in the introduction at a high level why GAT is preferable to GAN.\n- Maximin and minimax are terms specific to game theory which a typical ML researcher might not be familiar with. They should be defined, possibly first in an intuitive way and then formally.\n- The proposed algorithm seem quite sensitive on the step size. This might limit its practical applicability (minor issue: the step size is denoted as lambda in the text and gamma in the algorithm box).\n- The proposed generative sampling procedure will likely return a mode of the distribution rather than an unbiased sample. You would need to use an SGLD-like algorithm in order get unbiased samples.\n\nEDIT:\n\nThe revision addressed my concerns, I'm raising my evaluation to 7.", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper927/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper927/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Analyzing and Improving Generative Adversarial Training for Generative Modeling and Out-of-Distribution Detection", "authorids": ["~Xuwang_Yin2", "sl8jx@virginia.edu", "~Gustavo_Rohde1"], "authors": ["Xuwang Yin", "Shiying li", "Gustavo Rohde"], "keywords": ["Adversarial Training", "Generative Modeling", "Out-of-Distribution Detection", "GANs", "Generative adversarial networks"], "abstract": "Generative adversarial training (GAT) is a recently introduced adversarial defense method. Previous works have focused on empirical evaluations of its application to training robust predictive models. In this paper we focus on theoretical understanding of the GAT method and extending its application to generative modeling and out-of-distribution detection. We analyze the optimal solutions of the maximin formulation employed by the GAT objective, and make a comparative analysis of the minimax formulation employed by GANs. We use theoretical analysis and 2D simulations to understand the convergence property of the training algorithm. Based on these results, we develop an unconstrained GAT algorithm, and conduct comprehensive evaluations of the algorithm's application to image generation and adversarial out-of-distribution detection. Our results suggest that generative adversarial training is a promising new direction for the above applications.", "one-sentence_summary": "Theoretical understanding of the generative adversarial training method and extending its application to generative modeling and out-of-distribution detection", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yin|analyzing_and_improving_generative_adversarial_training_for_generative_modeling_and_outofdistribution_detection", "pdf": "/pdf/e3fbd7afb933742c99772e6059c83bbd775c75ef.pdf", "supplementary_material": "/attachment/7fc7331eb1d17022753419a67c9826c7319a6fce.zip", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=6Z09ydXbEv", "_bibtex": "@misc{\nyin2021analyzing,\ntitle={Analyzing and Improving Generative Adversarial Training for Generative Modeling and Out-of-Distribution Detection},\nauthor={Xuwang Yin and Shiying li and Gustavo Rohde},\nyear={2021},\nurl={https://openreview.net/forum?id=PsdsEbzxZWr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "PsdsEbzxZWr", "replyto": "PsdsEbzxZWr", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper927/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538131553, "tmdate": 1606915767575, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper927/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper927/-/Official_Review"}}}, {"id": "V8qV1CmkgiR", "original": null, "number": 2, "cdate": 1603873368004, "ddate": null, "tcdate": 1603873368004, "tmdate": 1606299002701, "tddate": null, "forum": "PsdsEbzxZWr", "replyto": "PsdsEbzxZWr", "invitation": "ICLR.cc/2021/Conference/Paper927/-/Official_Review", "content": {"title": "Interesting Ideas, but not ready yet. ", "review": "**1. Summary and contributions: Briefly summarize the paper and its contributions**\nThis work analyzed the optimal solutions for the Generative adversarial training (GAT) and the convergence property of the training algorithm. This work also compared the minimax and maximin games, both theoretically, and empirically, with the help of a nice 2D toy example. This work also developed an unconstrained version of GAT, and evaluated it on image generation and out of distribution detection tasks. \n\n##########################################################################\n\n**2. Strengths: Describe the strengths of the work. Typical criteria include: soundness of the claims (theoretical grounding, empirical evaluation), significance and novelty of the contribution, and relevance to the community.**\n\nI found the theoretical analysis to be interesting, and I especially like the 2D toy experiment in Figure 2, it strongly and clearly justified the importance of using a uniformly distributed p_(-k) distribution. \n\nI also liked the thorough discussion of the distinction between the minimax and maximin games. \n\nThe paper has interesting ideas and a lot of content, it also introduced adversarial OOD samples, which are all very interesting to me. \n\n##########################################################################\n\n**3. Weaknesses: Explain the limitations of this work along the same axes as above.**\n\nMemorization: \nI think optimizing the D approach is problematic in terms of memorization, there\u2019s nothing stopping the model to memorize the data, especially in the high dimensional space which makes the mass distribution to be very sparse. \n\nIn the Celeb A results in the middle of Figure 3, notice how the images (4,1), (4,3) look almost exactly the same, and they are also very similar to (1,3), (3,1) and (3,4).  \n\nIn figure 2b and 2c, notice how the distribution all collapses to 1 point, I wonder whether this is one perspective or intuition on this problem. \n\nUnverified claim: \nAt the bottom of page 7, \u201cThese results suggest that with a high capacity model and proper training, a robust OOD detection system is within reach.\u201d where these results referred to reducing the data complexity. I think the results in this paper are not enough to make this claim. The authors\u2019 logic here is that, if their model performs better on a simple dataset, then it implies the problem is insufficient model capacity. The assumption here is that the model can scale, which is completely unverified. Just because a model works well on MNSIT does not imply it is even possible to scale this model on ImageNet. An unverified claim like this is always a warning sign as it may mislead the readers and community. \n\nLack of ablation study: \nI really liked the toy experiments in Figure 2, which justified the use of uniform distribution in the data space. However this only provides intuition for the higher dimensional cases, it is still necessary to conduct an ablation study to verify this is indeed the case for higher-dimensional cases, for scientific rigor.  \n\nUnfair comparison: \nFor image generation results, the authors\u2019 method was compared with GANs, and the motivation is that both of the methods are trained adversarially. However, the generators of GANs never got to see the real data during train time. Here authors are optimizing D, which is trained on real data, thus I don\u2019t think it is fair to compare with GANs.  \n\nWeak baseline for OOD (outlier exposure) and insufficient comparison:\nOutlier exposure is no longer the state of the art OOD detection methods and there are so many other methods that perform better than outlier exposure on CIFAR10, for example, see Detecting Out-of-Distribution Examples with Gram Matrices: https://arxiv.org/abs/1912.12510 \n \nLack of Related Work section: \nThis paper does not have a related work section. The related works are very briefly discussed in the introduction, but I think that is far from enough. I understand there is a page limit, but even putting a related work section in the appendix would be very helpful for readers to get a more complete understanding of where this work stands. \n\nUnclear writings: See section 4.\n\n ##########################################################################\n\n**4. Clarity: Is the paper well written?**\nTypos: \nAt the top of page 3: as a results -> as a result\nBottom of page 8: a OOD -> an OOD\n\nAmbiguity: \n\u201celementary mass\u201d was not defined before being used, it\u2019d be helpful to the readers to briefly define what \u201celementary mass\u201d means in this specific context. \n\n\u201cIn order to cause more local maxima to be eliminated\u201d could be worded better. \n\n##########################################################################\n\n**5. Reasons for score**\nIn conclusion, the ideas are very interesting but I think there is a lot more work to be done before this paper could be accepted. \n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper927/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper927/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Analyzing and Improving Generative Adversarial Training for Generative Modeling and Out-of-Distribution Detection", "authorids": ["~Xuwang_Yin2", "sl8jx@virginia.edu", "~Gustavo_Rohde1"], "authors": ["Xuwang Yin", "Shiying li", "Gustavo Rohde"], "keywords": ["Adversarial Training", "Generative Modeling", "Out-of-Distribution Detection", "GANs", "Generative adversarial networks"], "abstract": "Generative adversarial training (GAT) is a recently introduced adversarial defense method. Previous works have focused on empirical evaluations of its application to training robust predictive models. In this paper we focus on theoretical understanding of the GAT method and extending its application to generative modeling and out-of-distribution detection. We analyze the optimal solutions of the maximin formulation employed by the GAT objective, and make a comparative analysis of the minimax formulation employed by GANs. We use theoretical analysis and 2D simulations to understand the convergence property of the training algorithm. Based on these results, we develop an unconstrained GAT algorithm, and conduct comprehensive evaluations of the algorithm's application to image generation and adversarial out-of-distribution detection. Our results suggest that generative adversarial training is a promising new direction for the above applications.", "one-sentence_summary": "Theoretical understanding of the generative adversarial training method and extending its application to generative modeling and out-of-distribution detection", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yin|analyzing_and_improving_generative_adversarial_training_for_generative_modeling_and_outofdistribution_detection", "pdf": "/pdf/e3fbd7afb933742c99772e6059c83bbd775c75ef.pdf", "supplementary_material": "/attachment/7fc7331eb1d17022753419a67c9826c7319a6fce.zip", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=6Z09ydXbEv", "_bibtex": "@misc{\nyin2021analyzing,\ntitle={Analyzing and Improving Generative Adversarial Training for Generative Modeling and Out-of-Distribution Detection},\nauthor={Xuwang Yin and Shiying li and Gustavo Rohde},\nyear={2021},\nurl={https://openreview.net/forum?id=PsdsEbzxZWr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "PsdsEbzxZWr", "replyto": "PsdsEbzxZWr", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper927/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538131553, "tmdate": 1606915767575, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper927/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper927/-/Official_Review"}}}, {"id": "n8ux7o5Bea", "original": null, "number": 12, "cdate": 1606298981322, "ddate": null, "tcdate": 1606298981322, "tmdate": 1606298981322, "tddate": null, "forum": "PsdsEbzxZWr", "replyto": "WHbaIksuqlQ", "invitation": "ICLR.cc/2021/Conference/Paper927/-/Official_Comment", "content": {"title": "Changing rating from 3 to 4", "comment": "\nThanks for the authors' reply! I have read the replies and the revised papers and decided to raise my score from 3 to 4. I understand there has been a lot of work put into this during the rebuttal period, but I still think this paper is not yet ready for the community. \n\n I am increasing my score because of the following reasons:. \n\n**\u201cWe have revised this paragraph and removed the unwarranted claim (Page 7).\u201d**\n\n**\u201cWe have conducted an ablation study (Appendix H1) where we used the CIFAR-10 class 0 data as the  dataset, and respectively used ImageNet and a CIFAR-10 subset (data from class 1 \u2013 class 9) as the  dataset.\u201d**\n\n**\u201cWe have included in the section 5.1 an comparison with more baselines and state-of-the-art methods\u201d**\n\n**\u201cWe have included in the section 5.1 an comparison with more baselines and state-of-the-art methods (including the suggested one), for both the standard (Table 3) and adversarial (Table 4) OOD detection task.\u201d**\n\n**\u201cIn appendix A we have provided a review on recent work related to out-of-distribution detection. \u201c**\n\nI think the related work section is pretty comprehensive, well done!\nVarious typo fixes and clarifications. \n\nI am only increasing from 3 to 4 because of the following reasons: \n\n**Memorization**\n\n**\u201cGenerative models that work via the maximizing likelihood principle all have this potential problem. This is because maximizing likelihood is equivalent to minimizing the KL divergence, and a KL divergence of 0 indicates that the generated distribution and target distribution are identical \u2013 for empirical distributions, identical means the supports of the two distributions form a one-to-one match, which is exactly the definition of \u201cmemorization\u201d.\u201d**\n\nI would disagree on this point, I don\u2019t think all generative models have this problem. If you look at the latent interpolation results from state of the art VAEs, AAEs and GANs it is clear that there is no way the model is simply memorizing the samples, as the transition between two generated samples are very smooth when interpolating in the latent space. For VAEs, the maximum likelihood objective corresponds to minimizing the Forward KL, which encourages the model\u2019s mode covering results, which then encourage generating diverse examples, and this doesn\u2019t really imply overfitting because the model distribution has to be smooth while the data distribution is not, so KL never goes to zero.\n\nI looked at the samples in the appendix, it seems similar problems still persist, there are many samples which look almost the same. \n\n**Unfair comparison**\n**\u201cIn Appendix L we have included 256x256 generation results on the CelebA-HQ, Bedroom, and ImageNet Dog dataset, and we encourage the reviewer to compare our results with results produced by state-of-the-art GANs models like StyleGAN and BigGAN.\u201d**\nI\u2019ve compared the samples, it still seems that the proposed model suffers badly from memorization as there are very similar samples. This problem should be revealed more clearly if you evaluate the samples with Fr\u00e9chet Inception Distance (FID) or Inception Score (IS). This is another main reason why I think this paper is not ready for the ICLR community yet. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper927/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper927/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Analyzing and Improving Generative Adversarial Training for Generative Modeling and Out-of-Distribution Detection", "authorids": ["~Xuwang_Yin2", "sl8jx@virginia.edu", "~Gustavo_Rohde1"], "authors": ["Xuwang Yin", "Shiying li", "Gustavo Rohde"], "keywords": ["Adversarial Training", "Generative Modeling", "Out-of-Distribution Detection", "GANs", "Generative adversarial networks"], "abstract": "Generative adversarial training (GAT) is a recently introduced adversarial defense method. Previous works have focused on empirical evaluations of its application to training robust predictive models. In this paper we focus on theoretical understanding of the GAT method and extending its application to generative modeling and out-of-distribution detection. We analyze the optimal solutions of the maximin formulation employed by the GAT objective, and make a comparative analysis of the minimax formulation employed by GANs. We use theoretical analysis and 2D simulations to understand the convergence property of the training algorithm. Based on these results, we develop an unconstrained GAT algorithm, and conduct comprehensive evaluations of the algorithm's application to image generation and adversarial out-of-distribution detection. Our results suggest that generative adversarial training is a promising new direction for the above applications.", "one-sentence_summary": "Theoretical understanding of the generative adversarial training method and extending its application to generative modeling and out-of-distribution detection", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yin|analyzing_and_improving_generative_adversarial_training_for_generative_modeling_and_outofdistribution_detection", "pdf": "/pdf/e3fbd7afb933742c99772e6059c83bbd775c75ef.pdf", "supplementary_material": "/attachment/7fc7331eb1d17022753419a67c9826c7319a6fce.zip", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=6Z09ydXbEv", "_bibtex": "@misc{\nyin2021analyzing,\ntitle={Analyzing and Improving Generative Adversarial Training for Generative Modeling and Out-of-Distribution Detection},\nauthor={Xuwang Yin and Shiying li and Gustavo Rohde},\nyear={2021},\nurl={https://openreview.net/forum?id=PsdsEbzxZWr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "PsdsEbzxZWr", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper927/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper927/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper927/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper927/Authors|ICLR.cc/2021/Conference/Paper927/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper927/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923865695, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper927/-/Official_Comment"}}}, {"id": "tgB0sY20Uy", "original": null, "number": 4, "cdate": 1605610916047, "ddate": null, "tcdate": 1605610916047, "tmdate": 1606271738133, "tddate": null, "forum": "PsdsEbzxZWr", "replyto": "iAkiqxK2V2x", "invitation": "ICLR.cc/2021/Conference/Paper927/-/Official_Comment", "content": {"title": "Response to Reviewer1", "comment": "Dear Reviewer: \n\nThank you very much for your time and insightful comments! We have revised the paper based on your feedback. Below we response to your questions.\n\n**The key point of the theoretical analysis comparing GAT with GAN is only made at the end. It would be better to mention in the introduction at a high level why GAT is preferable to GAN.**\n\nThanks for the suggestion. We have included in the introduction a brief discussion about the relative merit of the proposed approach.\n\n**Maximin and minimax are terms specific to game theory which a typical ML researcher might not be familiar with. They should be defined, possibly first in an intuitive way and then formally.**\n\nThanks for the suggestion. In Appendix B we have included a discussion about these two problems in the game theory framework. To make the materials more accessible, in Appendix C we also included a demonstration of the general strategy for solving a maximin problem.\n\n**The proposed algorithm seem quite sensitive on the step size. This might limit its practical applicability (minor issue: the step size is denoted as lambda in the text and gamma in the algorithm box).**\n\nWe agree that the step size could be a potential problem. However, just as our failure mode diagnosis indicates, the problem of step size being too large can be detected in an early stage of training. We also observed that our algorithm works as long as the step size is below a certain threshold. Given these information, we think a working step size can be quickly found using binary search. Besides, in the paper we have provided the step size values for future work\u2019s reference.\n\nThanks, we have fixed the notation error.\n\n**The proposed generative sampling procedure will likely return a mode of the distribution rather than an unbiased sample. You would need to use an SGLD-like algorithm in order get unbiased samples.**\n\nThanks for the suggestion. This is a known limitation of our method and we have included a discussion about this issue on Page 9. We think this issue might be mitigated by constraining the number of steps and step sizes, as the gradient-based generation process is governed by these two parameters. The suggested algorithm looks new and interesting to us, and we will definitely look into it. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper927/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper927/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Analyzing and Improving Generative Adversarial Training for Generative Modeling and Out-of-Distribution Detection", "authorids": ["~Xuwang_Yin2", "sl8jx@virginia.edu", "~Gustavo_Rohde1"], "authors": ["Xuwang Yin", "Shiying li", "Gustavo Rohde"], "keywords": ["Adversarial Training", "Generative Modeling", "Out-of-Distribution Detection", "GANs", "Generative adversarial networks"], "abstract": "Generative adversarial training (GAT) is a recently introduced adversarial defense method. Previous works have focused on empirical evaluations of its application to training robust predictive models. In this paper we focus on theoretical understanding of the GAT method and extending its application to generative modeling and out-of-distribution detection. We analyze the optimal solutions of the maximin formulation employed by the GAT objective, and make a comparative analysis of the minimax formulation employed by GANs. We use theoretical analysis and 2D simulations to understand the convergence property of the training algorithm. Based on these results, we develop an unconstrained GAT algorithm, and conduct comprehensive evaluations of the algorithm's application to image generation and adversarial out-of-distribution detection. Our results suggest that generative adversarial training is a promising new direction for the above applications.", "one-sentence_summary": "Theoretical understanding of the generative adversarial training method and extending its application to generative modeling and out-of-distribution detection", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yin|analyzing_and_improving_generative_adversarial_training_for_generative_modeling_and_outofdistribution_detection", "pdf": "/pdf/e3fbd7afb933742c99772e6059c83bbd775c75ef.pdf", "supplementary_material": "/attachment/7fc7331eb1d17022753419a67c9826c7319a6fce.zip", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=6Z09ydXbEv", "_bibtex": "@misc{\nyin2021analyzing,\ntitle={Analyzing and Improving Generative Adversarial Training for Generative Modeling and Out-of-Distribution Detection},\nauthor={Xuwang Yin and Shiying li and Gustavo Rohde},\nyear={2021},\nurl={https://openreview.net/forum?id=PsdsEbzxZWr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "PsdsEbzxZWr", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper927/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper927/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper927/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper927/Authors|ICLR.cc/2021/Conference/Paper927/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper927/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923865695, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper927/-/Official_Comment"}}}, {"id": "9T2bgqhc2a9", "original": null, "number": 2, "cdate": 1605610332023, "ddate": null, "tcdate": 1605610332023, "tmdate": 1606268760071, "tddate": null, "forum": "PsdsEbzxZWr", "replyto": "PsdsEbzxZWr", "invitation": "ICLR.cc/2021/Conference/Paper927/-/Official_Comment", "content": {"title": "Source code", "comment": "Hi everyone,\n\nWe have submitted our source code as supplementary material. All experimental results can be reproduced using the accompanying notebooks (see README for instructions). Please let us know if you have any problem with the code.\n\nThanks."}, "signatures": ["ICLR.cc/2021/Conference/Paper927/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper927/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Analyzing and Improving Generative Adversarial Training for Generative Modeling and Out-of-Distribution Detection", "authorids": ["~Xuwang_Yin2", "sl8jx@virginia.edu", "~Gustavo_Rohde1"], "authors": ["Xuwang Yin", "Shiying li", "Gustavo Rohde"], "keywords": ["Adversarial Training", "Generative Modeling", "Out-of-Distribution Detection", "GANs", "Generative adversarial networks"], "abstract": "Generative adversarial training (GAT) is a recently introduced adversarial defense method. Previous works have focused on empirical evaluations of its application to training robust predictive models. In this paper we focus on theoretical understanding of the GAT method and extending its application to generative modeling and out-of-distribution detection. We analyze the optimal solutions of the maximin formulation employed by the GAT objective, and make a comparative analysis of the minimax formulation employed by GANs. We use theoretical analysis and 2D simulations to understand the convergence property of the training algorithm. Based on these results, we develop an unconstrained GAT algorithm, and conduct comprehensive evaluations of the algorithm's application to image generation and adversarial out-of-distribution detection. Our results suggest that generative adversarial training is a promising new direction for the above applications.", "one-sentence_summary": "Theoretical understanding of the generative adversarial training method and extending its application to generative modeling and out-of-distribution detection", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yin|analyzing_and_improving_generative_adversarial_training_for_generative_modeling_and_outofdistribution_detection", "pdf": "/pdf/e3fbd7afb933742c99772e6059c83bbd775c75ef.pdf", "supplementary_material": "/attachment/7fc7331eb1d17022753419a67c9826c7319a6fce.zip", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=6Z09ydXbEv", "_bibtex": "@misc{\nyin2021analyzing,\ntitle={Analyzing and Improving Generative Adversarial Training for Generative Modeling and Out-of-Distribution Detection},\nauthor={Xuwang Yin and Shiying li and Gustavo Rohde},\nyear={2021},\nurl={https://openreview.net/forum?id=PsdsEbzxZWr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "PsdsEbzxZWr", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper927/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper927/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper927/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper927/Authors|ICLR.cc/2021/Conference/Paper927/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper927/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923865695, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper927/-/Official_Comment"}}}, {"id": "1TI1-8WhuUp", "original": null, "number": 11, "cdate": 1606156318176, "ddate": null, "tcdate": 1606156318176, "tmdate": 1606248168497, "tddate": null, "forum": "PsdsEbzxZWr", "replyto": "PsdsEbzxZWr", "invitation": "ICLR.cc/2021/Conference/Paper927/-/Official_Comment", "content": {"title": "Additional results on adversarial OOD detection ", "comment": "Dear reviewers,\n\nOur method's results in Table 4 were computed using PGD attack with particular steps and step size. This test might be inadequate as the chosen attack can hardly be the most effective one. To provide additional credentials to our method's robustness, we ran a robustness test under attacks of different PGD configurations, including one with 5 random restarts as employed by Bitterwolf et al. (2020). The test results are included in Table 30 of Appendix N. Overall we did not find any robustness issue as the results obtained with different configurations are quite similar.\n\nIn addition, we have added a comparison of our method with other methods on the SVHN dataset in Table 31, and a similar robustness test in Table 32. Consistent with the CIFAR-10 result, our method's robustness is sound and performance is competitive.\n\nThe notebooks for reproducing these results have been included in the source code."}, "signatures": ["ICLR.cc/2021/Conference/Paper927/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper927/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Analyzing and Improving Generative Adversarial Training for Generative Modeling and Out-of-Distribution Detection", "authorids": ["~Xuwang_Yin2", "sl8jx@virginia.edu", "~Gustavo_Rohde1"], "authors": ["Xuwang Yin", "Shiying li", "Gustavo Rohde"], "keywords": ["Adversarial Training", "Generative Modeling", "Out-of-Distribution Detection", "GANs", "Generative adversarial networks"], "abstract": "Generative adversarial training (GAT) is a recently introduced adversarial defense method. Previous works have focused on empirical evaluations of its application to training robust predictive models. In this paper we focus on theoretical understanding of the GAT method and extending its application to generative modeling and out-of-distribution detection. We analyze the optimal solutions of the maximin formulation employed by the GAT objective, and make a comparative analysis of the minimax formulation employed by GANs. We use theoretical analysis and 2D simulations to understand the convergence property of the training algorithm. Based on these results, we develop an unconstrained GAT algorithm, and conduct comprehensive evaluations of the algorithm's application to image generation and adversarial out-of-distribution detection. Our results suggest that generative adversarial training is a promising new direction for the above applications.", "one-sentence_summary": "Theoretical understanding of the generative adversarial training method and extending its application to generative modeling and out-of-distribution detection", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yin|analyzing_and_improving_generative_adversarial_training_for_generative_modeling_and_outofdistribution_detection", "pdf": "/pdf/e3fbd7afb933742c99772e6059c83bbd775c75ef.pdf", "supplementary_material": "/attachment/7fc7331eb1d17022753419a67c9826c7319a6fce.zip", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=6Z09ydXbEv", "_bibtex": "@misc{\nyin2021analyzing,\ntitle={Analyzing and Improving Generative Adversarial Training for Generative Modeling and Out-of-Distribution Detection},\nauthor={Xuwang Yin and Shiying li and Gustavo Rohde},\nyear={2021},\nurl={https://openreview.net/forum?id=PsdsEbzxZWr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "PsdsEbzxZWr", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper927/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper927/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper927/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper927/Authors|ICLR.cc/2021/Conference/Paper927/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper927/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923865695, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper927/-/Official_Comment"}}}, {"id": "VYYX2wy0nT", "original": null, "number": 5, "cdate": 1605611349931, "ddate": null, "tcdate": 1605611349931, "tmdate": 1606066277111, "tddate": null, "forum": "PsdsEbzxZWr", "replyto": "V8qV1CmkgiR", "invitation": "ICLR.cc/2021/Conference/Paper927/-/Official_Comment", "content": {"title": "Response to Reviewer4", "comment": "Dear reviewer, \n\nThank you very much for your time and constructive feedback! We have conducted more experiments and revised the paper according to your suggestions. Below, we respond to each of your comments and look forward to your further feedback.\n\n**Memorization: I think optimizing the D approach is problematic in terms of memorization, there\u2019s nothing stopping the model to memorize the data, especially in the high dimensional space which makes the mass distribution to be very sparse.**\n\nWe completely agree that our approach has the potential problem of overfitting. Our analysis says that in Figure 1 scenario 1 an ideal $D$ solution has no local maxima and global maxima at support of $p_{k}$, which means overfitting when $p_{k}$ is an empirical distribution. However, the overfitting issue is not unique to our approach. Generative models that work via the maximizing likelihood principle all have this potential problem. This is because maximizing likelihood is equivalent to minimizing the KL divergence, and a KL divergence of 0 indicates that the generated distribution and target distribution are identical \u2013 for empirical distributions, identical means the supports of the two distributions form a one-to-one match, which is exactly the definition of \u201cmemorization\u201d. GANs is little bit different since it works by minimizing the JS divergence, but for empirical distributions a JS divergence of 0 also means that samples of two distributions form a one-to-one match. When that happens, the generator can only generate what are already in the target training data.\n\nSecond, our analysis of Figure 1 scenario 1 uses two assumptions, one is that the perturbation space covers the support of $p_{k}$, and the other is that $D$ has enough capacity (which is not always true in practice especially for high dimensional complex data). Regarding the first assumption, our training algorithm (Algorithm 3) uses $K$ to control the perturbation size - this actually provides a mechanism for constraining the perturbation space and mitigating the overfitting problem. In the appendix we also have results demonstrating this effect: models trained with different $K$ tend to generate samples of different levels of fidelity.\n\n**In the Celeb A results in the middle of Figure 3, notice how the images (4,1), (4,3) look almost exactly the same, and they are also very similar to (1,3), (3,1) and (3,4).**\n\n**In figure 2b and 2c, notice how the distribution all collapses to 1 point, I wonder whether this is one perspective or intuition on this problem.**\n\nThis is indeed a limitation of our approach. For the 2D case, it seems that the D model we obtained via the algorithm has a single global maximum (Figure 2b) or a few global maxima (Figure 2c). In these cases, performing gradient ascent on $D$ causes the $p_{-k}$ data to be concentrated on these maxima points. For the face experiment, it seems that the samples are stuck at local maxima points, as the resulting images do not resemble real face images; several images look the same because their seed images are trapped at the same  local maximum point. This issue tends to happen when the source images are not diverse enough. For instance, in Figure 17 we can see when source images are Gaussian  noise or uniform noise the resulting images looks quite similar. \nWe think at least for the case where $D$ has no local maxima, this issue could be mitigated by properly controlling number of steps and step size when performing gradient ascent on $D$. We are also looking into the SGLD algorithm suggested by reviewer 1. We have included a discussion about this limitation on Page 9.\n\n**Unverified claim: At the bottom of page 7, \u201cThese results suggest that with a high capacity model and proper training, a robust OOD detection system is within reach.\u201d where these results referred to reducing the data complexity. I think the results in this paper are not enough to make this claim. The authors\u2019 logic here is that, if their model performs better on a simple dataset, then it implies the problem is insufficient model capacity. The assumption here is that the model can scale, which is completely unverified. Just because a model works well on MNSIT does not imply it is even possible to scale this model on ImageNet. An unverified claim like this is always a warning sign as it may mislead the readers and community.**\n\nWe agree that the CIFAR10-classes 0 experiment is insufficient, as it only focuses on the \u201cdata complexity\u201d side. We conducted an additional experiment where we stick to the same CIFAR10 data but used ResNet18 as the $D$ model. ResNet18 is much larger than the default model of ResNet-CIFAR in terms of disk space (43MB vs. 4.6MB), but the performance increase is only marginal (Appendix H2). Given these results we are unable to conclude that it is \u201csimply a capacity and data complexity\u201d issue. We have revised this paragraph and removed the unwarranted claim (Page 7). \n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper927/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper927/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Analyzing and Improving Generative Adversarial Training for Generative Modeling and Out-of-Distribution Detection", "authorids": ["~Xuwang_Yin2", "sl8jx@virginia.edu", "~Gustavo_Rohde1"], "authors": ["Xuwang Yin", "Shiying li", "Gustavo Rohde"], "keywords": ["Adversarial Training", "Generative Modeling", "Out-of-Distribution Detection", "GANs", "Generative adversarial networks"], "abstract": "Generative adversarial training (GAT) is a recently introduced adversarial defense method. Previous works have focused on empirical evaluations of its application to training robust predictive models. In this paper we focus on theoretical understanding of the GAT method and extending its application to generative modeling and out-of-distribution detection. We analyze the optimal solutions of the maximin formulation employed by the GAT objective, and make a comparative analysis of the minimax formulation employed by GANs. We use theoretical analysis and 2D simulations to understand the convergence property of the training algorithm. Based on these results, we develop an unconstrained GAT algorithm, and conduct comprehensive evaluations of the algorithm's application to image generation and adversarial out-of-distribution detection. Our results suggest that generative adversarial training is a promising new direction for the above applications.", "one-sentence_summary": "Theoretical understanding of the generative adversarial training method and extending its application to generative modeling and out-of-distribution detection", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yin|analyzing_and_improving_generative_adversarial_training_for_generative_modeling_and_outofdistribution_detection", "pdf": "/pdf/e3fbd7afb933742c99772e6059c83bbd775c75ef.pdf", "supplementary_material": "/attachment/7fc7331eb1d17022753419a67c9826c7319a6fce.zip", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=6Z09ydXbEv", "_bibtex": "@misc{\nyin2021analyzing,\ntitle={Analyzing and Improving Generative Adversarial Training for Generative Modeling and Out-of-Distribution Detection},\nauthor={Xuwang Yin and Shiying li and Gustavo Rohde},\nyear={2021},\nurl={https://openreview.net/forum?id=PsdsEbzxZWr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "PsdsEbzxZWr", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper927/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper927/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper927/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper927/Authors|ICLR.cc/2021/Conference/Paper927/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper927/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923865695, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper927/-/Official_Comment"}}}, {"id": "x7gMykgYXC", "original": null, "number": 10, "cdate": 1605976392722, "ddate": null, "tcdate": 1605976392722, "tmdate": 1606061792574, "tddate": null, "forum": "PsdsEbzxZWr", "replyto": "qKlTMRZcu0", "invitation": "ICLR.cc/2021/Conference/Paper927/-/Official_Comment", "content": {"title": "updates on mathematical proof", "comment": "Dear Reviewer3,\n\nWe have included the full proof in Appendix M of the updated manuscript (main text in Section 3.2 have also been updated accordingly). \n\nWe also note that the ablation study on uniform noise in Appendix H1 seems to contradict this mathematical analysis and the 2D results. Our interpretation is that in terms of Euclidean distance, real image data live in low-dimensional manifolds that are close to each other, while uniform noise live on the unit-cube surface and is far away from real data; uniform noise is still effective for this task,  but it is far less data efficient than real image data. With uniform noise, a much larger number of inner iterations and $K$ value in Algorithm 3 may be needed to reach a satisfying detection performance. We have provided an discussion of this problem in Appendix H1."}, "signatures": ["ICLR.cc/2021/Conference/Paper927/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper927/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Analyzing and Improving Generative Adversarial Training for Generative Modeling and Out-of-Distribution Detection", "authorids": ["~Xuwang_Yin2", "sl8jx@virginia.edu", "~Gustavo_Rohde1"], "authors": ["Xuwang Yin", "Shiying li", "Gustavo Rohde"], "keywords": ["Adversarial Training", "Generative Modeling", "Out-of-Distribution Detection", "GANs", "Generative adversarial networks"], "abstract": "Generative adversarial training (GAT) is a recently introduced adversarial defense method. Previous works have focused on empirical evaluations of its application to training robust predictive models. In this paper we focus on theoretical understanding of the GAT method and extending its application to generative modeling and out-of-distribution detection. We analyze the optimal solutions of the maximin formulation employed by the GAT objective, and make a comparative analysis of the minimax formulation employed by GANs. We use theoretical analysis and 2D simulations to understand the convergence property of the training algorithm. Based on these results, we develop an unconstrained GAT algorithm, and conduct comprehensive evaluations of the algorithm's application to image generation and adversarial out-of-distribution detection. Our results suggest that generative adversarial training is a promising new direction for the above applications.", "one-sentence_summary": "Theoretical understanding of the generative adversarial training method and extending its application to generative modeling and out-of-distribution detection", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yin|analyzing_and_improving_generative_adversarial_training_for_generative_modeling_and_outofdistribution_detection", "pdf": "/pdf/e3fbd7afb933742c99772e6059c83bbd775c75ef.pdf", "supplementary_material": "/attachment/7fc7331eb1d17022753419a67c9826c7319a6fce.zip", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=6Z09ydXbEv", "_bibtex": "@misc{\nyin2021analyzing,\ntitle={Analyzing and Improving Generative Adversarial Training for Generative Modeling and Out-of-Distribution Detection},\nauthor={Xuwang Yin and Shiying li and Gustavo Rohde},\nyear={2021},\nurl={https://openreview.net/forum?id=PsdsEbzxZWr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "PsdsEbzxZWr", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper927/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper927/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper927/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper927/Authors|ICLR.cc/2021/Conference/Paper927/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper927/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923865695, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper927/-/Official_Comment"}}}, {"id": "qKlTMRZcu0", "original": null, "number": 8, "cdate": 1605611953605, "ddate": null, "tcdate": 1605611953605, "tmdate": 1606061658079, "tddate": null, "forum": "PsdsEbzxZWr", "replyto": "tSGmwiv2jGU", "invitation": "ICLR.cc/2021/Conference/Paper927/-/Official_Comment", "content": {"title": "Response to Reviewer3 (cont.)", "comment": "**Section 3.2: a practical consideration is that Step 2 cannot be perfectly solved and the authors thus proposed to use a p_{-k} that is uniformly distributed in the data space. The authors claimed that such condition always results in no maxima and global maxima at Supp(p_k). Can this conclusion be theoretically proved? One potential limitation is that using a p_{-k} that is uniformly distributed could lead to a long training time for achieving a satisfactory performance especially in high dimensional.**\n\nWe apologize for the confusion.  We agree that our description is misleading and we have rewritten this paragraph (Page 5).\nAs discussion in \"practical considerations\", step 2 cannot perfectly solve the inner problem because it is a gradient-based search method and the $D$ function could be a highly non-concave function. This issue, as our step-by-step 2D experiment demonstrates, is solved by the alternating optimizing algorithm. The introduction of uniform distribution (or any other \"well distributed\" data) is for the purpose of obtaining a $D$ model that has defined behavior in the whole data space, which we could not even in the 2D case when $p_{-k}$ is concentrated in a small subspace. We actually did not claim this strategy could always result in a $D$ with no local minima and global maxima at support of $p_{k}$; we meant that in several trials of the 2D experiment we consistently observed the above phenomenon. \n\nIn addition, we did not use uniform noise to train model in our real data experiments; we have added an ablation study (Appendix H1) to demonstrate that with uniform noise as the $p_{-k}$ dataset, with the same amout of training time, we are unable to get a model that is useful for detecting real OOD data in high dimensional space. \n\nRegarding your question of whether we can mathematically show that when $p_{-k}$ is uniform distribution the algorithm converges to a $D$ solution with no local maxima and global maxima at support of $p_{k}$, we think it is a very good question, and it has motivated us to think deeper about this problem. After some discussion we think the answer is yes, and it is in fact quite straightforward to show that. \n\n### updates\nWe have included a full poof in Appendix M of the updated manuscript.\n\nWe thank the reviewer for raising this question; previously we only have some intuition, but now we have developed a solid theoretical understanding of this important property of the algorithm.  \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper927/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper927/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Analyzing and Improving Generative Adversarial Training for Generative Modeling and Out-of-Distribution Detection", "authorids": ["~Xuwang_Yin2", "sl8jx@virginia.edu", "~Gustavo_Rohde1"], "authors": ["Xuwang Yin", "Shiying li", "Gustavo Rohde"], "keywords": ["Adversarial Training", "Generative Modeling", "Out-of-Distribution Detection", "GANs", "Generative adversarial networks"], "abstract": "Generative adversarial training (GAT) is a recently introduced adversarial defense method. Previous works have focused on empirical evaluations of its application to training robust predictive models. In this paper we focus on theoretical understanding of the GAT method and extending its application to generative modeling and out-of-distribution detection. We analyze the optimal solutions of the maximin formulation employed by the GAT objective, and make a comparative analysis of the minimax formulation employed by GANs. We use theoretical analysis and 2D simulations to understand the convergence property of the training algorithm. Based on these results, we develop an unconstrained GAT algorithm, and conduct comprehensive evaluations of the algorithm's application to image generation and adversarial out-of-distribution detection. Our results suggest that generative adversarial training is a promising new direction for the above applications.", "one-sentence_summary": "Theoretical understanding of the generative adversarial training method and extending its application to generative modeling and out-of-distribution detection", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yin|analyzing_and_improving_generative_adversarial_training_for_generative_modeling_and_outofdistribution_detection", "pdf": "/pdf/e3fbd7afb933742c99772e6059c83bbd775c75ef.pdf", "supplementary_material": "/attachment/7fc7331eb1d17022753419a67c9826c7319a6fce.zip", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=6Z09ydXbEv", "_bibtex": "@misc{\nyin2021analyzing,\ntitle={Analyzing and Improving Generative Adversarial Training for Generative Modeling and Out-of-Distribution Detection},\nauthor={Xuwang Yin and Shiying li and Gustavo Rohde},\nyear={2021},\nurl={https://openreview.net/forum?id=PsdsEbzxZWr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "PsdsEbzxZWr", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper927/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper927/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper927/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper927/Authors|ICLR.cc/2021/Conference/Paper927/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper927/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923865695, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper927/-/Official_Comment"}}}, {"id": "UXdKOsFrG0z", "original": null, "number": 3, "cdate": 1605610594137, "ddate": null, "tcdate": 1605610594137, "tmdate": 1605977429162, "tddate": null, "forum": "PsdsEbzxZWr", "replyto": "PsdsEbzxZWr", "invitation": "ICLR.cc/2021/Conference/Paper927/-/Official_Comment", "content": {"title": "Summary of revision", "comment": "Dear reviewers,\n\nThank you very much for your valuable feedback! We have made our best effort to address your concerns, and we believe that the manuscript quality has improved a lot thanks to your feedback. \n\nIn the updated manuscript we have made the following changes:\n- added a review of related work on out-of-distribution detection (Appendix A)\n- added comparisons with more baselines and state-of-the-art methods, for both the standard (Table 3), and the adversarial OOD detection task (Table 4)\n- added an ablation study on uniform noise and data diversity (Appendix H.1)\n- added a discussion about the limitation of our generative modeling approach (Page 9)\n- added a introductory discussion about maximin and minimax problem in game theory in Appendix B, and a demonstration of the strategy for solving a maximin problem in Appendix C\n- added high-resolution (256x256) generation results on CelebA-HQ-256, Bedroom-256, and ImageNet-Dog-256 dataset (Appendix L)\n- in Appendix M added a mathematical proof that when $p_{-k}$ is a uniform distribution Algorithm 1 converges to a $D$ solution with no local maxima and global maxima at support of $p_{k}$\n- various writing improvements\n\nAll changes are in blue fonts. \n\nAlthough the high-resolution generation results are not requested by reviewers, we think they could provide more information for evaluating our method. Our own assessment is that they are not as good as those produced by cutting edge GANs models. But we believe that these results highlight the potential of our method. In addition, they were obtained using the standard ResNet50 architecture without any architecture engineering. We note state-of-the-art Bedroom256 results can be found at StyleGAN [1] Figure 10, and ImageNet Dog 128 results can be found at BigGANs [3] (results are class-conditional, for unconditional ImageNet Dog results see [2]).\n\n[1] Karras, Tero, Samuli Laine, and Timo Aila. \"A style-based generator architecture for generative adversarial networks.\" Proceedings of the IEEE conference on computer vision and pattern recognition. 2019.\n\n[2] Zhang, Han, et al. \"Stackgan++: Realistic image synthesis with stacked generative adversarial networks.\" IEEE transactions on pattern analysis and machine intelligence 41.8 (2018): 1947-1962.\n\n[3] Brock, Andrew, Jeff Donahue, and Karen Simonyan. \"Large scale gan training for high fidelity natural image synthesis.\" arXiv preprint arXiv:1809.11096 (2018).\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper927/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper927/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Analyzing and Improving Generative Adversarial Training for Generative Modeling and Out-of-Distribution Detection", "authorids": ["~Xuwang_Yin2", "sl8jx@virginia.edu", "~Gustavo_Rohde1"], "authors": ["Xuwang Yin", "Shiying li", "Gustavo Rohde"], "keywords": ["Adversarial Training", "Generative Modeling", "Out-of-Distribution Detection", "GANs", "Generative adversarial networks"], "abstract": "Generative adversarial training (GAT) is a recently introduced adversarial defense method. Previous works have focused on empirical evaluations of its application to training robust predictive models. In this paper we focus on theoretical understanding of the GAT method and extending its application to generative modeling and out-of-distribution detection. We analyze the optimal solutions of the maximin formulation employed by the GAT objective, and make a comparative analysis of the minimax formulation employed by GANs. We use theoretical analysis and 2D simulations to understand the convergence property of the training algorithm. Based on these results, we develop an unconstrained GAT algorithm, and conduct comprehensive evaluations of the algorithm's application to image generation and adversarial out-of-distribution detection. Our results suggest that generative adversarial training is a promising new direction for the above applications.", "one-sentence_summary": "Theoretical understanding of the generative adversarial training method and extending its application to generative modeling and out-of-distribution detection", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yin|analyzing_and_improving_generative_adversarial_training_for_generative_modeling_and_outofdistribution_detection", "pdf": "/pdf/e3fbd7afb933742c99772e6059c83bbd775c75ef.pdf", "supplementary_material": "/attachment/7fc7331eb1d17022753419a67c9826c7319a6fce.zip", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=6Z09ydXbEv", "_bibtex": "@misc{\nyin2021analyzing,\ntitle={Analyzing and Improving Generative Adversarial Training for Generative Modeling and Out-of-Distribution Detection},\nauthor={Xuwang Yin and Shiying li and Gustavo Rohde},\nyear={2021},\nurl={https://openreview.net/forum?id=PsdsEbzxZWr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "PsdsEbzxZWr", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper927/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper927/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper927/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper927/Authors|ICLR.cc/2021/Conference/Paper927/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper927/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923865695, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper927/-/Official_Comment"}}}, {"id": "WHbaIksuqlQ", "original": null, "number": 6, "cdate": 1605611721332, "ddate": null, "tcdate": 1605611721332, "tmdate": 1605976641905, "tddate": null, "forum": "PsdsEbzxZWr", "replyto": "VYYX2wy0nT", "invitation": "ICLR.cc/2021/Conference/Paper927/-/Official_Comment", "content": {"title": "Response to Reviewer4 (cont.)", "comment": "**Lack of ablation study: I really liked the toy experiments in Figure 2, which justified the use of uniform distribution in the data space. However this only provides intuition for the higher dimensional cases, it is still necessary to conduct an ablation study to verify this is indeed the case for higher-dimensional cases, for scientific rigor.**\n\nWe completely agree. We have conducted an ablation study (Appendix H1) where we used the CIFAR-10 class 0 data as the $p_{k}$ dataset, and respectively used ImageNet and a CIFAR-10 subset (data from class 1 \u2013 class 9) as the $p_{-k}$ dataset. ImageNet is a much larger and more diverse dataset, and it is observed that the model trained with ImageNet archives much better standard and adversarial OOD detection performance (Table 9 vs. Table 10). This confirms our intuition that in high dimensional space a large and diverse dataset should be used. We also demonstrated that (with the same amout of training time) the model didn't develop capability for detecting real image OOD inputs when uniform noise is used as $p_{-k}$ (Table 11 and Table 12). \n\n**Unfair comparison: For image generation results, the authors\u2019 method was compared with GANs, and the motivation is that both of the methods are trained adversarially. However, the generators of GANs never got to see the real data during train time. Here authors are optimizing D, which is trained on real data, thus I don\u2019t think it is fair to compare with GANs.**\n\nAs stated in Section 5, we choose to compare with GANs because the proposed approach and GANs are closed linked (one solves the maximin problem and the other solves the minimax problem). Our evaluations are based on standard datasets, and we did not use any extra target distribution data to train our models. We also used the same model architecture and batch size to make sure setups are as close as possible. In GANs it is by design that the generator does not directly observe the data; the GANs frameworks relies on this design to learn data distribution.  Our approach, like many other generative models, requires the data to be directly observed by the module that is responsible for generation. \nThat said, we do agree that it is helpful to compare with state-of-the-art methods in the high-resolution regime. In Appendix L we have included 256x256 generation results on the CelebA-HQ, Bedroom, and ImageNet Dog dataset, and we encourage the reviewer to compare our results with results produced by state-of-the-art GANs models like StyleGAN and BigGAN.\n\n**Weak baseline for OOD (outlier exposure) and insufficient comparison: Outlier exposure is no longer the state of the art OOD detection methods and there are so many other methods that perform better than outlier exposure on CIFAR10, for example, see Detecting Out-of-Distribution Examples with Gram Matrices: https://arxiv.org/abs/1912.12510**\n\nWe completely agree. We have included in the section 5.1 an comparison with more baselines and state-of-the-art methods (including the suggested one), for both the standard (Table 3) and adversarial (Table 4) OOD detection task.\n\n**Lack of Related Work section: This paper does not have a related work section. The related works are very briefly discussed in the introduction, but I think that is far from enough. I understand there is a page limit, but even putting a related work section in the appendix would be very helpful for readers to get a more complete understanding of where this work stands.**\n\nWe completely agree. In appendix A we have provided a review on recent work related to out-of-distribution detection. Given the rapid development of this field, we are sure that not all related works are properly discussed. We appreciate it if the reviewer could point out missed work that we should address.\n\n**Unclear writings: See section 4.**\n\nThanks for the feedback. We tried to improve the writing by incorporating a discussion about the suggested ablation study. We hope things are clearer now.  If the reviewer finds any other place with unclear writing, we are happy to make the changes.\n\n**Typos: At the top of page 3: as a results -> as a result Bottom of page 8: a OOD -> an OOD**\n\nThanks, we have fixed the typo.\n\n**Ambiguity: \u201celementary mass\u201d was not defined before being used, it\u2019d be helpful to the readers to briefly define what \u201celementary mass\u201d means in this specific context.**\n\nThanks for pointing this out. \u201celementary mass\u201d is a term used in the optimal transport community without rigorous definition [1]. To avoid confusion, we have replaced the term with \u201cmass\u201d.\n\n[1] Peyr\u00e9, Gabriel, and Marco Cuturi. \"Computational Optimal Transport: With Applications to Data Science.\" Foundations and Trends\u00ae in Machine Learning 11.5-6 (2019): 355-607.\n\n**\u201cIn order to cause more local maxima to be eliminated\u201d could be worded better.**\n\nThanks for the suggestion. We agree that this part is not clearly written. We have revised this part (page 6) by considering the suggested ablation study.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper927/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper927/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Analyzing and Improving Generative Adversarial Training for Generative Modeling and Out-of-Distribution Detection", "authorids": ["~Xuwang_Yin2", "sl8jx@virginia.edu", "~Gustavo_Rohde1"], "authors": ["Xuwang Yin", "Shiying li", "Gustavo Rohde"], "keywords": ["Adversarial Training", "Generative Modeling", "Out-of-Distribution Detection", "GANs", "Generative adversarial networks"], "abstract": "Generative adversarial training (GAT) is a recently introduced adversarial defense method. Previous works have focused on empirical evaluations of its application to training robust predictive models. In this paper we focus on theoretical understanding of the GAT method and extending its application to generative modeling and out-of-distribution detection. We analyze the optimal solutions of the maximin formulation employed by the GAT objective, and make a comparative analysis of the minimax formulation employed by GANs. We use theoretical analysis and 2D simulations to understand the convergence property of the training algorithm. Based on these results, we develop an unconstrained GAT algorithm, and conduct comprehensive evaluations of the algorithm's application to image generation and adversarial out-of-distribution detection. Our results suggest that generative adversarial training is a promising new direction for the above applications.", "one-sentence_summary": "Theoretical understanding of the generative adversarial training method and extending its application to generative modeling and out-of-distribution detection", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yin|analyzing_and_improving_generative_adversarial_training_for_generative_modeling_and_outofdistribution_detection", "pdf": "/pdf/e3fbd7afb933742c99772e6059c83bbd775c75ef.pdf", "supplementary_material": "/attachment/7fc7331eb1d17022753419a67c9826c7319a6fce.zip", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=6Z09ydXbEv", "_bibtex": "@misc{\nyin2021analyzing,\ntitle={Analyzing and Improving Generative Adversarial Training for Generative Modeling and Out-of-Distribution Detection},\nauthor={Xuwang Yin and Shiying li and Gustavo Rohde},\nyear={2021},\nurl={https://openreview.net/forum?id=PsdsEbzxZWr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "PsdsEbzxZWr", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper927/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper927/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper927/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper927/Authors|ICLR.cc/2021/Conference/Paper927/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper927/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923865695, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper927/-/Official_Comment"}}}, {"id": "tSGmwiv2jGU", "original": null, "number": 7, "cdate": 1605611886033, "ddate": null, "tcdate": 1605611886033, "tmdate": 1605611886033, "tddate": null, "forum": "PsdsEbzxZWr", "replyto": "O62PxY7-hqV", "invitation": "ICLR.cc/2021/Conference/Paper927/-/Official_Comment", "content": {"title": "Response to Reviewer3", "comment": "Dear reviewer, \nThank you very much for your time and constructive feedback! We have conducted more experiments and revised the paper to address you concerns. Below, we respond to each of your comments and look forward to your further feedback.\n\n**In Section 3.1, for the convenience of analysis, the authors transformed equation (4) to (5). Are these two optimization problems equivalent? Seems not. Please add brief explanation.**\n\n**Algorithm 1 is claimed to solve equation (5). However, in Step 2 the maximization is still over B(x,\\epsilon). It seems that Algorithm 1 is to solve equation (4). Please explain.**\n\nThanks for pointing this out. Problem (4) to (5) are equivalent when $\\mathbb{B}(x,\\epsilon)=\\mathcal{S}$, which we have assumed (\u201cinstead of using $\\epsilon$-balls imposed on individual data samples, we use the notion of a common perturbation space\u2026\u201d ) but didn\u2019t explicitly stated. Similarly, we should have mentioned that Algorithm 1 solves problem (4) only when  $\\mathbb{B}(x,\\epsilon)=\\mathcal{S}$. We have made this more explicit in the updated manuscript.\n\n**OOD detection is an active area and there are a lot of papers regarding this. In experiment, the authors mainly focused on evaluating the proposed algorithm, and did not compare it with any other OOD detection methods, e.g., log-likelihood based and likelihood ratio based. Although K = 0 can be thought of as exposing to outliers, the resulting algorithm is still under GAT framework. As OOD detection is suggested as a main application of GAT, it is important to compare it with state-of-the-art.**\n\nWe completely agree. We have included a review of related work on OOD detection in appendix A. In the results section we also added a comparison with several baselines and state-of-the-art methods, for both the standard (Table 3) and adversarial (Table 4) OOD detection task.\n\n**It is hard to compare the quality of generation by only observing the generated images. It's better to show some quantitative comparison by using, e.g, FID score, which is commonly adopted for evaluating GANs.**\n\nWe agree. We have included the FID scores in table 8.\n\n**Some typos:  In equation (3), should be \\lambda.**\n\nThanks, we have fixed the notation error.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper927/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper927/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Analyzing and Improving Generative Adversarial Training for Generative Modeling and Out-of-Distribution Detection", "authorids": ["~Xuwang_Yin2", "sl8jx@virginia.edu", "~Gustavo_Rohde1"], "authors": ["Xuwang Yin", "Shiying li", "Gustavo Rohde"], "keywords": ["Adversarial Training", "Generative Modeling", "Out-of-Distribution Detection", "GANs", "Generative adversarial networks"], "abstract": "Generative adversarial training (GAT) is a recently introduced adversarial defense method. Previous works have focused on empirical evaluations of its application to training robust predictive models. In this paper we focus on theoretical understanding of the GAT method and extending its application to generative modeling and out-of-distribution detection. We analyze the optimal solutions of the maximin formulation employed by the GAT objective, and make a comparative analysis of the minimax formulation employed by GANs. We use theoretical analysis and 2D simulations to understand the convergence property of the training algorithm. Based on these results, we develop an unconstrained GAT algorithm, and conduct comprehensive evaluations of the algorithm's application to image generation and adversarial out-of-distribution detection. Our results suggest that generative adversarial training is a promising new direction for the above applications.", "one-sentence_summary": "Theoretical understanding of the generative adversarial training method and extending its application to generative modeling and out-of-distribution detection", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yin|analyzing_and_improving_generative_adversarial_training_for_generative_modeling_and_outofdistribution_detection", "pdf": "/pdf/e3fbd7afb933742c99772e6059c83bbd775c75ef.pdf", "supplementary_material": "/attachment/7fc7331eb1d17022753419a67c9826c7319a6fce.zip", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=6Z09ydXbEv", "_bibtex": "@misc{\nyin2021analyzing,\ntitle={Analyzing and Improving Generative Adversarial Training for Generative Modeling and Out-of-Distribution Detection},\nauthor={Xuwang Yin and Shiying li and Gustavo Rohde},\nyear={2021},\nurl={https://openreview.net/forum?id=PsdsEbzxZWr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "PsdsEbzxZWr", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper927/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper927/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper927/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper927/Authors|ICLR.cc/2021/Conference/Paper927/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper927/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923865695, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper927/-/Official_Comment"}}}, {"id": "O62PxY7-hqV", "original": null, "number": 1, "cdate": 1602776293917, "ddate": null, "tcdate": 1602776293917, "tmdate": 1605024573480, "tddate": null, "forum": "PsdsEbzxZWr", "replyto": "PsdsEbzxZWr", "invitation": "ICLR.cc/2021/Conference/Paper927/-/Official_Review", "content": {"title": "Theoretical analysis of GAT; but need to compare with state-of-the-art and add quantitative comparison on generation.", "review": "The authors investigated generative adversarial training (GAT) and made two main contributions: 1) theoretically analyzed its maxmin objective and compared  with the minmax formulation used by GANs, and 2) applied GAT to OOD detection and generative models. Extensive experiments were performed for evaluating the proposed algorithm.\n\n############################\n\nStrong points:\n\n. The authors analyzed the maxmin formulation of GAT theoretically by deriving the optimal solutions under different scenarios, and also showed the conditions under which the algorithm converges to the optimal solution.\n\n. The authors pointed out the difference between the maxmin formulation and the minmax formulation used by GANs, and extended GAT for two applications: OOD detection and generative models.\n\n. Extensive experiments were performed for evaluating the proposed algorithm.\n\n############################\n\nWeak points:\n\n. In Section 3.1, for the convenience of analysis, the authors transformed equation (4) to (5). Are these two optimization problems equivalent? Seems not. Please add brief explanation.\n\n. Algorithm 1 is claimed to solve equation (5). However, in Step 2 the maximization is still over B(x,\\epsilon). It seems that Algorithm 1 is to solve equation (4). Please explain.\n\n. Section 3.2: a practical consideration is that Step 2 cannot be perfectly solved and the authors thus proposed to use a p_{-k} that is uniformly distributed in the data space. The authors claimed that  such condition always results in no maxima and global maxima at Supp(p_k). Can this conclusion be theoretically proved? One potential limitation is that using a p_{-k} that is uniformly distributed could lead to a long training time for achieving a satisfactory performance especially in high dimensional.\n\n. OOD detection is an active area and there are a lot of papers regarding this. In experiment, the authors mainly focused on evaluating the proposed algorithm, and did not compare it with any other OOD detection methods, e.g., log-likelihood based and likelihood ratio based. Although K = 0 can be thought of as exposing to outliers, the resulting algorithm is still under GAT framework. As OOD detection is suggested as a main application of GAT, it is important to compare it with state-of-the-art.\n\n. It is hard to compare the quality of generation by only observing the generated images. It's better to show some quantitative comparison by using, e.g, FID score, which is commonly adopted for evaluating GANs. \n\n####################\nSome typos:\n\n. In equation (3), should be \\lambda.", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper927/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper927/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Analyzing and Improving Generative Adversarial Training for Generative Modeling and Out-of-Distribution Detection", "authorids": ["~Xuwang_Yin2", "sl8jx@virginia.edu", "~Gustavo_Rohde1"], "authors": ["Xuwang Yin", "Shiying li", "Gustavo Rohde"], "keywords": ["Adversarial Training", "Generative Modeling", "Out-of-Distribution Detection", "GANs", "Generative adversarial networks"], "abstract": "Generative adversarial training (GAT) is a recently introduced adversarial defense method. Previous works have focused on empirical evaluations of its application to training robust predictive models. In this paper we focus on theoretical understanding of the GAT method and extending its application to generative modeling and out-of-distribution detection. We analyze the optimal solutions of the maximin formulation employed by the GAT objective, and make a comparative analysis of the minimax formulation employed by GANs. We use theoretical analysis and 2D simulations to understand the convergence property of the training algorithm. Based on these results, we develop an unconstrained GAT algorithm, and conduct comprehensive evaluations of the algorithm's application to image generation and adversarial out-of-distribution detection. Our results suggest that generative adversarial training is a promising new direction for the above applications.", "one-sentence_summary": "Theoretical understanding of the generative adversarial training method and extending its application to generative modeling and out-of-distribution detection", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yin|analyzing_and_improving_generative_adversarial_training_for_generative_modeling_and_outofdistribution_detection", "pdf": "/pdf/e3fbd7afb933742c99772e6059c83bbd775c75ef.pdf", "supplementary_material": "/attachment/7fc7331eb1d17022753419a67c9826c7319a6fce.zip", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=6Z09ydXbEv", "_bibtex": "@misc{\nyin2021analyzing,\ntitle={Analyzing and Improving Generative Adversarial Training for Generative Modeling and Out-of-Distribution Detection},\nauthor={Xuwang Yin and Shiying li and Gustavo Rohde},\nyear={2021},\nurl={https://openreview.net/forum?id=PsdsEbzxZWr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "PsdsEbzxZWr", "replyto": "PsdsEbzxZWr", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper927/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538131553, "tmdate": 1606915767575, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper927/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper927/-/Official_Review"}}}], "count": 15}