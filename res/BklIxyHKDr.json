{"notes": [{"id": "BklIxyHKDr", "original": "SkxeAVouDS", "number": 1507, "cdate": 1569439470109, "ddate": null, "tcdate": 1569439470109, "tmdate": 1577168277489, "tddate": null, "forum": "BklIxyHKDr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Deep k-NN for Noisy Labels", "authors": ["Dara Bahri", "Heinrich Jiang", "Maya Gupta"], "authorids": ["dbahri@google.com", "heinrichj@google.com", "mayagupta@google.com"], "keywords": [], "abstract": "Modern machine learning models are often trained on examples with noisy labels that hurt performance and are hard to identify. In this paper, we provide an empirical study showing that a simple $k$-nearest neighbor-based filtering approach on the logit layer of a preliminary model can  remove mislabeled training data and produce more accurate models than some recently proposed methods. We also provide new statistical guarantees into its efficacy.", "pdf": "/pdf/85b02d1aa7edbbea2cf01fab7b0adf90d7ccbbb0.pdf", "paperhash": "bahri|deep_knn_for_noisy_labels", "original_pdf": "/attachment/54a9fcd601a5dd0bc2dacf05143ddbe39e6cd5b7.pdf", "_bibtex": "@misc{\nbahri2020deep,\ntitle={Deep k-{\\{}NN{\\}} for Noisy Labels},\nauthor={Dara Bahri and Heinrich Jiang and Maya Gupta},\nyear={2020},\nurl={https://openreview.net/forum?id=BklIxyHKDr}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 6, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "GOXrTep2ho", "original": null, "number": 1, "cdate": 1576798725104, "ddate": null, "tcdate": 1576798725104, "tmdate": 1576800911401, "tddate": null, "forum": "BklIxyHKDr", "replyto": "BklIxyHKDr", "invitation": "ICLR.cc/2020/Conference/Paper1507/-/Decision", "content": {"decision": "Reject", "comment": "The paper proposed and analyze a k-NN method for identifying corrupted labels for training deep neural networks.\n\nAlthough a reviewer pointed out that the noisy k-NN contribution is interesting, I think the paper can be much improved further due to the followings:\n\n(a) Lack of state-of-the-art baselines to compare.\n(b) Lack of important recent related work, i.e., \"Robust Inference via Generative Classifiers for Handling Noisy Labels\" from ICML 2019 (see https://arxiv.org/abs/1901.11300). The paper also runs a clustering-like algorithm for handling noisy labels, and the authors should compare and discuss why the proposed method is superior.\n(c) Poor write-up, e.g., address what is missing in existing methods from many different perspectives as this is a quite well-studied popular problem.\n\nHence, I recommend rejection.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep k-NN for Noisy Labels", "authors": ["Dara Bahri", "Heinrich Jiang", "Maya Gupta"], "authorids": ["dbahri@google.com", "heinrichj@google.com", "mayagupta@google.com"], "keywords": [], "abstract": "Modern machine learning models are often trained on examples with noisy labels that hurt performance and are hard to identify. In this paper, we provide an empirical study showing that a simple $k$-nearest neighbor-based filtering approach on the logit layer of a preliminary model can  remove mislabeled training data and produce more accurate models than some recently proposed methods. We also provide new statistical guarantees into its efficacy.", "pdf": "/pdf/85b02d1aa7edbbea2cf01fab7b0adf90d7ccbbb0.pdf", "paperhash": "bahri|deep_knn_for_noisy_labels", "original_pdf": "/attachment/54a9fcd601a5dd0bc2dacf05143ddbe39e6cd5b7.pdf", "_bibtex": "@misc{\nbahri2020deep,\ntitle={Deep k-{\\{}NN{\\}} for Noisy Labels},\nauthor={Dara Bahri and Heinrich Jiang and Maya Gupta},\nyear={2020},\nurl={https://openreview.net/forum?id=BklIxyHKDr}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "BklIxyHKDr", "replyto": "BklIxyHKDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795709948, "tmdate": 1576800258832, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1507/-/Decision"}}}, {"id": "Bye4OdB4KS", "original": null, "number": 1, "cdate": 1571211371932, "ddate": null, "tcdate": 1571211371932, "tmdate": 1574250281760, "tddate": null, "forum": "BklIxyHKDr", "replyto": "BklIxyHKDr", "invitation": "ICLR.cc/2020/Conference/Paper1507/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #3", "review": "This paper provided \"an empirical study showing that a simple k-nearest neighbor-based filtering approach on the logit layer of a preliminary model can remove mislabeled training data and produce more accurate models than some recently proposed methods\". Even though it has many theoretical analysis and experiments, the paper itself is poorly written. There is no intuitive discussion on what is missing in existing methods, why the proposed method can be better, and when the proposed method may also fail.\n\nNote that an important related work is missing, namely \"Robust Inference via Generative Classifiers for Handling Noisy Labels\" from ICML 2019 (see https://arxiv.org/abs/1901.11300). The idea of that paper is also making use of the learned representations of ANY discriminative neural classifier, where the geometric information of the hidden feature spaces can help to distinguish correctly and incorrectly labeled training data. That paper was a 20-min long oral presentation at Hall A (i.e., one of the most crowded sessions), and the authors should really compare with it both conceptually and experimentally.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1507/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1507/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep k-NN for Noisy Labels", "authors": ["Dara Bahri", "Heinrich Jiang", "Maya Gupta"], "authorids": ["dbahri@google.com", "heinrichj@google.com", "mayagupta@google.com"], "keywords": [], "abstract": "Modern machine learning models are often trained on examples with noisy labels that hurt performance and are hard to identify. In this paper, we provide an empirical study showing that a simple $k$-nearest neighbor-based filtering approach on the logit layer of a preliminary model can  remove mislabeled training data and produce more accurate models than some recently proposed methods. We also provide new statistical guarantees into its efficacy.", "pdf": "/pdf/85b02d1aa7edbbea2cf01fab7b0adf90d7ccbbb0.pdf", "paperhash": "bahri|deep_knn_for_noisy_labels", "original_pdf": "/attachment/54a9fcd601a5dd0bc2dacf05143ddbe39e6cd5b7.pdf", "_bibtex": "@misc{\nbahri2020deep,\ntitle={Deep k-{\\{}NN{\\}} for Noisy Labels},\nauthor={Dara Bahri and Heinrich Jiang and Maya Gupta},\nyear={2020},\nurl={https://openreview.net/forum?id=BklIxyHKDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BklIxyHKDr", "replyto": "BklIxyHKDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1507/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1507/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575473127493, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1507/Reviewers"], "noninvitees": [], "tcdate": 1570237736376, "tmdate": 1575473127503, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1507/-/Official_Review"}}}, {"id": "S1lnl2tCKB", "original": null, "number": 2, "cdate": 1571884020128, "ddate": null, "tcdate": 1571884020128, "tmdate": 1574058994822, "tddate": null, "forum": "BklIxyHKDr", "replyto": "BklIxyHKDr", "invitation": "ICLR.cc/2020/Conference/Paper1507/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "1: Reject", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #2", "review": "The authors propose to apply k-NN on the intermediate representations of neural networks for data cleaning. They prove some theoretical properties of k-NN and demonstrate that the proposed data cleaning approach is effective for some tasks.\n\nIt is recommended to reject the paper, with the following concerns in mind.\n\n(1) The proposed approach is not deeply studied. For instance, what's the difference of applying k-NN on raw features, the earlier representations, the later representations, or even \"all\" representations? What's the effect of similarity/distance functions on the k-NN? Without the deeper study, Section 3 is at best a naive use of k-NN for data cleaning, and it is not clear whether the contribution is substantial.\n\n(2) The theoretical analysis does not seem related to applying k-NN to *deep learning* intermediate features. It seems more related to applying k-NN in general. If so, it is also not clear how the theoretical analysis advances current knowledge about k-NN. Are the results original or known? What are the best theoretical results in the literature to compare with?\n\nI thank the authors for answering about the originality. I agree that the original theoretical results is an important contribution on its own, but putting it in the context of deep learning is arguably not the best angle to present the contribution.\n\n(3) It is not clear whether the experiments are compared with respect to state-of-the-art (or at least it is hard to see from Section 2). It seems that rather straightforward baselines are being compared.\n\nI thank the authors for clarifying this.\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1507/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1507/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep k-NN for Noisy Labels", "authors": ["Dara Bahri", "Heinrich Jiang", "Maya Gupta"], "authorids": ["dbahri@google.com", "heinrichj@google.com", "mayagupta@google.com"], "keywords": [], "abstract": "Modern machine learning models are often trained on examples with noisy labels that hurt performance and are hard to identify. In this paper, we provide an empirical study showing that a simple $k$-nearest neighbor-based filtering approach on the logit layer of a preliminary model can  remove mislabeled training data and produce more accurate models than some recently proposed methods. We also provide new statistical guarantees into its efficacy.", "pdf": "/pdf/85b02d1aa7edbbea2cf01fab7b0adf90d7ccbbb0.pdf", "paperhash": "bahri|deep_knn_for_noisy_labels", "original_pdf": "/attachment/54a9fcd601a5dd0bc2dacf05143ddbe39e6cd5b7.pdf", "_bibtex": "@misc{\nbahri2020deep,\ntitle={Deep k-{\\{}NN{\\}} for Noisy Labels},\nauthor={Dara Bahri and Heinrich Jiang and Maya Gupta},\nyear={2020},\nurl={https://openreview.net/forum?id=BklIxyHKDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BklIxyHKDr", "replyto": "BklIxyHKDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1507/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1507/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575473127493, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1507/Reviewers"], "noninvitees": [], "tcdate": 1570237736376, "tmdate": 1575473127503, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1507/-/Official_Review"}}}, {"id": "HyeMkEW2ir", "original": null, "number": 4, "cdate": 1573815258371, "ddate": null, "tcdate": 1573815258371, "tmdate": 1573815258371, "tddate": null, "forum": "BklIxyHKDr", "replyto": "S1lnl2tCKB", "invitation": "ICLR.cc/2020/Conference/Paper1507/-/Official_Comment", "content": {"title": "response", "comment": "Re (2):\nThe reviewer is correct that the analysis does not look at the intermediate features. Still, we believe that our representation-agnostic results are quite general and significant. They are general because the results say that given any initial representation, under mild assumptions, the kNN will be able to recover the noisy labels at least as well as any method up to logarithmic factors. The results are original --they use some insights from a recent analysis of kNN in the noiseless setting (Jiang 2019) and we provide a novel nonparametric assumption (kNN spread, Def 2) and show precise theoretical guarantees under this quantity as well as the other quantities in more classical results. Moreover, compared to previous works which analyze the noisy kNN setting (which we\u2019ve cited), to the best of our knowledge, our results are the only finite-sample results while the previous works are asymptotic.\n\nRe (3):\nWe consider Gold Loss Correction a state-of-the-art method and the other methods very competitive. We have added two new baselines as well.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1507/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1507/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep k-NN for Noisy Labels", "authors": ["Dara Bahri", "Heinrich Jiang", "Maya Gupta"], "authorids": ["dbahri@google.com", "heinrichj@google.com", "mayagupta@google.com"], "keywords": [], "abstract": "Modern machine learning models are often trained on examples with noisy labels that hurt performance and are hard to identify. In this paper, we provide an empirical study showing that a simple $k$-nearest neighbor-based filtering approach on the logit layer of a preliminary model can  remove mislabeled training data and produce more accurate models than some recently proposed methods. We also provide new statistical guarantees into its efficacy.", "pdf": "/pdf/85b02d1aa7edbbea2cf01fab7b0adf90d7ccbbb0.pdf", "paperhash": "bahri|deep_knn_for_noisy_labels", "original_pdf": "/attachment/54a9fcd601a5dd0bc2dacf05143ddbe39e6cd5b7.pdf", "_bibtex": "@misc{\nbahri2020deep,\ntitle={Deep k-{\\{}NN{\\}} for Noisy Labels},\nauthor={Dara Bahri and Heinrich Jiang and Maya Gupta},\nyear={2020},\nurl={https://openreview.net/forum?id=BklIxyHKDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BklIxyHKDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1507/Authors", "ICLR.cc/2020/Conference/Paper1507/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1507/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1507/Reviewers", "ICLR.cc/2020/Conference/Paper1507/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1507/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1507/Authors|ICLR.cc/2020/Conference/Paper1507/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504154985, "tmdate": 1576860536524, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1507/Authors", "ICLR.cc/2020/Conference/Paper1507/Reviewers", "ICLR.cc/2020/Conference/Paper1507/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1507/-/Official_Comment"}}}, {"id": "S1xjoW-2ir", "original": null, "number": 3, "cdate": 1573814690755, "ddate": null, "tcdate": 1573814690755, "tmdate": 1573814690755, "tddate": null, "forum": "BklIxyHKDr", "replyto": "rkxFL70AYH", "invitation": "ICLR.cc/2020/Conference/Paper1507/-/Official_Comment", "content": {"title": "Response", "comment": "Thank you for your detailed review.\n\nRe: \"Since the theory highly depends on this quantity, the authors should after Definition 2 justify why they chose to base their results around S_k, and why intuitively it is the right quantity.\"\n\nS_k is a natural way to quantify how spread out a set of points is in the k-NN setting which helps us quantify the impact of points with corrupted labels on the k-NN classifier. In the theoretical results, note that we only use S_2 to keep the statements simple. That is, S_2 is the minimum pairwise distance --we will clarify this in the text and stick with this simpler and perhaps more intuitive definition.\n-----------\nRe: \"I encourage another experiment where the label corruption is not completely at random, that is it depends on the values of x itself. \"\n\nWe decided to focus on label-dependent noise, which we capture in our \"hard-flip\" corruption, since it's far more common than input feature-dependent noise both in practice and in the existing literature.\n-----------\nRe: \"In particular I encourage a synthetic experiment where the authors look at corruptions with varying S_k (minimum k-NN spread) values, to empirically verify the their theory holds and this really is a meaningful quantity to consider in the label-corruption setting.\"\n\nThis is a good suggestion. We have added a new section to the main text to study this on a two-Gaussian binary classification simulation where we insert corruptions on a grid. We show that as the grid width gets smaller, the more clean samples we need.\n-----------\nRe: \"Why don't the authors show the results for vanilla deep kNN trained on the full (noisy + clean) dataset in their experiments. This seems important to ascertain the benefits that might be attributed to simply switching to kNN.  Or is deep kNN generally worse that the original model trained on the full dataset?\"\n\nWe have added this as a new baseline (\"kNN-Classify\") for all our experiments. We found that it performs better than the original model trained on the full data, but that it is still worse than our kNN filtering method, all in all.\n-----------\nRe: \"Why didn't the authors show the original model trained on the full dataset?\nIs it because it always does worse than all the baselines considered in the paper?\nI would expect it sometimes does much better than Control (eg. when noise rates are low), and this is the straightforward approach must practitioners would use.\"\n\nThis is a good suggestion. We have added this as a new baseline (\"Full\") for all our experiments.\n-----------\nRe: Why don't the authors present the accuracy of the k-NN method at identifying corrupted datapoints vs the other methods that aim to explicitly identify the corrupted datapoints?\nIn general, it seems the authors did not compare other filtering baselines, which would be more related to their method, for example:\nLearning with Confident Examples: Rank Pruning for Robust Classification with Noisy Labels\nNorthcutt et al. (2017). https://arxiv.org/abs/1705.01936\n\nThis work, while relevant, focuses solely on binary classification while our method is applicable to general multiclass classification.\n-----------\nRe: \"It would be valuable to the scientific community if the authors can comment on: \nRolnick et al. (2018). Deep Learning is Robust to Massive Label Noise. https://arxiv.org/pdf/1705.10694.pdf  \n\n- Some similar looking ideas have been proposed in: \n\nGao et al. (2018). On the Resistance of Nearest Neighbor To Random Noisy Labels\nhttps://pdfs.semanticscholar.org/4227/918020c15b719c415e93eb63d436583f1745.pdf\n\nParvin et al. (2010). A Modification on K-Nearest Neighbor Classifier.\nhttps://globaljournals.org/GJCST_Volume10/7-A-Modification-on-K-Nearest-Neighbor-Classifier.pdf\n\nso the authors should contrast their method/analysis against those papers.\"\n\nWe have now commented on these papers in the Related Works. "}, "signatures": ["ICLR.cc/2020/Conference/Paper1507/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1507/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep k-NN for Noisy Labels", "authors": ["Dara Bahri", "Heinrich Jiang", "Maya Gupta"], "authorids": ["dbahri@google.com", "heinrichj@google.com", "mayagupta@google.com"], "keywords": [], "abstract": "Modern machine learning models are often trained on examples with noisy labels that hurt performance and are hard to identify. In this paper, we provide an empirical study showing that a simple $k$-nearest neighbor-based filtering approach on the logit layer of a preliminary model can  remove mislabeled training data and produce more accurate models than some recently proposed methods. We also provide new statistical guarantees into its efficacy.", "pdf": "/pdf/85b02d1aa7edbbea2cf01fab7b0adf90d7ccbbb0.pdf", "paperhash": "bahri|deep_knn_for_noisy_labels", "original_pdf": "/attachment/54a9fcd601a5dd0bc2dacf05143ddbe39e6cd5b7.pdf", "_bibtex": "@misc{\nbahri2020deep,\ntitle={Deep k-{\\{}NN{\\}} for Noisy Labels},\nauthor={Dara Bahri and Heinrich Jiang and Maya Gupta},\nyear={2020},\nurl={https://openreview.net/forum?id=BklIxyHKDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BklIxyHKDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1507/Authors", "ICLR.cc/2020/Conference/Paper1507/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1507/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1507/Reviewers", "ICLR.cc/2020/Conference/Paper1507/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1507/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1507/Authors|ICLR.cc/2020/Conference/Paper1507/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504154985, "tmdate": 1576860536524, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1507/Authors", "ICLR.cc/2020/Conference/Paper1507/Reviewers", "ICLR.cc/2020/Conference/Paper1507/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1507/-/Official_Comment"}}}, {"id": "rkxFL70AYH", "original": null, "number": 3, "cdate": 1571902289263, "ddate": null, "tcdate": 1571902289263, "tmdate": 1572972459544, "tddate": null, "forum": "BklIxyHKDr", "replyto": "BklIxyHKDr", "invitation": "ICLR.cc/2020/Conference/Paper1507/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a k-NN method for identifying corrupted labels, and then applies this k-NN in the representation space of a deep neural net rather than the original feature space. Overall the paper is well written and the results look quite convincing\n\nThe theory appears to be important (if somewhat straightforward-looking) contributions of existing k-NN theory to the corrupted labels setting, based on the key quantity the authors defined as S_k, the minimum k-NN spread.\n\nSince the theory highly depends on this quantity, the authors should after Definition 2 justify why they chose to base their results around S_k, and why intuitively it is the right quantity.\n\n- I encourage another experiment where the label corruption is not completely at random, that is it depends on the values of x itself.  \n\n- In particular I encourage a synthetic experiment where the authors look at corruptions with varying S_k (minimum k-NN spread) values, to empirically verify the their theory holds and this really is a meaningful quantity to consider in the label-corruption setting.\n\n- Why don't the authors show the results for vanilla deep kNN trained on the full (noisy + clean) dataset in their experiments. This seems important to ascertain the benefits that might be attributed to simply switching to kNN.  Or is deep kNN generally worse that the original model trained on the full dataset?\n\n- Why didn't the authors show the original model trained on the full dataset?\nIs it because it always does worse than all the baselines considered in the paper?\nI would expect it sometimes does much better than Control (eg. when noise rates are low), and this is the straightforward approach must practitioners would use.\n\n- Why don't the authors present the accuracy of the k-NN method at identifying corrupted datapoints vs the other methods that aim to explicitly identify the corrupted datapoints?\nIn general, it seems the authors did not compare other filtering baselines, which would be more related to their method, for example:\n\nLearning with Confident Examples: Rank Pruning for Robust Classification with Noisy Labels\nNorthcutt et al. (2017). https://arxiv.org/abs/1705.01936\n\n\n- It would be valuable to the scientific community if the authors can comment on: \nRolnick et al. (2018). Deep Learning is Robust to Massive Label Noise. https://arxiv.org/pdf/1705.10694.pdf  \n\n- Some similar looking ideas have been proposed in: \n\nGao et al. (2018). On the Resistance of Nearest Neighbor To Random Noisy Labels\nhttps://pdfs.semanticscholar.org/4227/918020c15b719c415e93eb63d436583f1745.pdf\n\nParvin et al. (2010). A Modification on K-Nearest Neighbor Classifier.\nhttps://globaljournals.org/GJCST_Volume10/7-A-Modification-on-K-Nearest-Neighbor-Classifier.pdf\n\nso the authors should contrast their method/analysis against those papers.\n\n\n- In Thm 1: \"w.r.t. X\" should be \"w.r.t. x\" (lower case)\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1507/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1507/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep k-NN for Noisy Labels", "authors": ["Dara Bahri", "Heinrich Jiang", "Maya Gupta"], "authorids": ["dbahri@google.com", "heinrichj@google.com", "mayagupta@google.com"], "keywords": [], "abstract": "Modern machine learning models are often trained on examples with noisy labels that hurt performance and are hard to identify. In this paper, we provide an empirical study showing that a simple $k$-nearest neighbor-based filtering approach on the logit layer of a preliminary model can  remove mislabeled training data and produce more accurate models than some recently proposed methods. We also provide new statistical guarantees into its efficacy.", "pdf": "/pdf/85b02d1aa7edbbea2cf01fab7b0adf90d7ccbbb0.pdf", "paperhash": "bahri|deep_knn_for_noisy_labels", "original_pdf": "/attachment/54a9fcd601a5dd0bc2dacf05143ddbe39e6cd5b7.pdf", "_bibtex": "@misc{\nbahri2020deep,\ntitle={Deep k-{\\{}NN{\\}} for Noisy Labels},\nauthor={Dara Bahri and Heinrich Jiang and Maya Gupta},\nyear={2020},\nurl={https://openreview.net/forum?id=BklIxyHKDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BklIxyHKDr", "replyto": "BklIxyHKDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1507/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1507/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575473127493, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1507/Reviewers"], "noninvitees": [], "tcdate": 1570237736376, "tmdate": 1575473127503, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1507/-/Official_Review"}}}], "count": 7}