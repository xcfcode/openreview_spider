{"notes": [{"id": "ryxHii09KQ", "original": "BygMMIt9t7", "number": 618, "cdate": 1538087836697, "ddate": null, "tcdate": 1538087836697, "tmdate": 1545355402660, "tddate": null, "forum": "ryxHii09KQ", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "In Your Pace: Learning the Right Example at the Right Time", "abstract": "Training neural networks is traditionally done by sequentially providing random mini-batches sampled uniformly from the entire dataset. In our work, we show that sampling mini-batches non-uniformly can both enhance the speed of learning and improve the final accuracy of the trained network. Specifically, we decompose the problem using the principles of curriculum learning: first, we sort the data by some difficulty measure; second, we sample mini-batches with a gradually increasing level of difficulty. We focus on CNNs trained on image recognition. Initially, we define the difficulty of a training image using transfer learning from some competitive \"teacher\" network trained on the Imagenet database, showing improvement in learning speed and final performance for both small and competitive networks, using the CIFAR-10 and the CIFAR-100 datasets. We then suggest a bootstrap alternative to evaluate the difficulty of points using the same network without relying on a \"teacher\" network, thus increasing the applicability of our suggested method. We compare this approach to a related version of Self-Paced Learning, showing that our method benefits learning while SPL impairs it.", "keywords": ["Curriculum Learning", "Transfer Learning", "Self-Paced Learning", "Image Recognition"], "authorids": ["guy.hacohen@mail.huji.ac.il", "daphna@cs.huji.ac.il"], "authors": ["Guy Hacohen", "Daphna Weinshall"], "TL;DR": "We provide a formal definition of curriculum learning for deep neural networks, empirically showing how it can improve learning performance without additional human supervision and in a problem-free manner.", "pdf": "/pdf/c30ee6e4856bb888bd4943a6148c321ed98b5dfe.pdf", "paperhash": "hacohen|in_your_pace_learning_the_right_example_at_the_right_time", "_bibtex": "@misc{\nhacohen2019in,\ntitle={In Your Pace: Learning the Right Example at the Right Time},\nauthor={Guy Hacohen and Daphna Weinshall},\nyear={2019},\nurl={https://openreview.net/forum?id=ryxHii09KQ},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "H1lqyTzegV", "original": null, "number": 1, "cdate": 1544723682428, "ddate": null, "tcdate": 1544723682428, "tmdate": 1545354510221, "tddate": null, "forum": "ryxHii09KQ", "replyto": "ryxHii09KQ", "invitation": "ICLR.cc/2019/Conference/-/Paper618/Meta_Review", "content": {"metareview": "This paper presents an interesting strategy of curriculum learning for training neural networks, where mini-batches of samples are formed with a gradually increasing level of difficulty.  \nWhile reviewers acknowledge the importance of studying the curriculum learning and the potential usefulness of the proposed approach for training neural networks, they raised several important concerns that place this paper bellow the acceptance bar: (1) empirical results are not convincing (R2, R3); comparisons on other datasets (large-scale) and with state-of-the-art methods would substantially strengthen the evaluation (R3); see also R2\u2019s concerns regarding the comprehensive study; (2) important references and baseline methods are missing \u2013 see R2\u2019s suggestions how to improve; (3) limited technical novelty -- R1 has provided a very detailed review questioning novelty of the proposed approach w.r.t. Weinshall et al, 2018.  \nAnother suggestions to further strengthen and extend the manuscript is to consider curriculum and anti-curriculum learning for increasing performance (R1). \nThe authors provided additional experiment on a subset of 7 classes from the  ImageNet dataset, but this does not show the advantage of the proposed model in a large-scale learning setting. \nThe AC decided that addressing (1)-(3) is indeed important for understanding the contribution in this work, and it is difficult to assess the scope of the contribution without addressing them. \n", "confidence": "5: The area chair is absolutely certain", "recommendation": "Reject", "title": "Meta-Review"}, "signatures": ["ICLR.cc/2019/Conference/Paper618/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper618/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "In Your Pace: Learning the Right Example at the Right Time", "abstract": "Training neural networks is traditionally done by sequentially providing random mini-batches sampled uniformly from the entire dataset. In our work, we show that sampling mini-batches non-uniformly can both enhance the speed of learning and improve the final accuracy of the trained network. Specifically, we decompose the problem using the principles of curriculum learning: first, we sort the data by some difficulty measure; second, we sample mini-batches with a gradually increasing level of difficulty. We focus on CNNs trained on image recognition. Initially, we define the difficulty of a training image using transfer learning from some competitive \"teacher\" network trained on the Imagenet database, showing improvement in learning speed and final performance for both small and competitive networks, using the CIFAR-10 and the CIFAR-100 datasets. We then suggest a bootstrap alternative to evaluate the difficulty of points using the same network without relying on a \"teacher\" network, thus increasing the applicability of our suggested method. We compare this approach to a related version of Self-Paced Learning, showing that our method benefits learning while SPL impairs it.", "keywords": ["Curriculum Learning", "Transfer Learning", "Self-Paced Learning", "Image Recognition"], "authorids": ["guy.hacohen@mail.huji.ac.il", "daphna@cs.huji.ac.il"], "authors": ["Guy Hacohen", "Daphna Weinshall"], "TL;DR": "We provide a formal definition of curriculum learning for deep neural networks, empirically showing how it can improve learning performance without additional human supervision and in a problem-free manner.", "pdf": "/pdf/c30ee6e4856bb888bd4943a6148c321ed98b5dfe.pdf", "paperhash": "hacohen|in_your_pace_learning_the_right_example_at_the_right_time", "_bibtex": "@misc{\nhacohen2019in,\ntitle={In Your Pace: Learning the Right Example at the Right Time},\nauthor={Guy Hacohen and Daphna Weinshall},\nyear={2019},\nurl={https://openreview.net/forum?id=ryxHii09KQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper618/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353150941, "tddate": null, "super": null, "final": null, "reply": {"forum": "ryxHii09KQ", "replyto": "ryxHii09KQ", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper618/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper618/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper618/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353150941}}}, {"id": "HJle-twYRQ", "original": null, "number": 3, "cdate": 1543235832189, "ddate": null, "tcdate": 1543235832189, "tmdate": 1543235832189, "tddate": null, "forum": "ryxHii09KQ", "replyto": "ryxHii09KQ", "invitation": "ICLR.cc/2019/Conference/-/Paper618/Official_Comment", "content": {"title": "Thank you for your reviews!", "comment": "Following the reviews, we've added a section showing that curriculum by transfer achieves similar qualitative improvements to network generalization also when trained on a subset of the popular ImageNet dataset.\nWe've included a broader review of the relevant literature, emphasizing the difference between previous works and ours."}, "signatures": ["ICLR.cc/2019/Conference/Paper618/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper618/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper618/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "In Your Pace: Learning the Right Example at the Right Time", "abstract": "Training neural networks is traditionally done by sequentially providing random mini-batches sampled uniformly from the entire dataset. In our work, we show that sampling mini-batches non-uniformly can both enhance the speed of learning and improve the final accuracy of the trained network. Specifically, we decompose the problem using the principles of curriculum learning: first, we sort the data by some difficulty measure; second, we sample mini-batches with a gradually increasing level of difficulty. We focus on CNNs trained on image recognition. Initially, we define the difficulty of a training image using transfer learning from some competitive \"teacher\" network trained on the Imagenet database, showing improvement in learning speed and final performance for both small and competitive networks, using the CIFAR-10 and the CIFAR-100 datasets. We then suggest a bootstrap alternative to evaluate the difficulty of points using the same network without relying on a \"teacher\" network, thus increasing the applicability of our suggested method. We compare this approach to a related version of Self-Paced Learning, showing that our method benefits learning while SPL impairs it.", "keywords": ["Curriculum Learning", "Transfer Learning", "Self-Paced Learning", "Image Recognition"], "authorids": ["guy.hacohen@mail.huji.ac.il", "daphna@cs.huji.ac.il"], "authors": ["Guy Hacohen", "Daphna Weinshall"], "TL;DR": "We provide a formal definition of curriculum learning for deep neural networks, empirically showing how it can improve learning performance without additional human supervision and in a problem-free manner.", "pdf": "/pdf/c30ee6e4856bb888bd4943a6148c321ed98b5dfe.pdf", "paperhash": "hacohen|in_your_pace_learning_the_right_example_at_the_right_time", "_bibtex": "@misc{\nhacohen2019in,\ntitle={In Your Pace: Learning the Right Example at the Right Time},\nauthor={Guy Hacohen and Daphna Weinshall},\nyear={2019},\nurl={https://openreview.net/forum?id=ryxHii09KQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper618/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621611129, "tddate": null, "super": null, "final": null, "reply": {"forum": "ryxHii09KQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper618/Authors", "ICLR.cc/2019/Conference/Paper618/Reviewers", "ICLR.cc/2019/Conference/Paper618/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper618/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper618/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper618/Authors|ICLR.cc/2019/Conference/Paper618/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper618/Reviewers", "ICLR.cc/2019/Conference/Paper618/Authors", "ICLR.cc/2019/Conference/Paper618/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621611129}}}, {"id": "rJxxa4IY3m", "original": null, "number": 3, "cdate": 1541133496323, "ddate": null, "tcdate": 1541133496323, "tmdate": 1541533839123, "tddate": null, "forum": "ryxHii09KQ", "replyto": "ryxHii09KQ", "invitation": "ICLR.cc/2019/Conference/-/Paper618/Official_Review", "content": {"title": "a good start", "review": "In my opinion this paper is generally of good quality and clarity, modest originality and significance.\n\nStrengths:\n- The experiments are very thorough. Hyperparameters were honestly optimized. The method does show some modest improvements in the experiments provided by the authors.\n- The analysis of the results is quite insightful.\n\nWeaknesses:\n- The experiments are done on CIFAR-10, CIFAR-100 and subsets of CIFAR-100. These were good data sets a few years ago and still are good data sets to test the code and sanity of the idea, but concluding anything strong based on the results obtained with them is not a good idea.\n- The authors claim the formalization of the problem to be one of their contributions. It is difficult for me to accept it. The formalization that the authors proposed is basically the definition of curriculum learning. There is no novelty about this.\n- The proposed method introduces a lot of complexity for very small gains. While these results are scientifically interesting, I don't expect it to be of practical use.\n- The results in Figure 3 are very far from the state of the art. I realize that they were obtained with a simple network, however, showing improvements in this regime is not that convincing.  Even the results with the VGG network are very far from the best available models.\n- I suggest checking the papers citing Bengio et al. (2009) to find lots of closely related papers. \n\nIn summary, it is not a bad paper, but the experimental results are not sufficient to conclude that much. Experiments with ImageNet or some other large data set would be advisable to increase significance of this work. ", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper618/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "In Your Pace: Learning the Right Example at the Right Time", "abstract": "Training neural networks is traditionally done by sequentially providing random mini-batches sampled uniformly from the entire dataset. In our work, we show that sampling mini-batches non-uniformly can both enhance the speed of learning and improve the final accuracy of the trained network. Specifically, we decompose the problem using the principles of curriculum learning: first, we sort the data by some difficulty measure; second, we sample mini-batches with a gradually increasing level of difficulty. We focus on CNNs trained on image recognition. Initially, we define the difficulty of a training image using transfer learning from some competitive \"teacher\" network trained on the Imagenet database, showing improvement in learning speed and final performance for both small and competitive networks, using the CIFAR-10 and the CIFAR-100 datasets. We then suggest a bootstrap alternative to evaluate the difficulty of points using the same network without relying on a \"teacher\" network, thus increasing the applicability of our suggested method. We compare this approach to a related version of Self-Paced Learning, showing that our method benefits learning while SPL impairs it.", "keywords": ["Curriculum Learning", "Transfer Learning", "Self-Paced Learning", "Image Recognition"], "authorids": ["guy.hacohen@mail.huji.ac.il", "daphna@cs.huji.ac.il"], "authors": ["Guy Hacohen", "Daphna Weinshall"], "TL;DR": "We provide a formal definition of curriculum learning for deep neural networks, empirically showing how it can improve learning performance without additional human supervision and in a problem-free manner.", "pdf": "/pdf/c30ee6e4856bb888bd4943a6148c321ed98b5dfe.pdf", "paperhash": "hacohen|in_your_pace_learning_the_right_example_at_the_right_time", "_bibtex": "@misc{\nhacohen2019in,\ntitle={In Your Pace: Learning the Right Example at the Right Time},\nauthor={Guy Hacohen and Daphna Weinshall},\nyear={2019},\nurl={https://openreview.net/forum?id=ryxHii09KQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper618/Official_Review", "cdate": 1542234418583, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "ryxHii09KQ", "replyto": "ryxHii09KQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper618/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335764860, "tmdate": 1552335764860, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper618/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "BklIbzwd3X", "original": null, "number": 2, "cdate": 1541071357972, "ddate": null, "tcdate": 1541071357972, "tmdate": 1541533838917, "tddate": null, "forum": "ryxHii09KQ", "replyto": "ryxHii09KQ", "invitation": "ICLR.cc/2019/Conference/-/Paper618/Official_Review", "content": {"title": "lacking convincing comparison", "review": "This paper studies an interesting and meaningful topic that what is the potential of curriculum learning (CL) in training dnn.  The authors decompose CL into two main parts: scoring function and pacing function. Towards both parts, several candidate functions are proposed and verified.  The paper is presented quite clear and gives contribution to better understand CL in the literature of DNN.\n\nHowever, I have several concerns towards the status of this paper.\n\nFirst, quite a few important related works are missing by the authors. Just name a few, [1] studies designing data curriculum by predictive uncertainty. [2,3] studies how to derive data driven curriculum along NN training. In particular, the objective of [2] is exactly \u201clearning the right examples at the right time\u201d. All these three papers focus on, or at least talk about, neural network training. Unfortunately, none of them are compared with, or even referenced. \n\nSecond, although comprehensive study towards different curriculum strategy are given, I found it largely unconvincing. I tried hard to discover a *detailed accuracy number on a benchmark dataset with unchanged setting* but found only case 4. By \u2018unchanged\u2019 I mean it is not a subpart of the whole dataset, or using a rarely seen nn architecture.  If it is such `changed\u2019 settings, the results are largely unconvincing since we do not know what the exact baseline is. For the only \u2018unchanged\u2019 setting 4 including VGG on CIFAR100, unfortunately the results seem not good (Fig 4a). I understand that some previous work such as the cited [Weinshall et.all 2018] also used the same setting: however it does not mean such settings give *clear and convincing* results of whether CL plays significant role in training DNN. Furthermore, I also expect the results of comparing in terms of wall clock time (including all your bootstrapping training time) but not merely batch numbers. \n\n[1] Chang, Haw-Shiuan, Erik Learned-Miller, and Andrew McCallum. \"Active Bias: Training More Accurate Neural Networks by Emphasizing High Variance Samples.\" NIPS. 2017.\n\n[2]  Fan, Y., Tian, F., Qin, T., Li, X. Y., & Liu, T. Y. Learning to Teach. ICLR 2018\n\n[3] Jiang, Lu, et al. \"MentorNet: Learning data-driven curriculum for very deep neural networks on corrupted labels.\" ICML. 2018.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper618/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "In Your Pace: Learning the Right Example at the Right Time", "abstract": "Training neural networks is traditionally done by sequentially providing random mini-batches sampled uniformly from the entire dataset. In our work, we show that sampling mini-batches non-uniformly can both enhance the speed of learning and improve the final accuracy of the trained network. Specifically, we decompose the problem using the principles of curriculum learning: first, we sort the data by some difficulty measure; second, we sample mini-batches with a gradually increasing level of difficulty. We focus on CNNs trained on image recognition. Initially, we define the difficulty of a training image using transfer learning from some competitive \"teacher\" network trained on the Imagenet database, showing improvement in learning speed and final performance for both small and competitive networks, using the CIFAR-10 and the CIFAR-100 datasets. We then suggest a bootstrap alternative to evaluate the difficulty of points using the same network without relying on a \"teacher\" network, thus increasing the applicability of our suggested method. We compare this approach to a related version of Self-Paced Learning, showing that our method benefits learning while SPL impairs it.", "keywords": ["Curriculum Learning", "Transfer Learning", "Self-Paced Learning", "Image Recognition"], "authorids": ["guy.hacohen@mail.huji.ac.il", "daphna@cs.huji.ac.il"], "authors": ["Guy Hacohen", "Daphna Weinshall"], "TL;DR": "We provide a formal definition of curriculum learning for deep neural networks, empirically showing how it can improve learning performance without additional human supervision and in a problem-free manner.", "pdf": "/pdf/c30ee6e4856bb888bd4943a6148c321ed98b5dfe.pdf", "paperhash": "hacohen|in_your_pace_learning_the_right_example_at_the_right_time", "_bibtex": "@misc{\nhacohen2019in,\ntitle={In Your Pace: Learning the Right Example at the Right Time},\nauthor={Guy Hacohen and Daphna Weinshall},\nyear={2019},\nurl={https://openreview.net/forum?id=ryxHii09KQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper618/Official_Review", "cdate": 1542234418583, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "ryxHii09KQ", "replyto": "ryxHii09KQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper618/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335764860, "tmdate": 1552335764860, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper618/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "rkgoETj43m", "original": null, "number": 1, "cdate": 1540828466907, "ddate": null, "tcdate": 1540828466907, "tmdate": 1541533838713, "tddate": null, "forum": "ryxHii09KQ", "replyto": "ryxHii09KQ", "invitation": "ICLR.cc/2019/Conference/-/Paper618/Official_Review", "content": {"title": "Investigates an interesting problem but has limited novelty and presents limited insights", "review": "This problem of interest in this paper is Curriculum Learning (CL), in the context of deep learning in particular. CL refers to learning a non-random order of presenting the training examples to the learner, typically with easier examples presented before difficult ones, to guide learning more effectively. This has been shown to both speed up learning and lead to better generalization, especially for more challenging problems. In this paper, they claim that their contribution is to decompose the problem of CL into learning two functions: the scoring function and the pacing function, with the role of the former being to estimate the difficulty of each training example and the latter to moderate the schedule of presenting increasingly more challenging examples throughout training.\n\nOverall, I found it hard to understand from reading the paper what exactly is new versus what is borrowed from previous work. In particular, after reading Weinshall et al, I realized that they have already proposed a number of things that are experimented with here: 1) they proposed the approach of transfer learning from a previously-trained network as a means of estimating the \u2018scoring function\u2019. 2) they also distinguish between learning to estimate the difficulty of examples, and learning the schedule of decreasing difficulty throughout learning, which is actually stated here as the contribution of this paper. In particular, in Section 3 of Weinshall et al, there is a sub-section named \u201cscheduling the appearance of training examples\u201d where they describe what in the terminology of this paper would be called their pacing function. They experiment with two variants: fixed, and adaptive, which are very similar to two of the pacing functions proposed here.\n\nBootstrapping:\nA component of this work that didn\u2019t appear in Weinshall et al, is the bootstrapping approach to estimating the scoring function. In general, this involves using the same network that is being trained on the task to estimate the difficulty of the training examples. The authors explain that there are two ways to do this: estimate how easy each training example is with respect to the \u2018current hypothesis\u2019 (the weights of the network at the current step), and with respect to the \u2018final hypothesis\u2019, which they estimate if I understand correctly as the network at the end of training. The latter would necessitate first training the network in the standard way, and then using it to estimate how easy or hard each example is, and using those estimates to re-train the network from scratch using that curriculum. They refer to the former as self-paced learning and to the latter as self-taught learning. I find these names confusing in that they don\u2019t really convey what the difference is between the two. Further, while self-paced learning has been studied before (e.g. Kuman et al), I\u2019m not sure about self-taught learning. Is this a term that the authors here coined? If not, it would be useful to add a reference. \n\nUsing easy / hard examples as judged by the current / final hypothesis:\nWhen using the current hypothesis, under some conditions, Weinshall et al showed that choosing harder examples is actually more beneficial than easy examples, similar in spirit to hard negative mining. On the other hand, when using the final hypothesis to estimate examples\u2019 difficulty, using a schedule of increasing difficulty is beneficial. Based on this, I have two comments: 1) It would therefore be useful to implement a version that uses the current hypothesis to estimate how easy each example is (like the self-paced scoring function) but then invert these estimates, in effect choosing the most challenging instead of the easiest ones as is done for anti-curriculum learning. This would be a hybrid between the current self-paced scoring function and anti-curriculum scoring function that would essentially implement the hard negative mining technique in this context. 2) It would be useful to comment on the differences between the self-paced scoring function used here, and that in Kumar et al. In particular, in this case using a curriculum based on this scoring function seems to harm training but in Kumar et al, they showed it actually increased performance in a number of different cases. Why does one work but the other doesn\u2019t?\n\nExperiments:\nThe experiments are presented in a subset of 5 classes from CIFAR-10 (also used by Weinshall et al.), but also in the full CIFAR-10 and CIFAR-100 datasets. They used both a small CNN (same as in Weinshall et al) as well as a VGG architecture. Overall, their results are comparable to what was previously known: using a curriculum computed by transfer leads to improved learning speed and final performance (though sometimes very slightly) compared to the standard training, and the training with a random curriculum. Further, the benefit is larger when the task is harder (as measured by the final vanilla-trained performance). By computing the distances between the gradients obtained from using a curriculum (via the transfer scoring function) and no curriculum confirms that these two training setups indeed drive the learning in different directions; an analysis similar to Weinshall et al. Also, since, as was previously known and they also observe, the benefit of CL is larger at the beginning of training, they propose a single-step pacing function that performs similarly to other pacing functions while is simpler and more computationally effective. The idea is to decrease only once the proportion of easy examples used in mini-batches, via a step function. Therefore at the start many easy examples are used, and after this threshold is surpassed, few easy examples are used.\n \nOverall, I don\u2019t feel the contribution of this paper is large enough to recommend acceptance. The main points that guided this decision are: \n1) The relationship with previous work is not clear. In particular, Weinshall et al seems to have already proposed a few components that are claimed to be the contribution of this paper, as elaborated on above. The authors should mention that the transfer scoring function was borrowed from Weinshall et al, clarify the differences between their pacing functions from those in Weinshall et al., etc. \n2) The usefulness of using easy or hard experiments when consulting the current or final hypothesis is discussed but not explored sufficiently. An additional experiment is proposed above to add another \u2018data point\u2019 to this discussion. \n3) self-paced learning is presented as something that doesn\u2019t work and wasn\u2019t expected to work. However, in the past successes were shown with this method, so it would be useful to clarify the difference in setup, and justify this difference.\n4) It seems that the experiments resulted to similar conclusions to what was already known. While it\u2019s useful to confirm these findings on additional datasets, I didn\u2019t feel that there was a significant insight gained from them.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper618/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "In Your Pace: Learning the Right Example at the Right Time", "abstract": "Training neural networks is traditionally done by sequentially providing random mini-batches sampled uniformly from the entire dataset. In our work, we show that sampling mini-batches non-uniformly can both enhance the speed of learning and improve the final accuracy of the trained network. Specifically, we decompose the problem using the principles of curriculum learning: first, we sort the data by some difficulty measure; second, we sample mini-batches with a gradually increasing level of difficulty. We focus on CNNs trained on image recognition. Initially, we define the difficulty of a training image using transfer learning from some competitive \"teacher\" network trained on the Imagenet database, showing improvement in learning speed and final performance for both small and competitive networks, using the CIFAR-10 and the CIFAR-100 datasets. We then suggest a bootstrap alternative to evaluate the difficulty of points using the same network without relying on a \"teacher\" network, thus increasing the applicability of our suggested method. We compare this approach to a related version of Self-Paced Learning, showing that our method benefits learning while SPL impairs it.", "keywords": ["Curriculum Learning", "Transfer Learning", "Self-Paced Learning", "Image Recognition"], "authorids": ["guy.hacohen@mail.huji.ac.il", "daphna@cs.huji.ac.il"], "authors": ["Guy Hacohen", "Daphna Weinshall"], "TL;DR": "We provide a formal definition of curriculum learning for deep neural networks, empirically showing how it can improve learning performance without additional human supervision and in a problem-free manner.", "pdf": "/pdf/c30ee6e4856bb888bd4943a6148c321ed98b5dfe.pdf", "paperhash": "hacohen|in_your_pace_learning_the_right_example_at_the_right_time", "_bibtex": "@misc{\nhacohen2019in,\ntitle={In Your Pace: Learning the Right Example at the Right Time},\nauthor={Guy Hacohen and Daphna Weinshall},\nyear={2019},\nurl={https://openreview.net/forum?id=ryxHii09KQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper618/Official_Review", "cdate": 1542234418583, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "ryxHii09KQ", "replyto": "ryxHii09KQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper618/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335764860, "tmdate": 1552335764860, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper618/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SylJfSAKsQ", "original": null, "number": 2, "cdate": 1540117766652, "ddate": null, "tcdate": 1540117766652, "tmdate": 1540117766652, "tddate": null, "forum": "ryxHii09KQ", "replyto": "ryxHii09KQ", "invitation": "ICLR.cc/2019/Conference/-/Paper618/Official_Comment", "content": {"title": "Addiotional empirical results on ImageNet", "comment": "We've added empirical results on a subset of 7 classes from Imagenet, using the same architecture as cases 1, 2, 3 for both the curriculum by transfer and vanilla test cases.\nThe results show same qualitative behavior as the previous cases: curriculum by transfer reaches higher accuracy faster and converges to a better final solution.\n\nThe paper will be updated once we will be allowed to modify it."}, "signatures": ["ICLR.cc/2019/Conference/Paper618/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper618/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper618/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "In Your Pace: Learning the Right Example at the Right Time", "abstract": "Training neural networks is traditionally done by sequentially providing random mini-batches sampled uniformly from the entire dataset. In our work, we show that sampling mini-batches non-uniformly can both enhance the speed of learning and improve the final accuracy of the trained network. Specifically, we decompose the problem using the principles of curriculum learning: first, we sort the data by some difficulty measure; second, we sample mini-batches with a gradually increasing level of difficulty. We focus on CNNs trained on image recognition. Initially, we define the difficulty of a training image using transfer learning from some competitive \"teacher\" network trained on the Imagenet database, showing improvement in learning speed and final performance for both small and competitive networks, using the CIFAR-10 and the CIFAR-100 datasets. We then suggest a bootstrap alternative to evaluate the difficulty of points using the same network without relying on a \"teacher\" network, thus increasing the applicability of our suggested method. We compare this approach to a related version of Self-Paced Learning, showing that our method benefits learning while SPL impairs it.", "keywords": ["Curriculum Learning", "Transfer Learning", "Self-Paced Learning", "Image Recognition"], "authorids": ["guy.hacohen@mail.huji.ac.il", "daphna@cs.huji.ac.il"], "authors": ["Guy Hacohen", "Daphna Weinshall"], "TL;DR": "We provide a formal definition of curriculum learning for deep neural networks, empirically showing how it can improve learning performance without additional human supervision and in a problem-free manner.", "pdf": "/pdf/c30ee6e4856bb888bd4943a6148c321ed98b5dfe.pdf", "paperhash": "hacohen|in_your_pace_learning_the_right_example_at_the_right_time", "_bibtex": "@misc{\nhacohen2019in,\ntitle={In Your Pace: Learning the Right Example at the Right Time},\nauthor={Guy Hacohen and Daphna Weinshall},\nyear={2019},\nurl={https://openreview.net/forum?id=ryxHii09KQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper618/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621611129, "tddate": null, "super": null, "final": null, "reply": {"forum": "ryxHii09KQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper618/Authors", "ICLR.cc/2019/Conference/Paper618/Reviewers", "ICLR.cc/2019/Conference/Paper618/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper618/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper618/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper618/Authors|ICLR.cc/2019/Conference/Paper618/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper618/Reviewers", "ICLR.cc/2019/Conference/Paper618/Authors", "ICLR.cc/2019/Conference/Paper618/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621611129}}}, {"id": "HJl8pfYo9Q", "original": null, "number": 1, "cdate": 1539179198465, "ddate": null, "tcdate": 1539179198465, "tmdate": 1539179198465, "tddate": null, "forum": "ryxHii09KQ", "replyto": "r1xQui-t97", "invitation": "ICLR.cc/2019/Conference/-/Paper618/Official_Comment", "content": {"title": "Answers", "comment": "Thank you for your comment.\n\n1. Case 2 and Case 3 are performed on the same moderate size network described in Case 1, which was not constructed with cifar10 and cifar100 in mind - hence the low performance. In case 4, we used a competitive architecture for CIFAR-100, with a much better performance (results can be seen in Fig. 4.). No data augmentation was used in any of the experiments described in the paper.\n\n2. In all cases, the learning rate was decreased exponentially every fixed number of iterations (similarly to the way we increase the data size, depicted in Fig. 1.). The use of cyclic learning rate (as in Fig. 10 in the appendix) was done as a control measure only, with a cyclic period calculated as suggested in Smith (2017). We will add to the appendix graphs that show qualitatively these learning rate scheduling functions, for clarification."}, "signatures": ["ICLR.cc/2019/Conference/Paper618/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper618/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper618/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "In Your Pace: Learning the Right Example at the Right Time", "abstract": "Training neural networks is traditionally done by sequentially providing random mini-batches sampled uniformly from the entire dataset. In our work, we show that sampling mini-batches non-uniformly can both enhance the speed of learning and improve the final accuracy of the trained network. Specifically, we decompose the problem using the principles of curriculum learning: first, we sort the data by some difficulty measure; second, we sample mini-batches with a gradually increasing level of difficulty. We focus on CNNs trained on image recognition. Initially, we define the difficulty of a training image using transfer learning from some competitive \"teacher\" network trained on the Imagenet database, showing improvement in learning speed and final performance for both small and competitive networks, using the CIFAR-10 and the CIFAR-100 datasets. We then suggest a bootstrap alternative to evaluate the difficulty of points using the same network without relying on a \"teacher\" network, thus increasing the applicability of our suggested method. We compare this approach to a related version of Self-Paced Learning, showing that our method benefits learning while SPL impairs it.", "keywords": ["Curriculum Learning", "Transfer Learning", "Self-Paced Learning", "Image Recognition"], "authorids": ["guy.hacohen@mail.huji.ac.il", "daphna@cs.huji.ac.il"], "authors": ["Guy Hacohen", "Daphna Weinshall"], "TL;DR": "We provide a formal definition of curriculum learning for deep neural networks, empirically showing how it can improve learning performance without additional human supervision and in a problem-free manner.", "pdf": "/pdf/c30ee6e4856bb888bd4943a6148c321ed98b5dfe.pdf", "paperhash": "hacohen|in_your_pace_learning_the_right_example_at_the_right_time", "_bibtex": "@misc{\nhacohen2019in,\ntitle={In Your Pace: Learning the Right Example at the Right Time},\nauthor={Guy Hacohen and Daphna Weinshall},\nyear={2019},\nurl={https://openreview.net/forum?id=ryxHii09KQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper618/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621611129, "tddate": null, "super": null, "final": null, "reply": {"forum": "ryxHii09KQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper618/Authors", "ICLR.cc/2019/Conference/Paper618/Reviewers", "ICLR.cc/2019/Conference/Paper618/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper618/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper618/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper618/Authors|ICLR.cc/2019/Conference/Paper618/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper618/Reviewers", "ICLR.cc/2019/Conference/Paper618/Authors", "ICLR.cc/2019/Conference/Paper618/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621611129}}}, {"id": "r1xQui-t97", "original": null, "number": 1, "cdate": 1539017579168, "ddate": null, "tcdate": 1539017579168, "tmdate": 1539017579168, "tddate": null, "forum": "ryxHii09KQ", "replyto": "ryxHii09KQ", "invitation": "ICLR.cc/2019/Conference/-/Paper618/Public_Comment", "content": {"comment": "1. The results of cifar10 and cifar100 are very low as shown in Case 2 and Case 3. Do you conduct a data augmentation in your experiments? If you did, is the score of one image changing?\n2. The setting of learning rate should provide more detailed explanation. How long is the cyclic period? If you can show the learning rare and your pace function in one fig, this will be great.", "title": "It is a meaningful work, but there are a few questions"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper618/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "In Your Pace: Learning the Right Example at the Right Time", "abstract": "Training neural networks is traditionally done by sequentially providing random mini-batches sampled uniformly from the entire dataset. In our work, we show that sampling mini-batches non-uniformly can both enhance the speed of learning and improve the final accuracy of the trained network. Specifically, we decompose the problem using the principles of curriculum learning: first, we sort the data by some difficulty measure; second, we sample mini-batches with a gradually increasing level of difficulty. We focus on CNNs trained on image recognition. Initially, we define the difficulty of a training image using transfer learning from some competitive \"teacher\" network trained on the Imagenet database, showing improvement in learning speed and final performance for both small and competitive networks, using the CIFAR-10 and the CIFAR-100 datasets. We then suggest a bootstrap alternative to evaluate the difficulty of points using the same network without relying on a \"teacher\" network, thus increasing the applicability of our suggested method. We compare this approach to a related version of Self-Paced Learning, showing that our method benefits learning while SPL impairs it.", "keywords": ["Curriculum Learning", "Transfer Learning", "Self-Paced Learning", "Image Recognition"], "authorids": ["guy.hacohen@mail.huji.ac.il", "daphna@cs.huji.ac.il"], "authors": ["Guy Hacohen", "Daphna Weinshall"], "TL;DR": "We provide a formal definition of curriculum learning for deep neural networks, empirically showing how it can improve learning performance without additional human supervision and in a problem-free manner.", "pdf": "/pdf/c30ee6e4856bb888bd4943a6148c321ed98b5dfe.pdf", "paperhash": "hacohen|in_your_pace_learning_the_right_example_at_the_right_time", "_bibtex": "@misc{\nhacohen2019in,\ntitle={In Your Pace: Learning the Right Example at the Right Time},\nauthor={Guy Hacohen and Daphna Weinshall},\nyear={2019},\nurl={https://openreview.net/forum?id=ryxHii09KQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper618/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311793173, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "ryxHii09KQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper618/Authors", "ICLR.cc/2019/Conference/Paper618/Reviewers", "ICLR.cc/2019/Conference/Paper618/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper618/Authors", "ICLR.cc/2019/Conference/Paper618/Reviewers", "ICLR.cc/2019/Conference/Paper618/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311793173}}}], "count": 9}