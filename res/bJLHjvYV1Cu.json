{"notes": [{"id": "bJLHjvYV1Cu", "original": "6YtFATnruU9", "number": 743, "cdate": 1601308087033, "ddate": null, "tcdate": 1601308087033, "tmdate": 1614985758322, "tddate": null, "forum": "bJLHjvYV1Cu", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Optimizing Loss Functions Through Multivariate Taylor Polynomial Parameterization", "authorids": ["~Santiago_Gonzalez1", "~Risto_Miikkulainen1"], "authors": ["Santiago Gonzalez", "Risto Miikkulainen"], "keywords": ["taylorglo", "loss function", "metalearning", "evolution", "deep networks", "evolutionary strategies", "taylor polynomials", "glo"], "abstract": "Metalearning of deep neural network (DNN) architectures and hyperparameters has become an increasingly important area of research. Loss functions are a type of metaknowledge that is crucial to effective training of DNNs, however, their potential role in metalearning has not yet been fully explored. Whereas early work focused on genetic programming (GP) on tree representations, this paper proposes continuous CMA-ES optimization of multivariate Taylor polynomial parameterizations. This approach, TaylorGLO, makes it possible to represent and search useful loss functions more effectively. In MNIST, CIFAR-10, and SVHN benchmark tasks, TaylorGLO finds new loss functions that outperform functions previously discovered through GP, as well as the standard cross-entropy loss, in fewer generations. These functions serve to regularize the learning task by discouraging overfitting to the labels, which is particularly useful in tasks where limited training data is available. The results thus demonstrate that loss function optimization is a productive new avenue for metalearning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gonzalez|optimizing_loss_functions_through_multivariate_taylor_polynomial_parameterization", "one-sentence_summary": "This paper introduces TaylorGLO, a technique that metalearns loss functions that result in higher performance deep networks.", "pdf": "/pdf/734bda46642cce971fe4cc895f79e849744efffc.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=37UGiFHRQq", "_bibtex": "@misc{\ngonzalez2021optimizing,\ntitle={Optimizing Loss Functions Through Multivariate Taylor Polynomial Parameterization},\nauthor={Santiago Gonzalez and Risto Miikkulainen},\nyear={2021},\nurl={https://openreview.net/forum?id=bJLHjvYV1Cu}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "P7OthB-zMQK", "original": null, "number": 1, "cdate": 1610040376409, "ddate": null, "tcdate": 1610040376409, "tmdate": 1610473968676, "tddate": null, "forum": "bJLHjvYV1Cu", "replyto": "bJLHjvYV1Cu", "invitation": "ICLR.cc/2021/Conference/Paper743/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "Pros: Reviewers generally agreed the paper was well written and is easy to follow. The goal of learning loss functions also seems quite promising.\n\nCons: There were concerns about whether credit for experimental performance was attributable to the core algorithm+functional form presented in the paper. There was also some skepticism about the specific form of the learned loss. Of greatest concern, no reviewer argued for acceptance during discussion, and one reviewer lowered their score during discussion."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Optimizing Loss Functions Through Multivariate Taylor Polynomial Parameterization", "authorids": ["~Santiago_Gonzalez1", "~Risto_Miikkulainen1"], "authors": ["Santiago Gonzalez", "Risto Miikkulainen"], "keywords": ["taylorglo", "loss function", "metalearning", "evolution", "deep networks", "evolutionary strategies", "taylor polynomials", "glo"], "abstract": "Metalearning of deep neural network (DNN) architectures and hyperparameters has become an increasingly important area of research. Loss functions are a type of metaknowledge that is crucial to effective training of DNNs, however, their potential role in metalearning has not yet been fully explored. Whereas early work focused on genetic programming (GP) on tree representations, this paper proposes continuous CMA-ES optimization of multivariate Taylor polynomial parameterizations. This approach, TaylorGLO, makes it possible to represent and search useful loss functions more effectively. In MNIST, CIFAR-10, and SVHN benchmark tasks, TaylorGLO finds new loss functions that outperform functions previously discovered through GP, as well as the standard cross-entropy loss, in fewer generations. These functions serve to regularize the learning task by discouraging overfitting to the labels, which is particularly useful in tasks where limited training data is available. The results thus demonstrate that loss function optimization is a productive new avenue for metalearning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gonzalez|optimizing_loss_functions_through_multivariate_taylor_polynomial_parameterization", "one-sentence_summary": "This paper introduces TaylorGLO, a technique that metalearns loss functions that result in higher performance deep networks.", "pdf": "/pdf/734bda46642cce971fe4cc895f79e849744efffc.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=37UGiFHRQq", "_bibtex": "@misc{\ngonzalez2021optimizing,\ntitle={Optimizing Loss Functions Through Multivariate Taylor Polynomial Parameterization},\nauthor={Santiago Gonzalez and Risto Miikkulainen},\nyear={2021},\nurl={https://openreview.net/forum?id=bJLHjvYV1Cu}\n}"}, "tags": [], "invitation": {"reply": {"forum": "bJLHjvYV1Cu", "replyto": "bJLHjvYV1Cu", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040376395, "tmdate": 1610473968659, "id": "ICLR.cc/2021/Conference/Paper743/-/Decision"}}}, {"id": "QTtNGg2CeQI", "original": null, "number": 6, "cdate": 1606282332096, "ddate": null, "tcdate": 1606282332096, "tmdate": 1606282332096, "tddate": null, "forum": "bJLHjvYV1Cu", "replyto": "9Ib7z82B0Kx", "invitation": "ICLR.cc/2021/Conference/Paper743/-/Official_Comment", "content": {"title": "Thank you", "comment": "Thank you for the revisions. Appendix F contains some really striking results! Order 3 results in 5% accuracy drop, and even order 5 has 3% accuracy loss. That is really surprising to me but useful to know.\n\nI maintain my rating as I would have liked to see a more rigorous analysis of the learning rate effects (the authors depict a learning rate sweep on a single task + suboptimal model architecture, and the learning rate scheduling effects are not addressed). Regardless, I think this is a paper that I would enjoy seeing in ICLR."}, "signatures": ["ICLR.cc/2021/Conference/Paper743/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper743/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Optimizing Loss Functions Through Multivariate Taylor Polynomial Parameterization", "authorids": ["~Santiago_Gonzalez1", "~Risto_Miikkulainen1"], "authors": ["Santiago Gonzalez", "Risto Miikkulainen"], "keywords": ["taylorglo", "loss function", "metalearning", "evolution", "deep networks", "evolutionary strategies", "taylor polynomials", "glo"], "abstract": "Metalearning of deep neural network (DNN) architectures and hyperparameters has become an increasingly important area of research. Loss functions are a type of metaknowledge that is crucial to effective training of DNNs, however, their potential role in metalearning has not yet been fully explored. Whereas early work focused on genetic programming (GP) on tree representations, this paper proposes continuous CMA-ES optimization of multivariate Taylor polynomial parameterizations. This approach, TaylorGLO, makes it possible to represent and search useful loss functions more effectively. In MNIST, CIFAR-10, and SVHN benchmark tasks, TaylorGLO finds new loss functions that outperform functions previously discovered through GP, as well as the standard cross-entropy loss, in fewer generations. These functions serve to regularize the learning task by discouraging overfitting to the labels, which is particularly useful in tasks where limited training data is available. The results thus demonstrate that loss function optimization is a productive new avenue for metalearning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gonzalez|optimizing_loss_functions_through_multivariate_taylor_polynomial_parameterization", "one-sentence_summary": "This paper introduces TaylorGLO, a technique that metalearns loss functions that result in higher performance deep networks.", "pdf": "/pdf/734bda46642cce971fe4cc895f79e849744efffc.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=37UGiFHRQq", "_bibtex": "@misc{\ngonzalez2021optimizing,\ntitle={Optimizing Loss Functions Through Multivariate Taylor Polynomial Parameterization},\nauthor={Santiago Gonzalez and Risto Miikkulainen},\nyear={2021},\nurl={https://openreview.net/forum?id=bJLHjvYV1Cu}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "bJLHjvYV1Cu", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper743/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper743/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper743/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper743/Authors|ICLR.cc/2021/Conference/Paper743/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper743/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923867696, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper743/-/Official_Comment"}}}, {"id": "2tZODRBOUC6", "original": null, "number": 5, "cdate": 1606279993339, "ddate": null, "tcdate": 1606279993339, "tmdate": 1606279993339, "tddate": null, "forum": "bJLHjvYV1Cu", "replyto": "cLgkTS0IIf-", "invitation": "ICLR.cc/2021/Conference/Paper743/-/Official_Comment", "content": {"title": "Response", "comment": "Dear Reviewer 3,\n\nThank you for taking the time to review our paper. We would like to address your concerns as follows:\n\n**Cons:**\n\n*(On a single comparison with GLO:)* TaylorGLO was only compared to GLO on MNIST due to the exorbitant compute costs associated with running GLO for models that take longer. The scalability of TaylorGLO to deeper models is one key advantage over GLO.\n\n*(On impact with a broader range of settings:)* To demonstrate the efficacy of TaylorGLO more generally,  new experiments were included with different regularization techniques, an additional dataset, and an additional architecture:\nSettings:\n- CIFAR-10 AlexNet + Cutout\n- CIFAR-10 AlexNet + CutMix\n- CIFAR-10 AllCNN-C + CutMix\n- CIFAR-100 PyramidNet 110a48\n- CIFAR-100 PyramidNet 110a48 + Cutout\n\nAdditionally, experiment durations and environmental impact have been added to Appendix G. Most experiments take tens of hours to run, making TaylorGLO practical in many settings, especially since it needs to be run only once for each architecture / dataset pair.\n\nThe results are included in the main paper in Table 1. They all demonstrate significant improvements with TaylorGLO, strengthening the conclusions of the paper. Thanks for the suggestion!\n\n*(On hypotheses tested with t-test:)* The t-tests are included to show that the results are robust to all stochasticity in training. Generalization to different tasks is a different question, and not the focus of this current paper. The tested hypothesis is now clarified in Appendix A.5.\n\n**Question 1:**\n\nCMA-ES is a more general approach in that it allows non-differentiable objectives to be used to guide the search. In this paper, validation accuracy is itself used as an objective, but in future work we plan to extend the approach, additional objectives such as calibration.\n\nWe hope these new analyses and the updates to the paper have addressed your concerns.\n\nBest regards,\n\u2013 The Authors\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper743/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper743/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Optimizing Loss Functions Through Multivariate Taylor Polynomial Parameterization", "authorids": ["~Santiago_Gonzalez1", "~Risto_Miikkulainen1"], "authors": ["Santiago Gonzalez", "Risto Miikkulainen"], "keywords": ["taylorglo", "loss function", "metalearning", "evolution", "deep networks", "evolutionary strategies", "taylor polynomials", "glo"], "abstract": "Metalearning of deep neural network (DNN) architectures and hyperparameters has become an increasingly important area of research. Loss functions are a type of metaknowledge that is crucial to effective training of DNNs, however, their potential role in metalearning has not yet been fully explored. Whereas early work focused on genetic programming (GP) on tree representations, this paper proposes continuous CMA-ES optimization of multivariate Taylor polynomial parameterizations. This approach, TaylorGLO, makes it possible to represent and search useful loss functions more effectively. In MNIST, CIFAR-10, and SVHN benchmark tasks, TaylorGLO finds new loss functions that outperform functions previously discovered through GP, as well as the standard cross-entropy loss, in fewer generations. These functions serve to regularize the learning task by discouraging overfitting to the labels, which is particularly useful in tasks where limited training data is available. The results thus demonstrate that loss function optimization is a productive new avenue for metalearning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gonzalez|optimizing_loss_functions_through_multivariate_taylor_polynomial_parameterization", "one-sentence_summary": "This paper introduces TaylorGLO, a technique that metalearns loss functions that result in higher performance deep networks.", "pdf": "/pdf/734bda46642cce971fe4cc895f79e849744efffc.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=37UGiFHRQq", "_bibtex": "@misc{\ngonzalez2021optimizing,\ntitle={Optimizing Loss Functions Through Multivariate Taylor Polynomial Parameterization},\nauthor={Santiago Gonzalez and Risto Miikkulainen},\nyear={2021},\nurl={https://openreview.net/forum?id=bJLHjvYV1Cu}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "bJLHjvYV1Cu", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper743/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper743/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper743/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper743/Authors|ICLR.cc/2021/Conference/Paper743/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper743/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923867696, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper743/-/Official_Comment"}}}, {"id": "FycrBI3EX2i", "original": null, "number": 4, "cdate": 1606279869640, "ddate": null, "tcdate": 1606279869640, "tmdate": 1606279869640, "tddate": null, "forum": "bJLHjvYV1Cu", "replyto": "XtAUky9cDNe", "invitation": "ICLR.cc/2021/Conference/Paper743/-/Official_Comment", "content": {"title": "Response", "comment": "Dear Reviewer 4,\n\nThank you for taking the time to review our paper. We would like to address your concerns as follows:\n\n(On function shape:) The experiments in this paper use third-order polynomials, and therefore the loss functions can indeed have at most one valley. More general TaylorGLO parameterizations, such as higher order polynomials, can in principle represent functions that are not useful loss functions. However, because the search space is smooth, CMA-ES is likely to quickly narrow in on areas with good parameterizations even in such settings. \n\n(On higher order approximations:) The decision to use third-order approximation was based on preliminary experiments where different orders from 2 to 6 were evaluated empirically, and third order found to strike the best balance between expressiveness and computational cost. Those results can be rerun and included in the final revised version if they are deemed helpful. For the current revision we did run a new experiment that evaluated the approximation ability of higher-order polynomials wrt. cross-entropy loss. The results, presented in a new Appendix E, demonstrate that higher order approximations are indeed better, and suggest that it might be useful to adjust the order of approximation to the task and architecture, as well as to the available computational resources.\n\n(On computational cost:) Experiment durations and environmental impact estimates have been added as a new Appendix G. Most experiments take only tens of hours to run, making TaylorGLO practical for many settings, especially since it needs to only be run once for each architecture / dataset pair.\nTaylorGLO was only compared to GLO on MNIST due to the exorbitant compute costs associated with running GLO for models that take longer. Indeed, the scalability of TaylorGLO to deeper models is one key advantage over GLO.\nWe hope these new analyses and updates to the paper have addressed your concerns.\n\nBest regards,\n\u2013 The Authors\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper743/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper743/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Optimizing Loss Functions Through Multivariate Taylor Polynomial Parameterization", "authorids": ["~Santiago_Gonzalez1", "~Risto_Miikkulainen1"], "authors": ["Santiago Gonzalez", "Risto Miikkulainen"], "keywords": ["taylorglo", "loss function", "metalearning", "evolution", "deep networks", "evolutionary strategies", "taylor polynomials", "glo"], "abstract": "Metalearning of deep neural network (DNN) architectures and hyperparameters has become an increasingly important area of research. Loss functions are a type of metaknowledge that is crucial to effective training of DNNs, however, their potential role in metalearning has not yet been fully explored. Whereas early work focused on genetic programming (GP) on tree representations, this paper proposes continuous CMA-ES optimization of multivariate Taylor polynomial parameterizations. This approach, TaylorGLO, makes it possible to represent and search useful loss functions more effectively. In MNIST, CIFAR-10, and SVHN benchmark tasks, TaylorGLO finds new loss functions that outperform functions previously discovered through GP, as well as the standard cross-entropy loss, in fewer generations. These functions serve to regularize the learning task by discouraging overfitting to the labels, which is particularly useful in tasks where limited training data is available. The results thus demonstrate that loss function optimization is a productive new avenue for metalearning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gonzalez|optimizing_loss_functions_through_multivariate_taylor_polynomial_parameterization", "one-sentence_summary": "This paper introduces TaylorGLO, a technique that metalearns loss functions that result in higher performance deep networks.", "pdf": "/pdf/734bda46642cce971fe4cc895f79e849744efffc.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=37UGiFHRQq", "_bibtex": "@misc{\ngonzalez2021optimizing,\ntitle={Optimizing Loss Functions Through Multivariate Taylor Polynomial Parameterization},\nauthor={Santiago Gonzalez and Risto Miikkulainen},\nyear={2021},\nurl={https://openreview.net/forum?id=bJLHjvYV1Cu}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "bJLHjvYV1Cu", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper743/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper743/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper743/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper743/Authors|ICLR.cc/2021/Conference/Paper743/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper743/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923867696, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper743/-/Official_Comment"}}}, {"id": "9Ib7z82B0Kx", "original": null, "number": 3, "cdate": 1606279795379, "ddate": null, "tcdate": 1606279795379, "tmdate": 1606279795379, "tddate": null, "forum": "bJLHjvYV1Cu", "replyto": "NRs3f0TXoNx", "invitation": "ICLR.cc/2021/Conference/Paper743/-/Official_Comment", "content": {"title": "Response", "comment": "Dear Reviewer 2,\n\nThank you for taking the time to review our paper. We would like to address your concerns as follows:\n\n**RE: Implicit learning rate tuning:**\n\tWe appreciate this recommendation and have taken action. Indeed, we want to ensure that the improvement in performance is not solely due to an implicitly modified learning rate. In a new Appendix E, we provide a set of experiments that address this issue specifically. A sweep of learning rates with the cross-entropy loss shows that even with highly unstable learning rates that can improve accuracy in a handful of training sessions, the improvement is much smaller than that gained by training with a TaylorGLO loss function. We did not have time within the rebuttal period to analyze the effect of learning rate schedules, but we will continue the experiments and will include such analyses in the final submission.\n\n**RE: Accuracy of Taylor approximations of the cross-entropy loss function:**\n\tThank you for this suggestion. We have now performed these analyses in Appendix F. As expected, higher-order approximations yield better performance, although all have accuracies that are a few percentage points lower than the actual cross-entropy loss. Thus, since TaylorGLO loss functions outperform the non-approximated cross-entropy loss, TaylorGLO discovers significantly different functions that are not merely approximations of the cross-entropy loss. Increasing the approximation order is an interesting (although computationally expensive) direction of future work, as is now mentioned in Appendix F.\n\n**RE: Small adjustments:**\n\tThe recommendations on arbitrary norm and citep have been implemented in the revised paper. The related work section has been updated to point out that to our knowledge, TaylorGLO is the first method to utilize fixed-length vectors and continuous loss function optimization.\n\nWe hope these new analyses and the updates to the paper have addressed your concerns.\n\nBest regards,\n\u2013 The Authors\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper743/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper743/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Optimizing Loss Functions Through Multivariate Taylor Polynomial Parameterization", "authorids": ["~Santiago_Gonzalez1", "~Risto_Miikkulainen1"], "authors": ["Santiago Gonzalez", "Risto Miikkulainen"], "keywords": ["taylorglo", "loss function", "metalearning", "evolution", "deep networks", "evolutionary strategies", "taylor polynomials", "glo"], "abstract": "Metalearning of deep neural network (DNN) architectures and hyperparameters has become an increasingly important area of research. Loss functions are a type of metaknowledge that is crucial to effective training of DNNs, however, their potential role in metalearning has not yet been fully explored. Whereas early work focused on genetic programming (GP) on tree representations, this paper proposes continuous CMA-ES optimization of multivariate Taylor polynomial parameterizations. This approach, TaylorGLO, makes it possible to represent and search useful loss functions more effectively. In MNIST, CIFAR-10, and SVHN benchmark tasks, TaylorGLO finds new loss functions that outperform functions previously discovered through GP, as well as the standard cross-entropy loss, in fewer generations. These functions serve to regularize the learning task by discouraging overfitting to the labels, which is particularly useful in tasks where limited training data is available. The results thus demonstrate that loss function optimization is a productive new avenue for metalearning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gonzalez|optimizing_loss_functions_through_multivariate_taylor_polynomial_parameterization", "one-sentence_summary": "This paper introduces TaylorGLO, a technique that metalearns loss functions that result in higher performance deep networks.", "pdf": "/pdf/734bda46642cce971fe4cc895f79e849744efffc.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=37UGiFHRQq", "_bibtex": "@misc{\ngonzalez2021optimizing,\ntitle={Optimizing Loss Functions Through Multivariate Taylor Polynomial Parameterization},\nauthor={Santiago Gonzalez and Risto Miikkulainen},\nyear={2021},\nurl={https://openreview.net/forum?id=bJLHjvYV1Cu}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "bJLHjvYV1Cu", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper743/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper743/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper743/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper743/Authors|ICLR.cc/2021/Conference/Paper743/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper743/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923867696, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper743/-/Official_Comment"}}}, {"id": "IAGZYwJU5aw", "original": null, "number": 2, "cdate": 1606279717161, "ddate": null, "tcdate": 1606279717161, "tmdate": 1606279717161, "tddate": null, "forum": "bJLHjvYV1Cu", "replyto": "-i14pOGorf", "invitation": "ICLR.cc/2021/Conference/Paper743/-/Official_Comment", "content": {"title": "Response", "comment": "Dear Reviewer 1,\n\nThank you for taking the time to review our paper. We would like to clarify your concern:\n\nThe goal of the TaylorGLO approach is not to use the Taylor expansion to approximate the loss function, instead, the Taylor polynomials  constitute a representation of the loss function that makes it possible to search for good loss functions effectively. In other words, the Taylor polynomials are not approximations of loss functions---they are loss functions themselves. How expressive these functions are is still an interesting question. To shed light on it, a new study is presented in Appendix F of the revised paper, analyzing the performance of Taylor approximations of the cross-entropy loss function. As expected, higher-order approximations yield better performance, although all have accuracies that are a few percentage points lower than the actual cross-entropy loss. This result suggests that higher order polynomials might be used to improve performance of TaylorGLO further, albeit with an increased computational cost. However, since third-order TaylorGLO loss functions already outperform the non-approximated cross-entropy loss, this result demonstrates that TaylorGLO discovers significantly different functions that are not merely approximations of the cross-entropy loss.\n\nWe hope this new analysis and the updates to the paper have addressed your concern.\n\nBest regards,\n\u2013 The Authors"}, "signatures": ["ICLR.cc/2021/Conference/Paper743/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper743/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Optimizing Loss Functions Through Multivariate Taylor Polynomial Parameterization", "authorids": ["~Santiago_Gonzalez1", "~Risto_Miikkulainen1"], "authors": ["Santiago Gonzalez", "Risto Miikkulainen"], "keywords": ["taylorglo", "loss function", "metalearning", "evolution", "deep networks", "evolutionary strategies", "taylor polynomials", "glo"], "abstract": "Metalearning of deep neural network (DNN) architectures and hyperparameters has become an increasingly important area of research. Loss functions are a type of metaknowledge that is crucial to effective training of DNNs, however, their potential role in metalearning has not yet been fully explored. Whereas early work focused on genetic programming (GP) on tree representations, this paper proposes continuous CMA-ES optimization of multivariate Taylor polynomial parameterizations. This approach, TaylorGLO, makes it possible to represent and search useful loss functions more effectively. In MNIST, CIFAR-10, and SVHN benchmark tasks, TaylorGLO finds new loss functions that outperform functions previously discovered through GP, as well as the standard cross-entropy loss, in fewer generations. These functions serve to regularize the learning task by discouraging overfitting to the labels, which is particularly useful in tasks where limited training data is available. The results thus demonstrate that loss function optimization is a productive new avenue for metalearning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gonzalez|optimizing_loss_functions_through_multivariate_taylor_polynomial_parameterization", "one-sentence_summary": "This paper introduces TaylorGLO, a technique that metalearns loss functions that result in higher performance deep networks.", "pdf": "/pdf/734bda46642cce971fe4cc895f79e849744efffc.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=37UGiFHRQq", "_bibtex": "@misc{\ngonzalez2021optimizing,\ntitle={Optimizing Loss Functions Through Multivariate Taylor Polynomial Parameterization},\nauthor={Santiago Gonzalez and Risto Miikkulainen},\nyear={2021},\nurl={https://openreview.net/forum?id=bJLHjvYV1Cu}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "bJLHjvYV1Cu", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper743/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper743/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper743/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper743/Authors|ICLR.cc/2021/Conference/Paper743/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper743/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923867696, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper743/-/Official_Comment"}}}, {"id": "cLgkTS0IIf-", "original": null, "number": 1, "cdate": 1603720813593, "ddate": null, "tcdate": 1603720813593, "tmdate": 1605024617302, "tddate": null, "forum": "bJLHjvYV1Cu", "replyto": "bJLHjvYV1Cu", "invitation": "ICLR.cc/2021/Conference/Paper743/-/Official_Review", "content": {"title": "Useful idea, but evaluation could be improved.", "review": "This paper presents the TaylorGLO method for learning loss functions for classification.\n\n## Pros\n1. The paper is well written and quite easy to follow.\n2. It proposes the novel idea of parameterising learned loss functions as Taylor polynomials, which overcomes the downside of previous work that relies on a slow two-stage optimisation process that first infers the structure of the loss function.\n3. Networks trained with the learned loss functions typically outperform those trained with cross entropy.\n4. Analysis is undertaken to determine why the learned loss functions work better than cross entropy (they penalise overly confident outputs), and also characterise when the loss functions discovered with TaylorGLO are most effective (better sample efficiency).\n\n## Cons\n1. The main performance comparison is with cross entropy, but there exist other methods for learning loss functions (e.g., GLO and [1]). The performance comparison with these methods should be more comprehensive: currently there is only a single comparison with GLO that uses a simple network trained on MNIST, which is not enough to make general conclusions about the relative performance of the two methods.\n2. The potential for impact is limited. The main improvements in performance over cross entropy are seen when using older network architectures, but when using state of the art networks the improvements are much smaller. If it was shown that the learned loss functions could be transferred to new tasks, hence resulting in a modest performance increase without any additional computational overhead, then I would be inclined to revisit this point.\n3. It is unclear what hypothesis is being tested when applying t-tests: given that the same data is used for each performance measurement it seems the hypothesis is that performance gains are robust to the choice of random seed. Hypothesis tests are typically used in machine learning to determine whether the experimental results will generalise to new tasks [2]. As they are currently presented, the results are likely to mislead some readers into inferring a false sense of generality of the results.\n\n## Questions\n1. Why use CMA-ES over gradient-based bi-level optimisation methods commonly used in meta-learning? Gradient-based optimisation is typically faster than evolutionary methods, and it seems like it would be possible to apply it in this situation.\n\n[1] Sarah Bechtle et al. Meta-Learning via Learned Loss. arXiv:1906.05374, 2019.\n[2] Janez Dem\u0161ar. Statistical Comparisons of Classifiers over Multiple Data Sets. Journal of Machine Learning Research, 7(1):1\u221230, 2006.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper743/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper743/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Optimizing Loss Functions Through Multivariate Taylor Polynomial Parameterization", "authorids": ["~Santiago_Gonzalez1", "~Risto_Miikkulainen1"], "authors": ["Santiago Gonzalez", "Risto Miikkulainen"], "keywords": ["taylorglo", "loss function", "metalearning", "evolution", "deep networks", "evolutionary strategies", "taylor polynomials", "glo"], "abstract": "Metalearning of deep neural network (DNN) architectures and hyperparameters has become an increasingly important area of research. Loss functions are a type of metaknowledge that is crucial to effective training of DNNs, however, their potential role in metalearning has not yet been fully explored. Whereas early work focused on genetic programming (GP) on tree representations, this paper proposes continuous CMA-ES optimization of multivariate Taylor polynomial parameterizations. This approach, TaylorGLO, makes it possible to represent and search useful loss functions more effectively. In MNIST, CIFAR-10, and SVHN benchmark tasks, TaylorGLO finds new loss functions that outperform functions previously discovered through GP, as well as the standard cross-entropy loss, in fewer generations. These functions serve to regularize the learning task by discouraging overfitting to the labels, which is particularly useful in tasks where limited training data is available. The results thus demonstrate that loss function optimization is a productive new avenue for metalearning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gonzalez|optimizing_loss_functions_through_multivariate_taylor_polynomial_parameterization", "one-sentence_summary": "This paper introduces TaylorGLO, a technique that metalearns loss functions that result in higher performance deep networks.", "pdf": "/pdf/734bda46642cce971fe4cc895f79e849744efffc.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=37UGiFHRQq", "_bibtex": "@misc{\ngonzalez2021optimizing,\ntitle={Optimizing Loss Functions Through Multivariate Taylor Polynomial Parameterization},\nauthor={Santiago Gonzalez and Risto Miikkulainen},\nyear={2021},\nurl={https://openreview.net/forum?id=bJLHjvYV1Cu}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "bJLHjvYV1Cu", "replyto": "bJLHjvYV1Cu", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper743/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538136106, "tmdate": 1606915765850, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper743/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper743/-/Official_Review"}}}, {"id": "XtAUky9cDNe", "original": null, "number": 2, "cdate": 1603800871648, "ddate": null, "tcdate": 1603800871648, "tmdate": 1605024617234, "tddate": null, "forum": "bJLHjvYV1Cu", "replyto": "bJLHjvYV1Cu", "invitation": "ICLR.cc/2021/Conference/Paper743/-/Official_Review", "content": {"title": "Interesting idea, but not convincing enough", "review": "This paper proposed a method, called TaylorGLO, to learn the loss functions, for training deep neural network, by meta-learning. Specifically, the authors proposed to parameterize the loss function with multivariate Taylor polynomial, and then learn the parameters in the polynomial using evolutionary algorithm within the meta-learning framework. The experiments showed improved performance of the TaylorGLO over cross-entropy baseline on several datasets and with different network architectures.\n\nAlthough the performance seems promising, there are several issues should be addressed:\n\n1. My major concern is about the parameterization of the loss function. As is well known, a qualified loss function in machine learning generally contains only one valley concerning its geometric shape. However, with Taylor polynomial parameterization, a function might have multiple valleys, or even have no minimum.\n\n2. The authors set the order of Taylor polynomial to 3, and simply claimed this is a better trade-off compared with other orders. However, this claim should be supported by empirical evaluations, i.e., use other orders to see the differences regarding the performance and evolution time.\n\n3. Since the expensive evolutionary process is required to learn the loss function before really train the model, the computational cost should be shown and discussed in detail, especially that the performance improvement is not that much in some cases as shown in Table 1.\n\n4. The comparison with GLO was only performed on MNIST dataset, where the accuracies between the two methods are very close.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper743/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper743/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Optimizing Loss Functions Through Multivariate Taylor Polynomial Parameterization", "authorids": ["~Santiago_Gonzalez1", "~Risto_Miikkulainen1"], "authors": ["Santiago Gonzalez", "Risto Miikkulainen"], "keywords": ["taylorglo", "loss function", "metalearning", "evolution", "deep networks", "evolutionary strategies", "taylor polynomials", "glo"], "abstract": "Metalearning of deep neural network (DNN) architectures and hyperparameters has become an increasingly important area of research. Loss functions are a type of metaknowledge that is crucial to effective training of DNNs, however, their potential role in metalearning has not yet been fully explored. Whereas early work focused on genetic programming (GP) on tree representations, this paper proposes continuous CMA-ES optimization of multivariate Taylor polynomial parameterizations. This approach, TaylorGLO, makes it possible to represent and search useful loss functions more effectively. In MNIST, CIFAR-10, and SVHN benchmark tasks, TaylorGLO finds new loss functions that outperform functions previously discovered through GP, as well as the standard cross-entropy loss, in fewer generations. These functions serve to regularize the learning task by discouraging overfitting to the labels, which is particularly useful in tasks where limited training data is available. The results thus demonstrate that loss function optimization is a productive new avenue for metalearning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gonzalez|optimizing_loss_functions_through_multivariate_taylor_polynomial_parameterization", "one-sentence_summary": "This paper introduces TaylorGLO, a technique that metalearns loss functions that result in higher performance deep networks.", "pdf": "/pdf/734bda46642cce971fe4cc895f79e849744efffc.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=37UGiFHRQq", "_bibtex": "@misc{\ngonzalez2021optimizing,\ntitle={Optimizing Loss Functions Through Multivariate Taylor Polynomial Parameterization},\nauthor={Santiago Gonzalez and Risto Miikkulainen},\nyear={2021},\nurl={https://openreview.net/forum?id=bJLHjvYV1Cu}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "bJLHjvYV1Cu", "replyto": "bJLHjvYV1Cu", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper743/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538136106, "tmdate": 1606915765850, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper743/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper743/-/Official_Review"}}}, {"id": "NRs3f0TXoNx", "original": null, "number": 3, "cdate": 1603918953130, "ddate": null, "tcdate": 1603918953130, "tmdate": 1605024617157, "tddate": null, "forum": "bJLHjvYV1Cu", "replyto": "bJLHjvYV1Cu", "invitation": "ICLR.cc/2021/Conference/Paper743/-/Official_Review", "content": {"title": "Interesting idea and exciting area of work! How can we isolate effects of loss fn optimization vs. learning rate, etc?", "review": "Warning: I'm not an expert in loss function metalearning, so I've attempted to provide my perspective as someone interested in the topic but unfamiliar with prior work.\n\n# Summary\n\nThis paper tackles the problem of loss function metalearning by proposing a novel parameterization of loss functions based on multivariate polynomial expressions.\n\nThe intuitive motivation is that any continuous and full differentiable loss function can be written as a $k$-th order multivariate Taylor expansion (i.e. a multivariate polynomial expression). So for high enough $k$, the space of such polynomials will be a reasonable approximation of the space of all loss functions we might care about.\n\nThe claimed advantages of the method are smoothness, lack of poles, compositionality, meaningful metric (closeness in parameter space ==> similar loss function), and tunable complexity (via $k$) of the search space.\n\nThe paper then proposes a loss function optimizer named TaylorGLO, which proposes to optimize loss functions in parameter space (i.e. coefficient space) using CMA-ES, which requires a continuous optimization space.\n\n# Strengths\n\nThis is a neat idea, although I think calling it TaylorGLO is a bit of a misnomer as we're not starting with arbitrary functions that we're then approximating via Taylor expansion. Rather, the paper optimizes directly in polynomial space. Maybe \"PolyGLO\" is a more accurate name? The point can be made within the paper text that for sufficiently high $k$, the space of polynomials becomes an arbitrarily good approximation of any cont/diffble loss function thanks to Taylor's thm.\n\nRegardless, I really like this approach! It definitely seems like a more promising approach than the referenced prior approach, which first optimizes the structure of the loss fn in discrete (tree) space and then optimizes the coefficients via continuous methods. The fact that the experiments found a function that outperforms \"BaikalCMA\" is welcome confirmation that we don't lose much by staying in continuous space.\n\n# Weaknesses / unanswered questions\n\nI'm worried that to some extent, the improvements from the new loss functions are thanks to an implicit learning rate tuning effect. The derivative of the loss function w.r.t. the model output directly scales the parameter updates for backpropogation-based SGD. So, maybe what's happening is that the learning rate is being tuned implicitly by TaylorGLO choosing a steeper or more compressed loss function. Maybe there's even a learning rate *scheduling* effect that's coming from TaylorGLO choosing loss functions with higher curvatures.\n\nI think it's quite important to disentangle the effects of the loss function optimization from an implicit learning rate tuning, so I would ideally like to see experiments comparing TaylorGLO's best-accuracy loss function with plain old CE sweeped across learning rates and LR scheduling approaches. The difference in accuracy between TaylorGLO and a comprehensive CE sweep will be a more accurate indication of the effects of the loss optimization itself.\n\nI'm also curious to know the extent of prior art in continuous loss function optimization. This approach seems like the first thing one would do if trying to design a loss function metalearning approach in continuous space, so I'd like to hear about what (if anything) has been done before.\n\n# Overall Rating\n\nI rate this paper a 6. I would be happy to see it in ICLR as is, but I think for its results to be exciting to the ICLR audience at large, the paper needs to convincingly isolate the effects of the function optimization itself.\n\n# Comments\n\n- p.1 \"arbitrarily long, fixed-length vectors in a Hilbert space\" seems self-contradicting. Is this meant to say \"arbitrary norm\"?\n- Formatting of some of the \\citep's can be improved, e.g. CMA-ES.\n- I'd love to see a comparison of classification accuracy between standard cross-entropy and $k$-th order approximations of CE to get a sense of how much the fidelity loss actually matters.", "rating": "6: Marginally above acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2021/Conference/Paper743/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper743/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Optimizing Loss Functions Through Multivariate Taylor Polynomial Parameterization", "authorids": ["~Santiago_Gonzalez1", "~Risto_Miikkulainen1"], "authors": ["Santiago Gonzalez", "Risto Miikkulainen"], "keywords": ["taylorglo", "loss function", "metalearning", "evolution", "deep networks", "evolutionary strategies", "taylor polynomials", "glo"], "abstract": "Metalearning of deep neural network (DNN) architectures and hyperparameters has become an increasingly important area of research. Loss functions are a type of metaknowledge that is crucial to effective training of DNNs, however, their potential role in metalearning has not yet been fully explored. Whereas early work focused on genetic programming (GP) on tree representations, this paper proposes continuous CMA-ES optimization of multivariate Taylor polynomial parameterizations. This approach, TaylorGLO, makes it possible to represent and search useful loss functions more effectively. In MNIST, CIFAR-10, and SVHN benchmark tasks, TaylorGLO finds new loss functions that outperform functions previously discovered through GP, as well as the standard cross-entropy loss, in fewer generations. These functions serve to regularize the learning task by discouraging overfitting to the labels, which is particularly useful in tasks where limited training data is available. The results thus demonstrate that loss function optimization is a productive new avenue for metalearning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gonzalez|optimizing_loss_functions_through_multivariate_taylor_polynomial_parameterization", "one-sentence_summary": "This paper introduces TaylorGLO, a technique that metalearns loss functions that result in higher performance deep networks.", "pdf": "/pdf/734bda46642cce971fe4cc895f79e849744efffc.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=37UGiFHRQq", "_bibtex": "@misc{\ngonzalez2021optimizing,\ntitle={Optimizing Loss Functions Through Multivariate Taylor Polynomial Parameterization},\nauthor={Santiago Gonzalez and Risto Miikkulainen},\nyear={2021},\nurl={https://openreview.net/forum?id=bJLHjvYV1Cu}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "bJLHjvYV1Cu", "replyto": "bJLHjvYV1Cu", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper743/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538136106, "tmdate": 1606915765850, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper743/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper743/-/Official_Review"}}}, {"id": "-i14pOGorf", "original": null, "number": 4, "cdate": 1604001131666, "ddate": null, "tcdate": 1604001131666, "tmdate": 1605024617096, "tddate": null, "forum": "bJLHjvYV1Cu", "replyto": "bJLHjvYV1Cu", "invitation": "ICLR.cc/2021/Conference/Paper743/-/Official_Review", "content": {"title": "This paper investigates the optimization of loss functions through multivariate Taylor expansion. ", "review": "In this paper, the authors studied the optimization problem of loss functions, where the loss function is not referred to as the error criterion but the empirical error term in empirical risk minimization. The main approach proposed in this paper is the polynomial parametrization of the loss function via multivariate Taylor expansion, namely, equation (5), which should also be regarded as the main contribution of this paper.   \n\nPros: The paper is well written and is easy to follow. The main approach and its rationality are well articulated. It is good to see that different approximates of loss functions are compared. The effectiveness of the proposed approach is also well illustrated numerically.\n\nCons: While I believe that the proposed approach works, it is not clear to me what is the relation between the newly proposed approach and the original approach where loss functions are used directly instead of their approximates. In my opinion, the new approach works not because of the approximate of the loss function but because of the new error criterion which is essentially a polynomial. This gives my main concern. I'm expecting more comments in this regard.       ", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper743/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper743/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Optimizing Loss Functions Through Multivariate Taylor Polynomial Parameterization", "authorids": ["~Santiago_Gonzalez1", "~Risto_Miikkulainen1"], "authors": ["Santiago Gonzalez", "Risto Miikkulainen"], "keywords": ["taylorglo", "loss function", "metalearning", "evolution", "deep networks", "evolutionary strategies", "taylor polynomials", "glo"], "abstract": "Metalearning of deep neural network (DNN) architectures and hyperparameters has become an increasingly important area of research. Loss functions are a type of metaknowledge that is crucial to effective training of DNNs, however, their potential role in metalearning has not yet been fully explored. Whereas early work focused on genetic programming (GP) on tree representations, this paper proposes continuous CMA-ES optimization of multivariate Taylor polynomial parameterizations. This approach, TaylorGLO, makes it possible to represent and search useful loss functions more effectively. In MNIST, CIFAR-10, and SVHN benchmark tasks, TaylorGLO finds new loss functions that outperform functions previously discovered through GP, as well as the standard cross-entropy loss, in fewer generations. These functions serve to regularize the learning task by discouraging overfitting to the labels, which is particularly useful in tasks where limited training data is available. The results thus demonstrate that loss function optimization is a productive new avenue for metalearning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gonzalez|optimizing_loss_functions_through_multivariate_taylor_polynomial_parameterization", "one-sentence_summary": "This paper introduces TaylorGLO, a technique that metalearns loss functions that result in higher performance deep networks.", "pdf": "/pdf/734bda46642cce971fe4cc895f79e849744efffc.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=37UGiFHRQq", "_bibtex": "@misc{\ngonzalez2021optimizing,\ntitle={Optimizing Loss Functions Through Multivariate Taylor Polynomial Parameterization},\nauthor={Santiago Gonzalez and Risto Miikkulainen},\nyear={2021},\nurl={https://openreview.net/forum?id=bJLHjvYV1Cu}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "bJLHjvYV1Cu", "replyto": "bJLHjvYV1Cu", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper743/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538136106, "tmdate": 1606915765850, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper743/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper743/-/Official_Review"}}}], "count": 11}