{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1528124453559, "tcdate": 1518402171948, "number": 78, "cdate": 1518402171948, "id": "r1VF9dCUG", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "r1VF9dCUG", "signatures": ["~Devansh_Arpit1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "Finding Flatter Minima with SGD", "abstract": "It has been discussed that over-parameterized deep neural networks (DNNs) trained using stochastic gradient descent (SGD) with smaller batch sizes generalize better compared with those trained with larger batch sizes. Additionally, model parameters found by small batch size SGD tend to be in flatter regions. We extend these empirical observations and experimentally show that both large learning rate and small batch size contribute towards SGD finding flatter minima that generalize well. Conversely, we find that small learning rates and large batch sizes lead to sharper minima that correlate with poor generalization in DNNs.", "paperhash": "jastrzbski|finding_flatter_minima_with_sgd", "keywords": ["SGD", "flat minima"], "_bibtex": "@misc{\n  jastrz\u0119bski2018finding,\n  title={Finding Flatter Minima with SGD},\n  author={Stanis\u0142aw Jastrz\u0119bski and Zachary Kenton and Devansh Arpit and Nicolas Ballas and Asja Fischer and Yoshua Bengio and Amos Storkey},\n  year={2018},\n  url={https://openreview.net/forum?id=r1VF9dCUG}\n}", "authorids": ["devansharpit@gmail.com"], "authors": ["Stanis\u0142aw Jastrz\u0119bski", "Zachary Kenton", "Devansh Arpit", "Nicolas Ballas", "Asja Fischer", "Yoshua Bengio", "Amos Storkey"], "TL;DR": "Small batch size and large learning rate steer SGD towards flat minima", "pdf": "/pdf/564f1bb4b11636655731ee8bbe10b925d32c2087.pdf"}, "nonreaders": [], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582994618, "tcdate": 1519404874116, "number": 1, "cdate": 1519404874116, "id": "HyfIw6pPG", "invitation": "ICLR.cc/2018/Workshop/-/Paper78/Official_Review", "forum": "r1VF9dCUG", "replyto": "r1VF9dCUG", "signatures": ["ICLR.cc/2018/Workshop/Paper78/AnonReviewer5"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper78/AnonReviewer5"], "content": {"title": "A poster I would be happy to visit at ICLR", "rating": "8: Top 50% of accepted papers, clear accept", "review": "The whole \"flat minima\" topic in deep learning is being actively debated, without much hard data. I appreciate the authors doing these experiments and sharing them with the community. I would be happy to come and visit a poster about this experiment, and think it would add to ICLR.\n\nOne question/comment on the content: you show a plot of the largest eigenvalue of the Hessian. But I can manipulate it by multiplying the cost function by 100 (and correspondingly decreasing the step size by 100 to maintain stability). Wouldn't it be more interesting to plot the condition number of the Hessian (i.e., the ratio of the largest to smallest eigenvalue?)", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Finding Flatter Minima with SGD", "abstract": "It has been discussed that over-parameterized deep neural networks (DNNs) trained using stochastic gradient descent (SGD) with smaller batch sizes generalize better compared with those trained with larger batch sizes. Additionally, model parameters found by small batch size SGD tend to be in flatter regions. We extend these empirical observations and experimentally show that both large learning rate and small batch size contribute towards SGD finding flatter minima that generalize well. Conversely, we find that small learning rates and large batch sizes lead to sharper minima that correlate with poor generalization in DNNs.", "paperhash": "jastrzbski|finding_flatter_minima_with_sgd", "keywords": ["SGD", "flat minima"], "_bibtex": "@misc{\n  jastrz\u0119bski2018finding,\n  title={Finding Flatter Minima with SGD},\n  author={Stanis\u0142aw Jastrz\u0119bski and Zachary Kenton and Devansh Arpit and Nicolas Ballas and Asja Fischer and Yoshua Bengio and Amos Storkey},\n  year={2018},\n  url={https://openreview.net/forum?id=r1VF9dCUG}\n}", "authorids": ["devansharpit@gmail.com"], "authors": ["Stanis\u0142aw Jastrz\u0119bski", "Zachary Kenton", "Devansh Arpit", "Nicolas Ballas", "Asja Fischer", "Yoshua Bengio", "Amos Storkey"], "TL;DR": "Small batch size and large learning rate steer SGD towards flat minima", "pdf": "/pdf/564f1bb4b11636655731ee8bbe10b925d32c2087.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582994434, "id": "ICLR.cc/2018/Workshop/-/Paper78/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper78/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper78/AnonReviewer5", "ICLR.cc/2018/Workshop/Paper78/AnonReviewer4", "ICLR.cc/2018/Workshop/Paper78/AnonReviewer3"], "reply": {"forum": "r1VF9dCUG", "replyto": "r1VF9dCUG", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper78/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper78/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582994434}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582952436, "tcdate": 1520194286337, "number": 2, "cdate": 1520194286337, "id": "B1IeXCK_G", "invitation": "ICLR.cc/2018/Workshop/-/Paper78/Official_Review", "forum": "r1VF9dCUG", "replyto": "r1VF9dCUG", "signatures": ["ICLR.cc/2018/Workshop/Paper78/AnonReviewer4"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper78/AnonReviewer4"], "content": {"title": "Interesting study on the joint effect of learning rate and batch size.", "rating": "6: Marginally above acceptance threshold", "review": "This paper investigates the joint impact of learning and batch size on the optimization of feed-forward neural networks. Previous related work studied the effect of batch size alone, without considering the learning rate in such detail. The paper is clearly written, with interesting findings such as \"better generalization (and flatter region of convergence) correlates with higher learning rate to batch size ratio\", and that \"similar ratio yield similar learning dynamics\". Below are some suggestions that I think would make these claims more convincing. For future work, it would be interesting to see if these insights generalize to other model architectures (e.g. recurrent networks, attention models).\n\nSome notes for the authors:\n1. The second claim (Figures 2 and 3) would be more convincing if the architectures remain unchanged. The authors do mention that both architectures yielded similar results, but the presentation of two architectures as opposed to a single one for this specific study is distracting.\n2. There are some minor formatting issues. Figures 2 and 3 are too small and not legible at normal resolutions. The number sqrt(10) is not formatted correctly (see https://tex.stackexchange.com/questions/167892/square-root-radical-sign).\n3. For the last claim (on learning dynamics), it would be more convincing had the authors shown similar results for multiple baselines as opposed to a single one (learning rate=0.1/batch size=50).\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Finding Flatter Minima with SGD", "abstract": "It has been discussed that over-parameterized deep neural networks (DNNs) trained using stochastic gradient descent (SGD) with smaller batch sizes generalize better compared with those trained with larger batch sizes. Additionally, model parameters found by small batch size SGD tend to be in flatter regions. We extend these empirical observations and experimentally show that both large learning rate and small batch size contribute towards SGD finding flatter minima that generalize well. Conversely, we find that small learning rates and large batch sizes lead to sharper minima that correlate with poor generalization in DNNs.", "paperhash": "jastrzbski|finding_flatter_minima_with_sgd", "keywords": ["SGD", "flat minima"], "_bibtex": "@misc{\n  jastrz\u0119bski2018finding,\n  title={Finding Flatter Minima with SGD},\n  author={Stanis\u0142aw Jastrz\u0119bski and Zachary Kenton and Devansh Arpit and Nicolas Ballas and Asja Fischer and Yoshua Bengio and Amos Storkey},\n  year={2018},\n  url={https://openreview.net/forum?id=r1VF9dCUG}\n}", "authorids": ["devansharpit@gmail.com"], "authors": ["Stanis\u0142aw Jastrz\u0119bski", "Zachary Kenton", "Devansh Arpit", "Nicolas Ballas", "Asja Fischer", "Yoshua Bengio", "Amos Storkey"], "TL;DR": "Small batch size and large learning rate steer SGD towards flat minima", "pdf": "/pdf/564f1bb4b11636655731ee8bbe10b925d32c2087.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582994434, "id": "ICLR.cc/2018/Workshop/-/Paper78/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper78/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper78/AnonReviewer5", "ICLR.cc/2018/Workshop/Paper78/AnonReviewer4", "ICLR.cc/2018/Workshop/Paper78/AnonReviewer3"], "reply": {"forum": "r1VF9dCUG", "replyto": "r1VF9dCUG", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper78/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper78/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582994434}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582636851, "tcdate": 1520810971870, "number": 3, "cdate": 1520810971870, "id": "B1NkhNmKM", "invitation": "ICLR.cc/2018/Workshop/-/Paper78/Official_Review", "forum": "r1VF9dCUG", "replyto": "r1VF9dCUG", "signatures": ["ICLR.cc/2018/Workshop/Paper78/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper78/AnonReviewer3"], "content": {"title": "Some insights from experimental results, however, there is limited novalty ", "rating": "5: Marginally below acceptance threshold", "review": "This paper empirical studies the relationship among the mini-batch size, learning rate, the sharpness of the local minima and the final performance. In general, it shows that large learning rate and small batch size would usually lead to better generalization performance. The experiment is insightful and expected. There is not much theoretical analysis about any of these observations. Additionally, most of these relationships are known from previous work.\n\nDetailed comments:\n\n(1) The author should mention some highly related work that studies similar topic, such as \n    [1]Coupling Adaptive Batch Sizes with Learning Rates.Lukas Balles, Javier Romero, Philipp Hennig\n    [2] A Bayesian Perspective on Generalization and Stochastic Gradient Descent, Samuel L. Smith, Quoc V. Le\n\n(2)  Why does the work use spectral norm and Frobenius norm of Hessian to measure the flatness? Why not use the conditioned number (the ratio between largest eigenvalue and smallest once) of the Hessian would be a better metric?\n\n(3) In Figure 4, the gap between training accuracies for various setting is very large. For example, \"LR = 0.2, BS = 500\" leads to about 65% accuracy. \"LR=0.5, BS=500\" causes about 80% accuracy. I wonder whether such big differences can be expected in general.\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Finding Flatter Minima with SGD", "abstract": "It has been discussed that over-parameterized deep neural networks (DNNs) trained using stochastic gradient descent (SGD) with smaller batch sizes generalize better compared with those trained with larger batch sizes. Additionally, model parameters found by small batch size SGD tend to be in flatter regions. We extend these empirical observations and experimentally show that both large learning rate and small batch size contribute towards SGD finding flatter minima that generalize well. Conversely, we find that small learning rates and large batch sizes lead to sharper minima that correlate with poor generalization in DNNs.", "paperhash": "jastrzbski|finding_flatter_minima_with_sgd", "keywords": ["SGD", "flat minima"], "_bibtex": "@misc{\n  jastrz\u0119bski2018finding,\n  title={Finding Flatter Minima with SGD},\n  author={Stanis\u0142aw Jastrz\u0119bski and Zachary Kenton and Devansh Arpit and Nicolas Ballas and Asja Fischer and Yoshua Bengio and Amos Storkey},\n  year={2018},\n  url={https://openreview.net/forum?id=r1VF9dCUG}\n}", "authorids": ["devansharpit@gmail.com"], "authors": ["Stanis\u0142aw Jastrz\u0119bski", "Zachary Kenton", "Devansh Arpit", "Nicolas Ballas", "Asja Fischer", "Yoshua Bengio", "Amos Storkey"], "TL;DR": "Small batch size and large learning rate steer SGD towards flat minima", "pdf": "/pdf/564f1bb4b11636655731ee8bbe10b925d32c2087.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582994434, "id": "ICLR.cc/2018/Workshop/-/Paper78/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper78/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper78/AnonReviewer5", "ICLR.cc/2018/Workshop/Paper78/AnonReviewer4", "ICLR.cc/2018/Workshop/Paper78/AnonReviewer3"], "reply": {"forum": "r1VF9dCUG", "replyto": "r1VF9dCUG", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper78/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper78/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582994434}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521573563527, "tcdate": 1521573563527, "number": 90, "cdate": 1521573563188, "id": "H14pACRYM", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "r1VF9dCUG", "replyto": "r1VF9dCUG", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Accept", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "Congratulations, your paper was accepted to the ICLR workshop."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Finding Flatter Minima with SGD", "abstract": "It has been discussed that over-parameterized deep neural networks (DNNs) trained using stochastic gradient descent (SGD) with smaller batch sizes generalize better compared with those trained with larger batch sizes. Additionally, model parameters found by small batch size SGD tend to be in flatter regions. We extend these empirical observations and experimentally show that both large learning rate and small batch size contribute towards SGD finding flatter minima that generalize well. Conversely, we find that small learning rates and large batch sizes lead to sharper minima that correlate with poor generalization in DNNs.", "paperhash": "jastrzbski|finding_flatter_minima_with_sgd", "keywords": ["SGD", "flat minima"], "_bibtex": "@misc{\n  jastrz\u0119bski2018finding,\n  title={Finding Flatter Minima with SGD},\n  author={Stanis\u0142aw Jastrz\u0119bski and Zachary Kenton and Devansh Arpit and Nicolas Ballas and Asja Fischer and Yoshua Bengio and Amos Storkey},\n  year={2018},\n  url={https://openreview.net/forum?id=r1VF9dCUG}\n}", "authorids": ["devansharpit@gmail.com"], "authors": ["Stanis\u0142aw Jastrz\u0119bski", "Zachary Kenton", "Devansh Arpit", "Nicolas Ballas", "Asja Fischer", "Yoshua Bengio", "Amos Storkey"], "TL;DR": "Small batch size and large learning rate steer SGD towards flat minima", "pdf": "/pdf/564f1bb4b11636655731ee8bbe10b925d32c2087.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}], "count": 5}