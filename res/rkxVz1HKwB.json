{"notes": [{"id": "rkxVz1HKwB", "original": "rylzUWh_vB", "number": 1576, "cdate": 1569439500009, "ddate": null, "tcdate": 1569439500009, "tmdate": 1577168231506, "tddate": null, "forum": "rkxVz1HKwB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Certifiably Robust Interpretation in Deep Learning", "authors": ["Alexander Levine", "Sahil Singla", "Soheil Feizi"], "authorids": ["alevine0@cs.umd.edu", "ssingla@cs.umd.edu", "sfeizi@cs.umd.edu"], "keywords": ["deep learning interpretation", "robustness certificates", "adversarial examples"], "TL;DR": "We develop an interpretation procedure for deep learning models which is certifiably robust to adversarial attack.", "abstract": "Deep learning interpretation is essential to explain the reasoning behind model predictions. Understanding the robustness of interpretation methods is important especially in sensitive domains such as medical applications since interpretation results are often used in downstream tasks. Although gradient-based saliency maps are popular methods for deep learning interpretation, recent works show that they can be vulnerable to adversarial attacks. In this paper, we address this problem and provide a certifiable defense method for deep learning interpretation. We show that a sparsified version of the popular SmoothGrad method, which computes the average saliency maps over random perturbations of the input, is certifiably robust against adversarial perturbations. We obtain this result by extending recent bounds for certifiably robust smooth classifiers to the interpretation setting. Experiments on ImageNet samples validate our theory.", "code": "https://github.com/anonICLR5/robust-interpretation", "pdf": "/pdf/be7f39474ec772cfc66afbd22a4ecb050e893ef8.pdf", "paperhash": "levine|certifiably_robust_interpretation_in_deep_learning", "original_pdf": "/attachment/4718fa24063c9b72347882d05e7d1c45c996a769.pdf", "_bibtex": "@misc{\nlevine2020certifiably,\ntitle={Certifiably Robust Interpretation in Deep Learning},\nauthor={Alexander Levine and Sahil Singla and Soheil Feizi},\nyear={2020},\nurl={https://openreview.net/forum?id=rkxVz1HKwB}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 14, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "hvhITUvaMK", "original": null, "number": 11, "cdate": 1576888056227, "ddate": null, "tcdate": 1576888056227, "tmdate": 1576888056227, "tddate": null, "forum": "rkxVz1HKwB", "replyto": "LVUaRKvEg8", "invitation": "ICLR.cc/2020/Conference/Paper1576/-/Official_Comment", "content": {"title": "Author Response", "comment": "This review, unfortunately, mischaracterizes the main contribution of our paper. We propose a provable *defense* against adversarial attacks on saliency maps: such attacks were already previously proposed by other authors (Ghorbani et al. 2019). The existence of these attacks provides the motivation for provable defenses, e.g. our work."}, "signatures": ["ICLR.cc/2020/Conference/Paper1576/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1576/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Certifiably Robust Interpretation in Deep Learning", "authors": ["Alexander Levine", "Sahil Singla", "Soheil Feizi"], "authorids": ["alevine0@cs.umd.edu", "ssingla@cs.umd.edu", "sfeizi@cs.umd.edu"], "keywords": ["deep learning interpretation", "robustness certificates", "adversarial examples"], "TL;DR": "We develop an interpretation procedure for deep learning models which is certifiably robust to adversarial attack.", "abstract": "Deep learning interpretation is essential to explain the reasoning behind model predictions. Understanding the robustness of interpretation methods is important especially in sensitive domains such as medical applications since interpretation results are often used in downstream tasks. Although gradient-based saliency maps are popular methods for deep learning interpretation, recent works show that they can be vulnerable to adversarial attacks. In this paper, we address this problem and provide a certifiable defense method for deep learning interpretation. We show that a sparsified version of the popular SmoothGrad method, which computes the average saliency maps over random perturbations of the input, is certifiably robust against adversarial perturbations. We obtain this result by extending recent bounds for certifiably robust smooth classifiers to the interpretation setting. Experiments on ImageNet samples validate our theory.", "code": "https://github.com/anonICLR5/robust-interpretation", "pdf": "/pdf/be7f39474ec772cfc66afbd22a4ecb050e893ef8.pdf", "paperhash": "levine|certifiably_robust_interpretation_in_deep_learning", "original_pdf": "/attachment/4718fa24063c9b72347882d05e7d1c45c996a769.pdf", "_bibtex": "@misc{\nlevine2020certifiably,\ntitle={Certifiably Robust Interpretation in Deep Learning},\nauthor={Alexander Levine and Sahil Singla and Soheil Feizi},\nyear={2020},\nurl={https://openreview.net/forum?id=rkxVz1HKwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rkxVz1HKwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1576/Authors", "ICLR.cc/2020/Conference/Paper1576/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1576/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1576/Reviewers", "ICLR.cc/2020/Conference/Paper1576/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1576/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1576/Authors|ICLR.cc/2020/Conference/Paper1576/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504153971, "tmdate": 1576860554590, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1576/Authors", "ICLR.cc/2020/Conference/Paper1576/Reviewers", "ICLR.cc/2020/Conference/Paper1576/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1576/-/Official_Comment"}}}, {"id": "LVUaRKvEg8", "original": null, "number": 1, "cdate": 1576798726970, "ddate": null, "tcdate": 1576798726970, "tmdate": 1576800909509, "tddate": null, "forum": "rkxVz1HKwB", "replyto": "rkxVz1HKwB", "invitation": "ICLR.cc/2020/Conference/Paper1576/-/Decision", "content": {"decision": "Reject", "comment": "This paper discusses new methods to perform adversarial attacks on salience maps.\n\nIn its current form, this paper in its current form has unfortunately has not convinced several of the reviewers/commenters of the motivation behind proposing such a method. I tend to share the same opinion. I would encourage the authors to re-think the motivation of the work, and if there are indeed solid use cases to express them explicitly in the next version of the paper.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Certifiably Robust Interpretation in Deep Learning", "authors": ["Alexander Levine", "Sahil Singla", "Soheil Feizi"], "authorids": ["alevine0@cs.umd.edu", "ssingla@cs.umd.edu", "sfeizi@cs.umd.edu"], "keywords": ["deep learning interpretation", "robustness certificates", "adversarial examples"], "TL;DR": "We develop an interpretation procedure for deep learning models which is certifiably robust to adversarial attack.", "abstract": "Deep learning interpretation is essential to explain the reasoning behind model predictions. Understanding the robustness of interpretation methods is important especially in sensitive domains such as medical applications since interpretation results are often used in downstream tasks. Although gradient-based saliency maps are popular methods for deep learning interpretation, recent works show that they can be vulnerable to adversarial attacks. In this paper, we address this problem and provide a certifiable defense method for deep learning interpretation. We show that a sparsified version of the popular SmoothGrad method, which computes the average saliency maps over random perturbations of the input, is certifiably robust against adversarial perturbations. We obtain this result by extending recent bounds for certifiably robust smooth classifiers to the interpretation setting. Experiments on ImageNet samples validate our theory.", "code": "https://github.com/anonICLR5/robust-interpretation", "pdf": "/pdf/be7f39474ec772cfc66afbd22a4ecb050e893ef8.pdf", "paperhash": "levine|certifiably_robust_interpretation_in_deep_learning", "original_pdf": "/attachment/4718fa24063c9b72347882d05e7d1c45c996a769.pdf", "_bibtex": "@misc{\nlevine2020certifiably,\ntitle={Certifiably Robust Interpretation in Deep Learning},\nauthor={Alexander Levine and Sahil Singla and Soheil Feizi},\nyear={2020},\nurl={https://openreview.net/forum?id=rkxVz1HKwB}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "rkxVz1HKwB", "replyto": "rkxVz1HKwB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795711842, "tmdate": 1576800261112, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1576/-/Decision"}}}, {"id": "Hkxsru03FB", "original": null, "number": 1, "cdate": 1571772483318, "ddate": null, "tcdate": 1571772483318, "tmdate": 1574437867660, "tddate": null, "forum": "rkxVz1HKwB", "replyto": "rkxVz1HKwB", "invitation": "ICLR.cc/2020/Conference/Paper1576/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #3", "review": "This paper introduces an extension of Cohen et al. (2019)\u2019s result that allows one to derive robustness certificates for interpretation methods, as well as a bound on the top-K overlap of saliency methods. These results motivate the introduction of Sparsified SmoothGrad and a relaxation of this method that has differentiable elements. These introduced approaches adapt previous methods so the derived bounds are applicable. The proposed methods are shown to perform as well as Quadratic SmoothGrad (Smilkov et al. 2017) in CIFAR-10 experiments.\n\nI\u2019m not familiar with the field so it is hard for me to judge how novel the presented results are or whether the used baselines are the proper ones. That being said, the paper presents an interesting idea and it is relatively easy to read (I really appreciate the fact that for every theorem there is an interpretation, in words, for it). The only thing that sometimes makes the paper hard to read is when it starts to refer to too many constants without remind the reader what they are about. I have two complaints/questions about the relevance of the introduced bounds though. Right now, to me, it seems that the derived theoretical guarantees are not that relevant, hopefully the questions below will help clarify that.\n\nIn page 6, before introducing the \u201cSparsified SmoothGrad and its Relaxations\u201d, it is said that q is set to 2^13 because otherwise the gap would be too large in images from ImageNet, for example, when comparing to traditional values of q. However, ImageNet is never revisited in the paper. I was expecting to see ImageNet results in the experimental section but they are not there (or maybe some correlation between the gap and performance -- robustness). More than that, the Quadratic SmoothGrad, which doesn\u2019t have any theoretical guarantee, seems to perform as well as the proposed methods. So where is the gap/theoretical result relevant? What are the settings in which having a method with the derived theoretical guarantees shine? What are the limitations of Quadratic SmoothGrad? Right now, it seems to me that the \u201cSparsified SmoothGrad and its Relaxations\u201d and its empirical analysis weaken the paper, because they take a big chunk of it when there is not enough evidence to claim them as an important contribution. Am I missing something? I gave this paper a relatively low score because I\u2019m not certain about the relevance of its results, but if my questions are satisfactory answered, I\u2019ll be happy to update my score.\n\n------\n\n\n>>> Update after rebuttal: I stand by my score after the rebuttal. \n\nUnfortunately I'm not an expert in this area and I don't feel confident in having a very strong opinion about this paper. That being said, enough presentation issues were raised that make me uneasy about raising my score. I do agree with some of the concerns raised by other reviewers. \n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1576/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1576/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Certifiably Robust Interpretation in Deep Learning", "authors": ["Alexander Levine", "Sahil Singla", "Soheil Feizi"], "authorids": ["alevine0@cs.umd.edu", "ssingla@cs.umd.edu", "sfeizi@cs.umd.edu"], "keywords": ["deep learning interpretation", "robustness certificates", "adversarial examples"], "TL;DR": "We develop an interpretation procedure for deep learning models which is certifiably robust to adversarial attack.", "abstract": "Deep learning interpretation is essential to explain the reasoning behind model predictions. Understanding the robustness of interpretation methods is important especially in sensitive domains such as medical applications since interpretation results are often used in downstream tasks. Although gradient-based saliency maps are popular methods for deep learning interpretation, recent works show that they can be vulnerable to adversarial attacks. In this paper, we address this problem and provide a certifiable defense method for deep learning interpretation. We show that a sparsified version of the popular SmoothGrad method, which computes the average saliency maps over random perturbations of the input, is certifiably robust against adversarial perturbations. We obtain this result by extending recent bounds for certifiably robust smooth classifiers to the interpretation setting. Experiments on ImageNet samples validate our theory.", "code": "https://github.com/anonICLR5/robust-interpretation", "pdf": "/pdf/be7f39474ec772cfc66afbd22a4ecb050e893ef8.pdf", "paperhash": "levine|certifiably_robust_interpretation_in_deep_learning", "original_pdf": "/attachment/4718fa24063c9b72347882d05e7d1c45c996a769.pdf", "_bibtex": "@misc{\nlevine2020certifiably,\ntitle={Certifiably Robust Interpretation in Deep Learning},\nauthor={Alexander Levine and Sahil Singla and Soheil Feizi},\nyear={2020},\nurl={https://openreview.net/forum?id=rkxVz1HKwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rkxVz1HKwB", "replyto": "rkxVz1HKwB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1576/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1576/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574722376000, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1576/Reviewers"], "noninvitees": [], "tcdate": 1570237735370, "tmdate": 1574723089395, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1576/-/Official_Review"}}}, {"id": "rJe_xgpssr", "original": null, "number": 7, "cdate": 1573797872227, "ddate": null, "tcdate": 1573797872227, "tmdate": 1573797872227, "tddate": null, "forum": "rkxVz1HKwB", "replyto": "Hyenpjhior", "invitation": "ICLR.cc/2020/Conference/Paper1576/-/Official_Comment", "content": {"title": "still not a solid motivation", "comment": "In your response the motivation you present now is that there is some adversary which will corrupt the image. \n\nI don't agree and I echo the \"Rigor Police\" comment here that there there is no reasonable adversary here for medical images. How can a criminal profit? Who is the criminal? What do they gain? It is important that the work have a solid motivation which clearly translates to move us forward so we don't spend time solving problems that people don't have."}, "signatures": ["ICLR.cc/2020/Conference/Paper1576/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1576/AnonReviewer1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Certifiably Robust Interpretation in Deep Learning", "authors": ["Alexander Levine", "Sahil Singla", "Soheil Feizi"], "authorids": ["alevine0@cs.umd.edu", "ssingla@cs.umd.edu", "sfeizi@cs.umd.edu"], "keywords": ["deep learning interpretation", "robustness certificates", "adversarial examples"], "TL;DR": "We develop an interpretation procedure for deep learning models which is certifiably robust to adversarial attack.", "abstract": "Deep learning interpretation is essential to explain the reasoning behind model predictions. Understanding the robustness of interpretation methods is important especially in sensitive domains such as medical applications since interpretation results are often used in downstream tasks. Although gradient-based saliency maps are popular methods for deep learning interpretation, recent works show that they can be vulnerable to adversarial attacks. In this paper, we address this problem and provide a certifiable defense method for deep learning interpretation. We show that a sparsified version of the popular SmoothGrad method, which computes the average saliency maps over random perturbations of the input, is certifiably robust against adversarial perturbations. We obtain this result by extending recent bounds for certifiably robust smooth classifiers to the interpretation setting. Experiments on ImageNet samples validate our theory.", "code": "https://github.com/anonICLR5/robust-interpretation", "pdf": "/pdf/be7f39474ec772cfc66afbd22a4ecb050e893ef8.pdf", "paperhash": "levine|certifiably_robust_interpretation_in_deep_learning", "original_pdf": "/attachment/4718fa24063c9b72347882d05e7d1c45c996a769.pdf", "_bibtex": "@misc{\nlevine2020certifiably,\ntitle={Certifiably Robust Interpretation in Deep Learning},\nauthor={Alexander Levine and Sahil Singla and Soheil Feizi},\nyear={2020},\nurl={https://openreview.net/forum?id=rkxVz1HKwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rkxVz1HKwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1576/Authors", "ICLR.cc/2020/Conference/Paper1576/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1576/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1576/Reviewers", "ICLR.cc/2020/Conference/Paper1576/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1576/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1576/Authors|ICLR.cc/2020/Conference/Paper1576/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504153971, "tmdate": 1576860554590, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1576/Authors", "ICLR.cc/2020/Conference/Paper1576/Reviewers", "ICLR.cc/2020/Conference/Paper1576/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1576/-/Official_Comment"}}}, {"id": "BkeotR3osr", "original": null, "number": 6, "cdate": 1573797506931, "ddate": null, "tcdate": 1573797506931, "tmdate": 1573797506931, "tddate": null, "forum": "rkxVz1HKwB", "replyto": "HkgezgVA5H", "invitation": "ICLR.cc/2020/Conference/Paper1576/-/Official_Comment", "content": {"title": "Author Response", "comment": "We thank you for your comment. Your concerns here are equally applicable to the study of adversarial robustness in the classification case: in both instances, there is a desire to protect against adversarial attacks which may affect how a machine learning system makes decisions. The suggestion that users would \u201cignore the decision\u201d if a system returns an incorrect result assumes that the system is entirely redundant: that its output has no effect on the users\u2019 behavior. This is true in the classification case as well: if we a priori assume that the users know the correct classification before looking at the output, adversarial examples cannot possibly cause any harm. In addition to the medical examples laid out in the paper, gradient-based methods are also used for automated image segmentation and object localization: Subramanya, et al. (https://arxiv.org/abs/1812.02843) recently introduced an adversarial attack against GradCAM, a variation of gradient-based saliency maps which is tailored for object localization specifically.\nIn the classification case, the wide literature on adversarial robustness published in recent years indicates that  the community considers adversarial attacks to be an issue worthy of concern: attacks against interpretation are just as plausible from a security standpoint as (non-physical) attacks against classification.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1576/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1576/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Certifiably Robust Interpretation in Deep Learning", "authors": ["Alexander Levine", "Sahil Singla", "Soheil Feizi"], "authorids": ["alevine0@cs.umd.edu", "ssingla@cs.umd.edu", "sfeizi@cs.umd.edu"], "keywords": ["deep learning interpretation", "robustness certificates", "adversarial examples"], "TL;DR": "We develop an interpretation procedure for deep learning models which is certifiably robust to adversarial attack.", "abstract": "Deep learning interpretation is essential to explain the reasoning behind model predictions. Understanding the robustness of interpretation methods is important especially in sensitive domains such as medical applications since interpretation results are often used in downstream tasks. Although gradient-based saliency maps are popular methods for deep learning interpretation, recent works show that they can be vulnerable to adversarial attacks. In this paper, we address this problem and provide a certifiable defense method for deep learning interpretation. We show that a sparsified version of the popular SmoothGrad method, which computes the average saliency maps over random perturbations of the input, is certifiably robust against adversarial perturbations. We obtain this result by extending recent bounds for certifiably robust smooth classifiers to the interpretation setting. Experiments on ImageNet samples validate our theory.", "code": "https://github.com/anonICLR5/robust-interpretation", "pdf": "/pdf/be7f39474ec772cfc66afbd22a4ecb050e893ef8.pdf", "paperhash": "levine|certifiably_robust_interpretation_in_deep_learning", "original_pdf": "/attachment/4718fa24063c9b72347882d05e7d1c45c996a769.pdf", "_bibtex": "@misc{\nlevine2020certifiably,\ntitle={Certifiably Robust Interpretation in Deep Learning},\nauthor={Alexander Levine and Sahil Singla and Soheil Feizi},\nyear={2020},\nurl={https://openreview.net/forum?id=rkxVz1HKwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rkxVz1HKwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1576/Authors", "ICLR.cc/2020/Conference/Paper1576/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1576/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1576/Reviewers", "ICLR.cc/2020/Conference/Paper1576/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1576/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1576/Authors|ICLR.cc/2020/Conference/Paper1576/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504153971, "tmdate": 1576860554590, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1576/Authors", "ICLR.cc/2020/Conference/Paper1576/Reviewers", "ICLR.cc/2020/Conference/Paper1576/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1576/-/Official_Comment"}}}, {"id": "H1e7_anior", "original": null, "number": 5, "cdate": 1573797226694, "ddate": null, "tcdate": 1573797226694, "tmdate": 1573797226694, "tddate": null, "forum": "rkxVz1HKwB", "replyto": "H1e5BxVC5r", "invitation": "ICLR.cc/2020/Conference/Paper1576/-/Official_Comment", "content": {"title": "Author Response", "comment": " We thank you for your comment.\n\nQualitative Evaluation on ImageNet: We have added additional qualitative comparisons on ImageNet, in Figure 3 and Appendix G.\n\t\nGradients, SoftPlus and Transfer vs Whitebox Attacks: In order to use first-order methods to adversarially attack gradient-based interpretations, a network must have defined second derivatives with respect to the input image (because the saliency map itself consists of the first derivatives of the output with respect to the input image).  ReLU networks thus cannot be attacked in this way. Therefore, we use a proxy network with SoftPlus activations to determine the direction of the attack.\n\nFigure 2 Y Axis: This is the 60th percentile of the robustness certificate: 60 percent of images have robustness certificates at least this large.\n\nRank-based Certificates: it is clear that an $L_p$ norm based metric would be inappropriate for the purpose of certifying similarity between saliency maps: in most works using gradient-based saliency maps (e.g., Sundararajan et al.  (2017)), the top values are clipped for visualization purposes, so that the rest of the interpretation can be scaled to a reasonable color range without being dominated by a few large outlier pixels. This suggests that an $L_p$ norm approach may be meaningless, because an $L_p$ norm could be dominated by the behavior of outlier values. The fact that this clipping is accepted practice also indicates that it is the relative rank of the importance of features of an image, rather than the absolute ratios between salience measures, that is important for interpretation.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1576/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1576/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Certifiably Robust Interpretation in Deep Learning", "authors": ["Alexander Levine", "Sahil Singla", "Soheil Feizi"], "authorids": ["alevine0@cs.umd.edu", "ssingla@cs.umd.edu", "sfeizi@cs.umd.edu"], "keywords": ["deep learning interpretation", "robustness certificates", "adversarial examples"], "TL;DR": "We develop an interpretation procedure for deep learning models which is certifiably robust to adversarial attack.", "abstract": "Deep learning interpretation is essential to explain the reasoning behind model predictions. Understanding the robustness of interpretation methods is important especially in sensitive domains such as medical applications since interpretation results are often used in downstream tasks. Although gradient-based saliency maps are popular methods for deep learning interpretation, recent works show that they can be vulnerable to adversarial attacks. In this paper, we address this problem and provide a certifiable defense method for deep learning interpretation. We show that a sparsified version of the popular SmoothGrad method, which computes the average saliency maps over random perturbations of the input, is certifiably robust against adversarial perturbations. We obtain this result by extending recent bounds for certifiably robust smooth classifiers to the interpretation setting. Experiments on ImageNet samples validate our theory.", "code": "https://github.com/anonICLR5/robust-interpretation", "pdf": "/pdf/be7f39474ec772cfc66afbd22a4ecb050e893ef8.pdf", "paperhash": "levine|certifiably_robust_interpretation_in_deep_learning", "original_pdf": "/attachment/4718fa24063c9b72347882d05e7d1c45c996a769.pdf", "_bibtex": "@misc{\nlevine2020certifiably,\ntitle={Certifiably Robust Interpretation in Deep Learning},\nauthor={Alexander Levine and Sahil Singla and Soheil Feizi},\nyear={2020},\nurl={https://openreview.net/forum?id=rkxVz1HKwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rkxVz1HKwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1576/Authors", "ICLR.cc/2020/Conference/Paper1576/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1576/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1576/Reviewers", "ICLR.cc/2020/Conference/Paper1576/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1576/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1576/Authors|ICLR.cc/2020/Conference/Paper1576/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504153971, "tmdate": 1576860554590, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1576/Authors", "ICLR.cc/2020/Conference/Paper1576/Reviewers", "ICLR.cc/2020/Conference/Paper1576/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1576/-/Official_Comment"}}}, {"id": "HkeO62hooB", "original": null, "number": 4, "cdate": 1573797056051, "ddate": null, "tcdate": 1573797056051, "tmdate": 1573797056051, "tddate": null, "forum": "rkxVz1HKwB", "replyto": "Hkxsru03FB", "invitation": "ICLR.cc/2020/Conference/Paper1576/-/Official_Comment", "content": {"title": "Author Response", "comment": "We thank you for your feedback. We evaluate our robustness certificates on ImageNet samples in Figure 2. To address the concern about the gap between empirical and certified robustness, we show this gap on CIFAR samples in Appendix J. While the size of  perturbations with certified robustness is small compared to the empirical robustness on these samples, our main contribution is to demonstrate that a minor variation on the commonly-used SmoothGrad technique does in fact have a robustness guarantee: furthermore, this is the first robustness certificate for interpretation that can be evaluated at the ImageNet scale. This minor modification to SmoothGrad has little effect on the visual output (Figure 3).  Additionally, in testing the empirical attacks (Figure 4), we show that both quadratic SmoothGrad and our variant are empirically robust. Therefore, the variant (sparsified SmoothGrad) combines the visual quality and empirical robustness of Quadratic SmoothGrad with an additional theoretical guarantee of robustness.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1576/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1576/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Certifiably Robust Interpretation in Deep Learning", "authors": ["Alexander Levine", "Sahil Singla", "Soheil Feizi"], "authorids": ["alevine0@cs.umd.edu", "ssingla@cs.umd.edu", "sfeizi@cs.umd.edu"], "keywords": ["deep learning interpretation", "robustness certificates", "adversarial examples"], "TL;DR": "We develop an interpretation procedure for deep learning models which is certifiably robust to adversarial attack.", "abstract": "Deep learning interpretation is essential to explain the reasoning behind model predictions. Understanding the robustness of interpretation methods is important especially in sensitive domains such as medical applications since interpretation results are often used in downstream tasks. Although gradient-based saliency maps are popular methods for deep learning interpretation, recent works show that they can be vulnerable to adversarial attacks. In this paper, we address this problem and provide a certifiable defense method for deep learning interpretation. We show that a sparsified version of the popular SmoothGrad method, which computes the average saliency maps over random perturbations of the input, is certifiably robust against adversarial perturbations. We obtain this result by extending recent bounds for certifiably robust smooth classifiers to the interpretation setting. Experiments on ImageNet samples validate our theory.", "code": "https://github.com/anonICLR5/robust-interpretation", "pdf": "/pdf/be7f39474ec772cfc66afbd22a4ecb050e893ef8.pdf", "paperhash": "levine|certifiably_robust_interpretation_in_deep_learning", "original_pdf": "/attachment/4718fa24063c9b72347882d05e7d1c45c996a769.pdf", "_bibtex": "@misc{\nlevine2020certifiably,\ntitle={Certifiably Robust Interpretation in Deep Learning},\nauthor={Alexander Levine and Sahil Singla and Soheil Feizi},\nyear={2020},\nurl={https://openreview.net/forum?id=rkxVz1HKwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rkxVz1HKwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1576/Authors", "ICLR.cc/2020/Conference/Paper1576/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1576/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1576/Reviewers", "ICLR.cc/2020/Conference/Paper1576/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1576/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1576/Authors|ICLR.cc/2020/Conference/Paper1576/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504153971, "tmdate": 1576860554590, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1576/Authors", "ICLR.cc/2020/Conference/Paper1576/Reviewers", "ICLR.cc/2020/Conference/Paper1576/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1576/-/Official_Comment"}}}, {"id": "Hyenpjhior", "original": null, "number": 3, "cdate": 1573796803764, "ddate": null, "tcdate": 1573796803764, "tmdate": 1573796803764, "tddate": null, "forum": "rkxVz1HKwB", "replyto": "r1epdz-RKr", "invitation": "ICLR.cc/2020/Conference/Paper1576/-/Official_Comment", "content": {"title": "Author Response", "comment": "We respectfully disagree. We believe that you may have misunderstood the main point of the paper. You mention that:\n\n \u201cI believe right now just the basic gradient is sufficient to indicate the region of interest.\u201d\n\nThe central issue here is that basic gradient methods may NOT in fact indicate the region of interest in an image. A small adversarial noise can keep the label as is but change the basic gradient result significantly. This is the problem that we are addressing in this paper. \n\nAs you mention, gradient-based saliency maps represent only a local first order approximation to the true influence of each feature on the decision. This leads to two issues:\n\n* Low quality natural interpretations: as noted by Smilkov, et al. (2017) the gradient with respect to a particular pixel may \u201cfluctuate sharply at small scales\u201d and therefore be \u201cless meaningful than a local average of gradient values.\u201d This observation led to the development of SmoothGrad. To put this simply, a large gradient value over a (very) small range of input values of a feature represents in total a small influence on the class score by that feature. However, if the input image happens to be within this interval where the gradient is large, the feature will erroneously appear to be highly salient. In practice, this leads to simple gradient-based interpretations looking \u201cnoisy,\u201d as apparently random pixels appear to be highly salient.\n\n* Adversarial attacks on interpretation: as demonstrated by Ghorbani, et al. (2019), one can adversarially craft examples where the basic gradient interpretation is in fact very different from the true region of interest. This is a direct consequence of the saliency map being a \u201cfirst order approximation\u201d: it is therefore possible to make this approximation adversarially bad, by crafting a small perturbation to the input.\n \nAs detailed in the paper, saliency maps are used in a broad range of highly sensitive downstream applications, including in medical imaging and object localization. Because an adversarial attack has been proposed by Ghorbani et al. (2019) which can distort saliency maps, it is therefore a topic of interest to defend against this type of adversarial attack. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1576/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1576/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Certifiably Robust Interpretation in Deep Learning", "authors": ["Alexander Levine", "Sahil Singla", "Soheil Feizi"], "authorids": ["alevine0@cs.umd.edu", "ssingla@cs.umd.edu", "sfeizi@cs.umd.edu"], "keywords": ["deep learning interpretation", "robustness certificates", "adversarial examples"], "TL;DR": "We develop an interpretation procedure for deep learning models which is certifiably robust to adversarial attack.", "abstract": "Deep learning interpretation is essential to explain the reasoning behind model predictions. Understanding the robustness of interpretation methods is important especially in sensitive domains such as medical applications since interpretation results are often used in downstream tasks. Although gradient-based saliency maps are popular methods for deep learning interpretation, recent works show that they can be vulnerable to adversarial attacks. In this paper, we address this problem and provide a certifiable defense method for deep learning interpretation. We show that a sparsified version of the popular SmoothGrad method, which computes the average saliency maps over random perturbations of the input, is certifiably robust against adversarial perturbations. We obtain this result by extending recent bounds for certifiably robust smooth classifiers to the interpretation setting. Experiments on ImageNet samples validate our theory.", "code": "https://github.com/anonICLR5/robust-interpretation", "pdf": "/pdf/be7f39474ec772cfc66afbd22a4ecb050e893ef8.pdf", "paperhash": "levine|certifiably_robust_interpretation_in_deep_learning", "original_pdf": "/attachment/4718fa24063c9b72347882d05e7d1c45c996a769.pdf", "_bibtex": "@misc{\nlevine2020certifiably,\ntitle={Certifiably Robust Interpretation in Deep Learning},\nauthor={Alexander Levine and Sahil Singla and Soheil Feizi},\nyear={2020},\nurl={https://openreview.net/forum?id=rkxVz1HKwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rkxVz1HKwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1576/Authors", "ICLR.cc/2020/Conference/Paper1576/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1576/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1576/Reviewers", "ICLR.cc/2020/Conference/Paper1576/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1576/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1576/Authors|ICLR.cc/2020/Conference/Paper1576/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504153971, "tmdate": 1576860554590, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1576/Authors", "ICLR.cc/2020/Conference/Paper1576/Reviewers", "ICLR.cc/2020/Conference/Paper1576/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1576/-/Official_Comment"}}}, {"id": "H1guSchjjS", "original": null, "number": 2, "cdate": 1573796416056, "ddate": null, "tcdate": 1573796416056, "tmdate": 1573796416056, "tddate": null, "forum": "rkxVz1HKwB", "replyto": "SJlg24cQcr", "invitation": "ICLR.cc/2020/Conference/Paper1576/-/Official_Comment", "content": {"title": "Author Response", "comment": "We thank you for your constructive feedback. To address your comments:\n\n1 and 2. Note that we sparsify the saliency maps before smoothing: in other words, the final smoothed saliency map will be non-sparse, because pixels which are less salient overall may still occur in the top 10% in a minority of random samples. Empirically, we find that this sparsification prior to averaging has little effect on the final smoothed interpretation: in particular, the results are visually very similar to the quadratic SmoothGrad proposed by (Smilkov et al. 2017). This was shown in Figure 3 on an ImageNet sample, as well as on additional CIFAR samples in Appendix G. To address this comment, we have added additional ImageNet samples both in the body of the paper (Figure 3) and in the appendix (Appendix G).\n\n3: We have added empirical tests using additional values of the sparsification parameter to Figure 4.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1576/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1576/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Certifiably Robust Interpretation in Deep Learning", "authors": ["Alexander Levine", "Sahil Singla", "Soheil Feizi"], "authorids": ["alevine0@cs.umd.edu", "ssingla@cs.umd.edu", "sfeizi@cs.umd.edu"], "keywords": ["deep learning interpretation", "robustness certificates", "adversarial examples"], "TL;DR": "We develop an interpretation procedure for deep learning models which is certifiably robust to adversarial attack.", "abstract": "Deep learning interpretation is essential to explain the reasoning behind model predictions. Understanding the robustness of interpretation methods is important especially in sensitive domains such as medical applications since interpretation results are often used in downstream tasks. Although gradient-based saliency maps are popular methods for deep learning interpretation, recent works show that they can be vulnerable to adversarial attacks. In this paper, we address this problem and provide a certifiable defense method for deep learning interpretation. We show that a sparsified version of the popular SmoothGrad method, which computes the average saliency maps over random perturbations of the input, is certifiably robust against adversarial perturbations. We obtain this result by extending recent bounds for certifiably robust smooth classifiers to the interpretation setting. Experiments on ImageNet samples validate our theory.", "code": "https://github.com/anonICLR5/robust-interpretation", "pdf": "/pdf/be7f39474ec772cfc66afbd22a4ecb050e893ef8.pdf", "paperhash": "levine|certifiably_robust_interpretation_in_deep_learning", "original_pdf": "/attachment/4718fa24063c9b72347882d05e7d1c45c996a769.pdf", "_bibtex": "@misc{\nlevine2020certifiably,\ntitle={Certifiably Robust Interpretation in Deep Learning},\nauthor={Alexander Levine and Sahil Singla and Soheil Feizi},\nyear={2020},\nurl={https://openreview.net/forum?id=rkxVz1HKwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rkxVz1HKwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1576/Authors", "ICLR.cc/2020/Conference/Paper1576/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1576/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1576/Reviewers", "ICLR.cc/2020/Conference/Paper1576/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1576/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1576/Authors|ICLR.cc/2020/Conference/Paper1576/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504153971, "tmdate": 1576860554590, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1576/Authors", "ICLR.cc/2020/Conference/Paper1576/Reviewers", "ICLR.cc/2020/Conference/Paper1576/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1576/-/Official_Comment"}}}, {"id": "r1epdz-RKr", "original": null, "number": 2, "cdate": 1571848820790, "ddate": null, "tcdate": 1571848820790, "tmdate": 1572972450508, "tddate": null, "forum": "rkxVz1HKwB", "replyto": "rkxVz1HKwB", "invitation": "ICLR.cc/2020/Conference/Paper1576/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a way to testify how much a SmoothGrad saliency can vary from the true saliency attesting to the adversarial robustness but with the goal of interpretation.\n\nAt the premise of this work I do not think the paper motivates the value of such a robustness certificate. Using the gradient (with SmoothGrad), while providing a reasonable interpretation of the model, is just a linear approximation of the true explanation of the prediction. So saying we have the correct approximation is not so useful. I also am not sure we need such a method. For example imagine a doctor is looking at a saliency map and we are sure that it is correct first order approximation because of some method. What were the negative cases where this would fail? How would this method improve that? I believe right now just the basic gradient is sufficient to indicate the region of interest.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1576/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1576/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Certifiably Robust Interpretation in Deep Learning", "authors": ["Alexander Levine", "Sahil Singla", "Soheil Feizi"], "authorids": ["alevine0@cs.umd.edu", "ssingla@cs.umd.edu", "sfeizi@cs.umd.edu"], "keywords": ["deep learning interpretation", "robustness certificates", "adversarial examples"], "TL;DR": "We develop an interpretation procedure for deep learning models which is certifiably robust to adversarial attack.", "abstract": "Deep learning interpretation is essential to explain the reasoning behind model predictions. Understanding the robustness of interpretation methods is important especially in sensitive domains such as medical applications since interpretation results are often used in downstream tasks. Although gradient-based saliency maps are popular methods for deep learning interpretation, recent works show that they can be vulnerable to adversarial attacks. In this paper, we address this problem and provide a certifiable defense method for deep learning interpretation. We show that a sparsified version of the popular SmoothGrad method, which computes the average saliency maps over random perturbations of the input, is certifiably robust against adversarial perturbations. We obtain this result by extending recent bounds for certifiably robust smooth classifiers to the interpretation setting. Experiments on ImageNet samples validate our theory.", "code": "https://github.com/anonICLR5/robust-interpretation", "pdf": "/pdf/be7f39474ec772cfc66afbd22a4ecb050e893ef8.pdf", "paperhash": "levine|certifiably_robust_interpretation_in_deep_learning", "original_pdf": "/attachment/4718fa24063c9b72347882d05e7d1c45c996a769.pdf", "_bibtex": "@misc{\nlevine2020certifiably,\ntitle={Certifiably Robust Interpretation in Deep Learning},\nauthor={Alexander Levine and Sahil Singla and Soheil Feizi},\nyear={2020},\nurl={https://openreview.net/forum?id=rkxVz1HKwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rkxVz1HKwB", "replyto": "rkxVz1HKwB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1576/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1576/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574722376000, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1576/Reviewers"], "noninvitees": [], "tcdate": 1570237735370, "tmdate": 1574723089395, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1576/-/Official_Review"}}}, {"id": "SJlg24cQcr", "original": null, "number": 3, "cdate": 1572213927544, "ddate": null, "tcdate": 1572213927544, "tmdate": 1572972450464, "tddate": null, "forum": "rkxVz1HKwB", "replyto": "rkxVz1HKwB", "invitation": "ICLR.cc/2020/Conference/Paper1576/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "The work addresses an important problem of robustness of interpretation methods against adversarial perturbations. The problem is well motivated as several gradient-based interpretations are sensitive to small adversarial perturbations. \n\nThe authors present a framework to compute the robustness certificate (more precisely, a lower bound to the actual robustness) of any general saliency map over an input example. They further propose variants of SmoothGrad interpretation method which are claimed to be more robust.    \n\nThe empirical validation of the underlying theory and use of the sparsified (and relaxed) SmoothGradient interpretation methods is unconvincing because of the following reasons:\n\n1. In the demonstrated experiment, the proposed alternative to SmoothGrad involves setting the lowest 90% of the saliency values to zero, and the top 10% (for sparsified SmoothGrad) or top 1% (in the case of relaxed sparsified SmoothGrad) to one. The problem with clamping most of the lower values to zero and the remainder (or most of the remainder) higher values to one is that it defeats the purpose of having a saliency map in the first place, which exist to characterize the relative importance of the input features. \n\n2. The paper claims that the proposed variant maintains the high visual quality of SmoothGrad, however, the claim is unsubstantiated. With the current setup, there is a clear trade-off between robustness and fidelity of interpretation, which the paper fails to acknowledge. In principle, one can always build extremely sparse or dense interpretation methods (close to all zeros or all ones), which would produce high robustness certificates but would be much less meaningful as they are not faithful to the underlying mechanism of prediction, and the characteristics of the input.\n\n3. The authors present empirical evidence on just one set of sparsification parameters and K. It would be more conclusive to evaluate the robustness of the proposed variations with different values of sparsification parameters, and K.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1576/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1576/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Certifiably Robust Interpretation in Deep Learning", "authors": ["Alexander Levine", "Sahil Singla", "Soheil Feizi"], "authorids": ["alevine0@cs.umd.edu", "ssingla@cs.umd.edu", "sfeizi@cs.umd.edu"], "keywords": ["deep learning interpretation", "robustness certificates", "adversarial examples"], "TL;DR": "We develop an interpretation procedure for deep learning models which is certifiably robust to adversarial attack.", "abstract": "Deep learning interpretation is essential to explain the reasoning behind model predictions. Understanding the robustness of interpretation methods is important especially in sensitive domains such as medical applications since interpretation results are often used in downstream tasks. Although gradient-based saliency maps are popular methods for deep learning interpretation, recent works show that they can be vulnerable to adversarial attacks. In this paper, we address this problem and provide a certifiable defense method for deep learning interpretation. We show that a sparsified version of the popular SmoothGrad method, which computes the average saliency maps over random perturbations of the input, is certifiably robust against adversarial perturbations. We obtain this result by extending recent bounds for certifiably robust smooth classifiers to the interpretation setting. Experiments on ImageNet samples validate our theory.", "code": "https://github.com/anonICLR5/robust-interpretation", "pdf": "/pdf/be7f39474ec772cfc66afbd22a4ecb050e893ef8.pdf", "paperhash": "levine|certifiably_robust_interpretation_in_deep_learning", "original_pdf": "/attachment/4718fa24063c9b72347882d05e7d1c45c996a769.pdf", "_bibtex": "@misc{\nlevine2020certifiably,\ntitle={Certifiably Robust Interpretation in Deep Learning},\nauthor={Alexander Levine and Sahil Singla and Soheil Feizi},\nyear={2020},\nurl={https://openreview.net/forum?id=rkxVz1HKwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rkxVz1HKwB", "replyto": "rkxVz1HKwB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1576/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1576/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574722376000, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1576/Reviewers"], "noninvitees": [], "tcdate": 1570237735370, "tmdate": 1574723089395, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1576/-/Official_Review"}}}, {"id": "S1ezL4_05H", "original": null, "number": 3, "cdate": 1572926537957, "ddate": null, "tcdate": 1572926537957, "tmdate": 1572926537957, "tddate": null, "forum": "rkxVz1HKwB", "replyto": "H1e5BxVC5r", "invitation": "ICLR.cc/2020/Conference/Paper1576/-/Public_Comment", "content": {"title": "Softplus networks are not horrible", "comment": "Dear Mr. Police,\n\nYou say \"Softplus networks are horrible.\" This statement is extremely unfair to softplus networks. The Swish paper [1] provides comprehensive results for 9 networks with different activation functions. Softplus often outperforms ReLU.\n\nI have not read this paper and this comment is not an endorsement of anything besides softplus networks.\n\n[1] Ramachandran, P., Zoph, B., & Le, Q. V. (2017). Searching for activation functions. https://arxiv.org/abs/1710.05941"}, "signatures": ["~Simon_Kornblith1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Simon_Kornblith1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Certifiably Robust Interpretation in Deep Learning", "authors": ["Alexander Levine", "Sahil Singla", "Soheil Feizi"], "authorids": ["alevine0@cs.umd.edu", "ssingla@cs.umd.edu", "sfeizi@cs.umd.edu"], "keywords": ["deep learning interpretation", "robustness certificates", "adversarial examples"], "TL;DR": "We develop an interpretation procedure for deep learning models which is certifiably robust to adversarial attack.", "abstract": "Deep learning interpretation is essential to explain the reasoning behind model predictions. Understanding the robustness of interpretation methods is important especially in sensitive domains such as medical applications since interpretation results are often used in downstream tasks. Although gradient-based saliency maps are popular methods for deep learning interpretation, recent works show that they can be vulnerable to adversarial attacks. In this paper, we address this problem and provide a certifiable defense method for deep learning interpretation. We show that a sparsified version of the popular SmoothGrad method, which computes the average saliency maps over random perturbations of the input, is certifiably robust against adversarial perturbations. We obtain this result by extending recent bounds for certifiably robust smooth classifiers to the interpretation setting. Experiments on ImageNet samples validate our theory.", "code": "https://github.com/anonICLR5/robust-interpretation", "pdf": "/pdf/be7f39474ec772cfc66afbd22a4ecb050e893ef8.pdf", "paperhash": "levine|certifiably_robust_interpretation_in_deep_learning", "original_pdf": "/attachment/4718fa24063c9b72347882d05e7d1c45c996a769.pdf", "_bibtex": "@misc{\nlevine2020certifiably,\ntitle={Certifiably Robust Interpretation in Deep Learning},\nauthor={Alexander Levine and Sahil Singla and Soheil Feizi},\nyear={2020},\nurl={https://openreview.net/forum?id=rkxVz1HKwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rkxVz1HKwB", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504192783, "tmdate": 1576860587744, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper1576/Authors", "ICLR.cc/2020/Conference/Paper1576/Reviewers", "ICLR.cc/2020/Conference/Paper1576/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1576/-/Public_Comment"}}}, {"id": "H1e5BxVC5r", "original": null, "number": 2, "cdate": 1572909121741, "ddate": null, "tcdate": 1572909121741, "tmdate": 1572909121741, "tddate": null, "forum": "rkxVz1HKwB", "replyto": "HkgezgVA5H", "invitation": "ICLR.cc/2020/Conference/Paper1576/-/Public_Comment", "content": {"title": "method and empirical evaluation", "comment": "The paper claims that Scaled smoothgrad and Quadratic smoothgrad give vacuous bounds. So, they develop a new method \u201cSparsified Smoothgrad\u201d. And to qualitatively illlustrate this method, one example is provided on imagenet (the kind of datasets people really care about, nobody cares about MNIST) (Figure 1). With one example, how do we know if this newly developed method really performs well. Please see papers like Gradcam, where they show several examples to illustrate the interpretation methods.\n\n\u201cWe test Relaxed Sparsified SmoothGrad (\u03b3 = 0.01, \u03c4 = 0.1), rather than Sparsified SmoothGrad because our attack is gradient-based and Sparsified SmoothGrad has no defined gradients\u201d. If methods don\u2019t have explicit gradients, there are several attack strategies. Refer \u201cObfuscated gradients give a false sense of security\u201d paper for more details.\n\n\u201cWe tested on ResNet-18 with CIFAR-10 with the attacker using a separately-trained, fully differentiable version of ResNet-18, with SoftPlus activations in place of ReLU\u201d. Why can\u2019t you use ReLU networks? Softplus networks are horrible and give really poor performance when trained. So obviously, the attacks are going to be sub-optimal when these models are used for creating attacks. And is there a reason why transfer attacks are used and not white-box attacks?\n\nHonestly, its hard to understand the experimental evaluation. Lot of notations. Empirical section is dense, and hard to parse. In Figure 2, what is the robustness certificate in y-axis? Is it the rank certificate? \u201cThe lines shown are for the 60th percentile guarantee, meaning that 60 percent of images had guarantees at least as tight as those shown\u201d. What do you mean tight as those shown?\n\nFigure 2 is shown for the case K=0.2n, which is 20% of the entire image. Now, 20% is a big fraction of the image. Consider this case: An image has a small component contributing to a prediction, say 1% - this is not some example I made up for arguments\u2019 sake. In several medical imaging and vision applications, this happens. Now, only 1% of the image is relevant in making prediction, and let us say gradient based saliency methods correct picked this top-1% overlap i.e., in the saliency map top 1% has high value, and others have very low value. By the bound you show in Figure 2, you guarantee that the prediction stays within 20%. For all you know, the method could highlight some noise, and push the 1% correct prediction to a low value (as the pixels other than 1% had low values in the original saliency map). In this case, the bound becomes useless. All I am saying is K is something that should not be picked before hand. And it is very important to analyze the certification rate as a function of K. 20% is still a very big number, and people really care about what happens for small K.\n\nThe previous paragraph clearly states some issues with rank certificate. May be a better metric to look at is L_p norm between predicted and perturbed saliency maps?\n\nIn my opinion, empirical evaluation is quite weak to access the importance of the approach. Attacks are created with Softplus network which are extremely weak in the first place. Whitebox setting is not considered. It\u2019s hard to say the importance of provided bounds at high values of  K. Very few qualitative results are presented at imagenet scale. Effect of certification as a function of K is not analyzed. MNIST and CIFAR-10 are simple classification tasks with small images, so the results obtained here are not reflective of what happens as the size of images increase to Imagenet or COCO scale. Rank certification by itself can lead to issues, so it\u2019s not clear if this is even the right form of certification to look at. \n"}, "signatures": ["~Rigor_Police1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Rigor_Police1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Certifiably Robust Interpretation in Deep Learning", "authors": ["Alexander Levine", "Sahil Singla", "Soheil Feizi"], "authorids": ["alevine0@cs.umd.edu", "ssingla@cs.umd.edu", "sfeizi@cs.umd.edu"], "keywords": ["deep learning interpretation", "robustness certificates", "adversarial examples"], "TL;DR": "We develop an interpretation procedure for deep learning models which is certifiably robust to adversarial attack.", "abstract": "Deep learning interpretation is essential to explain the reasoning behind model predictions. Understanding the robustness of interpretation methods is important especially in sensitive domains such as medical applications since interpretation results are often used in downstream tasks. Although gradient-based saliency maps are popular methods for deep learning interpretation, recent works show that they can be vulnerable to adversarial attacks. In this paper, we address this problem and provide a certifiable defense method for deep learning interpretation. We show that a sparsified version of the popular SmoothGrad method, which computes the average saliency maps over random perturbations of the input, is certifiably robust against adversarial perturbations. We obtain this result by extending recent bounds for certifiably robust smooth classifiers to the interpretation setting. Experiments on ImageNet samples validate our theory.", "code": "https://github.com/anonICLR5/robust-interpretation", "pdf": "/pdf/be7f39474ec772cfc66afbd22a4ecb050e893ef8.pdf", "paperhash": "levine|certifiably_robust_interpretation_in_deep_learning", "original_pdf": "/attachment/4718fa24063c9b72347882d05e7d1c45c996a769.pdf", "_bibtex": "@misc{\nlevine2020certifiably,\ntitle={Certifiably Robust Interpretation in Deep Learning},\nauthor={Alexander Levine and Sahil Singla and Soheil Feizi},\nyear={2020},\nurl={https://openreview.net/forum?id=rkxVz1HKwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rkxVz1HKwB", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504192783, "tmdate": 1576860587744, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper1576/Authors", "ICLR.cc/2020/Conference/Paper1576/Reviewers", "ICLR.cc/2020/Conference/Paper1576/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1576/-/Public_Comment"}}}, {"id": "HkgezgVA5H", "original": null, "number": 1, "cdate": 1572909063542, "ddate": null, "tcdate": 1572909063542, "tmdate": 1572909063542, "tddate": null, "forum": "rkxVz1HKwB", "replyto": "rkxVz1HKwB", "invitation": "ICLR.cc/2020/Conference/Paper1576/-/Public_Comment", "content": {"title": "motivation", "comment": "Recently, use of \u201cmedical applications\u201d to motivate an idea has become a trend in machine learning. This paper is no different. The paper claims that gradient based saliency maps are used in several medical applications [paragraph 1] and these interpretation methods can be attacked. Really? Do you really believe that if neural network interpretation methods are used by doctors, attackers will gain access to these models that easily? And do you really believe that medical data can be tampered so easily to create adversarial attacks? Please think about plausibility of this attack and the legal implications this would have before making comments like these. \n\nLet\u2019s now say these interpretation methods used were somehow attacked. Do you really think doctors will blindly trust these systems and diagnose the patients? Doctors have had years of experience and these AI systems would merely be used to aid diagnosis. Whenever the system interprets something different, doctors would just ignore the suggestion. Before all this, there are millions of considerations on how AI should be used by doctors. Nobody knows this yet. \n\nAnother application where interpretation can be used is model debugging. But, this happens in development phase and adversarial attacks don\u2019t make any sense here. So, I am not even convinced why addressing the problem of robust interpretation is important in the first place.\n\n\n"}, "signatures": ["~Rigor_Police1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Rigor_Police1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Certifiably Robust Interpretation in Deep Learning", "authors": ["Alexander Levine", "Sahil Singla", "Soheil Feizi"], "authorids": ["alevine0@cs.umd.edu", "ssingla@cs.umd.edu", "sfeizi@cs.umd.edu"], "keywords": ["deep learning interpretation", "robustness certificates", "adversarial examples"], "TL;DR": "We develop an interpretation procedure for deep learning models which is certifiably robust to adversarial attack.", "abstract": "Deep learning interpretation is essential to explain the reasoning behind model predictions. Understanding the robustness of interpretation methods is important especially in sensitive domains such as medical applications since interpretation results are often used in downstream tasks. Although gradient-based saliency maps are popular methods for deep learning interpretation, recent works show that they can be vulnerable to adversarial attacks. In this paper, we address this problem and provide a certifiable defense method for deep learning interpretation. We show that a sparsified version of the popular SmoothGrad method, which computes the average saliency maps over random perturbations of the input, is certifiably robust against adversarial perturbations. We obtain this result by extending recent bounds for certifiably robust smooth classifiers to the interpretation setting. Experiments on ImageNet samples validate our theory.", "code": "https://github.com/anonICLR5/robust-interpretation", "pdf": "/pdf/be7f39474ec772cfc66afbd22a4ecb050e893ef8.pdf", "paperhash": "levine|certifiably_robust_interpretation_in_deep_learning", "original_pdf": "/attachment/4718fa24063c9b72347882d05e7d1c45c996a769.pdf", "_bibtex": "@misc{\nlevine2020certifiably,\ntitle={Certifiably Robust Interpretation in Deep Learning},\nauthor={Alexander Levine and Sahil Singla and Soheil Feizi},\nyear={2020},\nurl={https://openreview.net/forum?id=rkxVz1HKwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rkxVz1HKwB", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504192783, "tmdate": 1576860587744, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper1576/Authors", "ICLR.cc/2020/Conference/Paper1576/Reviewers", "ICLR.cc/2020/Conference/Paper1576/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1576/-/Public_Comment"}}}], "count": 15}