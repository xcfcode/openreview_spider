{"notes": [{"tddate": null, "ddate": null, "tmdate": 1518730188991, "tcdate": 1508976975008, "number": 103, "cdate": 1518730188981, "id": "SJD8YjCpW", "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "forum": "SJD8YjCpW", "original": "rJ8IFjAT-", "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference"], "content": {"title": "Balanced and Deterministic Weight-sharing Helps Network Performance", "abstract": "Weight-sharing plays a significant role in the success of many deep neural networks, by increasing memory efficiency and incorporating useful inductive priors about the problem into the network. But understanding how weight-sharing can be used effectively in general is a topic that has not been studied extensively. Chen et al. (2015) proposed HashedNets, which augments a multi-layer perceptron with a hash table, as a method for neural network compression. We generalize this method into a framework (ArbNets) that allows for efficient arbitrary weight-sharing, and use it to study the role of weight-sharing in neural networks. We show that common neural networks can be expressed as ArbNets with different hash functions. We also present two novel hash functions, the Dirichlet hash and the Neighborhood hash, and use them to demonstrate experimentally that balanced and deterministic weight-sharing helps with the performance of a neural network.", "pdf": "/pdf/34b1c5d539a7372128f2df9faa496f8cda146e3c.pdf", "TL;DR": "Studied the role of weight sharing in neural networks using hash functions, found that a balanced and deterministic hash function helps network performance.", "paperhash": "chang|balanced_and_deterministic_weightsharing_helps_network_performance", "_bibtex": "@misc{\nchang2018balanced,\ntitle={Balanced and Deterministic Weight-sharing Helps Network Performance},\nauthor={Oscar Chang and Hod Lipson},\nyear={2018},\nurl={https://openreview.net/forum?id=SJD8YjCpW},\n}", "keywords": ["Weight-sharing", "Weight sharing", "Weight tying", "neural networks", "entropy", "hash function", "hash table", "balance", "sparse", "sparsity", "hashednets"], "authors": ["Oscar Chang", "Hod Lipson"], "authorids": ["oscar.chang@columbia.edu", "hod.lipson@columbia.edu"]}, "nonreaders": [], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1506717071958, "id": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Conference"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference"]}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"authors": {"required": false, "order": 1, "values-regex": ".*", "description": "Comma separated list of author names, as they appear in the paper."}, "authorids": {"required": false, "order": 2, "values-regex": ".*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "cdate": 1506717071958}}, "tauthor": "ICLR.cc/2018/Conference"}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1517260075285, "tcdate": 1517250254565, "number": 897, "cdate": 1517250254547, "id": "H1vRIJpSf", "invitation": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "forum": "SJD8YjCpW", "replyto": "SJD8YjCpW", "signatures": ["ICLR.cc/2018/Conference/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Program_Chairs"], "content": {"decision": "Reject", "title": "ICLR 2018 Conference Acceptance Decision", "comment": "An empirical study of weight sharing for neural networks is interesting, but all of the reviewers found the experiments insufficient without enough baseline comparisons."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Balanced and Deterministic Weight-sharing Helps Network Performance", "abstract": "Weight-sharing plays a significant role in the success of many deep neural networks, by increasing memory efficiency and incorporating useful inductive priors about the problem into the network. But understanding how weight-sharing can be used effectively in general is a topic that has not been studied extensively. Chen et al. (2015) proposed HashedNets, which augments a multi-layer perceptron with a hash table, as a method for neural network compression. We generalize this method into a framework (ArbNets) that allows for efficient arbitrary weight-sharing, and use it to study the role of weight-sharing in neural networks. We show that common neural networks can be expressed as ArbNets with different hash functions. We also present two novel hash functions, the Dirichlet hash and the Neighborhood hash, and use them to demonstrate experimentally that balanced and deterministic weight-sharing helps with the performance of a neural network.", "pdf": "/pdf/34b1c5d539a7372128f2df9faa496f8cda146e3c.pdf", "TL;DR": "Studied the role of weight sharing in neural networks using hash functions, found that a balanced and deterministic hash function helps network performance.", "paperhash": "chang|balanced_and_deterministic_weightsharing_helps_network_performance", "_bibtex": "@misc{\nchang2018balanced,\ntitle={Balanced and Deterministic Weight-sharing Helps Network Performance},\nauthor={Oscar Chang and Hod Lipson},\nyear={2018},\nurl={https://openreview.net/forum?id=SJD8YjCpW},\n}", "keywords": ["Weight-sharing", "Weight sharing", "Weight tying", "neural networks", "entropy", "hash function", "hash table", "balance", "sparse", "sparsity", "hashednets"], "authors": ["Oscar Chang", "Hod Lipson"], "authorids": ["oscar.chang@columbia.edu", "hod.lipson@columbia.edu"]}, "tags": [], "invitation": {"id": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "rdate": null, "ddate": null, "expdate": 1541175629000, "duedate": null, "tmdate": 1541177635767, "tddate": null, "super": null, "final": null, "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": {"values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Conference/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Conference Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": [], "noninvitees": [], "writers": ["ICLR.cc/2018/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1541177635767}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642378318, "tcdate": 1511772690519, "number": 1, "cdate": 1511772690519, "id": "rJqGz8tlf", "invitation": "ICLR.cc/2018/Conference/-/Paper103/Official_Review", "forum": "SJD8YjCpW", "replyto": "SJD8YjCpW", "signatures": ["ICLR.cc/2018/Conference/Paper103/AnonReviewer3"], "readers": ["everyone"], "content": {"title": "The manuscript contains few insights", "rating": "4: Ok but not good enough - rejection", "review": "The manuscript advocates to study the weight sharing in a more systematic way by proposing ArbNets which defines the weight sharing function as a hash function. In this framework, any existing neural network architectures, including CNN and RNN, could be incorporated into ArbNets.\n\nThe manuscript is not well written. There are multiple grammar errors and typos. Content-wise, it is already well known that CNN and RNN can be expressed as general MLP with weight sharing. The introduction of ArbNets does not bring much value or insight to this area. So it seems that most content before experimental section is common sense.\n\nIn the experimental section, it is interesting to see how different hash function with different level of entropy can affect the performance of neural nets. However, this single observation cannot enrich the whole manuscript. Two questions:\n(1) What is the definition of sparsity here, and how is it controlled?\n(2) There seems to be a step change in Figure 3. All the results are either between 10 to 20, or near 50. And the blue line goes up and down. Is this expected?", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Balanced and Deterministic Weight-sharing Helps Network Performance", "abstract": "Weight-sharing plays a significant role in the success of many deep neural networks, by increasing memory efficiency and incorporating useful inductive priors about the problem into the network. But understanding how weight-sharing can be used effectively in general is a topic that has not been studied extensively. Chen et al. (2015) proposed HashedNets, which augments a multi-layer perceptron with a hash table, as a method for neural network compression. We generalize this method into a framework (ArbNets) that allows for efficient arbitrary weight-sharing, and use it to study the role of weight-sharing in neural networks. We show that common neural networks can be expressed as ArbNets with different hash functions. We also present two novel hash functions, the Dirichlet hash and the Neighborhood hash, and use them to demonstrate experimentally that balanced and deterministic weight-sharing helps with the performance of a neural network.", "pdf": "/pdf/34b1c5d539a7372128f2df9faa496f8cda146e3c.pdf", "TL;DR": "Studied the role of weight sharing in neural networks using hash functions, found that a balanced and deterministic hash function helps network performance.", "paperhash": "chang|balanced_and_deterministic_weightsharing_helps_network_performance", "_bibtex": "@misc{\nchang2018balanced,\ntitle={Balanced and Deterministic Weight-sharing Helps Network Performance},\nauthor={Oscar Chang and Hod Lipson},\nyear={2018},\nurl={https://openreview.net/forum?id=SJD8YjCpW},\n}", "keywords": ["Weight-sharing", "Weight sharing", "Weight tying", "neural networks", "entropy", "hash function", "hash table", "balance", "sparse", "sparsity", "hashednets"], "authors": ["Oscar Chang", "Hod Lipson"], "authorids": ["oscar.chang@columbia.edu", "hod.lipson@columbia.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642378220, "id": "ICLR.cc/2018/Conference/-/Paper103/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper103/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper103/AnonReviewer3", "ICLR.cc/2018/Conference/Paper103/AnonReviewer1", "ICLR.cc/2018/Conference/Paper103/AnonReviewer4"], "reply": {"forum": "SJD8YjCpW", "replyto": "SJD8YjCpW", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper103/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642378220}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642378278, "tcdate": 1511816888696, "number": 2, "cdate": 1511816888696, "id": "rybTRlqgz", "invitation": "ICLR.cc/2018/Conference/-/Paper103/Official_Review", "forum": "SJD8YjCpW", "replyto": "SJD8YjCpW", "signatures": ["ICLR.cc/2018/Conference/Paper103/AnonReviewer1"], "readers": ["everyone"], "content": {"title": "A framework for studying weight sharing", "rating": "4: Ok but not good enough - rejection", "review": "This paper proposes a general framework for studying weight sharing in neural networks. They further suggest two hash functions and study the role of different properties of these hash functions in the performance.\n\nThe paper is well-written and clear. It is a follow-up on Chen et al. (2015) which introduced HashedNets. Therefore, the idea of using hash functions is not novel. This paper suggests a framework to study different hash functions. However, the experimental results do not seem adequate to validate this framework. One issue here is lack of a baseline for performance comparison. Otherwise, the significance of the results is not clear.\n\n\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Balanced and Deterministic Weight-sharing Helps Network Performance", "abstract": "Weight-sharing plays a significant role in the success of many deep neural networks, by increasing memory efficiency and incorporating useful inductive priors about the problem into the network. But understanding how weight-sharing can be used effectively in general is a topic that has not been studied extensively. Chen et al. (2015) proposed HashedNets, which augments a multi-layer perceptron with a hash table, as a method for neural network compression. We generalize this method into a framework (ArbNets) that allows for efficient arbitrary weight-sharing, and use it to study the role of weight-sharing in neural networks. We show that common neural networks can be expressed as ArbNets with different hash functions. We also present two novel hash functions, the Dirichlet hash and the Neighborhood hash, and use them to demonstrate experimentally that balanced and deterministic weight-sharing helps with the performance of a neural network.", "pdf": "/pdf/34b1c5d539a7372128f2df9faa496f8cda146e3c.pdf", "TL;DR": "Studied the role of weight sharing in neural networks using hash functions, found that a balanced and deterministic hash function helps network performance.", "paperhash": "chang|balanced_and_deterministic_weightsharing_helps_network_performance", "_bibtex": "@misc{\nchang2018balanced,\ntitle={Balanced and Deterministic Weight-sharing Helps Network Performance},\nauthor={Oscar Chang and Hod Lipson},\nyear={2018},\nurl={https://openreview.net/forum?id=SJD8YjCpW},\n}", "keywords": ["Weight-sharing", "Weight sharing", "Weight tying", "neural networks", "entropy", "hash function", "hash table", "balance", "sparse", "sparsity", "hashednets"], "authors": ["Oscar Chang", "Hod Lipson"], "authorids": ["oscar.chang@columbia.edu", "hod.lipson@columbia.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642378220, "id": "ICLR.cc/2018/Conference/-/Paper103/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper103/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper103/AnonReviewer3", "ICLR.cc/2018/Conference/Paper103/AnonReviewer1", "ICLR.cc/2018/Conference/Paper103/AnonReviewer4"], "reply": {"forum": "SJD8YjCpW", "replyto": "SJD8YjCpW", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper103/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642378220}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642378239, "tcdate": 1513373946726, "number": 3, "cdate": 1513373946726, "id": "BkmW-pbMG", "invitation": "ICLR.cc/2018/Conference/-/Paper103/Official_Review", "forum": "SJD8YjCpW", "replyto": "SJD8YjCpW", "signatures": ["ICLR.cc/2018/Conference/Paper103/AnonReviewer4"], "readers": ["everyone"], "content": {"title": "limited novelty", "rating": "4: Ok but not good enough - rejection", "review": "This paper has limited novelty, the ideas has been previously proposed in HashedNet and Deep Compression. The experimental section is week, with only mnist and cifar results it's not convincing to the community whether this method is general. ", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Balanced and Deterministic Weight-sharing Helps Network Performance", "abstract": "Weight-sharing plays a significant role in the success of many deep neural networks, by increasing memory efficiency and incorporating useful inductive priors about the problem into the network. But understanding how weight-sharing can be used effectively in general is a topic that has not been studied extensively. Chen et al. (2015) proposed HashedNets, which augments a multi-layer perceptron with a hash table, as a method for neural network compression. We generalize this method into a framework (ArbNets) that allows for efficient arbitrary weight-sharing, and use it to study the role of weight-sharing in neural networks. We show that common neural networks can be expressed as ArbNets with different hash functions. We also present two novel hash functions, the Dirichlet hash and the Neighborhood hash, and use them to demonstrate experimentally that balanced and deterministic weight-sharing helps with the performance of a neural network.", "pdf": "/pdf/34b1c5d539a7372128f2df9faa496f8cda146e3c.pdf", "TL;DR": "Studied the role of weight sharing in neural networks using hash functions, found that a balanced and deterministic hash function helps network performance.", "paperhash": "chang|balanced_and_deterministic_weightsharing_helps_network_performance", "_bibtex": "@misc{\nchang2018balanced,\ntitle={Balanced and Deterministic Weight-sharing Helps Network Performance},\nauthor={Oscar Chang and Hod Lipson},\nyear={2018},\nurl={https://openreview.net/forum?id=SJD8YjCpW},\n}", "keywords": ["Weight-sharing", "Weight sharing", "Weight tying", "neural networks", "entropy", "hash function", "hash table", "balance", "sparse", "sparsity", "hashednets"], "authors": ["Oscar Chang", "Hod Lipson"], "authorids": ["oscar.chang@columbia.edu", "hod.lipson@columbia.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642378220, "id": "ICLR.cc/2018/Conference/-/Paper103/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper103/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper103/AnonReviewer3", "ICLR.cc/2018/Conference/Paper103/AnonReviewer1", "ICLR.cc/2018/Conference/Paper103/AnonReviewer4"], "reply": {"forum": "SJD8YjCpW", "replyto": "SJD8YjCpW", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper103/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642378220}}}], "count": 5}