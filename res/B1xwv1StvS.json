{"notes": [{"id": "B1xwv1StvS", "original": "S1ek5ha_wH", "number": 1771, "cdate": 1569439583511, "ddate": null, "tcdate": 1569439583511, "tmdate": 1577168232028, "tddate": null, "forum": "B1xwv1StvS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["maulana@comp.nus.edu.sg", "leews@comp.nus.edu.sg"], "title": "Few-shot Learning by Focusing on Differences", "authors": ["Muhammad Rizki Maulana", "Lee Wee Sun"], "pdf": "/pdf/94f69b914c6104758239cf83f50b3f318bab188a.pdf", "TL;DR": "We propose a model for few-shot classification that incorporates explicit prior which construct class representatives that are orthogonal to the local average of closely related class representatives.", "abstract": "Few-shot classification may involve differentiating data that belongs to a different level of labels granularity. Compounded by the fact that the number of available labeled examples are scarce in the novel classification set, relying solely on the loss function to implicitly guide the classifier to separate data based on its label might not be enough; few-shot classifier needs to be very biased to perform well. In this paper, we propose a model that incorporates a simple prior: focusing on differences by building a dissimilar set of class representations. The model treats a class representation as a vector and removes its component that is shared among closely related class representatives. It does so through the combination of learned attention and vector orthogonalization. Our model works well on our newly introduced dataset, Hierarchical-CIFAR, that contains different level of labels granularity. It also substantially improved the performance on fine-grained classification dataset, CUB; whereas staying competitive on standard benchmarks such as mini-Imagenet, Omniglot, and few-shot dataset derived from CIFAR.", "keywords": ["Deep learning", "few-shot learning"], "paperhash": "maulana|fewshot_learning_by_focusing_on_differences", "original_pdf": "/attachment/5ea7c3c9148a08a4873bf907926664eee2ba6b02.pdf", "_bibtex": "@misc{\nmaulana2020fewshot,\ntitle={Few-shot Learning by Focusing on Differences},\nauthor={Muhammad Rizki Maulana and Lee Wee Sun},\nyear={2020},\nurl={https://openreview.net/forum?id=B1xwv1StvS}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "c8lmtADCE", "original": null, "number": 1, "cdate": 1576798732082, "ddate": null, "tcdate": 1576798732082, "tmdate": 1576800904355, "tddate": null, "forum": "B1xwv1StvS", "replyto": "B1xwv1StvS", "invitation": "ICLR.cc/2020/Conference/Paper1771/-/Decision", "content": {"decision": "Reject", "comment": "Main content:\n\n[Blind review #3] The authors propose a metric based model for few-shot learning. The goal of the proposed technique is to incorporate a prior that highlight better the dissimilarity between closely related class prototype. Thus, the proposed paper is related to prototypical neural network (use of prototype to represent a class) but differ from it by using inner product scoring  as a similarity measure instead of the use of euclidean distance. There is also close similarity between the proposed method and matching network.\n\n[Blind review #2] The stated contributions of the paper are: (1) a method for performing few-shot learning and (2) an approach for building harder few-shot learning datasets from existing datasets. The authors describe a model for creating a task-aware embedding for different novel sets (for different image classification settings) using a nonlinear self-attention-like mechanism applied to the centroid of the global embeddings for each class. The resulting embeddings are used per class with an additional attention layer applied on the embeddings from the other classes to identify closely-related classes and consider the part of the embedding orthogonal to the attention-weighted-average of these closely-related classes. They compare the accuracy of their model vs others in the 1-shot and 5-shot setting on various datasets, including a derived dataset from CIFAR which they call Hierarchical-CIFAR.\n\n--\n\nDiscussion:\n\nAll reviews agree on a weak reject.\n\n--\n\nRecommendation and justification:\n\nWhile the ideas appear to be on a good track, the paper itself is poorly written - as one review put it, more like notes to themselves, rather than a well-written document to the ICLR audience.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["maulana@comp.nus.edu.sg", "leews@comp.nus.edu.sg"], "title": "Few-shot Learning by Focusing on Differences", "authors": ["Muhammad Rizki Maulana", "Lee Wee Sun"], "pdf": "/pdf/94f69b914c6104758239cf83f50b3f318bab188a.pdf", "TL;DR": "We propose a model for few-shot classification that incorporates explicit prior which construct class representatives that are orthogonal to the local average of closely related class representatives.", "abstract": "Few-shot classification may involve differentiating data that belongs to a different level of labels granularity. Compounded by the fact that the number of available labeled examples are scarce in the novel classification set, relying solely on the loss function to implicitly guide the classifier to separate data based on its label might not be enough; few-shot classifier needs to be very biased to perform well. In this paper, we propose a model that incorporates a simple prior: focusing on differences by building a dissimilar set of class representations. The model treats a class representation as a vector and removes its component that is shared among closely related class representatives. It does so through the combination of learned attention and vector orthogonalization. Our model works well on our newly introduced dataset, Hierarchical-CIFAR, that contains different level of labels granularity. It also substantially improved the performance on fine-grained classification dataset, CUB; whereas staying competitive on standard benchmarks such as mini-Imagenet, Omniglot, and few-shot dataset derived from CIFAR.", "keywords": ["Deep learning", "few-shot learning"], "paperhash": "maulana|fewshot_learning_by_focusing_on_differences", "original_pdf": "/attachment/5ea7c3c9148a08a4873bf907926664eee2ba6b02.pdf", "_bibtex": "@misc{\nmaulana2020fewshot,\ntitle={Few-shot Learning by Focusing on Differences},\nauthor={Muhammad Rizki Maulana and Lee Wee Sun},\nyear={2020},\nurl={https://openreview.net/forum?id=B1xwv1StvS}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "B1xwv1StvS", "replyto": "B1xwv1StvS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795706409, "tmdate": 1576800254457, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1771/-/Decision"}}}, {"id": "SylXjPS3ir", "original": null, "number": 4, "cdate": 1573832603044, "ddate": null, "tcdate": 1573832603044, "tmdate": 1573838200942, "tddate": null, "forum": "B1xwv1StvS", "replyto": "ryx1J6kTFB", "invitation": "ICLR.cc/2020/Conference/Paper1771/-/Official_Comment", "content": {"title": "Clarification for Official Blind Review #3", "comment": "Thanks for reviewing our paper.\n\n\nSimilarity with Prototypical Networks and Matching Network\n---\nOur proposed method is similar to the prototypical networks (Snell et al., 2017) -- and subsequently Mensink et al. (2013) -- in its use of mean representation of class (or prototypes). The similarity stops there, as the prototypical networks directly perform classification by comparing the distance of the new input to each prototype. They assume that the embedding function that produces the prototypes can sufficiently capture useful and general enough representation that is transferable to the novel set; it only computes global task embedding. As shown in our results, their assumption breaks down when there are changes in class granularity or the label is of fine granularity. In contrast, our method does not directly classify on the prototypes; instead, it transforms the prototypes, producing task-aware embeddings that are locally orthogonal to the shared components belonging to different classes. Thus, classification is performed by computing a Softmax over dot product between the new point and the task-aware prototypes.\n\t\nThe Dissimilarity Network uses context embedding similar to the full context embedding (FCE) extension of the matching network (Vinyals et al., 2016). However, there are some glaring differences in how they operate. The matching network carries the entire support for the prediction. It predicts the label of an unknown point by computing the linear combination of the label of its support set; as the support set grows, the memory increases linearly with it. Their full context embedding conditions the prediction on the entire support set; computing the task-aware embedding is quadratic in the number of elements of the support set. Moreover, as they do not construct an explicit reference for the classes to condition into, it is less clear how reasonable separation of a point belonging to different classes can be maximized. Our model only maintains a set of prototypes and classifies a new point based on how orthogonal its representation to the prototypes. Since we condition the prediction only on the prototypes, it will not grow with the size of the support set. Moreover, computing the task-aware embedding is only quadratic in the number of prototypes (i.e., labels) -- as opposed to the number of support set. We also explicitly computes the representations to be dissimilar; lifting the reliance on learning sufficiently separable inter-class representations only to the loss function.\n\nWe have also updated our related works to give more focus on the metric and similarity learning methods.\n\n\nNovelty, Mathematical Inaccuracies, & Clarity issues\n---\nWe have updated the paper to better highlight the novelty of our work, as well as incorporating the comments given.\n\n\nComparison Fairness\n---\nFour-layer convolution backbone (Conv-4) with an input size of 84x84 is used as a feature extractor for all methods for a fair comparison. Apart from that, our method is parameterized by the self-attention with 2 layers BLSTM for query, key, and value. The rest of the methods follow exactly from their description in their respective papers. For example, RelationNet is parameterized by an additional 2 layers of convolution networks and 2 fully-connected layers.\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1771/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1771/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["maulana@comp.nus.edu.sg", "leews@comp.nus.edu.sg"], "title": "Few-shot Learning by Focusing on Differences", "authors": ["Muhammad Rizki Maulana", "Lee Wee Sun"], "pdf": "/pdf/94f69b914c6104758239cf83f50b3f318bab188a.pdf", "TL;DR": "We propose a model for few-shot classification that incorporates explicit prior which construct class representatives that are orthogonal to the local average of closely related class representatives.", "abstract": "Few-shot classification may involve differentiating data that belongs to a different level of labels granularity. Compounded by the fact that the number of available labeled examples are scarce in the novel classification set, relying solely on the loss function to implicitly guide the classifier to separate data based on its label might not be enough; few-shot classifier needs to be very biased to perform well. In this paper, we propose a model that incorporates a simple prior: focusing on differences by building a dissimilar set of class representations. The model treats a class representation as a vector and removes its component that is shared among closely related class representatives. It does so through the combination of learned attention and vector orthogonalization. Our model works well on our newly introduced dataset, Hierarchical-CIFAR, that contains different level of labels granularity. It also substantially improved the performance on fine-grained classification dataset, CUB; whereas staying competitive on standard benchmarks such as mini-Imagenet, Omniglot, and few-shot dataset derived from CIFAR.", "keywords": ["Deep learning", "few-shot learning"], "paperhash": "maulana|fewshot_learning_by_focusing_on_differences", "original_pdf": "/attachment/5ea7c3c9148a08a4873bf907926664eee2ba6b02.pdf", "_bibtex": "@misc{\nmaulana2020fewshot,\ntitle={Few-shot Learning by Focusing on Differences},\nauthor={Muhammad Rizki Maulana and Lee Wee Sun},\nyear={2020},\nurl={https://openreview.net/forum?id=B1xwv1StvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1xwv1StvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1771/Authors", "ICLR.cc/2020/Conference/Paper1771/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1771/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1771/Reviewers", "ICLR.cc/2020/Conference/Paper1771/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1771/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1771/Authors|ICLR.cc/2020/Conference/Paper1771/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504151123, "tmdate": 1576860554394, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1771/Authors", "ICLR.cc/2020/Conference/Paper1771/Reviewers", "ICLR.cc/2020/Conference/Paper1771/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1771/-/Official_Comment"}}}, {"id": "SJgTwPrnjB", "original": null, "number": 3, "cdate": 1573832549266, "ddate": null, "tcdate": 1573832549266, "tmdate": 1573833230032, "tddate": null, "forum": "B1xwv1StvS", "replyto": "S1esrDQ0YS", "invitation": "ICLR.cc/2020/Conference/Paper1771/-/Official_Comment", "content": {"title": "Clarification for Official Blind Review #1", "comment": "Thanks for taking the time to review our paper.\n\n\nExplanation about Components\n---\nWe have updated the manuscript to include more explanation about the methods that we rely upon.\n\n\nTerms & Similarity Learning\n---\nWe have updated the manuscript to use more appropriate terms: inductive bias and similarity learning. \n\nOur method adopts the approach of similarity learning. Instead of learning a distance or similarity function, we learn a space (embedding) that works well with a fixed similarity-based classifier in that space. Specifically, our model learns to construct a space that is optimized to separate data that belong to different classes for a classifier that uses dot-product as its similarity function.\n\n\nApplication on General Classification Tasks\n---\nIn the usual classification tasks, the labels are fixed during the training and testing. Our approach is advantageous if the labels are changing between task -- as in few-shot classification.\n\n\nDetailed Comments\n---\nWe have updated our manuscript based on the above comments. We also updated the name of our harder variant of CIFAR dataset to CIFAR-Hard to avoid confusion -- as CIFAR is already hierarchically labeled. For point (1), it\u2019s true that our new dataset can be seen through the lens of experiment design. However, it\u2019s also true that the dataset is new, in a sense that the dataset comprises of a new set of image label pairs, with the label derived from the original CIFAR.\n\n\nResults & Discussions\n---\nWe have added more discussion about the results in the manuscript. Please have a look at the updated manuscript. "}, "signatures": ["ICLR.cc/2020/Conference/Paper1771/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1771/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["maulana@comp.nus.edu.sg", "leews@comp.nus.edu.sg"], "title": "Few-shot Learning by Focusing on Differences", "authors": ["Muhammad Rizki Maulana", "Lee Wee Sun"], "pdf": "/pdf/94f69b914c6104758239cf83f50b3f318bab188a.pdf", "TL;DR": "We propose a model for few-shot classification that incorporates explicit prior which construct class representatives that are orthogonal to the local average of closely related class representatives.", "abstract": "Few-shot classification may involve differentiating data that belongs to a different level of labels granularity. Compounded by the fact that the number of available labeled examples are scarce in the novel classification set, relying solely on the loss function to implicitly guide the classifier to separate data based on its label might not be enough; few-shot classifier needs to be very biased to perform well. In this paper, we propose a model that incorporates a simple prior: focusing on differences by building a dissimilar set of class representations. The model treats a class representation as a vector and removes its component that is shared among closely related class representatives. It does so through the combination of learned attention and vector orthogonalization. Our model works well on our newly introduced dataset, Hierarchical-CIFAR, that contains different level of labels granularity. It also substantially improved the performance on fine-grained classification dataset, CUB; whereas staying competitive on standard benchmarks such as mini-Imagenet, Omniglot, and few-shot dataset derived from CIFAR.", "keywords": ["Deep learning", "few-shot learning"], "paperhash": "maulana|fewshot_learning_by_focusing_on_differences", "original_pdf": "/attachment/5ea7c3c9148a08a4873bf907926664eee2ba6b02.pdf", "_bibtex": "@misc{\nmaulana2020fewshot,\ntitle={Few-shot Learning by Focusing on Differences},\nauthor={Muhammad Rizki Maulana and Lee Wee Sun},\nyear={2020},\nurl={https://openreview.net/forum?id=B1xwv1StvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1xwv1StvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1771/Authors", "ICLR.cc/2020/Conference/Paper1771/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1771/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1771/Reviewers", "ICLR.cc/2020/Conference/Paper1771/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1771/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1771/Authors|ICLR.cc/2020/Conference/Paper1771/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504151123, "tmdate": 1576860554394, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1771/Authors", "ICLR.cc/2020/Conference/Paper1771/Reviewers", "ICLR.cc/2020/Conference/Paper1771/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1771/-/Official_Comment"}}}, {"id": "H1eexvS3sH", "original": null, "number": 2, "cdate": 1573832423625, "ddate": null, "tcdate": 1573832423625, "tmdate": 1573832570823, "tddate": null, "forum": "B1xwv1StvS", "replyto": "S1l8TWFRFS", "invitation": "ICLR.cc/2020/Conference/Paper1771/-/Official_Comment", "content": {"title": "Clarification for Official Blind Review #2", "comment": "Thank you very much for your review.\n\n\nSimilarity Learning\n---\nWe have updated our manuscript and present the approach as similarity learning. We learn a function that maps to a space that is optimized for dissimilarity. Since our notion of dissimilarity is based on vector orthogonality, it\u2019s only natural that we use an inner product as a similarity function in such space. \n\n\nPermutation Invariance\n---\nWe are aware of the sequential nature of BLSTM, which can be counter-intuitive as we are modeling a set-to-set operation which shouldn\u2019t have any preference for ordering. However, we found empirically that this setup offers more performance gain compared to the use of traditional linear function as attention embedding function. The BLSTM may learn to ignore the unimportance of set ordering due to the nature of episodic training, which exposes it to many permutations of the possible class-orderings. Moreover, the attention also gives a global context of the member of the set, which could further alleviate the ordering issues (if any).\n\n\nHarder Benchmark\n---\nOur approach requires the dataset that we derived from to have at least two different levels of class granularity. For example, CIFAR dataset which has two levels of labels granularity. ImageNet labels also form a hierarchy which -- through this method -- can be derived into several hard few-shot classification datasets. In the case where different labels of granularity are absent, one may be able to construct new labels by exploring the natural hierarchy which may present.\n\nWe leave out the detail of the construction of the validation set, not the dataset.\n\n\nComparison Fairness\n---\nNo finetuning is performed on CIFAR-H and CIFAR-FS, all the hyperparameters are the one used in the mini-ImageNet dataset. This applies to all methods (including ours).\n\n\nResults & Discussions\n---\nWe have added more discussion about the results in the manuscript. Please have a look at the updated manuscript. \n\n\nMathematical Inaccuracies, Clarity, and Grammatical Errors\n---\nWe have updated the manuscript to address the issues."}, "signatures": ["ICLR.cc/2020/Conference/Paper1771/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1771/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["maulana@comp.nus.edu.sg", "leews@comp.nus.edu.sg"], "title": "Few-shot Learning by Focusing on Differences", "authors": ["Muhammad Rizki Maulana", "Lee Wee Sun"], "pdf": "/pdf/94f69b914c6104758239cf83f50b3f318bab188a.pdf", "TL;DR": "We propose a model for few-shot classification that incorporates explicit prior which construct class representatives that are orthogonal to the local average of closely related class representatives.", "abstract": "Few-shot classification may involve differentiating data that belongs to a different level of labels granularity. Compounded by the fact that the number of available labeled examples are scarce in the novel classification set, relying solely on the loss function to implicitly guide the classifier to separate data based on its label might not be enough; few-shot classifier needs to be very biased to perform well. In this paper, we propose a model that incorporates a simple prior: focusing on differences by building a dissimilar set of class representations. The model treats a class representation as a vector and removes its component that is shared among closely related class representatives. It does so through the combination of learned attention and vector orthogonalization. Our model works well on our newly introduced dataset, Hierarchical-CIFAR, that contains different level of labels granularity. It also substantially improved the performance on fine-grained classification dataset, CUB; whereas staying competitive on standard benchmarks such as mini-Imagenet, Omniglot, and few-shot dataset derived from CIFAR.", "keywords": ["Deep learning", "few-shot learning"], "paperhash": "maulana|fewshot_learning_by_focusing_on_differences", "original_pdf": "/attachment/5ea7c3c9148a08a4873bf907926664eee2ba6b02.pdf", "_bibtex": "@misc{\nmaulana2020fewshot,\ntitle={Few-shot Learning by Focusing on Differences},\nauthor={Muhammad Rizki Maulana and Lee Wee Sun},\nyear={2020},\nurl={https://openreview.net/forum?id=B1xwv1StvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1xwv1StvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1771/Authors", "ICLR.cc/2020/Conference/Paper1771/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1771/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1771/Reviewers", "ICLR.cc/2020/Conference/Paper1771/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1771/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1771/Authors|ICLR.cc/2020/Conference/Paper1771/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504151123, "tmdate": 1576860554394, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1771/Authors", "ICLR.cc/2020/Conference/Paper1771/Reviewers", "ICLR.cc/2020/Conference/Paper1771/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1771/-/Official_Comment"}}}, {"id": "ryx1J6kTFB", "original": null, "number": 1, "cdate": 1571777751444, "ddate": null, "tcdate": 1571777751444, "tmdate": 1572972425663, "tddate": null, "forum": "B1xwv1StvS", "replyto": "B1xwv1StvS", "invitation": "ICLR.cc/2020/Conference/Paper1771/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "In this paper the authors propose a metric based model for few-shot learning. The goal of the proposed technique is to incorporate a prior that highlight better the dissimilarity between closely related class prototype. Thus, the proposed paper is related to prototypical neural network (use of prototype to represent a class) but differ from it by using inner product scoring  as a similarity measure instead of the use of euclidean distance. There is also close similarity between the proposed method and matching network \n\n\noverall, the paper does not  highlight the novelty of their proposed method especially prototypical network and matching network. Thus, the related work session is so general and does not tackle the close models in details. The experiments do not provide convincing evidence of the correctness of the proposed approach.\n\nSeveral parts are unclear/incomprehensible:\n(1) The Introduction is confusing and does not demonstrate the problem that the paper is trying to solve. Specifically, the described intuition (Mill\u2019s method of difference) is not convincing \n(2) the first sentence of  the section \u201cOur work.\u201d  (page 1) is long and unclear \u2026  \u201cIn this paper, we propose a model that focuses on the differences in the support set of closely related classes in assigning the class label to a new instance in the novel task.\u201d\n(3) the use of the two level of embedding is confusing and not clear. Figure 1 is also confusion and not clear.\n\nthe correctness of the proposed approach is not proved by the conducted experiment and does not provide convincing and fair comparison with SoA techniques:\n(1) The experiments do not provide the details of the used architecture compared to your baseline.  (how many layers are used in both embedding systems)\n(2) In Table 1 you are using the results reported by Chen et al. (2019) did you use his framework (Resnet or 4 layers CNN)\n\n**Minor comments** \nThe definition of the embedding function f = (f_g o f_f)  (in line 1 page 5) is not consistent with the domain of each function  f_g is defined on R^H x R^H.\n\nK is not defined (last line page 3)\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1771/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1771/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["maulana@comp.nus.edu.sg", "leews@comp.nus.edu.sg"], "title": "Few-shot Learning by Focusing on Differences", "authors": ["Muhammad Rizki Maulana", "Lee Wee Sun"], "pdf": "/pdf/94f69b914c6104758239cf83f50b3f318bab188a.pdf", "TL;DR": "We propose a model for few-shot classification that incorporates explicit prior which construct class representatives that are orthogonal to the local average of closely related class representatives.", "abstract": "Few-shot classification may involve differentiating data that belongs to a different level of labels granularity. Compounded by the fact that the number of available labeled examples are scarce in the novel classification set, relying solely on the loss function to implicitly guide the classifier to separate data based on its label might not be enough; few-shot classifier needs to be very biased to perform well. In this paper, we propose a model that incorporates a simple prior: focusing on differences by building a dissimilar set of class representations. The model treats a class representation as a vector and removes its component that is shared among closely related class representatives. It does so through the combination of learned attention and vector orthogonalization. Our model works well on our newly introduced dataset, Hierarchical-CIFAR, that contains different level of labels granularity. It also substantially improved the performance on fine-grained classification dataset, CUB; whereas staying competitive on standard benchmarks such as mini-Imagenet, Omniglot, and few-shot dataset derived from CIFAR.", "keywords": ["Deep learning", "few-shot learning"], "paperhash": "maulana|fewshot_learning_by_focusing_on_differences", "original_pdf": "/attachment/5ea7c3c9148a08a4873bf907926664eee2ba6b02.pdf", "_bibtex": "@misc{\nmaulana2020fewshot,\ntitle={Few-shot Learning by Focusing on Differences},\nauthor={Muhammad Rizki Maulana and Lee Wee Sun},\nyear={2020},\nurl={https://openreview.net/forum?id=B1xwv1StvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "B1xwv1StvS", "replyto": "B1xwv1StvS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1771/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1771/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575896838108, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1771/Reviewers"], "noninvitees": [], "tcdate": 1570237732525, "tmdate": 1575896838123, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1771/-/Official_Review"}}}, {"id": "S1esrDQ0YS", "original": null, "number": 2, "cdate": 1571858243508, "ddate": null, "tcdate": 1571858243508, "tmdate": 1572972425617, "tddate": null, "forum": "B1xwv1StvS", "replyto": "B1xwv1StvS", "invitation": "ICLR.cc/2020/Conference/Paper1771/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The authors propose a new neural network model, called as Dissimilarity Network, to improve the few-shot learning accuracy.\nOverall the idea is well motivated that by emphasizing the difference among classes, the model can achieve more accurate predictions for classes where only limited data points are available for training.\nHowever, the paper is not quite well written.\nFirstly, much of the work is built upon previous work including attention mechanisms, episodic training for few-shot learning. Such components are the core of this work because the attention mechanisms implement the class-awareness, and the episodic training facilitates the LSTM structure. Yet these are not well explained and not much context is provided, thus making the paper hard to follow.\nSecondly, some terms are fairly overloaded, or not clearly defined. For example, the \u201cprior\u201d as mentioned in both the abstract and the introduction doesn\u2019t refer to the commonly interpreted term as in the Bayesian settings, but rather as a hand-waiving term to indicate the model design. Also, the terms, \u201cscore\u201d, \u201cmetric\u201d, \u201cdissimilarity\u201d are mentioned in the paper but the paper is not really learning the metric, to my understanding. Thus the details of the paper is quite hard to grasp. \nLastly, the idea of designing the global embedding and the task aware embedding is interesting but shouldn\u2019t really be restricted to few-shot learning. It would be interesting to test the idea on general classification tasks, for example in a simple cross validation settings.\nThus I think the paper would be stronger if the above are addressed and it\u2019s not ready for publishing yet in its current form.\n\nBelow are some more detailed comments:\n1)\tIn the abstract, the \u201cnewly introduced dataset H-CIFAR\u201d is not precise to me; my understanding is that the paper proposes such an experiment design for testing how well a classifier can predict the labels with hierarchy. The current writing refers to that the authors comprises a completely new dataset with new labels.\n2)\tIn the last sentence of the second paragraph in Introduction, the question is asked \u201cwhat prior\u201d should be reasonable. Since the authors didn\u2019t really add any priors in a Bayesian settings but rather designed an architecture, I suggest to reword something like \u201chow to explicitly encode hierarchies into the model structure\u201d.\n3)\tIn Section 2.1, some more description for \u201cepisodic training\u201d would be nice: why should it be used? How is it used and why it makes sense in the few-shot learning context?\n4)\tIn Section 2.2, it would be nice to add the mathematical definition of \u201cprototype\u201d.\n5)\tIn Section 2.2.1, it would be nice to define \u201cH\u201d.\n6)\tIn Section 2.2.2, is M required to be fixed given it\u2019s episodic training? Also it would be nice to add more details about the attention mechanism.\n7)\tIn the result section, it would be nice to discuss when the proposed method is doing better than other methods, for example RelationNet, as well as when it\u2019s worse since different datasets show different results.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1771/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1771/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["maulana@comp.nus.edu.sg", "leews@comp.nus.edu.sg"], "title": "Few-shot Learning by Focusing on Differences", "authors": ["Muhammad Rizki Maulana", "Lee Wee Sun"], "pdf": "/pdf/94f69b914c6104758239cf83f50b3f318bab188a.pdf", "TL;DR": "We propose a model for few-shot classification that incorporates explicit prior which construct class representatives that are orthogonal to the local average of closely related class representatives.", "abstract": "Few-shot classification may involve differentiating data that belongs to a different level of labels granularity. Compounded by the fact that the number of available labeled examples are scarce in the novel classification set, relying solely on the loss function to implicitly guide the classifier to separate data based on its label might not be enough; few-shot classifier needs to be very biased to perform well. In this paper, we propose a model that incorporates a simple prior: focusing on differences by building a dissimilar set of class representations. The model treats a class representation as a vector and removes its component that is shared among closely related class representatives. It does so through the combination of learned attention and vector orthogonalization. Our model works well on our newly introduced dataset, Hierarchical-CIFAR, that contains different level of labels granularity. It also substantially improved the performance on fine-grained classification dataset, CUB; whereas staying competitive on standard benchmarks such as mini-Imagenet, Omniglot, and few-shot dataset derived from CIFAR.", "keywords": ["Deep learning", "few-shot learning"], "paperhash": "maulana|fewshot_learning_by_focusing_on_differences", "original_pdf": "/attachment/5ea7c3c9148a08a4873bf907926664eee2ba6b02.pdf", "_bibtex": "@misc{\nmaulana2020fewshot,\ntitle={Few-shot Learning by Focusing on Differences},\nauthor={Muhammad Rizki Maulana and Lee Wee Sun},\nyear={2020},\nurl={https://openreview.net/forum?id=B1xwv1StvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "B1xwv1StvS", "replyto": "B1xwv1StvS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1771/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1771/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575896838108, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1771/Reviewers"], "noninvitees": [], "tcdate": 1570237732525, "tmdate": 1575896838123, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1771/-/Official_Review"}}}, {"id": "S1l8TWFRFS", "original": null, "number": 3, "cdate": 1571881406401, "ddate": null, "tcdate": 1571881406401, "tmdate": 1572972425570, "tddate": null, "forum": "B1xwv1StvS", "replyto": "B1xwv1StvS", "invitation": "ICLR.cc/2020/Conference/Paper1771/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The stated contributions of the paper are: (1) a method for performing few-shot learning and (2) an approach for building harder few-shot learning datasets from existing datasets. The authors describe a model for creating a task-aware embedding for different novel sets (for different image classification settings) using a nonlinear self-attention-like mechanism applied to the centroid of the global embeddings for each class. The resulting embeddings are used per class with an additional attention layer applied on the embeddings from the other classes to identify closely-related classes and consider the part of the embedding orthogonal to the attention-weighted-average of these closely-related classes. They compare the accuracy of their model vs others in the 1-shot and 5-shot setting on various datasets, including a derived dataset from CIFAR which they call Hierarchical-CIFAR.\n\nOverall, while we like the concepts/ideas and the problem is definitely important, we were not enthusiastic about the paper. First, we found the write up to be cryptic, involving very long unclear statements. It read as if the authors were writing for themselves and not for ICLR general audience. Beyond the writing style, we found the paper to have:\n\n* Inadequate description of the model, including mathematical inaccuracies.\n* Inadequate description of Hierarchical-CIFAR, motivation, and evaluation.\n\nDescription of model:\n------------------------------\nThe manuscripts describe the presented approach as metric learning but make no use of a distance function in the different spaces they map to. Instead, the manuscript defines an inner product over embeddings to compute similarities.\n\nThe manuscript describe their \u201cself-attention operation\u201d as a dynamic set-to-set operation. While the usual definition of self-attention is permutation invariant, the definition presented here is not and thus cannot be accurately described as a mapping between sets. Specifically, in the usual presentation of self-attention the only sharing of information between different set elements is during the outer product of the key and query vectors. The use of a BLSTM between \u201cneighboring elements\u201d of the set of prototype vectors violates this assumption and induces a lack of permutation invariance. This makes the method sensitive to permutations of classes, which does not make sense for predicting unordered classes.\n\nIn sections 2.2.3 and 2.2.4 the material is presented twice but slightly differently. For example, the definition of $b_k$ inline before equation 3 differs from equation 6 later in the text.\n\nEquation 7 is incorrect and should not exclude the current class from the denominator.\n\nThe description of how to classify new points after equation 6 is poorly explained. The description of what happens when $h_V$ is a \u201cBLTSM\u201d [sic (should be BLSTM)] is noninformative.\n\nThe manuscript describes two dimension sizes $H$ and $M$ but the definition of $attn$ requires that $H = M$.\n\nDescription of new dataset and evaluations:\n------------------------------------------------------------\nOne of the stated contributions of the manuscript is a methodology to build harder few-shot learning datasets. Section 4.2 is the only place in the text that appears to address this point, but is unclear where either new finer-grained or coarser-grained labels are coming from (new manual annotation or otherwise). The manuscript \u201cleave[s] out the detail of its construction for simplicity\u201d, but it is unclear what is being done here in the first place.\n\nThe manuscript does not detail tuning competing methods on the new dataset and so it is unclear whether it is a fair comparison.\n\nThe manuscript presents evaluations without any discussion of differences in performance between datasets or the 1-shot/5-shot settings. For example, their method is significantly better on CUB on 1-shot but not so much on 5-shot, on the other hand it is not significantly better on 1-shot for H-CIFAR and CIFAR-HS but then becomes better than the rest with 5-shot.\n\nAdditional comments/corrections\n---------------------------------------------\nThere were numerous typos and grammatical errors that were present in the manuscript that did not directly impact this evaluation but should be fixed in the future."}, "signatures": ["ICLR.cc/2020/Conference/Paper1771/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1771/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["maulana@comp.nus.edu.sg", "leews@comp.nus.edu.sg"], "title": "Few-shot Learning by Focusing on Differences", "authors": ["Muhammad Rizki Maulana", "Lee Wee Sun"], "pdf": "/pdf/94f69b914c6104758239cf83f50b3f318bab188a.pdf", "TL;DR": "We propose a model for few-shot classification that incorporates explicit prior which construct class representatives that are orthogonal to the local average of closely related class representatives.", "abstract": "Few-shot classification may involve differentiating data that belongs to a different level of labels granularity. Compounded by the fact that the number of available labeled examples are scarce in the novel classification set, relying solely on the loss function to implicitly guide the classifier to separate data based on its label might not be enough; few-shot classifier needs to be very biased to perform well. In this paper, we propose a model that incorporates a simple prior: focusing on differences by building a dissimilar set of class representations. The model treats a class representation as a vector and removes its component that is shared among closely related class representatives. It does so through the combination of learned attention and vector orthogonalization. Our model works well on our newly introduced dataset, Hierarchical-CIFAR, that contains different level of labels granularity. It also substantially improved the performance on fine-grained classification dataset, CUB; whereas staying competitive on standard benchmarks such as mini-Imagenet, Omniglot, and few-shot dataset derived from CIFAR.", "keywords": ["Deep learning", "few-shot learning"], "paperhash": "maulana|fewshot_learning_by_focusing_on_differences", "original_pdf": "/attachment/5ea7c3c9148a08a4873bf907926664eee2ba6b02.pdf", "_bibtex": "@misc{\nmaulana2020fewshot,\ntitle={Few-shot Learning by Focusing on Differences},\nauthor={Muhammad Rizki Maulana and Lee Wee Sun},\nyear={2020},\nurl={https://openreview.net/forum?id=B1xwv1StvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "B1xwv1StvS", "replyto": "B1xwv1StvS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1771/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1771/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575896838108, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1771/Reviewers"], "noninvitees": [], "tcdate": 1570237732525, "tmdate": 1575896838123, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1771/-/Official_Review"}}}], "count": 8}