{"notes": [{"id": "rsf1z-JSj87", "original": "GxKJg2I0Ax5", "number": 3246, "cdate": 1601308360625, "ddate": null, "tcdate": 1601308360625, "tmdate": 1615981404385, "tddate": null, "forum": "rsf1z-JSj87", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "End-to-end Adversarial Text-to-Speech", "authorids": ["~Jeff_Donahue1", "~Sander_Dieleman1", "~Mikolaj_Binkowski1", "~Erich_Elsen1", "~Karen_Simonyan1"], "authors": ["Jeff Donahue", "Sander Dieleman", "Mikolaj Binkowski", "Erich Elsen", "Karen Simonyan"], "keywords": ["text-to-speech", "speech synthesis", "adversarial", "GAN", "end-to-end", "feed-forward", "generative model"], "abstract": "Modern text-to-speech synthesis pipelines typically involve multiple processing stages, each of which is designed or learnt independently from the rest. In this work, we take on the challenging task of learning to synthesise speech from normalised text or phonemes in an end-to-end manner, resulting in models which operate directly on character or phoneme input sequences and produce raw speech audio outputs. Our proposed generator is feed-forward and thus efficient for both training and inference, using a differentiable alignment scheme based on token length prediction. It learns to produce high fidelity audio through a combination of adversarial feedback and prediction losses constraining the generated audio to roughly match the ground truth in terms of its total duration and mel-spectrogram. To allow the model to capture temporal variation in the generated audio, we employ soft dynamic time warping in the spectrogram-based prediction loss. The resulting model achieves a mean opinion score exceeding 4 on a 5 point scale, which is comparable to the state-of-the-art models relying on multi-stage training and additional supervision.", "one-sentence_summary": "Efficient, adversarially trained feed-forward text-to-speech model producing high-quality speech, learnt end-to-end in a single stage.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "donahue|endtoend_adversarial_texttospeech", "pdf": "/pdf/37c14e59635044278b29d57254fbd09950d3d37a.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ndonahue2021endtoend,\ntitle={End-to-end Adversarial Text-to-Speech},\nauthor={Jeff Donahue and Sander Dieleman and Mikolaj Binkowski and Erich Elsen and Karen Simonyan},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=rsf1z-JSj87}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 13, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "GwC8L3Cjbeq", "original": null, "number": 1, "cdate": 1610040474048, "ddate": null, "tcdate": 1610040474048, "tmdate": 1610474078372, "tddate": null, "forum": "rsf1z-JSj87", "replyto": "rsf1z-JSj87", "invitation": "ICLR.cc/2021/Conference/Paper3246/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Oral)", "comment": "This paper investigates a speech synthesis approach that directly generates raw audios from text or phoneme inputs in an end-to-end fashion.  The approach first maps the input texts/phonemes into a representation sequence that is aligned with the output at a lower sampling frequency by a differentiable aligner and then upsamples the representation sequence to the full audio frequency by a decoder.  A number of techniques including adversarial training and soft DTW are applied to improve the training.  The experimental results are good. There are raised concerns from the reviewers which are mostly cleared by the rebuttal of the authors.  After the rebuttal and discussion, all reviewers are supportive on accepting the paper. "}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "End-to-end Adversarial Text-to-Speech", "authorids": ["~Jeff_Donahue1", "~Sander_Dieleman1", "~Mikolaj_Binkowski1", "~Erich_Elsen1", "~Karen_Simonyan1"], "authors": ["Jeff Donahue", "Sander Dieleman", "Mikolaj Binkowski", "Erich Elsen", "Karen Simonyan"], "keywords": ["text-to-speech", "speech synthesis", "adversarial", "GAN", "end-to-end", "feed-forward", "generative model"], "abstract": "Modern text-to-speech synthesis pipelines typically involve multiple processing stages, each of which is designed or learnt independently from the rest. In this work, we take on the challenging task of learning to synthesise speech from normalised text or phonemes in an end-to-end manner, resulting in models which operate directly on character or phoneme input sequences and produce raw speech audio outputs. Our proposed generator is feed-forward and thus efficient for both training and inference, using a differentiable alignment scheme based on token length prediction. It learns to produce high fidelity audio through a combination of adversarial feedback and prediction losses constraining the generated audio to roughly match the ground truth in terms of its total duration and mel-spectrogram. To allow the model to capture temporal variation in the generated audio, we employ soft dynamic time warping in the spectrogram-based prediction loss. The resulting model achieves a mean opinion score exceeding 4 on a 5 point scale, which is comparable to the state-of-the-art models relying on multi-stage training and additional supervision.", "one-sentence_summary": "Efficient, adversarially trained feed-forward text-to-speech model producing high-quality speech, learnt end-to-end in a single stage.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "donahue|endtoend_adversarial_texttospeech", "pdf": "/pdf/37c14e59635044278b29d57254fbd09950d3d37a.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ndonahue2021endtoend,\ntitle={End-to-end Adversarial Text-to-Speech},\nauthor={Jeff Donahue and Sander Dieleman and Mikolaj Binkowski and Erich Elsen and Karen Simonyan},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=rsf1z-JSj87}\n}"}, "tags": [], "invitation": {"reply": {"forum": "rsf1z-JSj87", "replyto": "rsf1z-JSj87", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040474035, "tmdate": 1610474078356, "id": "ICLR.cc/2021/Conference/Paper3246/-/Decision"}}}, {"id": "DZIoaLtr8C8", "original": null, "number": 10, "cdate": 1605804302553, "ddate": null, "tcdate": 1605804302553, "tmdate": 1605804302553, "tddate": null, "forum": "rsf1z-JSj87", "replyto": "tBRNyZXRi6h", "invitation": "ICLR.cc/2021/Conference/Paper3246/-/Official_Comment", "content": {"title": "CPU benchmark", "comment": "Thanks for the interest! We just ran a CPU benchmark; we get 8.5x realtime inference on a 6 core Intel Xeon E5-1650 v4. We've updated Appendix A again. (It's potentially an imperfect benchmark as it's running on a desktop machine sharing the CPU with other background processes etc., but we're not easily able to better isolate this.)"}, "signatures": ["ICLR.cc/2021/Conference/Paper3246/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3246/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "End-to-end Adversarial Text-to-Speech", "authorids": ["~Jeff_Donahue1", "~Sander_Dieleman1", "~Mikolaj_Binkowski1", "~Erich_Elsen1", "~Karen_Simonyan1"], "authors": ["Jeff Donahue", "Sander Dieleman", "Mikolaj Binkowski", "Erich Elsen", "Karen Simonyan"], "keywords": ["text-to-speech", "speech synthesis", "adversarial", "GAN", "end-to-end", "feed-forward", "generative model"], "abstract": "Modern text-to-speech synthesis pipelines typically involve multiple processing stages, each of which is designed or learnt independently from the rest. In this work, we take on the challenging task of learning to synthesise speech from normalised text or phonemes in an end-to-end manner, resulting in models which operate directly on character or phoneme input sequences and produce raw speech audio outputs. Our proposed generator is feed-forward and thus efficient for both training and inference, using a differentiable alignment scheme based on token length prediction. It learns to produce high fidelity audio through a combination of adversarial feedback and prediction losses constraining the generated audio to roughly match the ground truth in terms of its total duration and mel-spectrogram. To allow the model to capture temporal variation in the generated audio, we employ soft dynamic time warping in the spectrogram-based prediction loss. The resulting model achieves a mean opinion score exceeding 4 on a 5 point scale, which is comparable to the state-of-the-art models relying on multi-stage training and additional supervision.", "one-sentence_summary": "Efficient, adversarially trained feed-forward text-to-speech model producing high-quality speech, learnt end-to-end in a single stage.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "donahue|endtoend_adversarial_texttospeech", "pdf": "/pdf/37c14e59635044278b29d57254fbd09950d3d37a.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ndonahue2021endtoend,\ntitle={End-to-end Adversarial Text-to-Speech},\nauthor={Jeff Donahue and Sander Dieleman and Mikolaj Binkowski and Erich Elsen and Karen Simonyan},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=rsf1z-JSj87}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "rsf1z-JSj87", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3246/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3246/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3246/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3246/Authors|ICLR.cc/2021/Conference/Paper3246/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3246/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839534, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3246/-/Official_Comment"}}}, {"id": "tBRNyZXRi6h", "original": null, "number": 9, "cdate": 1605733167032, "ddate": null, "tcdate": 1605733167032, "tmdate": 1605733167032, "tddate": null, "forum": "rsf1z-JSj87", "replyto": "LzCDSOH3QCe", "invitation": "ICLR.cc/2021/Conference/Paper3246/-/Official_Comment", "content": {"title": "CPU benchmark", "comment": "Thanks for the update, good you managed to resolve some issues. Any chance the model can scale to real time on CPU as well? This would be quite impactful."}, "signatures": ["ICLR.cc/2021/Conference/Paper3246/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3246/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "End-to-end Adversarial Text-to-Speech", "authorids": ["~Jeff_Donahue1", "~Sander_Dieleman1", "~Mikolaj_Binkowski1", "~Erich_Elsen1", "~Karen_Simonyan1"], "authors": ["Jeff Donahue", "Sander Dieleman", "Mikolaj Binkowski", "Erich Elsen", "Karen Simonyan"], "keywords": ["text-to-speech", "speech synthesis", "adversarial", "GAN", "end-to-end", "feed-forward", "generative model"], "abstract": "Modern text-to-speech synthesis pipelines typically involve multiple processing stages, each of which is designed or learnt independently from the rest. In this work, we take on the challenging task of learning to synthesise speech from normalised text or phonemes in an end-to-end manner, resulting in models which operate directly on character or phoneme input sequences and produce raw speech audio outputs. Our proposed generator is feed-forward and thus efficient for both training and inference, using a differentiable alignment scheme based on token length prediction. It learns to produce high fidelity audio through a combination of adversarial feedback and prediction losses constraining the generated audio to roughly match the ground truth in terms of its total duration and mel-spectrogram. To allow the model to capture temporal variation in the generated audio, we employ soft dynamic time warping in the spectrogram-based prediction loss. The resulting model achieves a mean opinion score exceeding 4 on a 5 point scale, which is comparable to the state-of-the-art models relying on multi-stage training and additional supervision.", "one-sentence_summary": "Efficient, adversarially trained feed-forward text-to-speech model producing high-quality speech, learnt end-to-end in a single stage.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "donahue|endtoend_adversarial_texttospeech", "pdf": "/pdf/37c14e59635044278b29d57254fbd09950d3d37a.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ndonahue2021endtoend,\ntitle={End-to-end Adversarial Text-to-Speech},\nauthor={Jeff Donahue and Sander Dieleman and Mikolaj Binkowski and Erich Elsen and Karen Simonyan},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=rsf1z-JSj87}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "rsf1z-JSj87", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3246/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3246/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3246/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3246/Authors|ICLR.cc/2021/Conference/Paper3246/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3246/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839534, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3246/-/Official_Comment"}}}, {"id": "LzCDSOH3QCe", "original": null, "number": 8, "cdate": 1605718148236, "ddate": null, "tcdate": 1605718148236, "tmdate": 1605718148236, "tddate": null, "forum": "rsf1z-JSj87", "replyto": "IMEl-MxSn3L", "invitation": "ICLR.cc/2021/Conference/Paper3246/-/Official_Comment", "content": {"title": "Updated inference benchmarks", "comment": "We wanted to let you know we've just updated the manuscript again to update our TPU benchmark and add a GPU benchmark. When improving our benchmarking setup we found there were large IO bottlenecks that made our original TPU benchmark artificially slow, and resolving those bottlenecks improved the measured speed by a factor of more than 5.\n\nIn our updated benchmarks, with TPU v3 the batched inference speed is 157x realtime per chip (79x realtime per core); with a single V100 GPU, batched inference runs at 207x realtime. Please see the updated Appendix A and newly added Table 3 for additional details."}, "signatures": ["ICLR.cc/2021/Conference/Paper3246/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3246/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "End-to-end Adversarial Text-to-Speech", "authorids": ["~Jeff_Donahue1", "~Sander_Dieleman1", "~Mikolaj_Binkowski1", "~Erich_Elsen1", "~Karen_Simonyan1"], "authors": ["Jeff Donahue", "Sander Dieleman", "Mikolaj Binkowski", "Erich Elsen", "Karen Simonyan"], "keywords": ["text-to-speech", "speech synthesis", "adversarial", "GAN", "end-to-end", "feed-forward", "generative model"], "abstract": "Modern text-to-speech synthesis pipelines typically involve multiple processing stages, each of which is designed or learnt independently from the rest. In this work, we take on the challenging task of learning to synthesise speech from normalised text or phonemes in an end-to-end manner, resulting in models which operate directly on character or phoneme input sequences and produce raw speech audio outputs. Our proposed generator is feed-forward and thus efficient for both training and inference, using a differentiable alignment scheme based on token length prediction. It learns to produce high fidelity audio through a combination of adversarial feedback and prediction losses constraining the generated audio to roughly match the ground truth in terms of its total duration and mel-spectrogram. To allow the model to capture temporal variation in the generated audio, we employ soft dynamic time warping in the spectrogram-based prediction loss. The resulting model achieves a mean opinion score exceeding 4 on a 5 point scale, which is comparable to the state-of-the-art models relying on multi-stage training and additional supervision.", "one-sentence_summary": "Efficient, adversarially trained feed-forward text-to-speech model producing high-quality speech, learnt end-to-end in a single stage.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "donahue|endtoend_adversarial_texttospeech", "pdf": "/pdf/37c14e59635044278b29d57254fbd09950d3d37a.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ndonahue2021endtoend,\ntitle={End-to-end Adversarial Text-to-Speech},\nauthor={Jeff Donahue and Sander Dieleman and Mikolaj Binkowski and Erich Elsen and Karen Simonyan},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=rsf1z-JSj87}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "rsf1z-JSj87", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3246/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3246/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3246/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3246/Authors|ICLR.cc/2021/Conference/Paper3246/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3246/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839534, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3246/-/Official_Comment"}}}, {"id": "NW5i-SHulfT", "original": null, "number": 7, "cdate": 1605718084209, "ddate": null, "tcdate": 1605718084209, "tmdate": 1605718084209, "tddate": null, "forum": "rsf1z-JSj87", "replyto": "6qOADSWl3aJ", "invitation": "ICLR.cc/2021/Conference/Paper3246/-/Official_Comment", "content": {"title": "Updated inference benchmarks", "comment": "We wanted to let you know we've just updated the manuscript again to update our TPU benchmark and add a GPU benchmark. When improving our benchmarking setup we found there were large IO bottlenecks that made our original TPU benchmark artificially slow, and resolving those bottlenecks improved the measured speed by a factor of more than 5.\n\nIn our updated benchmarks, with TPU v3 the batched inference speed is 157x realtime per chip (79x realtime per core); with a single V100 GPU, batched inference runs at 207x realtime. Please see the updated Appendix A and newly added Table 3 for additional details."}, "signatures": ["ICLR.cc/2021/Conference/Paper3246/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3246/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "End-to-end Adversarial Text-to-Speech", "authorids": ["~Jeff_Donahue1", "~Sander_Dieleman1", "~Mikolaj_Binkowski1", "~Erich_Elsen1", "~Karen_Simonyan1"], "authors": ["Jeff Donahue", "Sander Dieleman", "Mikolaj Binkowski", "Erich Elsen", "Karen Simonyan"], "keywords": ["text-to-speech", "speech synthesis", "adversarial", "GAN", "end-to-end", "feed-forward", "generative model"], "abstract": "Modern text-to-speech synthesis pipelines typically involve multiple processing stages, each of which is designed or learnt independently from the rest. In this work, we take on the challenging task of learning to synthesise speech from normalised text or phonemes in an end-to-end manner, resulting in models which operate directly on character or phoneme input sequences and produce raw speech audio outputs. Our proposed generator is feed-forward and thus efficient for both training and inference, using a differentiable alignment scheme based on token length prediction. It learns to produce high fidelity audio through a combination of adversarial feedback and prediction losses constraining the generated audio to roughly match the ground truth in terms of its total duration and mel-spectrogram. To allow the model to capture temporal variation in the generated audio, we employ soft dynamic time warping in the spectrogram-based prediction loss. The resulting model achieves a mean opinion score exceeding 4 on a 5 point scale, which is comparable to the state-of-the-art models relying on multi-stage training and additional supervision.", "one-sentence_summary": "Efficient, adversarially trained feed-forward text-to-speech model producing high-quality speech, learnt end-to-end in a single stage.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "donahue|endtoend_adversarial_texttospeech", "pdf": "/pdf/37c14e59635044278b29d57254fbd09950d3d37a.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ndonahue2021endtoend,\ntitle={End-to-end Adversarial Text-to-Speech},\nauthor={Jeff Donahue and Sander Dieleman and Mikolaj Binkowski and Erich Elsen and Karen Simonyan},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=rsf1z-JSj87}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "rsf1z-JSj87", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3246/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3246/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3246/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3246/Authors|ICLR.cc/2021/Conference/Paper3246/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3246/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839534, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3246/-/Official_Comment"}}}, {"id": "IMEl-MxSn3L", "original": null, "number": 6, "cdate": 1605618358001, "ddate": null, "tcdate": 1605618358001, "tmdate": 1605618358001, "tddate": null, "forum": "rsf1z-JSj87", "replyto": "KAiTv-9pk4", "invitation": "ICLR.cc/2021/Conference/Paper3246/-/Official_Comment", "content": {"title": "Thank you for your comments.", "comment": "Thank you for your comments.\n\nWe have added Tacotron 2 to Table 1 as suggested. From our perspective the benefits of EATS relative to Tacotron 2 include the fact that EATS directly yields an efficient feed-forward speech generator (not requiring autoregressive decoding with slow inference or separate distillation to a feed-forward model), in addition to the fact that it's trained in just one stage (rather than two separate pretraining stages to separately learn the characters->mel-spectrograms and mel-spectrograms->audio mappings).\n\nThanks for the reference to SING -- we have updated the part of our related work discussion (Sec. 3) that addresses the L1 spectrogram prediction loss to reference this work.\n\nTransformer attention's poor generalisation to longer utterances is likely due to positional encodings; the model is not trained on enough long utterances to generalise well there. The same observation was made in \"Location-Relative Attention Mechanisms for Robust Long-Form Speech Synthesis\" (Battenberg et al., 2020; Sec 3.4, Fig. 3).\n\nBenchmark: Appendix A in our originally submitted manuscript includes a TPU inference benchmark: batched inference with EATS generates speech at over 15x realtime per TPU v3 core (or over 30x per TPU chip). We will add a GPU benchmark in a later update."}, "signatures": ["ICLR.cc/2021/Conference/Paper3246/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3246/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "End-to-end Adversarial Text-to-Speech", "authorids": ["~Jeff_Donahue1", "~Sander_Dieleman1", "~Mikolaj_Binkowski1", "~Erich_Elsen1", "~Karen_Simonyan1"], "authors": ["Jeff Donahue", "Sander Dieleman", "Mikolaj Binkowski", "Erich Elsen", "Karen Simonyan"], "keywords": ["text-to-speech", "speech synthesis", "adversarial", "GAN", "end-to-end", "feed-forward", "generative model"], "abstract": "Modern text-to-speech synthesis pipelines typically involve multiple processing stages, each of which is designed or learnt independently from the rest. In this work, we take on the challenging task of learning to synthesise speech from normalised text or phonemes in an end-to-end manner, resulting in models which operate directly on character or phoneme input sequences and produce raw speech audio outputs. Our proposed generator is feed-forward and thus efficient for both training and inference, using a differentiable alignment scheme based on token length prediction. It learns to produce high fidelity audio through a combination of adversarial feedback and prediction losses constraining the generated audio to roughly match the ground truth in terms of its total duration and mel-spectrogram. To allow the model to capture temporal variation in the generated audio, we employ soft dynamic time warping in the spectrogram-based prediction loss. The resulting model achieves a mean opinion score exceeding 4 on a 5 point scale, which is comparable to the state-of-the-art models relying on multi-stage training and additional supervision.", "one-sentence_summary": "Efficient, adversarially trained feed-forward text-to-speech model producing high-quality speech, learnt end-to-end in a single stage.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "donahue|endtoend_adversarial_texttospeech", "pdf": "/pdf/37c14e59635044278b29d57254fbd09950d3d37a.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ndonahue2021endtoend,\ntitle={End-to-end Adversarial Text-to-Speech},\nauthor={Jeff Donahue and Sander Dieleman and Mikolaj Binkowski and Erich Elsen and Karen Simonyan},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=rsf1z-JSj87}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "rsf1z-JSj87", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3246/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3246/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3246/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3246/Authors|ICLR.cc/2021/Conference/Paper3246/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3246/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839534, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3246/-/Official_Comment"}}}, {"id": "7h227VS-rB6", "original": null, "number": 5, "cdate": 1605618324898, "ddate": null, "tcdate": 1605618324898, "tmdate": 1605618324898, "tddate": null, "forum": "rsf1z-JSj87", "replyto": "X8I_4qbVlcI", "invitation": "ICLR.cc/2021/Conference/Paper3246/-/Official_Comment", "content": {"title": "Thank you for your comments.", "comment": "Thank you for your comments.\n\nWe acknowledge that the aggregate loss function for EATS isn\u2019t simple by any means, which is a potential downside of the method. However, through our ablation study, we have verified that each of the components in the loss function is actually necessary to achieve an acceptable MOS score of above 4 -- disabling any one single feature reduces the MOS score to 3.8 or lower, as shown in Table 1. Simplifying the loss function and the training procedure is an important goal for future work, as it would make the method more robust and easier to adapt to other datasets, speakers and/or languages.\n\nIt is true that the final MOS we achieve falls short of GAN-TTS and that the main benefit of EATS relative to GAN-TTS is its end-to-end training paradigm. We believe that approaches operating in this paradigm are valuable in their potential to scale to and learn better intermediate representations from larger and more diverse TTS datasets (as we see evidence of when changing from the single speaker to multi-speaker setting), as well as for low-resource languages where language-specific tools (e.g., phonemization tools and hand-crafted language-specific text/speech features) may not be available.\n\nRegarding prosody control, if we had prosody labels available at training time, we could incorporate these into the EATS training setup in various ways. For example, if we had a single prosody label per utterance, we could use these in the same way we input speaker IDs into our speaker-conditional generator via class-conditional batch normalization. If the prosody labels were time-varying, they could be concatenated with the temporally aligned output of our proposed aligner. If you were referring to post-hoc prosody control by e.g. manipulating the latents (or intermediate representations) of a model trained without any explicit prosody inputs, it's not immediately clear how one would do this with EATS' unstructured intermediate representations. Learning a latent space with more structure that could allow for such manipulations would be an interesting future direction."}, "signatures": ["ICLR.cc/2021/Conference/Paper3246/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3246/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "End-to-end Adversarial Text-to-Speech", "authorids": ["~Jeff_Donahue1", "~Sander_Dieleman1", "~Mikolaj_Binkowski1", "~Erich_Elsen1", "~Karen_Simonyan1"], "authors": ["Jeff Donahue", "Sander Dieleman", "Mikolaj Binkowski", "Erich Elsen", "Karen Simonyan"], "keywords": ["text-to-speech", "speech synthesis", "adversarial", "GAN", "end-to-end", "feed-forward", "generative model"], "abstract": "Modern text-to-speech synthesis pipelines typically involve multiple processing stages, each of which is designed or learnt independently from the rest. In this work, we take on the challenging task of learning to synthesise speech from normalised text or phonemes in an end-to-end manner, resulting in models which operate directly on character or phoneme input sequences and produce raw speech audio outputs. Our proposed generator is feed-forward and thus efficient for both training and inference, using a differentiable alignment scheme based on token length prediction. It learns to produce high fidelity audio through a combination of adversarial feedback and prediction losses constraining the generated audio to roughly match the ground truth in terms of its total duration and mel-spectrogram. To allow the model to capture temporal variation in the generated audio, we employ soft dynamic time warping in the spectrogram-based prediction loss. The resulting model achieves a mean opinion score exceeding 4 on a 5 point scale, which is comparable to the state-of-the-art models relying on multi-stage training and additional supervision.", "one-sentence_summary": "Efficient, adversarially trained feed-forward text-to-speech model producing high-quality speech, learnt end-to-end in a single stage.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "donahue|endtoend_adversarial_texttospeech", "pdf": "/pdf/37c14e59635044278b29d57254fbd09950d3d37a.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ndonahue2021endtoend,\ntitle={End-to-end Adversarial Text-to-Speech},\nauthor={Jeff Donahue and Sander Dieleman and Mikolaj Binkowski and Erich Elsen and Karen Simonyan},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=rsf1z-JSj87}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "rsf1z-JSj87", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3246/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3246/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3246/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3246/Authors|ICLR.cc/2021/Conference/Paper3246/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3246/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839534, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3246/-/Official_Comment"}}}, {"id": "6qOADSWl3aJ", "original": null, "number": 4, "cdate": 1605618282355, "ddate": null, "tcdate": 1605618282355, "tmdate": 1605618282355, "tddate": null, "forum": "rsf1z-JSj87", "replyto": "u2k3RT7lCkw", "invitation": "ICLR.cc/2021/Conference/Paper3246/-/Official_Comment", "content": {"title": "Thank you for your comments.", "comment": "Thank you for your comments.\n\nAlthough we do not provide a direct comparison with other methods in terms of computational cost and model size, we do discuss the cost of inference in Appendix A. Although there was no room for this in the main paper, we have moved it there (Sec. 2) in the updated version due to relaxed space constraints. In short, with large enough batch sizes, the model generates speech an order of magnitude (15x) faster than realtime on a single TPU v3 core.\n\nRegarding the lack of phoneme-level alignment in the training data, and how EATS deals with this: we do not select the corresponding text snippet for the two second training window. Instead, the aligner always produces length predictions for the full character/phoneme sequence. We then crop out the interpolated aligner output which corresponds to the training window (which is possible because the output is now approximately aligned with the audio), and feed that to the GAN-TTS generator to produce an audio signal. The dynamic time warping loss helps to account for the fact that the generated audio signal may not be perfectly aligned with the ground truth within the selected window. As we point out at the end of Section 2.5, we still implicitly assume that the start and the end of the training window will be perfectly aligned, which is not true in general. Nevertheless, this doesn\u2019t seem to be much of an issue in practice.\n\nAlthough DTW is a relatively well-known algorithm, it has not seen much use in TTS recently (except in evaluation metrics). Reiterating the algorithm also makes it easier to explain how the \u201csoft\u201d version differs from the original. That said, we will consider trimming down this section, or potentially moving part of it to the appendix, depending on space constraints.\n\n\u2018T\u2019 in Section 2.1 and \u2018T\u2019 in Section 2.4 both refer to the time axis, but as you point out, they have slightly different meanings (due to different resolutions). This could cause confusion. We have updated the manuscript to use a different symbol (S) in Sec. 2.1.\n\nIn Table 1, we disable each component separately (in other words, the ablations are not additive). The cited MOS results of 3.551 (no monotonic interpolation) and 3.559 (no DTW) are for different ablations. For the former, DTW is enabled but monotonic interpolation is disabled; for the latter, it\u2019s the other way around. Hence, the result without monotonic interpolation should be compared to the full EATS result at the bottom of the table (4.083, with everything enabled), indicating a clear loss of performance when disabling the monotonic aligner. This turns out to be because attention-based alignment generalises very poorly to longer utterances (the ablation samples we shared also reflect this). To avoid confusion, we have clarified in the manuscript that the ablations are not additive, but rather subtractive: each ablation represents the full EATS system, minus one particular feature. This demonstrates that all these features are necessary to achieve an MOS score above 4."}, "signatures": ["ICLR.cc/2021/Conference/Paper3246/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3246/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "End-to-end Adversarial Text-to-Speech", "authorids": ["~Jeff_Donahue1", "~Sander_Dieleman1", "~Mikolaj_Binkowski1", "~Erich_Elsen1", "~Karen_Simonyan1"], "authors": ["Jeff Donahue", "Sander Dieleman", "Mikolaj Binkowski", "Erich Elsen", "Karen Simonyan"], "keywords": ["text-to-speech", "speech synthesis", "adversarial", "GAN", "end-to-end", "feed-forward", "generative model"], "abstract": "Modern text-to-speech synthesis pipelines typically involve multiple processing stages, each of which is designed or learnt independently from the rest. In this work, we take on the challenging task of learning to synthesise speech from normalised text or phonemes in an end-to-end manner, resulting in models which operate directly on character or phoneme input sequences and produce raw speech audio outputs. Our proposed generator is feed-forward and thus efficient for both training and inference, using a differentiable alignment scheme based on token length prediction. It learns to produce high fidelity audio through a combination of adversarial feedback and prediction losses constraining the generated audio to roughly match the ground truth in terms of its total duration and mel-spectrogram. To allow the model to capture temporal variation in the generated audio, we employ soft dynamic time warping in the spectrogram-based prediction loss. The resulting model achieves a mean opinion score exceeding 4 on a 5 point scale, which is comparable to the state-of-the-art models relying on multi-stage training and additional supervision.", "one-sentence_summary": "Efficient, adversarially trained feed-forward text-to-speech model producing high-quality speech, learnt end-to-end in a single stage.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "donahue|endtoend_adversarial_texttospeech", "pdf": "/pdf/37c14e59635044278b29d57254fbd09950d3d37a.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ndonahue2021endtoend,\ntitle={End-to-end Adversarial Text-to-Speech},\nauthor={Jeff Donahue and Sander Dieleman and Mikolaj Binkowski and Erich Elsen and Karen Simonyan},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=rsf1z-JSj87}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "rsf1z-JSj87", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3246/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3246/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3246/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3246/Authors|ICLR.cc/2021/Conference/Paper3246/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3246/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839534, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3246/-/Official_Comment"}}}, {"id": "NG05YBWPGgV", "original": null, "number": 3, "cdate": 1605618221075, "ddate": null, "tcdate": 1605618221075, "tmdate": 1605618221075, "tddate": null, "forum": "rsf1z-JSj87", "replyto": "8i0Rsbolbje", "invitation": "ICLR.cc/2021/Conference/Paper3246/-/Official_Comment", "content": {"title": "Thank you for your comments.", "comment": "Thank you for your comments.\n\nThe phrase \u201cend-to-end\u201d has been used to mean a variety of different things in the context of TTS research. To clarify the intended meaning, we have included Table 5 in the appendix, which provides an overview of the different pipeline stages that are used in related work. Our work uses only one such stage, which is feed-forward and trained directly (without requiring e.g. distillation).\n\nFor our best results, we do indeed use a text normalisation and phonemisation step, but nevertheless, our model does a reasonable job directly from characters (apart from the occasional mispronunciation), which is why we feel calling this paradigm \u201cend-to-end\u201d is justified. But we do acknowledge that the use of normalisation and phonemisation for our best results is a limitation, and note it explicitly in the last paragraph of the manuscript (Sec. 5, Discussion). \n\nWe have amended Section 2 (in the text just before 2.1, as suggested) to better motivate our use of an adversarial loss function, as this runs counter to the current prevailing trend of using likelihood-based models for TTS. The use of adversarial discriminators and losses facilitates efficient feed-forward generation and inference. Also, we believe the mode seeking behaviour of adversarial losses, and the resulting focus on realism in favour of diversity, is actually an advantage in the case of TTS, because it enables the model to use its capacity more effectively. (This was already briefly mentioned in Section 3, but should also have been pointed out in Section 2.)\n\nThe subsections of Section 2 are ordered to focus on the architecture and training setup first, and then the different components of the loss function are described. We'll consider reorganising this to improve readability in a later update, e.g. by dividing it explicitly into these two parts.\n\nRegarding English spelling (Section 2.7), our choice of words was perhaps unfortunate, and we have rephrased this. We meant to refer to the fact that many English phonemes can be spelled in a plethora of different ways (e.g. [f] in fish, laugh, half, graph, sapphire), and many graphemes can correspond to multiple different phonemes depending on context (e.g. <ough> in though, tough, through, thorough). Many languages have orthographies that are much more regular than English in this respect (e.g. Spanish, Turkish, Finnish), but admittedly no human language is probably fully regular in this regard.\n\nWe have also updated Section 4.1 of the manuscript to state clearly that a private dataset was used for this work."}, "signatures": ["ICLR.cc/2021/Conference/Paper3246/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3246/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "End-to-end Adversarial Text-to-Speech", "authorids": ["~Jeff_Donahue1", "~Sander_Dieleman1", "~Mikolaj_Binkowski1", "~Erich_Elsen1", "~Karen_Simonyan1"], "authors": ["Jeff Donahue", "Sander Dieleman", "Mikolaj Binkowski", "Erich Elsen", "Karen Simonyan"], "keywords": ["text-to-speech", "speech synthesis", "adversarial", "GAN", "end-to-end", "feed-forward", "generative model"], "abstract": "Modern text-to-speech synthesis pipelines typically involve multiple processing stages, each of which is designed or learnt independently from the rest. In this work, we take on the challenging task of learning to synthesise speech from normalised text or phonemes in an end-to-end manner, resulting in models which operate directly on character or phoneme input sequences and produce raw speech audio outputs. Our proposed generator is feed-forward and thus efficient for both training and inference, using a differentiable alignment scheme based on token length prediction. It learns to produce high fidelity audio through a combination of adversarial feedback and prediction losses constraining the generated audio to roughly match the ground truth in terms of its total duration and mel-spectrogram. To allow the model to capture temporal variation in the generated audio, we employ soft dynamic time warping in the spectrogram-based prediction loss. The resulting model achieves a mean opinion score exceeding 4 on a 5 point scale, which is comparable to the state-of-the-art models relying on multi-stage training and additional supervision.", "one-sentence_summary": "Efficient, adversarially trained feed-forward text-to-speech model producing high-quality speech, learnt end-to-end in a single stage.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "donahue|endtoend_adversarial_texttospeech", "pdf": "/pdf/37c14e59635044278b29d57254fbd09950d3d37a.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ndonahue2021endtoend,\ntitle={End-to-end Adversarial Text-to-Speech},\nauthor={Jeff Donahue and Sander Dieleman and Mikolaj Binkowski and Erich Elsen and Karen Simonyan},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=rsf1z-JSj87}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "rsf1z-JSj87", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3246/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3246/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3246/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3246/Authors|ICLR.cc/2021/Conference/Paper3246/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3246/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839534, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3246/-/Official_Comment"}}}, {"id": "KAiTv-9pk4", "original": null, "number": 1, "cdate": 1603825668720, "ddate": null, "tcdate": 1603825668720, "tmdate": 1605024037964, "tddate": null, "forum": "rsf1z-JSj87", "replyto": "rsf1z-JSj87", "invitation": "ICLR.cc/2021/Conference/Paper3246/-/Official_Review", "content": {"title": "Review of End-to-end adversarial text-to-speech", "review": "\n## Summary\n\nThe authors propose EATS, a method for TTS from unaligned audio and text data, directly to the waveform. Previous work either use aligned phonetic features, or output spectrograms that are later converted to a waveform by a deep vocoder model.\n\nIn order to achieve this, the authors had to use several tricks, some already existing, for instance taken from the GAN TTS architecture, and some novel. The three key novelties are:\n- differentiable monotonic attention with gaussian kernel and length prediction.\n- dynamic time warping for the spectrogram loss.\n- using both spectrogram and waveform domain discriminators\n\nThe authors provide a comprehensive ablation study with MOS score, although their model is under the state of the art by a significant margin.\n\n## Review\n\nThis paper builds on GAN TTS, and tries to make it trainable end-to-end without aligned features.\n\nThe two main contributions, namely dynamic time warping and monotonic attention with gaussian kernel are both elegant, and can likely be used for many other applications related to time series with heterogeneous time scales. In particular, the time warping loss allows to accomodate both for the natural irregularities in spoken speech, as well as providing sufficient signal for the monotonic attention to work.\n\nThe rest of the architecture is very similar to GAN TTS except for the spectrogram domain discriminator that was added.\n\nWhile the model is under the state of the art for TTS, the samples are already quite convincing. The authors conduct a thorough ablation study, both with MOS and audio samples.\n\nOverall I think this is a really good paper, that is likely to prove quite useful for the development of end to end speech synthesis solution. As I already mentioned, I also believe that the approach of using dynamic time warping and monotonic attention can be used for other kind of time series.\n\n\n## Remarks and questions to the authors\n\n- Table 1, MOS for Tacotron 2 would be very informative. All the baselines are trained on aligned data while Tacotron is a legitimate contender for EATS as it can be trained on the same data. The point of the authors is that their methods is simpler because the training is in one stage. However, given the large number of losses and components in their model, with their respective hyper-parameters to tune, I'm not entirely sold on the simplicity argument. The tacotron 2 paper reports a MOS of 4.5 but on a private dataset.\n- Section 3, [1] used the same simple L1 + log spectrogram loss as used here.\n- I was surprised by the bad performance of the transformer attention, in particular in the audio samples, the output for this model is garbage towards the end of the signal. Any clue on why this would happen?\n- It would be interesting to have a benchmark, in particular, can the model generate speech in real time on GPU and on CPU?\n\n[1] SING: Symbol-to-Instrument Neural Generator, Defossez et al. Neurips 2018.", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3246/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3246/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "End-to-end Adversarial Text-to-Speech", "authorids": ["~Jeff_Donahue1", "~Sander_Dieleman1", "~Mikolaj_Binkowski1", "~Erich_Elsen1", "~Karen_Simonyan1"], "authors": ["Jeff Donahue", "Sander Dieleman", "Mikolaj Binkowski", "Erich Elsen", "Karen Simonyan"], "keywords": ["text-to-speech", "speech synthesis", "adversarial", "GAN", "end-to-end", "feed-forward", "generative model"], "abstract": "Modern text-to-speech synthesis pipelines typically involve multiple processing stages, each of which is designed or learnt independently from the rest. In this work, we take on the challenging task of learning to synthesise speech from normalised text or phonemes in an end-to-end manner, resulting in models which operate directly on character or phoneme input sequences and produce raw speech audio outputs. Our proposed generator is feed-forward and thus efficient for both training and inference, using a differentiable alignment scheme based on token length prediction. It learns to produce high fidelity audio through a combination of adversarial feedback and prediction losses constraining the generated audio to roughly match the ground truth in terms of its total duration and mel-spectrogram. To allow the model to capture temporal variation in the generated audio, we employ soft dynamic time warping in the spectrogram-based prediction loss. The resulting model achieves a mean opinion score exceeding 4 on a 5 point scale, which is comparable to the state-of-the-art models relying on multi-stage training and additional supervision.", "one-sentence_summary": "Efficient, adversarially trained feed-forward text-to-speech model producing high-quality speech, learnt end-to-end in a single stage.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "donahue|endtoend_adversarial_texttospeech", "pdf": "/pdf/37c14e59635044278b29d57254fbd09950d3d37a.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ndonahue2021endtoend,\ntitle={End-to-end Adversarial Text-to-Speech},\nauthor={Jeff Donahue and Sander Dieleman and Mikolaj Binkowski and Erich Elsen and Karen Simonyan},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=rsf1z-JSj87}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "rsf1z-JSj87", "replyto": "rsf1z-JSj87", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3246/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538079316, "tmdate": 1606915793117, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3246/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3246/-/Official_Review"}}}, {"id": "X8I_4qbVlcI", "original": null, "number": 2, "cdate": 1603851707612, "ddate": null, "tcdate": 1603851707612, "tmdate": 1605024037872, "tddate": null, "forum": "rsf1z-JSj87", "replyto": "rsf1z-JSj87", "invitation": "ICLR.cc/2021/Conference/Paper3246/-/Official_Review", "content": {"title": "Elegant architectural design but complicated training", "review": "This paper proposes a novel TTS system that 1) relies on almost no intermediate representation; and 2) is entirely feedforward instead of autoregressive. There are two major strengths in the paper. First, several modules in the proposed system are novel and smart, including the aligner and the dynamic programming loss, and it is, to my knowledge, the first feedforward text-to-speech system that does not rely on the intermediate representation. Second, the experiment result is convincing. There are, however, some room for improvement and questions for the author to clarify.\n\nFirst, the elegance in the architecture is overshadowed by the complicated training algorithm. The training loss is like the superposition of common loss terms in the speech synthesis community, making the method look a bit heuristic. The complicated training algorithm also makes the proposed method harder to reproduce. It would be helpful if the authors can provide brief guidelines for readers trying to reimplement EATS, such as how to tune the hyperparameters.\n\nSecond, notice that EATS performs slightly worse than GAN-TTS, which does not quite show the benefit of end-to-end training (unlike ClariNet). I understand that there are a lot of challenges in training EATS, and the authors have briefly discussed this in Section 5. However, it is worthwhile to expand the discussion a bit by showing further experiments that demonstrate the potential benefit of end-to-end training.\n\nFinally, although end-to-end comes with the (potential) merits of improved data efficiency and improved quality, it also has its downsides. Without a clearly interpretable hidden representation, it is harder to have direct control over prosody. How would prosody control be possible under the end-to-end framework?\n\nDespite the weaknesses, this paper makes sufficiently novel contributions in TTS, making it above the acceptance threshold. I would look forward to further justifications of the EATS paradigm.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3246/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3246/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "End-to-end Adversarial Text-to-Speech", "authorids": ["~Jeff_Donahue1", "~Sander_Dieleman1", "~Mikolaj_Binkowski1", "~Erich_Elsen1", "~Karen_Simonyan1"], "authors": ["Jeff Donahue", "Sander Dieleman", "Mikolaj Binkowski", "Erich Elsen", "Karen Simonyan"], "keywords": ["text-to-speech", "speech synthesis", "adversarial", "GAN", "end-to-end", "feed-forward", "generative model"], "abstract": "Modern text-to-speech synthesis pipelines typically involve multiple processing stages, each of which is designed or learnt independently from the rest. In this work, we take on the challenging task of learning to synthesise speech from normalised text or phonemes in an end-to-end manner, resulting in models which operate directly on character or phoneme input sequences and produce raw speech audio outputs. Our proposed generator is feed-forward and thus efficient for both training and inference, using a differentiable alignment scheme based on token length prediction. It learns to produce high fidelity audio through a combination of adversarial feedback and prediction losses constraining the generated audio to roughly match the ground truth in terms of its total duration and mel-spectrogram. To allow the model to capture temporal variation in the generated audio, we employ soft dynamic time warping in the spectrogram-based prediction loss. The resulting model achieves a mean opinion score exceeding 4 on a 5 point scale, which is comparable to the state-of-the-art models relying on multi-stage training and additional supervision.", "one-sentence_summary": "Efficient, adversarially trained feed-forward text-to-speech model producing high-quality speech, learnt end-to-end in a single stage.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "donahue|endtoend_adversarial_texttospeech", "pdf": "/pdf/37c14e59635044278b29d57254fbd09950d3d37a.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ndonahue2021endtoend,\ntitle={End-to-end Adversarial Text-to-Speech},\nauthor={Jeff Donahue and Sander Dieleman and Mikolaj Binkowski and Erich Elsen and Karen Simonyan},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=rsf1z-JSj87}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "rsf1z-JSj87", "replyto": "rsf1z-JSj87", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3246/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538079316, "tmdate": 1606915793117, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3246/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3246/-/Official_Review"}}}, {"id": "u2k3RT7lCkw", "original": null, "number": 3, "cdate": 1603898930693, "ddate": null, "tcdate": 1603898930693, "tmdate": 1605024037804, "tddate": null, "forum": "rsf1z-JSj87", "replyto": "rsf1z-JSj87", "invitation": "ICLR.cc/2021/Conference/Paper3246/-/Official_Review", "content": {"title": "A fully end-to-end text-to-speech model with limited supervision that generates speech with high quality.", "review": "This paper proposes a fully end-to-end TTS system with adversarial training. The proposed method has three main advantages: 1) a simple, end-to-end pipeline with only two submodules, with a fully differentiable and efficient architecture; 2) a flexible dynamic time warping to compute the loss between the predicted (generated) and true spectrograms; 3) a good MOS performance compared with the other state-of-the-art systems with more supervision. The evaluation is done very thoroughly with a variety of ablation studies.\n\nOverall, I vote for accepting the paper. First of all, the paper is very well organized and easy to follow. Related works are well summarized, while focusing on the main differences on the proposed method. Method is explained in great detail with easy-to-follow descriptions, proper equations, and very appropriate figures. Solid evaluation is performed, and experimental results and speech samples are convincing.\n\nPros:\n+ The structure of the paper allows an easy read.\n+ The main contributions are clearly stated and supported by the experiments.\n+ Major works on the similar topic are widely covered and referenced.\n+ Evaluation is thorough enough to support the arguments with in-depth ablation studies.\n+ Appendices provide useful, supplementary information.\n\nCons:\n- No comparison over the computational cost nor model size is presented. It is of particular interest because the proposed model is non-autoregressive, and thus may be capable of a causal, real-time inference.\n- No use of widely accepted benchmark datasets. More direct comparison would be of interest.\n\nMinor comments/questions:\n- If I understood correctly, the training sample has no phoneme-level alignment. Instead, only a sentence-speech pair is provided. If so, how do you select the corresponding text snippet from a sentence when you randomly sample 2 seconds of audio from the training examples whose length varies from 1 to 20 seconds?\n- Section 2.5 on DTW is rather lengthy. DTW is a quite well-known algorithm for alignment between the two sequences, the detailed explanation on the algorithm may be omitted without the loss of readability, in my opinion.\n- In Section 2.1, T is used to denote the total number of output times steps of the aligner, while T in Section 2.4 denotes the number of mel-frequency frames. Are these T's identical?\n- The proposed aligner module doesn't seem to be very useful compared with the attention-based aligner as seen in the ablation study (Table 1): very small improvement from 3.551 to 3.559 MOS. Can you provide more explanations?", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3246/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3246/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "End-to-end Adversarial Text-to-Speech", "authorids": ["~Jeff_Donahue1", "~Sander_Dieleman1", "~Mikolaj_Binkowski1", "~Erich_Elsen1", "~Karen_Simonyan1"], "authors": ["Jeff Donahue", "Sander Dieleman", "Mikolaj Binkowski", "Erich Elsen", "Karen Simonyan"], "keywords": ["text-to-speech", "speech synthesis", "adversarial", "GAN", "end-to-end", "feed-forward", "generative model"], "abstract": "Modern text-to-speech synthesis pipelines typically involve multiple processing stages, each of which is designed or learnt independently from the rest. In this work, we take on the challenging task of learning to synthesise speech from normalised text or phonemes in an end-to-end manner, resulting in models which operate directly on character or phoneme input sequences and produce raw speech audio outputs. Our proposed generator is feed-forward and thus efficient for both training and inference, using a differentiable alignment scheme based on token length prediction. It learns to produce high fidelity audio through a combination of adversarial feedback and prediction losses constraining the generated audio to roughly match the ground truth in terms of its total duration and mel-spectrogram. To allow the model to capture temporal variation in the generated audio, we employ soft dynamic time warping in the spectrogram-based prediction loss. The resulting model achieves a mean opinion score exceeding 4 on a 5 point scale, which is comparable to the state-of-the-art models relying on multi-stage training and additional supervision.", "one-sentence_summary": "Efficient, adversarially trained feed-forward text-to-speech model producing high-quality speech, learnt end-to-end in a single stage.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "donahue|endtoend_adversarial_texttospeech", "pdf": "/pdf/37c14e59635044278b29d57254fbd09950d3d37a.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ndonahue2021endtoend,\ntitle={End-to-end Adversarial Text-to-Speech},\nauthor={Jeff Donahue and Sander Dieleman and Mikolaj Binkowski and Erich Elsen and Karen Simonyan},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=rsf1z-JSj87}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "rsf1z-JSj87", "replyto": "rsf1z-JSj87", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3246/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538079316, "tmdate": 1606915793117, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3246/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3246/-/Official_Review"}}}, {"id": "8i0Rsbolbje", "original": null, "number": 4, "cdate": 1604324104446, "ddate": null, "tcdate": 1604324104446, "tmdate": 1605024037739, "tddate": null, "forum": "rsf1z-JSj87", "replyto": "rsf1z-JSj87", "invitation": "ICLR.cc/2021/Conference/Paper3246/-/Official_Review", "content": {"title": "Review", "review": "This paper presents an approach for end-to-end speech synthesis, where every step is learned jointly with the others. Specifically,  the proposed model takes a character sequence as input and outputs an audio signal directly. The model is trained using an combination of losses, including an adversarial loss. In the experimental section, the proposed approach is compared against strong baselines, and ablation studies are presented.\n\nPros:\n- The proposed approach is novel and a significant step toward competitive end-to-end models.\n- The related work is thorough as far as I can tell.\n- The experiments are insightful, showing the impact of each part of the system.\n\nCons:\n- The performance are promising, but still below the baselines.\n- The end-to-end claim is a bit misleading as the character-based model is not performing well, and the phoneme-based model is not really end-to-end, as the g2p part is not trained jointly.\n- The paper is sometime not easy to follow.\n\nDetailed comments:\n- The reason behind using an adversarial loss is not really explained in the paper. A few lines before section 2.1 would help clarify that.\n- The order of the sub-sections (2.1-2.7) in Section 2 is not intuitive and seems a bit random, making the section slightly hard to follow.  \n- Section 2.7: \"inconsistent spelling rules of the English language\" -> It's not about inconsistent rules: every language has inconsistent rules, several dialects and variations in the pronunciation of words. Please rephrase.\n- It's not clear which dataset was used in the experiment. If it is a private dataset, please state it clearly.\n\nOverall, despite the paper's two main weaknesses (not fully end-to-end and lower performance),  I think it is a significant step towards fully end-to-end model and should be accepted. ", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3246/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3246/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "End-to-end Adversarial Text-to-Speech", "authorids": ["~Jeff_Donahue1", "~Sander_Dieleman1", "~Mikolaj_Binkowski1", "~Erich_Elsen1", "~Karen_Simonyan1"], "authors": ["Jeff Donahue", "Sander Dieleman", "Mikolaj Binkowski", "Erich Elsen", "Karen Simonyan"], "keywords": ["text-to-speech", "speech synthesis", "adversarial", "GAN", "end-to-end", "feed-forward", "generative model"], "abstract": "Modern text-to-speech synthesis pipelines typically involve multiple processing stages, each of which is designed or learnt independently from the rest. In this work, we take on the challenging task of learning to synthesise speech from normalised text or phonemes in an end-to-end manner, resulting in models which operate directly on character or phoneme input sequences and produce raw speech audio outputs. Our proposed generator is feed-forward and thus efficient for both training and inference, using a differentiable alignment scheme based on token length prediction. It learns to produce high fidelity audio through a combination of adversarial feedback and prediction losses constraining the generated audio to roughly match the ground truth in terms of its total duration and mel-spectrogram. To allow the model to capture temporal variation in the generated audio, we employ soft dynamic time warping in the spectrogram-based prediction loss. The resulting model achieves a mean opinion score exceeding 4 on a 5 point scale, which is comparable to the state-of-the-art models relying on multi-stage training and additional supervision.", "one-sentence_summary": "Efficient, adversarially trained feed-forward text-to-speech model producing high-quality speech, learnt end-to-end in a single stage.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "donahue|endtoend_adversarial_texttospeech", "pdf": "/pdf/37c14e59635044278b29d57254fbd09950d3d37a.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ndonahue2021endtoend,\ntitle={End-to-end Adversarial Text-to-Speech},\nauthor={Jeff Donahue and Sander Dieleman and Mikolaj Binkowski and Erich Elsen and Karen Simonyan},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=rsf1z-JSj87}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "rsf1z-JSj87", "replyto": "rsf1z-JSj87", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3246/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538079316, "tmdate": 1606915793117, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3246/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3246/-/Official_Review"}}}], "count": 14}