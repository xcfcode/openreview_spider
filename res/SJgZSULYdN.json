{"notes": [{"id": "SJgZSULYdN", "original": "rJl_sptO_4", "number": 44, "cdate": 1553716792846, "ddate": null, "tcdate": 1553716792846, "tmdate": 1562083047914, "tddate": null, "forum": "SJgZSULYdN", "replyto": null, "invitation": "ICLR.cc/2019/Workshop/DeepGenStruct/-/Blind_Submission", "content": {"title": "HYPE:  Human-eYe Perceptual Evaluation of Generative Models", "authors": ["Sharon Zhou", "Mitchell Gordon", "Ranjay Krishna", "Austin Narcomey", "Durim Morina", "Michael S. Bernstein"], "authorids": ["sharonz@cs.stanford.edu", "mgord@cs.stanford.edu", "ranjaykrishna@cs.stanford.edu", "aon2@cs.stanford.edu", "dmorina@cs.stanford.edu", "msb@cs.stanford.edu"], "keywords": ["Generative models", "generative evaluation", "evaluation metric", "human evaluation"], "TL;DR": "HYPE is a reliable human evaluation metric for scoring generative models, starting with human face generation across 4 GANs.", "abstract": "Generative models often use human evaluations to determine and justify progress. Unfortunately, existing human evaluation methods are ad-hoc: there is currently no standardized, validated evaluation that: (1) measures perceptual fidelity, (2) is reliable, (3) separates models into clear rank order, and (4) ensures high-quality measurement without intractable cost. In response, we construct Human-eYe Perceptual Evaluation (HYPE), a human metric that is (1) grounded in psychophysics research in perception, (2) reliable across different sets of randomly sampled outputs from a model, (3) results in separable model performances, and (4) efficient in cost and time. We introduce two methods. The first, HYPE-Time, measures visual perception under adaptive time constraints to determine the minimum length of time (e.g., 250ms) that model output such as a generated face needs to be visible for people to distinguish it as real or fake. The second, HYPE-Infinity, measures human error rate on fake and real images with no time constraints, maintaining stability and drastically reducing time and cost. We test HYPE across four state-of-the-art generative adversarial networks (GANs) on unconditional image generation using two datasets, the popular CelebA and the newer higher-resolution FFHQ, and two sampling techniques of model outputs. By simulating HYPE's evaluation multiple times, we demonstrate consistent ranking of different models, identifying StyleGAN with truncation trick sampling (27.6% HYPE-Infinity deception rate, with roughly one quarter of images being misclassified by humans) as superior to StyleGAN without truncation (19.0%) on FFHQ. ", "pdf": "/pdf/bf03a37326070c926bac77f5c99e0f803351d5d3.pdf", "paperhash": "zhou|hype_humaneye_perceptual_evaluation_of_generative_models"}, "signatures": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "details": {"replyCount": 3, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/DeepGenStruct/-/Blind_Submission", "cdate": 1547567085825, "reply": {"forum": null, "replyto": null, "readers": {"values-regex": [".*"]}, "writers": {"values": ["ICLR.cc/2019/Workshop/DeepGenStruct"]}, "signatures": {"values": ["ICLR.cc/2019/Workshop/DeepGenStruct"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}}}, "tcdate": 1547567085825, "tmdate": 1555704438520, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "invitees": ["~"], "signatures": ["ICLR.cc/2019/Workshop/DeepGenStruct"]}}, "tauthor": "OpenReview.net"}, {"id": "Bkl2aSe45E", "original": null, "number": 2, "cdate": 1555461572381, "ddate": null, "tcdate": 1555461572381, "tmdate": 1556906143670, "tddate": null, "forum": "SJgZSULYdN", "replyto": "SJgZSULYdN", "invitation": "ICLR.cc/2019/Workshop/DeepGenStruct/-/Paper44/Official_Review", "content": {"title": "Review for \"HYPE: Human-eYe Perceptual Evaluation of Generative Models\"", "review": "The paper introduces two methods for evaluating generative models, \"HYPE_time\" and \"HYPE_infinity\". Both methods use human reaction times or error rates when asked to distinguish fake images from real data. \n\nSeveral comments about the paper:\n\nPros:\n- The paper is very well written and clear.\n- The process is fully automated and reproducible, which is a drastic difference with other user studies this reviewer is aware of. The process is thoroughly documented.\n- This paper is quite out there, the ideas are to me very novel and reasonably surprising (at the degree of thoroughness that this paper makes use of them). I don't think this is ready for a conference, but in terms of giving a space for more risky ideas to be discussed (as workshops should), I think this paper fits the bill.\n- As mentioned in the paper, the method seems to be reliable, moderately fast (10 minutes), and measures perceptual fidelity well.\n- I think the experiments are enough evidence to support the authors' claims.\n\nCons:\n- It's undiscussed (and to me unlikely) wether the proposed scores are good for ranking models that are bad (but one a lot worse than the other, such as models in the middle of training), given that distinguishability for bad models might be essentially the same, or require a lot more time and humans to reach confidence intervals that are non-overlapping.\n- There is an emphasis on 'reality', which ignores 'diversity', and I think the paper should stress more that this in effect is not trying to provide a way to evaluate generative models per se, but simply the 'reality' of the samples.\n- In the same lines of the above, 'reality' is not the same as 'fidelity', a model that for examples produces reasonable faces but ignores modeling the background of an image distribution has obviously worse sample quality, but a human might think the samples to be more real. Essentially, it focuses to much on the human-centric preconceived notion of reality, rather than a comparison (human based or not) between the true and generated data distributions.\n- All experiments are on faces, which makes the reader wander wether the accuracy or 'cost-effectiveness' of the method depends on how good humans are at judging the quality of faces.\n- It's quite expensive to run this thing ($60 makes it unusable for e.g. grid searches).\n- The method is obviously specific to data types like images where humans have a notion of what a 'real' sample is, which limits the method from ever working in things like representation spaces.", "rating": "4: Top 50% of accepted papers, clear accept", "confidence": "2: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Workshop/DeepGenStruct/Paper44/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/DeepGenStruct/Paper44/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "HYPE:  Human-eYe Perceptual Evaluation of Generative Models", "authors": ["Sharon Zhou", "Mitchell Gordon", "Ranjay Krishna", "Austin Narcomey", "Durim Morina", "Michael S. Bernstein"], "authorids": ["sharonz@cs.stanford.edu", "mgord@cs.stanford.edu", "ranjaykrishna@cs.stanford.edu", "aon2@cs.stanford.edu", "dmorina@cs.stanford.edu", "msb@cs.stanford.edu"], "keywords": ["Generative models", "generative evaluation", "evaluation metric", "human evaluation"], "TL;DR": "HYPE is a reliable human evaluation metric for scoring generative models, starting with human face generation across 4 GANs.", "abstract": "Generative models often use human evaluations to determine and justify progress. Unfortunately, existing human evaluation methods are ad-hoc: there is currently no standardized, validated evaluation that: (1) measures perceptual fidelity, (2) is reliable, (3) separates models into clear rank order, and (4) ensures high-quality measurement without intractable cost. In response, we construct Human-eYe Perceptual Evaluation (HYPE), a human metric that is (1) grounded in psychophysics research in perception, (2) reliable across different sets of randomly sampled outputs from a model, (3) results in separable model performances, and (4) efficient in cost and time. We introduce two methods. The first, HYPE-Time, measures visual perception under adaptive time constraints to determine the minimum length of time (e.g., 250ms) that model output such as a generated face needs to be visible for people to distinguish it as real or fake. The second, HYPE-Infinity, measures human error rate on fake and real images with no time constraints, maintaining stability and drastically reducing time and cost. We test HYPE across four state-of-the-art generative adversarial networks (GANs) on unconditional image generation using two datasets, the popular CelebA and the newer higher-resolution FFHQ, and two sampling techniques of model outputs. By simulating HYPE's evaluation multiple times, we demonstrate consistent ranking of different models, identifying StyleGAN with truncation trick sampling (27.6% HYPE-Infinity deception rate, with roughly one quarter of images being misclassified by humans) as superior to StyleGAN without truncation (19.0%) on FFHQ. ", "pdf": "/pdf/bf03a37326070c926bac77f5c99e0f803351d5d3.pdf", "paperhash": "zhou|hype_humaneye_perceptual_evaluation_of_generative_models"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/DeepGenStruct/-/Paper44/Official_Review", "cdate": 1554234171369, "reply": {"forum": "SJgZSULYdN", "replyto": "SJgZSULYdN", "readers": [".*"], "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2019/Workshop/DeepGenStruct/Paper44/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2019/Workshop/DeepGenStruct/Paper44/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["5: Top 15% of accepted papers, strong accept", "4: Top 50% of accepted papers, clear accept", "3: Marginally above acceptance threshold", "2: Marginally below acceptance threshold", "1: Strong rejection"], "required": true}, "confidence": {"order": 4, "value-radio": ["3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "2: The reviewer is fairly confident that the evaluation is correct", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "tcdate": 1554234171369, "tmdate": 1556906092887, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "invitees": ["ICLR.cc/2019/Workshop/DeepGenStruct/Paper44/Reviewers"], "signatures": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "details": {"writable": true}}}}, {"id": "rJxJvdtzcE", "original": null, "number": 1, "cdate": 1555368023477, "ddate": null, "tcdate": 1555368023477, "tmdate": 1556906143453, "tddate": null, "forum": "SJgZSULYdN", "replyto": "SJgZSULYdN", "invitation": "ICLR.cc/2019/Workshop/DeepGenStruct/-/Paper44/Official_Review", "content": {"title": "Timely work, well presented", "review": "The paper proposes a framework for human evaluation of generative models of images. It is based on samples, so it is compatible with any flavour of generative model (likelihood-based, adversarial or otherwise). Two different evaluation strategies are proposed: one based on the time it takes for humans to distinguish generated images from real images, and another which simply measures the percentage of images that are wrongly classified.\n\nThe implementation of the human evaluation setup is described in appropriate detail, and attention to cost is also given. The results are comprehensive and statistical tests are used to show their significance. The approach is also compared to FID, a computational evaluation metric that is currently popular.\n\nOverall, this is work is timely and it is well presented, so I am in favour of acceptance. Nevertheless I have a few more comments and suggested improvements below:\n\n* It is demonstrated that the correlation of HYPE and FID is relatively poor, and it is implied that this demonstrates that FID is a poor metric. However, as the authors state earlier on in the paper, HYPE can only measure realism, not diversity. FID is explicitly constructed to also be affected by sample diversity, so in that light it is not surprising that the two do not correlate very well, and that higher truncation leads to improved HYPE but worse FID scores -- it is well known that truncation reduces diversity of the samples, in favour of improved fidelity. (I do not wish to imply that FID is actually a good metric -- I do believe it is a poor metric, but not for this particular reason.)\n\n* While the authors state clearly that HYPE does not measure diversity, I think it would be worth discussing in more detail how one could use human evaluation to measure diversity, as it is arguably a more interesting challenge. As it stands, the HYPE metric could probably be fooled by a \"model\" which simply stores a few training examples and randomly selects them with equal probability. Also measuring the diversity of the samples in some way would prevent this kind of cheating.\n\n* A common issue with human evaluation is ambiguity in the task specification: the raters are instructed to determine which images are real, but they may be prone to misinterpreting the task in a way that biases the results. While rater training and immediate feedback undoubtedly help to limit this effect, it is still worth considering this carefully, and I think a few diagrams or screenshots of the rater interface would be useful additions to the manuscript in this respect.\n\n* In the introduction, it is implied that likelihood (measured in the input space) would be the ideal metric for generative models if it were always easy to compute (which it often isn't). Theis et al. (2015), cited there, also call this into question. I find the juxtaposition of this citation and the sentence before it a bit misleading.", "rating": "4: Top 50% of accepted papers, clear accept", "confidence": "2: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Workshop/DeepGenStruct/Paper44/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/DeepGenStruct/Paper44/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "HYPE:  Human-eYe Perceptual Evaluation of Generative Models", "authors": ["Sharon Zhou", "Mitchell Gordon", "Ranjay Krishna", "Austin Narcomey", "Durim Morina", "Michael S. Bernstein"], "authorids": ["sharonz@cs.stanford.edu", "mgord@cs.stanford.edu", "ranjaykrishna@cs.stanford.edu", "aon2@cs.stanford.edu", "dmorina@cs.stanford.edu", "msb@cs.stanford.edu"], "keywords": ["Generative models", "generative evaluation", "evaluation metric", "human evaluation"], "TL;DR": "HYPE is a reliable human evaluation metric for scoring generative models, starting with human face generation across 4 GANs.", "abstract": "Generative models often use human evaluations to determine and justify progress. Unfortunately, existing human evaluation methods are ad-hoc: there is currently no standardized, validated evaluation that: (1) measures perceptual fidelity, (2) is reliable, (3) separates models into clear rank order, and (4) ensures high-quality measurement without intractable cost. In response, we construct Human-eYe Perceptual Evaluation (HYPE), a human metric that is (1) grounded in psychophysics research in perception, (2) reliable across different sets of randomly sampled outputs from a model, (3) results in separable model performances, and (4) efficient in cost and time. We introduce two methods. The first, HYPE-Time, measures visual perception under adaptive time constraints to determine the minimum length of time (e.g., 250ms) that model output such as a generated face needs to be visible for people to distinguish it as real or fake. The second, HYPE-Infinity, measures human error rate on fake and real images with no time constraints, maintaining stability and drastically reducing time and cost. We test HYPE across four state-of-the-art generative adversarial networks (GANs) on unconditional image generation using two datasets, the popular CelebA and the newer higher-resolution FFHQ, and two sampling techniques of model outputs. By simulating HYPE's evaluation multiple times, we demonstrate consistent ranking of different models, identifying StyleGAN with truncation trick sampling (27.6% HYPE-Infinity deception rate, with roughly one quarter of images being misclassified by humans) as superior to StyleGAN without truncation (19.0%) on FFHQ. ", "pdf": "/pdf/bf03a37326070c926bac77f5c99e0f803351d5d3.pdf", "paperhash": "zhou|hype_humaneye_perceptual_evaluation_of_generative_models"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/DeepGenStruct/-/Paper44/Official_Review", "cdate": 1554234171369, "reply": {"forum": "SJgZSULYdN", "replyto": "SJgZSULYdN", "readers": [".*"], "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2019/Workshop/DeepGenStruct/Paper44/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2019/Workshop/DeepGenStruct/Paper44/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["5: Top 15% of accepted papers, strong accept", "4: Top 50% of accepted papers, clear accept", "3: Marginally above acceptance threshold", "2: Marginally below acceptance threshold", "1: Strong rejection"], "required": true}, "confidence": {"order": 4, "value-radio": ["3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "2: The reviewer is fairly confident that the evaluation is correct", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "tcdate": 1554234171369, "tmdate": 1556906092887, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "invitees": ["ICLR.cc/2019/Workshop/DeepGenStruct/Paper44/Reviewers"], "signatures": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "details": {"writable": true}}}}, {"id": "BJxL0Xuw5E", "original": null, "number": 1, "cdate": 1555690446058, "ddate": null, "tcdate": 1555690446058, "tmdate": 1556906143197, "tddate": null, "forum": "SJgZSULYdN", "replyto": "SJgZSULYdN", "invitation": "ICLR.cc/2019/Workshop/DeepGenStruct/-/Paper44/Decision", "content": {"title": "Acceptance Decision", "decision": "Accept"}, "signatures": ["ICLR.cc/2019/Workshop/DeepGenStruct/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/DeepGenStruct/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "HYPE:  Human-eYe Perceptual Evaluation of Generative Models", "authors": ["Sharon Zhou", "Mitchell Gordon", "Ranjay Krishna", "Austin Narcomey", "Durim Morina", "Michael S. Bernstein"], "authorids": ["sharonz@cs.stanford.edu", "mgord@cs.stanford.edu", "ranjaykrishna@cs.stanford.edu", "aon2@cs.stanford.edu", "dmorina@cs.stanford.edu", "msb@cs.stanford.edu"], "keywords": ["Generative models", "generative evaluation", "evaluation metric", "human evaluation"], "TL;DR": "HYPE is a reliable human evaluation metric for scoring generative models, starting with human face generation across 4 GANs.", "abstract": "Generative models often use human evaluations to determine and justify progress. Unfortunately, existing human evaluation methods are ad-hoc: there is currently no standardized, validated evaluation that: (1) measures perceptual fidelity, (2) is reliable, (3) separates models into clear rank order, and (4) ensures high-quality measurement without intractable cost. In response, we construct Human-eYe Perceptual Evaluation (HYPE), a human metric that is (1) grounded in psychophysics research in perception, (2) reliable across different sets of randomly sampled outputs from a model, (3) results in separable model performances, and (4) efficient in cost and time. We introduce two methods. The first, HYPE-Time, measures visual perception under adaptive time constraints to determine the minimum length of time (e.g., 250ms) that model output such as a generated face needs to be visible for people to distinguish it as real or fake. The second, HYPE-Infinity, measures human error rate on fake and real images with no time constraints, maintaining stability and drastically reducing time and cost. We test HYPE across four state-of-the-art generative adversarial networks (GANs) on unconditional image generation using two datasets, the popular CelebA and the newer higher-resolution FFHQ, and two sampling techniques of model outputs. By simulating HYPE's evaluation multiple times, we demonstrate consistent ranking of different models, identifying StyleGAN with truncation trick sampling (27.6% HYPE-Infinity deception rate, with roughly one quarter of images being misclassified by humans) as superior to StyleGAN without truncation (19.0%) on FFHQ. ", "pdf": "/pdf/bf03a37326070c926bac77f5c99e0f803351d5d3.pdf", "paperhash": "zhou|hype_humaneye_perceptual_evaluation_of_generative_models"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/DeepGenStruct/-/Paper44/Decision", "cdate": 1554814602508, "reply": {"forum": "SJgZSULYdN", "replyto": "SJgZSULYdN", "readers": [".*"], "nonreaders": {"values": []}, "writers": {"values-regex": ["ICLR.cc/2019/Workshop/DeepGenStruct/Program_Chairs"], "description": "How your identity will be displayed."}, "signatures": {"values": ["ICLR.cc/2019/Workshop/DeepGenStruct/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"title": {"order": 1, "required": true, "value": "Acceptance Decision"}, "decision": {"order": 2, "required": true, "value-radio": ["Accept", "Reject"], "description": "Acceptance decision"}, "comment": {"order": 3, "required": false, "value-regex": "[\\S\\s]{0,5000}", "description": ""}}}, "tcdate": 1554814602508, "tmdate": 1556906102472, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "invitees": ["ICLR.cc/2019/Workshop/DeepGenStruct/Program_Chairs"], "signatures": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "details": {"writable": true}}}}], "count": 4}