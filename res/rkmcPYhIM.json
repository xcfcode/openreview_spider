{"notes": [{"tddate": null, "ddate": null, "original": null, "tmdate": 1521573613048, "tcdate": 1521573613048, "number": 295, "cdate": 1521573612704, "id": "rkHeyJJ9G", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "rkmcPYhIM", "replyto": "rkmcPYhIM", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Accept", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "This paper was invited to the workshop track based on reviews at the main conference."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Exponentially vanishing sub-optimal local minima in multilayer neural networks", "abstract": "Background: Statistical mechanics results (Dauphin et al. (2014); Choromanska et al. (2015)) suggest that local minima with high error are exponentially rare in high dimensions. However, to prove low error guarantees for Multilayer Neural Networks (MNNs), previous works so far required either a heavily modified MNN model or training method, strong assumptions on the labels (e.g., \u201cnear\u201d linear separability), or an unrealistically wide hidden layer with \\Omega\\(N) units.  \nResults: We examine a MNN with one hidden layer of piecewise linear units, a single output, and a quadratic loss. We prove that, with high probability in the limit of N\\rightarrow\\infty datapoints, the volume of differentiable regions of the empiric loss containing sub-optimal differentiable local minima is exponentially vanishing in comparison with the same volume of global minima, given standard normal input of dimension d_0=\\tilde{\\Omega}(\\sqrt{N}), and a more realistic number of d_1=\\tilde{\\Omega}(N/d_0) hidden units. We demonstrate our results numerically: for example, 0% binary classification training error on CIFAR with only N/d_0 = 16 hidden neurons.", "pdf": "/pdf/c5f504a882396c1e9c891c896075ce480c7afcf0.pdf", "TL;DR": "\"Bad\" local minima are vanishing in a multilayer neural net: a proof with more reasonable assumptions than before", "paperhash": "soudry|exponentially_vanishing_suboptimal_local_minima_in_multilayer_neural_networks", "_bibtex": "@misc{\nsoudry2018exponentially,\ntitle={Exponentially vanishing sub-optimal local minima in multilayer neural networks},\nauthor={Daniel Soudry and Elad Hoffer},\nyear={2018},\nurl={https://openreview.net/forum?id=Hkfmn5n6W},\n}", "keywords": ["neural networks", "theory", "optimization", "local minima", "loss landscape"], "authors": ["Daniel Soudry", "Elad Hoffer"], "authorids": ["daniel.soudry@gmail.com", "elad.hoffer@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1518730190277, "tcdate": 1518274442857, "number": 55, "cdate": 1518274442857, "id": "rkmcPYhIM", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "rkmcPYhIM", "original": "Hkfmn5n6W", "signatures": ["~Daniel_Soudry1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "Exponentially vanishing sub-optimal local minima in multilayer neural networks", "abstract": "Background: Statistical mechanics results (Dauphin et al. (2014); Choromanska et al. (2015)) suggest that local minima with high error are exponentially rare in high dimensions. However, to prove low error guarantees for Multilayer Neural Networks (MNNs), previous works so far required either a heavily modified MNN model or training method, strong assumptions on the labels (e.g., \u201cnear\u201d linear separability), or an unrealistically wide hidden layer with \\Omega\\(N) units.  \nResults: We examine a MNN with one hidden layer of piecewise linear units, a single output, and a quadratic loss. We prove that, with high probability in the limit of N\\rightarrow\\infty datapoints, the volume of differentiable regions of the empiric loss containing sub-optimal differentiable local minima is exponentially vanishing in comparison with the same volume of global minima, given standard normal input of dimension d_0=\\tilde{\\Omega}(\\sqrt{N}), and a more realistic number of d_1=\\tilde{\\Omega}(N/d_0) hidden units. We demonstrate our results numerically: for example, 0% binary classification training error on CIFAR with only N/d_0 = 16 hidden neurons.", "pdf": "/pdf/c5f504a882396c1e9c891c896075ce480c7afcf0.pdf", "TL;DR": "\"Bad\" local minima are vanishing in a multilayer neural net: a proof with more reasonable assumptions than before", "paperhash": "soudry|exponentially_vanishing_suboptimal_local_minima_in_multilayer_neural_networks", "_bibtex": "@misc{\nsoudry2018exponentially,\ntitle={Exponentially vanishing sub-optimal local minima in multilayer neural networks},\nauthor={Daniel Soudry and Elad Hoffer},\nyear={2018},\nurl={https://openreview.net/forum?id=Hkfmn5n6W},\n}", "keywords": ["neural networks", "theory", "optimization", "local minima", "loss landscape"], "authors": ["Daniel Soudry", "Elad Hoffer"], "authorids": ["daniel.soudry@gmail.com", "elad.hoffer@gmail.com"]}, "nonreaders": [], "details": {"replyCount": 1, "writable": false, "overwriting": [], "revisions": true, "tags": [], "original": {"tddate": null, "ddate": null, "tmdate": 1518730190277, "tcdate": 1508842522390, "number": 63, "cdate": 1518730190266, "id": "Hkfmn5n6W", "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "forum": "Hkfmn5n6W", "original": "BJWXh93T-", "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference"], "content": {"title": "Exponentially vanishing sub-optimal local minima in multilayer neural networks", "abstract": "Background: Statistical mechanics results (Dauphin et al. (2014); Choromanska et al. (2015)) suggest that local minima with high error are exponentially rare in high dimensions. However, to prove low error guarantees for Multilayer Neural Networks (MNNs), previous works so far required either a heavily modified MNN model or training method, strong assumptions on the labels (e.g., \u201cnear\u201d linear separability), or an unrealistically wide hidden layer with \\Omega\\(N) units. \n\nResults: We examine a MNN with one hidden layer of piecewise linear units, a single output, and a quadratic loss. We prove that, with high probability in the limit of N\\rightarrow\\infty datapoints, the volume of differentiable regions of the empiric loss containing sub-optimal differentiable local minima is exponentially vanishing in comparison with the same volume of global minima, given standard normal input of dimension d_0=\\tilde{\\Omega}(\\sqrt{N}), and a more realistic number of d_1=\\tilde{\\Omega}(N/d_0) hidden units. We demonstrate our results numerically: for example, 0% binary classification training error on CIFAR with only N/d_0 = 16 hidden neurons.", "pdf": "/pdf/ceb9329449138c194037615b5d36b3edeb75b25e.pdf", "TL;DR": "\"Bad\" local minima are vanishing in a multilayer neural net: a proof with more reasonable assumptions than before", "paperhash": "soudry|exponentially_vanishing_suboptimal_local_minima_in_multilayer_neural_networks", "_bibtex": "@misc{\nsoudry2018exponentially,\ntitle={Exponentially vanishing sub-optimal local minima in multilayer neural networks},\nauthor={Daniel Soudry and Elad Hoffer},\nyear={2018},\nurl={https://openreview.net/forum?id=Hkfmn5n6W},\n}", "keywords": ["neural networks", "theory", "optimization", "local minima", "loss landscape"], "authors": ["Daniel Soudry", "Elad Hoffer"], "authorids": ["daniel.soudry@gmail.com", "elad.hoffer@gmail.com"]}, "nonreaders": []}, "originalWritable": false, "originalInvitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1506717071958, "id": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Conference"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference"]}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"authors": {"required": false, "order": 1, "values-regex": ".*", "description": "Comma separated list of author names, as they appear in the paper."}, "authorids": {"required": false, "order": 2, "values-regex": ".*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "cdate": 1506717071958}, "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}}], "count": 2}