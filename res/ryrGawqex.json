{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1487738330422, "tcdate": 1478290701211, "number": 440, "id": "ryrGawqex", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "ryrGawqex", "signatures": ["~Moshe_Looks1"], "readers": ["everyone"], "content": {"title": "Deep Learning with Dynamic Computation Graphs", "abstract": "Neural networks that compute over graph structures are a natural fit for problems in a variety of domains, including natural language (parse trees) and cheminformatics (molecular graphs). However, since the computation graph has a different shape and size for every input, such networks do not directly support batched training or inference. They are also difficult to implement in popular deep learning libraries, which are based on static data-flow graphs. We introduce a technique called dynamic batching, which not only batches together operations between different input graphs of dissimilar shape, but also between different nodes within a single input graph. The technique allows us to create static graphs, using popular libraries, that emulate dynamic computation graphs of arbitrary shape and size. We further present a high-level library of compositional blocks that simplifies the creation of dynamic graph models. Using the library, we demonstrate concise and batch-wise parallel implementations for a variety of models from the literature.", "pdf": "/pdf/443b19c32d88a8ba6eb9d035f5bcff9c82320c01.pdf", "TL;DR": "We make batching effective and easy to use for neural nets where every input may have a different shape (e.g. TreeRNNs).", "paperhash": "looks|deep_learning_with_dynamic_computation_graphs", "keywords": ["Deep learning"], "conflicts": ["google.com"], "authors": ["Moshe Looks", "Marcello Herreshoff", "DeLesley Hutchins", "Peter Norvig"], "authorids": ["madscience@google.com", "marcelloh@google.com", "delesley@google.com", "pnorvig@google.com"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 14, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "tmdate": 1486491950237, "tcdate": 1486491950237, "number": 7, "id": "SkL4Zcvug", "invitation": "ICLR.cc/2017/conference/-/paper440/public/comment", "forum": "ryrGawqex", "replyto": "ryrGawqex", "signatures": ["~Moshe_Looks1"], "readers": ["everyone"], "writers": ["~Moshe_Looks1"], "content": {"title": "uploaded minor revision", "comment": "Adds \\iclrfinalcopy and a link to the github repo which is now live"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Learning with Dynamic Computation Graphs", "abstract": "Neural networks that compute over graph structures are a natural fit for problems in a variety of domains, including natural language (parse trees) and cheminformatics (molecular graphs). However, since the computation graph has a different shape and size for every input, such networks do not directly support batched training or inference. They are also difficult to implement in popular deep learning libraries, which are based on static data-flow graphs. We introduce a technique called dynamic batching, which not only batches together operations between different input graphs of dissimilar shape, but also between different nodes within a single input graph. The technique allows us to create static graphs, using popular libraries, that emulate dynamic computation graphs of arbitrary shape and size. We further present a high-level library of compositional blocks that simplifies the creation of dynamic graph models. Using the library, we demonstrate concise and batch-wise parallel implementations for a variety of models from the literature.", "pdf": "/pdf/443b19c32d88a8ba6eb9d035f5bcff9c82320c01.pdf", "TL;DR": "We make batching effective and easy to use for neural nets where every input may have a different shape (e.g. TreeRNNs).", "paperhash": "looks|deep_learning_with_dynamic_computation_graphs", "keywords": ["Deep learning"], "conflicts": ["google.com"], "authors": ["Moshe Looks", "Marcello Herreshoff", "DeLesley Hutchins", "Peter Norvig"], "authorids": ["madscience@google.com", "marcelloh@google.com", "delesley@google.com", "pnorvig@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287576057, "id": "ICLR.cc/2017/conference/-/paper440/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ryrGawqex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper440/reviewers", "ICLR.cc/2017/conference/paper440/areachairs"], "cdate": 1485287576057}}}, {"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396585954, "tcdate": 1486396585954, "number": 1, "id": "Bkz23fIug", "invitation": "ICLR.cc/2017/conference/-/paper440/acceptance", "forum": "ryrGawqex", "replyto": "ryrGawqex", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "All reviewers viewed the paper favorably as a nice/helpful contribution to the implementation of this important class of methods.", "decision": "Accept (Poster)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Learning with Dynamic Computation Graphs", "abstract": "Neural networks that compute over graph structures are a natural fit for problems in a variety of domains, including natural language (parse trees) and cheminformatics (molecular graphs). However, since the computation graph has a different shape and size for every input, such networks do not directly support batched training or inference. They are also difficult to implement in popular deep learning libraries, which are based on static data-flow graphs. We introduce a technique called dynamic batching, which not only batches together operations between different input graphs of dissimilar shape, but also between different nodes within a single input graph. The technique allows us to create static graphs, using popular libraries, that emulate dynamic computation graphs of arbitrary shape and size. We further present a high-level library of compositional blocks that simplifies the creation of dynamic graph models. Using the library, we demonstrate concise and batch-wise parallel implementations for a variety of models from the literature.", "pdf": "/pdf/443b19c32d88a8ba6eb9d035f5bcff9c82320c01.pdf", "TL;DR": "We make batching effective and easy to use for neural nets where every input may have a different shape (e.g. TreeRNNs).", "paperhash": "looks|deep_learning_with_dynamic_computation_graphs", "keywords": ["Deep learning"], "conflicts": ["google.com"], "authors": ["Moshe Looks", "Marcello Herreshoff", "DeLesley Hutchins", "Peter Norvig"], "authorids": ["madscience@google.com", "marcelloh@google.com", "delesley@google.com", "pnorvig@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396586474, "id": "ICLR.cc/2017/conference/-/paper440/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "ryrGawqex", "replyto": "ryrGawqex", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396586474}}}, {"tddate": null, "tmdate": 1484916631596, "tcdate": 1484916631596, "number": 2, "id": "SkxiDY1De", "invitation": "ICLR.cc/2017/conference/-/paper440/official/comment", "forum": "ryrGawqex", "replyto": "H126B0lBl", "signatures": ["ICLR.cc/2017/conference/paper440/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper440/AnonReviewer2"], "content": {"title": "re:  ideas for better presentation, thoughts on real-world problems", "comment": "Yes I agree that the diagram might be confusing and it is good to hear that you are planning to apply this to ASR. I understand that seq2seq models are not perfectly suitable for this task, but a hybrid HMM system (with frame-wise labeled observation) on a task similar to Switchboard should show a significant speedup."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Learning with Dynamic Computation Graphs", "abstract": "Neural networks that compute over graph structures are a natural fit for problems in a variety of domains, including natural language (parse trees) and cheminformatics (molecular graphs). However, since the computation graph has a different shape and size for every input, such networks do not directly support batched training or inference. They are also difficult to implement in popular deep learning libraries, which are based on static data-flow graphs. We introduce a technique called dynamic batching, which not only batches together operations between different input graphs of dissimilar shape, but also between different nodes within a single input graph. The technique allows us to create static graphs, using popular libraries, that emulate dynamic computation graphs of arbitrary shape and size. We further present a high-level library of compositional blocks that simplifies the creation of dynamic graph models. Using the library, we demonstrate concise and batch-wise parallel implementations for a variety of models from the literature.", "pdf": "/pdf/443b19c32d88a8ba6eb9d035f5bcff9c82320c01.pdf", "TL;DR": "We make batching effective and easy to use for neural nets where every input may have a different shape (e.g. TreeRNNs).", "paperhash": "looks|deep_learning_with_dynamic_computation_graphs", "keywords": ["Deep learning"], "conflicts": ["google.com"], "authors": ["Moshe Looks", "Marcello Herreshoff", "DeLesley Hutchins", "Peter Norvig"], "authorids": ["madscience@google.com", "marcelloh@google.com", "delesley@google.com", "pnorvig@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287575929, "id": "ICLR.cc/2017/conference/-/paper440/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "ryrGawqex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper440/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper440/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper440/reviewers", "ICLR.cc/2017/conference/paper440/areachairs"], "cdate": 1485287575929}}}, {"tddate": null, "tmdate": 1484639620354, "tcdate": 1481905649996, "number": 2, "id": "BJqeUcbVe", "invitation": "ICLR.cc/2017/conference/-/paper440/official/review", "forum": "ryrGawqex", "replyto": "ryrGawqex", "signatures": ["ICLR.cc/2017/conference/paper440/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper440/AnonReviewer1"], "content": {"title": "", "rating": "8: Top 50% of accepted papers, clear accept", "review": "The paper presents a novel strategy to deal with dynamic computation graphs. They arise, when the computation is dynamically influenced by the input data, such as in LSTMs. The authors propose an `unrolling' strategy over the operations done at every step, which allows a new kind of batching of inputs.\n\nThe presented idea is novel and the results clearly indicate the potential of the approach. For the sake of clarity of the presentation I would drop parts of Section 3 (\"A combinator library for neural networks\") which presents technical details that are in general interesting, but do not help the understanding of the core idea of the paper. The presented experimental results on the \"Stanford Sentiment Treebank\" are in my opinion not supporting the claim of the paper, which is towards speed, than a little bit confusing. It is important to point out that even though the presented ensemble \"[...] variant sets a new state-of-the-art on both subtasks\" [p. 8], this is not due to the framework, not even due to the model (comp. lines 4 and 2 of Tab. 2), but probably, and this can only be speculated about, due to the ensemble averaging. I would appreciate a clearer argumentation in this respect.\n\nUpdate on Jan. 17th:\nafter the authors update for their newest revision, I increase my rating to 8 due to the again improved, now very clear argumentation.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Learning with Dynamic Computation Graphs", "abstract": "Neural networks that compute over graph structures are a natural fit for problems in a variety of domains, including natural language (parse trees) and cheminformatics (molecular graphs). However, since the computation graph has a different shape and size for every input, such networks do not directly support batched training or inference. They are also difficult to implement in popular deep learning libraries, which are based on static data-flow graphs. We introduce a technique called dynamic batching, which not only batches together operations between different input graphs of dissimilar shape, but also between different nodes within a single input graph. The technique allows us to create static graphs, using popular libraries, that emulate dynamic computation graphs of arbitrary shape and size. We further present a high-level library of compositional blocks that simplifies the creation of dynamic graph models. Using the library, we demonstrate concise and batch-wise parallel implementations for a variety of models from the literature.", "pdf": "/pdf/443b19c32d88a8ba6eb9d035f5bcff9c82320c01.pdf", "TL;DR": "We make batching effective and easy to use for neural nets where every input may have a different shape (e.g. TreeRNNs).", "paperhash": "looks|deep_learning_with_dynamic_computation_graphs", "keywords": ["Deep learning"], "conflicts": ["google.com"], "authors": ["Moshe Looks", "Marcello Herreshoff", "DeLesley Hutchins", "Peter Norvig"], "authorids": ["madscience@google.com", "marcelloh@google.com", "delesley@google.com", "pnorvig@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512584806, "id": "ICLR.cc/2017/conference/-/paper440/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper440/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper440/AnonReviewer2", "ICLR.cc/2017/conference/paper440/AnonReviewer1", "ICLR.cc/2017/conference/paper440/AnonReviewer3"], "reply": {"forum": "ryrGawqex", "replyto": "ryrGawqex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper440/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper440/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512584806}}}, {"tddate": null, "tmdate": 1484344091198, "tcdate": 1484344091198, "number": 6, "id": "B1mmo68Ux", "invitation": "ICLR.cc/2017/conference/-/paper440/public/comment", "forum": "ryrGawqex", "replyto": "ryrGawqex", "signatures": ["~Moshe_Looks1"], "readers": ["everyone"], "writers": ["~Moshe_Looks1"], "content": {"title": "minor revisions in response to reviewer comments", "comment": "I've just uploaded a minor revision in response to reviewer comments, in particular clarifying the intent of sections 3 and 3.5.1 with new/revised initial paragraphs. This pushed us a bit over the page limit so I moved the code example from 3.4 into an appendix per AnonReviewer1's suggestion. Thanks again AnonReviewers for your time and attention, I think this version is certainly an improvement over the original draft."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Learning with Dynamic Computation Graphs", "abstract": "Neural networks that compute over graph structures are a natural fit for problems in a variety of domains, including natural language (parse trees) and cheminformatics (molecular graphs). However, since the computation graph has a different shape and size for every input, such networks do not directly support batched training or inference. They are also difficult to implement in popular deep learning libraries, which are based on static data-flow graphs. We introduce a technique called dynamic batching, which not only batches together operations between different input graphs of dissimilar shape, but also between different nodes within a single input graph. The technique allows us to create static graphs, using popular libraries, that emulate dynamic computation graphs of arbitrary shape and size. We further present a high-level library of compositional blocks that simplifies the creation of dynamic graph models. Using the library, we demonstrate concise and batch-wise parallel implementations for a variety of models from the literature.", "pdf": "/pdf/443b19c32d88a8ba6eb9d035f5bcff9c82320c01.pdf", "TL;DR": "We make batching effective and easy to use for neural nets where every input may have a different shape (e.g. TreeRNNs).", "paperhash": "looks|deep_learning_with_dynamic_computation_graphs", "keywords": ["Deep learning"], "conflicts": ["google.com"], "authors": ["Moshe Looks", "Marcello Herreshoff", "DeLesley Hutchins", "Peter Norvig"], "authorids": ["madscience@google.com", "marcelloh@google.com", "delesley@google.com", "pnorvig@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287576057, "id": "ICLR.cc/2017/conference/-/paper440/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ryrGawqex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper440/reviewers", "ICLR.cc/2017/conference/paper440/areachairs"], "cdate": 1485287576057}}}, {"tddate": null, "tmdate": 1483559332782, "tcdate": 1483559332782, "number": 5, "id": "r1psb0qBg", "invitation": "ICLR.cc/2017/conference/-/paper440/public/comment", "forum": "ryrGawqex", "replyto": "SyOL-J8Nx", "signatures": ["~DeLesley_Hutchins1"], "readers": ["everyone"], "writers": ["~DeLesley_Hutchins1"], "content": {"title": "Some numbers for \"super slow\"", "comment": "\nTo put some numbers on Moshe's claim of \"super slow\", it takes about 17 seconds on our test machine to build and differentiate a graph using TensorFlow's single-threaded python API for the trees in the paper, vs 0.5 seconds on CPU or 0.17 seconds on GPU to run it.  Thus, building a new graph for each tree would cause a further slowdown of 30x to 100x over the numbers in the paper.\n\nHowever, this slowdown is entirely due to the current implementation of the TensorFlow graph construction API, which is not optimized for this use case.  Future versions of TensorFlow, or other libraries, could potentially reduce this overhead to negligible amounts.  Thus, in the paper we measure only the intrinsic cost of the batching itself, which is caused by hardware issues such as cache, memory bandwidth, and GPU utilization, and thus applies to any library implementation.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Learning with Dynamic Computation Graphs", "abstract": "Neural networks that compute over graph structures are a natural fit for problems in a variety of domains, including natural language (parse trees) and cheminformatics (molecular graphs). However, since the computation graph has a different shape and size for every input, such networks do not directly support batched training or inference. They are also difficult to implement in popular deep learning libraries, which are based on static data-flow graphs. We introduce a technique called dynamic batching, which not only batches together operations between different input graphs of dissimilar shape, but also between different nodes within a single input graph. The technique allows us to create static graphs, using popular libraries, that emulate dynamic computation graphs of arbitrary shape and size. We further present a high-level library of compositional blocks that simplifies the creation of dynamic graph models. Using the library, we demonstrate concise and batch-wise parallel implementations for a variety of models from the literature.", "pdf": "/pdf/443b19c32d88a8ba6eb9d035f5bcff9c82320c01.pdf", "TL;DR": "We make batching effective and easy to use for neural nets where every input may have a different shape (e.g. TreeRNNs).", "paperhash": "looks|deep_learning_with_dynamic_computation_graphs", "keywords": ["Deep learning"], "conflicts": ["google.com"], "authors": ["Moshe Looks", "Marcello Herreshoff", "DeLesley Hutchins", "Peter Norvig"], "authorids": ["madscience@google.com", "marcelloh@google.com", "delesley@google.com", "pnorvig@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287576057, "id": "ICLR.cc/2017/conference/-/paper440/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ryrGawqex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper440/reviewers", "ICLR.cc/2017/conference/paper440/areachairs"], "cdate": 1485287576057}}}, {"tddate": null, "tmdate": 1483369919907, "tcdate": 1483369919907, "number": 1, "id": "rJ_T6k_re", "invitation": "ICLR.cc/2017/conference/-/paper440/official/comment", "forum": "ryrGawqex", "replyto": "SJXULKbHl", "signatures": ["ICLR.cc/2017/conference/paper440/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper440/AnonReviewer1"], "content": {"title": "no title", "comment": "Thank you for your explanation of the paper structure, that explains for me better why you decided to include the experiment!\n\nI totally see your point  \"[...] that dynamic batching enables implementing deep learning models (which are growing ever more complex) at a higher level of abstraction (vs. manual batching) and thus a more rapid feedback loop for trying out novel model variants\". This will be super hard to profoundly argue for, but the SST experiment could give an example for it. Maybe even \"just\" a clearer line of argumentation on why the experiment is included and what should be shown with it and that actually Tab. 3 is at least as important as Tab. 2 could already help!\n\nI understand that space is very scarce, but both points:\n\n* the aim of the SST experiment,\n* the importance of Tab. 3 over Tab. 2,\n* and the source of improvement in Tab. 2,\n\nshould really receive more attention. Maybe some space could be gained dropping (a) code example(s)? All of Section 3 argues for more compact representations and easier design of complex architectures, maybe this can be summarized in a more condensed way?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Learning with Dynamic Computation Graphs", "abstract": "Neural networks that compute over graph structures are a natural fit for problems in a variety of domains, including natural language (parse trees) and cheminformatics (molecular graphs). However, since the computation graph has a different shape and size for every input, such networks do not directly support batched training or inference. They are also difficult to implement in popular deep learning libraries, which are based on static data-flow graphs. We introduce a technique called dynamic batching, which not only batches together operations between different input graphs of dissimilar shape, but also between different nodes within a single input graph. The technique allows us to create static graphs, using popular libraries, that emulate dynamic computation graphs of arbitrary shape and size. We further present a high-level library of compositional blocks that simplifies the creation of dynamic graph models. Using the library, we demonstrate concise and batch-wise parallel implementations for a variety of models from the literature.", "pdf": "/pdf/443b19c32d88a8ba6eb9d035f5bcff9c82320c01.pdf", "TL;DR": "We make batching effective and easy to use for neural nets where every input may have a different shape (e.g. TreeRNNs).", "paperhash": "looks|deep_learning_with_dynamic_computation_graphs", "keywords": ["Deep learning"], "conflicts": ["google.com"], "authors": ["Moshe Looks", "Marcello Herreshoff", "DeLesley Hutchins", "Peter Norvig"], "authorids": ["madscience@google.com", "marcelloh@google.com", "delesley@google.com", "pnorvig@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287575929, "id": "ICLR.cc/2017/conference/-/paper440/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "ryrGawqex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper440/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper440/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper440/reviewers", "ICLR.cc/2017/conference/paper440/areachairs"], "cdate": 1485287575929}}}, {"tddate": null, "tmdate": 1482951016990, "tcdate": 1482951016990, "number": 4, "id": "rJZ_Yt-rl", "invitation": "ICLR.cc/2017/conference/-/paper440/public/comment", "forum": "ryrGawqex", "replyto": "SyOL-J8Nx", "signatures": ["~Moshe_Looks1"], "readers": ["everyone"], "writers": ["~Moshe_Looks1"], "content": {"title": "creating TF graphs is currently super-slow and modification is incompatible with distributed training", "comment": "Thanks for the kind review. The reason that we don't compare to explicitly creating TF graphs for non-uniform batches is that this would entail modifying the graph in the course of training. There are two problems with this:\n\n1. TF graph creation/modification currently happens in pure Python, and is thus super slow\n\n2. In distributed training, TF does not allow the graph to be modified once training has commenced (so even if #1 were resolved in the future and graph creation were fast, it still wouldn't work at scale).\n\nBecause of #1 we were careful not to measure the cost of graph creation in any of our experiments. If/when #1 is resolved (i.e. with a C/C++ API for graph creation) then it will be feasible to perform the experiments you suggest within TF (and it might be possible to perform them today using some other framework), but I don't expect #2 to change anytime soon; the assumption of an entirely static graph for distributed training seems to be baked in pretty deeply to the framework.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Learning with Dynamic Computation Graphs", "abstract": "Neural networks that compute over graph structures are a natural fit for problems in a variety of domains, including natural language (parse trees) and cheminformatics (molecular graphs). However, since the computation graph has a different shape and size for every input, such networks do not directly support batched training or inference. They are also difficult to implement in popular deep learning libraries, which are based on static data-flow graphs. We introduce a technique called dynamic batching, which not only batches together operations between different input graphs of dissimilar shape, but also between different nodes within a single input graph. The technique allows us to create static graphs, using popular libraries, that emulate dynamic computation graphs of arbitrary shape and size. We further present a high-level library of compositional blocks that simplifies the creation of dynamic graph models. Using the library, we demonstrate concise and batch-wise parallel implementations for a variety of models from the literature.", "pdf": "/pdf/443b19c32d88a8ba6eb9d035f5bcff9c82320c01.pdf", "TL;DR": "We make batching effective and easy to use for neural nets where every input may have a different shape (e.g. TreeRNNs).", "paperhash": "looks|deep_learning_with_dynamic_computation_graphs", "keywords": ["Deep learning"], "conflicts": ["google.com"], "authors": ["Moshe Looks", "Marcello Herreshoff", "DeLesley Hutchins", "Peter Norvig"], "authorids": ["madscience@google.com", "marcelloh@google.com", "delesley@google.com", "pnorvig@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287576057, "id": "ICLR.cc/2017/conference/-/paper440/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ryrGawqex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper440/reviewers", "ICLR.cc/2017/conference/paper440/areachairs"], "cdate": 1485287576057}}}, {"tddate": null, "tmdate": 1482950218755, "tcdate": 1482950218755, "number": 3, "id": "SJXULKbHl", "invitation": "ICLR.cc/2017/conference/-/paper440/public/comment", "forum": "ryrGawqex", "replyto": "BJqeUcbVe", "signatures": ["~Moshe_Looks1"], "readers": ["everyone"], "writers": ["~Moshe_Looks1"], "content": {"title": "good points", "comment": "Thanks for the kind review. You are right that some of sec. 3 and especially the SST experimental results are a bit of a tangent. My motivation for including them was that, while the core of the paper is certainly dynamic batching and the speed results, we are also claiming that dynamic batching enables implementing deep learning models (which are growing ever more complex) at a higher level of abstraction (vs. manual batching) and thus a more rapid feedback loop for trying out novel model variants (and thus obtaining superior results). Besides the SST experiments, are there any parts of Section 3 in particular that you'd suggest dropping?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Learning with Dynamic Computation Graphs", "abstract": "Neural networks that compute over graph structures are a natural fit for problems in a variety of domains, including natural language (parse trees) and cheminformatics (molecular graphs). However, since the computation graph has a different shape and size for every input, such networks do not directly support batched training or inference. They are also difficult to implement in popular deep learning libraries, which are based on static data-flow graphs. We introduce a technique called dynamic batching, which not only batches together operations between different input graphs of dissimilar shape, but also between different nodes within a single input graph. The technique allows us to create static graphs, using popular libraries, that emulate dynamic computation graphs of arbitrary shape and size. We further present a high-level library of compositional blocks that simplifies the creation of dynamic graph models. Using the library, we demonstrate concise and batch-wise parallel implementations for a variety of models from the literature.", "pdf": "/pdf/443b19c32d88a8ba6eb9d035f5bcff9c82320c01.pdf", "TL;DR": "We make batching effective and easy to use for neural nets where every input may have a different shape (e.g. TreeRNNs).", "paperhash": "looks|deep_learning_with_dynamic_computation_graphs", "keywords": ["Deep learning"], "conflicts": ["google.com"], "authors": ["Moshe Looks", "Marcello Herreshoff", "DeLesley Hutchins", "Peter Norvig"], "authorids": ["madscience@google.com", "marcelloh@google.com", "delesley@google.com", "pnorvig@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287576057, "id": "ICLR.cc/2017/conference/-/paper440/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ryrGawqex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper440/reviewers", "ICLR.cc/2017/conference/paper440/areachairs"], "cdate": 1485287576057}}}, {"tddate": null, "tmdate": 1482905027636, "tcdate": 1482905027636, "number": 2, "id": "H126B0lBl", "invitation": "ICLR.cc/2017/conference/-/paper440/public/comment", "forum": "ryrGawqex", "replyto": "SkRHZS-Ee", "signatures": ["~Moshe_Looks1"], "readers": ["everyone"], "writers": ["~Moshe_Looks1"], "content": {"title": "ideas for better presentation, thoughts on real-world problems", "comment": "Thanks for the kind review.\n\nIn terms of improving the presentation / visualization, did you have anything specific in mind? One thing that we could add would be an illustration showing the unrolled wiring diagram that you get for a batch of inputs with different shapes - we've put one together at https://goo.gl/chKVtZ . My original thought was that it had too many boxes and arrows and would be more confusing than enlightening, but if you or the other reviewers think that it is useful then we could update the paper to include it.\n\nRegarding showing the effect of dynamic batching on a large real-world problem, I agree that this would help put the improvements in context. One caveat is that I don't think that dynamic batching has much to offer for classic seq-to-seq models where the computation graph is essentially a linear chain with no variation in structure from one example to the next. Were there particular ASR/SMT models that you had in mind where the shape of the computation graph varies from example to example? (parenthetically, applying dynamic batching and the TensorFlow Fold library to large real-world problem is *exactly* what our team is most focused on right now, although not currently ASR or SMT ... and not quite ready for publication yet, unfortunately)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Learning with Dynamic Computation Graphs", "abstract": "Neural networks that compute over graph structures are a natural fit for problems in a variety of domains, including natural language (parse trees) and cheminformatics (molecular graphs). However, since the computation graph has a different shape and size for every input, such networks do not directly support batched training or inference. They are also difficult to implement in popular deep learning libraries, which are based on static data-flow graphs. We introduce a technique called dynamic batching, which not only batches together operations between different input graphs of dissimilar shape, but also between different nodes within a single input graph. The technique allows us to create static graphs, using popular libraries, that emulate dynamic computation graphs of arbitrary shape and size. We further present a high-level library of compositional blocks that simplifies the creation of dynamic graph models. Using the library, we demonstrate concise and batch-wise parallel implementations for a variety of models from the literature.", "pdf": "/pdf/443b19c32d88a8ba6eb9d035f5bcff9c82320c01.pdf", "TL;DR": "We make batching effective and easy to use for neural nets where every input may have a different shape (e.g. TreeRNNs).", "paperhash": "looks|deep_learning_with_dynamic_computation_graphs", "keywords": ["Deep learning"], "conflicts": ["google.com"], "authors": ["Moshe Looks", "Marcello Herreshoff", "DeLesley Hutchins", "Peter Norvig"], "authorids": ["madscience@google.com", "marcelloh@google.com", "delesley@google.com", "pnorvig@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287576057, "id": "ICLR.cc/2017/conference/-/paper440/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ryrGawqex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper440/reviewers", "ICLR.cc/2017/conference/paper440/areachairs"], "cdate": 1485287576057}}}, {"tddate": null, "tmdate": 1482187088134, "tcdate": 1482187088134, "number": 3, "id": "SyOL-J8Nx", "invitation": "ICLR.cc/2017/conference/-/paper440/official/review", "forum": "ryrGawqex", "replyto": "ryrGawqex", "signatures": ["ICLR.cc/2017/conference/paper440/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper440/AnonReviewer3"], "content": {"title": "Description of a promising software package.", "rating": "7: Good paper, accept", "review": "Authors describe implementation of TensorFlow Fold which allows one to run various computations without modifying computation graph. They achieve this by creating a generic scheduler as a TensorFlow computation graph, which can accept graph description as input and execute it.\n\nThey show clear benefits to this approach for tasks where computation changes for each datapoint, such as the case with TreeRNN.\n\nIn the experiments, they compare against having static batch (same graph structure repeated many times) and batch size 1.\n\nThe reason my score is 7 and not higher is because they do not provide comparison to the main alternative of their method -- someone could create a new TensorFlow graph for each dynamic batch. In other words, instead of using their graph as the scheduling algorithm, one could explicitly create each non-uniform batch as a TensorFlow graph, and run that using standard TensorFlow.", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Learning with Dynamic Computation Graphs", "abstract": "Neural networks that compute over graph structures are a natural fit for problems in a variety of domains, including natural language (parse trees) and cheminformatics (molecular graphs). However, since the computation graph has a different shape and size for every input, such networks do not directly support batched training or inference. They are also difficult to implement in popular deep learning libraries, which are based on static data-flow graphs. We introduce a technique called dynamic batching, which not only batches together operations between different input graphs of dissimilar shape, but also between different nodes within a single input graph. The technique allows us to create static graphs, using popular libraries, that emulate dynamic computation graphs of arbitrary shape and size. We further present a high-level library of compositional blocks that simplifies the creation of dynamic graph models. Using the library, we demonstrate concise and batch-wise parallel implementations for a variety of models from the literature.", "pdf": "/pdf/443b19c32d88a8ba6eb9d035f5bcff9c82320c01.pdf", "TL;DR": "We make batching effective and easy to use for neural nets where every input may have a different shape (e.g. TreeRNNs).", "paperhash": "looks|deep_learning_with_dynamic_computation_graphs", "keywords": ["Deep learning"], "conflicts": ["google.com"], "authors": ["Moshe Looks", "Marcello Herreshoff", "DeLesley Hutchins", "Peter Norvig"], "authorids": ["madscience@google.com", "marcelloh@google.com", "delesley@google.com", "pnorvig@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512584806, "id": "ICLR.cc/2017/conference/-/paper440/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper440/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper440/AnonReviewer2", "ICLR.cc/2017/conference/paper440/AnonReviewer1", "ICLR.cc/2017/conference/paper440/AnonReviewer3"], "reply": {"forum": "ryrGawqex", "replyto": "ryrGawqex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper440/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper440/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512584806}}}, {"tddate": null, "tmdate": 1481883974552, "tcdate": 1481883974552, "number": 1, "id": "SkRHZS-Ee", "invitation": "ICLR.cc/2017/conference/-/paper440/official/review", "forum": "ryrGawqex", "replyto": "ryrGawqex", "signatures": ["ICLR.cc/2017/conference/paper440/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper440/AnonReviewer2"], "content": {"title": "A new method to optimize computation graphs", "rating": "8: Top 50% of accepted papers, clear accept", "review": "The paper describes a novel technique to improve the efficiency of computation graphs in deep learning frameworks. An impressive speedup can be observed in their implementation within TensorFlow. The content is presented with sufficient clarity, although some more graphical illustrations could be useful. This work is relevant in order to achieve highest performance in neural network training.\n\n\nPros:\n\n- significant speed improvements through dynamic batching\n- source code provided\n\n\nCons:\n\n- the effect on a large real-world (ASR, SMT) would allow the reader to put the improvements better into context\n- presentation/vizualisation can be improved ", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Learning with Dynamic Computation Graphs", "abstract": "Neural networks that compute over graph structures are a natural fit for problems in a variety of domains, including natural language (parse trees) and cheminformatics (molecular graphs). However, since the computation graph has a different shape and size for every input, such networks do not directly support batched training or inference. They are also difficult to implement in popular deep learning libraries, which are based on static data-flow graphs. We introduce a technique called dynamic batching, which not only batches together operations between different input graphs of dissimilar shape, but also between different nodes within a single input graph. The technique allows us to create static graphs, using popular libraries, that emulate dynamic computation graphs of arbitrary shape and size. We further present a high-level library of compositional blocks that simplifies the creation of dynamic graph models. Using the library, we demonstrate concise and batch-wise parallel implementations for a variety of models from the literature.", "pdf": "/pdf/443b19c32d88a8ba6eb9d035f5bcff9c82320c01.pdf", "TL;DR": "We make batching effective and easy to use for neural nets where every input may have a different shape (e.g. TreeRNNs).", "paperhash": "looks|deep_learning_with_dynamic_computation_graphs", "keywords": ["Deep learning"], "conflicts": ["google.com"], "authors": ["Moshe Looks", "Marcello Herreshoff", "DeLesley Hutchins", "Peter Norvig"], "authorids": ["madscience@google.com", "marcelloh@google.com", "delesley@google.com", "pnorvig@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512584806, "id": "ICLR.cc/2017/conference/-/paper440/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper440/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper440/AnonReviewer2", "ICLR.cc/2017/conference/paper440/AnonReviewer1", "ICLR.cc/2017/conference/paper440/AnonReviewer3"], "reply": {"forum": "ryrGawqex", "replyto": "ryrGawqex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper440/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper440/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512584806}}}, {"tddate": null, "tmdate": 1480722001299, "tcdate": 1480722001295, "number": 1, "id": "HJF8LKk7g", "invitation": "ICLR.cc/2017/conference/-/paper440/public/comment", "forum": "ryrGawqex", "replyto": "H15We-JXe", "signatures": ["~Moshe_Looks1"], "readers": ["everyone"], "writers": ["~Moshe_Looks1"], "content": {"title": "number of possible operations is assumed to be constant", "comment": "Hi, thanks for reviewing!  If we take m to be the number of possible operations then scaling is O(m * lg(n)), so the answer to your question is that we are treating m as a constant; e.g the experiments we are describing in this section m=2; the operations being LSTM (internal nodes) and embedding lookup (leaves). Certainly if e.g. every node is a different operation then there's no way to do any batching at all!\n\nTreating the number of possible operations as a constant seems reasonable, esp. noting that an operation can be composed of multiple ops e.g. a fully connected layer h(w*x + b) counts as one operation rather than 3 (h, *, +), and standard optimizations like operator fusion are possible as well."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Learning with Dynamic Computation Graphs", "abstract": "Neural networks that compute over graph structures are a natural fit for problems in a variety of domains, including natural language (parse trees) and cheminformatics (molecular graphs). However, since the computation graph has a different shape and size for every input, such networks do not directly support batched training or inference. They are also difficult to implement in popular deep learning libraries, which are based on static data-flow graphs. We introduce a technique called dynamic batching, which not only batches together operations between different input graphs of dissimilar shape, but also between different nodes within a single input graph. The technique allows us to create static graphs, using popular libraries, that emulate dynamic computation graphs of arbitrary shape and size. We further present a high-level library of compositional blocks that simplifies the creation of dynamic graph models. Using the library, we demonstrate concise and batch-wise parallel implementations for a variety of models from the literature.", "pdf": "/pdf/443b19c32d88a8ba6eb9d035f5bcff9c82320c01.pdf", "TL;DR": "We make batching effective and easy to use for neural nets where every input may have a different shape (e.g. TreeRNNs).", "paperhash": "looks|deep_learning_with_dynamic_computation_graphs", "keywords": ["Deep learning"], "conflicts": ["google.com"], "authors": ["Moshe Looks", "Marcello Herreshoff", "DeLesley Hutchins", "Peter Norvig"], "authorids": ["madscience@google.com", "marcelloh@google.com", "delesley@google.com", "pnorvig@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287576057, "id": "ICLR.cc/2017/conference/-/paper440/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ryrGawqex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper440/reviewers", "ICLR.cc/2017/conference/paper440/areachairs"], "cdate": 1485287576057}}}, {"tddate": null, "tmdate": 1480687618465, "tcdate": 1480687618461, "number": 1, "id": "H15We-JXe", "invitation": "ICLR.cc/2017/conference/-/paper440/pre-review/question", "forum": "ryrGawqex", "replyto": "ryrGawqex", "signatures": ["ICLR.cc/2017/conference/paper440/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper440/AnonReviewer1"], "content": {"title": "Scaling of dynamic batching", "question": "In the paper, you state on p. 3, last paragraph, that \"Dynamic batching instantiates each operation only once, and invokes it once for each depth, so the number of kernel invocations is log(n), rather than n, where n is tree size\". Probably I just did not understand a detail, but I do not see how the number of kernel invocations could scale with log(n) in arbitrary cases. Couldn't it (hypothetically) happen that at each tree level a different operation is invoked? In that case, it would still scale linearly with tree depth, or do I misunderstand?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Learning with Dynamic Computation Graphs", "abstract": "Neural networks that compute over graph structures are a natural fit for problems in a variety of domains, including natural language (parse trees) and cheminformatics (molecular graphs). However, since the computation graph has a different shape and size for every input, such networks do not directly support batched training or inference. They are also difficult to implement in popular deep learning libraries, which are based on static data-flow graphs. We introduce a technique called dynamic batching, which not only batches together operations between different input graphs of dissimilar shape, but also between different nodes within a single input graph. The technique allows us to create static graphs, using popular libraries, that emulate dynamic computation graphs of arbitrary shape and size. We further present a high-level library of compositional blocks that simplifies the creation of dynamic graph models. Using the library, we demonstrate concise and batch-wise parallel implementations for a variety of models from the literature.", "pdf": "/pdf/443b19c32d88a8ba6eb9d035f5bcff9c82320c01.pdf", "TL;DR": "We make batching effective and easy to use for neural nets where every input may have a different shape (e.g. TreeRNNs).", "paperhash": "looks|deep_learning_with_dynamic_computation_graphs", "keywords": ["Deep learning"], "conflicts": ["google.com"], "authors": ["Moshe Looks", "Marcello Herreshoff", "DeLesley Hutchins", "Peter Norvig"], "authorids": ["madscience@google.com", "marcelloh@google.com", "delesley@google.com", "pnorvig@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959278759, "id": "ICLR.cc/2017/conference/-/paper440/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper440/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper440/AnonReviewer1"], "reply": {"forum": "ryrGawqex", "replyto": "ryrGawqex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper440/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper440/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959278759}}}], "count": 15}