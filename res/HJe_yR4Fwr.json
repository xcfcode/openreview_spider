{"notes": [{"id": "HJe_yR4Fwr", "original": "S1l-GqMuvB", "number": 898, "cdate": 1569439200056, "ddate": null, "tcdate": 1569439200056, "tmdate": 1583912051969, "tddate": null, "forum": "HJe_yR4Fwr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["colinwei@stanford.edu", "tengyuma@cs.stanford.edu"], "title": "Improved Sample Complexities for Deep Neural Networks and Robust Classification via an All-Layer Margin", "authors": ["Colin Wei", "Tengyu Ma"], "pdf": "/pdf/68a22f3da586dac673478e301763f5516fdf3f10.pdf", "TL;DR": "We propose a new notion of margin that has a direct relationship with neural net generalization, and obtain improved generalization bounds for neural nets and robust classification by analyzing this margin.", "abstract": "For linear classifiers, the relationship between (normalized) output margin and generalization is captured in a clear and simple bound \u2013 a large output margin implies good generalization. Unfortunately, for deep models, this relationship is less clear: existing analyses of the output margin give complicated bounds which sometimes depend exponentially on depth. In this work, we propose to instead analyze a new notion of margin, which we call the \u201call-layer margin.\u201d Our analysis reveals that the all-layer margin has a clear and direct relationship with generalization for deep models. This enables the following concrete applications of the all-layer margin: 1) by analyzing the all-layer margin, we obtain tighter generalization bounds for neural nets which depend on Jacobian and hidden layer norms and remove the exponential dependency on depth 2) our neural net results easily translate to the adversarially robust setting, giving the first direct analysis of robust test error for deep networks, and 3) we present a theoretically inspired training algorithm for increasing the all-layer margin. Our algorithm improves both clean and adversarially robust test performance over strong baselines in practice.", "keywords": ["deep learning theory", "generalization bounds", "adversarially robust generalization", "data-dependent generalization bounds"], "paperhash": "wei|improved_sample_complexities_for_deep_neural_networks_and_robust_classification_via_an_alllayer_margin", "_bibtex": "@inproceedings{\nWei2020Improved,\ntitle={Improved Sample Complexities for Deep Neural Networks and Robust Classification via an All-Layer Margin},\nauthor={Colin Wei and Tengyu Ma},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJe_yR4Fwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/3e0278730e9ebd1eceb363e4af77390c6795935d.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "yyWlCpuSTl", "original": null, "number": 1, "cdate": 1576798709095, "ddate": null, "tcdate": 1576798709095, "tmdate": 1576800927270, "tddate": null, "forum": "HJe_yR4Fwr", "replyto": "HJe_yR4Fwr", "invitation": "ICLR.cc/2020/Conference/Paper898/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "This works presents a new and interesting notion of margin for deep neural networks (that incorporates representation at all layers). It then develops generalization bounds based on the introduced margin. The reviewers pointed some concerns, including some notation issues, complexity in case of residual networks, removal of exponential dependence on depth,  and dependence on a hard to compute quantity - \\kapp^{adv}. Some of these concerns were addressed by the authors. At the end, most of the reviewers find the notion of all-layer margin introduced in this paper a very novel and promising idea for characterizing generalization in deep networks. Agreeing with reviewers, I recommend accept. However, I request the authors to accommodate remaining comments /concerns raised by R1 in the final version of your paper. In particular, in your response to R1 you mentioned for one case you saw improvement even with dropout, but that is not mentioned in the revision; Please include related details in the draft.\n", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["colinwei@stanford.edu", "tengyuma@cs.stanford.edu"], "title": "Improved Sample Complexities for Deep Neural Networks and Robust Classification via an All-Layer Margin", "authors": ["Colin Wei", "Tengyu Ma"], "pdf": "/pdf/68a22f3da586dac673478e301763f5516fdf3f10.pdf", "TL;DR": "We propose a new notion of margin that has a direct relationship with neural net generalization, and obtain improved generalization bounds for neural nets and robust classification by analyzing this margin.", "abstract": "For linear classifiers, the relationship between (normalized) output margin and generalization is captured in a clear and simple bound \u2013 a large output margin implies good generalization. Unfortunately, for deep models, this relationship is less clear: existing analyses of the output margin give complicated bounds which sometimes depend exponentially on depth. In this work, we propose to instead analyze a new notion of margin, which we call the \u201call-layer margin.\u201d Our analysis reveals that the all-layer margin has a clear and direct relationship with generalization for deep models. This enables the following concrete applications of the all-layer margin: 1) by analyzing the all-layer margin, we obtain tighter generalization bounds for neural nets which depend on Jacobian and hidden layer norms and remove the exponential dependency on depth 2) our neural net results easily translate to the adversarially robust setting, giving the first direct analysis of robust test error for deep networks, and 3) we present a theoretically inspired training algorithm for increasing the all-layer margin. Our algorithm improves both clean and adversarially robust test performance over strong baselines in practice.", "keywords": ["deep learning theory", "generalization bounds", "adversarially robust generalization", "data-dependent generalization bounds"], "paperhash": "wei|improved_sample_complexities_for_deep_neural_networks_and_robust_classification_via_an_alllayer_margin", "_bibtex": "@inproceedings{\nWei2020Improved,\ntitle={Improved Sample Complexities for Deep Neural Networks and Robust Classification via an All-Layer Margin},\nauthor={Colin Wei and Tengyu Ma},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJe_yR4Fwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/3e0278730e9ebd1eceb363e4af77390c6795935d.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "HJe_yR4Fwr", "replyto": "HJe_yR4Fwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795712687, "tmdate": 1576800262121, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper898/-/Decision"}}}, {"id": "BklNuffTtB", "original": null, "number": 1, "cdate": 1571787372255, "ddate": null, "tcdate": 1571787372255, "tmdate": 1574638894109, "tddate": null, "forum": "HJe_yR4Fwr", "replyto": "HJe_yR4Fwr", "invitation": "ICLR.cc/2020/Conference/Paper898/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #1", "review": "This paper proves generalization bounds for Neural networks in terms of all layer margins. All layer margins are the smallest relative perturbations  of outputs of each layer, that result in misclassification. This new quantity allows to show generalization bounds that scales as sum of complexities of each layer and inversely scales with the all layer margin. The resulting bound does not have an explicit exponential dependence on depth, unlike some earlier bounds in terms of output margin.\n\nThe paper makes strong claims in the abstract and introduction that the analysis removes exponential dependency of capacity on depth. However I don\u2019t think this is possible in the worst case. The exponential dependency is now hidden in the all layer margin (please correct me if I am mistaken). Note that the bound could still be interesting if this quantity is small for real world networks of interest. However the paper currently lacks this discussion. The paper need to make it clear that the bound only avoids explicit exponential dependence on depth. This brings me to another major drawback of the all layer margin. \n\nThis quantity is not computable as it requires solving a min-max problem, to find the best perturbation for each data point. So we cannot easily test if real networks that generalize well do have smaller all layer margin. The paper provides a lower bound for this quantity in terms of layer norms, network Jacobian norms, and output margin - which can be large. While all layer margin is certainly interesting from analysis perspective, as it allows to show bounds that are relatively cleaner, and behaves well with composition functions, it is not clear to me that it doesn\u2019t decrease exponentially for deeper networks. A discussion on this can help the paper. \n\nThis paper is along the recent line of work on data dependent generalization bounds by Nagarajan  & Kolter 2019, Wei & Ma 2019.  Note that these earlier papers also prove generalization bounds that also do not have explicit exponential dependence on depth. This point also needs to be made clearer. \n\nGiven the adversarial nature of the definition of the all layer margin, the same framework is used to show a robust generalization bound, which is nice. However, the bound again depends on a hard to compute quantity - \\kapp^{adv}, making it less clear about the utility of such a bound.\n\nFinally the paper presents experiments, where to encourage higher all layer margin, the paper relies on min max gradient updates, similar to adversarial training, and shows better performance with this. I am curious if authors experiments with dropout, as it also adds \u201cperturbations\u201d to the output activations of each layer, and if the gains are orthogonal to dropout. Also how does the performance of AMO compared with  adversarial training methods (PGD) with small \\epsilon? The experiments currently are lacking to compare AMO with existing techniques.\n\nOverall I believe the paper needs to be rewritten with correctly stated contributions and need a more nuanced discussion of the benefits and drawbacks of the all layer margin and the  corresponding generalization bounds. \n\n** Post response comments **\nAuthors have clarified some of my earlier concerns. But the paper still requires rewriting to properly emphasize that it does not avoid exponential dependence on depth, but rather avoids an explicit dependence. Also there needs to be discussion about the computability of  \\kapp^{adv}. Finally adding experiments with dropout should make it more clear about the advantages of the proposed training method. Because the current draft does not address these yet, I am leaving my score at 3. My actual rating is more on the borderline (5), but the current system does not allow such scores. \n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper898/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper898/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["colinwei@stanford.edu", "tengyuma@cs.stanford.edu"], "title": "Improved Sample Complexities for Deep Neural Networks and Robust Classification via an All-Layer Margin", "authors": ["Colin Wei", "Tengyu Ma"], "pdf": "/pdf/68a22f3da586dac673478e301763f5516fdf3f10.pdf", "TL;DR": "We propose a new notion of margin that has a direct relationship with neural net generalization, and obtain improved generalization bounds for neural nets and robust classification by analyzing this margin.", "abstract": "For linear classifiers, the relationship between (normalized) output margin and generalization is captured in a clear and simple bound \u2013 a large output margin implies good generalization. Unfortunately, for deep models, this relationship is less clear: existing analyses of the output margin give complicated bounds which sometimes depend exponentially on depth. In this work, we propose to instead analyze a new notion of margin, which we call the \u201call-layer margin.\u201d Our analysis reveals that the all-layer margin has a clear and direct relationship with generalization for deep models. This enables the following concrete applications of the all-layer margin: 1) by analyzing the all-layer margin, we obtain tighter generalization bounds for neural nets which depend on Jacobian and hidden layer norms and remove the exponential dependency on depth 2) our neural net results easily translate to the adversarially robust setting, giving the first direct analysis of robust test error for deep networks, and 3) we present a theoretically inspired training algorithm for increasing the all-layer margin. Our algorithm improves both clean and adversarially robust test performance over strong baselines in practice.", "keywords": ["deep learning theory", "generalization bounds", "adversarially robust generalization", "data-dependent generalization bounds"], "paperhash": "wei|improved_sample_complexities_for_deep_neural_networks_and_robust_classification_via_an_alllayer_margin", "_bibtex": "@inproceedings{\nWei2020Improved,\ntitle={Improved Sample Complexities for Deep Neural Networks and Robust Classification via an All-Layer Margin},\nauthor={Colin Wei and Tengyu Ma},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJe_yR4Fwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/3e0278730e9ebd1eceb363e4af77390c6795935d.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJe_yR4Fwr", "replyto": "HJe_yR4Fwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper898/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper898/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576193631442, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper898/Reviewers"], "noninvitees": [], "tcdate": 1570237745378, "tmdate": 1576193631462, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper898/-/Official_Review"}}}, {"id": "SkeSsuF3jr", "original": null, "number": 14, "cdate": 1573849245077, "ddate": null, "tcdate": 1573849245077, "tmdate": 1573849389775, "tddate": null, "forum": "HJe_yR4Fwr", "replyto": "SyegBsdnjB", "invitation": "ICLR.cc/2020/Conference/Paper898/-/Official_Comment", "content": {"title": "VGG-19 Improvement", "comment": "Thank you for reading our response. It is indeed interesting that AMO doesn\u2019t improve VGG as much as it does WideResNet models. We suspect that this is partially due to the fact that WideResNet models have larger capacity, but we agree that this is a question worthy of future investigation. However, we will note that the improvement is still there --- our algorithm improves VGG-19 by around ~0.6% for CIFAR10, which is only ~0.1% less than the improvement for our WideResNet-16-10 model and ~0.2% less than the improvement for our WideResNet-28-10 model for CIFAR10.  "}, "signatures": ["ICLR.cc/2020/Conference/Paper898/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper898/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["colinwei@stanford.edu", "tengyuma@cs.stanford.edu"], "title": "Improved Sample Complexities for Deep Neural Networks and Robust Classification via an All-Layer Margin", "authors": ["Colin Wei", "Tengyu Ma"], "pdf": "/pdf/68a22f3da586dac673478e301763f5516fdf3f10.pdf", "TL;DR": "We propose a new notion of margin that has a direct relationship with neural net generalization, and obtain improved generalization bounds for neural nets and robust classification by analyzing this margin.", "abstract": "For linear classifiers, the relationship between (normalized) output margin and generalization is captured in a clear and simple bound \u2013 a large output margin implies good generalization. Unfortunately, for deep models, this relationship is less clear: existing analyses of the output margin give complicated bounds which sometimes depend exponentially on depth. In this work, we propose to instead analyze a new notion of margin, which we call the \u201call-layer margin.\u201d Our analysis reveals that the all-layer margin has a clear and direct relationship with generalization for deep models. This enables the following concrete applications of the all-layer margin: 1) by analyzing the all-layer margin, we obtain tighter generalization bounds for neural nets which depend on Jacobian and hidden layer norms and remove the exponential dependency on depth 2) our neural net results easily translate to the adversarially robust setting, giving the first direct analysis of robust test error for deep networks, and 3) we present a theoretically inspired training algorithm for increasing the all-layer margin. Our algorithm improves both clean and adversarially robust test performance over strong baselines in practice.", "keywords": ["deep learning theory", "generalization bounds", "adversarially robust generalization", "data-dependent generalization bounds"], "paperhash": "wei|improved_sample_complexities_for_deep_neural_networks_and_robust_classification_via_an_alllayer_margin", "_bibtex": "@inproceedings{\nWei2020Improved,\ntitle={Improved Sample Complexities for Deep Neural Networks and Robust Classification via an All-Layer Margin},\nauthor={Colin Wei and Tengyu Ma},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJe_yR4Fwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/3e0278730e9ebd1eceb363e4af77390c6795935d.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJe_yR4Fwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper898/Authors", "ICLR.cc/2020/Conference/Paper898/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper898/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper898/Reviewers", "ICLR.cc/2020/Conference/Paper898/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper898/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper898/Authors|ICLR.cc/2020/Conference/Paper898/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504164504, "tmdate": 1576860556094, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper898/Authors", "ICLR.cc/2020/Conference/Paper898/Reviewers", "ICLR.cc/2020/Conference/Paper898/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper898/-/Official_Comment"}}}, {"id": "SyegBsdnjB", "original": null, "number": 13, "cdate": 1573845816329, "ddate": null, "tcdate": 1573845816329, "tmdate": 1573845816329, "tddate": null, "forum": "HJe_yR4Fwr", "replyto": "r1gLaBqjoS", "invitation": "ICLR.cc/2020/Conference/Paper898/-/Official_Comment", "content": {"title": "Thank you for the response.", "comment": "I thank the authors for the detailed response and new experiments despite that fact that I already recommended acceptance. All of my questions have been properly addressed.\nIt is interesting to see that the regularizer did not improve VGG-19 by a significant amount. It can be worthwhile to investigate why in the future work."}, "signatures": ["ICLR.cc/2020/Conference/Paper898/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper898/AnonReviewer2", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["colinwei@stanford.edu", "tengyuma@cs.stanford.edu"], "title": "Improved Sample Complexities for Deep Neural Networks and Robust Classification via an All-Layer Margin", "authors": ["Colin Wei", "Tengyu Ma"], "pdf": "/pdf/68a22f3da586dac673478e301763f5516fdf3f10.pdf", "TL;DR": "We propose a new notion of margin that has a direct relationship with neural net generalization, and obtain improved generalization bounds for neural nets and robust classification by analyzing this margin.", "abstract": "For linear classifiers, the relationship between (normalized) output margin and generalization is captured in a clear and simple bound \u2013 a large output margin implies good generalization. Unfortunately, for deep models, this relationship is less clear: existing analyses of the output margin give complicated bounds which sometimes depend exponentially on depth. In this work, we propose to instead analyze a new notion of margin, which we call the \u201call-layer margin.\u201d Our analysis reveals that the all-layer margin has a clear and direct relationship with generalization for deep models. This enables the following concrete applications of the all-layer margin: 1) by analyzing the all-layer margin, we obtain tighter generalization bounds for neural nets which depend on Jacobian and hidden layer norms and remove the exponential dependency on depth 2) our neural net results easily translate to the adversarially robust setting, giving the first direct analysis of robust test error for deep networks, and 3) we present a theoretically inspired training algorithm for increasing the all-layer margin. Our algorithm improves both clean and adversarially robust test performance over strong baselines in practice.", "keywords": ["deep learning theory", "generalization bounds", "adversarially robust generalization", "data-dependent generalization bounds"], "paperhash": "wei|improved_sample_complexities_for_deep_neural_networks_and_robust_classification_via_an_alllayer_margin", "_bibtex": "@inproceedings{\nWei2020Improved,\ntitle={Improved Sample Complexities for Deep Neural Networks and Robust Classification via an All-Layer Margin},\nauthor={Colin Wei and Tengyu Ma},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJe_yR4Fwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/3e0278730e9ebd1eceb363e4af77390c6795935d.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJe_yR4Fwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper898/Authors", "ICLR.cc/2020/Conference/Paper898/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper898/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper898/Reviewers", "ICLR.cc/2020/Conference/Paper898/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper898/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper898/Authors|ICLR.cc/2020/Conference/Paper898/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504164504, "tmdate": 1576860556094, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper898/Authors", "ICLR.cc/2020/Conference/Paper898/Reviewers", "ICLR.cc/2020/Conference/Paper898/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper898/-/Official_Comment"}}}, {"id": "SyxFeT5jiB", "original": null, "number": 12, "cdate": 1573788913126, "ddate": null, "tcdate": 1573788913126, "tmdate": 1573788913126, "tddate": null, "forum": "HJe_yR4Fwr", "replyto": "HJe_yR4Fwr", "invitation": "ICLR.cc/2020/Conference/Paper898/-/Official_Comment", "content": {"title": "Summary of Revisions", "comment": "Besides incorporating the specific feedback of the reviewers, the main change we made in the revision was demonstrating that AMO (our proposed regularization algorithm) can improve performance for robust classification. We combine AMO and the robust training algorithm of [Madry et al.\u201917] in the robust classification setting. In Section 5, Table 2, we present preliminary experimental results on CIFAR10 demonstrating that our robust AMO algorithm achieves ~6% improvement in robustness on a PGD attack model over the baseline algorithm of [Madry et. al\u201917]. This validates our theoretical results in Section 4 and suggests that AMO is a promising procedure for improving adversarial robustness. "}, "signatures": ["ICLR.cc/2020/Conference/Paper898/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper898/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["colinwei@stanford.edu", "tengyuma@cs.stanford.edu"], "title": "Improved Sample Complexities for Deep Neural Networks and Robust Classification via an All-Layer Margin", "authors": ["Colin Wei", "Tengyu Ma"], "pdf": "/pdf/68a22f3da586dac673478e301763f5516fdf3f10.pdf", "TL;DR": "We propose a new notion of margin that has a direct relationship with neural net generalization, and obtain improved generalization bounds for neural nets and robust classification by analyzing this margin.", "abstract": "For linear classifiers, the relationship between (normalized) output margin and generalization is captured in a clear and simple bound \u2013 a large output margin implies good generalization. Unfortunately, for deep models, this relationship is less clear: existing analyses of the output margin give complicated bounds which sometimes depend exponentially on depth. In this work, we propose to instead analyze a new notion of margin, which we call the \u201call-layer margin.\u201d Our analysis reveals that the all-layer margin has a clear and direct relationship with generalization for deep models. This enables the following concrete applications of the all-layer margin: 1) by analyzing the all-layer margin, we obtain tighter generalization bounds for neural nets which depend on Jacobian and hidden layer norms and remove the exponential dependency on depth 2) our neural net results easily translate to the adversarially robust setting, giving the first direct analysis of robust test error for deep networks, and 3) we present a theoretically inspired training algorithm for increasing the all-layer margin. Our algorithm improves both clean and adversarially robust test performance over strong baselines in practice.", "keywords": ["deep learning theory", "generalization bounds", "adversarially robust generalization", "data-dependent generalization bounds"], "paperhash": "wei|improved_sample_complexities_for_deep_neural_networks_and_robust_classification_via_an_alllayer_margin", "_bibtex": "@inproceedings{\nWei2020Improved,\ntitle={Improved Sample Complexities for Deep Neural Networks and Robust Classification via an All-Layer Margin},\nauthor={Colin Wei and Tengyu Ma},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJe_yR4Fwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/3e0278730e9ebd1eceb363e4af77390c6795935d.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJe_yR4Fwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper898/Authors", "ICLR.cc/2020/Conference/Paper898/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper898/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper898/Reviewers", "ICLR.cc/2020/Conference/Paper898/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper898/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper898/Authors|ICLR.cc/2020/Conference/Paper898/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504164504, "tmdate": 1576860556094, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper898/Authors", "ICLR.cc/2020/Conference/Paper898/Reviewers", "ICLR.cc/2020/Conference/Paper898/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper898/-/Official_Comment"}}}, {"id": "BJx2Bn5ojS", "original": null, "number": 11, "cdate": 1573788740040, "ddate": null, "tcdate": 1573788740040, "tmdate": 1573788740040, "tddate": null, "forum": "HJe_yR4Fwr", "replyto": "BklNuffTtB", "invitation": "ICLR.cc/2020/Conference/Paper898/-/Official_Comment", "content": {"title": "Response", "comment": "We thank the reviewer for the comments, which we will incorporate into the final version of our paper. \n\nA concern raised in the review is that our generalization bounds could still suffer severe (exponential) worst-case dependencies on depth. This concern also applies to the prior work of [Wei and Ma\u201919, Nagarajan and Kolter\u201919], as our bound in Theorem 3.1 has (tighter) dependencies on the same data-dependent quantities as theirs. We would address this concern by using the same argument as these prior works --- for networks trained in practice, the dependencies of these bounds on depth will be mild, as the quantities in these bounds will be well-behaved. For example, [Arora et. al\u201918, Nagarajan and Kolter\u201919] empirically observe networks trained in practice to have good Lipschitz constants and stability on the training sample. \n\nIn fact, *in the worst case*, it would appear that some sort of exponential depth dependency is unavoidable, as Bartlett et. al\u201917 and Golowich et. al\u201917 lower bound the worst-case complexity of neural networks with products of the weight matrix norms. The goal of our work (and prior work [Wei and Ma\u201919, Nagarajan and Kolter\u201919]) is to identify properties which result in good generalization of neural networks *in practice*, not in the worst case. Our theory and experiments identify the all-layer margin as such a property.\n \nResponses to specific points below:\n\n--- \u201cclaims ... that the analysis removes exponential dependency of capacity on depth ... don\u2019t think this is possible in the worst case.\u201d \n--- \u201cThe paper need to make it clear that the bound only avoids explicit exponential dependence on depth.\u201d\n--- \u201cNote that the bound could still be interesting if this quantity is small for real world networks of interest. However the paper currently lacks this discussion.\u201d \n--- \u201cThe paper provides a lower bound for this quantity in terms of layer norms, network Jacobian norms, and output margin - which can be large.\u201d \n\nAs noted above, avoiding an *explicit* exponential dependency is likely the best we can hope for, since lower bounds imply that complexity will be exponential in depth in the worst case. For networks trained in practice, our all-layer margin bound will be much better than the worst case. In Section 3, we analytically lower bound the all-layer margin in terms of the hidden layer and Jacobian norms computed on the training data, which were demonstrated to be well-behaved in practice by [Arora et. al\u201918, Nagarajan and Kolter\u201919].\n\nIn our revision, we have added more qualifiers emphasizing that the bound will be small if the Jacobian and hidden layer norms on the training data are small (as is the case in practice). We also note in the revision that the Jacobian norms, hidden layer norms, and output margin will be well-behaved in practice in the first paragraph of Section 3. \n\n--- \u201crobust generalization bound, which is nice ... depends on a hard to compute quantity - \\kapp^{adv}, making it less clear about the utility of such a bound.\u201d\n\nThe quantity \\kappa^{adv} depends only on the Jacobian and hidden layer norms and output margin in the adversarial perturbation neighborhood, and our bound suggests that regularizing these quantities during training could improve the robust generalization. In Section 5 Table 2 of the revision, we present preliminary experiments confirming this claim: our AMO algorithm improves upon the baseline method [Madry et. al \u201817] by around 6% in robust classification accuracy. \n\n--- \u201cI am curious if authors experiments with dropout, as it also adds \u201cperturbations\u201d to the output activations of each layer, and if the gains are orthogonal to dropout.\u201d\n\nWe experimented with using dropout for the same WideResNet architectures and obtained ~0.2% accuracy improvement after tuning. Thus, our AMO algorithm outperforms dropout by at least ~0.5%, indicating that the adversarial (as opposed to random, for dropout) nature of the AMO perturbation results in better generalization. We also found that AMO + dropout performed worse than our models trained with only AMO, though it still improved over models trained without AMO. We suspect that combining these two regularizers inhibits training too much. \n\n--- \u201cAlso how does the performance of AMO compared with adversarial training methods (PGD) with small \\epsilon?\u201d \n\nFor the clean classification setting, we are not aware of any work demonstrating that adversarial training methods can improve clean test accuracy. In fact, it has been observed that robust training can often hurt clean classification [Tsipras et. al\u201919, Zhang et. al\u201919, Raghunathan et. al\u201919]. \n\nFor robust classification, in Section 5, Table 2 of the revision, we provide preliminary results showing that AMO gives around 6% percent robust accuracy gains over adversarial training baselines (PGD, [Madry et. al\u201917]). For the final version of the paper, we will compare against additional baselines and attack models in the adversarially robust setting."}, "signatures": ["ICLR.cc/2020/Conference/Paper898/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper898/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["colinwei@stanford.edu", "tengyuma@cs.stanford.edu"], "title": "Improved Sample Complexities for Deep Neural Networks and Robust Classification via an All-Layer Margin", "authors": ["Colin Wei", "Tengyu Ma"], "pdf": "/pdf/68a22f3da586dac673478e301763f5516fdf3f10.pdf", "TL;DR": "We propose a new notion of margin that has a direct relationship with neural net generalization, and obtain improved generalization bounds for neural nets and robust classification by analyzing this margin.", "abstract": "For linear classifiers, the relationship between (normalized) output margin and generalization is captured in a clear and simple bound \u2013 a large output margin implies good generalization. Unfortunately, for deep models, this relationship is less clear: existing analyses of the output margin give complicated bounds which sometimes depend exponentially on depth. In this work, we propose to instead analyze a new notion of margin, which we call the \u201call-layer margin.\u201d Our analysis reveals that the all-layer margin has a clear and direct relationship with generalization for deep models. This enables the following concrete applications of the all-layer margin: 1) by analyzing the all-layer margin, we obtain tighter generalization bounds for neural nets which depend on Jacobian and hidden layer norms and remove the exponential dependency on depth 2) our neural net results easily translate to the adversarially robust setting, giving the first direct analysis of robust test error for deep networks, and 3) we present a theoretically inspired training algorithm for increasing the all-layer margin. Our algorithm improves both clean and adversarially robust test performance over strong baselines in practice.", "keywords": ["deep learning theory", "generalization bounds", "adversarially robust generalization", "data-dependent generalization bounds"], "paperhash": "wei|improved_sample_complexities_for_deep_neural_networks_and_robust_classification_via_an_alllayer_margin", "_bibtex": "@inproceedings{\nWei2020Improved,\ntitle={Improved Sample Complexities for Deep Neural Networks and Robust Classification via an All-Layer Margin},\nauthor={Colin Wei and Tengyu Ma},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJe_yR4Fwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/3e0278730e9ebd1eceb363e4af77390c6795935d.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJe_yR4Fwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper898/Authors", "ICLR.cc/2020/Conference/Paper898/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper898/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper898/Reviewers", "ICLR.cc/2020/Conference/Paper898/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper898/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper898/Authors|ICLR.cc/2020/Conference/Paper898/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504164504, "tmdate": 1576860556094, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper898/Authors", "ICLR.cc/2020/Conference/Paper898/Reviewers", "ICLR.cc/2020/Conference/Paper898/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper898/-/Official_Comment"}}}, {"id": "r1gLaBqjoS", "original": null, "number": 10, "cdate": 1573787070388, "ddate": null, "tcdate": 1573787070388, "tmdate": 1573787070388, "tddate": null, "forum": "HJe_yR4Fwr", "replyto": "rye0FNG0tB", "invitation": "ICLR.cc/2020/Conference/Paper898/-/Official_Comment", "content": {"title": "Response", "comment": "We thank the reviewer for the comments and feedback. The review noted that our paper \u201cis a valuable contribution both for theory and practices\u201d and \u201crelatively easy to follow for a topic so technically involved.\u201d We will incorporate feedback and comments into the final version of our paper. \n\nResponses to specific points below:\n\n--- \u201cI am a little confused on the role of section 3 \u2026 Is this bound just for comparison with previous work or provide a tractable upper bound?\u201d\n\nThe purpose of Section 3 is to show that the all-layer margin can be used to derive tighter generalization bounds depending on *known and computable* quantities --- our revision clarifies this point in the first paragraph of Section 3. Although the all-layer margin is likely hard to compute, Theorem 3.1 demonstrates that we can bound it by a function of the Jacobian norms, hidden layer norms, and output margin. This also lets us demonstrate that our bound improves upon prior work [Nagarajan and Kolter\u201919, Wei and Ma\u201919], which had worse dependencies on these same quantities. This also suggests that large all-layer margin intuitively encourages better Lipschitz-ness of the model. \n\n--- \u201cIn lemma B.1, why are there 2 reference matrices and how should they be chosen?\u201d\n\nOur bound measures the complexity of the weight matrices in two norms, Frobenius norm and (1, 1) norm, so the use of two reference matrices just allows additional flexibility in computing the bound. In most of the cases, A_{(i)} and B_{(i)} can be chosen to be 0. If one has a prior belief before training that the weights will be close to some matrix A_{(i)} in Frobenius norm or a different matrix B_{(i)} in (1,1)-norm, then we would set the reference matrices to those values. (E.g., in certain cases, one may have the prior belief that the weight matrix is close to the identity matrix.) \n \n--- \u201cIn a feedforward neural network, a single \\mathcal{F}_i is just a linear operator which has pretty small complexity, but how about networks with residual connections? ... since the proposed AMO is applied to residual networks, I want to hear the authors thought on this.\u201d\n\nIn standard resnets, the residual connections will not add any complexity since they are fixed functions, so the complexities of the individual layers will remain the same as in the feedforward case. \n\nIt is possible to extend the all-layer margin beyond feedforward networks to networks with arbitrary computational graphs. For ResNet, the resulting bound would look like the sum of the complexities of the weight matrices normalized by this generalized all-layer margin, which matches the form of the bound for the feedforward case. The simplest way to perform this extension is to have a perturbation \\delta at every node in the computational graph which is scaled by the norm of all its inputs. \n\n--- \u201cperhaps it would be nice to have some experiments on conventional feedforward networks\u201d\n\nFor the VGG-19 architecture on CIFAR10, our AMO algorithm improves over standard SGD by around 0.5% accuracy. (See Table 3 in Section F.1 of the revision.)\n\n--- \u201capplied the proposed algorithm to all layers instead of skipping the contents of the residual block.\u201d\n\nOur AMO experiments with WideResNet do in fact apply the perturbations after all the convolution layers and don\u2019t skip the contents of the residual block. \n\n--- \u201cCan the bound be accurately approximated? \u2026 If so, do the authors expect it to be non-vacuous?\u201d\n\nIt does not appear that the all-layer margin can be accurately approximated \u2014- doing so would require solving a seemingly intractable optimization problem. It also seems unlikely that the bound would be non-vacuous, since there are no known non-vacuous bounds for the  architectures and training algorithms used in practice (such as ResNets trained with vanilla SGD). Existing non-vacuous bounds appear to require specialized procedures both for obtaining the network and tuning the generalization bound [Dziugaite and Roy\u201917, Zhou et. al\u201919]. \n\nOur view is also that a generalization bound does not need to be non-vacuous for it to be meaningful or informative, as the bound could still reveal the important quantities for generalization, which informs the design of regularizers. We believe this is an advantage of our all-layer margin --- the generalization bound is simple and easy to interpret, and we use it to design a regularizer which performs well in practice. \n\n--- \u201csome experiments on the defense against adversarial attack would greatly strengthen the theoretical results\u201d\n\nIn Section 5, Table 2 of the revision, we have added preliminary results in the robust classification setting demonstrating improved performance over the robust training algorithm of [Madry et. al \u201817]. Our adversarially robust AMO procedure improves the robust accuracy by around 6% on PGD attacks. These results are based on preliminary experiments; for the final version of the paper, we will compare against additional baselines and attack models.  "}, "signatures": ["ICLR.cc/2020/Conference/Paper898/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper898/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["colinwei@stanford.edu", "tengyuma@cs.stanford.edu"], "title": "Improved Sample Complexities for Deep Neural Networks and Robust Classification via an All-Layer Margin", "authors": ["Colin Wei", "Tengyu Ma"], "pdf": "/pdf/68a22f3da586dac673478e301763f5516fdf3f10.pdf", "TL;DR": "We propose a new notion of margin that has a direct relationship with neural net generalization, and obtain improved generalization bounds for neural nets and robust classification by analyzing this margin.", "abstract": "For linear classifiers, the relationship between (normalized) output margin and generalization is captured in a clear and simple bound \u2013 a large output margin implies good generalization. Unfortunately, for deep models, this relationship is less clear: existing analyses of the output margin give complicated bounds which sometimes depend exponentially on depth. In this work, we propose to instead analyze a new notion of margin, which we call the \u201call-layer margin.\u201d Our analysis reveals that the all-layer margin has a clear and direct relationship with generalization for deep models. This enables the following concrete applications of the all-layer margin: 1) by analyzing the all-layer margin, we obtain tighter generalization bounds for neural nets which depend on Jacobian and hidden layer norms and remove the exponential dependency on depth 2) our neural net results easily translate to the adversarially robust setting, giving the first direct analysis of robust test error for deep networks, and 3) we present a theoretically inspired training algorithm for increasing the all-layer margin. Our algorithm improves both clean and adversarially robust test performance over strong baselines in practice.", "keywords": ["deep learning theory", "generalization bounds", "adversarially robust generalization", "data-dependent generalization bounds"], "paperhash": "wei|improved_sample_complexities_for_deep_neural_networks_and_robust_classification_via_an_alllayer_margin", "_bibtex": "@inproceedings{\nWei2020Improved,\ntitle={Improved Sample Complexities for Deep Neural Networks and Robust Classification via an All-Layer Margin},\nauthor={Colin Wei and Tengyu Ma},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJe_yR4Fwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/3e0278730e9ebd1eceb363e4af77390c6795935d.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJe_yR4Fwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper898/Authors", "ICLR.cc/2020/Conference/Paper898/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper898/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper898/Reviewers", "ICLR.cc/2020/Conference/Paper898/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper898/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper898/Authors|ICLR.cc/2020/Conference/Paper898/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504164504, "tmdate": 1576860556094, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper898/Authors", "ICLR.cc/2020/Conference/Paper898/Reviewers", "ICLR.cc/2020/Conference/Paper898/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper898/-/Official_Comment"}}}, {"id": "Hyl9zrqsjS", "original": null, "number": 9, "cdate": 1573786898114, "ddate": null, "tcdate": 1573786898114, "tmdate": 1573786898114, "tddate": null, "forum": "HJe_yR4Fwr", "replyto": "H1lRC3iCYH", "invitation": "ICLR.cc/2020/Conference/Paper898/-/Official_Comment", "content": {"title": "Response", "comment": "We thank the reviewer for the comments. The review noted that \u201coverall the paper was a pleasure to read.\u201d Though our proof techniques are simpler than prior work, our all-layer margin \u201calleviates the exponential dependency of depth\u201d in a \u201cnon-trivial and insightful\u201d way. We will incorporate feedback and comments into the final version of our paper. \n\nResponses to specific points below:\n\n--- \u201cThough results in sec5 are encouraging it would have been nice if it is shown that narrow and deep architectures benefit from such a regularization.\u201d \n\nFor a deeper but narrow architecture such as ResNet-52, we found that AMO improves performance when there is no data augmentation. However, with data augmentation, AMO gives less relative performance boost for ResNet-52 than for WideResNet architectures. We suspect that this is because the WideResNet models we use have a larger capacity than ResNet-52. Larger capacity allows the model to achieve a smaller perturbed loss (and therefore larger all-layer margin). Empirically, we observe that larger models do indeed obtain smaller perturbed loss. \n\nWe note that the importance of capacity has also been observed for adversarial robustness, which also involves a min-max perturbed loss objective (see Section 4 of [Madry et. al\u201917]).\n\n--- \u201cThough linear dependency is a useful step, it does not still reflect the observation in practice that in many applications deep networks generalize better than the shallow ones.\u201d\n\nWe agree that understanding the role of depth in generalization is an important theoretical question. Though this question largely remains open, we note that our bound can possibly improve with the depth of the network. At a high level, adding depth increases the expressivity of the network --- adding an extra layer could allow the model to fit the data with larger all-layer margin and *less* complexity required at each individual layer, making the overall bound better. To phrase this more technically: a deeper and more expressive model could fit the training data with smaller weight matrix, Jacobian, and hidden layer norms and larger output margin. This could imply a better bound for Theorem 3.1."}, "signatures": ["ICLR.cc/2020/Conference/Paper898/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper898/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["colinwei@stanford.edu", "tengyuma@cs.stanford.edu"], "title": "Improved Sample Complexities for Deep Neural Networks and Robust Classification via an All-Layer Margin", "authors": ["Colin Wei", "Tengyu Ma"], "pdf": "/pdf/68a22f3da586dac673478e301763f5516fdf3f10.pdf", "TL;DR": "We propose a new notion of margin that has a direct relationship with neural net generalization, and obtain improved generalization bounds for neural nets and robust classification by analyzing this margin.", "abstract": "For linear classifiers, the relationship between (normalized) output margin and generalization is captured in a clear and simple bound \u2013 a large output margin implies good generalization. Unfortunately, for deep models, this relationship is less clear: existing analyses of the output margin give complicated bounds which sometimes depend exponentially on depth. In this work, we propose to instead analyze a new notion of margin, which we call the \u201call-layer margin.\u201d Our analysis reveals that the all-layer margin has a clear and direct relationship with generalization for deep models. This enables the following concrete applications of the all-layer margin: 1) by analyzing the all-layer margin, we obtain tighter generalization bounds for neural nets which depend on Jacobian and hidden layer norms and remove the exponential dependency on depth 2) our neural net results easily translate to the adversarially robust setting, giving the first direct analysis of robust test error for deep networks, and 3) we present a theoretically inspired training algorithm for increasing the all-layer margin. Our algorithm improves both clean and adversarially robust test performance over strong baselines in practice.", "keywords": ["deep learning theory", "generalization bounds", "adversarially robust generalization", "data-dependent generalization bounds"], "paperhash": "wei|improved_sample_complexities_for_deep_neural_networks_and_robust_classification_via_an_alllayer_margin", "_bibtex": "@inproceedings{\nWei2020Improved,\ntitle={Improved Sample Complexities for Deep Neural Networks and Robust Classification via an All-Layer Margin},\nauthor={Colin Wei and Tengyu Ma},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJe_yR4Fwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/3e0278730e9ebd1eceb363e4af77390c6795935d.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJe_yR4Fwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper898/Authors", "ICLR.cc/2020/Conference/Paper898/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper898/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper898/Reviewers", "ICLR.cc/2020/Conference/Paper898/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper898/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper898/Authors|ICLR.cc/2020/Conference/Paper898/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504164504, "tmdate": 1576860556094, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper898/Authors", "ICLR.cc/2020/Conference/Paper898/Reviewers", "ICLR.cc/2020/Conference/Paper898/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper898/-/Official_Comment"}}}, {"id": "SJg504qojS", "original": null, "number": 8, "cdate": 1573786833570, "ddate": null, "tcdate": 1573786833570, "tmdate": 1573786833570, "tddate": null, "forum": "HJe_yR4Fwr", "replyto": "Hkxnszbmqr", "invitation": "ICLR.cc/2020/Conference/Paper898/-/Official_Comment", "content": {"title": "Response", "comment": "We thank the reviewer for the comments.  The review noted that our work \u201cprovides a new bound that is simpler and tighter\u201d than existing output margin-based generalization bounds, and our results are presented so that they \u201ccome out clearly to non-specialists too\u201d. We also incorporated the comments on the typos into our revision. "}, "signatures": ["ICLR.cc/2020/Conference/Paper898/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper898/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["colinwei@stanford.edu", "tengyuma@cs.stanford.edu"], "title": "Improved Sample Complexities for Deep Neural Networks and Robust Classification via an All-Layer Margin", "authors": ["Colin Wei", "Tengyu Ma"], "pdf": "/pdf/68a22f3da586dac673478e301763f5516fdf3f10.pdf", "TL;DR": "We propose a new notion of margin that has a direct relationship with neural net generalization, and obtain improved generalization bounds for neural nets and robust classification by analyzing this margin.", "abstract": "For linear classifiers, the relationship between (normalized) output margin and generalization is captured in a clear and simple bound \u2013 a large output margin implies good generalization. Unfortunately, for deep models, this relationship is less clear: existing analyses of the output margin give complicated bounds which sometimes depend exponentially on depth. In this work, we propose to instead analyze a new notion of margin, which we call the \u201call-layer margin.\u201d Our analysis reveals that the all-layer margin has a clear and direct relationship with generalization for deep models. This enables the following concrete applications of the all-layer margin: 1) by analyzing the all-layer margin, we obtain tighter generalization bounds for neural nets which depend on Jacobian and hidden layer norms and remove the exponential dependency on depth 2) our neural net results easily translate to the adversarially robust setting, giving the first direct analysis of robust test error for deep networks, and 3) we present a theoretically inspired training algorithm for increasing the all-layer margin. Our algorithm improves both clean and adversarially robust test performance over strong baselines in practice.", "keywords": ["deep learning theory", "generalization bounds", "adversarially robust generalization", "data-dependent generalization bounds"], "paperhash": "wei|improved_sample_complexities_for_deep_neural_networks_and_robust_classification_via_an_alllayer_margin", "_bibtex": "@inproceedings{\nWei2020Improved,\ntitle={Improved Sample Complexities for Deep Neural Networks and Robust Classification via an All-Layer Margin},\nauthor={Colin Wei and Tengyu Ma},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJe_yR4Fwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/3e0278730e9ebd1eceb363e4af77390c6795935d.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJe_yR4Fwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper898/Authors", "ICLR.cc/2020/Conference/Paper898/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper898/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper898/Reviewers", "ICLR.cc/2020/Conference/Paper898/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper898/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper898/Authors|ICLR.cc/2020/Conference/Paper898/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504164504, "tmdate": 1576860556094, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper898/Authors", "ICLR.cc/2020/Conference/Paper898/Reviewers", "ICLR.cc/2020/Conference/Paper898/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper898/-/Official_Comment"}}}, {"id": "rye0FNG0tB", "original": null, "number": 2, "cdate": 1571853446201, "ddate": null, "tcdate": 1571853446201, "tmdate": 1572972538409, "tddate": null, "forum": "HJe_yR4Fwr", "replyto": "HJe_yR4Fwr", "invitation": "ICLR.cc/2020/Conference/Paper898/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposes a new generalization bound for deep neural networks and develops a regularizer which optimize quantities related to the bound and improve generalization error on competitive baselines. The paper treats deep neural network as a composition of functions (i.e. the layers) and introduces a new complexity measure, all layer margins, which depends on norm-based perturbation simultaneously applied to all the hidden layers of the neural networks. The all-layer margin can be seen as a generalization of the popular margin definition in SVM\u2019s. Similarly, a generalization bound can be derived based on such all layer margins. The paper also shows that the bound has nice properties that improve upon some short-comings of previous efforts and compare favorably in terms of tightness. The paper also shows that this bound can be adapted to adversarial robustness of the deep model. Finally, a practical algorithm is proposed to optimize the approximation of the all-layer margin and improve upon the baseline models (i.e. trained with only cross-entropy) in a non-trivial manner.\n\nI really like the paper! Jiang et al. 19 showed a surprisingly strong correlation between the margins at the hidden layers and generalization, which begs the question whether a generalization bound can be proven based on these perturbations. This paper shows that it is indeed the case with a much more sophisticated yet intuitive definition of margin that depends all hidden layers at the same time. Other than the nice theoretical properties outlined in the paper, the most exciting property of the all layer margin is that it can be directly optimized which the paper shows at the end. On the technical front, I have carefully read section 2 and Appendix A and read the other proofs at a coarser granularity. Barring some small notational errors, I did not find major errors in the derivation since standard techniques are used after the definition of the all layer margin. For the algorithm, while I have some questions about the thoroughness and accuracy of the approximation the results are quite compelling.\n\nI think this paper should be accepted as I consider generalization to be one of the most important topics in deep learning and the paper is a valuable contribution both for theory and practices. In addition, the paper is relatively easy to follow for a topic so technically involved. \n\nBarring my endorsement, I have some small complaints and questions:\n    1. The notation for summation in the definition of \\psi is quite difficult to parse. It would be good to either adopt a more explicit notation or have some extra clarification.\n\n    2. The symbol for function F and the symbol for function class \\mathcal{F} are sometimes mixed. For example, in lemma 2.1, the m_F on LHS should be m \\circ \\mathcal{F}.\n\n    3. I am a little confused on the role of section 3 since, like the paper states, the bound in section 3 is always looser than section 2. Is this bound just for comparison with previous work or provide a tractable upper bound? If so this should be discussed.\n\n    4. In lemma B.1, why are there 2 reference matrices and how should they be chosen?\n\n    5. Condition A.1 assumes the function classes of each layer have some kind of bounded complexity. In a feedforward neural network, a single \\mathcal{F}_i is just a linear operator which has pretty small complexity, but how about networks with residual connections? Is this assumption still reasonable? Likewise, since the proposed AMO is applied to residual networks, I want to hear the authors thought on this.\n\n    6. Following the previous point, perhaps it would be nice to have some experiments on conventional feedforward networks and applied the proposed algorithm to all layers instead of skipping the contents of the residual block.\n\n    7. Can the bound be accurately approximated? To me this seems almost combinatorially hard. If so, do the authors expect it to be non-vacuous? Given the results of Jiang et al., it seems that it is plausible to obtain fairly tight bound conditioned on the data and architecture.\n\n    8. Adversarial robustness is a highly active and empirical-results-driven field, so some experiments on the defense against adversarial attack would greatly strengthen the theoretical results. Otherwise it\u2019s hard to gauge how useful it is.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper898/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper898/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["colinwei@stanford.edu", "tengyuma@cs.stanford.edu"], "title": "Improved Sample Complexities for Deep Neural Networks and Robust Classification via an All-Layer Margin", "authors": ["Colin Wei", "Tengyu Ma"], "pdf": "/pdf/68a22f3da586dac673478e301763f5516fdf3f10.pdf", "TL;DR": "We propose a new notion of margin that has a direct relationship with neural net generalization, and obtain improved generalization bounds for neural nets and robust classification by analyzing this margin.", "abstract": "For linear classifiers, the relationship between (normalized) output margin and generalization is captured in a clear and simple bound \u2013 a large output margin implies good generalization. Unfortunately, for deep models, this relationship is less clear: existing analyses of the output margin give complicated bounds which sometimes depend exponentially on depth. In this work, we propose to instead analyze a new notion of margin, which we call the \u201call-layer margin.\u201d Our analysis reveals that the all-layer margin has a clear and direct relationship with generalization for deep models. This enables the following concrete applications of the all-layer margin: 1) by analyzing the all-layer margin, we obtain tighter generalization bounds for neural nets which depend on Jacobian and hidden layer norms and remove the exponential dependency on depth 2) our neural net results easily translate to the adversarially robust setting, giving the first direct analysis of robust test error for deep networks, and 3) we present a theoretically inspired training algorithm for increasing the all-layer margin. Our algorithm improves both clean and adversarially robust test performance over strong baselines in practice.", "keywords": ["deep learning theory", "generalization bounds", "adversarially robust generalization", "data-dependent generalization bounds"], "paperhash": "wei|improved_sample_complexities_for_deep_neural_networks_and_robust_classification_via_an_alllayer_margin", "_bibtex": "@inproceedings{\nWei2020Improved,\ntitle={Improved Sample Complexities for Deep Neural Networks and Robust Classification via an All-Layer Margin},\nauthor={Colin Wei and Tengyu Ma},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJe_yR4Fwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/3e0278730e9ebd1eceb363e4af77390c6795935d.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJe_yR4Fwr", "replyto": "HJe_yR4Fwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper898/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper898/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576193631442, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper898/Reviewers"], "noninvitees": [], "tcdate": 1570237745378, "tmdate": 1576193631462, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper898/-/Official_Review"}}}, {"id": "H1lRC3iCYH", "original": null, "number": 3, "cdate": 1571892437898, "ddate": null, "tcdate": 1571892437898, "tmdate": 1572972538366, "tddate": null, "forum": "HJe_yR4Fwr", "replyto": "HJe_yR4Fwr", "invitation": "ICLR.cc/2020/Conference/Paper898/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper presents a novel and interesting way to measure the margin in the context of deep networks that removes the exponential dependency of depth in the corresponding generalization bounds. The key idea is to stabilize not only with respect to the input perturbations (which most existing works do and end up obtaining an exponential dependency), but simultaneous perturbations at every layer. It also shown that these ideas readily can be extended to provide bounds in the adversarial classification case. Finally, preliminary positive results on benchmarks showing how such an all-layer margin can be maximized during training are presented.\n\nMajor comments:\n1. I really like the fresh idea of simultaneous perturbations based analysis. Though it is clear that the proposed all-layer margin is upper bounded by margins with single perturbation, it is indeed non-trivial and insightful to understand that the same alleviates the exponential dependency of depth. More specifically, I think claim 2.1 is the the most insightful result, though simple to prove in hindsight. It would greatly help the readers if simple figures are used to explain this insightful result in the final manuscript. \n\n2. I think overall the paper was a pleasure to read especially with the way simplified analysis is presented in the main paper while postponing details to the Appendix.  It will be great if theorem 3.1 can further be simplified in notation and details just to present the main result that linear dependency is achieved via the lower bound for all-layer margin and to show improvement over existing bounds.\n\nMinor comments:\n1. Though results in sec5 are encouraging it would have been nice if it is shown that narrow and deep architectures benefit from such a regularization. As per the authors of WRN, the optimal architectures are not very deep.\n\n2. Though linear dependency is a useful step, it does not still reflect the observation in practice that in many applications deep networks generalize better than the shallow ones. Any comments towards this might help.\n\n3. There seem to be some typos: second min in (2.2) etc."}, "signatures": ["ICLR.cc/2020/Conference/Paper898/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper898/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["colinwei@stanford.edu", "tengyuma@cs.stanford.edu"], "title": "Improved Sample Complexities for Deep Neural Networks and Robust Classification via an All-Layer Margin", "authors": ["Colin Wei", "Tengyu Ma"], "pdf": "/pdf/68a22f3da586dac673478e301763f5516fdf3f10.pdf", "TL;DR": "We propose a new notion of margin that has a direct relationship with neural net generalization, and obtain improved generalization bounds for neural nets and robust classification by analyzing this margin.", "abstract": "For linear classifiers, the relationship between (normalized) output margin and generalization is captured in a clear and simple bound \u2013 a large output margin implies good generalization. Unfortunately, for deep models, this relationship is less clear: existing analyses of the output margin give complicated bounds which sometimes depend exponentially on depth. In this work, we propose to instead analyze a new notion of margin, which we call the \u201call-layer margin.\u201d Our analysis reveals that the all-layer margin has a clear and direct relationship with generalization for deep models. This enables the following concrete applications of the all-layer margin: 1) by analyzing the all-layer margin, we obtain tighter generalization bounds for neural nets which depend on Jacobian and hidden layer norms and remove the exponential dependency on depth 2) our neural net results easily translate to the adversarially robust setting, giving the first direct analysis of robust test error for deep networks, and 3) we present a theoretically inspired training algorithm for increasing the all-layer margin. Our algorithm improves both clean and adversarially robust test performance over strong baselines in practice.", "keywords": ["deep learning theory", "generalization bounds", "adversarially robust generalization", "data-dependent generalization bounds"], "paperhash": "wei|improved_sample_complexities_for_deep_neural_networks_and_robust_classification_via_an_alllayer_margin", "_bibtex": "@inproceedings{\nWei2020Improved,\ntitle={Improved Sample Complexities for Deep Neural Networks and Robust Classification via an All-Layer Margin},\nauthor={Colin Wei and Tengyu Ma},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJe_yR4Fwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/3e0278730e9ebd1eceb363e4af77390c6795935d.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJe_yR4Fwr", "replyto": "HJe_yR4Fwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper898/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper898/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576193631442, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper898/Reviewers"], "noninvitees": [], "tcdate": 1570237745378, "tmdate": 1576193631462, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper898/-/Official_Review"}}}, {"id": "Hkxnszbmqr", "original": null, "number": 4, "cdate": 1572176548258, "ddate": null, "tcdate": 1572176548258, "tmdate": 1572972538322, "tddate": null, "forum": "HJe_yR4Fwr", "replyto": "HJe_yR4Fwr", "invitation": "ICLR.cc/2020/Conference/Paper898/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "The paper presents a bound on the generalization error of a deep network in terms of margin at each layer of the network.  The starting premise is that extending the existing margin generalization bounds to deep networks worsen exponentially with the depth of the\nnetwork. Recent work which removed that exponential dependency is\nclaimed to require a more involved proof and complicated dependence on\ninput.  The paper provides a new bound that is simpler\nand tighter.\n\nA second contribution is to extend their bounds to robust classifier.\nSince their bounds depend on instance-specific margins, the extension\nto the robust case is straightforward. They just need to relax the\nmargin to the robustness boundary of the input. \n\nFinally, they present a new algorithm motivated by their bounds, that\nmaximized margin on all layers.  They show that the resultant network has much lower error than standard training.\n\nThe paper is well-presented and in spite of being theoretical is very nicely developed so that the main contributions come out clearly to non-specialists too.  \n\nA few minor comments:\nThe inner min in Equation 2.2 seems to be a typo.\n\nIn Theorem 2.1, there is typo around the definition of \\xi.\nBelow thoerem 2.1, the phrase \"depend on the q-th moment\" has 'q' undefined.\n\nTypo \"is has a\" in Theorem 3.1\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper898/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper898/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["colinwei@stanford.edu", "tengyuma@cs.stanford.edu"], "title": "Improved Sample Complexities for Deep Neural Networks and Robust Classification via an All-Layer Margin", "authors": ["Colin Wei", "Tengyu Ma"], "pdf": "/pdf/68a22f3da586dac673478e301763f5516fdf3f10.pdf", "TL;DR": "We propose a new notion of margin that has a direct relationship with neural net generalization, and obtain improved generalization bounds for neural nets and robust classification by analyzing this margin.", "abstract": "For linear classifiers, the relationship between (normalized) output margin and generalization is captured in a clear and simple bound \u2013 a large output margin implies good generalization. Unfortunately, for deep models, this relationship is less clear: existing analyses of the output margin give complicated bounds which sometimes depend exponentially on depth. In this work, we propose to instead analyze a new notion of margin, which we call the \u201call-layer margin.\u201d Our analysis reveals that the all-layer margin has a clear and direct relationship with generalization for deep models. This enables the following concrete applications of the all-layer margin: 1) by analyzing the all-layer margin, we obtain tighter generalization bounds for neural nets which depend on Jacobian and hidden layer norms and remove the exponential dependency on depth 2) our neural net results easily translate to the adversarially robust setting, giving the first direct analysis of robust test error for deep networks, and 3) we present a theoretically inspired training algorithm for increasing the all-layer margin. Our algorithm improves both clean and adversarially robust test performance over strong baselines in practice.", "keywords": ["deep learning theory", "generalization bounds", "adversarially robust generalization", "data-dependent generalization bounds"], "paperhash": "wei|improved_sample_complexities_for_deep_neural_networks_and_robust_classification_via_an_alllayer_margin", "_bibtex": "@inproceedings{\nWei2020Improved,\ntitle={Improved Sample Complexities for Deep Neural Networks and Robust Classification via an All-Layer Margin},\nauthor={Colin Wei and Tengyu Ma},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJe_yR4Fwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/3e0278730e9ebd1eceb363e4af77390c6795935d.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJe_yR4Fwr", "replyto": "HJe_yR4Fwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper898/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper898/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576193631442, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper898/Reviewers"], "noninvitees": [], "tcdate": 1570237745378, "tmdate": 1576193631462, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper898/-/Official_Review"}}}], "count": 13}