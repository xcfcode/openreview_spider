{"notes": [{"id": "SyVU6s05K7", "original": "HylI4dNFF7", "number": 807, "cdate": 1538087870354, "ddate": null, "tcdate": 1538087870354, "tmdate": 1550851962951, "tddate": null, "forum": "SyVU6s05K7", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Deep Frank-Wolfe For Neural Network Optimization", "abstract": "Learning a deep neural network requires solving a challenging optimization problem: it is a high-dimensional, non-convex and non-smooth minimization problem with a large number of terms. The current practice in neural network optimization is to rely on the stochastic gradient descent (SGD) algorithm or its adaptive variants. However, SGD requires a hand-designed schedule for the learning rate. In addition, its adaptive variants tend to produce solutions that generalize less well on unseen data than SGD with a hand-designed schedule. We present an optimization method that offers empirically the best of both worlds: our algorithm yields good generalization performance while requiring only one hyper-parameter. Our approach is based on a composite proximal framework, which exploits the compositional nature of deep neural networks and can leverage powerful convex optimization algorithms by design. Specifically, we employ the Frank-Wolfe (FW) algorithm for SVM, which computes an optimal step-size in closed-form at each time-step. We further show that the descent direction is given by a simple backward pass in the network, yielding the same computational cost per iteration as SGD. We present experiments on the CIFAR and SNLI data sets, where we demonstrate the significant superiority of our method over Adam, Adagrad, as well as the recently proposed BPGrad and AMSGrad. Furthermore, we compare our algorithm to SGD with a hand-designed learning rate schedule, and show that it provides similar generalization while often converging faster. The code is publicly available at https://github.com/oval-group/dfw.", "keywords": ["optimization", "conditional gradient", "Frank-Wolfe", "SVM"], "authorids": ["lberrada@robots.ox.ac.uk", "az@robots.ox.ac.uk", "pawan@robots.ox.ac.uk"], "authors": ["Leonard Berrada", "Andrew Zisserman", "M. Pawan Kumar"], "pdf": "/pdf/ce7e434aabd4bf9e453cdfbd301971686c81da17.pdf", "paperhash": "berrada|deep_frankwolfe_for_neural_network_optimization", "TL;DR": "We train neural networks by locally linearizing them and using a linear SVM solver (Frank-Wolfe) at each iteration.", "_bibtex": "@inproceedings{\nberrada2018deep,\ntitle={Deep Frank-Wolfe For Neural Network Optimization},\nauthor={Leonard Berrada and Andrew Zisserman and M. Pawan Kumar},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SyVU6s05K7},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "r1eK3RXJxE", "original": null, "number": 1, "cdate": 1544662704691, "ddate": null, "tcdate": 1544662704691, "tmdate": 1545354504508, "tddate": null, "forum": "SyVU6s05K7", "replyto": "SyVU6s05K7", "invitation": "ICLR.cc/2019/Conference/-/Paper807/Meta_Review", "content": {"metareview": "The paper was judged by the reviewers as providing interesting ideas, well-written and potentially having impact on future research on NN optimization.  The authors are asked to make sure they addressed reviewers comments clearly in the paper.", "confidence": "5: The area chair is absolutely certain", "recommendation": "Accept (Poster)", "title": "Metareview"}, "signatures": ["ICLR.cc/2019/Conference/Paper807/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper807/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Frank-Wolfe For Neural Network Optimization", "abstract": "Learning a deep neural network requires solving a challenging optimization problem: it is a high-dimensional, non-convex and non-smooth minimization problem with a large number of terms. The current practice in neural network optimization is to rely on the stochastic gradient descent (SGD) algorithm or its adaptive variants. However, SGD requires a hand-designed schedule for the learning rate. In addition, its adaptive variants tend to produce solutions that generalize less well on unseen data than SGD with a hand-designed schedule. We present an optimization method that offers empirically the best of both worlds: our algorithm yields good generalization performance while requiring only one hyper-parameter. Our approach is based on a composite proximal framework, which exploits the compositional nature of deep neural networks and can leverage powerful convex optimization algorithms by design. Specifically, we employ the Frank-Wolfe (FW) algorithm for SVM, which computes an optimal step-size in closed-form at each time-step. We further show that the descent direction is given by a simple backward pass in the network, yielding the same computational cost per iteration as SGD. We present experiments on the CIFAR and SNLI data sets, where we demonstrate the significant superiority of our method over Adam, Adagrad, as well as the recently proposed BPGrad and AMSGrad. Furthermore, we compare our algorithm to SGD with a hand-designed learning rate schedule, and show that it provides similar generalization while often converging faster. The code is publicly available at https://github.com/oval-group/dfw.", "keywords": ["optimization", "conditional gradient", "Frank-Wolfe", "SVM"], "authorids": ["lberrada@robots.ox.ac.uk", "az@robots.ox.ac.uk", "pawan@robots.ox.ac.uk"], "authors": ["Leonard Berrada", "Andrew Zisserman", "M. Pawan Kumar"], "pdf": "/pdf/ce7e434aabd4bf9e453cdfbd301971686c81da17.pdf", "paperhash": "berrada|deep_frankwolfe_for_neural_network_optimization", "TL;DR": "We train neural networks by locally linearizing them and using a linear SVM solver (Frank-Wolfe) at each iteration.", "_bibtex": "@inproceedings{\nberrada2018deep,\ntitle={Deep Frank-Wolfe For Neural Network Optimization},\nauthor={Leonard Berrada and Andrew Zisserman and M. Pawan Kumar},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SyVU6s05K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper807/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353080794, "tddate": null, "super": null, "final": null, "reply": {"forum": "SyVU6s05K7", "replyto": "SyVU6s05K7", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper807/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper807/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper807/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353080794}}}, {"id": "Bkg1zXr01N", "original": null, "number": 7, "cdate": 1544602375474, "ddate": null, "tcdate": 1544602375474, "tmdate": 1544602375474, "tddate": null, "forum": "SyVU6s05K7", "replyto": "S1ekiBNT0m", "invitation": "ICLR.cc/2019/Conference/-/Paper807/Official_Comment", "content": {"title": "Results with data augmentation", "comment": "We have performed additional experiments on the CIFAR data sets using data augmentation. We summarise here our findings, and we will provide more details in future versions of the paper. \n\nIn order to account for the additional variance introduced by the data augmentation, we allow the batch size to be chosen as 1x, 2x or 4x, where x is the original value of batch-size. Because of the heavy computational cost of the cross-validation (we tune the batch-size, regularization and initial learning rate), we provide results for SGD, DFW and the best performing adaptive gradient method, which is AMSGrad. For SGD the hyper-parameters are kept the same as in (Zagoruyko and Komodakis, 2016) and (Huang et al, 2017), and in particular benefits from hand-designed learning rate schedules. We refer to the Wide Residual Network architecture as WRN, and the DenseNet architecture as DN (details are available in the paper).\n\nWe obtain the following results:\n* WRN CIFAR-10  : AMSGrad 90.06, DFW 94.52, SGD 95.40\n* DN CIFAR-10     : AMSGrad 91.78, DFW 94.73, SGD 95.26\n* WRN CIFAR-100: AMSGrad 67.75, DFW 76.12, SGD 77.78\n* DN CIFAR-100    : AMSGrad 69.58, DFW 73.85, SGD 76.26\n\nComparing these results to Tables 1 and 2, it can be observed that all methods benefit from data augmentation, though with varying increases of performance. DFW systematically and significantly outperforms AMSGrad. In particular, it does so by more than 8% in the WRN-100 case."}, "signatures": ["ICLR.cc/2019/Conference/Paper807/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper807/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper807/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Frank-Wolfe For Neural Network Optimization", "abstract": "Learning a deep neural network requires solving a challenging optimization problem: it is a high-dimensional, non-convex and non-smooth minimization problem with a large number of terms. The current practice in neural network optimization is to rely on the stochastic gradient descent (SGD) algorithm or its adaptive variants. However, SGD requires a hand-designed schedule for the learning rate. In addition, its adaptive variants tend to produce solutions that generalize less well on unseen data than SGD with a hand-designed schedule. We present an optimization method that offers empirically the best of both worlds: our algorithm yields good generalization performance while requiring only one hyper-parameter. Our approach is based on a composite proximal framework, which exploits the compositional nature of deep neural networks and can leverage powerful convex optimization algorithms by design. Specifically, we employ the Frank-Wolfe (FW) algorithm for SVM, which computes an optimal step-size in closed-form at each time-step. We further show that the descent direction is given by a simple backward pass in the network, yielding the same computational cost per iteration as SGD. We present experiments on the CIFAR and SNLI data sets, where we demonstrate the significant superiority of our method over Adam, Adagrad, as well as the recently proposed BPGrad and AMSGrad. Furthermore, we compare our algorithm to SGD with a hand-designed learning rate schedule, and show that it provides similar generalization while often converging faster. The code is publicly available at https://github.com/oval-group/dfw.", "keywords": ["optimization", "conditional gradient", "Frank-Wolfe", "SVM"], "authorids": ["lberrada@robots.ox.ac.uk", "az@robots.ox.ac.uk", "pawan@robots.ox.ac.uk"], "authors": ["Leonard Berrada", "Andrew Zisserman", "M. Pawan Kumar"], "pdf": "/pdf/ce7e434aabd4bf9e453cdfbd301971686c81da17.pdf", "paperhash": "berrada|deep_frankwolfe_for_neural_network_optimization", "TL;DR": "We train neural networks by locally linearizing them and using a linear SVM solver (Frank-Wolfe) at each iteration.", "_bibtex": "@inproceedings{\nberrada2018deep,\ntitle={Deep Frank-Wolfe For Neural Network Optimization},\nauthor={Leonard Berrada and Andrew Zisserman and M. Pawan Kumar},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SyVU6s05K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper807/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619004, "tddate": null, "super": null, "final": null, "reply": {"forum": "SyVU6s05K7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper807/Authors", "ICLR.cc/2019/Conference/Paper807/Reviewers", "ICLR.cc/2019/Conference/Paper807/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper807/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper807/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper807/Authors|ICLR.cc/2019/Conference/Paper807/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper807/Reviewers", "ICLR.cc/2019/Conference/Paper807/Authors", "ICLR.cc/2019/Conference/Paper807/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619004}}}, {"id": "SJebCbtF67", "original": null, "number": 3, "cdate": 1542193609392, "ddate": null, "tcdate": 1542193609392, "tmdate": 1543484985713, "tddate": null, "forum": "SyVU6s05K7", "replyto": "BJlCnxODhX", "invitation": "ICLR.cc/2019/Conference/-/Paper807/Official_Comment", "content": {"title": "Thanks for the comments.", "comment": "We thank the reviewer for their comments and suggestions. We answer below:\n\n1. As the reviewer accurately points out, we choose to always employ the hinge loss for DFW in this paper because it gives an optimal step-size. In the new version of the paper, we have included additional baselines on the SNLI data set. This provides more empirical comparisons between the performance of CE and SVM for different optimizers.\n\n2. In appendix B.2 of the paper, we have added the convergence plot for all methods on the CIFAR data sets. \n\nIn some cases the training performance can show some oscillations. We emphasize that this is the result of cross-validating the initial learning rate based on the validation set performance: sometimes a better behaved convergence would be obtained on the training set with a lower learning rate. However this lower learning rate is not selected because it does not provide the best validation performance (this is consistent with our discussion on the step size in section 6).\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper807/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper807/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper807/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Frank-Wolfe For Neural Network Optimization", "abstract": "Learning a deep neural network requires solving a challenging optimization problem: it is a high-dimensional, non-convex and non-smooth minimization problem with a large number of terms. The current practice in neural network optimization is to rely on the stochastic gradient descent (SGD) algorithm or its adaptive variants. However, SGD requires a hand-designed schedule for the learning rate. In addition, its adaptive variants tend to produce solutions that generalize less well on unseen data than SGD with a hand-designed schedule. We present an optimization method that offers empirically the best of both worlds: our algorithm yields good generalization performance while requiring only one hyper-parameter. Our approach is based on a composite proximal framework, which exploits the compositional nature of deep neural networks and can leverage powerful convex optimization algorithms by design. Specifically, we employ the Frank-Wolfe (FW) algorithm for SVM, which computes an optimal step-size in closed-form at each time-step. We further show that the descent direction is given by a simple backward pass in the network, yielding the same computational cost per iteration as SGD. We present experiments on the CIFAR and SNLI data sets, where we demonstrate the significant superiority of our method over Adam, Adagrad, as well as the recently proposed BPGrad and AMSGrad. Furthermore, we compare our algorithm to SGD with a hand-designed learning rate schedule, and show that it provides similar generalization while often converging faster. The code is publicly available at https://github.com/oval-group/dfw.", "keywords": ["optimization", "conditional gradient", "Frank-Wolfe", "SVM"], "authorids": ["lberrada@robots.ox.ac.uk", "az@robots.ox.ac.uk", "pawan@robots.ox.ac.uk"], "authors": ["Leonard Berrada", "Andrew Zisserman", "M. Pawan Kumar"], "pdf": "/pdf/ce7e434aabd4bf9e453cdfbd301971686c81da17.pdf", "paperhash": "berrada|deep_frankwolfe_for_neural_network_optimization", "TL;DR": "We train neural networks by locally linearizing them and using a linear SVM solver (Frank-Wolfe) at each iteration.", "_bibtex": "@inproceedings{\nberrada2018deep,\ntitle={Deep Frank-Wolfe For Neural Network Optimization},\nauthor={Leonard Berrada and Andrew Zisserman and M. Pawan Kumar},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SyVU6s05K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper807/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619004, "tddate": null, "super": null, "final": null, "reply": {"forum": "SyVU6s05K7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper807/Authors", "ICLR.cc/2019/Conference/Paper807/Reviewers", "ICLR.cc/2019/Conference/Paper807/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper807/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper807/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper807/Authors|ICLR.cc/2019/Conference/Paper807/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper807/Reviewers", "ICLR.cc/2019/Conference/Paper807/Authors", "ICLR.cc/2019/Conference/Paper807/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619004}}}, {"id": "S1ekiBNT0m", "original": null, "number": 6, "cdate": 1543484823202, "ddate": null, "tcdate": 1543484823202, "tmdate": 1543484895762, "tddate": null, "forum": "SyVU6s05K7", "replyto": "Hkg1XJI3AX", "invitation": "ICLR.cc/2019/Conference/-/Paper807/Official_Comment", "content": {"title": "Our DenseNet architecture has 40 layers and not 250", "comment": "The architecture pointed out in the comment above uses 250 layers. In our experiments, and as specified in section 5.1 of our paper, we use a model with 40 layers. This explains the difference in performance.\n\nAs we have already stated:\n\n- Since all of the experiments use the same network and same training, the comparison proposed in this work is valid and fair.\n\n- We have verified that our implementation can reproduce the results reported in (Zagoruyko and Komodakis, 2016) when data augmentation is used, and will provide results using data augmentation once these are available."}, "signatures": ["ICLR.cc/2019/Conference/Paper807/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper807/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper807/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Frank-Wolfe For Neural Network Optimization", "abstract": "Learning a deep neural network requires solving a challenging optimization problem: it is a high-dimensional, non-convex and non-smooth minimization problem with a large number of terms. The current practice in neural network optimization is to rely on the stochastic gradient descent (SGD) algorithm or its adaptive variants. However, SGD requires a hand-designed schedule for the learning rate. In addition, its adaptive variants tend to produce solutions that generalize less well on unseen data than SGD with a hand-designed schedule. We present an optimization method that offers empirically the best of both worlds: our algorithm yields good generalization performance while requiring only one hyper-parameter. Our approach is based on a composite proximal framework, which exploits the compositional nature of deep neural networks and can leverage powerful convex optimization algorithms by design. Specifically, we employ the Frank-Wolfe (FW) algorithm for SVM, which computes an optimal step-size in closed-form at each time-step. We further show that the descent direction is given by a simple backward pass in the network, yielding the same computational cost per iteration as SGD. We present experiments on the CIFAR and SNLI data sets, where we demonstrate the significant superiority of our method over Adam, Adagrad, as well as the recently proposed BPGrad and AMSGrad. Furthermore, we compare our algorithm to SGD with a hand-designed learning rate schedule, and show that it provides similar generalization while often converging faster. The code is publicly available at https://github.com/oval-group/dfw.", "keywords": ["optimization", "conditional gradient", "Frank-Wolfe", "SVM"], "authorids": ["lberrada@robots.ox.ac.uk", "az@robots.ox.ac.uk", "pawan@robots.ox.ac.uk"], "authors": ["Leonard Berrada", "Andrew Zisserman", "M. Pawan Kumar"], "pdf": "/pdf/ce7e434aabd4bf9e453cdfbd301971686c81da17.pdf", "paperhash": "berrada|deep_frankwolfe_for_neural_network_optimization", "TL;DR": "We train neural networks by locally linearizing them and using a linear SVM solver (Frank-Wolfe) at each iteration.", "_bibtex": "@inproceedings{\nberrada2018deep,\ntitle={Deep Frank-Wolfe For Neural Network Optimization},\nauthor={Leonard Berrada and Andrew Zisserman and M. Pawan Kumar},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SyVU6s05K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper807/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619004, "tddate": null, "super": null, "final": null, "reply": {"forum": "SyVU6s05K7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper807/Authors", "ICLR.cc/2019/Conference/Paper807/Reviewers", "ICLR.cc/2019/Conference/Paper807/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper807/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper807/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper807/Authors|ICLR.cc/2019/Conference/Paper807/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper807/Reviewers", "ICLR.cc/2019/Conference/Paper807/Authors", "ICLR.cc/2019/Conference/Paper807/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619004}}}, {"id": "Hkg1XJI3AX", "original": null, "number": 2, "cdate": 1543425814523, "ddate": null, "tcdate": 1543425814523, "tmdate": 1543425840072, "tddate": null, "forum": "SyVU6s05K7", "replyto": "SJlpdCRsCX", "invitation": "ICLR.cc/2019/Conference/-/Paper807/Public_Comment", "content": {"comment": "In Table 2 of Huang et al 2017 (https://arxiv.org/abs/1608.06993 ), they reported the accuracies of 94.81 and 80.36 without data augmentation for CIFAR 10 and CIFAR 100, respectively. These are still significantly better than your reported baselines 92.02 and 70.33, especially on CIFAR 100.\n\nFurthermore, note that their reported results are only for DenseNet (k=24). For k=40, the results should be even better (very likely to be around 95.XX and 81.XX respectively).", "title": "Your baseline is still terrible without data augmentation."}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Frank-Wolfe For Neural Network Optimization", "abstract": "Learning a deep neural network requires solving a challenging optimization problem: it is a high-dimensional, non-convex and non-smooth minimization problem with a large number of terms. The current practice in neural network optimization is to rely on the stochastic gradient descent (SGD) algorithm or its adaptive variants. However, SGD requires a hand-designed schedule for the learning rate. In addition, its adaptive variants tend to produce solutions that generalize less well on unseen data than SGD with a hand-designed schedule. We present an optimization method that offers empirically the best of both worlds: our algorithm yields good generalization performance while requiring only one hyper-parameter. Our approach is based on a composite proximal framework, which exploits the compositional nature of deep neural networks and can leverage powerful convex optimization algorithms by design. Specifically, we employ the Frank-Wolfe (FW) algorithm for SVM, which computes an optimal step-size in closed-form at each time-step. We further show that the descent direction is given by a simple backward pass in the network, yielding the same computational cost per iteration as SGD. We present experiments on the CIFAR and SNLI data sets, where we demonstrate the significant superiority of our method over Adam, Adagrad, as well as the recently proposed BPGrad and AMSGrad. Furthermore, we compare our algorithm to SGD with a hand-designed learning rate schedule, and show that it provides similar generalization while often converging faster. The code is publicly available at https://github.com/oval-group/dfw.", "keywords": ["optimization", "conditional gradient", "Frank-Wolfe", "SVM"], "authorids": ["lberrada@robots.ox.ac.uk", "az@robots.ox.ac.uk", "pawan@robots.ox.ac.uk"], "authors": ["Leonard Berrada", "Andrew Zisserman", "M. Pawan Kumar"], "pdf": "/pdf/ce7e434aabd4bf9e453cdfbd301971686c81da17.pdf", "paperhash": "berrada|deep_frankwolfe_for_neural_network_optimization", "TL;DR": "We train neural networks by locally linearizing them and using a linear SVM solver (Frank-Wolfe) at each iteration.", "_bibtex": "@inproceedings{\nberrada2018deep,\ntitle={Deep Frank-Wolfe For Neural Network Optimization},\nauthor={Leonard Berrada and Andrew Zisserman and M. Pawan Kumar},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SyVU6s05K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper807/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311748277, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "SyVU6s05K7", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper807/Authors", "ICLR.cc/2019/Conference/Paper807/Reviewers", "ICLR.cc/2019/Conference/Paper807/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper807/Authors", "ICLR.cc/2019/Conference/Paper807/Reviewers", "ICLR.cc/2019/Conference/Paper807/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311748277}}}, {"id": "SJlpdCRsCX", "original": null, "number": 5, "cdate": 1543396981313, "ddate": null, "tcdate": 1543396981313, "tmdate": 1543397127891, "tddate": null, "forum": "SyVU6s05K7", "replyto": "SkeVgPnFRX", "invitation": "ICLR.cc/2019/Conference/-/Paper807/Official_Comment", "content": {"title": "The difference is due to data augmentation", "comment": "The comment above has pointed out a discrepancy between our results and those from (Zagoruyko and Komodakis, 2016). This is due to the fact that in contrast to (Zagoruyko and Komodakis, 2016), we do not use data augmentation in our CIFAR experiments. Since none of the baselines nor DFW makes use of data augmentation in our experiments, the comparison proposed in this work is valid and fair.\n\nIn its current version, the description of our experiments on the CIFAR datasets mistakenly indicates that we use data augmentation, which is not the case. We will correct this in future versions.\n\nAs a sanity check, we have verified that our implementation can reproduce the results reported in (Zagoruyko and Komodakis, 2016) when training the model with SGD and with data augmentation.\n\nWe will provide results using data augmentation once these are available."}, "signatures": ["ICLR.cc/2019/Conference/Paper807/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper807/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper807/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Frank-Wolfe For Neural Network Optimization", "abstract": "Learning a deep neural network requires solving a challenging optimization problem: it is a high-dimensional, non-convex and non-smooth minimization problem with a large number of terms. The current practice in neural network optimization is to rely on the stochastic gradient descent (SGD) algorithm or its adaptive variants. However, SGD requires a hand-designed schedule for the learning rate. In addition, its adaptive variants tend to produce solutions that generalize less well on unseen data than SGD with a hand-designed schedule. We present an optimization method that offers empirically the best of both worlds: our algorithm yields good generalization performance while requiring only one hyper-parameter. Our approach is based on a composite proximal framework, which exploits the compositional nature of deep neural networks and can leverage powerful convex optimization algorithms by design. Specifically, we employ the Frank-Wolfe (FW) algorithm for SVM, which computes an optimal step-size in closed-form at each time-step. We further show that the descent direction is given by a simple backward pass in the network, yielding the same computational cost per iteration as SGD. We present experiments on the CIFAR and SNLI data sets, where we demonstrate the significant superiority of our method over Adam, Adagrad, as well as the recently proposed BPGrad and AMSGrad. Furthermore, we compare our algorithm to SGD with a hand-designed learning rate schedule, and show that it provides similar generalization while often converging faster. The code is publicly available at https://github.com/oval-group/dfw.", "keywords": ["optimization", "conditional gradient", "Frank-Wolfe", "SVM"], "authorids": ["lberrada@robots.ox.ac.uk", "az@robots.ox.ac.uk", "pawan@robots.ox.ac.uk"], "authors": ["Leonard Berrada", "Andrew Zisserman", "M. Pawan Kumar"], "pdf": "/pdf/ce7e434aabd4bf9e453cdfbd301971686c81da17.pdf", "paperhash": "berrada|deep_frankwolfe_for_neural_network_optimization", "TL;DR": "We train neural networks by locally linearizing them and using a linear SVM solver (Frank-Wolfe) at each iteration.", "_bibtex": "@inproceedings{\nberrada2018deep,\ntitle={Deep Frank-Wolfe For Neural Network Optimization},\nauthor={Leonard Berrada and Andrew Zisserman and M. Pawan Kumar},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SyVU6s05K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper807/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619004, "tddate": null, "super": null, "final": null, "reply": {"forum": "SyVU6s05K7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper807/Authors", "ICLR.cc/2019/Conference/Paper807/Reviewers", "ICLR.cc/2019/Conference/Paper807/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper807/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper807/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper807/Authors|ICLR.cc/2019/Conference/Paper807/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper807/Reviewers", "ICLR.cc/2019/Conference/Paper807/Authors", "ICLR.cc/2019/Conference/Paper807/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619004}}}, {"id": "SkeVgPnFRX", "original": null, "number": 1, "cdate": 1543255787700, "ddate": null, "tcdate": 1543255787700, "tmdate": 1543258259812, "tddate": null, "forum": "SyVU6s05K7", "replyto": "SyVU6s05K7", "invitation": "ICLR.cc/2019/Conference/-/Paper807/Public_Comment", "content": {"comment": "You applied to the architecture WRN-40-4 to CIFAR10 and CIFAR 100.\n\nAs can be seen in Tables 1 and 2, SGD only achieves 90.08 and 66.78 on CIFAR 10 and 100, respectively.\n\nIn the original WRN paper (Zagoruyko and Komodakis, 2016 https://arxiv.org/abs/1605.07146 ), the reported results are 95.03 and 77.21 in Table 4. These results are reproducible:\n\nhttps://github.com/szagoruyko/wide-residual-networks\n\nSimilar things also happened to DenseNet. Huang et al 2017 (https://arxiv.org/abs/1608.06993 ) reported 96.54 and 82.82 in Table 2, but yours are 92.02 and 70.33.\n\nCompared with the results in Zagoruyko and Komodakis, 2016, the proposed deep FW algorithm is significantly worse. This is a huge difference!\n\nWRN and DenseNets are two of the most popular architectures, and their good baseline performance is well-known!\n", "title": "Why are your baselines so terrible?"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Frank-Wolfe For Neural Network Optimization", "abstract": "Learning a deep neural network requires solving a challenging optimization problem: it is a high-dimensional, non-convex and non-smooth minimization problem with a large number of terms. The current practice in neural network optimization is to rely on the stochastic gradient descent (SGD) algorithm or its adaptive variants. However, SGD requires a hand-designed schedule for the learning rate. In addition, its adaptive variants tend to produce solutions that generalize less well on unseen data than SGD with a hand-designed schedule. We present an optimization method that offers empirically the best of both worlds: our algorithm yields good generalization performance while requiring only one hyper-parameter. Our approach is based on a composite proximal framework, which exploits the compositional nature of deep neural networks and can leverage powerful convex optimization algorithms by design. Specifically, we employ the Frank-Wolfe (FW) algorithm for SVM, which computes an optimal step-size in closed-form at each time-step. We further show that the descent direction is given by a simple backward pass in the network, yielding the same computational cost per iteration as SGD. We present experiments on the CIFAR and SNLI data sets, where we demonstrate the significant superiority of our method over Adam, Adagrad, as well as the recently proposed BPGrad and AMSGrad. Furthermore, we compare our algorithm to SGD with a hand-designed learning rate schedule, and show that it provides similar generalization while often converging faster. The code is publicly available at https://github.com/oval-group/dfw.", "keywords": ["optimization", "conditional gradient", "Frank-Wolfe", "SVM"], "authorids": ["lberrada@robots.ox.ac.uk", "az@robots.ox.ac.uk", "pawan@robots.ox.ac.uk"], "authors": ["Leonard Berrada", "Andrew Zisserman", "M. Pawan Kumar"], "pdf": "/pdf/ce7e434aabd4bf9e453cdfbd301971686c81da17.pdf", "paperhash": "berrada|deep_frankwolfe_for_neural_network_optimization", "TL;DR": "We train neural networks by locally linearizing them and using a linear SVM solver (Frank-Wolfe) at each iteration.", "_bibtex": "@inproceedings{\nberrada2018deep,\ntitle={Deep Frank-Wolfe For Neural Network Optimization},\nauthor={Leonard Berrada and Andrew Zisserman and M. Pawan Kumar},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SyVU6s05K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper807/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311748277, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "SyVU6s05K7", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper807/Authors", "ICLR.cc/2019/Conference/Paper807/Reviewers", "ICLR.cc/2019/Conference/Paper807/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper807/Authors", "ICLR.cc/2019/Conference/Paper807/Reviewers", "ICLR.cc/2019/Conference/Paper807/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311748277}}}, {"id": "HklQyb9637", "original": null, "number": 2, "cdate": 1541411034665, "ddate": null, "tcdate": 1541411034665, "tmdate": 1542711527974, "tddate": null, "forum": "SyVU6s05K7", "replyto": "SyVU6s05K7", "invitation": "ICLR.cc/2019/Conference/-/Paper807/Official_Review", "content": {"title": "The proposed DFW lacks of sufficient novelty and the presented performance improvement needs more theoretical justification.", "review": "This paper proposes a Frank-Wolfe based method, called DFW, for training Deep Network. The DFW method linearizes the loss function into a smooth one, and also adopts Nesterov Momentum to accelerate the training. Both techniques have been widely used in the literature for similar settings. This paper mainly focuses on the algorithm part, but only empirically demonstrate the convergence results. \n\nAfter reading the authors\u2019 feedback and the paper again, I think overall this is a good paper and should be of broader interest to the broader audience in machine learning community. \n\nIn Section 6.1, the authors mention the good generalization is due to large number of steps at a high learning rate. Can we possibly get any theoretical justification on this? \n\nThis paper uses multi class hinge loss as an example for illustration. Can this approach be applied for structure prediction, for example, various ranking loss? ", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper807/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Deep Frank-Wolfe For Neural Network Optimization", "abstract": "Learning a deep neural network requires solving a challenging optimization problem: it is a high-dimensional, non-convex and non-smooth minimization problem with a large number of terms. The current practice in neural network optimization is to rely on the stochastic gradient descent (SGD) algorithm or its adaptive variants. However, SGD requires a hand-designed schedule for the learning rate. In addition, its adaptive variants tend to produce solutions that generalize less well on unseen data than SGD with a hand-designed schedule. We present an optimization method that offers empirically the best of both worlds: our algorithm yields good generalization performance while requiring only one hyper-parameter. Our approach is based on a composite proximal framework, which exploits the compositional nature of deep neural networks and can leverage powerful convex optimization algorithms by design. Specifically, we employ the Frank-Wolfe (FW) algorithm for SVM, which computes an optimal step-size in closed-form at each time-step. We further show that the descent direction is given by a simple backward pass in the network, yielding the same computational cost per iteration as SGD. We present experiments on the CIFAR and SNLI data sets, where we demonstrate the significant superiority of our method over Adam, Adagrad, as well as the recently proposed BPGrad and AMSGrad. Furthermore, we compare our algorithm to SGD with a hand-designed learning rate schedule, and show that it provides similar generalization while often converging faster. The code is publicly available at https://github.com/oval-group/dfw.", "keywords": ["optimization", "conditional gradient", "Frank-Wolfe", "SVM"], "authorids": ["lberrada@robots.ox.ac.uk", "az@robots.ox.ac.uk", "pawan@robots.ox.ac.uk"], "authors": ["Leonard Berrada", "Andrew Zisserman", "M. Pawan Kumar"], "pdf": "/pdf/ce7e434aabd4bf9e453cdfbd301971686c81da17.pdf", "paperhash": "berrada|deep_frankwolfe_for_neural_network_optimization", "TL;DR": "We train neural networks by locally linearizing them and using a linear SVM solver (Frank-Wolfe) at each iteration.", "_bibtex": "@inproceedings{\nberrada2018deep,\ntitle={Deep Frank-Wolfe For Neural Network Optimization},\nauthor={Leonard Berrada and Andrew Zisserman and M. Pawan Kumar},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SyVU6s05K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper807/Official_Review", "cdate": 1542234372500, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SyVU6s05K7", "replyto": "SyVU6s05K7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper807/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335806873, "tmdate": 1552335806873, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper807/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SygOKWFYTm", "original": null, "number": 2, "cdate": 1542193535742, "ddate": null, "tcdate": 1542193535742, "tmdate": 1542193535742, "tddate": null, "forum": "SyVU6s05K7", "replyto": "HklQyb9637", "invitation": "ICLR.cc/2019/Conference/-/Paper807/Official_Comment", "content": {"title": "Thanks for the comments. Clarification.", "comment": "We thank the reviewer for their comments. We provide answers below:\n\n* \u201cThe DFW linearizes the loss function into a smooth one, and also adopts Nesterov momentum to accelerate the training.\u201d \nWe would like to clarify this statement: one of the key ideas of the DFW algorithm is not to linearize the loss function $\\mathcal{L}$, but only the model $f$.\n\n* \u201cBoth techniques have been widely used in the literature for similar settings\u201d.\nWe wish to clarify the main technical contributions of this paper, since the SVM smoothing and the application of Nesterov acceleration are not the main novelty of this work. We discuss the summary of contributions (available at the end of section 1 of the paper) in the context of technical novelty.\n- Employing a composite framework allows us to use an efficient primal-dual algorithm. As stated by Reviewer 1, this is novel in the context of deep neural networks: \u201cTo my knowledge, the submission is the first sound attempt to adapt this type of Dual-based algorithm for optimization of Deep Neural Network [..]\u201d.\n- Crucially, our approach yields an update at the same computational cost per iteration as SGD and with the same level of parallelization. In contrast, in the closest approach to ours, the algorithm of Singh & Shawe-Taylor (2018) can only process a single sample at a time. This results in an approach whose runtime is virtually multiplied by the batch-size (it would be slower by two orders of magnitude in typical classification settings, including for the experiments of this paper).\n- We do not mean to claim that the application of Nesterov acceleration is a technical novelty in itself. However, its use is subtle in our case (see appendix A.7) and it is empirically crucial for good performance, hence its mention in the paper.\n- To the best of our knowledge, the hyper-parameter free smoothing approach that we propose in this work is novel (but is not the main contribution).\n\nWe have adapted the abstract and summary of contributions to focus on the main novelty, which is an optimization algorithm for deep neural networks with an optimal step-size at the same computational cost per iteration as SGD.\n\nIf the reviewer remains concerned by a lack of novelty, we would be grateful if he/she could provide specific references so that we can compare them in detail with the DFW algorithm.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper807/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper807/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper807/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Frank-Wolfe For Neural Network Optimization", "abstract": "Learning a deep neural network requires solving a challenging optimization problem: it is a high-dimensional, non-convex and non-smooth minimization problem with a large number of terms. The current practice in neural network optimization is to rely on the stochastic gradient descent (SGD) algorithm or its adaptive variants. However, SGD requires a hand-designed schedule for the learning rate. In addition, its adaptive variants tend to produce solutions that generalize less well on unseen data than SGD with a hand-designed schedule. We present an optimization method that offers empirically the best of both worlds: our algorithm yields good generalization performance while requiring only one hyper-parameter. Our approach is based on a composite proximal framework, which exploits the compositional nature of deep neural networks and can leverage powerful convex optimization algorithms by design. Specifically, we employ the Frank-Wolfe (FW) algorithm for SVM, which computes an optimal step-size in closed-form at each time-step. We further show that the descent direction is given by a simple backward pass in the network, yielding the same computational cost per iteration as SGD. We present experiments on the CIFAR and SNLI data sets, where we demonstrate the significant superiority of our method over Adam, Adagrad, as well as the recently proposed BPGrad and AMSGrad. Furthermore, we compare our algorithm to SGD with a hand-designed learning rate schedule, and show that it provides similar generalization while often converging faster. The code is publicly available at https://github.com/oval-group/dfw.", "keywords": ["optimization", "conditional gradient", "Frank-Wolfe", "SVM"], "authorids": ["lberrada@robots.ox.ac.uk", "az@robots.ox.ac.uk", "pawan@robots.ox.ac.uk"], "authors": ["Leonard Berrada", "Andrew Zisserman", "M. Pawan Kumar"], "pdf": "/pdf/ce7e434aabd4bf9e453cdfbd301971686c81da17.pdf", "paperhash": "berrada|deep_frankwolfe_for_neural_network_optimization", "TL;DR": "We train neural networks by locally linearizing them and using a linear SVM solver (Frank-Wolfe) at each iteration.", "_bibtex": "@inproceedings{\nberrada2018deep,\ntitle={Deep Frank-Wolfe For Neural Network Optimization},\nauthor={Leonard Berrada and Andrew Zisserman and M. Pawan Kumar},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SyVU6s05K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper807/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619004, "tddate": null, "super": null, "final": null, "reply": {"forum": "SyVU6s05K7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper807/Authors", "ICLR.cc/2019/Conference/Paper807/Reviewers", "ICLR.cc/2019/Conference/Paper807/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper807/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper807/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper807/Authors|ICLR.cc/2019/Conference/Paper807/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper807/Reviewers", "ICLR.cc/2019/Conference/Paper807/Authors", "ICLR.cc/2019/Conference/Paper807/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619004}}}, {"id": "Syei-gKKTm", "original": null, "number": 1, "cdate": 1542193154569, "ddate": null, "tcdate": 1542193154569, "tmdate": 1542193154569, "tddate": null, "forum": "SyVU6s05K7", "replyto": "r1g2QHW0h7", "invitation": "ICLR.cc/2019/Conference/-/Paper807/Official_Comment", "content": {"title": "Thanks for the detailed review and the suggestions.", "comment": "We thank the reviewer for their detailed review and for their suggestions. We answer point by point:\n\n*FW vs BCFW*\nThe (primal) proximal problem is created for a mini-batch of samples, and not for the entire data set (details in section 3.2). In other words, the primal problem consists of the proximal term which encourages proximity to the current iterate, the linearized regularization, and the average over the mini-batch of the losses applied to the linearized model. As a result, we can compute the Frank-Wolfe update for all dual coordinates simultaneously, and we do not need to operate in a block-coordinate fashion. We have included this clarification in the new version of the paper.\n\n*Batch-Size*\nWe thank the reviewer for this suggestion. We have adapted the description of Algorithm 1 accordingly.\n\n*Convex-Conjugate Loss*\nIn order to compare the DFW algorithm to the strongest possible baselines, we choose the baselines to use the CE loss in the CIFAR experiments. Indeed we have generally found CE to help the baselines in this setting. In addition, the hand-designed learning rate schedule of SGD and the l2 regularization were originally tuned for CE. \nIn the case of the SNLI data set, we allow the baseline to use either CE or SVM because using the hinge loss can increase their performance.\nFinally, we choose to always employ the multi-class hinge loss for DFW because it gives an optimal step-size in closed form for the dual, which is a key strength of the formulation.\n\n*BCFW vs BCD*\nWe thank the reviewer for this recommendation. It would be interesting indeed to explore how to exploit such updates in the context of the composite minimization framework for deep neural networks. In our case, we emphasize that for speed reasons, it is crucial to process the samples within a mini-batch in parallel, and this does not look straightforward with the algorithm in [3, E.3]. Therefore we believe that for this setting the FW algorithm permits faster updates thanks to an easy parallelization over the mini-batch on GPU.\n\n\n*Hyper-parameter*\nCounting a single hyper-parameter for SGD implicitly assumes that SGD can employ a constant step-size. Using such a constant step-size for SGD would incur a significant loss of performance (e.g. at least a few percents on the CIFAR data set). Therefore in order to obtain good performance, SGD requires a manual schedule of the learning rate, which involves many hyper-parameters to tune in practice.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper807/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper807/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper807/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Frank-Wolfe For Neural Network Optimization", "abstract": "Learning a deep neural network requires solving a challenging optimization problem: it is a high-dimensional, non-convex and non-smooth minimization problem with a large number of terms. The current practice in neural network optimization is to rely on the stochastic gradient descent (SGD) algorithm or its adaptive variants. However, SGD requires a hand-designed schedule for the learning rate. In addition, its adaptive variants tend to produce solutions that generalize less well on unseen data than SGD with a hand-designed schedule. We present an optimization method that offers empirically the best of both worlds: our algorithm yields good generalization performance while requiring only one hyper-parameter. Our approach is based on a composite proximal framework, which exploits the compositional nature of deep neural networks and can leverage powerful convex optimization algorithms by design. Specifically, we employ the Frank-Wolfe (FW) algorithm for SVM, which computes an optimal step-size in closed-form at each time-step. We further show that the descent direction is given by a simple backward pass in the network, yielding the same computational cost per iteration as SGD. We present experiments on the CIFAR and SNLI data sets, where we demonstrate the significant superiority of our method over Adam, Adagrad, as well as the recently proposed BPGrad and AMSGrad. Furthermore, we compare our algorithm to SGD with a hand-designed learning rate schedule, and show that it provides similar generalization while often converging faster. The code is publicly available at https://github.com/oval-group/dfw.", "keywords": ["optimization", "conditional gradient", "Frank-Wolfe", "SVM"], "authorids": ["lberrada@robots.ox.ac.uk", "az@robots.ox.ac.uk", "pawan@robots.ox.ac.uk"], "authors": ["Leonard Berrada", "Andrew Zisserman", "M. Pawan Kumar"], "pdf": "/pdf/ce7e434aabd4bf9e453cdfbd301971686c81da17.pdf", "paperhash": "berrada|deep_frankwolfe_for_neural_network_optimization", "TL;DR": "We train neural networks by locally linearizing them and using a linear SVM solver (Frank-Wolfe) at each iteration.", "_bibtex": "@inproceedings{\nberrada2018deep,\ntitle={Deep Frank-Wolfe For Neural Network Optimization},\nauthor={Leonard Berrada and Andrew Zisserman and M. Pawan Kumar},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SyVU6s05K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper807/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619004, "tddate": null, "super": null, "final": null, "reply": {"forum": "SyVU6s05K7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper807/Authors", "ICLR.cc/2019/Conference/Paper807/Reviewers", "ICLR.cc/2019/Conference/Paper807/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper807/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper807/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper807/Authors|ICLR.cc/2019/Conference/Paper807/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper807/Reviewers", "ICLR.cc/2019/Conference/Paper807/Authors", "ICLR.cc/2019/Conference/Paper807/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619004}}}, {"id": "r1g2QHW0h7", "original": null, "number": 3, "cdate": 1541440804013, "ddate": null, "tcdate": 1541440804013, "tmdate": 1541533674236, "tddate": null, "forum": "SyVU6s05K7", "replyto": "SyVU6s05K7", "invitation": "ICLR.cc/2019/Conference/-/Paper807/Official_Review", "content": {"title": "Interesting approach with room for improvement", "review": "Dual Block-Coordinate Frank-Wolfe (Dual-BCFW) has been widely used in the literature of non-smooth and strongly-convex stochastic optimization problems, such as (structural) Support Vector Machine. To my knowledge, the submission is the first sound attempt to adapt this type of Dual-based algorithm for optimization of Deep Neural Network, which employs a proximal-point method that linearizes not the whole loss function but only the DNN (up to the logits) to form a convex subproblem and then deal with the loss part in the dual.\n\nThe attempt is not perfect (actually with a couple of issues detailed below), but the proposed approach is inspiring and I personally would love it published to encourage more development along this thread.  The following points out a couple of items that could probably help further improve the paper.\n\n*FW vs BCFW*\n\nThe algorithm employed in the paper is actually not Frank-Wolfe (FW) but Block-Coordinate Frank-Wolfe (BCFW), as it minimizes w.r.t. a block of dual variables belonging to the min-batch of samples.\n\n*Batch Size*\n\nThough the algorithm can be easily extended to the min-batch case, the author should discuss more how the batch size is interpreted in this case (i.e. minimizing w.r.t. a larger block of dual variables belonging to the batch of samples) and the algorithmic block (Algorithm 1) should be presented in a way reflecting the batch size since this is the way people use an algorithm in practice (to improve the utilization rate of a GPU).\n\n*Convex-Conjugate Loss*\n\nThe Dual FW algorithm does not need to be used along with the hinge loss (SVM loss). All convex loss function can derive a dual formulation based on its convex-conjugate. See [1,2] for examples. It would be more insightful to compare SGD vs dual-BCFW when both of them are optimizing the same loss functions (either hinge loss or cross-entropy loss) in the experimental comparison.\n\n[1] Shalev-Shwartz, Shai, and Tong Zhang. \"Stochastic dual coordinate ascent methods for regularized loss minimization.\" JMLR (2013)\n[2] Tomioka, Ryota, Taiji Suzuki, and Masashi Sugiyama. \"Super-linear convergence of dual augmented Lagrangian algorithm for sparsity regularized estimation.\" JMLR (2011).\n\n*BCFW vs BCD*\n\nActually, (Lacoste-Julien, S. et al., 2013) proposes Dual-BCFW to optimize structural SVM because the problem contains exponentially many number of dual variables. For typical multiclass hinge loss problem the Dual Block-Coordinate Descent that minimizes w.r.t. all dual variables of a sample in a closed-form update converges faster without extra computational cost. See the details in, for example, [3, appendix for the multiclass hinge loss case].\n\n[3] Fan, Rong-En, et al. \"LIBLINEAR: A library for large linear classification.\" JMLR (2008).\n\n*Hyper-Parameter*\n\nThe proposed dual-BCFW still contains a hyperparameter (eta) due to the need to introduce a convex subproblem, which makes its number of hyperparameters still the same to SGD. ", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper807/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Frank-Wolfe For Neural Network Optimization", "abstract": "Learning a deep neural network requires solving a challenging optimization problem: it is a high-dimensional, non-convex and non-smooth minimization problem with a large number of terms. The current practice in neural network optimization is to rely on the stochastic gradient descent (SGD) algorithm or its adaptive variants. However, SGD requires a hand-designed schedule for the learning rate. In addition, its adaptive variants tend to produce solutions that generalize less well on unseen data than SGD with a hand-designed schedule. We present an optimization method that offers empirically the best of both worlds: our algorithm yields good generalization performance while requiring only one hyper-parameter. Our approach is based on a composite proximal framework, which exploits the compositional nature of deep neural networks and can leverage powerful convex optimization algorithms by design. Specifically, we employ the Frank-Wolfe (FW) algorithm for SVM, which computes an optimal step-size in closed-form at each time-step. We further show that the descent direction is given by a simple backward pass in the network, yielding the same computational cost per iteration as SGD. We present experiments on the CIFAR and SNLI data sets, where we demonstrate the significant superiority of our method over Adam, Adagrad, as well as the recently proposed BPGrad and AMSGrad. Furthermore, we compare our algorithm to SGD with a hand-designed learning rate schedule, and show that it provides similar generalization while often converging faster. The code is publicly available at https://github.com/oval-group/dfw.", "keywords": ["optimization", "conditional gradient", "Frank-Wolfe", "SVM"], "authorids": ["lberrada@robots.ox.ac.uk", "az@robots.ox.ac.uk", "pawan@robots.ox.ac.uk"], "authors": ["Leonard Berrada", "Andrew Zisserman", "M. Pawan Kumar"], "pdf": "/pdf/ce7e434aabd4bf9e453cdfbd301971686c81da17.pdf", "paperhash": "berrada|deep_frankwolfe_for_neural_network_optimization", "TL;DR": "We train neural networks by locally linearizing them and using a linear SVM solver (Frank-Wolfe) at each iteration.", "_bibtex": "@inproceedings{\nberrada2018deep,\ntitle={Deep Frank-Wolfe For Neural Network Optimization},\nauthor={Leonard Berrada and Andrew Zisserman and M. Pawan Kumar},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SyVU6s05K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper807/Official_Review", "cdate": 1542234372500, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SyVU6s05K7", "replyto": "SyVU6s05K7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper807/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335806873, "tmdate": 1552335806873, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper807/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "BJlCnxODhX", "original": null, "number": 1, "cdate": 1541009589656, "ddate": null, "tcdate": 1541009589656, "tmdate": 1541533673801, "tddate": null, "forum": "SyVU6s05K7", "replyto": "SyVU6s05K7", "invitation": "ICLR.cc/2019/Conference/-/Paper807/Official_Review", "content": {"title": "Good Paper", "review": "This paper introduced a proximal approach to optimize neural networks by linearizing the network output instead of the loss function. They demonstrate their algorithm on multi-class hinge loss, where they can show that optimal step size can be computed in close form without significant additional cost. Their experimental results showed competitive performance to SGD/Adam on the same network architectures. \n\n1. Figure 1 is crucial to the algorithm design as it aims to prove that Loss-Preserving Linearization (LPL) preserves information on loss function. While the authors provided numerical plots to compare it with the SGD linearization, I personally prefer to see some analytically comparsion between SGD linearization and LPL even on the simplest case. An appendix with more numerical comparisons on other loss functions might also be insightful. \n2. It seems LPL is mainly compared to SGD for convergence (e.g. Fig 2). In Table 2 I saw some optimizers end up with much lower test accuracy. Can the authors show the convergence plots of these methods (similar to Figure 2)?", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper807/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Frank-Wolfe For Neural Network Optimization", "abstract": "Learning a deep neural network requires solving a challenging optimization problem: it is a high-dimensional, non-convex and non-smooth minimization problem with a large number of terms. The current practice in neural network optimization is to rely on the stochastic gradient descent (SGD) algorithm or its adaptive variants. However, SGD requires a hand-designed schedule for the learning rate. In addition, its adaptive variants tend to produce solutions that generalize less well on unseen data than SGD with a hand-designed schedule. We present an optimization method that offers empirically the best of both worlds: our algorithm yields good generalization performance while requiring only one hyper-parameter. Our approach is based on a composite proximal framework, which exploits the compositional nature of deep neural networks and can leverage powerful convex optimization algorithms by design. Specifically, we employ the Frank-Wolfe (FW) algorithm for SVM, which computes an optimal step-size in closed-form at each time-step. We further show that the descent direction is given by a simple backward pass in the network, yielding the same computational cost per iteration as SGD. We present experiments on the CIFAR and SNLI data sets, where we demonstrate the significant superiority of our method over Adam, Adagrad, as well as the recently proposed BPGrad and AMSGrad. Furthermore, we compare our algorithm to SGD with a hand-designed learning rate schedule, and show that it provides similar generalization while often converging faster. The code is publicly available at https://github.com/oval-group/dfw.", "keywords": ["optimization", "conditional gradient", "Frank-Wolfe", "SVM"], "authorids": ["lberrada@robots.ox.ac.uk", "az@robots.ox.ac.uk", "pawan@robots.ox.ac.uk"], "authors": ["Leonard Berrada", "Andrew Zisserman", "M. Pawan Kumar"], "pdf": "/pdf/ce7e434aabd4bf9e453cdfbd301971686c81da17.pdf", "paperhash": "berrada|deep_frankwolfe_for_neural_network_optimization", "TL;DR": "We train neural networks by locally linearizing them and using a linear SVM solver (Frank-Wolfe) at each iteration.", "_bibtex": "@inproceedings{\nberrada2018deep,\ntitle={Deep Frank-Wolfe For Neural Network Optimization},\nauthor={Leonard Berrada and Andrew Zisserman and M. Pawan Kumar},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SyVU6s05K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper807/Official_Review", "cdate": 1542234372500, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SyVU6s05K7", "replyto": "SyVU6s05K7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper807/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335806873, "tmdate": 1552335806873, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper807/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 13}