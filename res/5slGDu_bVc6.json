{"notes": [{"id": "5slGDu_bVc6", "original": "2UwyaLSJjPE", "number": 3153, "cdate": 1601308349840, "ddate": null, "tcdate": 1601308349840, "tmdate": 1614985729356, "tddate": null, "forum": "5slGDu_bVc6", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Learning from deep model via exploring local targets", "authorids": ["~Wenxian_Shi1", "~Yuxuan_Song2", "~Hao_Zhou6", "libohan.05@bytedance.com", "~Lei_Li11"], "authors": ["Wenxian Shi", "Yuxuan Song", "Hao Zhou", "Bohan Li", "Lei Li"], "keywords": [], "abstract": "Deep neural networks often have huge number of parameters, which posts challenges in deployment in application scenarios with limited memory and computation capacity. Knowledge distillation is one approach to derive compact models from bigger ones. \nHowever, it has been observed that a converged heavy teacher model is strongly constrained for learning a compact student network and could make the optimization subject to poor local optima.  In this paper, we propose proKT, a new model-agnostic method by projecting the supervision signals of a teacher model into the student's parameter space. Such projection is implemented by decomposing the training objective  into  local intermediate targets with approximate mirror descent technique. The proposed method could be less sensitive with the quirks during optimization which could result in a better local optima. Experiments on both image and text datasets show that our proposed proKT consistently achieves the state-of-the-art performance comparing to all existing knowledge distillation methods.  ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shi|learning_from_deep_model_via_exploring_local_targets", "supplementary_material": "/attachment/fa850b70eaac3888832747c0c071b9556dabbfe0.zip", "pdf": "/pdf/82a33a5e8371ab8bf3bd9340fd64425ccd1990d5.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=G-aWicaitm", "_bibtex": "@misc{\nshi2021learning,\ntitle={Learning from deep model via exploring local targets},\nauthor={Wenxian Shi and Yuxuan Song and Hao Zhou and Bohan Li and Lei Li},\nyear={2021},\nurl={https://openreview.net/forum?id=5slGDu_bVc6}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "H35TF6Onzmh", "original": null, "number": 1, "cdate": 1610040410339, "ddate": null, "tcdate": 1610040410339, "tmdate": 1610474007622, "tddate": null, "forum": "5slGDu_bVc6", "replyto": "5slGDu_bVc6", "invitation": "ICLR.cc/2021/Conference/Paper3153/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "This paper proposed a new variant of knowledge distillation. The basic idea is interesting although similar ideas have more or less appeared in the literature as pointed out by the reviewers. Our main concern on this work is that the real empirical improvements are too limited such that it is hard to conclude that the proposed method can really perform better than the baseline. In the meantime, the proposed method is much more computationally expensive. "}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning from deep model via exploring local targets", "authorids": ["~Wenxian_Shi1", "~Yuxuan_Song2", "~Hao_Zhou6", "libohan.05@bytedance.com", "~Lei_Li11"], "authors": ["Wenxian Shi", "Yuxuan Song", "Hao Zhou", "Bohan Li", "Lei Li"], "keywords": [], "abstract": "Deep neural networks often have huge number of parameters, which posts challenges in deployment in application scenarios with limited memory and computation capacity. Knowledge distillation is one approach to derive compact models from bigger ones. \nHowever, it has been observed that a converged heavy teacher model is strongly constrained for learning a compact student network and could make the optimization subject to poor local optima.  In this paper, we propose proKT, a new model-agnostic method by projecting the supervision signals of a teacher model into the student's parameter space. Such projection is implemented by decomposing the training objective  into  local intermediate targets with approximate mirror descent technique. The proposed method could be less sensitive with the quirks during optimization which could result in a better local optima. Experiments on both image and text datasets show that our proposed proKT consistently achieves the state-of-the-art performance comparing to all existing knowledge distillation methods.  ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shi|learning_from_deep_model_via_exploring_local_targets", "supplementary_material": "/attachment/fa850b70eaac3888832747c0c071b9556dabbfe0.zip", "pdf": "/pdf/82a33a5e8371ab8bf3bd9340fd64425ccd1990d5.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=G-aWicaitm", "_bibtex": "@misc{\nshi2021learning,\ntitle={Learning from deep model via exploring local targets},\nauthor={Wenxian Shi and Yuxuan Song and Hao Zhou and Bohan Li and Lei Li},\nyear={2021},\nurl={https://openreview.net/forum?id=5slGDu_bVc6}\n}"}, "tags": [], "invitation": {"reply": {"forum": "5slGDu_bVc6", "replyto": "5slGDu_bVc6", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040410326, "tmdate": 1610474007606, "id": "ICLR.cc/2021/Conference/Paper3153/-/Decision"}}}, {"id": "nJ6JuczOdB1", "original": null, "number": 2, "cdate": 1603682559519, "ddate": null, "tcdate": 1603682559519, "tmdate": 1606963600787, "tddate": null, "forum": "5slGDu_bVc6", "replyto": "5slGDu_bVc6", "invitation": "ICLR.cc/2021/Conference/Paper3153/-/Official_Review", "content": {"title": "The proposed method is highly similar to some prior works. Experiments are not sufficient.", "review": "This paper follows the work of RCO[1], where knowledge distillation is conducted by learning from the optimization trajectories of the teacher rather than the converged teacher solely. The main difference is that in the proposed method ProKT, the student model learns from the teacher model step by step, while RCO downsamples the teacher's training trajectory with some sampling strategies, such as equal epoch interval strategy and greedy search strategy. The authors argue that smoothly changed targets can help the student out of poor local optima.\n\nPros:\n1. Although some few works have made attempts to explore the training route of teacher for distillation, this research direction is still under-studied and of great potentials in my view.\n\n2. The idea of making the teacher aware of the student for distillation is interesting.\n\nCons:\n1. My main concern is that the proposed ProKT is highly similar to some prior works such as deep mutual learning [2], KDCL [3], and PCL [4] from the perspective of methodology, although their motivations are somewhat different. This paper lacks comparisons and clarifications of the differences between the proposed method and these prior methods.\n\n2. The experiments are not sufficient to validate the proposed method. On image recognition tasks, comparisons are only conducted under heterogeneous architectures. However, distillation results under the same-style architecture should also be provided to give a more comprehensive view. \n\n3. The proposed ProKT actually demands several times more computation resources than other methods like KD. Strictly speaking, evaluating the proposed method in accuracy only is not fair. How to make the proposed method more efficient, or how to make more fair comparisons should be carefully considered.\n\n[1] Knowledge Distillation via Route Constrained Optimization, ICCV 2019.\n[2] Deep mutual learning, CVPR 2018\n[3] Online Knowledge Distillation via Collaborative Learning, CVPR 2020.\n[4] Peer Collaborative Learning for Online Knowledge Distillation.\n\n==============\npost-rebuttal:\nI have read all the comments from other reviewers and replies from the authors. All the reviewers are leaning to reject this paper due to the limited novelty and unfair and incomplete experimental comparisons. The authors' reply does not address my concerns, so I keep my initial attitude towards this work.", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper3153/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3153/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning from deep model via exploring local targets", "authorids": ["~Wenxian_Shi1", "~Yuxuan_Song2", "~Hao_Zhou6", "libohan.05@bytedance.com", "~Lei_Li11"], "authors": ["Wenxian Shi", "Yuxuan Song", "Hao Zhou", "Bohan Li", "Lei Li"], "keywords": [], "abstract": "Deep neural networks often have huge number of parameters, which posts challenges in deployment in application scenarios with limited memory and computation capacity. Knowledge distillation is one approach to derive compact models from bigger ones. \nHowever, it has been observed that a converged heavy teacher model is strongly constrained for learning a compact student network and could make the optimization subject to poor local optima.  In this paper, we propose proKT, a new model-agnostic method by projecting the supervision signals of a teacher model into the student's parameter space. Such projection is implemented by decomposing the training objective  into  local intermediate targets with approximate mirror descent technique. The proposed method could be less sensitive with the quirks during optimization which could result in a better local optima. Experiments on both image and text datasets show that our proposed proKT consistently achieves the state-of-the-art performance comparing to all existing knowledge distillation methods.  ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shi|learning_from_deep_model_via_exploring_local_targets", "supplementary_material": "/attachment/fa850b70eaac3888832747c0c071b9556dabbfe0.zip", "pdf": "/pdf/82a33a5e8371ab8bf3bd9340fd64425ccd1990d5.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=G-aWicaitm", "_bibtex": "@misc{\nshi2021learning,\ntitle={Learning from deep model via exploring local targets},\nauthor={Wenxian Shi and Yuxuan Song and Hao Zhou and Bohan Li and Lei Li},\nyear={2021},\nurl={https://openreview.net/forum?id=5slGDu_bVc6}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "5slGDu_bVc6", "replyto": "5slGDu_bVc6", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3153/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538081227, "tmdate": 1606915775494, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3153/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3153/-/Official_Review"}}}, {"id": "ol-UgnIGof", "original": null, "number": 1, "cdate": 1603641529565, "ddate": null, "tcdate": 1603641529565, "tmdate": 1606456910108, "tddate": null, "forum": "5slGDu_bVc6", "replyto": "5slGDu_bVc6", "invitation": "ICLR.cc/2021/Conference/Paper3153/-/Official_Review", "content": {"title": "comments of step-wise distillation", "review": "Overview summary: \n\nThis submission studies the knowledge distillation approach for deep learning models. The aspect is focused on the intermediate target models when learning for the student models. The authors claimed the previous teacher targets are poor to make student model to local optima or be unstable. The authors propose a new \"model-agnostic\" method, named ProKT, proximal knowledge teaching, which is implemented by decomposing the training objective of the student model into several local intermediate targets with approximate mirror descent technique. Specifically, the training of the teacher model and student model is step-wise, which means the teacher model updates and distills the knowledge after one step, and the student model then takes the teacher model at the current step as a target, and train the student model itself. To make the training more stable or less sensitive, the teacher model is constrained by a gap bounding between the teacher output distribution and student output distribution (KL divergence). The authors then evaluate their method on image classification task (CIFAR-100) and several text classification tasks, under different model structures, different training objectives. The results show that ProKT is able to achieve effective performance compared to previous knowledge distillation methods. \n\nGeneral comments:\nOverall speaking, the motivation of this submission is clearly described and the idea is easy to follow. I understand the authors are making a better job towards the distillation approach. But after carefully reading the method and the experimental results, I do have several concerns, which make me feel things can be better improved or better modeled. Please refer to my following detailed comments on the pros and cons. \n\nPros:\n1. First, I personally like the idea of better studying knowledge distillation problem, since kd is a very important and effective method in current deep learning models. This submission studies the internal learning path of the student model from the teacher model, with the viewpoint of how to modify the teacher model instead of the student model. The main claim is about the teacher targets (multiple intermediate targets) should be continuous so that the student model can be more stable to follow the guided signals. This is a good point.\n2. The authors present a clear description of the motivation and the proposed approach, which is very easy to follow. As for the implementation, it seems easy to do, though the authors do not open source their implementation. \n3. The authors conduct experiments on two different domains, which are the image and text classifications. The verification is performed under different settings (models, training objectives) so to demonstrate the universal usefulness of the approach. \n\nCons:\nThough the interesting points and the general well modeling. I do feel several concerns as follows:\n1. To be honest, the proposed method mainly do a step-wise distillation for the student model. To make a better constraint and stable training, the teacher model is constrained with the learning of the student model through a KL divergence between the two output distributions. This contribution is a little bit limited. First, the step-wise distillation is straight forward, the authors mainly criticize the discrete targets from previous methods, but the continuous setting modeled in this paper is not a very elegant way or surprising way. This modeling would cost several problems (as I listed in the below comments). This method is also very similar to mutual training, instead of the teacher and student models are different architectures. Second, the constraint between the student model and teacher model is somehow hard to make a clear convincing claim. In my opinion, the teacher model aims to make a strong performance by learning from the supervised target $y$, the goal mainly targets at the performance, this constraint may somehow limit the performance of the strong teacher model (e.g., figure 2(b)). Though the teacher model wants to provide a continuous signal for the student model (as the author claimed), the \"step-wise learning/distillation\" can guarantee this assumption in most cases for the student model. The probable large gap is not very possible (if the author can give several strong examples). Therefore, this is not so convincing.\n2. The continuous targets distillation have some problems from my side. For example, the training cost is increased for the student model. Previous knowledge distillation methods usually do several times distillation or only one-time distillation, which should be efficient to train since the student model is smaller than the teacher model, therefore the training step is less and the convergence speed is faster. With the step-wise distillation, the update steps are increased to be the same as the teacher model.  Besides, for ProKT, the memory (e.g., internal backpropagation parameters) request is more than previous methods, since the approach should update the teacher model and student model at the same time. While for previous works, only the update for the student model parameters required. \n3. As for the experiments, in Table 1, the gap between RCO and ProKT is really limited, which makes it hard to say that ProKT is stronger or more effective than RCO. The performances are similar, only slightly better from ProKT. A more confused setting is in Table 2, the BERT$_6$ + ProKT ($\\lambda=0$) is similar to the next line, which means the constraint is not so important (though in bi-LSTM, it shows better performance), or at least not stable/universally work (somehow this verifies my point of the small gap between teacher and student model distributions of my point 2). This seems to be a negative point for the verification. The differences between the bi-LSTM and BERT experiments also should be better claimed. \n\nSeveral other points and questions:\n1. The detailed values for $\\lambda$ and $\\alpha$ should be reported, in order to give a clear setting. \n2. In figure 2(a), the bleu line fo the student model training is interesting, it first increases the loss, do the authors have more explanations?\n3. In figure 3(a), the better way is to show the valid acc instead of test acc, this would be fairer.\n4. Minor errors, in 4.3.1, the third line of the second phrase, \"It could be found hat xxx\" -> \"It could be found that xxx\". \n5. A suggestion is that the detailed parameter number should be reported for the teacher and student models. \n\n-------------------\nupdated comments:\nI thank the authors to provide responses before the last time of rebuttal. Some of the questions are addressed, but it seems many points still keep disagreement. For example, the contribution is still not clear to me The training cost is increased a lot, this can not be ignored, though I understand in the industry it may be less important. However, for academic research, we should care about this. Besides, it seems that all reviewers agree with the limited contribution and unsatisfied experiments. ", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper3153/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3153/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning from deep model via exploring local targets", "authorids": ["~Wenxian_Shi1", "~Yuxuan_Song2", "~Hao_Zhou6", "libohan.05@bytedance.com", "~Lei_Li11"], "authors": ["Wenxian Shi", "Yuxuan Song", "Hao Zhou", "Bohan Li", "Lei Li"], "keywords": [], "abstract": "Deep neural networks often have huge number of parameters, which posts challenges in deployment in application scenarios with limited memory and computation capacity. Knowledge distillation is one approach to derive compact models from bigger ones. \nHowever, it has been observed that a converged heavy teacher model is strongly constrained for learning a compact student network and could make the optimization subject to poor local optima.  In this paper, we propose proKT, a new model-agnostic method by projecting the supervision signals of a teacher model into the student's parameter space. Such projection is implemented by decomposing the training objective  into  local intermediate targets with approximate mirror descent technique. The proposed method could be less sensitive with the quirks during optimization which could result in a better local optima. Experiments on both image and text datasets show that our proposed proKT consistently achieves the state-of-the-art performance comparing to all existing knowledge distillation methods.  ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shi|learning_from_deep_model_via_exploring_local_targets", "supplementary_material": "/attachment/fa850b70eaac3888832747c0c071b9556dabbfe0.zip", "pdf": "/pdf/82a33a5e8371ab8bf3bd9340fd64425ccd1990d5.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=G-aWicaitm", "_bibtex": "@misc{\nshi2021learning,\ntitle={Learning from deep model via exploring local targets},\nauthor={Wenxian Shi and Yuxuan Song and Hao Zhou and Bohan Li and Lei Li},\nyear={2021},\nurl={https://openreview.net/forum?id=5slGDu_bVc6}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "5slGDu_bVc6", "replyto": "5slGDu_bVc6", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3153/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538081227, "tmdate": 1606915775494, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3153/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3153/-/Official_Review"}}}, {"id": "VzZpZ5S-iF0", "original": null, "number": 5, "cdate": 1606215433628, "ddate": null, "tcdate": 1606215433628, "tmdate": 1606215510341, "tddate": null, "forum": "5slGDu_bVc6", "replyto": "ol-UgnIGof", "invitation": "ICLR.cc/2021/Conference/Paper3153/-/Official_Comment", "content": {"title": "Thanks for your insightful comments!", "comment": "Thanks for your insightful comments! Following are responses to your questions.\n\n### Q: the contribution is limited.\nThe contribution of our method includes two folds. First, we replace the discrete targets with continuous targets, which are constrained by the KL divergence to student distribution. Furthermore, we theoretically related our iterative optimization process with a mirror descent optimization, which reveals the insight of this model as projecting the teacher distributions to the distribution space of student models. As far as we know, other works such as mutual learning do not give such insight.\n\nThe constraint for teacher training is essential in our approach, for it represents the projection of the updated teacher distribution to the neighborhood of student distribution. For knowledge distillation, a teacher model with stronger performance does not always induce a better student, which has been proven empirically by a series of work, such as TA learning and RCO. Instead, a teacher providing easy-to-learn targets for students is beneficial. Thus, we introduce an explicit constraint for teacher training to guarantee the target distributions close to student distributions. Without this explicit constraint, the direction of parameters update is only determined by its cross-entropy with groud-truth, and it might cause the updated distribution to deviate heavily from the directions which are aligned with student distribution.\n\n### Q: comparison with mutual learning\nIn approach, the key difference between our method and mutual learning is that prokt has an **asymmetric** training objective: the student model does not get the supervision signal from the ground truth label while the teacher model does which corresponds to our theoretical motivation of gradient projection.\n\n### Q: training cost for ProKT\nThe target of knowledge distillation is to improve the efficiency of inference by using smaller and faster models. The training cost is not the main concern for the deployment of deep learning models.\n\n### Q: the gap between RCO and ProKT is limited and the differences between the bi-LSTM and BERT experiments also should be better claimed\nA: The improvement of ProKT over RCO is consistent and reliable. Furthermore, when the student model is heterogeneous to the teacher model (e.g. BERT_base to biLSTM), the advantage of our method will be more significant, because ProKT is able to reduce the capacity gap between teacher and student by projecting the signal of the teacher model into the function space of the student model. As shown in Table.2, improvement between heterogeneous models (e.g. BERT_base to biLSTM) is larger than improvement with homogeneous models (e.g. BERT_base to BERT_6). \n\n###  Other points:\n \n**Q: detailed values for lambda and alpha**\n\nWe chose the lambda according to the performance in the validation set. We will add the settings in the next version.\n\n **Q: the explanation for the first increases of loos In figure 2(a)**\n\nWe initialized the wight and bias of the last feed-forward layer as all zero, which makes the output distribution of teacher and student models become uniform distribution. Thus, in the beginning, the KL-divergence between student and teacher is zero. As the training progresses, the update of the teacher model is one step ahead of the student model and closer to the ground truth, which makes the discrepancy between the teacher model and the student model increase.  \n\n  **Q: minor errors  in 4.3.1 and suggestions for detailed parameter number**\n\nThanks for pointing out this error, and we have fixed it in the revision, and we will add the detailed parameter number in the next version."}, "signatures": ["ICLR.cc/2021/Conference/Paper3153/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3153/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning from deep model via exploring local targets", "authorids": ["~Wenxian_Shi1", "~Yuxuan_Song2", "~Hao_Zhou6", "libohan.05@bytedance.com", "~Lei_Li11"], "authors": ["Wenxian Shi", "Yuxuan Song", "Hao Zhou", "Bohan Li", "Lei Li"], "keywords": [], "abstract": "Deep neural networks often have huge number of parameters, which posts challenges in deployment in application scenarios with limited memory and computation capacity. Knowledge distillation is one approach to derive compact models from bigger ones. \nHowever, it has been observed that a converged heavy teacher model is strongly constrained for learning a compact student network and could make the optimization subject to poor local optima.  In this paper, we propose proKT, a new model-agnostic method by projecting the supervision signals of a teacher model into the student's parameter space. Such projection is implemented by decomposing the training objective  into  local intermediate targets with approximate mirror descent technique. The proposed method could be less sensitive with the quirks during optimization which could result in a better local optima. Experiments on both image and text datasets show that our proposed proKT consistently achieves the state-of-the-art performance comparing to all existing knowledge distillation methods.  ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shi|learning_from_deep_model_via_exploring_local_targets", "supplementary_material": "/attachment/fa850b70eaac3888832747c0c071b9556dabbfe0.zip", "pdf": "/pdf/82a33a5e8371ab8bf3bd9340fd64425ccd1990d5.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=G-aWicaitm", "_bibtex": "@misc{\nshi2021learning,\ntitle={Learning from deep model via exploring local targets},\nauthor={Wenxian Shi and Yuxuan Song and Hao Zhou and Bohan Li and Lei Li},\nyear={2021},\nurl={https://openreview.net/forum?id=5slGDu_bVc6}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "5slGDu_bVc6", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3153/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3153/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3153/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3153/Authors|ICLR.cc/2021/Conference/Paper3153/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3153/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923840571, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3153/-/Official_Comment"}}}, {"id": "ql6U_cw1CM1", "original": null, "number": 4, "cdate": 1606214626691, "ddate": null, "tcdate": 1606214626691, "tmdate": 1606214626691, "tddate": null, "forum": "5slGDu_bVc6", "replyto": "nJ6JuczOdB1", "invitation": "ICLR.cc/2021/Conference/Paper3153/-/Official_Comment", "content": {"title": "We sincerely thank you for your suggestions.", "comment": "### 1. Q: Relationship with deep mutual learning:\nA: We agree that the relationship between our proposed method and mutual learning needs to be further clarified. Moreover, the models involved in deep mutual learning settings typically have a similar capacity which differs from the capacity gap in the knowledge distillation settings. The key difference lies in the fact that though the learning procedure looks similar, Prokt has an asymmetric training objective: the student model does not get the supervision signal from the ground truth label while the teacher model does which corresponds to our motivation of gradient projection.\n\n### 2. Q: Experiments in different settings (same-style architecture)\nA: Thanks for your suggestion. We will include the additional results on same-style architecture settings in the revised version.\n\n### 3. Q\uff1aFair evaluation on Prokt:\nA: We agree that the required additional computation source should be considered in the experiment design to make a fair comparison. We will try different implementation which includes the discretized version of Prokt and continuous version of RCO as reviewer 3 mentioned above.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3153/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3153/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning from deep model via exploring local targets", "authorids": ["~Wenxian_Shi1", "~Yuxuan_Song2", "~Hao_Zhou6", "libohan.05@bytedance.com", "~Lei_Li11"], "authors": ["Wenxian Shi", "Yuxuan Song", "Hao Zhou", "Bohan Li", "Lei Li"], "keywords": [], "abstract": "Deep neural networks often have huge number of parameters, which posts challenges in deployment in application scenarios with limited memory and computation capacity. Knowledge distillation is one approach to derive compact models from bigger ones. \nHowever, it has been observed that a converged heavy teacher model is strongly constrained for learning a compact student network and could make the optimization subject to poor local optima.  In this paper, we propose proKT, a new model-agnostic method by projecting the supervision signals of a teacher model into the student's parameter space. Such projection is implemented by decomposing the training objective  into  local intermediate targets with approximate mirror descent technique. The proposed method could be less sensitive with the quirks during optimization which could result in a better local optima. Experiments on both image and text datasets show that our proposed proKT consistently achieves the state-of-the-art performance comparing to all existing knowledge distillation methods.  ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shi|learning_from_deep_model_via_exploring_local_targets", "supplementary_material": "/attachment/fa850b70eaac3888832747c0c071b9556dabbfe0.zip", "pdf": "/pdf/82a33a5e8371ab8bf3bd9340fd64425ccd1990d5.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=G-aWicaitm", "_bibtex": "@misc{\nshi2021learning,\ntitle={Learning from deep model via exploring local targets},\nauthor={Wenxian Shi and Yuxuan Song and Hao Zhou and Bohan Li and Lei Li},\nyear={2021},\nurl={https://openreview.net/forum?id=5slGDu_bVc6}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "5slGDu_bVc6", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3153/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3153/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3153/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3153/Authors|ICLR.cc/2021/Conference/Paper3153/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3153/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923840571, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3153/-/Official_Comment"}}}, {"id": "DxZDA7rAaeA", "original": null, "number": 3, "cdate": 1606214572280, "ddate": null, "tcdate": 1606214572280, "tmdate": 1606214572280, "tddate": null, "forum": "5slGDu_bVc6", "replyto": "rUjhkaW900", "invitation": "ICLR.cc/2021/Conference/Paper3153/-/Official_Comment", "content": {"title": "Thanks a lot for your constructive comments. ", "comment": "### 1.Q:  The relationship between the proposed method and the mutual learning setting?\nA\uff1aWe agree that the relationship between our proposed method and mutual learning needs to be further clarified. While our terminology selection is based on the ultimate goal of the training, i.e., whether the training procedure boosted one side or both sides. Moreover, the models involved in deep mutual learning settings typically have a similar capacity which differs from the capacity gap in the knowledge distillation settings.  The key difference lies in the fact that though the learning procedure looks similar, Prokt has an **asymmetric** training objective: the student model does not get the supervision signal from the ground truth label while the teacher model does which corresponds to our motivation of gradient projection.\n\n### 2.Q: Experiment results of multiple runs and different settings:\nA: We have performed part of additional experiments on the Cifar-10 settings, and the results are consistent with our current claim. We will include the new results along with the experiments of continuous RCO in future submission. "}, "signatures": ["ICLR.cc/2021/Conference/Paper3153/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3153/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning from deep model via exploring local targets", "authorids": ["~Wenxian_Shi1", "~Yuxuan_Song2", "~Hao_Zhou6", "libohan.05@bytedance.com", "~Lei_Li11"], "authors": ["Wenxian Shi", "Yuxuan Song", "Hao Zhou", "Bohan Li", "Lei Li"], "keywords": [], "abstract": "Deep neural networks often have huge number of parameters, which posts challenges in deployment in application scenarios with limited memory and computation capacity. Knowledge distillation is one approach to derive compact models from bigger ones. \nHowever, it has been observed that a converged heavy teacher model is strongly constrained for learning a compact student network and could make the optimization subject to poor local optima.  In this paper, we propose proKT, a new model-agnostic method by projecting the supervision signals of a teacher model into the student's parameter space. Such projection is implemented by decomposing the training objective  into  local intermediate targets with approximate mirror descent technique. The proposed method could be less sensitive with the quirks during optimization which could result in a better local optima. Experiments on both image and text datasets show that our proposed proKT consistently achieves the state-of-the-art performance comparing to all existing knowledge distillation methods.  ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shi|learning_from_deep_model_via_exploring_local_targets", "supplementary_material": "/attachment/fa850b70eaac3888832747c0c071b9556dabbfe0.zip", "pdf": "/pdf/82a33a5e8371ab8bf3bd9340fd64425ccd1990d5.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=G-aWicaitm", "_bibtex": "@misc{\nshi2021learning,\ntitle={Learning from deep model via exploring local targets},\nauthor={Wenxian Shi and Yuxuan Song and Hao Zhou and Bohan Li and Lei Li},\nyear={2021},\nurl={https://openreview.net/forum?id=5slGDu_bVc6}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "5slGDu_bVc6", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3153/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3153/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3153/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3153/Authors|ICLR.cc/2021/Conference/Paper3153/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3153/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923840571, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3153/-/Official_Comment"}}}, {"id": "yE7vs3cbFIn", "original": null, "number": 2, "cdate": 1606214417158, "ddate": null, "tcdate": 1606214417158, "tmdate": 1606214417158, "tddate": null, "forum": "5slGDu_bVc6", "replyto": "OvZFUDriAZL", "invitation": "ICLR.cc/2021/Conference/Paper3153/-/Official_Comment", "content": {"title": "Thanks a lot for your insightful comments.", "comment": "Thanks a lot for your insightful comments. In the following, we will respond to each of your concerns and suggestions.\n\n### 1. Q: the benefits over RCO\n\nCompared with RCO, our method has two benefits. In theory, our method distinguishes with RCO by the continuous target setting and the essential constraint in teacher training. The constraint provides a guarantee that the teacher model is projected in the neighborhood of student models. In the experiment, our improvement over RCO is consistent and reliable, especially in the heterogeneous setting, in which the capacity gap between the teacher model and the student model is significant while the projection process is specifically beneficial.\n\nBesides, the training time is not a bottleneck in practice. Because the model is trained once but runs unlimitedly, inference time is the main concern in the deployment of neural models. Our model has the same inference complexity as vanilla KD. \n\n### 2. Q: suggestion on an experiment of ImageNet and adding the error bars.\nThanks for your suggestions, we will conduct the experiment in ImageNet and add the error bars in the next version.\n\n### 3. Q: the possible variants of the ProKT algorithm \nThanks for your suggestions about more training variants. We will conduct the experiments and list the results in the next version.\n\n### 4. Q: minor detail in Algorithm 1\nThank you for pointing out that error in Algorithm 1. The teacher should get updated first. We fixed this error in the revision."}, "signatures": ["ICLR.cc/2021/Conference/Paper3153/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3153/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning from deep model via exploring local targets", "authorids": ["~Wenxian_Shi1", "~Yuxuan_Song2", "~Hao_Zhou6", "libohan.05@bytedance.com", "~Lei_Li11"], "authors": ["Wenxian Shi", "Yuxuan Song", "Hao Zhou", "Bohan Li", "Lei Li"], "keywords": [], "abstract": "Deep neural networks often have huge number of parameters, which posts challenges in deployment in application scenarios with limited memory and computation capacity. Knowledge distillation is one approach to derive compact models from bigger ones. \nHowever, it has been observed that a converged heavy teacher model is strongly constrained for learning a compact student network and could make the optimization subject to poor local optima.  In this paper, we propose proKT, a new model-agnostic method by projecting the supervision signals of a teacher model into the student's parameter space. Such projection is implemented by decomposing the training objective  into  local intermediate targets with approximate mirror descent technique. The proposed method could be less sensitive with the quirks during optimization which could result in a better local optima. Experiments on both image and text datasets show that our proposed proKT consistently achieves the state-of-the-art performance comparing to all existing knowledge distillation methods.  ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shi|learning_from_deep_model_via_exploring_local_targets", "supplementary_material": "/attachment/fa850b70eaac3888832747c0c071b9556dabbfe0.zip", "pdf": "/pdf/82a33a5e8371ab8bf3bd9340fd64425ccd1990d5.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=G-aWicaitm", "_bibtex": "@misc{\nshi2021learning,\ntitle={Learning from deep model via exploring local targets},\nauthor={Wenxian Shi and Yuxuan Song and Hao Zhou and Bohan Li and Lei Li},\nyear={2021},\nurl={https://openreview.net/forum?id=5slGDu_bVc6}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "5slGDu_bVc6", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3153/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3153/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3153/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3153/Authors|ICLR.cc/2021/Conference/Paper3153/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3153/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923840571, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3153/-/Official_Comment"}}}, {"id": "rUjhkaW900", "original": null, "number": 3, "cdate": 1603746569384, "ddate": null, "tcdate": 1603746569384, "tmdate": 1605024057420, "tddate": null, "forum": "5slGDu_bVc6", "replyto": "5slGDu_bVc6", "invitation": "ICLR.cc/2021/Conference/Paper3153/-/Official_Review", "content": {"title": "I vote for rejection as the paper lacks in novelty and results in small performance improvement.", "review": "**Summary**  \n\nThis paper presents a method of knowledge distillation, which showed better results than KD and RCO in experimental results. The difference between this method, ProKT and RCO is in the teacher model. In this method, the snapshot interval of the teacher model targeted by the student is more frequent (every training step) than the RCO (authors called this property as *continuous*), and the parameters of the teacher model are also updated during *student* training (*dynamic*).\n\n**Strong points**\n\n- The proposed method was evaluated for two modalities (visual and textual). As far as I know, most KD papers evaluate only one modality (image classifier or language model). Experimenting with both modalities increases the experimental robustness of ProKT.\n\n**Weak points**\n\n- This method is not traditional KD, and the way the teacher model learns simultaneously with the student is called mutual learning, as the author briefly mentioned in the paper. There is a terminological problem that arises because of this: if the teacher model is also trained with the student, then neither model can be called a \"teacher\" or \"student\". Thus I think that most of the terms used in the paper need to be rewritten based on mutual learning terminology.\n- In advance to the previous point, sharing knowledge between the big models and small models during the training is not new. To name a few, see [1, 2, 3].\n- Aside from everything, the performance improvement that ProKT alone brings seems limited compared to CRD, and it is difficult to trust because there is no information such as the number of runs or standard deviation in the results of the experiment.\n\n[1] Zhang, Ying, et al. \"Deep mutual learning.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018.  \n[2] Anil, Rohan, et al. \"Large scale distributed neural network training through online distillation.\" arXiv preprint arXiv:1804.03235 (2018).  \n[3] Park, Wonpyo, et al. \"Diversified Mutual Learning for Deep Metric Learning.\" arXiv preprint arXiv:2009.04170 (2020).  \n\n\n**Recommendation**\n\nI vote for rejection as the paper lacks in novelty and results in small performance improvement.\n\n**Supporting arguments for the recommendation**\n\nThe recommendation is based on what was said in the weakness section. In addition, RCO does not provide any information from the student to the teacher during the training (just as Hinton KD or CRD does), so for a fair comparison, it seems necessary to compare the performance with RCO by separating only *continuous* facet among the two properties of ProKT (*continuous* and *dynamic*) claimed by the authors.", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper3153/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3153/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning from deep model via exploring local targets", "authorids": ["~Wenxian_Shi1", "~Yuxuan_Song2", "~Hao_Zhou6", "libohan.05@bytedance.com", "~Lei_Li11"], "authors": ["Wenxian Shi", "Yuxuan Song", "Hao Zhou", "Bohan Li", "Lei Li"], "keywords": [], "abstract": "Deep neural networks often have huge number of parameters, which posts challenges in deployment in application scenarios with limited memory and computation capacity. Knowledge distillation is one approach to derive compact models from bigger ones. \nHowever, it has been observed that a converged heavy teacher model is strongly constrained for learning a compact student network and could make the optimization subject to poor local optima.  In this paper, we propose proKT, a new model-agnostic method by projecting the supervision signals of a teacher model into the student's parameter space. Such projection is implemented by decomposing the training objective  into  local intermediate targets with approximate mirror descent technique. The proposed method could be less sensitive with the quirks during optimization which could result in a better local optima. Experiments on both image and text datasets show that our proposed proKT consistently achieves the state-of-the-art performance comparing to all existing knowledge distillation methods.  ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shi|learning_from_deep_model_via_exploring_local_targets", "supplementary_material": "/attachment/fa850b70eaac3888832747c0c071b9556dabbfe0.zip", "pdf": "/pdf/82a33a5e8371ab8bf3bd9340fd64425ccd1990d5.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=G-aWicaitm", "_bibtex": "@misc{\nshi2021learning,\ntitle={Learning from deep model via exploring local targets},\nauthor={Wenxian Shi and Yuxuan Song and Hao Zhou and Bohan Li and Lei Li},\nyear={2021},\nurl={https://openreview.net/forum?id=5slGDu_bVc6}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "5slGDu_bVc6", "replyto": "5slGDu_bVc6", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3153/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538081227, "tmdate": 1606915775494, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3153/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3153/-/Official_Review"}}}, {"id": "OvZFUDriAZL", "original": null, "number": 4, "cdate": 1603908244494, "ddate": null, "tcdate": 1603908244494, "tmdate": 1605024057358, "tddate": null, "forum": "5slGDu_bVc6", "replyto": "5slGDu_bVc6", "invitation": "ICLR.cc/2021/Conference/Paper3153/-/Official_Review", "content": {"title": "This paper proposes a novel and model agnostic knowledge distillation method which improves the accuracy of the student model.", "review": "The proposed method is simple yet effective and achieves state-of-the-art results on both image and text classification tasks. Compared to the existing approach (RCO and TA-learning), the main advantage is that ProKT avoids manual control of the learning process, which makes the ProKT more generally applicable. Important related works such as RCO and TA-learning are cited and compared. The paper does not contain a theory part, but wherever possible, equations are provided to illustrate how the method works.\n\nConcerns: \nFrom my understanding, ProKT is a continuous version of RCO with an extra constraint on the KL divergence of the probabilistic distributions of the teacher and the student models. As listed in Table 1, the accuracy improvement over RCO is marginal. The computational cost of ProKT is much higher than RCO as it requires training the teacher and student simultaneously. Therefore, I think the authors need to justify the benefits over RCO other than avoiding selecting the discrete learning objectives manually. From my understanding, ProKT is a continuous version of RCO with an extra constraint on the KL divergence of the probabilistic distributions of the teacher and the student models. As listed in Table 1, the accuracy improvement over RCO is marginal. The computational cost of ProKT is much higher than RCO as it requires training the teacher and student simultaneously. Therefore, I think the authors need to justify the benefits over RCO other than avoiding selecting the discrete learning objectives manually. \n\nMoreover, this paper only provides an empirical evaluation on the CIFAR-100 dataset. It will be useful to see whether ProKT can achieve higher student accuracy on a more complex dataset such as ImageNet. The variability of results is also missing since there are no error bars for the results in Tables 1 and 2. Are the results averaged over multiple trials (if yes how many?), and is there a difference in variance between the methods?\n\nIn the proposed ProKT algorithm, both the student and teacher models are trained with the same number of iterations, and also the student model learns from an updated teacher model in each training step. Although the continuous and smooth learning targets might be desired,  I am curious about the performance of the possible variants of the ProKT algorithm. For example, let the teacher be several iterations ahead of the student or keep the same learning objective (teacher model) for several iterations, which can be viewed as something between ProKT and RCO.\n\nMinor detail:\nIn Algorithm 1, it shows that the student model is updated before the teacher model. Should it be the teacher gets updated first?\n\nReasons for score: In general, I like the idea of turning the discrete learning objective into a continuous one. However, as the proposed algorithm is largely inspired by RCO, the novelty of the paper is incremental. Besides, the performance improvement of the proposed ProKT scheme over RCO also is marginal. I would consider raising my score if the authors could better demonstrate the advantage of ProKT over RCO.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3153/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3153/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning from deep model via exploring local targets", "authorids": ["~Wenxian_Shi1", "~Yuxuan_Song2", "~Hao_Zhou6", "libohan.05@bytedance.com", "~Lei_Li11"], "authors": ["Wenxian Shi", "Yuxuan Song", "Hao Zhou", "Bohan Li", "Lei Li"], "keywords": [], "abstract": "Deep neural networks often have huge number of parameters, which posts challenges in deployment in application scenarios with limited memory and computation capacity. Knowledge distillation is one approach to derive compact models from bigger ones. \nHowever, it has been observed that a converged heavy teacher model is strongly constrained for learning a compact student network and could make the optimization subject to poor local optima.  In this paper, we propose proKT, a new model-agnostic method by projecting the supervision signals of a teacher model into the student's parameter space. Such projection is implemented by decomposing the training objective  into  local intermediate targets with approximate mirror descent technique. The proposed method could be less sensitive with the quirks during optimization which could result in a better local optima. Experiments on both image and text datasets show that our proposed proKT consistently achieves the state-of-the-art performance comparing to all existing knowledge distillation methods.  ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shi|learning_from_deep_model_via_exploring_local_targets", "supplementary_material": "/attachment/fa850b70eaac3888832747c0c071b9556dabbfe0.zip", "pdf": "/pdf/82a33a5e8371ab8bf3bd9340fd64425ccd1990d5.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=G-aWicaitm", "_bibtex": "@misc{\nshi2021learning,\ntitle={Learning from deep model via exploring local targets},\nauthor={Wenxian Shi and Yuxuan Song and Hao Zhou and Bohan Li and Lei Li},\nyear={2021},\nurl={https://openreview.net/forum?id=5slGDu_bVc6}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "5slGDu_bVc6", "replyto": "5slGDu_bVc6", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3153/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538081227, "tmdate": 1606915775494, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3153/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3153/-/Official_Review"}}}], "count": 10}