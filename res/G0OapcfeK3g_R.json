{"notes": [{"ddate": null, "legacy_migration": true, "tmdate": 1364235300000, "tcdate": 1364235300000, "number": 1, "id": "QOxbO7qFg2Och", "invitation": "ICLR.cc/2013/-/submission/reply", "forum": "G0OapcfeK3g_R", "replyto": "YlFHNQiVHDYVP", "signatures": ["Vamsi Potluru"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "Thanks again for your detailed comments. We will incorporate them into our paper."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Block Coordinate Descent for Sparse NMF", "decision": "conferencePoster-iclr2013-conference", "abstract": "Nonnegative matrix factorization (NMF) has become a ubiquitous tool for data analysis. An important variant is the sparse NMF problem which arises when we explicitly require the learnt features to be sparse. A natural measure of sparsity is the L$_0$ norm, however its optimization is NP-hard. Mixed norms, such as L$_1$/L$_2$ measure, have been shown to model sparsity robustly, based on intuitive attributes that such measures need to satisfy. This is in contrast to computationally cheaper alternatives such as the plain L$_1$ norm. However, present algorithms designed for optimizing the mixed norm L$_1$/L$_2$ are slow and other formulations for sparse NMF have been proposed such as those based on L$_1$ and L$_0$ norms. Our proposed algorithm allows us to solve the mixed norm sparsity constraints while not sacrificing computation time. We present experimental evidence on real-world datasets that shows our new algorithm performs an order of magnitude faster compared to the current state-of-the-art solvers optimizing the mixed norm and is suitable for large-scale datasets.", "pdf": "https://arxiv.org/abs/1301.3527", "paperhash": "potluru|block_coordinate_descent_for_sparse_nmf", "keywords": [], "conflicts": [], "authors": ["Vamsi Potluru", "Sergey M. Plis", "Jonathan Le Roux", "Barak A. Pearlmutter", "Vince D. Calhoun", "Thomas P. Hayes"], "authorids": ["vpotluru@mrn.org", "splis@mrn.org", "leroux@merl.com", "barak@cs.nuim.ie", "vcalhoun@mrn.org", "hayes@cs.unm.edu"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1363996080000, "tcdate": 1363996080000, "number": 4, "id": "YlFHNQiVHDYVP", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "G0OapcfeK3g_R", "replyto": "G0OapcfeK3g_R", "signatures": ["anonymous reviewer d723"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "Dear authors,\r\n\r\nthe revision of your paper is appreciated; the three major issues from my review have been resolved.\r\n\r\nI agree that explicit constraints may be harder to optimize, but the argument that then (non-expert) users can get the representation they want without fiddling parameters is a very good one and does motivate this line of research. I don't think a radiologist would want to spend too much time analyzing a brain scan using non-intuitive knobs. It would be nice if you could add some fMRI or sMRI images to enhance Figure 1.\r\n\r\nHere is a short list of things that may help you improve your paper (I emphasize that these are no must-haves):\r\n- Page 5, replace 'i' with 'i = 1' in the sums of the Lagrangian's derivatives (this would be consistent with the other sums).\r\n- In the first line after the derivatives, add gamma to the Lagrange parameters.\r\n- In the same paragraph, mention that the termination criterion from the for loop of Sparse-opt is equivalent to selecting the one that maximizes b' * y (you may want to cite [5] there).\r\n- In Algorithm 2 (Sparse-opt), p^star should be initialized just before the for loop (for the reason given in my review).\r\n- In line 3 of Algorithm 2, replace the two-element set (with ceil(k^2) and m) with an ordinary for-loop (with the new termination criterion, the order in which {ceil(k^2), ..., m} is traversed is important).\r\n- Page 7, Section 5.1: Tidy up the third bullet point (it's somehow intermixed with the van Hateren data set which you don't seem to use anyways).\r\n- Same section, fourth bullet point: use \tt or \u000for the URL to SPM5 (this would be consistent with the style you used for the other URLs). Add a point at the end of the text of that bullet point.\r\n- Page 8, Section 5.2, last paragraph: You should add one sentence how the running times behave when Bi-Sparse NMF from the Appendix is used, as there a Sparse-opt is carried out for high-dimensional vectors (dimension there equals the number of samples for the rows of H).\r\n- Page 10: Figure 6 should be moved to be on the same page as Section 5.2, where it is referenced.\r\n- Page 11, Section 7: Remove the 'heuristic' in the final line of the first paragraph.\r\n- Page 12, Reference [15] (Hsieh and Dhillon): The page here still reads 'xx'.\r\n- In Figures 4, 5 and 6 the y axes from the upper rows interfere with those of the lower rows.\r\n- There are also some stray white spaces throughout the paper that should be fixed."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Block Coordinate Descent for Sparse NMF", "decision": "conferencePoster-iclr2013-conference", "abstract": "Nonnegative matrix factorization (NMF) has become a ubiquitous tool for data analysis. An important variant is the sparse NMF problem which arises when we explicitly require the learnt features to be sparse. A natural measure of sparsity is the L$_0$ norm, however its optimization is NP-hard. Mixed norms, such as L$_1$/L$_2$ measure, have been shown to model sparsity robustly, based on intuitive attributes that such measures need to satisfy. This is in contrast to computationally cheaper alternatives such as the plain L$_1$ norm. However, present algorithms designed for optimizing the mixed norm L$_1$/L$_2$ are slow and other formulations for sparse NMF have been proposed such as those based on L$_1$ and L$_0$ norms. Our proposed algorithm allows us to solve the mixed norm sparsity constraints while not sacrificing computation time. We present experimental evidence on real-world datasets that shows our new algorithm performs an order of magnitude faster compared to the current state-of-the-art solvers optimizing the mixed norm and is suitable for large-scale datasets.", "pdf": "https://arxiv.org/abs/1301.3527", "paperhash": "potluru|block_coordinate_descent_for_sparse_nmf", "keywords": [], "conflicts": [], "authors": ["Vamsi Potluru", "Sergey M. Plis", "Jonathan Le Roux", "Barak A. Pearlmutter", "Vince D. Calhoun", "Thomas P. Hayes"], "authorids": ["vpotluru@mrn.org", "splis@mrn.org", "leroux@merl.com", "barak@cs.nuim.ie", "vcalhoun@mrn.org", "hayes@cs.unm.edu"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1363661460000, "tcdate": 1363661460000, "number": 7, "id": "cc18-e0C8uSHG", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "G0OapcfeK3g_R", "replyto": "G0OapcfeK3g_R", "signatures": ["Vamsi Potluru"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "Anonymous d723:\r\n\r\n1. Thanks for pointing out the bug in the projection operator algorithm Sparse-opt. We re-ran all the algorithms on\r\nthe datasets based on the suggested bugfix and generated new figures for all the datasets.                                   \r\n2. We highlight the efficiency of our algorithm (O(mlogm)) compared to worst-case scenario of O(m^2) for \r\nHoyer's projection algorithm. Our algorithm can be further improved to have linear time complexity by using a \r\npartitioning algorithm similar to the one in Quicksort.\r\n3. We have fixed most of the issues such as references to parallel updates, increasing figure sizes, modifying citations, and \r\nadding line numberings.\r\n\r\nAnonymous 1d08:\r\n\r\n1. Sparsity on the features can be set as user-defined intervals. This is illustrated on the ORL face dataset where\r\nwe are able to learn sets of local and global features. In practice, this enables the user to finely tune the \r\nfeatures based on domain knowledge.  For instance, in fMRI datasets, one could model the features for the \r\nbrain signals and that of the artifacts distinctly based on different sparsity intervals.\r\n\r\n2. Implicit regularization may lead to easier optimization problems but can be harder to interpret from a user point of view.\r\nThe regularization parameter maps to sparsity values of the features but it is hard to know what this mapping should be before\r\nthe algorithm is run.\r\n\r\n3. We have fixed the Lagrangian formulation to include the nonnegativity constraints and added a brief list of desirable \r\nproperties to section 2.2.\r\n\r\nAnonymous 202b:\r\n\r\nSparse PCA and dictionary learning are slightly different formulations than the one considered here. \r\nAlso, SPAMS does not consider the exact formulation of the problem we are tackling in this paper. \r\nWe are solving a explicitly constrained sparsity problem and this relates to the question posed by reviewer 1d08.\r\nSo, a direct comparison of running-times for algorithms solving different problem formulations would not be fair.   \r\nHopefully, the cost of running time of our algorithm pays off for applications where explicitly modeling the user requirements \r\nis of primary importance.\r\n\r\n\r\n-----------------\r\n\r\nWe have removed the convergence proof from the present draft based on the comments from the reviewers\r\nand Paul Scherrer. However, we are looking into fixing the proof for the final version.\r\nAlso, we are looking into other examples where one would like to explicitly constrain the sparsity of\r\nthe factorization.\r\n\r\nThanks again to all the reviewers for the constructive suggestions and insightful questions.\r\n\r\nIf the arxiv version is not updated by view time, please find a copy at:\r\nhttp://www.cs.unm.edu/~ismav/papers/ncnmf.pdf"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Block Coordinate Descent for Sparse NMF", "decision": "conferencePoster-iclr2013-conference", "abstract": "Nonnegative matrix factorization (NMF) has become a ubiquitous tool for data analysis. An important variant is the sparse NMF problem which arises when we explicitly require the learnt features to be sparse. A natural measure of sparsity is the L$_0$ norm, however its optimization is NP-hard. Mixed norms, such as L$_1$/L$_2$ measure, have been shown to model sparsity robustly, based on intuitive attributes that such measures need to satisfy. This is in contrast to computationally cheaper alternatives such as the plain L$_1$ norm. However, present algorithms designed for optimizing the mixed norm L$_1$/L$_2$ are slow and other formulations for sparse NMF have been proposed such as those based on L$_1$ and L$_0$ norms. Our proposed algorithm allows us to solve the mixed norm sparsity constraints while not sacrificing computation time. We present experimental evidence on real-world datasets that shows our new algorithm performs an order of magnitude faster compared to the current state-of-the-art solvers optimizing the mixed norm and is suitable for large-scale datasets.", "pdf": "https://arxiv.org/abs/1301.3527", "paperhash": "potluru|block_coordinate_descent_for_sparse_nmf", "keywords": [], "conflicts": [], "authors": ["Vamsi Potluru", "Sergey M. Plis", "Jonathan Le Roux", "Barak A. Pearlmutter", "Vince D. Calhoun", "Thomas P. Hayes"], "authorids": ["vpotluru@mrn.org", "splis@mrn.org", "leroux@merl.com", "barak@cs.nuim.ie", "vcalhoun@mrn.org", "hayes@cs.unm.edu"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1363287900000, "tcdate": 1363287900000, "number": 8, "id": "WYMDnhGXd0L_5", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "G0OapcfeK3g_R", "replyto": "G0OapcfeK3g_R", "signatures": ["Vamsi Potluru"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "Thanks to all the reviewers for their detailed and insightful comments and suggestions. \r\n\r\nWe are working on incorporating most of them in to our paper and should have the updated version this weekend."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Block Coordinate Descent for Sparse NMF", "decision": "conferencePoster-iclr2013-conference", "abstract": "Nonnegative matrix factorization (NMF) has become a ubiquitous tool for data analysis. An important variant is the sparse NMF problem which arises when we explicitly require the learnt features to be sparse. A natural measure of sparsity is the L$_0$ norm, however its optimization is NP-hard. Mixed norms, such as L$_1$/L$_2$ measure, have been shown to model sparsity robustly, based on intuitive attributes that such measures need to satisfy. This is in contrast to computationally cheaper alternatives such as the plain L$_1$ norm. However, present algorithms designed for optimizing the mixed norm L$_1$/L$_2$ are slow and other formulations for sparse NMF have been proposed such as those based on L$_1$ and L$_0$ norms. Our proposed algorithm allows us to solve the mixed norm sparsity constraints while not sacrificing computation time. We present experimental evidence on real-world datasets that shows our new algorithm performs an order of magnitude faster compared to the current state-of-the-art solvers optimizing the mixed norm and is suitable for large-scale datasets.", "pdf": "https://arxiv.org/abs/1301.3527", "paperhash": "potluru|block_coordinate_descent_for_sparse_nmf", "keywords": [], "conflicts": [], "authors": ["Vamsi Potluru", "Sergey M. Plis", "Jonathan Le Roux", "Barak A. Pearlmutter", "Vince D. Calhoun", "Thomas P. Hayes"], "authorids": ["vpotluru@mrn.org", "splis@mrn.org", "leroux@merl.com", "barak@cs.nuim.ie", "vcalhoun@mrn.org", "hayes@cs.unm.edu"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362274980000, "tcdate": 1362274980000, "number": 5, "id": "OEMFOvtudWEJh", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "G0OapcfeK3g_R", "replyto": "G0OapcfeK3g_R", "signatures": ["anonymous reviewer 202b"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Block Coordinate Descent for Sparse NMF", "review": "This paper considers a dictionary learning algorithm for positive data. The sparse NMF approach imposes sparsity on the atoms, and positivity on both atoms and decomposition coefficients. \r\n\r\nThe formulation is standard, i.e., applying a contraint on the L1-norm and L2-norm of the atoms. The (limited) novelty comes in the optimization formulation: (a) with respect to the atoms, block coordinate descent is used with exact updates which are based on an exact projection into a set of constraints (this projection appears to be novel, though the derivation is standard), (b) with respect to the decomposition coefficients, multiplicative updates are used.\r\n\r\nRunning-time comparisons are done, showing that the new formulation outperforms some existing approaches (from 2004 and 2007).\r\n\r\nPros:\r\n-Clever use of the structure of the problem for algorithm design\r\n\r\nCons:\r\n-The algorithm is not compared to the state of the art (there has been some progress in sparse PCA and dictionary learning since 2007). In particular, the SPAMS toolbox of [19] allows sparse dictionary learning with positivity constraints. A comparison with this toolbox would help to assess the significance of the improvements.\r\n-Limited novelty."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Block Coordinate Descent for Sparse NMF", "decision": "conferencePoster-iclr2013-conference", "abstract": "Nonnegative matrix factorization (NMF) has become a ubiquitous tool for data analysis. An important variant is the sparse NMF problem which arises when we explicitly require the learnt features to be sparse. A natural measure of sparsity is the L$_0$ norm, however its optimization is NP-hard. Mixed norms, such as L$_1$/L$_2$ measure, have been shown to model sparsity robustly, based on intuitive attributes that such measures need to satisfy. This is in contrast to computationally cheaper alternatives such as the plain L$_1$ norm. However, present algorithms designed for optimizing the mixed norm L$_1$/L$_2$ are slow and other formulations for sparse NMF have been proposed such as those based on L$_1$ and L$_0$ norms. Our proposed algorithm allows us to solve the mixed norm sparsity constraints while not sacrificing computation time. We present experimental evidence on real-world datasets that shows our new algorithm performs an order of magnitude faster compared to the current state-of-the-art solvers optimizing the mixed norm and is suitable for large-scale datasets.", "pdf": "https://arxiv.org/abs/1301.3527", "paperhash": "potluru|block_coordinate_descent_for_sparse_nmf", "keywords": [], "conflicts": [], "authors": ["Vamsi Potluru", "Sergey M. Plis", "Jonathan Le Roux", "Barak A. Pearlmutter", "Vince D. Calhoun", "Thomas P. Hayes"], "authorids": ["vpotluru@mrn.org", "splis@mrn.org", "leroux@merl.com", "barak@cs.nuim.ie", "vcalhoun@mrn.org", "hayes@cs.unm.edu"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362215700000, "tcdate": 1362215700000, "number": 3, "id": "gWF1WlYIRPpoT", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "G0OapcfeK3g_R", "replyto": "G0OapcfeK3g_R", "signatures": ["anonymous reviewer 1d08"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Block Coordinate Descent for Sparse NMF", "review": "Summary:\r\n\r\nThe paper presents a new optimization algorithm for solving NMF problems with the Euclidean norm as fitting cost and subject to sparsity constraints. The sparsity is imposed explicitly by adding an equality constraint to the optimization problem, imposing the sparsity measure proposed in [10] (referred as L1/L2 measure) of the columns of the matrix factors to be equal to a pre-defined constant. The contribution of this paper is to propose a more efficient optimization procedure for this problem. This is obtained mainly due to two variations on the original method introduced in [10]: (i) a block-coordinate descent strategy (ii) a fast algorithm for minimizing the subproblems involved in the obtained block coordinate scheme. Experimental evaluations show that the proposed algorithm runs substantially faster than previous works proposed in [7] and [10]. The paper is well written and the problem is clearly presented.\r\n\r\nPros:\r\n\r\n- The paper presents an algorithm to solve an optimization problem\r\nthat is significantly faster than available alternatives.\r\n\r\nCons:\r\n\r\n- it is not clear why this particular formulation is better than other similar alternatives that can be efficiently optimized\r\n- the proposed approach seems limited to work with the L2 norm as fitting cost.\r\n- the convergence results for the block coordinate scheme is not\r\napplicable to the proposed algorithm\r\n\r\nGeneral comment:\r\n\r\n1.\r\n\r\nThe measure used for sparsifying the NMF is an L1/L2 measure proposed in [10] (based on the relationship between the L1 and L2 norm of a given vector). The authors list interesting properties of this measure to justify its use and it seems a good option.\r\n\r\nI understand that it is not the purpose of this paper to study or compare different regularizers. However, I believe that the authors should provide clear examples where this precise formulation (with the equality constraint) is better. Maybe even empirical evaluation (or a reference to a paper performing this study). Having a hard constrain in the sparsity level for every data code (or dictionary atom) seems too restrictive.\r\n\r\nThis is a very relevant issue, since explicitly imposing the sparsity constraint leads to a harder optimization problem with slower optimization algorithms (as explained by the authors). An important modeling advantage is required to justify the increase in complexity.\r\n\r\nIn the work:\r\n\r\nBerry, M. W., et al. 'Algorithms and applications for approximate nonnegative matrix factorization.' Computational Statistics & Data Analysis 52.1 (2007): 155-173.\r\n\r\nthe authors adopt the sparsity measure form [10] but include it on a Lagrangian formulation. This implicit way of imposing sparsity can be combined with other fitting terms (e.g. beta divergences) and it is easier to optimize.\r\n\r\nThis was done with a very similar sparsity measure in the work:\r\n\r\nV, Tuomas. 'Monaural sound source separation by nonnegative matrix factorization with temporal continuity and sparseness criteria.' Audio, Speech, and Language Processing, IEEE Transactions on 15.3 (2007): 1066-1074.\r\n\r\nThe author proposes to add to the cost function a sparsity regularization term also of the form L1/L2 and was later used for audio source separation in:\r\n\r\nW. Felix, J. Feliu, and B. Schuller. 'Supervised and semi-supervised suppression of background music in monaural speech recordings.' Acoustics, Speech and Signal Processing (ICASSP), 2012 IEEE International Conference on. IEEE, 2012.\r\n\r\n2.\r\n\r\nThe strategy proposed in this paper alternatively fixes one matrix and minimizes over the other in a block coordinate fashion. In contrasts with [10], in which only a descent direction is searched. Maybe here there is another reason for the speed-up?\r\n\r\nWhen the minimization is performed on a matrix factor that is subject to the sparsity constraint the authors employ a block coordinate descent strategy, referred to as sequential-pass. The authors empirically demonstrate that this strategy leads to a significant improvement in time.\r\n\r\nThe  minimization over each block (or columns) leads to a linear maximization problem subject to constraining the L1 and L2 norm to be constant, referred as sparse-opt. The authors propose an algorithm for exactly solving this subproblem. This problem also appears in [10] but in a slightly different way. In [10] the author proposes a heuristic method for projecting a vector to meet the sparsity constraints (sort of a proximal projection but to a non-convex set).\r\n\r\nIn Theorem 3, the authors present a convergence result for a relaxed version of the sequential-pass. Specifically, they relax the constraint on the L2 norm to be an inequality (instead of an equality). In this new setting, imposing the L1 norm to be constant no longer implies that the sparsity measure is constant. The quotient L1/L2 should be used instead, but this is no longer coincides with the sparse-opt problem.\r\n\r\n\r\nOther minor comments:\r\n\r\n- In Section 2.2 and later in Section 6, the authors refer to the properties that a good sparsity measure should have, according to [12]. I think that it would help the clarity of the presentation to briefly list these properties in Section 2.2 instead of defining them within the text of Section 6. \r\n\r\n- In Section 3.1, the equation for the Lagrangian of the problem (5) should also include the term corresponding to the non-negativity constraint on y. This does not affect the derivation, since the multipliers for that constraint would be zero when the y_i are not, thus the obtained values of lambda, mu and obj would remain the same."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Block Coordinate Descent for Sparse NMF", "decision": "conferencePoster-iclr2013-conference", "abstract": "Nonnegative matrix factorization (NMF) has become a ubiquitous tool for data analysis. An important variant is the sparse NMF problem which arises when we explicitly require the learnt features to be sparse. A natural measure of sparsity is the L$_0$ norm, however its optimization is NP-hard. Mixed norms, such as L$_1$/L$_2$ measure, have been shown to model sparsity robustly, based on intuitive attributes that such measures need to satisfy. This is in contrast to computationally cheaper alternatives such as the plain L$_1$ norm. However, present algorithms designed for optimizing the mixed norm L$_1$/L$_2$ are slow and other formulations for sparse NMF have been proposed such as those based on L$_1$ and L$_0$ norms. Our proposed algorithm allows us to solve the mixed norm sparsity constraints while not sacrificing computation time. We present experimental evidence on real-world datasets that shows our new algorithm performs an order of magnitude faster compared to the current state-of-the-art solvers optimizing the mixed norm and is suitable for large-scale datasets.", "pdf": "https://arxiv.org/abs/1301.3527", "paperhash": "potluru|block_coordinate_descent_for_sparse_nmf", "keywords": [], "conflicts": [], "authors": ["Vamsi Potluru", "Sergey M. Plis", "Jonathan Le Roux", "Barak A. Pearlmutter", "Vince D. Calhoun", "Thomas P. Hayes"], "authorids": ["vpotluru@mrn.org", "splis@mrn.org", "leroux@merl.com", "barak@cs.nuim.ie", "vcalhoun@mrn.org", "hayes@cs.unm.edu"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362186300000, "tcdate": 1362186300000, "number": 1, "id": "KKA-Ef3zTjKbl", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "G0OapcfeK3g_R", "replyto": "G0OapcfeK3g_R", "signatures": ["anonymous reviewer d723"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Block Coordinate Descent for Sparse NMF", "review": "This paper proposes new algorithms to minimize the Non-negative Matrix Factorization (NMF) reconstruction error in Frobenius norm subject to additional sparseness constraints (NMFSC) as originally proposed by [R1]. The original method from [R1] to minimize the reconstruction error is a projected gradient descent. While in [R1] a geometrically inspired method is used to compute the projection onto the sparseness constraints, this paper proposes to use Lagrange multipliers instead. To solve the NMFSC problem, the authors propose to update the basis vectors one at a time (therefore their method is called Sequential Sparse NMF or SSNMF), while in ordinary NMF/NMFSC the entire matrix with the basis vectors is updated at once. Experiments are reported that show that SSNMF is one order of magnitude faster compared to the algorithm of [R1].\r\n\r\nThe paper may only propose more efficient algorithms to solve a known optimization problem instead of proposing new learnable representations, but the approach is interesting and the results are promising. There are however some major issues with the paper:\r\n\r\n(1) The sparseness projection of [R1] is essentially a Euclidean projection onto the intersection of a scaled probabilistic simplex (L1 sphere intersected with positive cone) and the scaled unit sphere (in L2 norm). The method of [R1] to compute this projection is an alternating projection algorithm (similar to the Dykstra algorithm for convex sets). The method was proven correct by [R2], and additionally it was shown that the projection is unique almost everywhere. Therefore, the method of [R1] and Algorithm 2 of the paper (Sparse-opt) should almost always compute the same result. In the paper, however, the sparseness projection of [R1] is denoted the 'projection-heuristic' while Sparse-opt is called 'exact', and when the projection of [R1] is used in the SSNMF algorithm instead of Sparse-opt the reconstruction error is no more monotonically decreasing as optimization proceeds. As both projection algorithms should compute the same, the plot should be identical for them when using the same starting points. Section 5.2 of the paper should be enhanced to verify whether both algorithms actually compute the same result and to find the bug that causes this contradiction.\r\n\r\n(2) The proposed Algorithm 2 can be considered a (non-trivial) extension of the projection onto a scaled probabilistic simplex as described by [R3] and is a valuable contribution. In the paper, there is however a bug in the execution (which may explain the discrepancies described in Issue (1)): There are no multipliers that enforce the entries of the projection to be non-negative, as would be required by Problem (5) in the paper. Analogously, in Algorithm 2 there is no check in the loop of Line 2 to guarantee the values for lambda and mu produce a feasible (that is non-negative) solution. I implemented the algorithm in Matlab and compared it to the sparseness projection of [R1] (which is freely available on the author's homepage). In the algorithm as given in the paper, p_star always equals m after line 3 and no correct solution to Problem (5) is found in general. If I add the check for a feasible solution, both Sparse-opt and the sparseness projection of [R1] compute numerically equal results. I first suspected there was a typo in the manuscript, but that still would not explain the contradictory results from Section 5.2 of the paper.\r\n\r\nOn the positive side, I did check the expressions for lambda, mu and obj as given in Algorithm 2, and found them correct. Further, the algorithm is empirically faster than that of [R1], and its run-time is guaranteed theoretically to be at most quasilinear.\r\n\r\nBased on the bugfix, I realized that the method from [R4] could be adapted to Sparse-opt to further enhance its run-time efficiency: Set p_star to m before the for loop of line 2 (in case all elements of the projection will be non-zero). Then, after computation of lambda and mu (obj does not need to be computed anymore with this modification), check if a_p < -mu(p) holds. If it does, set p_star to p - 1 and break the for loop. Line 3 of the algorithm should then be omitted. This modification fixes the algorithm, and additionally obj is not needed, and for lambda and mu simple scalars are sufficient to store at most two values of each.\r\n\r\n(3) As noted by Paul Shearer and confirmed by the first author of the paper (see public comments), the proof of Theorem 3 is flawed as the arguments there would only apply if the sparseness constraints would induce a convex set (which they don't). I wouldn't have any objections if Theorem 3 and its proof were withdrawn and removed from the manuscript.\r\n\r\nMoreover, I verified Algorithm 3 from the paper and found no obvious bugs. I implemented all algorithms and ran them on the ORL face data set and found that SSNMF computes a sparse representation. I did not check what happens without the bugfix for Algorithm 2, though. The authors should definitely fix the major issues and repeat the experiments before publication (judging from the run-time given in Figure 3 and Figure 4 this shouldn't take too long).\r\n\r\nThere are some minor issues too:\r\n- It should be briefly discussed whether SSNMF could benefit from a multi-threaded implementation as NMF/NMFSC do (in the experiments, the number of threads was set to one).\r\n- Figures should be enlarged and improved such that a difference between the plots is also noticeable when printed in black and white on letter size paper.\r\n- The references should be polished to achieve a consistent style (remove the URLs and ISSNs, don't use two different styles for JMLR ([7] and [10]) and NIPS ([6] and [17]), fix the page of [11], add volume and issue to [15] and [23], add the journal version of [21] unless that citation is withdrawn with Theorem 3, etc.).\r\n- Always cite using numbers in the main text ('cite{}') instead of using only the author names ('citeauthor{}') (e.g. Hoyer, Kim and Park, etc.), because now [9], [10] and [13], [14] could be confused.\r\n- The termination criteria should be described more elaborately for Algorithms 1, 3, and 4.\r\n- Page 2, just after Expression (1): This is only a convex combination if the rows of H are normed (wrt. L1), otherwise it's a conical combination.\r\n- Page 2, just after Expression (2): We use *subscripts* to denote... (missing s). Please also define what H_j^T would mean (is it (H_j)^T or (H^T)_j or something else?).\r\n- It would be nice to add line numbers to all algorithms (some have ones, some don't).\r\n- In Algorithm 3, Line 7: This should probably read G_j^T, as i is not defined here?\r\n- Mention the number of images for the sMRI data set in Section 5.1, and use '\u0000' or a footnote for the URL there.\r\n- Cite [7] in the third bullet point in Section 2.2.\r\n\r\nReferences:\r\n[R1] Hoyer. Non-negative Matrix Factorization with Sparseness Constraints. JMLR, 2004, vol. 5, pp. 1457-1469.\r\n[R2] Theis et al. First results on uniqueness of sparse non-negative matrix factorization. EUSIPCO, 2005, vol. 3, pp. 1672-1675.\r\n[R3] Duchi et al. Efficient Projections onto the l1-Ball for Learning in High Dimensions. ICML, 2008, pp. 272-279.\r\n[R4] Chen & Ye. Projection Onto A Simplex. arXiv:1101.6081v2, 2011."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Block Coordinate Descent for Sparse NMF", "decision": "conferencePoster-iclr2013-conference", "abstract": "Nonnegative matrix factorization (NMF) has become a ubiquitous tool for data analysis. An important variant is the sparse NMF problem which arises when we explicitly require the learnt features to be sparse. A natural measure of sparsity is the L$_0$ norm, however its optimization is NP-hard. Mixed norms, such as L$_1$/L$_2$ measure, have been shown to model sparsity robustly, based on intuitive attributes that such measures need to satisfy. This is in contrast to computationally cheaper alternatives such as the plain L$_1$ norm. However, present algorithms designed for optimizing the mixed norm L$_1$/L$_2$ are slow and other formulations for sparse NMF have been proposed such as those based on L$_1$ and L$_0$ norms. Our proposed algorithm allows us to solve the mixed norm sparsity constraints while not sacrificing computation time. We present experimental evidence on real-world datasets that shows our new algorithm performs an order of magnitude faster compared to the current state-of-the-art solvers optimizing the mixed norm and is suitable for large-scale datasets.", "pdf": "https://arxiv.org/abs/1301.3527", "paperhash": "potluru|block_coordinate_descent_for_sparse_nmf", "keywords": [], "conflicts": [], "authors": ["Vamsi Potluru", "Sergey M. Plis", "Jonathan Le Roux", "Barak A. Pearlmutter", "Vince D. Calhoun", "Thomas P. Hayes"], "authorids": ["vpotluru@mrn.org", "splis@mrn.org", "leroux@merl.com", "barak@cs.nuim.ie", "vcalhoun@mrn.org", "hayes@cs.unm.edu"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1361826300000, "tcdate": 1361826300000, "number": 2, "id": "Y8F18yu7HQ6aJ", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "G0OapcfeK3g_R", "replyto": "G0OapcfeK3g_R", "signatures": ["Vamsi Potluru"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "Thanks a lot for pointing this out. You are right about the issue. We\r\nare currently working on fixing the proof, as we hope that in our\r\nparticular case the objective function will force the L2 equality\r\nconstraint to be active at the optimum. The algorithm does still work\r\nfine in practice, and we have never encountered an occurrence of\r\ndivergence in our experiments. We will take out the proof if we\r\ncannot fix it by the review deadline."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Block Coordinate Descent for Sparse NMF", "decision": "conferencePoster-iclr2013-conference", "abstract": "Nonnegative matrix factorization (NMF) has become a ubiquitous tool for data analysis. An important variant is the sparse NMF problem which arises when we explicitly require the learnt features to be sparse. A natural measure of sparsity is the L$_0$ norm, however its optimization is NP-hard. Mixed norms, such as L$_1$/L$_2$ measure, have been shown to model sparsity robustly, based on intuitive attributes that such measures need to satisfy. This is in contrast to computationally cheaper alternatives such as the plain L$_1$ norm. However, present algorithms designed for optimizing the mixed norm L$_1$/L$_2$ are slow and other formulations for sparse NMF have been proposed such as those based on L$_1$ and L$_0$ norms. Our proposed algorithm allows us to solve the mixed norm sparsity constraints while not sacrificing computation time. We present experimental evidence on real-world datasets that shows our new algorithm performs an order of magnitude faster compared to the current state-of-the-art solvers optimizing the mixed norm and is suitable for large-scale datasets.", "pdf": "https://arxiv.org/abs/1301.3527", "paperhash": "potluru|block_coordinate_descent_for_sparse_nmf", "keywords": [], "conflicts": [], "authors": ["Vamsi Potluru", "Sergey M. Plis", "Jonathan Le Roux", "Barak A. Pearlmutter", "Vince D. Calhoun", "Thomas P. Hayes"], "authorids": ["vpotluru@mrn.org", "splis@mrn.org", "leroux@merl.com", "barak@cs.nuim.ie", "vcalhoun@mrn.org", "hayes@cs.unm.edu"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1360229520000, "tcdate": 1360229520000, "number": 6, "id": "9pQNdTOGrb9Pw", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "G0OapcfeK3g_R", "replyto": "G0OapcfeK3g_R", "signatures": ["Paul Shearer"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "The main convergence result in the paper, Theorem 3, does not prove what it purports to prove. Specifically the proof of Theorem 3 refers to a completely different optimization problem than the one the authors claim to be solving on page 5 and throughout the paper.\r\n\r\nIn the proof the authors replace the nonconvex constraint ||W_j||_2 = 1 on page 5 with the convex relaxation ||W_j||_2 <= 1. This relaxation appears to be standard, but it actually allows W_j to become arbitrarily nonsparse, for one may decrease L2 norm of a given W_j (while keeping ||W_j||_1 = k) simply by averaging a given W_j with a constant vector. Allowing arbitrary nonsparsity defeats the point of the proposed model, which is to maintain the sparsity of the W_j.\r\n\r\nTo keep the L1/L2 ratio bounded and thus maintain sparsity, the inequality should go in the other direction: ||W_j||_2 >= 1. But this is a nonconvex set so Nesterov's theorems do not apply. Theorem 3 for this problem must be proven by a different route (see for example Attouch 2011, http://www.optimization-online.org/DB_FILE/2010/12/2864.pdf), or one could forget proof and just say the algorithm seems to work fine empirically."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Block Coordinate Descent for Sparse NMF", "decision": "conferencePoster-iclr2013-conference", "abstract": "Nonnegative matrix factorization (NMF) has become a ubiquitous tool for data analysis. An important variant is the sparse NMF problem which arises when we explicitly require the learnt features to be sparse. A natural measure of sparsity is the L$_0$ norm, however its optimization is NP-hard. Mixed norms, such as L$_1$/L$_2$ measure, have been shown to model sparsity robustly, based on intuitive attributes that such measures need to satisfy. This is in contrast to computationally cheaper alternatives such as the plain L$_1$ norm. However, present algorithms designed for optimizing the mixed norm L$_1$/L$_2$ are slow and other formulations for sparse NMF have been proposed such as those based on L$_1$ and L$_0$ norms. Our proposed algorithm allows us to solve the mixed norm sparsity constraints while not sacrificing computation time. We present experimental evidence on real-world datasets that shows our new algorithm performs an order of magnitude faster compared to the current state-of-the-art solvers optimizing the mixed norm and is suitable for large-scale datasets.", "pdf": "https://arxiv.org/abs/1301.3527", "paperhash": "potluru|block_coordinate_descent_for_sparse_nmf", "keywords": [], "conflicts": [], "authors": ["Vamsi Potluru", "Sergey M. Plis", "Jonathan Le Roux", "Barak A. Pearlmutter", "Vince D. Calhoun", "Thomas P. Hayes"], "authorids": ["vpotluru@mrn.org", "splis@mrn.org", "leroux@merl.com", "barak@cs.nuim.ie", "vcalhoun@mrn.org", "hayes@cs.unm.edu"]}, "tags": [], "invitation": {}}}, {"replyto": null, "ddate": null, "legacy_migration": true, "tmdate": 1358403300000, "tcdate": 1358403300000, "number": 24, "id": "G0OapcfeK3g_R", "invitation": "ICLR.cc/2013/conference/-/submission", "forum": "G0OapcfeK3g_R", "signatures": ["vpotluru@mrn.org"], "readers": ["everyone"], "content": {"title": "Block Coordinate Descent for Sparse NMF", "decision": "conferencePoster-iclr2013-conference", "abstract": "Nonnegative matrix factorization (NMF) has become a ubiquitous tool for data analysis. An important variant is the sparse NMF problem which arises when we explicitly require the learnt features to be sparse. A natural measure of sparsity is the L$_0$ norm, however its optimization is NP-hard. Mixed norms, such as L$_1$/L$_2$ measure, have been shown to model sparsity robustly, based on intuitive attributes that such measures need to satisfy. This is in contrast to computationally cheaper alternatives such as the plain L$_1$ norm. However, present algorithms designed for optimizing the mixed norm L$_1$/L$_2$ are slow and other formulations for sparse NMF have been proposed such as those based on L$_1$ and L$_0$ norms. Our proposed algorithm allows us to solve the mixed norm sparsity constraints while not sacrificing computation time. We present experimental evidence on real-world datasets that shows our new algorithm performs an order of magnitude faster compared to the current state-of-the-art solvers optimizing the mixed norm and is suitable for large-scale datasets.", "pdf": "https://arxiv.org/abs/1301.3527", "paperhash": "potluru|block_coordinate_descent_for_sparse_nmf", "keywords": [], "conflicts": [], "authors": ["Vamsi Potluru", "Sergey M. Plis", "Jonathan Le Roux", "Barak A. Pearlmutter", "Vince D. Calhoun", "Thomas P. Hayes"], "authorids": ["vpotluru@mrn.org", "splis@mrn.org", "leroux@merl.com", "barak@cs.nuim.ie", "vcalhoun@mrn.org", "hayes@cs.unm.edu"]}, "writers": [], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1369422751717, "tmdate": 1496673673639, "cdate": 1496673673639, "tcdate": 1496673673639, "id": "ICLR.cc/2013/conference/-/submission", "writers": ["ICLR.cc/2013"], "signatures": ["OpenReview.net"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": []}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1377198751717}}}], "count": 10}