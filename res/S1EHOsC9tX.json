{"notes": [{"id": "S1EHOsC9tX", "original": "rylP2pxKYQ", "number": 353, "cdate": 1538087789291, "ddate": null, "tcdate": 1538087789291, "tmdate": 1548767423056, "tddate": null, "forum": "S1EHOsC9tX", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Towards the first adversarially robust neural network model on MNIST", "abstract": "Despite much effort, deep neural networks remain highly susceptible to tiny input perturbations and even for MNIST, one of the most common toy datasets in computer vision, no neural network model exists for which adversarial perturbations are large and make semantic sense to humans. We show that even the widely recognized and by far most successful L-inf defense by Madry et~al. (1) has lower L0 robustness than undefended networks and still highly susceptible to L2 perturbations, (2) classifies unrecognizable images with high certainty, (3) performs not much better than simple input binarization and (4) features adversarial perturbations that make little sense to humans. These results suggest that MNIST is far from being solved in terms of adversarial robustness. We present a novel robust classification model that performs analysis by synthesis using learned class-conditional data distributions. We derive bounds on the robustness and go to great length to empirically evaluate our model using maximally effective adversarial attacks by (a) applying decision-based, score-based, gradient-based and transfer-based attacks for several different Lp norms, (b) by designing a new attack that exploits the structure of our defended model and (c) by devising a novel decision-based attack that seeks to minimize the number of perturbed pixels (L0). The results suggest that our approach yields state-of-the-art robustness on MNIST against L0, L2 and L-inf perturbations and we demonstrate that most adversarial examples are strongly perturbed towards the perceptual boundary between the original and the adversarial class.", "keywords": ["adversarial examples", "MNIST", "robustness", "deep learning", "security"], "authorids": ["lukas.schott@bethgelab.org", "jonas.rauber@bethgelab.org", "matthias.bethge@bethgelab.org", "wieland.brendel@bethgelab.org"], "authors": ["Lukas Schott", "Jonas Rauber", "Matthias Bethge", "Wieland Brendel"], "pdf": "/pdf/ba645fe0aec0af7d9a401c8eb0c7462a25da3dba.pdf", "paperhash": "schott|towards_the_first_adversarially_robust_neural_network_model_on_mnist", "_bibtex": "@inproceedings{\nschott2018towards,\ntitle={Towards the first adversarially robust neural network model on {MNIST}},\nauthor={Lukas Schott and Jonas Rauber and Matthias Bethge and Wieland Brendel},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1EHOsC9tX},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 30, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "rygMClwWZN", "original": null, "number": 15, "cdate": 1545855177754, "ddate": null, "tcdate": 1545855177754, "tmdate": 1545855177754, "tddate": null, "forum": "S1EHOsC9tX", "replyto": "Hye7mhWGlE", "invitation": "ICLR.cc/2019/Conference/-/Paper353/Public_Comment", "content": {"comment": "Why is the l-0 norm an interesting threat model? Although it's a precursor to a more useful form of general robustness, it is very limited and it is not interesting to limit an attacker to this threat model. As such, it is indeed a short-coming of that defense that the robustness does not carry over to other threat models.", "title": "Threat models?"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards the first adversarially robust neural network model on MNIST", "abstract": "Despite much effort, deep neural networks remain highly susceptible to tiny input perturbations and even for MNIST, one of the most common toy datasets in computer vision, no neural network model exists for which adversarial perturbations are large and make semantic sense to humans. We show that even the widely recognized and by far most successful L-inf defense by Madry et~al. (1) has lower L0 robustness than undefended networks and still highly susceptible to L2 perturbations, (2) classifies unrecognizable images with high certainty, (3) performs not much better than simple input binarization and (4) features adversarial perturbations that make little sense to humans. These results suggest that MNIST is far from being solved in terms of adversarial robustness. We present a novel robust classification model that performs analysis by synthesis using learned class-conditional data distributions. We derive bounds on the robustness and go to great length to empirically evaluate our model using maximally effective adversarial attacks by (a) applying decision-based, score-based, gradient-based and transfer-based attacks for several different Lp norms, (b) by designing a new attack that exploits the structure of our defended model and (c) by devising a novel decision-based attack that seeks to minimize the number of perturbed pixels (L0). The results suggest that our approach yields state-of-the-art robustness on MNIST against L0, L2 and L-inf perturbations and we demonstrate that most adversarial examples are strongly perturbed towards the perceptual boundary between the original and the adversarial class.", "keywords": ["adversarial examples", "MNIST", "robustness", "deep learning", "security"], "authorids": ["lukas.schott@bethgelab.org", "jonas.rauber@bethgelab.org", "matthias.bethge@bethgelab.org", "wieland.brendel@bethgelab.org"], "authors": ["Lukas Schott", "Jonas Rauber", "Matthias Bethge", "Wieland Brendel"], "pdf": "/pdf/ba645fe0aec0af7d9a401c8eb0c7462a25da3dba.pdf", "paperhash": "schott|towards_the_first_adversarially_robust_neural_network_model_on_mnist", "_bibtex": "@inproceedings{\nschott2018towards,\ntitle={Towards the first adversarially robust neural network model on {MNIST}},\nauthor={Lukas Schott and Jonas Rauber and Matthias Bethge and Wieland Brendel},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1EHOsC9tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper353/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311859878, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "S1EHOsC9tX", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper353/Authors", "ICLR.cc/2019/Conference/Paper353/Reviewers", "ICLR.cc/2019/Conference/Paper353/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper353/Authors", "ICLR.cc/2019/Conference/Paper353/Reviewers", "ICLR.cc/2019/Conference/Paper353/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311859878}}}, {"id": "Hye7mhWGlE", "original": null, "number": 1, "cdate": 1544850458598, "ddate": null, "tcdate": 1544850458598, "tmdate": 1545354532393, "tddate": null, "forum": "S1EHOsC9tX", "replyto": "S1EHOsC9tX", "invitation": "ICLR.cc/2019/Conference/-/Paper353/Meta_Review", "content": {"metareview": "The paper presents a technique of training robust classification models that uses the input distribution within each class to achieve high accuracy and robustness against adversarial perturbations. \n\nStrengths:\n\n- The resulting model offers good robustness guarantees for a wide range of norm-bounded perturbations\n\n- The authors put a lot of care into the robustness evaluation\n\nWeaknesses: \n\n- Some of the \"shortcomings\" attributed to the previous work seem confusing, as the reported vulnerability corresponds to threat models that the previous work did not made claims about\n\nOverall, this looks like a valuable and interesting contribution.\n", "confidence": "5: The area chair is absolutely certain", "recommendation": "Accept (Poster)", "title": "Interesting progress on more versatile robustness guarantees"}, "signatures": ["ICLR.cc/2019/Conference/Paper353/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper353/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards the first adversarially robust neural network model on MNIST", "abstract": "Despite much effort, deep neural networks remain highly susceptible to tiny input perturbations and even for MNIST, one of the most common toy datasets in computer vision, no neural network model exists for which adversarial perturbations are large and make semantic sense to humans. We show that even the widely recognized and by far most successful L-inf defense by Madry et~al. (1) has lower L0 robustness than undefended networks and still highly susceptible to L2 perturbations, (2) classifies unrecognizable images with high certainty, (3) performs not much better than simple input binarization and (4) features adversarial perturbations that make little sense to humans. These results suggest that MNIST is far from being solved in terms of adversarial robustness. We present a novel robust classification model that performs analysis by synthesis using learned class-conditional data distributions. We derive bounds on the robustness and go to great length to empirically evaluate our model using maximally effective adversarial attacks by (a) applying decision-based, score-based, gradient-based and transfer-based attacks for several different Lp norms, (b) by designing a new attack that exploits the structure of our defended model and (c) by devising a novel decision-based attack that seeks to minimize the number of perturbed pixels (L0). The results suggest that our approach yields state-of-the-art robustness on MNIST against L0, L2 and L-inf perturbations and we demonstrate that most adversarial examples are strongly perturbed towards the perceptual boundary between the original and the adversarial class.", "keywords": ["adversarial examples", "MNIST", "robustness", "deep learning", "security"], "authorids": ["lukas.schott@bethgelab.org", "jonas.rauber@bethgelab.org", "matthias.bethge@bethgelab.org", "wieland.brendel@bethgelab.org"], "authors": ["Lukas Schott", "Jonas Rauber", "Matthias Bethge", "Wieland Brendel"], "pdf": "/pdf/ba645fe0aec0af7d9a401c8eb0c7462a25da3dba.pdf", "paperhash": "schott|towards_the_first_adversarially_robust_neural_network_model_on_mnist", "_bibtex": "@inproceedings{\nschott2018towards,\ntitle={Towards the first adversarially robust neural network model on {MNIST}},\nauthor={Lukas Schott and Jonas Rauber and Matthias Bethge and Wieland Brendel},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1EHOsC9tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper353/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353245191, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1EHOsC9tX", "replyto": "S1EHOsC9tX", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper353/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper353/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper353/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353245191}}}, {"id": "B1eu6wFIyN", "original": null, "number": 10, "cdate": 1544095680366, "ddate": null, "tcdate": 1544095680366, "tmdate": 1545234520153, "tddate": null, "forum": "S1EHOsC9tX", "replyto": "SyxI1zFUkE", "invitation": "ICLR.cc/2019/Conference/-/Paper353/Public_Comment", "content": {"comment": "I concur. Fashion-MNIST is a necessary datasets, which is similar to MNIST.  Why not to choose Fashion-MNIST  for analysis. The fact that the method performs well on MNIST is nice, but MNIST should be considered for what it is: a toy dataset. ", "title": "Concur "}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards the first adversarially robust neural network model on MNIST", "abstract": "Despite much effort, deep neural networks remain highly susceptible to tiny input perturbations and even for MNIST, one of the most common toy datasets in computer vision, no neural network model exists for which adversarial perturbations are large and make semantic sense to humans. We show that even the widely recognized and by far most successful L-inf defense by Madry et~al. (1) has lower L0 robustness than undefended networks and still highly susceptible to L2 perturbations, (2) classifies unrecognizable images with high certainty, (3) performs not much better than simple input binarization and (4) features adversarial perturbations that make little sense to humans. These results suggest that MNIST is far from being solved in terms of adversarial robustness. We present a novel robust classification model that performs analysis by synthesis using learned class-conditional data distributions. We derive bounds on the robustness and go to great length to empirically evaluate our model using maximally effective adversarial attacks by (a) applying decision-based, score-based, gradient-based and transfer-based attacks for several different Lp norms, (b) by designing a new attack that exploits the structure of our defended model and (c) by devising a novel decision-based attack that seeks to minimize the number of perturbed pixels (L0). The results suggest that our approach yields state-of-the-art robustness on MNIST against L0, L2 and L-inf perturbations and we demonstrate that most adversarial examples are strongly perturbed towards the perceptual boundary between the original and the adversarial class.", "keywords": ["adversarial examples", "MNIST", "robustness", "deep learning", "security"], "authorids": ["lukas.schott@bethgelab.org", "jonas.rauber@bethgelab.org", "matthias.bethge@bethgelab.org", "wieland.brendel@bethgelab.org"], "authors": ["Lukas Schott", "Jonas Rauber", "Matthias Bethge", "Wieland Brendel"], "pdf": "/pdf/ba645fe0aec0af7d9a401c8eb0c7462a25da3dba.pdf", "paperhash": "schott|towards_the_first_adversarially_robust_neural_network_model_on_mnist", "_bibtex": "@inproceedings{\nschott2018towards,\ntitle={Towards the first adversarially robust neural network model on {MNIST}},\nauthor={Lukas Schott and Jonas Rauber and Matthias Bethge and Wieland Brendel},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1EHOsC9tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper353/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311859878, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "S1EHOsC9tX", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper353/Authors", "ICLR.cc/2019/Conference/Paper353/Reviewers", "ICLR.cc/2019/Conference/Paper353/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper353/Authors", "ICLR.cc/2019/Conference/Paper353/Reviewers", "ICLR.cc/2019/Conference/Paper353/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311859878}}}, {"id": "r1lb0cYI1E", "original": null, "number": 12, "cdate": 1544096456927, "ddate": null, "tcdate": 1544096456927, "tmdate": 1545013318366, "tddate": null, "forum": "S1EHOsC9tX", "replyto": "SJxNatjURX", "invitation": "ICLR.cc/2019/Conference/-/Paper353/Public_Comment", "content": {"comment": "Why the authors choose these two classes (airplane, automobile) in the experiments, Not all classes on CIFAR 10 ? Do other categories have the similar results?", "title": "Question on CIFAR-10 experiments"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards the first adversarially robust neural network model on MNIST", "abstract": "Despite much effort, deep neural networks remain highly susceptible to tiny input perturbations and even for MNIST, one of the most common toy datasets in computer vision, no neural network model exists for which adversarial perturbations are large and make semantic sense to humans. We show that even the widely recognized and by far most successful L-inf defense by Madry et~al. (1) has lower L0 robustness than undefended networks and still highly susceptible to L2 perturbations, (2) classifies unrecognizable images with high certainty, (3) performs not much better than simple input binarization and (4) features adversarial perturbations that make little sense to humans. These results suggest that MNIST is far from being solved in terms of adversarial robustness. We present a novel robust classification model that performs analysis by synthesis using learned class-conditional data distributions. We derive bounds on the robustness and go to great length to empirically evaluate our model using maximally effective adversarial attacks by (a) applying decision-based, score-based, gradient-based and transfer-based attacks for several different Lp norms, (b) by designing a new attack that exploits the structure of our defended model and (c) by devising a novel decision-based attack that seeks to minimize the number of perturbed pixels (L0). The results suggest that our approach yields state-of-the-art robustness on MNIST against L0, L2 and L-inf perturbations and we demonstrate that most adversarial examples are strongly perturbed towards the perceptual boundary between the original and the adversarial class.", "keywords": ["adversarial examples", "MNIST", "robustness", "deep learning", "security"], "authorids": ["lukas.schott@bethgelab.org", "jonas.rauber@bethgelab.org", "matthias.bethge@bethgelab.org", "wieland.brendel@bethgelab.org"], "authors": ["Lukas Schott", "Jonas Rauber", "Matthias Bethge", "Wieland Brendel"], "pdf": "/pdf/ba645fe0aec0af7d9a401c8eb0c7462a25da3dba.pdf", "paperhash": "schott|towards_the_first_adversarially_robust_neural_network_model_on_mnist", "_bibtex": "@inproceedings{\nschott2018towards,\ntitle={Towards the first adversarially robust neural network model on {MNIST}},\nauthor={Lukas Schott and Jonas Rauber and Matthias Bethge and Wieland Brendel},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1EHOsC9tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper353/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311859878, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "S1EHOsC9tX", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper353/Authors", "ICLR.cc/2019/Conference/Paper353/Reviewers", "ICLR.cc/2019/Conference/Paper353/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper353/Authors", "ICLR.cc/2019/Conference/Paper353/Reviewers", "ICLR.cc/2019/Conference/Paper353/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311859878}}}, {"id": "S1xZgzdYJE", "original": null, "number": 12, "cdate": 1544286697381, "ddate": null, "tcdate": 1544286697381, "tmdate": 1544286697381, "tddate": null, "forum": "S1EHOsC9tX", "replyto": "BJx0OiRP1E", "invitation": "ICLR.cc/2019/Conference/-/Paper353/Official_Comment", "content": {"title": "white box", "comment": "We assume full knowledge of the model (= white-box setting) which we use to design a customised attack that optimises in the hidden latent space of the model. Furthermore we use score-based and decision-based adversarial attacks."}, "signatures": ["ICLR.cc/2019/Conference/Paper353/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper353/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper353/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards the first adversarially robust neural network model on MNIST", "abstract": "Despite much effort, deep neural networks remain highly susceptible to tiny input perturbations and even for MNIST, one of the most common toy datasets in computer vision, no neural network model exists for which adversarial perturbations are large and make semantic sense to humans. We show that even the widely recognized and by far most successful L-inf defense by Madry et~al. (1) has lower L0 robustness than undefended networks and still highly susceptible to L2 perturbations, (2) classifies unrecognizable images with high certainty, (3) performs not much better than simple input binarization and (4) features adversarial perturbations that make little sense to humans. These results suggest that MNIST is far from being solved in terms of adversarial robustness. We present a novel robust classification model that performs analysis by synthesis using learned class-conditional data distributions. We derive bounds on the robustness and go to great length to empirically evaluate our model using maximally effective adversarial attacks by (a) applying decision-based, score-based, gradient-based and transfer-based attacks for several different Lp norms, (b) by designing a new attack that exploits the structure of our defended model and (c) by devising a novel decision-based attack that seeks to minimize the number of perturbed pixels (L0). The results suggest that our approach yields state-of-the-art robustness on MNIST against L0, L2 and L-inf perturbations and we demonstrate that most adversarial examples are strongly perturbed towards the perceptual boundary between the original and the adversarial class.", "keywords": ["adversarial examples", "MNIST", "robustness", "deep learning", "security"], "authorids": ["lukas.schott@bethgelab.org", "jonas.rauber@bethgelab.org", "matthias.bethge@bethgelab.org", "wieland.brendel@bethgelab.org"], "authors": ["Lukas Schott", "Jonas Rauber", "Matthias Bethge", "Wieland Brendel"], "pdf": "/pdf/ba645fe0aec0af7d9a401c8eb0c7462a25da3dba.pdf", "paperhash": "schott|towards_the_first_adversarially_robust_neural_network_model_on_mnist", "_bibtex": "@inproceedings{\nschott2018towards,\ntitle={Towards the first adversarially robust neural network model on {MNIST}},\nauthor={Lukas Schott and Jonas Rauber and Matthias Bethge and Wieland Brendel},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1EHOsC9tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper353/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621604269, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1EHOsC9tX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper353/Authors", "ICLR.cc/2019/Conference/Paper353/Reviewers", "ICLR.cc/2019/Conference/Paper353/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper353/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper353/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper353/Authors|ICLR.cc/2019/Conference/Paper353/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper353/Reviewers", "ICLR.cc/2019/Conference/Paper353/Authors", "ICLR.cc/2019/Conference/Paper353/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621604269}}}, {"id": "BJx0OiRP1E", "original": null, "number": 13, "cdate": 1544182646489, "ddate": null, "tcdate": 1544182646489, "tmdate": 1544182646489, "tddate": null, "forum": "S1EHOsC9tX", "replyto": "S1EHOsC9tX", "invitation": "ICLR.cc/2019/Conference/-/Paper353/Public_Comment", "content": {"comment": "A nice paper, but there is one place I don't know very well.\n\nWhether the attacks is in white box mode or black box mode\uff1f\n\nI would appreciate it if the authors can answer my question.", "title": "attacks mode"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards the first adversarially robust neural network model on MNIST", "abstract": "Despite much effort, deep neural networks remain highly susceptible to tiny input perturbations and even for MNIST, one of the most common toy datasets in computer vision, no neural network model exists for which adversarial perturbations are large and make semantic sense to humans. We show that even the widely recognized and by far most successful L-inf defense by Madry et~al. (1) has lower L0 robustness than undefended networks and still highly susceptible to L2 perturbations, (2) classifies unrecognizable images with high certainty, (3) performs not much better than simple input binarization and (4) features adversarial perturbations that make little sense to humans. These results suggest that MNIST is far from being solved in terms of adversarial robustness. We present a novel robust classification model that performs analysis by synthesis using learned class-conditional data distributions. We derive bounds on the robustness and go to great length to empirically evaluate our model using maximally effective adversarial attacks by (a) applying decision-based, score-based, gradient-based and transfer-based attacks for several different Lp norms, (b) by designing a new attack that exploits the structure of our defended model and (c) by devising a novel decision-based attack that seeks to minimize the number of perturbed pixels (L0). The results suggest that our approach yields state-of-the-art robustness on MNIST against L0, L2 and L-inf perturbations and we demonstrate that most adversarial examples are strongly perturbed towards the perceptual boundary between the original and the adversarial class.", "keywords": ["adversarial examples", "MNIST", "robustness", "deep learning", "security"], "authorids": ["lukas.schott@bethgelab.org", "jonas.rauber@bethgelab.org", "matthias.bethge@bethgelab.org", "wieland.brendel@bethgelab.org"], "authors": ["Lukas Schott", "Jonas Rauber", "Matthias Bethge", "Wieland Brendel"], "pdf": "/pdf/ba645fe0aec0af7d9a401c8eb0c7462a25da3dba.pdf", "paperhash": "schott|towards_the_first_adversarially_robust_neural_network_model_on_mnist", "_bibtex": "@inproceedings{\nschott2018towards,\ntitle={Towards the first adversarially robust neural network model on {MNIST}},\nauthor={Lukas Schott and Jonas Rauber and Matthias Bethge and Wieland Brendel},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1EHOsC9tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper353/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311859878, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "S1EHOsC9tX", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper353/Authors", "ICLR.cc/2019/Conference/Paper353/Reviewers", "ICLR.cc/2019/Conference/Paper353/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper353/Authors", "ICLR.cc/2019/Conference/Paper353/Reviewers", "ICLR.cc/2019/Conference/Paper353/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311859878}}}, {"id": "SylDJKFLk4", "original": null, "number": 11, "cdate": 1544095966775, "ddate": null, "tcdate": 1544095966775, "tmdate": 1544096059377, "tddate": null, "forum": "S1EHOsC9tX", "replyto": "SJegx_sU0X", "invitation": "ICLR.cc/2019/Conference/-/Paper353/Public_Comment", "content": {"comment": "Nice to see the proposal work for 2 class CIFAR.  However, as the result show, the accuracy is greatly reduced. \nThere are some questions\uff1a\n\n1. Can you show us robustness results on 3-10 class CIFAR? As the category increases, will the robustness decrease?\n\n2. There are 10 categories in CIFAR10,  which two categories did you choose to experiment with? why?\n\n3. What about L0 robustness and L-inf robustness on 2 class CIFAR?", "title": "Robustness results on 3-10 class CIFAR\uff1f"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards the first adversarially robust neural network model on MNIST", "abstract": "Despite much effort, deep neural networks remain highly susceptible to tiny input perturbations and even for MNIST, one of the most common toy datasets in computer vision, no neural network model exists for which adversarial perturbations are large and make semantic sense to humans. We show that even the widely recognized and by far most successful L-inf defense by Madry et~al. (1) has lower L0 robustness than undefended networks and still highly susceptible to L2 perturbations, (2) classifies unrecognizable images with high certainty, (3) performs not much better than simple input binarization and (4) features adversarial perturbations that make little sense to humans. These results suggest that MNIST is far from being solved in terms of adversarial robustness. We present a novel robust classification model that performs analysis by synthesis using learned class-conditional data distributions. We derive bounds on the robustness and go to great length to empirically evaluate our model using maximally effective adversarial attacks by (a) applying decision-based, score-based, gradient-based and transfer-based attacks for several different Lp norms, (b) by designing a new attack that exploits the structure of our defended model and (c) by devising a novel decision-based attack that seeks to minimize the number of perturbed pixels (L0). The results suggest that our approach yields state-of-the-art robustness on MNIST against L0, L2 and L-inf perturbations and we demonstrate that most adversarial examples are strongly perturbed towards the perceptual boundary between the original and the adversarial class.", "keywords": ["adversarial examples", "MNIST", "robustness", "deep learning", "security"], "authorids": ["lukas.schott@bethgelab.org", "jonas.rauber@bethgelab.org", "matthias.bethge@bethgelab.org", "wieland.brendel@bethgelab.org"], "authors": ["Lukas Schott", "Jonas Rauber", "Matthias Bethge", "Wieland Brendel"], "pdf": "/pdf/ba645fe0aec0af7d9a401c8eb0c7462a25da3dba.pdf", "paperhash": "schott|towards_the_first_adversarially_robust_neural_network_model_on_mnist", "_bibtex": "@inproceedings{\nschott2018towards,\ntitle={Towards the first adversarially robust neural network model on {MNIST}},\nauthor={Lukas Schott and Jonas Rauber and Matthias Bethge and Wieland Brendel},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1EHOsC9tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper353/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311859878, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "S1EHOsC9tX", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper353/Authors", "ICLR.cc/2019/Conference/Paper353/Reviewers", "ICLR.cc/2019/Conference/Paper353/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper353/Authors", "ICLR.cc/2019/Conference/Paper353/Reviewers", "ICLR.cc/2019/Conference/Paper353/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311859878}}}, {"id": "SyxI1zFUkE", "original": null, "number": 9, "cdate": 1544094174108, "ddate": null, "tcdate": 1544094174108, "tmdate": 1544094174108, "tddate": null, "forum": "S1EHOsC9tX", "replyto": "SJegx_sU0X", "invitation": "ICLR.cc/2019/Conference/-/Paper353/Public_Comment", "content": {"comment": "As we know,  MNIST is too easy and over used, more importantly, it can not represent modern CV tasks.  Fashion-MNIST is an alternative dataset for MNIST. It would be interesting to see whether the proposal would work for more complicated tasks like Fashion-MNIST.", "title": "Have you tried the models on Fashion-MNIST?"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards the first adversarially robust neural network model on MNIST", "abstract": "Despite much effort, deep neural networks remain highly susceptible to tiny input perturbations and even for MNIST, one of the most common toy datasets in computer vision, no neural network model exists for which adversarial perturbations are large and make semantic sense to humans. We show that even the widely recognized and by far most successful L-inf defense by Madry et~al. (1) has lower L0 robustness than undefended networks and still highly susceptible to L2 perturbations, (2) classifies unrecognizable images with high certainty, (3) performs not much better than simple input binarization and (4) features adversarial perturbations that make little sense to humans. These results suggest that MNIST is far from being solved in terms of adversarial robustness. We present a novel robust classification model that performs analysis by synthesis using learned class-conditional data distributions. We derive bounds on the robustness and go to great length to empirically evaluate our model using maximally effective adversarial attacks by (a) applying decision-based, score-based, gradient-based and transfer-based attacks for several different Lp norms, (b) by designing a new attack that exploits the structure of our defended model and (c) by devising a novel decision-based attack that seeks to minimize the number of perturbed pixels (L0). The results suggest that our approach yields state-of-the-art robustness on MNIST against L0, L2 and L-inf perturbations and we demonstrate that most adversarial examples are strongly perturbed towards the perceptual boundary between the original and the adversarial class.", "keywords": ["adversarial examples", "MNIST", "robustness", "deep learning", "security"], "authorids": ["lukas.schott@bethgelab.org", "jonas.rauber@bethgelab.org", "matthias.bethge@bethgelab.org", "wieland.brendel@bethgelab.org"], "authors": ["Lukas Schott", "Jonas Rauber", "Matthias Bethge", "Wieland Brendel"], "pdf": "/pdf/ba645fe0aec0af7d9a401c8eb0c7462a25da3dba.pdf", "paperhash": "schott|towards_the_first_adversarially_robust_neural_network_model_on_mnist", "_bibtex": "@inproceedings{\nschott2018towards,\ntitle={Towards the first adversarially robust neural network model on {MNIST}},\nauthor={Lukas Schott and Jonas Rauber and Matthias Bethge and Wieland Brendel},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1EHOsC9tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper353/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311859878, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "S1EHOsC9tX", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper353/Authors", "ICLR.cc/2019/Conference/Paper353/Reviewers", "ICLR.cc/2019/Conference/Paper353/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper353/Authors", "ICLR.cc/2019/Conference/Paper353/Reviewers", "ICLR.cc/2019/Conference/Paper353/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311859878}}}, {"id": "rJlyOldp0X", "original": null, "number": 11, "cdate": 1543499879174, "ddate": null, "tcdate": 1543499879174, "tmdate": 1543499879174, "tddate": null, "forum": "S1EHOsC9tX", "replyto": "SyeyTVeH0Q", "invitation": "ICLR.cc/2019/Conference/-/Paper353/Official_Comment", "content": {"title": "Inferring latents decoder vs encoder, adversarial training", "comment": "1. \"The method goes to great computational expense to use Eq. 3 instead of Eq. 2. (8000 evaluations per sample) It would be interesting to see if it's worth it.\"\n\nWe performed a quick experiment with L2 Basic Iterative Method [1] for the ABS model and found that the median L2 robustness with the variational inference (Eq. 2) is 0.05 compared to 2.3 with optimization-based inference (Eq. 3). So indeed the optimization step is crucial to make the model robust.\n\n[1] https://foolbox.readthedocs.io/en/latest/modules/attacks/gradient.html#foolbox.attacks.L2BasicIterativeAttack\n\n\n2. \"Also, what if Madry's defense were trained to defend against L2=1.5 attacks? (This seems like a trivial generalization, but I'm not aware of anyone having done this). It would be interesting to see where such a defense would fit in Table 1.\"\n\nThat\u2019s indeed a very interesting question but the generalisation of Madry\u2019s defense is not as trivial (for a fair comparison we have to be careful in choosing the right iterative method (alternative to PGD) and choosing the optimal hyperparameters). We will try to follow up on this in the future but are currently concentrating on other parts of the ABS model (in particular scaling to more complex data sets)."}, "signatures": ["ICLR.cc/2019/Conference/Paper353/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper353/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper353/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards the first adversarially robust neural network model on MNIST", "abstract": "Despite much effort, deep neural networks remain highly susceptible to tiny input perturbations and even for MNIST, one of the most common toy datasets in computer vision, no neural network model exists for which adversarial perturbations are large and make semantic sense to humans. We show that even the widely recognized and by far most successful L-inf defense by Madry et~al. (1) has lower L0 robustness than undefended networks and still highly susceptible to L2 perturbations, (2) classifies unrecognizable images with high certainty, (3) performs not much better than simple input binarization and (4) features adversarial perturbations that make little sense to humans. These results suggest that MNIST is far from being solved in terms of adversarial robustness. We present a novel robust classification model that performs analysis by synthesis using learned class-conditional data distributions. We derive bounds on the robustness and go to great length to empirically evaluate our model using maximally effective adversarial attacks by (a) applying decision-based, score-based, gradient-based and transfer-based attacks for several different Lp norms, (b) by designing a new attack that exploits the structure of our defended model and (c) by devising a novel decision-based attack that seeks to minimize the number of perturbed pixels (L0). The results suggest that our approach yields state-of-the-art robustness on MNIST against L0, L2 and L-inf perturbations and we demonstrate that most adversarial examples are strongly perturbed towards the perceptual boundary between the original and the adversarial class.", "keywords": ["adversarial examples", "MNIST", "robustness", "deep learning", "security"], "authorids": ["lukas.schott@bethgelab.org", "jonas.rauber@bethgelab.org", "matthias.bethge@bethgelab.org", "wieland.brendel@bethgelab.org"], "authors": ["Lukas Schott", "Jonas Rauber", "Matthias Bethge", "Wieland Brendel"], "pdf": "/pdf/ba645fe0aec0af7d9a401c8eb0c7462a25da3dba.pdf", "paperhash": "schott|towards_the_first_adversarially_robust_neural_network_model_on_mnist", "_bibtex": "@inproceedings{\nschott2018towards,\ntitle={Towards the first adversarially robust neural network model on {MNIST}},\nauthor={Lukas Schott and Jonas Rauber and Matthias Bethge and Wieland Brendel},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1EHOsC9tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper353/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621604269, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1EHOsC9tX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper353/Authors", "ICLR.cc/2019/Conference/Paper353/Reviewers", "ICLR.cc/2019/Conference/Paper353/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper353/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper353/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper353/Authors|ICLR.cc/2019/Conference/Paper353/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper353/Reviewers", "ICLR.cc/2019/Conference/Paper353/Authors", "ICLR.cc/2019/Conference/Paper353/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621604269}}}, {"id": "HJgw5VPpAQ", "original": null, "number": 8, "cdate": 1543496847098, "ddate": null, "tcdate": 1543496847098, "tmdate": 1543497084121, "tddate": null, "forum": "S1EHOsC9tX", "replyto": "r1eCatzaRm", "invitation": "ICLR.cc/2019/Conference/-/Paper353/Public_Comment", "content": {"comment": "Thanks for your detailed reply, which clarifies most of my minor questions and concerns. \n Well, defense-GAN also learn a z in the latent space iteratively such that G(z) is close to the sample x. That's why I thought the methodology used in defense-GAN is somehow similar to what you did in the optimization-inference step. But defintely I see ABS is different from Defense-GAN. For BPDA, I know it is method to recover gradient, actually more precisely, estimate the non-zero and finite gradient that can be used for gradient-based algorithms. It seems like LatentDescentAttack did the similar thing. \n Thanks again, since I got many insights from the paper and your reply.\n\n\n\n\n", "title": "Thanks for your detailed reply"}, "signatures": ["~Tianhang_Zheng1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Tianhang_Zheng1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards the first adversarially robust neural network model on MNIST", "abstract": "Despite much effort, deep neural networks remain highly susceptible to tiny input perturbations and even for MNIST, one of the most common toy datasets in computer vision, no neural network model exists for which adversarial perturbations are large and make semantic sense to humans. We show that even the widely recognized and by far most successful L-inf defense by Madry et~al. (1) has lower L0 robustness than undefended networks and still highly susceptible to L2 perturbations, (2) classifies unrecognizable images with high certainty, (3) performs not much better than simple input binarization and (4) features adversarial perturbations that make little sense to humans. These results suggest that MNIST is far from being solved in terms of adversarial robustness. We present a novel robust classification model that performs analysis by synthesis using learned class-conditional data distributions. We derive bounds on the robustness and go to great length to empirically evaluate our model using maximally effective adversarial attacks by (a) applying decision-based, score-based, gradient-based and transfer-based attacks for several different Lp norms, (b) by designing a new attack that exploits the structure of our defended model and (c) by devising a novel decision-based attack that seeks to minimize the number of perturbed pixels (L0). The results suggest that our approach yields state-of-the-art robustness on MNIST against L0, L2 and L-inf perturbations and we demonstrate that most adversarial examples are strongly perturbed towards the perceptual boundary between the original and the adversarial class.", "keywords": ["adversarial examples", "MNIST", "robustness", "deep learning", "security"], "authorids": ["lukas.schott@bethgelab.org", "jonas.rauber@bethgelab.org", "matthias.bethge@bethgelab.org", "wieland.brendel@bethgelab.org"], "authors": ["Lukas Schott", "Jonas Rauber", "Matthias Bethge", "Wieland Brendel"], "pdf": "/pdf/ba645fe0aec0af7d9a401c8eb0c7462a25da3dba.pdf", "paperhash": "schott|towards_the_first_adversarially_robust_neural_network_model_on_mnist", "_bibtex": "@inproceedings{\nschott2018towards,\ntitle={Towards the first adversarially robust neural network model on {MNIST}},\nauthor={Lukas Schott and Jonas Rauber and Matthias Bethge and Wieland Brendel},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1EHOsC9tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper353/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311859878, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "S1EHOsC9tX", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper353/Authors", "ICLR.cc/2019/Conference/Paper353/Reviewers", "ICLR.cc/2019/Conference/Paper353/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper353/Authors", "ICLR.cc/2019/Conference/Paper353/Reviewers", "ICLR.cc/2019/Conference/Paper353/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311859878}}}, {"id": "r1eCatzaRm", "original": null, "number": 10, "cdate": 1543477702013, "ddate": null, "tcdate": 1543477702013, "tmdate": 1543477702013, "tddate": null, "forum": "S1EHOsC9tX", "replyto": "ByeIeWEi0m", "invitation": "ICLR.cc/2019/Conference/-/Paper353/Official_Comment", "content": {"title": "DenfenseGAN, L2 robustness intuition, verified lower bound, BPDA", "comment": "\"1.  As far as I can see, the defense method is quite like a composition of Defense-GAN and binarization method.\" \n\nOur method is very different from Defense GAN which is basically a sophisticated image denoising followed by a feedforward classifier. In contrast, we use class conditional generative models and no (vulnerable) feedforward classifier at all. \n\n\n\"As claimed in \"ensemble adversarial training\" (appendix), binarization can help MNIST model robust to Linfinity perturbation. But it is still not very intuitive to me why the model can be robust to L2 perturbation. \n\nOur intuition behind the ABS models L2 robustness is due to the Gaussian posterior (in pixelspace) in the reconstruction term, which ensures that small changes in the input can only entail small changes to the posterior likelihood and thus to the model decision. In other words, small changes in the input can only lead to small changes in the reconstruction error and so the logits (= reconstruction error + KL divergence) can only change slowly with varying inputs.\n\n\n\"(The bound given in eq. 8 seems close to 0, does it really make much sense?)\"\n\nOur verified lower bound of the mean L_2 robustness is 0.690 \u00b1 0.005 which is quite high compared to other SOTA methods which provide guarantees for the lower bound (i.e. Hein et al. [1] who have 0.48). \n\n[1] Matthias Hein and Maksym Andriushchenko. \"Formal guarantees on the robustness of a classifier against adversarial manipulation\". In Advances in Neural Information Processing Systems 30, pp. 2266\u20132276. Curran Associates, Inc., 2017.\n\n\n\"2. Another question is that Defense-GAN can be further attacked by BPDA proposed in https://arxiv.org/pdf/1802.00420.pdf. I was wondering did the proposed method suffer from the same problem (i.e., obfuscated gradients)?\"\n\nBPDA is not really an attack but a way to recover proper gradients for certain models (e.g. by pass-through estimators [2]). We do this in two ways: first by computing descent directions in the low-dimensional latent space (LatentDescentAttack) and second by estimating gradients using a finite-difference estimate (+ a \"pass through estimator\" [2] for the binary ABS model). The LatentDescentAttack is closest in spirit to BPDA (but adapted to our model).\n\n[2] Bengio, Y., Leonard, N., and Courville, A. \"Estimating or propagating gradients through stochastic neurons for conditional computation\". arXiv preprint arXiv:1308.3432, 2013."}, "signatures": ["ICLR.cc/2019/Conference/Paper353/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper353/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper353/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards the first adversarially robust neural network model on MNIST", "abstract": "Despite much effort, deep neural networks remain highly susceptible to tiny input perturbations and even for MNIST, one of the most common toy datasets in computer vision, no neural network model exists for which adversarial perturbations are large and make semantic sense to humans. We show that even the widely recognized and by far most successful L-inf defense by Madry et~al. (1) has lower L0 robustness than undefended networks and still highly susceptible to L2 perturbations, (2) classifies unrecognizable images with high certainty, (3) performs not much better than simple input binarization and (4) features adversarial perturbations that make little sense to humans. These results suggest that MNIST is far from being solved in terms of adversarial robustness. We present a novel robust classification model that performs analysis by synthesis using learned class-conditional data distributions. We derive bounds on the robustness and go to great length to empirically evaluate our model using maximally effective adversarial attacks by (a) applying decision-based, score-based, gradient-based and transfer-based attacks for several different Lp norms, (b) by designing a new attack that exploits the structure of our defended model and (c) by devising a novel decision-based attack that seeks to minimize the number of perturbed pixels (L0). The results suggest that our approach yields state-of-the-art robustness on MNIST against L0, L2 and L-inf perturbations and we demonstrate that most adversarial examples are strongly perturbed towards the perceptual boundary between the original and the adversarial class.", "keywords": ["adversarial examples", "MNIST", "robustness", "deep learning", "security"], "authorids": ["lukas.schott@bethgelab.org", "jonas.rauber@bethgelab.org", "matthias.bethge@bethgelab.org", "wieland.brendel@bethgelab.org"], "authors": ["Lukas Schott", "Jonas Rauber", "Matthias Bethge", "Wieland Brendel"], "pdf": "/pdf/ba645fe0aec0af7d9a401c8eb0c7462a25da3dba.pdf", "paperhash": "schott|towards_the_first_adversarially_robust_neural_network_model_on_mnist", "_bibtex": "@inproceedings{\nschott2018towards,\ntitle={Towards the first adversarially robust neural network model on {MNIST}},\nauthor={Lukas Schott and Jonas Rauber and Matthias Bethge and Wieland Brendel},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1EHOsC9tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper353/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621604269, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1EHOsC9tX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper353/Authors", "ICLR.cc/2019/Conference/Paper353/Reviewers", "ICLR.cc/2019/Conference/Paper353/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper353/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper353/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper353/Authors|ICLR.cc/2019/Conference/Paper353/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper353/Reviewers", "ICLR.cc/2019/Conference/Paper353/Authors", "ICLR.cc/2019/Conference/Paper353/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621604269}}}, {"id": "ByeIeWEi0m", "original": null, "number": 7, "cdate": 1543352557625, "ddate": null, "tcdate": 1543352557625, "tmdate": 1543352557625, "tddate": null, "forum": "S1EHOsC9tX", "replyto": "S1EHOsC9tX", "invitation": "ICLR.cc/2019/Conference/-/Paper353/Public_Comment", "content": {"comment": "A very interesting method! Just two small questions:\n\n1.  As far as I can see, the defense method is quite like a composition of Defense-GAN and binarization method. As claimed in \"ensemble adversarial training\" (appendix), binarization can help MNIST model robust to Linfinity perturbation. But it is still not very intuitive to me why the model can be robust to L2 perturbation. (The bound given in eq. 8 seems close to 0, does it really make much sense?)\n\n2. Another question is that Defense-GAN can be further attacked by BPDA proposed in https://arxiv.org/pdf/1802.00420.pdf. I was wondering did the proposed method suffer from the same problem (i.e., obfuscated gradients)?\n\nAnyway, the paper is pleasant to read. I would appreciate it if the authors can answer my questions.", "title": "Interesting method"}, "signatures": ["~Tianhang_Zheng1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Tianhang_Zheng1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards the first adversarially robust neural network model on MNIST", "abstract": "Despite much effort, deep neural networks remain highly susceptible to tiny input perturbations and even for MNIST, one of the most common toy datasets in computer vision, no neural network model exists for which adversarial perturbations are large and make semantic sense to humans. We show that even the widely recognized and by far most successful L-inf defense by Madry et~al. (1) has lower L0 robustness than undefended networks and still highly susceptible to L2 perturbations, (2) classifies unrecognizable images with high certainty, (3) performs not much better than simple input binarization and (4) features adversarial perturbations that make little sense to humans. These results suggest that MNIST is far from being solved in terms of adversarial robustness. We present a novel robust classification model that performs analysis by synthesis using learned class-conditional data distributions. We derive bounds on the robustness and go to great length to empirically evaluate our model using maximally effective adversarial attacks by (a) applying decision-based, score-based, gradient-based and transfer-based attacks for several different Lp norms, (b) by designing a new attack that exploits the structure of our defended model and (c) by devising a novel decision-based attack that seeks to minimize the number of perturbed pixels (L0). The results suggest that our approach yields state-of-the-art robustness on MNIST against L0, L2 and L-inf perturbations and we demonstrate that most adversarial examples are strongly perturbed towards the perceptual boundary between the original and the adversarial class.", "keywords": ["adversarial examples", "MNIST", "robustness", "deep learning", "security"], "authorids": ["lukas.schott@bethgelab.org", "jonas.rauber@bethgelab.org", "matthias.bethge@bethgelab.org", "wieland.brendel@bethgelab.org"], "authors": ["Lukas Schott", "Jonas Rauber", "Matthias Bethge", "Wieland Brendel"], "pdf": "/pdf/ba645fe0aec0af7d9a401c8eb0c7462a25da3dba.pdf", "paperhash": "schott|towards_the_first_adversarially_robust_neural_network_model_on_mnist", "_bibtex": "@inproceedings{\nschott2018towards,\ntitle={Towards the first adversarially robust neural network model on {MNIST}},\nauthor={Lukas Schott and Jonas Rauber and Matthias Bethge and Wieland Brendel},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1EHOsC9tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper353/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311859878, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "S1EHOsC9tX", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper353/Authors", "ICLR.cc/2019/Conference/Paper353/Reviewers", "ICLR.cc/2019/Conference/Paper353/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper353/Authors", "ICLR.cc/2019/Conference/Paper353/Reviewers", "ICLR.cc/2019/Conference/Paper353/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311859878}}}, {"id": "SJxNatjURX", "original": null, "number": 8, "cdate": 1543055803703, "ddate": null, "tcdate": 1543055803703, "tmdate": 1543223878583, "tddate": null, "forum": "S1EHOsC9tX", "replyto": "B1laSlHch7", "invitation": "ICLR.cc/2019/Conference/-/Paper353/Official_Comment", "content": {"title": "Scaling to other datasets, skewed datasets", "comment": "\"The main concern with this work is that it is heavily tailored towards MNIST and the authors do mention this. Scaling this to other datasets does not seem easy. \"\n\"Using VAEs to model the conditional class distributions is a nice idea, but how does this scale for datasets with large number of classes like imagenet? This would result in having 1000s of VAEs.\"\n\nFirst experiments suggest that our robustness is not limited to MNIST. To show this, we trained the proposed ABS model and a vanilla CNN on two class CIFAR and achieve a robustness ~3x larger than a CNN. \n\nRobustness results on 2 class CIFAR:\nmodel                                   accuracy    |         L2 robustness\nCNN                                       97.1%         |              0.8             (estimated with BIM)\nABS                                        89.7%         |              2.5            (estimated with LatentDescent attack)\n\nTo tackle the reduced accuracy of ABS on CIFAR-10 and other datasets, we are currently working on extensions of our architecture and the training procedure. First experiments show that this can improve the accuracy substantially over baseline ABS and still comes with the same robustness to adversarial perturbations (but this is beyond the scope of this paper).\n\n\n\"It would be nice to see this model behaves for skewed datasets.\"\n\nIn contrast to purely discriminative models that require manual rebalancing of the training data, our generative architecture can cope well with unbalanced datasets out of the box. To demonstrate this experimentally, we have trained a two-class MNIST classifier (ones vs. sevens) both on a balanced dataset, an unbalanced datasets (10 times as many sevens than ones during training) and a highly unbalanced dataset (100 times as many ones as sevens during training). They all perform similarly well:\n    \n                                                  accuracy      | L_2 median perturbation size with Latent Descent attack\nbalanced ABS                       99.6 +- 0.1%   |        3.5  +- 0.1\n10  :1 unbalanced ABS        99.3 +- 0.2%   |        3.4  +- 0.2\n100:1 unbalanced ABS        98.5 +- 0.2%   |        3.2  +- 0.2\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper353/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper353/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper353/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards the first adversarially robust neural network model on MNIST", "abstract": "Despite much effort, deep neural networks remain highly susceptible to tiny input perturbations and even for MNIST, one of the most common toy datasets in computer vision, no neural network model exists for which adversarial perturbations are large and make semantic sense to humans. We show that even the widely recognized and by far most successful L-inf defense by Madry et~al. (1) has lower L0 robustness than undefended networks and still highly susceptible to L2 perturbations, (2) classifies unrecognizable images with high certainty, (3) performs not much better than simple input binarization and (4) features adversarial perturbations that make little sense to humans. These results suggest that MNIST is far from being solved in terms of adversarial robustness. We present a novel robust classification model that performs analysis by synthesis using learned class-conditional data distributions. We derive bounds on the robustness and go to great length to empirically evaluate our model using maximally effective adversarial attacks by (a) applying decision-based, score-based, gradient-based and transfer-based attacks for several different Lp norms, (b) by designing a new attack that exploits the structure of our defended model and (c) by devising a novel decision-based attack that seeks to minimize the number of perturbed pixels (L0). The results suggest that our approach yields state-of-the-art robustness on MNIST against L0, L2 and L-inf perturbations and we demonstrate that most adversarial examples are strongly perturbed towards the perceptual boundary between the original and the adversarial class.", "keywords": ["adversarial examples", "MNIST", "robustness", "deep learning", "security"], "authorids": ["lukas.schott@bethgelab.org", "jonas.rauber@bethgelab.org", "matthias.bethge@bethgelab.org", "wieland.brendel@bethgelab.org"], "authors": ["Lukas Schott", "Jonas Rauber", "Matthias Bethge", "Wieland Brendel"], "pdf": "/pdf/ba645fe0aec0af7d9a401c8eb0c7462a25da3dba.pdf", "paperhash": "schott|towards_the_first_adversarially_robust_neural_network_model_on_mnist", "_bibtex": "@inproceedings{\nschott2018towards,\ntitle={Towards the first adversarially robust neural network model on {MNIST}},\nauthor={Lukas Schott and Jonas Rauber and Matthias Bethge and Wieland Brendel},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1EHOsC9tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper353/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621604269, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1EHOsC9tX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper353/Authors", "ICLR.cc/2019/Conference/Paper353/Reviewers", "ICLR.cc/2019/Conference/Paper353/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper353/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper353/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper353/Authors|ICLR.cc/2019/Conference/Paper353/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper353/Reviewers", "ICLR.cc/2019/Conference/Paper353/Authors", "ICLR.cc/2019/Conference/Paper353/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621604269}}}, {"id": "SJegx_sU0X", "original": null, "number": 6, "cdate": 1543055336422, "ddate": null, "tcdate": 1543055336422, "tmdate": 1543223855392, "tddate": null, "forum": "S1EHOsC9tX", "replyto": "SklgCVqq2Q", "invitation": "ICLR.cc/2019/Conference/-/Paper353/Official_Comment", "content": {"title": "Scaling to other datasets, unbalanced data, intuition behind the model, robustness lower bound, adversarial training", "comment": "\"Although the paper is designed for MNIST specifically, the proposed scheme should apply to other classification tasks. Have you tried the models on other datasets like CIFAR10/100? It would be interesting to see whether the proposal would work for more complicated tasks.\"\n\nFirst experiments suggest that our robustness is not limited to MNIST. To show this, we trained the proposed ABS model and a vanilla CNN on two class CIFAR and achieve a robustness ~3x larger than a CNN. \n\nRobustness results on 2 class CIFAR:\nmodel                                   accuracy     |         L2 robustness\nCNN                                       97.1%         |              0.8             (estimated with BIM)\nABS                                        89.7%          |              2.5            (estimated with LatentDescent attack)\n\nTo tackle the reduced accuracy of ABS on CIFAR-10 and other datasets, we are currently working on extensions of our architecture and the training procedure. First experiments show that this can improve the accuracy substantially over baseline ABS and still comes with the same robustness to adversarial perturbations (but this is beyond the scope of this paper).\n\n\n\"When the training data for each label is unbalanced, namely, some class has very few samples, would you expect the model to fail?\"\n\nIn contrast to purely discriminative models that require manual rebalancing of the training data, our generative architecture can cope well with unbalanced datasets out of the box. To demonstrate this experimentally, we have trained a two-class MNIST classifier (ones vs. sevens) both on a balanced dataset, an unbalanced datasets (10 times as many sevens than ones during training) and a highly unbalanced dataset (100 times as many ones as sevens during training). They all perform similarly well:\n    \n                                                  accuracy      | L_2 median perturbation size with Latent Descent attack\nbalanced ABS                      99.6 +- 0.1%    |        3.5  +- 0.1\n10  :1 unbalanced ABS       99.3 +- 0.2%    |        3.4  +- 0.2\n100:1 unbalanced ABS       98.5 +- 0.2%    |        3.2  +- 0.2\n\n\n\"It would be more interesting to add more intuition on why the proposed model is already robust by design.\"\n\nAdversarial training is used to prevent small changes in the input to make large changes in the model decision. In the ABS model, the Gaussian posterior in the reconstruction term ensures that small changes in the input can only entail small changes to the posterior likelihood and thus to the model decision. In other words, small changes in the input can only lead to small changes in the reconstruction error and so the logits (= reconstruction error + KL divergence) can only change slowly with varying inputs.\n\n\n\"Equation (8) is complicated and still model-dependent. Without further relaxation and simplification, it\u2019s not easy to see if this value is small or large, or to understand what kind of message this section is trying to pass.\"\n\nWe provide quantitative values in the results section \"Lower bounds on Robustness\" (we'll add a pointer). For ABS, the mean L2 perturbation (i.e. the mean of epsilon in eq. 8 across samples) is 0.69. For comparison, Hein et al. [1] reaches 0.48.\n\n[1] Matthias Hein and Maksym Andriushchenko. Formal guarantees on the robustness of a classifier against adversarial manipulation. In Advances in Neural Information Processing Systems 30, pp. 2266\u20132276. Curran Associates, Inc., 2017.\n\n\n\"Although the main contribution of the paper is to propose a model that is robust without further defending, the proposed model could still benefit from adversarial training. Have you tried to retrain your model using the adversarial examples you have got and see if it helps?\"\n\nIt's an interesting question as to whether a combination of analysis by synthesis and adversarial training can yield even better results. One potential problem could be that adversarial training makes little sense if adversarials are already at the perceptual boundary between two classes. This would need to be evaluated carefully and we feel that such an analysis goes beyond the scope of this paper. We will, however, release the code and the pretrained model for the community to play around with such ideas. Thanks for the suggestion!\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper353/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper353/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper353/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards the first adversarially robust neural network model on MNIST", "abstract": "Despite much effort, deep neural networks remain highly susceptible to tiny input perturbations and even for MNIST, one of the most common toy datasets in computer vision, no neural network model exists for which adversarial perturbations are large and make semantic sense to humans. We show that even the widely recognized and by far most successful L-inf defense by Madry et~al. (1) has lower L0 robustness than undefended networks and still highly susceptible to L2 perturbations, (2) classifies unrecognizable images with high certainty, (3) performs not much better than simple input binarization and (4) features adversarial perturbations that make little sense to humans. These results suggest that MNIST is far from being solved in terms of adversarial robustness. We present a novel robust classification model that performs analysis by synthesis using learned class-conditional data distributions. We derive bounds on the robustness and go to great length to empirically evaluate our model using maximally effective adversarial attacks by (a) applying decision-based, score-based, gradient-based and transfer-based attacks for several different Lp norms, (b) by designing a new attack that exploits the structure of our defended model and (c) by devising a novel decision-based attack that seeks to minimize the number of perturbed pixels (L0). The results suggest that our approach yields state-of-the-art robustness on MNIST against L0, L2 and L-inf perturbations and we demonstrate that most adversarial examples are strongly perturbed towards the perceptual boundary between the original and the adversarial class.", "keywords": ["adversarial examples", "MNIST", "robustness", "deep learning", "security"], "authorids": ["lukas.schott@bethgelab.org", "jonas.rauber@bethgelab.org", "matthias.bethge@bethgelab.org", "wieland.brendel@bethgelab.org"], "authors": ["Lukas Schott", "Jonas Rauber", "Matthias Bethge", "Wieland Brendel"], "pdf": "/pdf/ba645fe0aec0af7d9a401c8eb0c7462a25da3dba.pdf", "paperhash": "schott|towards_the_first_adversarially_robust_neural_network_model_on_mnist", "_bibtex": "@inproceedings{\nschott2018towards,\ntitle={Towards the first adversarially robust neural network model on {MNIST}},\nauthor={Lukas Schott and Jonas Rauber and Matthias Bethge and Wieland Brendel},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1EHOsC9tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper353/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621604269, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1EHOsC9tX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper353/Authors", "ICLR.cc/2019/Conference/Paper353/Reviewers", "ICLR.cc/2019/Conference/Paper353/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper353/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper353/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper353/Authors|ICLR.cc/2019/Conference/Paper353/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper353/Reviewers", "ICLR.cc/2019/Conference/Paper353/Authors", "ICLR.cc/2019/Conference/Paper353/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621604269}}}, {"id": "H1xX7jsIAX", "original": null, "number": 9, "cdate": 1543056155198, "ddate": null, "tcdate": 1543056155198, "tmdate": 1543056155198, "tddate": null, "forum": "S1EHOsC9tX", "replyto": "S1EHOsC9tX", "invitation": "ICLR.cc/2019/Conference/-/Paper353/Official_Comment", "content": {"title": "Summary of the updates to the manuscript ", "comment": "We would like to thank all reviewers for their valuable feedback. Regarding concerns we responded to each reviewer individually\n\nWe have uploaded an updated version of the paper with the following changes: \n\n1.) We provide additional intuitions behind the model architecture and its robustness \n\n2.) We have extended the section describing ideas to scale this approach to more complex datasets\n\n3) We provide preliminary results for two class CIFAR. \n\n4) Minor changes\n* fixed the correct image for distal adversarials for the ABS model \n* We changed p(x) to p(x|y) to be consistent\n* We added a pointer to to the results in section 4 \"TIGHT ESTIMATES  OF THE LOWER BOUND  FOR ADVERSARIAL EXAMPLES\"\n* We consistently refer to the sigma of the variational inference as \\sigma_q \n"}, "signatures": ["ICLR.cc/2019/Conference/Paper353/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper353/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper353/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards the first adversarially robust neural network model on MNIST", "abstract": "Despite much effort, deep neural networks remain highly susceptible to tiny input perturbations and even for MNIST, one of the most common toy datasets in computer vision, no neural network model exists for which adversarial perturbations are large and make semantic sense to humans. We show that even the widely recognized and by far most successful L-inf defense by Madry et~al. (1) has lower L0 robustness than undefended networks and still highly susceptible to L2 perturbations, (2) classifies unrecognizable images with high certainty, (3) performs not much better than simple input binarization and (4) features adversarial perturbations that make little sense to humans. These results suggest that MNIST is far from being solved in terms of adversarial robustness. We present a novel robust classification model that performs analysis by synthesis using learned class-conditional data distributions. We derive bounds on the robustness and go to great length to empirically evaluate our model using maximally effective adversarial attacks by (a) applying decision-based, score-based, gradient-based and transfer-based attacks for several different Lp norms, (b) by designing a new attack that exploits the structure of our defended model and (c) by devising a novel decision-based attack that seeks to minimize the number of perturbed pixels (L0). The results suggest that our approach yields state-of-the-art robustness on MNIST against L0, L2 and L-inf perturbations and we demonstrate that most adversarial examples are strongly perturbed towards the perceptual boundary between the original and the adversarial class.", "keywords": ["adversarial examples", "MNIST", "robustness", "deep learning", "security"], "authorids": ["lukas.schott@bethgelab.org", "jonas.rauber@bethgelab.org", "matthias.bethge@bethgelab.org", "wieland.brendel@bethgelab.org"], "authors": ["Lukas Schott", "Jonas Rauber", "Matthias Bethge", "Wieland Brendel"], "pdf": "/pdf/ba645fe0aec0af7d9a401c8eb0c7462a25da3dba.pdf", "paperhash": "schott|towards_the_first_adversarially_robust_neural_network_model_on_mnist", "_bibtex": "@inproceedings{\nschott2018towards,\ntitle={Towards the first adversarially robust neural network model on {MNIST}},\nauthor={Lukas Schott and Jonas Rauber and Matthias Bethge and Wieland Brendel},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1EHOsC9tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper353/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621604269, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1EHOsC9tX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper353/Authors", "ICLR.cc/2019/Conference/Paper353/Reviewers", "ICLR.cc/2019/Conference/Paper353/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper353/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper353/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper353/Authors|ICLR.cc/2019/Conference/Paper353/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper353/Reviewers", "ICLR.cc/2019/Conference/Paper353/Authors", "ICLR.cc/2019/Conference/Paper353/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621604269}}}, {"id": "r1lfWKsIA7", "original": null, "number": 7, "cdate": 1543055609706, "ddate": null, "tcdate": 1543055609706, "tmdate": 1543055609706, "tddate": null, "forum": "S1EHOsC9tX", "replyto": "rylr8jLc3X", "invitation": "ICLR.cc/2019/Conference/-/Paper353/Official_Comment", "content": {"title": "class conditional probabilities, strictness of ELBO", "comment": "\"it was not very clear to me that the authors were estimating the p(x) for each y. The transition from p(x|y) to p(x) at the end of page 3 was astute and confused me. The authors should make it more clear.\"\n\nWe agree, thank you for pointing this out. We changed  p(x) -> p(x|y) in Equation (2) and the text. \n\n\n\"it would be beneficial if the authors could comment on the how strict/loose the lower bound of (2) is, as it is critical in estimating the class specific density.\"\n\nFor a standard VAE trained on MNIST, the estimate of log p(x) is around -93 while true log-likeilhood is at around -87 (see https://openreview.net/pdf?id=HyZoi-WRb, Figure 3). Hence, the bound is neither extremely loose, nor extremely tight. In any case, one should keep in mind that the goal of the model is not optimal density estimation but accuracy and model robustness, so we can accept to be non-optimal. You may be right, however, that tighter bounds might also increase accuracy and robustness, which is an exciting question to be answered in future work."}, "signatures": ["ICLR.cc/2019/Conference/Paper353/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper353/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper353/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards the first adversarially robust neural network model on MNIST", "abstract": "Despite much effort, deep neural networks remain highly susceptible to tiny input perturbations and even for MNIST, one of the most common toy datasets in computer vision, no neural network model exists for which adversarial perturbations are large and make semantic sense to humans. We show that even the widely recognized and by far most successful L-inf defense by Madry et~al. (1) has lower L0 robustness than undefended networks and still highly susceptible to L2 perturbations, (2) classifies unrecognizable images with high certainty, (3) performs not much better than simple input binarization and (4) features adversarial perturbations that make little sense to humans. These results suggest that MNIST is far from being solved in terms of adversarial robustness. We present a novel robust classification model that performs analysis by synthesis using learned class-conditional data distributions. We derive bounds on the robustness and go to great length to empirically evaluate our model using maximally effective adversarial attacks by (a) applying decision-based, score-based, gradient-based and transfer-based attacks for several different Lp norms, (b) by designing a new attack that exploits the structure of our defended model and (c) by devising a novel decision-based attack that seeks to minimize the number of perturbed pixels (L0). The results suggest that our approach yields state-of-the-art robustness on MNIST against L0, L2 and L-inf perturbations and we demonstrate that most adversarial examples are strongly perturbed towards the perceptual boundary between the original and the adversarial class.", "keywords": ["adversarial examples", "MNIST", "robustness", "deep learning", "security"], "authorids": ["lukas.schott@bethgelab.org", "jonas.rauber@bethgelab.org", "matthias.bethge@bethgelab.org", "wieland.brendel@bethgelab.org"], "authors": ["Lukas Schott", "Jonas Rauber", "Matthias Bethge", "Wieland Brendel"], "pdf": "/pdf/ba645fe0aec0af7d9a401c8eb0c7462a25da3dba.pdf", "paperhash": "schott|towards_the_first_adversarially_robust_neural_network_model_on_mnist", "_bibtex": "@inproceedings{\nschott2018towards,\ntitle={Towards the first adversarially robust neural network model on {MNIST}},\nauthor={Lukas Schott and Jonas Rauber and Matthias Bethge and Wieland Brendel},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1EHOsC9tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper353/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621604269, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1EHOsC9tX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper353/Authors", "ICLR.cc/2019/Conference/Paper353/Reviewers", "ICLR.cc/2019/Conference/Paper353/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper353/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper353/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper353/Authors|ICLR.cc/2019/Conference/Paper353/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper353/Reviewers", "ICLR.cc/2019/Conference/Paper353/Authors", "ICLR.cc/2019/Conference/Paper353/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621604269}}}, {"id": "SyeyTVeH0Q", "original": null, "number": 6, "cdate": 1542943926551, "ddate": null, "tcdate": 1542943926551, "tmdate": 1542943926551, "tddate": null, "forum": "S1EHOsC9tX", "replyto": "H1lxliTMCX", "invitation": "ICLR.cc/2019/Conference/-/Paper353/Public_Comment", "content": {"comment": "Thanks!\n\nThe method goes to great computational expense to use Eq. 3 instead of Eq. 2. (8000 evaluations per sample) It would be interesting to see if it's worth it.\n\nAlso, what if Madry's defense were trained to defend against L2=1.5 attacks? (This seems like a trivial generalization, but I'm not aware of anyone having done this) It would be interesting to see where such a defense would fit in Table 1.", "title": "suggestions"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards the first adversarially robust neural network model on MNIST", "abstract": "Despite much effort, deep neural networks remain highly susceptible to tiny input perturbations and even for MNIST, one of the most common toy datasets in computer vision, no neural network model exists for which adversarial perturbations are large and make semantic sense to humans. We show that even the widely recognized and by far most successful L-inf defense by Madry et~al. (1) has lower L0 robustness than undefended networks and still highly susceptible to L2 perturbations, (2) classifies unrecognizable images with high certainty, (3) performs not much better than simple input binarization and (4) features adversarial perturbations that make little sense to humans. These results suggest that MNIST is far from being solved in terms of adversarial robustness. We present a novel robust classification model that performs analysis by synthesis using learned class-conditional data distributions. We derive bounds on the robustness and go to great length to empirically evaluate our model using maximally effective adversarial attacks by (a) applying decision-based, score-based, gradient-based and transfer-based attacks for several different Lp norms, (b) by designing a new attack that exploits the structure of our defended model and (c) by devising a novel decision-based attack that seeks to minimize the number of perturbed pixels (L0). The results suggest that our approach yields state-of-the-art robustness on MNIST against L0, L2 and L-inf perturbations and we demonstrate that most adversarial examples are strongly perturbed towards the perceptual boundary between the original and the adversarial class.", "keywords": ["adversarial examples", "MNIST", "robustness", "deep learning", "security"], "authorids": ["lukas.schott@bethgelab.org", "jonas.rauber@bethgelab.org", "matthias.bethge@bethgelab.org", "wieland.brendel@bethgelab.org"], "authors": ["Lukas Schott", "Jonas Rauber", "Matthias Bethge", "Wieland Brendel"], "pdf": "/pdf/ba645fe0aec0af7d9a401c8eb0c7462a25da3dba.pdf", "paperhash": "schott|towards_the_first_adversarially_robust_neural_network_model_on_mnist", "_bibtex": "@inproceedings{\nschott2018towards,\ntitle={Towards the first adversarially robust neural network model on {MNIST}},\nauthor={Lukas Schott and Jonas Rauber and Matthias Bethge and Wieland Brendel},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1EHOsC9tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper353/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311859878, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "S1EHOsC9tX", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper353/Authors", "ICLR.cc/2019/Conference/Paper353/Reviewers", "ICLR.cc/2019/Conference/Paper353/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper353/Authors", "ICLR.cc/2019/Conference/Paper353/Reviewers", "ICLR.cc/2019/Conference/Paper353/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311859878}}}, {"id": "H1lxliTMCX", "original": null, "number": 5, "cdate": 1542802151934, "ddate": null, "tcdate": 1542802151934, "tmdate": 1542802151934, "tddate": null, "forum": "S1EHOsC9tX", "replyto": "BygGXkaqT7", "invitation": "ICLR.cc/2019/Conference/-/Paper353/Official_Comment", "content": {"title": "sigma in Eq. 3, derivation of estimation of bound for adversarial examples ", "comment": "- How is \\sigma chosen in Eq.3? Is it different from \\sigma_q in Eq.7?\n\nThe \\sigma in Eq. 3 should be called \\sigma_q as well, thanks for pointing this out. We set \\sigma_q = 1 (the exact value doesn\u2019t really matter at this point since we do not sample from the posterior distribution during the optimization step). We\u2019ll add this to the \u201cModel and Training Details\u201d section in the appendix. \n\n\n- Why does it make sense to equate (7) and (6), upper and lower bounds? (I'm sure the authors thought it through, but it seems unclear from the text)\n\nRemember that an (untargeted) adversarial perturbation tries to maximally lower the likelihood of the true label and to maximally increase the likelihood of some other label. We here derive how much the likelihood of the true label can maximally decrease for a given norm-ball of epsilon (that\u2019s the lower bound), and what the maximum likelihood of any other class may be under the same constraint (that\u2019s the upper bound). The epsilon for which the lower and upper bound are the same is the maximum epsilon for which we can guarantee that the model will still predict the true label.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper353/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper353/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper353/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards the first adversarially robust neural network model on MNIST", "abstract": "Despite much effort, deep neural networks remain highly susceptible to tiny input perturbations and even for MNIST, one of the most common toy datasets in computer vision, no neural network model exists for which adversarial perturbations are large and make semantic sense to humans. We show that even the widely recognized and by far most successful L-inf defense by Madry et~al. (1) has lower L0 robustness than undefended networks and still highly susceptible to L2 perturbations, (2) classifies unrecognizable images with high certainty, (3) performs not much better than simple input binarization and (4) features adversarial perturbations that make little sense to humans. These results suggest that MNIST is far from being solved in terms of adversarial robustness. We present a novel robust classification model that performs analysis by synthesis using learned class-conditional data distributions. We derive bounds on the robustness and go to great length to empirically evaluate our model using maximally effective adversarial attacks by (a) applying decision-based, score-based, gradient-based and transfer-based attacks for several different Lp norms, (b) by designing a new attack that exploits the structure of our defended model and (c) by devising a novel decision-based attack that seeks to minimize the number of perturbed pixels (L0). The results suggest that our approach yields state-of-the-art robustness on MNIST against L0, L2 and L-inf perturbations and we demonstrate that most adversarial examples are strongly perturbed towards the perceptual boundary between the original and the adversarial class.", "keywords": ["adversarial examples", "MNIST", "robustness", "deep learning", "security"], "authorids": ["lukas.schott@bethgelab.org", "jonas.rauber@bethgelab.org", "matthias.bethge@bethgelab.org", "wieland.brendel@bethgelab.org"], "authors": ["Lukas Schott", "Jonas Rauber", "Matthias Bethge", "Wieland Brendel"], "pdf": "/pdf/ba645fe0aec0af7d9a401c8eb0c7462a25da3dba.pdf", "paperhash": "schott|towards_the_first_adversarially_robust_neural_network_model_on_mnist", "_bibtex": "@inproceedings{\nschott2018towards,\ntitle={Towards the first adversarially robust neural network model on {MNIST}},\nauthor={Lukas Schott and Jonas Rauber and Matthias Bethge and Wieland Brendel},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1EHOsC9tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper353/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621604269, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1EHOsC9tX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper353/Authors", "ICLR.cc/2019/Conference/Paper353/Reviewers", "ICLR.cc/2019/Conference/Paper353/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper353/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper353/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper353/Authors|ICLR.cc/2019/Conference/Paper353/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper353/Reviewers", "ICLR.cc/2019/Conference/Paper353/Authors", "ICLR.cc/2019/Conference/Paper353/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621604269}}}, {"id": "BygGXkaqT7", "original": null, "number": 5, "cdate": 1542274841956, "ddate": null, "tcdate": 1542274841956, "tmdate": 1542274875845, "tddate": null, "forum": "S1EHOsC9tX", "replyto": "S1EHOsC9tX", "invitation": "ICLR.cc/2019/Conference/-/Paper353/Public_Comment", "content": {"comment": "\n\n- How is \\sigma chosen in Eq.3? Is it different from \\sigma_q in Eq.7?\n\n- Why does it make sense to equate (7) and (6), upper and lower bounds? (I'm sure the authors thought it through, but it seems unclear from the text)", "title": "some questions"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards the first adversarially robust neural network model on MNIST", "abstract": "Despite much effort, deep neural networks remain highly susceptible to tiny input perturbations and even for MNIST, one of the most common toy datasets in computer vision, no neural network model exists for which adversarial perturbations are large and make semantic sense to humans. We show that even the widely recognized and by far most successful L-inf defense by Madry et~al. (1) has lower L0 robustness than undefended networks and still highly susceptible to L2 perturbations, (2) classifies unrecognizable images with high certainty, (3) performs not much better than simple input binarization and (4) features adversarial perturbations that make little sense to humans. These results suggest that MNIST is far from being solved in terms of adversarial robustness. We present a novel robust classification model that performs analysis by synthesis using learned class-conditional data distributions. We derive bounds on the robustness and go to great length to empirically evaluate our model using maximally effective adversarial attacks by (a) applying decision-based, score-based, gradient-based and transfer-based attacks for several different Lp norms, (b) by designing a new attack that exploits the structure of our defended model and (c) by devising a novel decision-based attack that seeks to minimize the number of perturbed pixels (L0). The results suggest that our approach yields state-of-the-art robustness on MNIST against L0, L2 and L-inf perturbations and we demonstrate that most adversarial examples are strongly perturbed towards the perceptual boundary between the original and the adversarial class.", "keywords": ["adversarial examples", "MNIST", "robustness", "deep learning", "security"], "authorids": ["lukas.schott@bethgelab.org", "jonas.rauber@bethgelab.org", "matthias.bethge@bethgelab.org", "wieland.brendel@bethgelab.org"], "authors": ["Lukas Schott", "Jonas Rauber", "Matthias Bethge", "Wieland Brendel"], "pdf": "/pdf/ba645fe0aec0af7d9a401c8eb0c7462a25da3dba.pdf", "paperhash": "schott|towards_the_first_adversarially_robust_neural_network_model_on_mnist", "_bibtex": "@inproceedings{\nschott2018towards,\ntitle={Towards the first adversarially robust neural network model on {MNIST}},\nauthor={Lukas Schott and Jonas Rauber and Matthias Bethge and Wieland Brendel},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1EHOsC9tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper353/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311859878, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "S1EHOsC9tX", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper353/Authors", "ICLR.cc/2019/Conference/Paper353/Reviewers", "ICLR.cc/2019/Conference/Paper353/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper353/Authors", "ICLR.cc/2019/Conference/Paper353/Reviewers", "ICLR.cc/2019/Conference/Paper353/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311859878}}}, {"id": "SklgCVqq2Q", "original": null, "number": 3, "cdate": 1541215431678, "ddate": null, "tcdate": 1541215431678, "tmdate": 1541534065706, "tddate": null, "forum": "S1EHOsC9tX", "replyto": "S1EHOsC9tX", "invitation": "ICLR.cc/2019/Conference/-/Paper353/Official_Review", "content": {"title": "a nice paper with space of improvement", "review": "This paper shows that the problem of defending MNIST is still unsuccessful. It hereby proposes a model that is robust by design specifically for the MNIST classification task. Unlike conventional classifiers, the proposal learns a class-dependent data distribution using VAEs, and conducts variational inference by optimizing over the latent space to estimate the classification logits. \n\nSome extensive experiments verify the model robustness with respect to different distance measure, with most state-of-the-art attacking schemes, and compared against several baselines. The added experiments with rotation and translation further consolidate the value of the work. \n\nOverall I think this is a nice paper. Although being lack of some good intuition, the proposed model indeed show superior robustness to previous defending approaches. Also, the model has some other benefits that are shown in Figure 3 and 4. The results show that the model has indeed learned the data distribution rather than roughly determining the decision boundary of the input space as most existing models do.\n\n\nHowever, I have the following comments that might help to improve the paper:\n\n1. It would be more interesting to add more intuition on why the proposed model is already robust by design. \n\n2. Although the paper is designed for MNIST specifically, the proposed scheme should apply to other classification tasks. Have you tried the models on other datasets like CIFAR10/100? It would be interesting to see whether the proposal would work for more complicated tasks. When the training data for each label is unbalanced, namely, some class has very few samples, would you expect the model to fail?\n\n3. Equation (8) is complicated and still model-dependent. Without further relaxation and simplification, it\u2019s not easy to see if this value is small or large, or to understand what kind of message this section is trying to pass. \n\n4. Although the main contribution of the paper is to propose a model that is robust without further defending, the proposed model could still benefit from adversarial training. Have you tried to retrain your model using the adversarial examples you have got and see if it helps?\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper353/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards the first adversarially robust neural network model on MNIST", "abstract": "Despite much effort, deep neural networks remain highly susceptible to tiny input perturbations and even for MNIST, one of the most common toy datasets in computer vision, no neural network model exists for which adversarial perturbations are large and make semantic sense to humans. We show that even the widely recognized and by far most successful L-inf defense by Madry et~al. (1) has lower L0 robustness than undefended networks and still highly susceptible to L2 perturbations, (2) classifies unrecognizable images with high certainty, (3) performs not much better than simple input binarization and (4) features adversarial perturbations that make little sense to humans. These results suggest that MNIST is far from being solved in terms of adversarial robustness. We present a novel robust classification model that performs analysis by synthesis using learned class-conditional data distributions. We derive bounds on the robustness and go to great length to empirically evaluate our model using maximally effective adversarial attacks by (a) applying decision-based, score-based, gradient-based and transfer-based attacks for several different Lp norms, (b) by designing a new attack that exploits the structure of our defended model and (c) by devising a novel decision-based attack that seeks to minimize the number of perturbed pixels (L0). The results suggest that our approach yields state-of-the-art robustness on MNIST against L0, L2 and L-inf perturbations and we demonstrate that most adversarial examples are strongly perturbed towards the perceptual boundary between the original and the adversarial class.", "keywords": ["adversarial examples", "MNIST", "robustness", "deep learning", "security"], "authorids": ["lukas.schott@bethgelab.org", "jonas.rauber@bethgelab.org", "matthias.bethge@bethgelab.org", "wieland.brendel@bethgelab.org"], "authors": ["Lukas Schott", "Jonas Rauber", "Matthias Bethge", "Wieland Brendel"], "pdf": "/pdf/ba645fe0aec0af7d9a401c8eb0c7462a25da3dba.pdf", "paperhash": "schott|towards_the_first_adversarially_robust_neural_network_model_on_mnist", "_bibtex": "@inproceedings{\nschott2018towards,\ntitle={Towards the first adversarially robust neural network model on {MNIST}},\nauthor={Lukas Schott and Jonas Rauber and Matthias Bethge and Wieland Brendel},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1EHOsC9tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper353/Official_Review", "cdate": 1542234480566, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "S1EHOsC9tX", "replyto": "S1EHOsC9tX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper353/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335705374, "tmdate": 1552335705374, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper353/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "rylr8jLc3X", "original": null, "number": 2, "cdate": 1541200716681, "ddate": null, "tcdate": 1541200716681, "tmdate": 1541534065477, "tddate": null, "forum": "S1EHOsC9tX", "replyto": "S1EHOsC9tX", "invitation": "ICLR.cc/2019/Conference/-/Paper353/Official_Review", "content": {"title": "Nice paper on adversarially robust models", "review": "In this paper, the authors argued that the current approaches are not robust to adversarial attacks, even for MNIST. They proposed a generative approach for classification, which uses variational autoencoder (VAE) to estimate the class specific feature distribution. Robustness guarantees are derived for their model. Through numeric studies, they demonstrated the performance of their proposal (ABS). They also demonstrated that many of the adversarial examples for their ABS model are actually meaningful to humans, which are different from existing approaches, such as SOTA.\n\nOverall this is a well written paper. The presentation of their methodology is clear, so are the numerical studies.\n\nSome comments:\n1) it was not very clear to me that the authors were estimating the p(x) for each y. The transition from p(x|y) to p(x) at the end of page 3 was astute and confused me. The authors should make it more clear.\n2) it would be beneficial if the authors could comment on the how strict/loose the lower bound of (2) is, as it is critical in estimating the class specific density.", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper353/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards the first adversarially robust neural network model on MNIST", "abstract": "Despite much effort, deep neural networks remain highly susceptible to tiny input perturbations and even for MNIST, one of the most common toy datasets in computer vision, no neural network model exists for which adversarial perturbations are large and make semantic sense to humans. We show that even the widely recognized and by far most successful L-inf defense by Madry et~al. (1) has lower L0 robustness than undefended networks and still highly susceptible to L2 perturbations, (2) classifies unrecognizable images with high certainty, (3) performs not much better than simple input binarization and (4) features adversarial perturbations that make little sense to humans. These results suggest that MNIST is far from being solved in terms of adversarial robustness. We present a novel robust classification model that performs analysis by synthesis using learned class-conditional data distributions. We derive bounds on the robustness and go to great length to empirically evaluate our model using maximally effective adversarial attacks by (a) applying decision-based, score-based, gradient-based and transfer-based attacks for several different Lp norms, (b) by designing a new attack that exploits the structure of our defended model and (c) by devising a novel decision-based attack that seeks to minimize the number of perturbed pixels (L0). The results suggest that our approach yields state-of-the-art robustness on MNIST against L0, L2 and L-inf perturbations and we demonstrate that most adversarial examples are strongly perturbed towards the perceptual boundary between the original and the adversarial class.", "keywords": ["adversarial examples", "MNIST", "robustness", "deep learning", "security"], "authorids": ["lukas.schott@bethgelab.org", "jonas.rauber@bethgelab.org", "matthias.bethge@bethgelab.org", "wieland.brendel@bethgelab.org"], "authors": ["Lukas Schott", "Jonas Rauber", "Matthias Bethge", "Wieland Brendel"], "pdf": "/pdf/ba645fe0aec0af7d9a401c8eb0c7462a25da3dba.pdf", "paperhash": "schott|towards_the_first_adversarially_robust_neural_network_model_on_mnist", "_bibtex": "@inproceedings{\nschott2018towards,\ntitle={Towards the first adversarially robust neural network model on {MNIST}},\nauthor={Lukas Schott and Jonas Rauber and Matthias Bethge and Wieland Brendel},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1EHOsC9tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper353/Official_Review", "cdate": 1542234480566, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "S1EHOsC9tX", "replyto": "S1EHOsC9tX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper353/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335705374, "tmdate": 1552335705374, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper353/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "B1laSlHch7", "original": null, "number": 1, "cdate": 1541193796848, "ddate": null, "tcdate": 1541193796848, "tmdate": 1541534065269, "tddate": null, "forum": "S1EHOsC9tX", "replyto": "S1EHOsC9tX", "invitation": "ICLR.cc/2019/Conference/-/Paper353/Official_Review", "content": {"title": "Review", "review": "Paper summary: The paper presents a robust Analysis by Synthesis classification model that uses the input distribution within each class to achieve high accuracy and robustness against adversarial perturbations. The architecture involves training VAEs for each class to learn p(x|y) and performing exact inference during evaluation. The authors show that ABS and binary ABS outperform other models in terms of robustness for L2, Linf and L0 attacks respectively. \n\nThe paper in general is well written and clear, and the approach of using generative methods such as VAE for better robustness is good. \n\nPros: \nUsing VAEs for modeling class conditional distributions for data is an exhaustive approach. The authors show in Fig 4 that ABS generates adversarials that are semantically meaningful for humans, which is not achieved by Madry et al and other models. \n\nCons: \n1) The main concern with this work is that it is heavily tailored towards MNIST and the authors do mention this. Scaling this to other datasets does not seem easy. \n2) Using VAEs to model the conditional class distributions is a nice idea, but how does this scale for datasets with large number of classes like imagenet? This would result in having 1000s of VAEs. \n3) It would be nice to see this model behaves for skewed datasets. \n\n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper353/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards the first adversarially robust neural network model on MNIST", "abstract": "Despite much effort, deep neural networks remain highly susceptible to tiny input perturbations and even for MNIST, one of the most common toy datasets in computer vision, no neural network model exists for which adversarial perturbations are large and make semantic sense to humans. We show that even the widely recognized and by far most successful L-inf defense by Madry et~al. (1) has lower L0 robustness than undefended networks and still highly susceptible to L2 perturbations, (2) classifies unrecognizable images with high certainty, (3) performs not much better than simple input binarization and (4) features adversarial perturbations that make little sense to humans. These results suggest that MNIST is far from being solved in terms of adversarial robustness. We present a novel robust classification model that performs analysis by synthesis using learned class-conditional data distributions. We derive bounds on the robustness and go to great length to empirically evaluate our model using maximally effective adversarial attacks by (a) applying decision-based, score-based, gradient-based and transfer-based attacks for several different Lp norms, (b) by designing a new attack that exploits the structure of our defended model and (c) by devising a novel decision-based attack that seeks to minimize the number of perturbed pixels (L0). The results suggest that our approach yields state-of-the-art robustness on MNIST against L0, L2 and L-inf perturbations and we demonstrate that most adversarial examples are strongly perturbed towards the perceptual boundary between the original and the adversarial class.", "keywords": ["adversarial examples", "MNIST", "robustness", "deep learning", "security"], "authorids": ["lukas.schott@bethgelab.org", "jonas.rauber@bethgelab.org", "matthias.bethge@bethgelab.org", "wieland.brendel@bethgelab.org"], "authors": ["Lukas Schott", "Jonas Rauber", "Matthias Bethge", "Wieland Brendel"], "pdf": "/pdf/ba645fe0aec0af7d9a401c8eb0c7462a25da3dba.pdf", "paperhash": "schott|towards_the_first_adversarially_robust_neural_network_model_on_mnist", "_bibtex": "@inproceedings{\nschott2018towards,\ntitle={Towards the first adversarially robust neural network model on {MNIST}},\nauthor={Lukas Schott and Jonas Rauber and Matthias Bethge and Wieland Brendel},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1EHOsC9tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper353/Official_Review", "cdate": 1542234480566, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "S1EHOsC9tX", "replyto": "S1EHOsC9tX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper353/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335705374, "tmdate": 1552335705374, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper353/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "BkgyRoDshm", "original": null, "number": 4, "cdate": 1541270471226, "ddate": null, "tcdate": 1541270471226, "tmdate": 1541270471226, "tddate": null, "forum": "S1EHOsC9tX", "replyto": "H1xrRbe537", "invitation": "ICLR.cc/2019/Conference/-/Paper353/Public_Comment", "content": {"comment": "That makes sense. Thanks a lot for the explanation.", "title": "Thanks"}, "signatures": ["~Florian_Tramer1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper353/Reviewers/Unsubmitted"], "writers": ["~Florian_Tramer1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards the first adversarially robust neural network model on MNIST", "abstract": "Despite much effort, deep neural networks remain highly susceptible to tiny input perturbations and even for MNIST, one of the most common toy datasets in computer vision, no neural network model exists for which adversarial perturbations are large and make semantic sense to humans. We show that even the widely recognized and by far most successful L-inf defense by Madry et~al. (1) has lower L0 robustness than undefended networks and still highly susceptible to L2 perturbations, (2) classifies unrecognizable images with high certainty, (3) performs not much better than simple input binarization and (4) features adversarial perturbations that make little sense to humans. These results suggest that MNIST is far from being solved in terms of adversarial robustness. We present a novel robust classification model that performs analysis by synthesis using learned class-conditional data distributions. We derive bounds on the robustness and go to great length to empirically evaluate our model using maximally effective adversarial attacks by (a) applying decision-based, score-based, gradient-based and transfer-based attacks for several different Lp norms, (b) by designing a new attack that exploits the structure of our defended model and (c) by devising a novel decision-based attack that seeks to minimize the number of perturbed pixels (L0). The results suggest that our approach yields state-of-the-art robustness on MNIST against L0, L2 and L-inf perturbations and we demonstrate that most adversarial examples are strongly perturbed towards the perceptual boundary between the original and the adversarial class.", "keywords": ["adversarial examples", "MNIST", "robustness", "deep learning", "security"], "authorids": ["lukas.schott@bethgelab.org", "jonas.rauber@bethgelab.org", "matthias.bethge@bethgelab.org", "wieland.brendel@bethgelab.org"], "authors": ["Lukas Schott", "Jonas Rauber", "Matthias Bethge", "Wieland Brendel"], "pdf": "/pdf/ba645fe0aec0af7d9a401c8eb0c7462a25da3dba.pdf", "paperhash": "schott|towards_the_first_adversarially_robust_neural_network_model_on_mnist", "_bibtex": "@inproceedings{\nschott2018towards,\ntitle={Towards the first adversarially robust neural network model on {MNIST}},\nauthor={Lukas Schott and Jonas Rauber and Matthias Bethge and Wieland Brendel},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1EHOsC9tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper353/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311859878, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "S1EHOsC9tX", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper353/Authors", "ICLR.cc/2019/Conference/Paper353/Reviewers", "ICLR.cc/2019/Conference/Paper353/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper353/Authors", "ICLR.cc/2019/Conference/Paper353/Reviewers", "ICLR.cc/2019/Conference/Paper353/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311859878}}}, {"id": "H1xrRbe537", "original": null, "number": 4, "cdate": 1541173708713, "ddate": null, "tcdate": 1541173708713, "tmdate": 1541173708713, "tddate": null, "forum": "S1EHOsC9tX", "replyto": "B1gdc1g92m", "invitation": "ICLR.cc/2019/Conference/-/Paper353/Official_Comment", "content": {"title": "Transformations beyond the data", "comment": "You can think of AbS as incorporating an explicit Gaussian noise model (by means of the Gaussian posterior): it basically assumes that the signal (the digit) is corrupted by noise. In return, as long as the corrupted images stay close (in terms of L2) to the original image, the AbS will not change it's decision. The difference between rotations and L_infty perturbations is that the latter still stay close to the original image in terms of L2 (at least roughly), whereas small rotations can easily lead to large L2 distances."}, "signatures": ["ICLR.cc/2019/Conference/Paper353/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper353/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper353/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards the first adversarially robust neural network model on MNIST", "abstract": "Despite much effort, deep neural networks remain highly susceptible to tiny input perturbations and even for MNIST, one of the most common toy datasets in computer vision, no neural network model exists for which adversarial perturbations are large and make semantic sense to humans. We show that even the widely recognized and by far most successful L-inf defense by Madry et~al. (1) has lower L0 robustness than undefended networks and still highly susceptible to L2 perturbations, (2) classifies unrecognizable images with high certainty, (3) performs not much better than simple input binarization and (4) features adversarial perturbations that make little sense to humans. These results suggest that MNIST is far from being solved in terms of adversarial robustness. We present a novel robust classification model that performs analysis by synthesis using learned class-conditional data distributions. We derive bounds on the robustness and go to great length to empirically evaluate our model using maximally effective adversarial attacks by (a) applying decision-based, score-based, gradient-based and transfer-based attacks for several different Lp norms, (b) by designing a new attack that exploits the structure of our defended model and (c) by devising a novel decision-based attack that seeks to minimize the number of perturbed pixels (L0). The results suggest that our approach yields state-of-the-art robustness on MNIST against L0, L2 and L-inf perturbations and we demonstrate that most adversarial examples are strongly perturbed towards the perceptual boundary between the original and the adversarial class.", "keywords": ["adversarial examples", "MNIST", "robustness", "deep learning", "security"], "authorids": ["lukas.schott@bethgelab.org", "jonas.rauber@bethgelab.org", "matthias.bethge@bethgelab.org", "wieland.brendel@bethgelab.org"], "authors": ["Lukas Schott", "Jonas Rauber", "Matthias Bethge", "Wieland Brendel"], "pdf": "/pdf/ba645fe0aec0af7d9a401c8eb0c7462a25da3dba.pdf", "paperhash": "schott|towards_the_first_adversarially_robust_neural_network_model_on_mnist", "_bibtex": "@inproceedings{\nschott2018towards,\ntitle={Towards the first adversarially robust neural network model on {MNIST}},\nauthor={Lukas Schott and Jonas Rauber and Matthias Bethge and Wieland Brendel},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1EHOsC9tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper353/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621604269, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1EHOsC9tX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper353/Authors", "ICLR.cc/2019/Conference/Paper353/Reviewers", "ICLR.cc/2019/Conference/Paper353/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper353/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper353/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper353/Authors|ICLR.cc/2019/Conference/Paper353/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper353/Reviewers", "ICLR.cc/2019/Conference/Paper353/Authors", "ICLR.cc/2019/Conference/Paper353/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621604269}}}, {"id": "B1gdc1g92m", "original": null, "number": 3, "cdate": 1541173135538, "ddate": null, "tcdate": 1541173135538, "tmdate": 1541173135538, "tddate": null, "forum": "S1EHOsC9tX", "replyto": "SylqQqa6jQ", "invitation": "ICLR.cc/2019/Conference/-/Paper353/Public_Comment", "content": {"comment": "Thanks a lot for doing this, very cool results!\n\nI agree with your point about the hardness of learning transformations that go beyond what is in the dataset.\nThis raises an interesting question regarding the difference between rotation/translations and l_p perturbations. Intuitively, large l_infty perturbations also go beyond typical data transformations. Yet AbS seems to do fine with them. \n\n", "title": "Transformations beyond the data"}, "signatures": ["~Florian_Tramer1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper353/Reviewers/Unsubmitted"], "writers": ["~Florian_Tramer1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards the first adversarially robust neural network model on MNIST", "abstract": "Despite much effort, deep neural networks remain highly susceptible to tiny input perturbations and even for MNIST, one of the most common toy datasets in computer vision, no neural network model exists for which adversarial perturbations are large and make semantic sense to humans. We show that even the widely recognized and by far most successful L-inf defense by Madry et~al. (1) has lower L0 robustness than undefended networks and still highly susceptible to L2 perturbations, (2) classifies unrecognizable images with high certainty, (3) performs not much better than simple input binarization and (4) features adversarial perturbations that make little sense to humans. These results suggest that MNIST is far from being solved in terms of adversarial robustness. We present a novel robust classification model that performs analysis by synthesis using learned class-conditional data distributions. We derive bounds on the robustness and go to great length to empirically evaluate our model using maximally effective adversarial attacks by (a) applying decision-based, score-based, gradient-based and transfer-based attacks for several different Lp norms, (b) by designing a new attack that exploits the structure of our defended model and (c) by devising a novel decision-based attack that seeks to minimize the number of perturbed pixels (L0). The results suggest that our approach yields state-of-the-art robustness on MNIST against L0, L2 and L-inf perturbations and we demonstrate that most adversarial examples are strongly perturbed towards the perceptual boundary between the original and the adversarial class.", "keywords": ["adversarial examples", "MNIST", "robustness", "deep learning", "security"], "authorids": ["lukas.schott@bethgelab.org", "jonas.rauber@bethgelab.org", "matthias.bethge@bethgelab.org", "wieland.brendel@bethgelab.org"], "authors": ["Lukas Schott", "Jonas Rauber", "Matthias Bethge", "Wieland Brendel"], "pdf": "/pdf/ba645fe0aec0af7d9a401c8eb0c7462a25da3dba.pdf", "paperhash": "schott|towards_the_first_adversarially_robust_neural_network_model_on_mnist", "_bibtex": "@inproceedings{\nschott2018towards,\ntitle={Towards the first adversarially robust neural network model on {MNIST}},\nauthor={Lukas Schott and Jonas Rauber and Matthias Bethge and Wieland Brendel},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1EHOsC9tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper353/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311859878, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "S1EHOsC9tX", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper353/Authors", "ICLR.cc/2019/Conference/Paper353/Reviewers", "ICLR.cc/2019/Conference/Paper353/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper353/Authors", "ICLR.cc/2019/Conference/Paper353/Reviewers", "ICLR.cc/2019/Conference/Paper353/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311859878}}}, {"id": "ByexLNECsX", "original": null, "number": 3, "cdate": 1540404296268, "ddate": null, "tcdate": 1540404296268, "tmdate": 1540404342300, "tddate": null, "forum": "S1EHOsC9tX", "replyto": "S1EHOsC9tX", "invitation": "ICLR.cc/2019/Conference/-/Paper353/Official_Comment", "content": {"title": "Small update of L0 results", "comment": "Dear reviewers and readers,\n\nwe performed additional robustness evaluations and discovered a minor issue with the random seed in the Salt and Pepper (S&P) attack. We reevaluated robustness against S&P as well as Pointwise attack (which uses S&P for initialization) and found small changes in the L0 results:\n\nFormat:           Binary ABS robustness  |  ABS robustness\n\nL2 Pointwise Attack:           no change  |    4.8 -> 4.6\nL2 overall:                             no change  |     no change\n\nL0 Salt&Pepper Noise:  158.5 -> 146.0  |  182.5 -> 165.0\nL0 Pointwise Attack:         36.5 ->  22.0  |   22.0 ->  16.5\nL0 overall:                           36.0 ->  21.5  |   22.0 ->  16.5\n\nWe will update table 1 and figure 2 in the manuscript accordingly. No conclusions or statements in the paper are affected."}, "signatures": ["ICLR.cc/2019/Conference/Paper353/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper353/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper353/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards the first adversarially robust neural network model on MNIST", "abstract": "Despite much effort, deep neural networks remain highly susceptible to tiny input perturbations and even for MNIST, one of the most common toy datasets in computer vision, no neural network model exists for which adversarial perturbations are large and make semantic sense to humans. We show that even the widely recognized and by far most successful L-inf defense by Madry et~al. (1) has lower L0 robustness than undefended networks and still highly susceptible to L2 perturbations, (2) classifies unrecognizable images with high certainty, (3) performs not much better than simple input binarization and (4) features adversarial perturbations that make little sense to humans. These results suggest that MNIST is far from being solved in terms of adversarial robustness. We present a novel robust classification model that performs analysis by synthesis using learned class-conditional data distributions. We derive bounds on the robustness and go to great length to empirically evaluate our model using maximally effective adversarial attacks by (a) applying decision-based, score-based, gradient-based and transfer-based attacks for several different Lp norms, (b) by designing a new attack that exploits the structure of our defended model and (c) by devising a novel decision-based attack that seeks to minimize the number of perturbed pixels (L0). The results suggest that our approach yields state-of-the-art robustness on MNIST against L0, L2 and L-inf perturbations and we demonstrate that most adversarial examples are strongly perturbed towards the perceptual boundary between the original and the adversarial class.", "keywords": ["adversarial examples", "MNIST", "robustness", "deep learning", "security"], "authorids": ["lukas.schott@bethgelab.org", "jonas.rauber@bethgelab.org", "matthias.bethge@bethgelab.org", "wieland.brendel@bethgelab.org"], "authors": ["Lukas Schott", "Jonas Rauber", "Matthias Bethge", "Wieland Brendel"], "pdf": "/pdf/ba645fe0aec0af7d9a401c8eb0c7462a25da3dba.pdf", "paperhash": "schott|towards_the_first_adversarially_robust_neural_network_model_on_mnist", "_bibtex": "@inproceedings{\nschott2018towards,\ntitle={Towards the first adversarially robust neural network model on {MNIST}},\nauthor={Lukas Schott and Jonas Rauber and Matthias Bethge and Wieland Brendel},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1EHOsC9tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper353/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621604269, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1EHOsC9tX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper353/Authors", "ICLR.cc/2019/Conference/Paper353/Reviewers", "ICLR.cc/2019/Conference/Paper353/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper353/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper353/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper353/Authors|ICLR.cc/2019/Conference/Paper353/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper353/Reviewers", "ICLR.cc/2019/Conference/Paper353/Authors", "ICLR.cc/2019/Conference/Paper353/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621604269}}}, {"id": "SylqQqa6jQ", "original": null, "number": 2, "cdate": 1540377122381, "ddate": null, "tcdate": 1540377122381, "tmdate": 1540377122381, "tddate": null, "forum": "S1EHOsC9tX", "replyto": "rJxefCOMom", "invitation": "ICLR.cc/2019/Conference/-/Paper353/Official_Comment", "content": {"title": "Results of spatial transformation attack", "comment": "Dear Florian, that's a great suggestion! I took the time to re-implement the spatial attack in Foolbox (because our whole evaluation setup is based on it) and tested (1) a vanilla MNIST network (the one used by Madry et al, as taken from Madry's challenge), (2) the Madry et al. defense (the secret model in Madry's challenge) and (3) our AbS model. We used the same transformation ranges as [Engstrom et al.] (translations: +- 3px, rotation +- 30 degrees). Here are the results:\n\n(1 - Vanilla) Translation-only: 12,3%  ---  Rotation-only: 12.7%  ---  Translation & Rotation: 0.01%\n(2 - Madry) Translation-only:       9%  ---  Rotation-only: 66.0%  ---  Translation & Rotation: 0%\n(3 - AbS)      Translation-only: 25.5%  ---  Rotation-only: 67.1%  ---  Translation & Rotation: 0.3%\n\nI am not yet able to reproduce the large difference between vanilla and defended network present in [Engstrom et al]. We found the defense by Madry et al. work a little worse than reported in [Engstrom et al], in particular with respect to translations, while we found the vanilla network to perform much worse (we used a different one than in [Engstrom et al.] though, which probably explains the difference). AbS performs much better than vanilla in both rotation and translation and also performs better than Madry et al. on shifts. Frankly, I'd expected the AbS to perform even better but on the other hand, if the transformations go beyond the typical transformations of the data than there is no reason why the AbS should learn them."}, "signatures": ["ICLR.cc/2019/Conference/Paper353/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper353/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper353/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards the first adversarially robust neural network model on MNIST", "abstract": "Despite much effort, deep neural networks remain highly susceptible to tiny input perturbations and even for MNIST, one of the most common toy datasets in computer vision, no neural network model exists for which adversarial perturbations are large and make semantic sense to humans. We show that even the widely recognized and by far most successful L-inf defense by Madry et~al. (1) has lower L0 robustness than undefended networks and still highly susceptible to L2 perturbations, (2) classifies unrecognizable images with high certainty, (3) performs not much better than simple input binarization and (4) features adversarial perturbations that make little sense to humans. These results suggest that MNIST is far from being solved in terms of adversarial robustness. We present a novel robust classification model that performs analysis by synthesis using learned class-conditional data distributions. We derive bounds on the robustness and go to great length to empirically evaluate our model using maximally effective adversarial attacks by (a) applying decision-based, score-based, gradient-based and transfer-based attacks for several different Lp norms, (b) by designing a new attack that exploits the structure of our defended model and (c) by devising a novel decision-based attack that seeks to minimize the number of perturbed pixels (L0). The results suggest that our approach yields state-of-the-art robustness on MNIST against L0, L2 and L-inf perturbations and we demonstrate that most adversarial examples are strongly perturbed towards the perceptual boundary between the original and the adversarial class.", "keywords": ["adversarial examples", "MNIST", "robustness", "deep learning", "security"], "authorids": ["lukas.schott@bethgelab.org", "jonas.rauber@bethgelab.org", "matthias.bethge@bethgelab.org", "wieland.brendel@bethgelab.org"], "authors": ["Lukas Schott", "Jonas Rauber", "Matthias Bethge", "Wieland Brendel"], "pdf": "/pdf/ba645fe0aec0af7d9a401c8eb0c7462a25da3dba.pdf", "paperhash": "schott|towards_the_first_adversarially_robust_neural_network_model_on_mnist", "_bibtex": "@inproceedings{\nschott2018towards,\ntitle={Towards the first adversarially robust neural network model on {MNIST}},\nauthor={Lukas Schott and Jonas Rauber and Matthias Bethge and Wieland Brendel},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1EHOsC9tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper353/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621604269, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1EHOsC9tX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper353/Authors", "ICLR.cc/2019/Conference/Paper353/Reviewers", "ICLR.cc/2019/Conference/Paper353/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper353/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper353/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper353/Authors|ICLR.cc/2019/Conference/Paper353/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper353/Reviewers", "ICLR.cc/2019/Conference/Paper353/Authors", "ICLR.cc/2019/Conference/Paper353/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621604269}}}, {"id": "rJxefCOMom", "original": null, "number": 2, "cdate": 1539636743748, "ddate": null, "tcdate": 1539636743748, "tmdate": 1539636786622, "tddate": null, "forum": "S1EHOsC9tX", "replyto": "S1EHOsC9tX", "invitation": "ICLR.cc/2019/Conference/-/Paper353/Public_Comment", "content": {"comment": "Did you measure the robustness of your model to small (worst-case) rotations and translations? (https://arxiv.org/abs/1712.02779)\n\nI think these attacks could be good candidates to further show that your model is not subject to some form of gradient masking, as the worst-case perturbation can be found via exhaustive search.\n\nIncidentally, rotations and translations are another class of perturbations that the l-infinity model of Madry et al. is not robust against (that's what the above paper by the same authors shows). The paper also shows that you can adversarially train a model to be robust to rotations and translations, but I don't think it says anything about training a model that is robust to both rotations/translations and l-infinity attacks (which your model might be)", "title": "Rotations/translations"}, "signatures": ["~Florian_Tramer1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper353/Reviewers/Unsubmitted"], "writers": ["~Florian_Tramer1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards the first adversarially robust neural network model on MNIST", "abstract": "Despite much effort, deep neural networks remain highly susceptible to tiny input perturbations and even for MNIST, one of the most common toy datasets in computer vision, no neural network model exists for which adversarial perturbations are large and make semantic sense to humans. We show that even the widely recognized and by far most successful L-inf defense by Madry et~al. (1) has lower L0 robustness than undefended networks and still highly susceptible to L2 perturbations, (2) classifies unrecognizable images with high certainty, (3) performs not much better than simple input binarization and (4) features adversarial perturbations that make little sense to humans. These results suggest that MNIST is far from being solved in terms of adversarial robustness. We present a novel robust classification model that performs analysis by synthesis using learned class-conditional data distributions. We derive bounds on the robustness and go to great length to empirically evaluate our model using maximally effective adversarial attacks by (a) applying decision-based, score-based, gradient-based and transfer-based attacks for several different Lp norms, (b) by designing a new attack that exploits the structure of our defended model and (c) by devising a novel decision-based attack that seeks to minimize the number of perturbed pixels (L0). The results suggest that our approach yields state-of-the-art robustness on MNIST against L0, L2 and L-inf perturbations and we demonstrate that most adversarial examples are strongly perturbed towards the perceptual boundary between the original and the adversarial class.", "keywords": ["adversarial examples", "MNIST", "robustness", "deep learning", "security"], "authorids": ["lukas.schott@bethgelab.org", "jonas.rauber@bethgelab.org", "matthias.bethge@bethgelab.org", "wieland.brendel@bethgelab.org"], "authors": ["Lukas Schott", "Jonas Rauber", "Matthias Bethge", "Wieland Brendel"], "pdf": "/pdf/ba645fe0aec0af7d9a401c8eb0c7462a25da3dba.pdf", "paperhash": "schott|towards_the_first_adversarially_robust_neural_network_model_on_mnist", "_bibtex": "@inproceedings{\nschott2018towards,\ntitle={Towards the first adversarially robust neural network model on {MNIST}},\nauthor={Lukas Schott and Jonas Rauber and Matthias Bethge and Wieland Brendel},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1EHOsC9tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper353/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311859878, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "S1EHOsC9tX", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper353/Authors", "ICLR.cc/2019/Conference/Paper353/Reviewers", "ICLR.cc/2019/Conference/Paper353/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper353/Authors", "ICLR.cc/2019/Conference/Paper353/Reviewers", "ICLR.cc/2019/Conference/Paper353/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311859878}}}, {"id": "SygNj-kI57", "original": null, "number": 1, "cdate": 1538810268051, "ddate": null, "tcdate": 1538810268051, "tmdate": 1538982321170, "tddate": null, "forum": "S1EHOsC9tX", "replyto": "HJgxiO8b5m", "invitation": "ICLR.cc/2019/Conference/-/Paper353/Official_Comment", "content": {"title": "ABS model is robust to background pixel attack", "comment": "Thanks for your comment! We tested our ABS model against one of the background pixel attacks suggested in fig. 6 of https://arxiv.org/pdf/1807.06732.pdf (random lines added on top of the samples) and found a strong robustness against such perturbations (96% accuracy for two lines, 86% for four lines and 54% for eight lines [difficult even for humans], see https://ibb.co/cpDt9K for samples). The combination of Madry et al. with weight decay is certainly interesting but out of the scope of this paper. Thanks for the L1 reference, we'll include it in the manuscript."}, "signatures": ["ICLR.cc/2019/Conference/Paper353/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper353/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper353/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards the first adversarially robust neural network model on MNIST", "abstract": "Despite much effort, deep neural networks remain highly susceptible to tiny input perturbations and even for MNIST, one of the most common toy datasets in computer vision, no neural network model exists for which adversarial perturbations are large and make semantic sense to humans. We show that even the widely recognized and by far most successful L-inf defense by Madry et~al. (1) has lower L0 robustness than undefended networks and still highly susceptible to L2 perturbations, (2) classifies unrecognizable images with high certainty, (3) performs not much better than simple input binarization and (4) features adversarial perturbations that make little sense to humans. These results suggest that MNIST is far from being solved in terms of adversarial robustness. We present a novel robust classification model that performs analysis by synthesis using learned class-conditional data distributions. We derive bounds on the robustness and go to great length to empirically evaluate our model using maximally effective adversarial attacks by (a) applying decision-based, score-based, gradient-based and transfer-based attacks for several different Lp norms, (b) by designing a new attack that exploits the structure of our defended model and (c) by devising a novel decision-based attack that seeks to minimize the number of perturbed pixels (L0). The results suggest that our approach yields state-of-the-art robustness on MNIST against L0, L2 and L-inf perturbations and we demonstrate that most adversarial examples are strongly perturbed towards the perceptual boundary between the original and the adversarial class.", "keywords": ["adversarial examples", "MNIST", "robustness", "deep learning", "security"], "authorids": ["lukas.schott@bethgelab.org", "jonas.rauber@bethgelab.org", "matthias.bethge@bethgelab.org", "wieland.brendel@bethgelab.org"], "authors": ["Lukas Schott", "Jonas Rauber", "Matthias Bethge", "Wieland Brendel"], "pdf": "/pdf/ba645fe0aec0af7d9a401c8eb0c7462a25da3dba.pdf", "paperhash": "schott|towards_the_first_adversarially_robust_neural_network_model_on_mnist", "_bibtex": "@inproceedings{\nschott2018towards,\ntitle={Towards the first adversarially robust neural network model on {MNIST}},\nauthor={Lukas Schott and Jonas Rauber and Matthias Bethge and Wieland Brendel},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1EHOsC9tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper353/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621604269, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1EHOsC9tX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper353/Authors", "ICLR.cc/2019/Conference/Paper353/Reviewers", "ICLR.cc/2019/Conference/Paper353/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper353/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper353/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper353/Authors|ICLR.cc/2019/Conference/Paper353/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper353/Reviewers", "ICLR.cc/2019/Conference/Paper353/Authors", "ICLR.cc/2019/Conference/Paper353/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621604269}}}, {"id": "HJgxiO8b5m", "original": null, "number": 1, "cdate": 1538513047858, "ddate": null, "tcdate": 1538513047858, "tmdate": 1538513068217, "tddate": null, "forum": "S1EHOsC9tX", "replyto": "S1EHOsC9tX", "invitation": "ICLR.cc/2019/Conference/-/Paper353/Public_Comment", "content": {"comment": "Since a major claim of this paper (the first claim listed in the abstract) is that the Madry et al 2017 model doesn't defend against L0 or L2 attacks, it seems like it would make sense to discuss earlier related work that showed the Madry et al 2017 model doesn't defend against attacks other than Linf threat model it was designed for. To the best of my knowledge, the first such work was the demonstration that it doesn't defend against L1 attacks, which seem to not be mentioned at all in this submission: https://arxiv.org/abs/1710.10733\n There is also the background pixel attack (fig 6 of https://arxiv.org/pdf/1807.06732.pdf ) and a variety of threat models described by https://arxiv.org/abs/1804.03308 where weight decay outperforms Linf-adversarial training.", "title": "Related work"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper353/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards the first adversarially robust neural network model on MNIST", "abstract": "Despite much effort, deep neural networks remain highly susceptible to tiny input perturbations and even for MNIST, one of the most common toy datasets in computer vision, no neural network model exists for which adversarial perturbations are large and make semantic sense to humans. We show that even the widely recognized and by far most successful L-inf defense by Madry et~al. (1) has lower L0 robustness than undefended networks and still highly susceptible to L2 perturbations, (2) classifies unrecognizable images with high certainty, (3) performs not much better than simple input binarization and (4) features adversarial perturbations that make little sense to humans. These results suggest that MNIST is far from being solved in terms of adversarial robustness. We present a novel robust classification model that performs analysis by synthesis using learned class-conditional data distributions. We derive bounds on the robustness and go to great length to empirically evaluate our model using maximally effective adversarial attacks by (a) applying decision-based, score-based, gradient-based and transfer-based attacks for several different Lp norms, (b) by designing a new attack that exploits the structure of our defended model and (c) by devising a novel decision-based attack that seeks to minimize the number of perturbed pixels (L0). The results suggest that our approach yields state-of-the-art robustness on MNIST against L0, L2 and L-inf perturbations and we demonstrate that most adversarial examples are strongly perturbed towards the perceptual boundary between the original and the adversarial class.", "keywords": ["adversarial examples", "MNIST", "robustness", "deep learning", "security"], "authorids": ["lukas.schott@bethgelab.org", "jonas.rauber@bethgelab.org", "matthias.bethge@bethgelab.org", "wieland.brendel@bethgelab.org"], "authors": ["Lukas Schott", "Jonas Rauber", "Matthias Bethge", "Wieland Brendel"], "pdf": "/pdf/ba645fe0aec0af7d9a401c8eb0c7462a25da3dba.pdf", "paperhash": "schott|towards_the_first_adversarially_robust_neural_network_model_on_mnist", "_bibtex": "@inproceedings{\nschott2018towards,\ntitle={Towards the first adversarially robust neural network model on {MNIST}},\nauthor={Lukas Schott and Jonas Rauber and Matthias Bethge and Wieland Brendel},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1EHOsC9tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper353/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311859878, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "S1EHOsC9tX", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper353/Authors", "ICLR.cc/2019/Conference/Paper353/Reviewers", "ICLR.cc/2019/Conference/Paper353/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper353/Authors", "ICLR.cc/2019/Conference/Paper353/Reviewers", "ICLR.cc/2019/Conference/Paper353/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311859878}}}], "count": 31}