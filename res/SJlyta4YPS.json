{"notes": [{"id": "SJlyta4YPS", "original": "HyeUMj3PvS", "number": 654, "cdate": 1569439094933, "ddate": null, "tcdate": 1569439094933, "tmdate": 1577168265533, "tddate": null, "forum": "SJlyta4YPS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "DeepEnFM: Deep neural networks with Encoder enhanced Factorization Machine", "authors": ["Qiang Sun", "Zhinan Cheng", "Yanwei Fu", "Wenxuan Wang", "Yu-Gang Jiang", "Xiangyang Xue"], "authorids": ["sunqiang85@gmail.com", "zhinancheng.bryan@gmail.com", "yanweifu@fudan.edu.cn", "wxwang.iris@gmail.com", "ygj@fudan.edu.cn", "xyxue@fudan.edu.cn"], "keywords": ["CTR", "Attention", "Transformer", "Encoder"], "TL;DR": "DNN and Encoder enhanced FM with bilinear attention and max-pooling for CTR", "abstract": "Click Through Rate (CTR) prediction is a critical task in industrial applications, especially for online social and commerce applications. It is challenging to find a proper way to automatically discover the effective cross features in CTR tasks. We propose a novel model for CTR tasks, called Deep neural networks with Encoder enhanced Factorization Machine (DeepEnFM). Instead of learning the cross features directly, DeepEnFM adopts the Transformer encoder as a backbone to align the feature embeddings with the clues of other fields. The embeddings generated from encoder are beneficial for the further feature interactions. Particularly, DeepEnFM utilizes a bilinear approach to generate different similarity functions with respect to different field pairs. Furthermore, the max-pooling method makes DeepEnFM feasible to capture both the supplementary and suppressing information among different attention heads. Our model is validated on the Criteo and Avazu datasets, and achieves state-of-art performance.", "pdf": "/pdf/172f7a073710dcf32034814f99bd0b87fa2d863c.pdf", "paperhash": "sun|deepenfm_deep_neural_networks_with_encoder_enhanced_factorization_machine", "original_pdf": "/attachment/430f0df746e86f4641be8a46379ac4d58c1623c1.pdf", "_bibtex": "@misc{\nsun2020deepenfm,\ntitle={DeepEn{\\{}FM{\\}}: Deep neural networks with Encoder enhanced Factorization Machine},\nauthor={Qiang Sun and Zhinan Cheng and Yanwei Fu and Wenxuan Wang and Yu-Gang Jiang and Xiangyang Xue},\nyear={2020},\nurl={https://openreview.net/forum?id=SJlyta4YPS}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "A_3Ru-_Cw", "original": null, "number": 1, "cdate": 1576798702442, "ddate": null, "tcdate": 1576798702442, "tmdate": 1576800933576, "tddate": null, "forum": "SJlyta4YPS", "replyto": "SJlyta4YPS", "invitation": "ICLR.cc/2020/Conference/Paper654/-/Decision", "content": {"decision": "Reject", "comment": "The authors address the problem of CTR prediction by using a Transformer based encoder to capture interactions between features. They suggest simple modifications to the basic Multiple Head Self Attention (MSHA) mechanism and show that they get the best performance on two publicly available datasets. \n\nWhile the reviewers agreed that this work is of practical importance, they had a few objections which I have summarised below: \n1) Lack of novelty: The reviewers felt that the adoption of MSHA for the CTR task was straightforward. The suggested modifications in the form of Bilinear similarity and max-pooling were viewed as incremental contributions. \n2) Lack of comparison with existing work: The reviewers suggested some additional baselines (Deep and Cross) which need to be added (the authors have responded that they will do so later).\n3) Need to strengthen experiments: The reviewers appreciated the ablation studies done by the authors but requested for more studies to convincingly demonstrate the effect of some components. One reviewer also pointed that the authors should control form model complexity to ensure an apples-to-apples comparison (I agree that many papers in the past have not done this but going froward I have a hunch that many reviewers will start asking for this) . \n\nIMO, the above comments are important and the authors should try to address them in subsequent submissions.\n\nBased on the reviewer comments and lack of any response from the authors, I recommend that the paper in it current form cannot be accepted. ", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DeepEnFM: Deep neural networks with Encoder enhanced Factorization Machine", "authors": ["Qiang Sun", "Zhinan Cheng", "Yanwei Fu", "Wenxuan Wang", "Yu-Gang Jiang", "Xiangyang Xue"], "authorids": ["sunqiang85@gmail.com", "zhinancheng.bryan@gmail.com", "yanweifu@fudan.edu.cn", "wxwang.iris@gmail.com", "ygj@fudan.edu.cn", "xyxue@fudan.edu.cn"], "keywords": ["CTR", "Attention", "Transformer", "Encoder"], "TL;DR": "DNN and Encoder enhanced FM with bilinear attention and max-pooling for CTR", "abstract": "Click Through Rate (CTR) prediction is a critical task in industrial applications, especially for online social and commerce applications. It is challenging to find a proper way to automatically discover the effective cross features in CTR tasks. We propose a novel model for CTR tasks, called Deep neural networks with Encoder enhanced Factorization Machine (DeepEnFM). Instead of learning the cross features directly, DeepEnFM adopts the Transformer encoder as a backbone to align the feature embeddings with the clues of other fields. The embeddings generated from encoder are beneficial for the further feature interactions. Particularly, DeepEnFM utilizes a bilinear approach to generate different similarity functions with respect to different field pairs. Furthermore, the max-pooling method makes DeepEnFM feasible to capture both the supplementary and suppressing information among different attention heads. Our model is validated on the Criteo and Avazu datasets, and achieves state-of-art performance.", "pdf": "/pdf/172f7a073710dcf32034814f99bd0b87fa2d863c.pdf", "paperhash": "sun|deepenfm_deep_neural_networks_with_encoder_enhanced_factorization_machine", "original_pdf": "/attachment/430f0df746e86f4641be8a46379ac4d58c1623c1.pdf", "_bibtex": "@misc{\nsun2020deepenfm,\ntitle={DeepEn{\\{}FM{\\}}: Deep neural networks with Encoder enhanced Factorization Machine},\nauthor={Qiang Sun and Zhinan Cheng and Yanwei Fu and Wenxuan Wang and Yu-Gang Jiang and Xiangyang Xue},\nyear={2020},\nurl={https://openreview.net/forum?id=SJlyta4YPS}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "SJlyta4YPS", "replyto": "SJlyta4YPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795713104, "tmdate": 1576800262645, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper654/-/Decision"}}}, {"id": "Hke6LHdoiS", "original": null, "number": 4, "cdate": 1573778773327, "ddate": null, "tcdate": 1573778773327, "tmdate": 1573778773327, "tddate": null, "forum": "SJlyta4YPS", "replyto": "ryxkuCyxFr", "invitation": "ICLR.cc/2020/Conference/Paper654/-/Official_Comment", "content": {"title": "Response to Reviewer 1", "comment": "We thank the reviewer for the feedback and address the concerns in detail below\n1.\tRelevance to ICLR\nThanks, our paper aims to use the encoder to gain better field feature representation for CTR task, which is relevant to learning representations.\n2.\tThe meaning of \u201cDNN learns at bit-wise level.\u201d\nThe statement \u201cDNN learns at bit-wise level\u201d means that DNN learns the feature representation by the linear and non-linear transformation of neurons. The FM learns the inner product of two features, which is a vector-wise level. \n3.\tMajor component study.\nThanks, we have added the ablation study for the main component. We have removed the FM, DNN to analyze the contribution of FM and DNN in our work. The results show that the AUC of \u201cDeepEnFM w/o DNN\u201d is 0.8037 on criteo dataset, which is higher than AutoInt, which demonstrates the effectiveness of our encoder with bilinear and max-pooling method. Surprisingly, the AUC of  \u201cDeepEnFM w/o FM\u201d is 0.8059 on criteo dataset, which is very close to the full model(0.8077). The gap to the full model demonstrates the effect of each component."}, "signatures": ["ICLR.cc/2020/Conference/Paper654/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper654/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DeepEnFM: Deep neural networks with Encoder enhanced Factorization Machine", "authors": ["Qiang Sun", "Zhinan Cheng", "Yanwei Fu", "Wenxuan Wang", "Yu-Gang Jiang", "Xiangyang Xue"], "authorids": ["sunqiang85@gmail.com", "zhinancheng.bryan@gmail.com", "yanweifu@fudan.edu.cn", "wxwang.iris@gmail.com", "ygj@fudan.edu.cn", "xyxue@fudan.edu.cn"], "keywords": ["CTR", "Attention", "Transformer", "Encoder"], "TL;DR": "DNN and Encoder enhanced FM with bilinear attention and max-pooling for CTR", "abstract": "Click Through Rate (CTR) prediction is a critical task in industrial applications, especially for online social and commerce applications. It is challenging to find a proper way to automatically discover the effective cross features in CTR tasks. We propose a novel model for CTR tasks, called Deep neural networks with Encoder enhanced Factorization Machine (DeepEnFM). Instead of learning the cross features directly, DeepEnFM adopts the Transformer encoder as a backbone to align the feature embeddings with the clues of other fields. The embeddings generated from encoder are beneficial for the further feature interactions. Particularly, DeepEnFM utilizes a bilinear approach to generate different similarity functions with respect to different field pairs. Furthermore, the max-pooling method makes DeepEnFM feasible to capture both the supplementary and suppressing information among different attention heads. Our model is validated on the Criteo and Avazu datasets, and achieves state-of-art performance.", "pdf": "/pdf/172f7a073710dcf32034814f99bd0b87fa2d863c.pdf", "paperhash": "sun|deepenfm_deep_neural_networks_with_encoder_enhanced_factorization_machine", "original_pdf": "/attachment/430f0df746e86f4641be8a46379ac4d58c1623c1.pdf", "_bibtex": "@misc{\nsun2020deepenfm,\ntitle={DeepEn{\\{}FM{\\}}: Deep neural networks with Encoder enhanced Factorization Machine},\nauthor={Qiang Sun and Zhinan Cheng and Yanwei Fu and Wenxuan Wang and Yu-Gang Jiang and Xiangyang Xue},\nyear={2020},\nurl={https://openreview.net/forum?id=SJlyta4YPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJlyta4YPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper654/Authors", "ICLR.cc/2020/Conference/Paper654/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper654/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper654/Reviewers", "ICLR.cc/2020/Conference/Paper654/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper654/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper654/Authors|ICLR.cc/2020/Conference/Paper654/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504168244, "tmdate": 1576860540889, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper654/Authors", "ICLR.cc/2020/Conference/Paper654/Reviewers", "ICLR.cc/2020/Conference/Paper654/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper654/-/Official_Comment"}}}, {"id": "r1x65Ediir", "original": null, "number": 3, "cdate": 1573778581295, "ddate": null, "tcdate": 1573778581295, "tmdate": 1573778581295, "tddate": null, "forum": "SJlyta4YPS", "replyto": "S1ltPsiCFB", "invitation": "ICLR.cc/2020/Conference/Paper654/-/Official_Comment", "content": {"title": "Response to Reviewer 3", "comment": "We thank the reviewer for the feedback and address the concerns in detail below\n1.\tAbout the novelty.\nThe transformer encoder based approaches (e.g. BERT, ViLBERT) have gained great success in NLP and Vision & Language tasks. The application of encoder to CTR is promising to improve the performance by generating context-aware field embedding. And the utilization of encoder with the addition of bilinear and max-pooling method is novel in the CTR task.\n\n2.\tThe field embedding size and computation cost\nThanks, the assumption that \u201cthe field embedding size is very low\u201d is made by the observation from works on criteo and Avazu datasets. Actually, the embedding size is controllable, we can project the field feature into a low dimension space, then feed it to the DeepEnFM. With a low embedding size, the computation cost problem of bilinear can be alleviated.\n \n3.\tThe experiment of the ordinary MHSA\nThe experiment of the ordinary MHSA shows that the AUC is 0.8027 on Criteo data, which also beats the other baselines such as PNN*(0.7991), DeepFM(0.7928), AutoInt(0.8021). This implies that the encoder is effective for CTR task as well as NLP tasks. And the improvement over AutoInt demonstrates the contribution of the point-wise feed-forward layer in the encoder layer. Our DeepEnFM is 0.8077 with the bilinear and max-pooling method. "}, "signatures": ["ICLR.cc/2020/Conference/Paper654/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper654/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DeepEnFM: Deep neural networks with Encoder enhanced Factorization Machine", "authors": ["Qiang Sun", "Zhinan Cheng", "Yanwei Fu", "Wenxuan Wang", "Yu-Gang Jiang", "Xiangyang Xue"], "authorids": ["sunqiang85@gmail.com", "zhinancheng.bryan@gmail.com", "yanweifu@fudan.edu.cn", "wxwang.iris@gmail.com", "ygj@fudan.edu.cn", "xyxue@fudan.edu.cn"], "keywords": ["CTR", "Attention", "Transformer", "Encoder"], "TL;DR": "DNN and Encoder enhanced FM with bilinear attention and max-pooling for CTR", "abstract": "Click Through Rate (CTR) prediction is a critical task in industrial applications, especially for online social and commerce applications. It is challenging to find a proper way to automatically discover the effective cross features in CTR tasks. We propose a novel model for CTR tasks, called Deep neural networks with Encoder enhanced Factorization Machine (DeepEnFM). Instead of learning the cross features directly, DeepEnFM adopts the Transformer encoder as a backbone to align the feature embeddings with the clues of other fields. The embeddings generated from encoder are beneficial for the further feature interactions. Particularly, DeepEnFM utilizes a bilinear approach to generate different similarity functions with respect to different field pairs. Furthermore, the max-pooling method makes DeepEnFM feasible to capture both the supplementary and suppressing information among different attention heads. Our model is validated on the Criteo and Avazu datasets, and achieves state-of-art performance.", "pdf": "/pdf/172f7a073710dcf32034814f99bd0b87fa2d863c.pdf", "paperhash": "sun|deepenfm_deep_neural_networks_with_encoder_enhanced_factorization_machine", "original_pdf": "/attachment/430f0df746e86f4641be8a46379ac4d58c1623c1.pdf", "_bibtex": "@misc{\nsun2020deepenfm,\ntitle={DeepEn{\\{}FM{\\}}: Deep neural networks with Encoder enhanced Factorization Machine},\nauthor={Qiang Sun and Zhinan Cheng and Yanwei Fu and Wenxuan Wang and Yu-Gang Jiang and Xiangyang Xue},\nyear={2020},\nurl={https://openreview.net/forum?id=SJlyta4YPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJlyta4YPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper654/Authors", "ICLR.cc/2020/Conference/Paper654/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper654/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper654/Reviewers", "ICLR.cc/2020/Conference/Paper654/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper654/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper654/Authors|ICLR.cc/2020/Conference/Paper654/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504168244, "tmdate": 1576860540889, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper654/Authors", "ICLR.cc/2020/Conference/Paper654/Reviewers", "ICLR.cc/2020/Conference/Paper654/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper654/-/Official_Comment"}}}, {"id": "HJg8BN_jsH", "original": null, "number": 2, "cdate": 1573778494454, "ddate": null, "tcdate": 1573778494454, "tmdate": 1573778494454, "tddate": null, "forum": "SJlyta4YPS", "replyto": "SJeEerv-5B", "invitation": "ICLR.cc/2020/Conference/Paper654/-/Official_Comment", "content": {"title": "Response to Reviewer 2", "comment": "We thank the reviewer for the feedback and address the concerns in detail below \n1.\tThe capacity control problem.\nThe capacity problem can be alleviated by projecting the field embedding into a low dimension space and increasing the head number, since the bilinear weight size is $(fieldnum*embsize)^2/headnum$ \n2.\tWhat about Max-pooling when head number is 1.\nThe Max-pooling has no effects when the head number is 1. Max-pooling is effective when head_num is bigger than 1. As we mentioned in the ablation study of head_num, though head_num=1 has the best performance, head_num=2 achieves the best balance between the performance and the cost. In the case when head_num=2, max-pooling works.\n3.\tMissing baselines.\nThanks, Baselines such as xDeepFM, Deep & Cross will be added later."}, "signatures": ["ICLR.cc/2020/Conference/Paper654/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper654/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DeepEnFM: Deep neural networks with Encoder enhanced Factorization Machine", "authors": ["Qiang Sun", "Zhinan Cheng", "Yanwei Fu", "Wenxuan Wang", "Yu-Gang Jiang", "Xiangyang Xue"], "authorids": ["sunqiang85@gmail.com", "zhinancheng.bryan@gmail.com", "yanweifu@fudan.edu.cn", "wxwang.iris@gmail.com", "ygj@fudan.edu.cn", "xyxue@fudan.edu.cn"], "keywords": ["CTR", "Attention", "Transformer", "Encoder"], "TL;DR": "DNN and Encoder enhanced FM with bilinear attention and max-pooling for CTR", "abstract": "Click Through Rate (CTR) prediction is a critical task in industrial applications, especially for online social and commerce applications. It is challenging to find a proper way to automatically discover the effective cross features in CTR tasks. We propose a novel model for CTR tasks, called Deep neural networks with Encoder enhanced Factorization Machine (DeepEnFM). Instead of learning the cross features directly, DeepEnFM adopts the Transformer encoder as a backbone to align the feature embeddings with the clues of other fields. The embeddings generated from encoder are beneficial for the further feature interactions. Particularly, DeepEnFM utilizes a bilinear approach to generate different similarity functions with respect to different field pairs. Furthermore, the max-pooling method makes DeepEnFM feasible to capture both the supplementary and suppressing information among different attention heads. Our model is validated on the Criteo and Avazu datasets, and achieves state-of-art performance.", "pdf": "/pdf/172f7a073710dcf32034814f99bd0b87fa2d863c.pdf", "paperhash": "sun|deepenfm_deep_neural_networks_with_encoder_enhanced_factorization_machine", "original_pdf": "/attachment/430f0df746e86f4641be8a46379ac4d58c1623c1.pdf", "_bibtex": "@misc{\nsun2020deepenfm,\ntitle={DeepEn{\\{}FM{\\}}: Deep neural networks with Encoder enhanced Factorization Machine},\nauthor={Qiang Sun and Zhinan Cheng and Yanwei Fu and Wenxuan Wang and Yu-Gang Jiang and Xiangyang Xue},\nyear={2020},\nurl={https://openreview.net/forum?id=SJlyta4YPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJlyta4YPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper654/Authors", "ICLR.cc/2020/Conference/Paper654/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper654/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper654/Reviewers", "ICLR.cc/2020/Conference/Paper654/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper654/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper654/Authors|ICLR.cc/2020/Conference/Paper654/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504168244, "tmdate": 1576860540889, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper654/Authors", "ICLR.cc/2020/Conference/Paper654/Reviewers", "ICLR.cc/2020/Conference/Paper654/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper654/-/Official_Comment"}}}, {"id": "ryxkuCyxFr", "original": null, "number": 1, "cdate": 1570926183339, "ddate": null, "tcdate": 1570926183339, "tmdate": 1572972568712, "tddate": null, "forum": "SJlyta4YPS", "replyto": "SJlyta4YPS", "invitation": "ICLR.cc/2020/Conference/Paper654/-/Official_Review", "content": {"rating": "1: Reject", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "N/A", "review": "The authors propose a model for Click-Through Rate Prediction using a model consisting of an embedding layer, a Transformer stack, a Factorization Machine, and a DNN. \n\nI have several major concerns about the submission:\n2. Relevance: This work is extremely application specific, the application is not relevant to this community.\n1. Clarity and writing: The contributions which are relevant to the ICLR community are not explained well and the paper needs copy-editing for English grammar\n4. Novelty: While seemingly showing good results on some benchmarks, the model is a mix of many components and it's not clear which components actually improve performance and would be worth further study. \n\n\nMinor comments:\n\nApplying the DNN directly on top of the embeddings, and having a parallel stack of Encoder-FM, is not well explained. What does it mean that \"DNN aims at bit-wise level\" if the DNN receives the same embedding features as the encoder, which supposedly \"learn[s] at vector wise level\"? \n\nReferences to datasets are missing\n\nAblation study is limited, and has surprising results. E.g. even completely removing self-attention barely makes a dent in how well the method compares to other published work, moving it from rank 1 to rank 2. Otherwise only small tweaks with even more minor effects are made. What about removing e.g. the FM, other major components?\n\nThe biggest architectural innovations here are the bi-linear attention mechanism and max-pooling self attention. They are hard to interpret in this context. It's not clear how they would perform in a simpler architecture (e.g. vanilla BERT or Transformer) and in the context of a more standard benchmark. That study would have a lot more relevance to this community than the present one. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper654/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper654/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DeepEnFM: Deep neural networks with Encoder enhanced Factorization Machine", "authors": ["Qiang Sun", "Zhinan Cheng", "Yanwei Fu", "Wenxuan Wang", "Yu-Gang Jiang", "Xiangyang Xue"], "authorids": ["sunqiang85@gmail.com", "zhinancheng.bryan@gmail.com", "yanweifu@fudan.edu.cn", "wxwang.iris@gmail.com", "ygj@fudan.edu.cn", "xyxue@fudan.edu.cn"], "keywords": ["CTR", "Attention", "Transformer", "Encoder"], "TL;DR": "DNN and Encoder enhanced FM with bilinear attention and max-pooling for CTR", "abstract": "Click Through Rate (CTR) prediction is a critical task in industrial applications, especially for online social and commerce applications. It is challenging to find a proper way to automatically discover the effective cross features in CTR tasks. We propose a novel model for CTR tasks, called Deep neural networks with Encoder enhanced Factorization Machine (DeepEnFM). Instead of learning the cross features directly, DeepEnFM adopts the Transformer encoder as a backbone to align the feature embeddings with the clues of other fields. The embeddings generated from encoder are beneficial for the further feature interactions. Particularly, DeepEnFM utilizes a bilinear approach to generate different similarity functions with respect to different field pairs. Furthermore, the max-pooling method makes DeepEnFM feasible to capture both the supplementary and suppressing information among different attention heads. Our model is validated on the Criteo and Avazu datasets, and achieves state-of-art performance.", "pdf": "/pdf/172f7a073710dcf32034814f99bd0b87fa2d863c.pdf", "paperhash": "sun|deepenfm_deep_neural_networks_with_encoder_enhanced_factorization_machine", "original_pdf": "/attachment/430f0df746e86f4641be8a46379ac4d58c1623c1.pdf", "_bibtex": "@misc{\nsun2020deepenfm,\ntitle={DeepEn{\\{}FM{\\}}: Deep neural networks with Encoder enhanced Factorization Machine},\nauthor={Qiang Sun and Zhinan Cheng and Yanwei Fu and Wenxuan Wang and Yu-Gang Jiang and Xiangyang Xue},\nyear={2020},\nurl={https://openreview.net/forum?id=SJlyta4YPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SJlyta4YPS", "replyto": "SJlyta4YPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper654/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper654/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575699100795, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper654/Reviewers"], "noninvitees": [], "tcdate": 1570237749013, "tmdate": 1575699100811, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper654/-/Official_Review"}}}, {"id": "S1ltPsiCFB", "original": null, "number": 2, "cdate": 1571892065314, "ddate": null, "tcdate": 1571892065314, "tmdate": 1572972568677, "tddate": null, "forum": "SJlyta4YPS", "replyto": "SJlyta4YPS", "invitation": "ICLR.cc/2020/Conference/Paper654/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This papers proposes DeepEnFM approach for CTR prediction task. In detail, Transformer encoder is applied on top of embeddings to generate new projected embeddings. Such transformer encoder is composed of self-attention with bilinear (to replace dot) and multi-head, which is followed by a mx pooling layer and then a FC layer. Position encoding is utilized then. Besides, some resnet-style trick in placed in the middle. Such encoder output is fed into FM and raw embeddings are feed into DNN part. These two parts are then used for final prediction. Some experimental results show the improvement of the proposed method over other methods.\n\nThe major questions are:\n\n*  The assumption of \u201cThe field embedding size is very low in CTR\u201d is not reasonable. Do we have any study to verify this hypothesis?\n* Regarding to above hypothesis, i think it doesn\u2019t hold for all the CTR prediction tasks. Computation cost will be dramatically increased when embedding size increases because of bilinear between key and query and the FC on top of self-attention.\n* The novelty of the proposed method needs to justified to reach the bar of ICLR. The major reason is that 1) the proposed method just replaces MHSA with two changes, i.e., bilinear + max pooling, 2) other tricks such as resnet-style connection, layer norm and position encoding have been adopted everywhere.\n* The gain of proposed method is not so clear though the author test to remove each component from the architecture. As the change of encoder part is on top of MHSA, but there is no experiment to show the gain compared to using original MHSA instead of newly proposed bilinear + max pooling. I suggest to do this for better understanding the gain of changes.\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper654/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper654/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DeepEnFM: Deep neural networks with Encoder enhanced Factorization Machine", "authors": ["Qiang Sun", "Zhinan Cheng", "Yanwei Fu", "Wenxuan Wang", "Yu-Gang Jiang", "Xiangyang Xue"], "authorids": ["sunqiang85@gmail.com", "zhinancheng.bryan@gmail.com", "yanweifu@fudan.edu.cn", "wxwang.iris@gmail.com", "ygj@fudan.edu.cn", "xyxue@fudan.edu.cn"], "keywords": ["CTR", "Attention", "Transformer", "Encoder"], "TL;DR": "DNN and Encoder enhanced FM with bilinear attention and max-pooling for CTR", "abstract": "Click Through Rate (CTR) prediction is a critical task in industrial applications, especially for online social and commerce applications. It is challenging to find a proper way to automatically discover the effective cross features in CTR tasks. We propose a novel model for CTR tasks, called Deep neural networks with Encoder enhanced Factorization Machine (DeepEnFM). Instead of learning the cross features directly, DeepEnFM adopts the Transformer encoder as a backbone to align the feature embeddings with the clues of other fields. The embeddings generated from encoder are beneficial for the further feature interactions. Particularly, DeepEnFM utilizes a bilinear approach to generate different similarity functions with respect to different field pairs. Furthermore, the max-pooling method makes DeepEnFM feasible to capture both the supplementary and suppressing information among different attention heads. Our model is validated on the Criteo and Avazu datasets, and achieves state-of-art performance.", "pdf": "/pdf/172f7a073710dcf32034814f99bd0b87fa2d863c.pdf", "paperhash": "sun|deepenfm_deep_neural_networks_with_encoder_enhanced_factorization_machine", "original_pdf": "/attachment/430f0df746e86f4641be8a46379ac4d58c1623c1.pdf", "_bibtex": "@misc{\nsun2020deepenfm,\ntitle={DeepEn{\\{}FM{\\}}: Deep neural networks with Encoder enhanced Factorization Machine},\nauthor={Qiang Sun and Zhinan Cheng and Yanwei Fu and Wenxuan Wang and Yu-Gang Jiang and Xiangyang Xue},\nyear={2020},\nurl={https://openreview.net/forum?id=SJlyta4YPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SJlyta4YPS", "replyto": "SJlyta4YPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper654/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper654/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575699100795, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper654/Reviewers"], "noninvitees": [], "tcdate": 1570237749013, "tmdate": 1575699100811, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper654/-/Official_Review"}}}, {"id": "SJeEerv-5B", "original": null, "number": 3, "cdate": 1572070636102, "ddate": null, "tcdate": 1572070636102, "tmdate": 1572972568634, "tddate": null, "forum": "SJlyta4YPS", "replyto": "SJlyta4YPS", "invitation": "ICLR.cc/2020/Conference/Paper654/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper applies Multi-Head Self-Attention (MHSA) to a CTR prediction model with some small changes. The empirical results on two public datasets show it improves performance over some baselines.\n\nFirst of all, the novelty of the proposed algorithm is limited in that it mainly applies existing mulit-head self-attention. The paper does include some small modifications to MHSA and achieves better performance, such as bi-linear similarity and max-pooling. However, the nature of these changes seems more incremental.\n\nThe experiment section is very detailed and the paper conducts several ablation studies to understand which components contribute the most, which is nice. However, the paper is missing several important baselines, for example, Deep & Cross [1], which makes the results less convincing.\n\nAnother issue with the paper is that it does not control the model capacity when comparing performance. It is usually the case that increasing model capacity leads to better performance. Given that MHSA and bi-linear similarity have increased a lot of model parameters, it is more fair to compare performance across models with similar capacity. In fact, in [1], they show the logloss on Criteo dataset can be as low as 0.4423 when using large enough parameters.\n\nMinor: in the ablation study, it shows head = 1 has the best performance. In this case, why max-pooling is needed? \n\nReference:\n[1] Wang, R., Fu, B., Fu, G. and Wang, M., 2017, August. Deep & cross network for ad click predictions. In Proceedings of the ADKDD'17 (p. 12). ACM."}, "signatures": ["ICLR.cc/2020/Conference/Paper654/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper654/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DeepEnFM: Deep neural networks with Encoder enhanced Factorization Machine", "authors": ["Qiang Sun", "Zhinan Cheng", "Yanwei Fu", "Wenxuan Wang", "Yu-Gang Jiang", "Xiangyang Xue"], "authorids": ["sunqiang85@gmail.com", "zhinancheng.bryan@gmail.com", "yanweifu@fudan.edu.cn", "wxwang.iris@gmail.com", "ygj@fudan.edu.cn", "xyxue@fudan.edu.cn"], "keywords": ["CTR", "Attention", "Transformer", "Encoder"], "TL;DR": "DNN and Encoder enhanced FM with bilinear attention and max-pooling for CTR", "abstract": "Click Through Rate (CTR) prediction is a critical task in industrial applications, especially for online social and commerce applications. It is challenging to find a proper way to automatically discover the effective cross features in CTR tasks. We propose a novel model for CTR tasks, called Deep neural networks with Encoder enhanced Factorization Machine (DeepEnFM). Instead of learning the cross features directly, DeepEnFM adopts the Transformer encoder as a backbone to align the feature embeddings with the clues of other fields. The embeddings generated from encoder are beneficial for the further feature interactions. Particularly, DeepEnFM utilizes a bilinear approach to generate different similarity functions with respect to different field pairs. Furthermore, the max-pooling method makes DeepEnFM feasible to capture both the supplementary and suppressing information among different attention heads. Our model is validated on the Criteo and Avazu datasets, and achieves state-of-art performance.", "pdf": "/pdf/172f7a073710dcf32034814f99bd0b87fa2d863c.pdf", "paperhash": "sun|deepenfm_deep_neural_networks_with_encoder_enhanced_factorization_machine", "original_pdf": "/attachment/430f0df746e86f4641be8a46379ac4d58c1623c1.pdf", "_bibtex": "@misc{\nsun2020deepenfm,\ntitle={DeepEn{\\{}FM{\\}}: Deep neural networks with Encoder enhanced Factorization Machine},\nauthor={Qiang Sun and Zhinan Cheng and Yanwei Fu and Wenxuan Wang and Yu-Gang Jiang and Xiangyang Xue},\nyear={2020},\nurl={https://openreview.net/forum?id=SJlyta4YPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SJlyta4YPS", "replyto": "SJlyta4YPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper654/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper654/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575699100795, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper654/Reviewers"], "noninvitees": [], "tcdate": 1570237749013, "tmdate": 1575699100811, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper654/-/Official_Review"}}}], "count": 8}