{"notes": [{"id": "GTGb3M_KcUl", "original": "ei73Qji5Dhc", "number": 174, "cdate": 1601308028083, "ddate": null, "tcdate": 1601308028083, "tmdate": 1615858207080, "tddate": null, "forum": "GTGb3M_KcUl", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "DynaTune: Dynamic Tensor Program Optimization in Deep Neural Network Compilation", "authorids": ["~Minjia_Zhang1", "t-meli@microsoft.com", "~Chi_Wang3", "mingqli@microsoft.com"], "authors": ["Minjia Zhang", "Menghao Li", "Chi Wang", "Mingqin Li"], "keywords": ["Efficient Deep Learning Inference", "Scalability", "Code Compilation", "Bayesian Inference"], "abstract": "Recently, the DL compiler, together with Learning to Compile has proven to be a powerful technique for optimizing deep learning models. However, existing methods focus on accelerating the convergence speed of the individual tensor operator rather than the convergence speed of the entire model, which results in long optimization time to obtain a desired latency.\n\nIn this paper, we present a new method called DynaTune, which provides significantly faster convergence speed to optimize a DNN model. In particular, we consider a Multi-Armed Bandit (MAB) model for the tensor program optimization problem. We use UCB to handle the decision-making of time-slot-based optimization, and we devise a Bayesian belief model that allows predicting the potential performance gain of each operator with uncertainty quantification, which guides the optimization process. We evaluate and compare DynaTune with the state-of-the-art DL compiler. The experiment results show that DynaTune is 1.2--2.4 times faster to achieve the same optimization quality for a range of models across different hardware architectures. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|dynatune_dynamic_tensor_program_optimization_in_deep_neural_network_compilation", "one-sentence_summary": "We accelerate tensor program optimization by considering it as a multi-armed bandits problem and using Bayesian inference to achieve fast convergence.", "pdf": "/pdf/f2330b850544ed7b0157ff0411638fd7ee8aefc0.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhang2021dynatune,\ntitle={DynaTune: Dynamic Tensor Program Optimization in Deep Neural Network Compilation},\nauthor={Minjia Zhang and Menghao Li and Chi Wang and Mingqin Li},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=GTGb3M_KcUl}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 16, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "WyxAUF5R_p", "original": null, "number": 1, "cdate": 1610040478731, "ddate": null, "tcdate": 1610040478731, "tmdate": 1610474083460, "tddate": null, "forum": "GTGb3M_KcUl", "replyto": "GTGb3M_KcUl", "invitation": "ICLR.cc/2021/Conference/Paper174/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Poster)", "comment": "This paper applies multi-armed bandits to tuning deep learning code optimization. All reviewers agreed that this is an exploratory paper that opens up a new research area. My main criticism is algorithmic. In particular, the paper applies a 20-year old algorithm to a problem with a small number of arms. It is definitely not as impressive as\n\nhttps://papers.nips.cc/paper/2018/file/f33ba15effa5c10e873bf3842afb46a6-Paper.pdf\n\nwho studied a different (but related) problem. The tuning problem in this paper also seems non-stochastic and contextual, while the authors apply a stochastic non-contextual bandit algorithm.\n\nI shared these concerns with the reviewers, who insisted that the application is important enough to justify the acceptance of the paper. I respect their opinion and therefore suggest an acceptance. I encourage the authors to take my comments into account when revising the paper."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DynaTune: Dynamic Tensor Program Optimization in Deep Neural Network Compilation", "authorids": ["~Minjia_Zhang1", "t-meli@microsoft.com", "~Chi_Wang3", "mingqli@microsoft.com"], "authors": ["Minjia Zhang", "Menghao Li", "Chi Wang", "Mingqin Li"], "keywords": ["Efficient Deep Learning Inference", "Scalability", "Code Compilation", "Bayesian Inference"], "abstract": "Recently, the DL compiler, together with Learning to Compile has proven to be a powerful technique for optimizing deep learning models. However, existing methods focus on accelerating the convergence speed of the individual tensor operator rather than the convergence speed of the entire model, which results in long optimization time to obtain a desired latency.\n\nIn this paper, we present a new method called DynaTune, which provides significantly faster convergence speed to optimize a DNN model. In particular, we consider a Multi-Armed Bandit (MAB) model for the tensor program optimization problem. We use UCB to handle the decision-making of time-slot-based optimization, and we devise a Bayesian belief model that allows predicting the potential performance gain of each operator with uncertainty quantification, which guides the optimization process. We evaluate and compare DynaTune with the state-of-the-art DL compiler. The experiment results show that DynaTune is 1.2--2.4 times faster to achieve the same optimization quality for a range of models across different hardware architectures. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|dynatune_dynamic_tensor_program_optimization_in_deep_neural_network_compilation", "one-sentence_summary": "We accelerate tensor program optimization by considering it as a multi-armed bandits problem and using Bayesian inference to achieve fast convergence.", "pdf": "/pdf/f2330b850544ed7b0157ff0411638fd7ee8aefc0.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhang2021dynatune,\ntitle={DynaTune: Dynamic Tensor Program Optimization in Deep Neural Network Compilation},\nauthor={Minjia Zhang and Menghao Li and Chi Wang and Mingqin Li},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=GTGb3M_KcUl}\n}"}, "tags": [], "invitation": {"reply": {"forum": "GTGb3M_KcUl", "replyto": "GTGb3M_KcUl", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040478719, "tmdate": 1610474083444, "id": "ICLR.cc/2021/Conference/Paper174/-/Decision"}}}, {"id": "m1Vs7FffRqT", "original": null, "number": 14, "cdate": 1606181032020, "ddate": null, "tcdate": 1606181032020, "tmdate": 1606181032020, "tddate": null, "forum": "GTGb3M_KcUl", "replyto": "GTGb3M_KcUl", "invitation": "ICLR.cc/2021/Conference/Paper174/-/Official_Comment", "content": {"title": "Response and revision", "comment": "We sincerely thank all reviewers for their positive and constructive comments, time, and effort for improving the paper! Apart from the revision that has already been made to the manuscript, we will incorporate the reviewers' additional suggestions in the final version of the paper. We also very much appreciate all reviewers for sharing unique knowledge and observations, which we believe opens opportunities for our future studies.  "}, "signatures": ["ICLR.cc/2021/Conference/Paper174/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper174/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DynaTune: Dynamic Tensor Program Optimization in Deep Neural Network Compilation", "authorids": ["~Minjia_Zhang1", "t-meli@microsoft.com", "~Chi_Wang3", "mingqli@microsoft.com"], "authors": ["Minjia Zhang", "Menghao Li", "Chi Wang", "Mingqin Li"], "keywords": ["Efficient Deep Learning Inference", "Scalability", "Code Compilation", "Bayesian Inference"], "abstract": "Recently, the DL compiler, together with Learning to Compile has proven to be a powerful technique for optimizing deep learning models. However, existing methods focus on accelerating the convergence speed of the individual tensor operator rather than the convergence speed of the entire model, which results in long optimization time to obtain a desired latency.\n\nIn this paper, we present a new method called DynaTune, which provides significantly faster convergence speed to optimize a DNN model. In particular, we consider a Multi-Armed Bandit (MAB) model for the tensor program optimization problem. We use UCB to handle the decision-making of time-slot-based optimization, and we devise a Bayesian belief model that allows predicting the potential performance gain of each operator with uncertainty quantification, which guides the optimization process. We evaluate and compare DynaTune with the state-of-the-art DL compiler. The experiment results show that DynaTune is 1.2--2.4 times faster to achieve the same optimization quality for a range of models across different hardware architectures. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|dynatune_dynamic_tensor_program_optimization_in_deep_neural_network_compilation", "one-sentence_summary": "We accelerate tensor program optimization by considering it as a multi-armed bandits problem and using Bayesian inference to achieve fast convergence.", "pdf": "/pdf/f2330b850544ed7b0157ff0411638fd7ee8aefc0.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhang2021dynatune,\ntitle={DynaTune: Dynamic Tensor Program Optimization in Deep Neural Network Compilation},\nauthor={Minjia Zhang and Menghao Li and Chi Wang and Mingqin Li},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=GTGb3M_KcUl}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "GTGb3M_KcUl", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper174/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper174/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper174/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper174/Authors|ICLR.cc/2021/Conference/Paper174/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper174/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923873829, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper174/-/Official_Comment"}}}, {"id": "UyMTDbY4wrO", "original": null, "number": 13, "cdate": 1605950958621, "ddate": null, "tcdate": 1605950958621, "tmdate": 1605951431425, "tddate": null, "forum": "GTGb3M_KcUl", "replyto": "509H3QIz0-l", "invitation": "ICLR.cc/2021/Conference/Paper174/-/Official_Comment", "content": {"title": "I raise my score as my concerns are addressed.", "comment": "Hi authors,\n\nThanks for the extensive rebuttal and added experiments. Most of my concerns are addressed.\nI agree that we should not discourage the publication of original ideas that are developed independently and concurrently.\nI think accepting this paper is totally reasonable. The discussion and new techniques are beneficial to the ML+compiler community.\nMy last suggestion is to also mention Ansor as a concurrent work in the main body with a comparison. It is better to summarize their pros and cons in the main body. I think a few sentences are enough, while the experiment figures can be put in the appendix.\n\nI raised my score to 6."}, "signatures": ["ICLR.cc/2021/Conference/Paper174/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper174/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DynaTune: Dynamic Tensor Program Optimization in Deep Neural Network Compilation", "authorids": ["~Minjia_Zhang1", "t-meli@microsoft.com", "~Chi_Wang3", "mingqli@microsoft.com"], "authors": ["Minjia Zhang", "Menghao Li", "Chi Wang", "Mingqin Li"], "keywords": ["Efficient Deep Learning Inference", "Scalability", "Code Compilation", "Bayesian Inference"], "abstract": "Recently, the DL compiler, together with Learning to Compile has proven to be a powerful technique for optimizing deep learning models. However, existing methods focus on accelerating the convergence speed of the individual tensor operator rather than the convergence speed of the entire model, which results in long optimization time to obtain a desired latency.\n\nIn this paper, we present a new method called DynaTune, which provides significantly faster convergence speed to optimize a DNN model. In particular, we consider a Multi-Armed Bandit (MAB) model for the tensor program optimization problem. We use UCB to handle the decision-making of time-slot-based optimization, and we devise a Bayesian belief model that allows predicting the potential performance gain of each operator with uncertainty quantification, which guides the optimization process. We evaluate and compare DynaTune with the state-of-the-art DL compiler. The experiment results show that DynaTune is 1.2--2.4 times faster to achieve the same optimization quality for a range of models across different hardware architectures. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|dynatune_dynamic_tensor_program_optimization_in_deep_neural_network_compilation", "one-sentence_summary": "We accelerate tensor program optimization by considering it as a multi-armed bandits problem and using Bayesian inference to achieve fast convergence.", "pdf": "/pdf/f2330b850544ed7b0157ff0411638fd7ee8aefc0.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhang2021dynatune,\ntitle={DynaTune: Dynamic Tensor Program Optimization in Deep Neural Network Compilation},\nauthor={Minjia Zhang and Menghao Li and Chi Wang and Mingqin Li},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=GTGb3M_KcUl}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "GTGb3M_KcUl", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper174/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper174/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper174/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper174/Authors|ICLR.cc/2021/Conference/Paper174/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper174/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923873829, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper174/-/Official_Comment"}}}, {"id": "udcIN0vsQF7", "original": null, "number": 3, "cdate": 1603900184371, "ddate": null, "tcdate": 1603900184371, "tmdate": 1605950707595, "tddate": null, "forum": "GTGb3M_KcUl", "replyto": "GTGb3M_KcUl", "invitation": "ICLR.cc/2021/Conference/Paper174/-/Official_Review", "content": {"title": "Nice idea. But lacking a comparision with a highly related paper.", "review": "##########################################################################\n\nSummary:\n\nThe paper proposes an algorithm to optimize the auto-tuning time for compiling neural networks.\n It dynamically allocates time to different operators with a multi-armed bandit algorithm and a Bayesian belief model.\nThe evaluation results show the proposed method can achieve significant speedup.\n\n##########################################################################\n\nReasons for score: \n\nI lean to rejection because of some flaws in the paper.\n1. It does not mention a highly related paper that follows the same high-level idea (i.e., dynamically allocating time resource). The section 6 task scheduler in this paper (https://arxiv.org/abs/2006.06762) proposes a heuristic-based algorithm for dynamically allocating time resource. It also predicts room for improvement of each task. It supports user-defined objective functions and utilizes similarity between tasks. The authors should at least include a comparison with this method. The code of this method is merged into the TVM project (https://github.com/apache/incubator-tvm/blob/main/python/tvm/auto_scheduler/task_scheduler.py), which is the baseline of DynaTune.\n2. The formulation of reward can be improved\nIn section 4.1, the authors define the reward as the latency reduction of one operator. However, an operator can appear multiple times in a network. So a weight should be multiplied to the reduction. The weight of an operator is the number of the appearance of it in the network.\n3. Some assumptions can be improved\nIn challenge 2 of section 3, the authors claim \"We note that the optimization of tensor operators is independent of each other\". This is not true because different operators can share a cost model. They can contribute training data to the shared cost model. The shared cost model then influences the search of all operators. By using a shared cost model and transfer learning, the search time can be greatly reduced (as shown in AutoTVM and Ansor), so we cannot say the optimization of different operators is independent. How to model this effect is very challenging.\n\n\n##########################################################################\n\nPros:\n\n- Overall, the observations and ideas are correct. The paper is well-written with necessary background information.\n- The proposed methods are all reasonable\n- The evaluation results are satisfactory and well structured. It correctly evaluates all aspects that I want to know.\n\n##########################################################################\n\nSuggestion for improvement:\n\nThe authors can improve their formulation and add a comparison with the related paper I mentioned.", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper174/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper174/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DynaTune: Dynamic Tensor Program Optimization in Deep Neural Network Compilation", "authorids": ["~Minjia_Zhang1", "t-meli@microsoft.com", "~Chi_Wang3", "mingqli@microsoft.com"], "authors": ["Minjia Zhang", "Menghao Li", "Chi Wang", "Mingqin Li"], "keywords": ["Efficient Deep Learning Inference", "Scalability", "Code Compilation", "Bayesian Inference"], "abstract": "Recently, the DL compiler, together with Learning to Compile has proven to be a powerful technique for optimizing deep learning models. However, existing methods focus on accelerating the convergence speed of the individual tensor operator rather than the convergence speed of the entire model, which results in long optimization time to obtain a desired latency.\n\nIn this paper, we present a new method called DynaTune, which provides significantly faster convergence speed to optimize a DNN model. In particular, we consider a Multi-Armed Bandit (MAB) model for the tensor program optimization problem. We use UCB to handle the decision-making of time-slot-based optimization, and we devise a Bayesian belief model that allows predicting the potential performance gain of each operator with uncertainty quantification, which guides the optimization process. We evaluate and compare DynaTune with the state-of-the-art DL compiler. The experiment results show that DynaTune is 1.2--2.4 times faster to achieve the same optimization quality for a range of models across different hardware architectures. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|dynatune_dynamic_tensor_program_optimization_in_deep_neural_network_compilation", "one-sentence_summary": "We accelerate tensor program optimization by considering it as a multi-armed bandits problem and using Bayesian inference to achieve fast convergence.", "pdf": "/pdf/f2330b850544ed7b0157ff0411638fd7ee8aefc0.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhang2021dynatune,\ntitle={DynaTune: Dynamic Tensor Program Optimization in Deep Neural Network Compilation},\nauthor={Minjia Zhang and Menghao Li and Chi Wang and Mingqin Li},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=GTGb3M_KcUl}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "GTGb3M_KcUl", "replyto": "GTGb3M_KcUl", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper174/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538148791, "tmdate": 1606915794396, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper174/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper174/-/Official_Review"}}}, {"id": "0X8y5uGwq9U", "original": null, "number": 12, "cdate": 1605836971336, "ddate": null, "tcdate": 1605836971336, "tmdate": 1605836971336, "tddate": null, "forum": "GTGb3M_KcUl", "replyto": "c9oH_hWi7Cg", "invitation": "ICLR.cc/2021/Conference/Paper174/-/Official_Comment", "content": {"title": "I maintain my score as my concerns are addressed", "comment": "The authors have addressed my concerns quite extensively. I will maintain my score. Adding elements from the response to the paper (maybe in the supplementary part) will strengthen the paper, in my opinion. "}, "signatures": ["ICLR.cc/2021/Conference/Paper174/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper174/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DynaTune: Dynamic Tensor Program Optimization in Deep Neural Network Compilation", "authorids": ["~Minjia_Zhang1", "t-meli@microsoft.com", "~Chi_Wang3", "mingqli@microsoft.com"], "authors": ["Minjia Zhang", "Menghao Li", "Chi Wang", "Mingqin Li"], "keywords": ["Efficient Deep Learning Inference", "Scalability", "Code Compilation", "Bayesian Inference"], "abstract": "Recently, the DL compiler, together with Learning to Compile has proven to be a powerful technique for optimizing deep learning models. However, existing methods focus on accelerating the convergence speed of the individual tensor operator rather than the convergence speed of the entire model, which results in long optimization time to obtain a desired latency.\n\nIn this paper, we present a new method called DynaTune, which provides significantly faster convergence speed to optimize a DNN model. In particular, we consider a Multi-Armed Bandit (MAB) model for the tensor program optimization problem. We use UCB to handle the decision-making of time-slot-based optimization, and we devise a Bayesian belief model that allows predicting the potential performance gain of each operator with uncertainty quantification, which guides the optimization process. We evaluate and compare DynaTune with the state-of-the-art DL compiler. The experiment results show that DynaTune is 1.2--2.4 times faster to achieve the same optimization quality for a range of models across different hardware architectures. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|dynatune_dynamic_tensor_program_optimization_in_deep_neural_network_compilation", "one-sentence_summary": "We accelerate tensor program optimization by considering it as a multi-armed bandits problem and using Bayesian inference to achieve fast convergence.", "pdf": "/pdf/f2330b850544ed7b0157ff0411638fd7ee8aefc0.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhang2021dynatune,\ntitle={DynaTune: Dynamic Tensor Program Optimization in Deep Neural Network Compilation},\nauthor={Minjia Zhang and Menghao Li and Chi Wang and Mingqin Li},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=GTGb3M_KcUl}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "GTGb3M_KcUl", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper174/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper174/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper174/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper174/Authors|ICLR.cc/2021/Conference/Paper174/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper174/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923873829, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper174/-/Official_Comment"}}}, {"id": "ZIMVPPjMoK-", "original": null, "number": 4, "cdate": 1603931808741, "ddate": null, "tcdate": 1603931808741, "tmdate": 1605818469291, "tddate": null, "forum": "GTGb3M_KcUl", "replyto": "GTGb3M_KcUl", "invitation": "ICLR.cc/2021/Conference/Paper174/-/Official_Review", "content": {"title": "Review of DynaTune", "review": "In this paper, the authors develop DynaTune which achieves faster convergence speed to optimize a DNN model when compared to the state-of-the-art DL compiler, AutoTVM. The key idea is a time-slot-based scheduling method based on UCB-type multi-armed bandit policy. At each time, the scheduler chooses an action to maximize the latency reduction. In practice, A Bayesian belief model via MCMC is used to capture current knowledge of the optimization results to predict future performance, which helps make better decisions and expedites the convergence speed. The idea of using MAB in DL compiler is very interesting. The numerical experiments also demonstrate clear advantage of the proposed DynaTune. My concerns are as follows. \n\n1. In the experiments, it would be more convincing to numerically compare with more advanced DL compiler, e.g., Adams et al. (2019); Chameleon in Ahn et al. (2020). The Chameleon is also a RL based approach for DL compiler.\n\n2. The authors used $C=0.2$ when initial latency of an operator is <1ms, and $C=2$ for all the other cases. Can authors provide more justification on the choice of $C$? Some sensitivity analysis could be helpful. \n\n3. In the end of the first paragraph in Section 5 (line 8 on page 6), the sentence is not finished. \n\n\n\n", "rating": "7: Good paper, accept", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2021/Conference/Paper174/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper174/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DynaTune: Dynamic Tensor Program Optimization in Deep Neural Network Compilation", "authorids": ["~Minjia_Zhang1", "t-meli@microsoft.com", "~Chi_Wang3", "mingqli@microsoft.com"], "authors": ["Minjia Zhang", "Menghao Li", "Chi Wang", "Mingqin Li"], "keywords": ["Efficient Deep Learning Inference", "Scalability", "Code Compilation", "Bayesian Inference"], "abstract": "Recently, the DL compiler, together with Learning to Compile has proven to be a powerful technique for optimizing deep learning models. However, existing methods focus on accelerating the convergence speed of the individual tensor operator rather than the convergence speed of the entire model, which results in long optimization time to obtain a desired latency.\n\nIn this paper, we present a new method called DynaTune, which provides significantly faster convergence speed to optimize a DNN model. In particular, we consider a Multi-Armed Bandit (MAB) model for the tensor program optimization problem. We use UCB to handle the decision-making of time-slot-based optimization, and we devise a Bayesian belief model that allows predicting the potential performance gain of each operator with uncertainty quantification, which guides the optimization process. We evaluate and compare DynaTune with the state-of-the-art DL compiler. The experiment results show that DynaTune is 1.2--2.4 times faster to achieve the same optimization quality for a range of models across different hardware architectures. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|dynatune_dynamic_tensor_program_optimization_in_deep_neural_network_compilation", "one-sentence_summary": "We accelerate tensor program optimization by considering it as a multi-armed bandits problem and using Bayesian inference to achieve fast convergence.", "pdf": "/pdf/f2330b850544ed7b0157ff0411638fd7ee8aefc0.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhang2021dynatune,\ntitle={DynaTune: Dynamic Tensor Program Optimization in Deep Neural Network Compilation},\nauthor={Minjia Zhang and Menghao Li and Chi Wang and Mingqin Li},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=GTGb3M_KcUl}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "GTGb3M_KcUl", "replyto": "GTGb3M_KcUl", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper174/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538148791, "tmdate": 1606915794396, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper174/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper174/-/Official_Review"}}}, {"id": "wewmKr5XB1z", "original": null, "number": 11, "cdate": 1605818452345, "ddate": null, "tcdate": 1605818452345, "tmdate": 1605818452345, "tddate": null, "forum": "GTGb3M_KcUl", "replyto": "n6SA-HD5j4c", "invitation": "ICLR.cc/2021/Conference/Paper174/-/Official_Comment", "content": {"title": "My comments are fully addressed", "comment": "Thanks the authors for the detailed response. My comments are fully addressed. So I increased my score."}, "signatures": ["ICLR.cc/2021/Conference/Paper174/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper174/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DynaTune: Dynamic Tensor Program Optimization in Deep Neural Network Compilation", "authorids": ["~Minjia_Zhang1", "t-meli@microsoft.com", "~Chi_Wang3", "mingqli@microsoft.com"], "authors": ["Minjia Zhang", "Menghao Li", "Chi Wang", "Mingqin Li"], "keywords": ["Efficient Deep Learning Inference", "Scalability", "Code Compilation", "Bayesian Inference"], "abstract": "Recently, the DL compiler, together with Learning to Compile has proven to be a powerful technique for optimizing deep learning models. However, existing methods focus on accelerating the convergence speed of the individual tensor operator rather than the convergence speed of the entire model, which results in long optimization time to obtain a desired latency.\n\nIn this paper, we present a new method called DynaTune, which provides significantly faster convergence speed to optimize a DNN model. In particular, we consider a Multi-Armed Bandit (MAB) model for the tensor program optimization problem. We use UCB to handle the decision-making of time-slot-based optimization, and we devise a Bayesian belief model that allows predicting the potential performance gain of each operator with uncertainty quantification, which guides the optimization process. We evaluate and compare DynaTune with the state-of-the-art DL compiler. The experiment results show that DynaTune is 1.2--2.4 times faster to achieve the same optimization quality for a range of models across different hardware architectures. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|dynatune_dynamic_tensor_program_optimization_in_deep_neural_network_compilation", "one-sentence_summary": "We accelerate tensor program optimization by considering it as a multi-armed bandits problem and using Bayesian inference to achieve fast convergence.", "pdf": "/pdf/f2330b850544ed7b0157ff0411638fd7ee8aefc0.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhang2021dynatune,\ntitle={DynaTune: Dynamic Tensor Program Optimization in Deep Neural Network Compilation},\nauthor={Minjia Zhang and Menghao Li and Chi Wang and Mingqin Li},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=GTGb3M_KcUl}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "GTGb3M_KcUl", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper174/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper174/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper174/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper174/Authors|ICLR.cc/2021/Conference/Paper174/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper174/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923873829, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper174/-/Official_Comment"}}}, {"id": "Fn3VGKYoXNI", "original": null, "number": 10, "cdate": 1605771017085, "ddate": null, "tcdate": 1605771017085, "tmdate": 1605771017085, "tddate": null, "forum": "GTGb3M_KcUl", "replyto": "BXuDO2r6I4m", "invitation": "ICLR.cc/2021/Conference/Paper174/-/Official_Comment", "content": {"title": "Comments", "comment": "Regarding Q2, from experiences in running numerous experiments, there were situations where running many steps of GA (beyond 800-1000 steps, something like 3000-4000 steps) would yield better results than running SA with XGBoost. This seemed (my speculation) to be due to some overfitting issue of the XGBoost cost models. As this was the case, I am hoping that using DynaTune to enable more steps of optimization may present some interesting results. I believe such results would be interesting addition to the paper.\n\nOverall, I am satisfied with the authors' response. I really hope to see this paper in the conference as I believe this paper presents an interesting application of ML (Bandits) in the context of Systems that may have practical impact in ML compilers."}, "signatures": ["ICLR.cc/2021/Conference/Paper174/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper174/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DynaTune: Dynamic Tensor Program Optimization in Deep Neural Network Compilation", "authorids": ["~Minjia_Zhang1", "t-meli@microsoft.com", "~Chi_Wang3", "mingqli@microsoft.com"], "authors": ["Minjia Zhang", "Menghao Li", "Chi Wang", "Mingqin Li"], "keywords": ["Efficient Deep Learning Inference", "Scalability", "Code Compilation", "Bayesian Inference"], "abstract": "Recently, the DL compiler, together with Learning to Compile has proven to be a powerful technique for optimizing deep learning models. However, existing methods focus on accelerating the convergence speed of the individual tensor operator rather than the convergence speed of the entire model, which results in long optimization time to obtain a desired latency.\n\nIn this paper, we present a new method called DynaTune, which provides significantly faster convergence speed to optimize a DNN model. In particular, we consider a Multi-Armed Bandit (MAB) model for the tensor program optimization problem. We use UCB to handle the decision-making of time-slot-based optimization, and we devise a Bayesian belief model that allows predicting the potential performance gain of each operator with uncertainty quantification, which guides the optimization process. We evaluate and compare DynaTune with the state-of-the-art DL compiler. The experiment results show that DynaTune is 1.2--2.4 times faster to achieve the same optimization quality for a range of models across different hardware architectures. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|dynatune_dynamic_tensor_program_optimization_in_deep_neural_network_compilation", "one-sentence_summary": "We accelerate tensor program optimization by considering it as a multi-armed bandits problem and using Bayesian inference to achieve fast convergence.", "pdf": "/pdf/f2330b850544ed7b0157ff0411638fd7ee8aefc0.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhang2021dynatune,\ntitle={DynaTune: Dynamic Tensor Program Optimization in Deep Neural Network Compilation},\nauthor={Minjia Zhang and Menghao Li and Chi Wang and Mingqin Li},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=GTGb3M_KcUl}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "GTGb3M_KcUl", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper174/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper174/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper174/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper174/Authors|ICLR.cc/2021/Conference/Paper174/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper174/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923873829, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper174/-/Official_Comment"}}}, {"id": "BXuDO2r6I4m", "original": null, "number": 8, "cdate": 1605764966256, "ddate": null, "tcdate": 1605764966256, "tmdate": 1605764966256, "tddate": null, "forum": "GTGb3M_KcUl", "replyto": "W2Eo_6827zB", "invitation": "ICLR.cc/2021/Conference/Paper174/-/Official_Comment", "content": {"title": "Response to Reviewer #2", "comment": "Thank you for your positive feedback! We address the comments and feedback you provided below.\n\n**Q1: How does the end-to-end latency compare to [1]?**\n\nA1: NeoCPU [1] studies how to get the best performance of convolution operators on CPU. DynaTune is complementary to [1], as it proposes a general technique to accelerate the optimization speed of multiple tensor operators by better allocating time resources. It also looks beyond CNN workloads and CPU and targets more diverse network structures (e.g., Transformer networks) and heterogeneous hardware (e.g., CPU and GPU). Moreover, both NeoCPU and DynaTune are built on top of the open source TVM project. The optimizations in NeoCPU, such as data layout transformation and vectorization, had been merged into the target-dependent passes of TVM, whereas the optimization of DynaTune is at the Learning-to-Compile stage (please kindly refer to the background section and Figure 1 to see their place in the DL compilation pipeline). The end-to-end latency of TVM is already a result of applying the optimizations from [1]. Figure 10 in the manuscript reports the end-to-end inference time comparison between TVM and DynaTune for SqueezeNet and ResNet on CPU. DynaTune is **16\\% and 50\\% faster** than TVM on SqueezeNet and ResNet, respectively. \n\n**Q2: How are the results affected when put together with GA and Random Search for the actual optimization (line 6, 9 of Algorithm 1)**\n\nA2: We thank the reviewer for the comment about the synergy between DynaTune and alternative optimization methods. We use simulated annealing (SA) and XGBoost based statistical cost model to optimize individual operators. We choose SA + XGBoost in our experiments because [2] shows that SA + XGBoost is more effective than random search and genetic algorithms for tensor optimization (shown in Figure 4 in [2]). For this reason, SA + XGBoost is chosen as the default optimization scheme in AutoTVM. However, the reviewer is correct that DynaTune can be combined with alternative mechanisms beyond SA + XGBoost for the actual optimization to achieve even better performance. This would make an interesting future study.\n\n[1] Liu et al. \"Optimizing CNN Model Inference on CPUs\", https://arxiv.org/abs/1809.02697\n[2] Chen et al., \"Learning to Optimize Tensor Programs\", https://arxiv.org/pdf/1805.08166.pdf\n\nWe hope our response has mostly addressed your questions. We are glad to continue the discussion to address any remaining questions you may have."}, "signatures": ["ICLR.cc/2021/Conference/Paper174/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper174/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DynaTune: Dynamic Tensor Program Optimization in Deep Neural Network Compilation", "authorids": ["~Minjia_Zhang1", "t-meli@microsoft.com", "~Chi_Wang3", "mingqli@microsoft.com"], "authors": ["Minjia Zhang", "Menghao Li", "Chi Wang", "Mingqin Li"], "keywords": ["Efficient Deep Learning Inference", "Scalability", "Code Compilation", "Bayesian Inference"], "abstract": "Recently, the DL compiler, together with Learning to Compile has proven to be a powerful technique for optimizing deep learning models. However, existing methods focus on accelerating the convergence speed of the individual tensor operator rather than the convergence speed of the entire model, which results in long optimization time to obtain a desired latency.\n\nIn this paper, we present a new method called DynaTune, which provides significantly faster convergence speed to optimize a DNN model. In particular, we consider a Multi-Armed Bandit (MAB) model for the tensor program optimization problem. We use UCB to handle the decision-making of time-slot-based optimization, and we devise a Bayesian belief model that allows predicting the potential performance gain of each operator with uncertainty quantification, which guides the optimization process. We evaluate and compare DynaTune with the state-of-the-art DL compiler. The experiment results show that DynaTune is 1.2--2.4 times faster to achieve the same optimization quality for a range of models across different hardware architectures. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|dynatune_dynamic_tensor_program_optimization_in_deep_neural_network_compilation", "one-sentence_summary": "We accelerate tensor program optimization by considering it as a multi-armed bandits problem and using Bayesian inference to achieve fast convergence.", "pdf": "/pdf/f2330b850544ed7b0157ff0411638fd7ee8aefc0.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhang2021dynatune,\ntitle={DynaTune: Dynamic Tensor Program Optimization in Deep Neural Network Compilation},\nauthor={Minjia Zhang and Menghao Li and Chi Wang and Mingqin Li},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=GTGb3M_KcUl}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "GTGb3M_KcUl", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper174/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper174/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper174/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper174/Authors|ICLR.cc/2021/Conference/Paper174/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper174/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923873829, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper174/-/Official_Comment"}}}, {"id": "c9oH_hWi7Cg", "original": null, "number": 7, "cdate": 1605764639903, "ddate": null, "tcdate": 1605764639903, "tmdate": 1605764750609, "tddate": null, "forum": "GTGb3M_KcUl", "replyto": "icXZ-7Mb49n", "invitation": "ICLR.cc/2021/Conference/Paper174/-/Official_Comment", "content": {"title": "Response to Reviewer #3 -- Part 2", "comment": "**Q3: The baseline AutoTVM (Chen et al., 2018b) is somewhat old. However, I am not sure if a better baseline is available.**\n\nA3: We understand the reviewer's concern. There have been very few attempts to utilize a scheduler to simultaneously optimize multiple tensor operators in deep neural networks. To relax the reviewer's concern, we added additional experiments on Chameleon from Ahn et al. (https://arxiv.org/pdf/2001.08743.pdf) in Appendix C. Chameleon, which we cited and discussed in the related work section, is a recent work that uses reinforcement learning and adaptive sampling to accelerate the search space exploration of a tensor operator, but it still optimizes one operator at a time. Therefore, although the optimization speed of each operator has been improved, the overall convergence speed is still bounded by the least optimized tensor operators. We did not compare with Chameleon in the original draft because we considered DynaTune as a technique that was compatible and complementary with existing DL compilation techniques that focused on optimizing individual operators. \n\nWe use the source code of Chameleon from https://github.com/anony-sub/chameleon. We follow the provided instructions and evaluate Chameleon on the same set of models: ResNet-18 and SqueezeNet on CPU, VGG and Transformer on GPU. Figure 19c and Figure 19d in Appendix C of the revised manuscript show that although Chameleon is faster than the baseline AutoTVM on VGG and Transformer on GPU, it is much slower than DynaTune to converge to the lowest latency. When running on CPU, we observe that Chameleon is surprisingly slower than the baseline AutoTVM on ResNet-18 (Figure 19a) and SqueezeNet (Figure 19b). By checking the results in the Chameleon paper, we find that Chameleon seems to have only been evaluated on GPU. We analyze the performance and find that the RL optimizer in Chameleon adds a non-trivial amount of overhead than the default optimizer (simulated annealing + XGBoost cost model) in AutoTVM on CPU. As a result, although Chameleon reduces the hardware measurement cost and the number of iterations to find the best configuration, its optimization time on CPU is longer than the baseline AutoTVM and is much slower than DynaTune on CPU.  Overall, DynaTune is **1.4--4.7 times faster** than Chameleon to reach the same latency. Although Dynatune is faster than Chameleon, we want to point out that DynaTune can be combined with Chameleon to achieve better performance, at least on GPU. \n\nWe hope we have addressed most of your concerns. Please feel free to let us know if you have any other questions."}, "signatures": ["ICLR.cc/2021/Conference/Paper174/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper174/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DynaTune: Dynamic Tensor Program Optimization in Deep Neural Network Compilation", "authorids": ["~Minjia_Zhang1", "t-meli@microsoft.com", "~Chi_Wang3", "mingqli@microsoft.com"], "authors": ["Minjia Zhang", "Menghao Li", "Chi Wang", "Mingqin Li"], "keywords": ["Efficient Deep Learning Inference", "Scalability", "Code Compilation", "Bayesian Inference"], "abstract": "Recently, the DL compiler, together with Learning to Compile has proven to be a powerful technique for optimizing deep learning models. However, existing methods focus on accelerating the convergence speed of the individual tensor operator rather than the convergence speed of the entire model, which results in long optimization time to obtain a desired latency.\n\nIn this paper, we present a new method called DynaTune, which provides significantly faster convergence speed to optimize a DNN model. In particular, we consider a Multi-Armed Bandit (MAB) model for the tensor program optimization problem. We use UCB to handle the decision-making of time-slot-based optimization, and we devise a Bayesian belief model that allows predicting the potential performance gain of each operator with uncertainty quantification, which guides the optimization process. We evaluate and compare DynaTune with the state-of-the-art DL compiler. The experiment results show that DynaTune is 1.2--2.4 times faster to achieve the same optimization quality for a range of models across different hardware architectures. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|dynatune_dynamic_tensor_program_optimization_in_deep_neural_network_compilation", "one-sentence_summary": "We accelerate tensor program optimization by considering it as a multi-armed bandits problem and using Bayesian inference to achieve fast convergence.", "pdf": "/pdf/f2330b850544ed7b0157ff0411638fd7ee8aefc0.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhang2021dynatune,\ntitle={DynaTune: Dynamic Tensor Program Optimization in Deep Neural Network Compilation},\nauthor={Minjia Zhang and Menghao Li and Chi Wang and Mingqin Li},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=GTGb3M_KcUl}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "GTGb3M_KcUl", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper174/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper174/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper174/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper174/Authors|ICLR.cc/2021/Conference/Paper174/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper174/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923873829, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper174/-/Official_Comment"}}}, {"id": "icXZ-7Mb49n", "original": null, "number": 6, "cdate": 1605764606956, "ddate": null, "tcdate": 1605764606956, "tmdate": 1605764606956, "tddate": null, "forum": "GTGb3M_KcUl", "replyto": "PD9kK6AM66M", "invitation": "ICLR.cc/2021/Conference/Paper174/-/Official_Comment", "content": {"title": "Response to Reviewer #3 -- Part 1", "comment": "We thank Reviewer#3 very much for the positive feedback and for highlighting the significance of our work. \n\n**Q1: How Fig 2,3,4 are obtained is not clear to me. Some pointers to empirical studies or the methodology used are necessary.**\n\nA1: Here, we would like to address the reviewer's concern by providing the details on how Figure 2,3,4 are obtained with some pointers. Figure 2 shows the analysis results of 18 tunable operators in the VGG model on GPU. The amount of computation of each operator is measured as the floating-point operations (FLOPs), which can often be calculated based on the shape and dimensionality of an operator, e.g., a matrix-multiplication [M, K] x [K, N] has 2 x M x K x N floating-point operations. For FLOPs calculation of Conv2D, the reviewer can refer to A.1 in https://arxiv.org/pdf/1611.06440.pdf. The compilation framework TVM we use automatically calculates FLOPs for each operator. The \"optimization gain\" is calculated as the reduction of wall-clock time from each operator, and the \"optimization cost\" is calculated as the wall-clock time spent to obtain the optimized latency. They are normalized by the total latency reduction or optimization time in Figure 2.\n\nFigure 3 shows the code transformation space of a Conv2D operator in ResNet-18 on CPU. In this case, the performance of this operator varies based on the tile size (a code optimization technique) along the input channel and output channel while having other knobs fixed. The knobs control various aspects of the optimization and determine whether the code (1) fully utilizes the internal parallelism within processors, (2) uses the shared memory wisely, and (3) maximizes data locality. A summary of the knobs can be found in Table 1 in https://arxiv.org/pdf/2001.08743.pdf. \n\nFigure 4 plots the optimization curves of all operators in ResNet-18 using AutoTVM on Nvidia Tesla P100 GPU. The x-axis represents the number of optimization iterations, and the y-axis represents the best-seen GFLOPS (Giga Floating Point Operations Per Second) at each iteration step of a task. \n\n**Q2: The details behind the variability estimation using the MCMC method is also not clearly written.**\n\nA2: We thank the reviewer's comment. Appendix B has a more detailed analysis of the MCMC method. Figure 18 plots the time series of the parameters in the Markov chains to assess the behavior of our MCMC sampling. We also report the mean acceptance fraction of the ensemble MCMC sampler, which indicates that our MCMC sampler has generated a sufficient number of parameter samples that have gained good information about the underlying distribution. We can provide more analysis results per the reviewer's requests. "}, "signatures": ["ICLR.cc/2021/Conference/Paper174/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper174/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DynaTune: Dynamic Tensor Program Optimization in Deep Neural Network Compilation", "authorids": ["~Minjia_Zhang1", "t-meli@microsoft.com", "~Chi_Wang3", "mingqli@microsoft.com"], "authors": ["Minjia Zhang", "Menghao Li", "Chi Wang", "Mingqin Li"], "keywords": ["Efficient Deep Learning Inference", "Scalability", "Code Compilation", "Bayesian Inference"], "abstract": "Recently, the DL compiler, together with Learning to Compile has proven to be a powerful technique for optimizing deep learning models. However, existing methods focus on accelerating the convergence speed of the individual tensor operator rather than the convergence speed of the entire model, which results in long optimization time to obtain a desired latency.\n\nIn this paper, we present a new method called DynaTune, which provides significantly faster convergence speed to optimize a DNN model. In particular, we consider a Multi-Armed Bandit (MAB) model for the tensor program optimization problem. We use UCB to handle the decision-making of time-slot-based optimization, and we devise a Bayesian belief model that allows predicting the potential performance gain of each operator with uncertainty quantification, which guides the optimization process. We evaluate and compare DynaTune with the state-of-the-art DL compiler. The experiment results show that DynaTune is 1.2--2.4 times faster to achieve the same optimization quality for a range of models across different hardware architectures. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|dynatune_dynamic_tensor_program_optimization_in_deep_neural_network_compilation", "one-sentence_summary": "We accelerate tensor program optimization by considering it as a multi-armed bandits problem and using Bayesian inference to achieve fast convergence.", "pdf": "/pdf/f2330b850544ed7b0157ff0411638fd7ee8aefc0.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhang2021dynatune,\ntitle={DynaTune: Dynamic Tensor Program Optimization in Deep Neural Network Compilation},\nauthor={Minjia Zhang and Menghao Li and Chi Wang and Mingqin Li},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=GTGb3M_KcUl}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "GTGb3M_KcUl", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper174/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper174/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper174/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper174/Authors|ICLR.cc/2021/Conference/Paper174/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper174/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923873829, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper174/-/Official_Comment"}}}, {"id": "wYY0ZdrdlMs", "original": null, "number": 5, "cdate": 1605764109597, "ddate": null, "tcdate": 1605764109597, "tmdate": 1605764109597, "tddate": null, "forum": "GTGb3M_KcUl", "replyto": "509H3QIz0-l", "invitation": "ICLR.cc/2021/Conference/Paper174/-/Official_Comment", "content": {"title": "Response to Reviewer #1 -- Part 2", "comment": "**Q2: The formulation of reward can be improved in section 4.1**\n\nA2: We appreciate the reviewer's comment. Reusing the same optimization plan for operators that have the same shape is an implementation-level detail, and we believe our main results and conclusion still hold against the multi-tensor-operator optimization problem we formulate in Section 4.1. That being said, this is a valid design point in practice, and we may obtain even better end-to-end latency by incorporating this optimization. Moreover, this optimization should also be easy to incorporate in our design, e.g., by multiplying the point estimate of the future latency reduction in the reward function with a weight that represents the times of the corresponding operator appears in the network. We will consider incorporating it.\n\n**Q3: \"Some assumptions can be improved in challenge 2 of section 3\".**\n\nA3: Regarding the \"dependency among tensor operators\", we revised the text in Section 3 to be more accurate. From an execution point of view, the tuning of each tensor operator is independent of each other. From the transfer learning and collaborative optimization point of view, there are indeed opportunities to exploit task similarities/dependencies to accelerate the convergence speed. For example, the paper the reviewer shared with us exploits structure similarities among tensor operators to predict the expected latency reduction. However, as the reviewer also pointed out, how to model this effect is very challenging, especially in a theoretically grounded way. We would like to discuss these and explore them as future work.\n\nWe hope our response has addressed most of your concerns. We are glad to continue discussion to address any other questions you may have.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper174/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper174/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DynaTune: Dynamic Tensor Program Optimization in Deep Neural Network Compilation", "authorids": ["~Minjia_Zhang1", "t-meli@microsoft.com", "~Chi_Wang3", "mingqli@microsoft.com"], "authors": ["Minjia Zhang", "Menghao Li", "Chi Wang", "Mingqin Li"], "keywords": ["Efficient Deep Learning Inference", "Scalability", "Code Compilation", "Bayesian Inference"], "abstract": "Recently, the DL compiler, together with Learning to Compile has proven to be a powerful technique for optimizing deep learning models. However, existing methods focus on accelerating the convergence speed of the individual tensor operator rather than the convergence speed of the entire model, which results in long optimization time to obtain a desired latency.\n\nIn this paper, we present a new method called DynaTune, which provides significantly faster convergence speed to optimize a DNN model. In particular, we consider a Multi-Armed Bandit (MAB) model for the tensor program optimization problem. We use UCB to handle the decision-making of time-slot-based optimization, and we devise a Bayesian belief model that allows predicting the potential performance gain of each operator with uncertainty quantification, which guides the optimization process. We evaluate and compare DynaTune with the state-of-the-art DL compiler. The experiment results show that DynaTune is 1.2--2.4 times faster to achieve the same optimization quality for a range of models across different hardware architectures. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|dynatune_dynamic_tensor_program_optimization_in_deep_neural_network_compilation", "one-sentence_summary": "We accelerate tensor program optimization by considering it as a multi-armed bandits problem and using Bayesian inference to achieve fast convergence.", "pdf": "/pdf/f2330b850544ed7b0157ff0411638fd7ee8aefc0.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhang2021dynatune,\ntitle={DynaTune: Dynamic Tensor Program Optimization in Deep Neural Network Compilation},\nauthor={Minjia Zhang and Menghao Li and Chi Wang and Mingqin Li},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=GTGb3M_KcUl}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "GTGb3M_KcUl", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper174/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper174/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper174/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper174/Authors|ICLR.cc/2021/Conference/Paper174/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper174/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923873829, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper174/-/Official_Comment"}}}, {"id": "509H3QIz0-l", "original": null, "number": 3, "cdate": 1605763935942, "ddate": null, "tcdate": 1605763935942, "tmdate": 1605763935942, "tddate": null, "forum": "GTGb3M_KcUl", "replyto": "udcIN0vsQF7", "invitation": "ICLR.cc/2021/Conference/Paper174/-/Official_Comment", "content": {"title": "Response to Reviewer #1 -- Part 1 (On lacking a comparison with a highly related paper)", "comment": "We thank Reviewer#1 very much for the insightful comments. Our detailed response is listed below.\n\n**Q1: \"Lacking a comparison with a highly related paper.\"**\n\nA1: We appreciate the reviewer's comments. It seems the reviewer's main concern is the lack of comparisons against a method from Zheng et al., recently published in OSDI 2020. OSDI'20 is just held on November 4-6, which is just a few days before today. We thank the reviewer for sharing this latest news, and we will cite it. But we would like you to reconsider using that reason to reject our paper, because the ICLR reviewer instructions (https://iclr.cc/Conferences/2021/ReviewerGuide#faq) explicitly mention that \"Authors are encouraged to cite and discuss all relevant papers, but they may be excused for not knowing about **papers not published in peer-reviewed conference proceedings or journals**.\" Now, we check the code for that paper on Github suggested by the reviewer: \\url{https://github.com/apache/incubator-tvm/commits/main/python/tvm/auto_scheduler/task_scheduler.py}. The commit history shows that it was checked in on **October 18th**. That was **after** this manuscript was submitted to ICLR (October 2nd). \n\nThat being said, to relax the reviewer's concern, we worked on providing a comparison result with that approach. We uploaded a revised version of the manuscript, in which we added the comparison results to Appendix D. However, we hope to reach the consensus that the evaluation of the merit of this manuscript should not be based on that new experiment. For the evaluation, we use the implementation from https://github.com/apache/incubator-tvm/blob/main/python/tvm/auto_scheduler/task_scheduler.py. The original Ansor paper does not seem to describe how to set the hyperparameters for its scheduler, so we use the default hyperparameters $\\alpha=0.2, \\beta=2$, backward_window_size=3 in the code. We use the same evaluation methodology as ours by performing five independent runs of each configuration with different random seeds and reporting the median together with a 95\\% confidence interval. Figure 20a-20d in Appendix D of the revised manuscript show that both DynaTune and the Ansor scheduler outperform the baseline AutoTVM by a large margin. This is expected, because both approaches accelerate the convergence of model optimization through dynamic optimization. We also added Figure 21a-21d to show a more detailed comparison between DynaTune and Ansor. Overall, Ansor seems to reduce the latency faster in the beginning, but DynaTune can always catch up and is often quicker to reach the lowest latency. For example, DynaTune achieves a faster convergence to reach the lowest latency than Ansor on ResNet-18, VGG, and Transformer. \n\nAfter reading the Ansor paper in more detail, we find that although the Ansor scheduler from Zheng et al. contains a similar high-level idea of \"dynamically allocating time resources to different tasks\", the exact mechanism on how to allocate resources is very different from ours. The Ansor task scheduler decides which task to optimize based on a heuristic score, which is a weighted sum between the latency reduction rate of a task in a recent small time window and an expected latency reduction in the future based on task similarity information. The estimation of the score not only requires defining and heuristically adjusting similarity groups but also requires two hyperparameters $\\alpha$ and $\\beta$ to control the weight to decide which estimations to trust more. However, it is not immediately clear how such weights should be set and how to adapt them to different models. For example, the default hyperparameter settings may cause the scheduling to be overly greedy (i.e., getting stuck at a local optimum), which may explain why Ansor converges faster in the beginning but is slower to reach the best latency towards the end. In contrast, we use a Bayesian belief model to predict how much expected latency reduction from each task in the next time slot, and all the free parameters in our belief model are taken care of by MCMC sampling, which gives our method some plausible advantage in terms of simplicity. Furthermore, our approach takes uncertainty quantification into account, which presumably helps the search escape from local optimum. Apart from these major differences, our manuscript also exclusively provides detailed analysis on the issue of improportional optimization cost/gain (Fig 2), the characteristics of the code transformation space (Fig 3), the shape and pattern of DL code optimization curves (Fig 4), and the connection between tensor compilation and MABs, which we believe would be helpful to provide insight to the community as to why dynamic tensor compilation is needed and what its challenge is. \n\nGiven these facts, in our humble opinion, the reviewer may want to reconsider the position on the paper and not discourage the publication of original ideas that have been developed independently and concurrently. "}, "signatures": ["ICLR.cc/2021/Conference/Paper174/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper174/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DynaTune: Dynamic Tensor Program Optimization in Deep Neural Network Compilation", "authorids": ["~Minjia_Zhang1", "t-meli@microsoft.com", "~Chi_Wang3", "mingqli@microsoft.com"], "authors": ["Minjia Zhang", "Menghao Li", "Chi Wang", "Mingqin Li"], "keywords": ["Efficient Deep Learning Inference", "Scalability", "Code Compilation", "Bayesian Inference"], "abstract": "Recently, the DL compiler, together with Learning to Compile has proven to be a powerful technique for optimizing deep learning models. However, existing methods focus on accelerating the convergence speed of the individual tensor operator rather than the convergence speed of the entire model, which results in long optimization time to obtain a desired latency.\n\nIn this paper, we present a new method called DynaTune, which provides significantly faster convergence speed to optimize a DNN model. In particular, we consider a Multi-Armed Bandit (MAB) model for the tensor program optimization problem. We use UCB to handle the decision-making of time-slot-based optimization, and we devise a Bayesian belief model that allows predicting the potential performance gain of each operator with uncertainty quantification, which guides the optimization process. We evaluate and compare DynaTune with the state-of-the-art DL compiler. The experiment results show that DynaTune is 1.2--2.4 times faster to achieve the same optimization quality for a range of models across different hardware architectures. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|dynatune_dynamic_tensor_program_optimization_in_deep_neural_network_compilation", "one-sentence_summary": "We accelerate tensor program optimization by considering it as a multi-armed bandits problem and using Bayesian inference to achieve fast convergence.", "pdf": "/pdf/f2330b850544ed7b0157ff0411638fd7ee8aefc0.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhang2021dynatune,\ntitle={DynaTune: Dynamic Tensor Program Optimization in Deep Neural Network Compilation},\nauthor={Minjia Zhang and Menghao Li and Chi Wang and Mingqin Li},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=GTGb3M_KcUl}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "GTGb3M_KcUl", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper174/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper174/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper174/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper174/Authors|ICLR.cc/2021/Conference/Paper174/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper174/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923873829, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper174/-/Official_Comment"}}}, {"id": "n6SA-HD5j4c", "original": null, "number": 2, "cdate": 1605763623217, "ddate": null, "tcdate": 1605763623217, "tmdate": 1605763623217, "tddate": null, "forum": "GTGb3M_KcUl", "replyto": "ZIMVPPjMoK-", "invitation": "ICLR.cc/2021/Conference/Paper174/-/Official_Comment", "content": {"title": "Response to Reviewer #4", "comment": "We thank reviewer #4 very much for the detailed summary of our paper and the constructive comments. We provide a detailed response below.\n\n**Q1: In the experiments, it would be more convincing to numerically compare with more advanced DL compiler, e.g., Adams et al. (2019); Chameleon in Ahn et al. (2020). The Chameleon is also a RL based approach for DL compiler.**\n\nA1: We understand the reviewer's concern. To help address the reviewer's concern, we added additional experiments on Chameleon in Appendix C. We choose Chameleon because it is also built on top of AutoTVM and more recent. However, we would like to remark that the dynamic scheduling algorithm in DynaTune is compatible and complementary with existing DL compilation techniques, including Adams et al. (2019) and Chameleon in Ahn et al. (2020), both of which focus on optimizing individual operators.\n\nWe use the source code of Chameleon from https://github.com/anony-sub/chameleon. We follow the provided instructions and evaluate Chameleon on the same set of models: ResNet-18 and SqueezeNet on CPU, VGG, and Transformer on GPU. We add Figures 19a--19d in Appendix C of the revised manuscript (uploaded) to show the comparison results. Figure 19c and Figure 19d show that although Chameleon is faster than the baseline AutoTVM on VGG and Transformer on GPU, it is much slower than DynaTune to converge to the lowest latency. Different from our approach, Chameleon uses reinforcement learning to more efficiently explore the code transformation space of a tensor operator, but it still sequentially optimizes one operator at a time. Therefore, although the optimization speed of each operator has been improved, the overall convergence speed is still bounded by the least optimized tensor operators. In contrast, DynaTune focuses on improving the convergence speed of multi-tensor-operators and dynamically allocates time to improve the overall convergence speed. \n\nThe original Chameleon paper seems to only evaluate its approach on GPU, but doing model inference on CPU is not uncommon. When running on CPU, a bit surprisingly, we observe that Chameleon is slower than the baseline AutoTVM on ResNet-18 (Figure 19a) and SqueezeNet (Figure 19b). We analyze the performance and find that the RL optimizer in Chameleon adds a non-trivial amount of overhead than the default optimizer (simulated annealing + XGBoost cost model) in AutoTVM on CPU. As a result, although Chameleon reduces the hardware measurement cost and the number of iterations to find the best configuration, its optimization time on CPU is longer than the baseline AutoTVM and is much slower than DynaTune on CPU.\n\nOverall, DynaTune is **1.4\u20134.7 times faster** than Chameleon to reach the same latency.  Although Dynatune is faster than Chameleon, we want to point out that DynaTune can be combined with Chameleon to achieve better performance, at least on GPU. \n\n**Q2: The authors used C=0.2 when the initial latency of an operator is <1ms, and C=2 for all the other cases. Can authors provide more justification on the choice of C? Some sensitivity analysis could be helpful.**\n\nA2: C > 0 is a parameter that enables to control the exploration/exploitation trade-off. We choose a default value of C = 2 suggested by the theory in [1], which we find to be robust to different range of latencies. When the initial latency is <1ms, we empirically find that C=0.2 leads to increased performance, which we report, although we can't theoretically verify it.\n\n[1] P. Auer, N. Cesa-Bianchi, and P. Fischer, \"Finite-time Analysis of the Multiarmed Bandit Problem,\" Machine learning, vol. 47, no. 2, pp. 235\u2013256, 2002.\n\n**Q3: \"In the end of the first paragraph in Section 5 (line 8 on page 6), the sentence is not finished.\"**\n\nA3: We thank the reviewer for the comment. We have fixed the text in the revision. \n\nWe hope that our response has mostly addressed the reviewer\u2019s concerns. We are happy to continue a discussion to address any other questions the reviewer may have."}, "signatures": ["ICLR.cc/2021/Conference/Paper174/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper174/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DynaTune: Dynamic Tensor Program Optimization in Deep Neural Network Compilation", "authorids": ["~Minjia_Zhang1", "t-meli@microsoft.com", "~Chi_Wang3", "mingqli@microsoft.com"], "authors": ["Minjia Zhang", "Menghao Li", "Chi Wang", "Mingqin Li"], "keywords": ["Efficient Deep Learning Inference", "Scalability", "Code Compilation", "Bayesian Inference"], "abstract": "Recently, the DL compiler, together with Learning to Compile has proven to be a powerful technique for optimizing deep learning models. However, existing methods focus on accelerating the convergence speed of the individual tensor operator rather than the convergence speed of the entire model, which results in long optimization time to obtain a desired latency.\n\nIn this paper, we present a new method called DynaTune, which provides significantly faster convergence speed to optimize a DNN model. In particular, we consider a Multi-Armed Bandit (MAB) model for the tensor program optimization problem. We use UCB to handle the decision-making of time-slot-based optimization, and we devise a Bayesian belief model that allows predicting the potential performance gain of each operator with uncertainty quantification, which guides the optimization process. We evaluate and compare DynaTune with the state-of-the-art DL compiler. The experiment results show that DynaTune is 1.2--2.4 times faster to achieve the same optimization quality for a range of models across different hardware architectures. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|dynatune_dynamic_tensor_program_optimization_in_deep_neural_network_compilation", "one-sentence_summary": "We accelerate tensor program optimization by considering it as a multi-armed bandits problem and using Bayesian inference to achieve fast convergence.", "pdf": "/pdf/f2330b850544ed7b0157ff0411638fd7ee8aefc0.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhang2021dynatune,\ntitle={DynaTune: Dynamic Tensor Program Optimization in Deep Neural Network Compilation},\nauthor={Minjia Zhang and Menghao Li and Chi Wang and Mingqin Li},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=GTGb3M_KcUl}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "GTGb3M_KcUl", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper174/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper174/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper174/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper174/Authors|ICLR.cc/2021/Conference/Paper174/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper174/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923873829, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper174/-/Official_Comment"}}}, {"id": "PD9kK6AM66M", "original": null, "number": 2, "cdate": 1603852455708, "ddate": null, "tcdate": 1603852455708, "tmdate": 1605024747429, "tddate": null, "forum": "GTGb3M_KcUl", "replyto": "GTGb3M_KcUl", "invitation": "ICLR.cc/2021/Conference/Paper174/-/Official_Review", "content": {"title": "The paper approaches DNN compilation by formulating it as a Multi-armed bandit problem. Interesting challenges to DNN compilation are identified, a sound methodology is proposed and experimentally validated.", "review": "The paper investigates the use of online optimization to improve the performance in Deep Neural Network (DNN) compilation. \nThe three drawbacks of the existing approach/challenges in DNN compilation highlighted in the paper are (1) optimizing individual tensor operator performance instead of that of the entire model, (2) Static scheduling is oblivious to the optimization behavior, (3) extrapolating estimated performance while tuning.  \n\nThe authors model the model as scheduling in a time-slotted system, where they model the average performance (mean reward) of each operator using some performance curve obtained empirically. These curves capture the variability of operators across training duration, which helps with (1) and (2). For tackling (3),  the variability of the performance (noise) of each operator is estimated using some MCMC sampling techniques. Finally, using the mean reward and noise they cast the problem as finding the expected optimal schedule - a sequence of operators to be chosen during the training, as a Multi-armed bandit problem. The authors then design the DynaTune Algorithm base on a well-known UCB algorithm to solve this problem. \n\nThe experiments show DynaTune outperforms AutoTVM (Chen et al., 2018b) the apparent state-of-the-art (see, disclaimer below) in many important DL architectures. \n\n\nPros:\nThe problem of optimizing DL architectures is an important problem, as they are ubiquitous. The three challenges mentioned here sound credible (to an outsider), and in gains of dynamic scheduling is well known. Overall, the problem is well-motivated, and the solution proposed is justified. The experiments seem to give DynaTune an edge over an existing method. \n\n\n\nCons:\n* How Fig 2,3,4 are obtained is not clear to me. Some pointers to empirical studies or the methodology used are necessary. \n\n* The details behind the variability estimation using the MCMC method is also not clearly written. \n\n* The baseline AutoTVM (Chen et al., 2018b) is somewhat old. However, I am not sure if a better baseline is available. \n\n\n\n\n\n\n\nDisclaimer: Although I am familiar with works on Bandits, I am a complete outsider to the topic of DNN compilation.", "rating": "7: Good paper, accept", "confidence": "1: The reviewer's evaluation is an educated guess"}, "signatures": ["ICLR.cc/2021/Conference/Paper174/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper174/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DynaTune: Dynamic Tensor Program Optimization in Deep Neural Network Compilation", "authorids": ["~Minjia_Zhang1", "t-meli@microsoft.com", "~Chi_Wang3", "mingqli@microsoft.com"], "authors": ["Minjia Zhang", "Menghao Li", "Chi Wang", "Mingqin Li"], "keywords": ["Efficient Deep Learning Inference", "Scalability", "Code Compilation", "Bayesian Inference"], "abstract": "Recently, the DL compiler, together with Learning to Compile has proven to be a powerful technique for optimizing deep learning models. However, existing methods focus on accelerating the convergence speed of the individual tensor operator rather than the convergence speed of the entire model, which results in long optimization time to obtain a desired latency.\n\nIn this paper, we present a new method called DynaTune, which provides significantly faster convergence speed to optimize a DNN model. In particular, we consider a Multi-Armed Bandit (MAB) model for the tensor program optimization problem. We use UCB to handle the decision-making of time-slot-based optimization, and we devise a Bayesian belief model that allows predicting the potential performance gain of each operator with uncertainty quantification, which guides the optimization process. We evaluate and compare DynaTune with the state-of-the-art DL compiler. The experiment results show that DynaTune is 1.2--2.4 times faster to achieve the same optimization quality for a range of models across different hardware architectures. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|dynatune_dynamic_tensor_program_optimization_in_deep_neural_network_compilation", "one-sentence_summary": "We accelerate tensor program optimization by considering it as a multi-armed bandits problem and using Bayesian inference to achieve fast convergence.", "pdf": "/pdf/f2330b850544ed7b0157ff0411638fd7ee8aefc0.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhang2021dynatune,\ntitle={DynaTune: Dynamic Tensor Program Optimization in Deep Neural Network Compilation},\nauthor={Minjia Zhang and Menghao Li and Chi Wang and Mingqin Li},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=GTGb3M_KcUl}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "GTGb3M_KcUl", "replyto": "GTGb3M_KcUl", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper174/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538148791, "tmdate": 1606915794396, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper174/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper174/-/Official_Review"}}}, {"id": "W2Eo_6827zB", "original": null, "number": 1, "cdate": 1603785805369, "ddate": null, "tcdate": 1603785805369, "tmdate": 1605024747194, "tddate": null, "forum": "GTGb3M_KcUl", "replyto": "GTGb3M_KcUl", "invitation": "ICLR.cc/2021/Conference/Paper174/-/Official_Review", "content": {"title": "Official Blind Review #2", "review": "The paper tackles the optimization speed of the deep neural networks. The paper models the operator optimization scheduling as a Multi-Armed Bandit problem. DynaTune first selects the operation to optimize in a discrete slot of time using a Bayesian Belief Model, then optimizes the model given the slot of time.\n\nThis interesting paper may help expedite the compilation time of the deep neural networks. As such, this paper tackles on a very important problem. I view this paper as a lineage of papers on \"Expedited Compilation\" and I believe this fresh view of the problem will open up more possibilities for exploration.\n\nModeling uncertainty to understand the gains of optimizing a particular operator is very interesting, and the way it leverages this to schedule the optimization process itself is novel.\n\nIt seems that the approach can be generically combined with different optimization frameworks such as genetic algorithms. It would be nice to have such discussion and would provide interesting insights.\n\nAs a side note, however, there was a paper the tackled the optimization of the inference speed of the end-to-end network [1] which is also applied to TVM. It would be interesting to see how the overall latency achieved by DynaTune compares to this approach.\n\nQuestions:\n1. How does the end-to-end latency compare to [1]?\n2. How are the results affected when put together with GA and Random Search for the actual optimization (line 6, 9 of Algorithm 1)\n\n[1] \"Optimizing CNN Model Inference on CPUs\", ATC 2019", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper174/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper174/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DynaTune: Dynamic Tensor Program Optimization in Deep Neural Network Compilation", "authorids": ["~Minjia_Zhang1", "t-meli@microsoft.com", "~Chi_Wang3", "mingqli@microsoft.com"], "authors": ["Minjia Zhang", "Menghao Li", "Chi Wang", "Mingqin Li"], "keywords": ["Efficient Deep Learning Inference", "Scalability", "Code Compilation", "Bayesian Inference"], "abstract": "Recently, the DL compiler, together with Learning to Compile has proven to be a powerful technique for optimizing deep learning models. However, existing methods focus on accelerating the convergence speed of the individual tensor operator rather than the convergence speed of the entire model, which results in long optimization time to obtain a desired latency.\n\nIn this paper, we present a new method called DynaTune, which provides significantly faster convergence speed to optimize a DNN model. In particular, we consider a Multi-Armed Bandit (MAB) model for the tensor program optimization problem. We use UCB to handle the decision-making of time-slot-based optimization, and we devise a Bayesian belief model that allows predicting the potential performance gain of each operator with uncertainty quantification, which guides the optimization process. We evaluate and compare DynaTune with the state-of-the-art DL compiler. The experiment results show that DynaTune is 1.2--2.4 times faster to achieve the same optimization quality for a range of models across different hardware architectures. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|dynatune_dynamic_tensor_program_optimization_in_deep_neural_network_compilation", "one-sentence_summary": "We accelerate tensor program optimization by considering it as a multi-armed bandits problem and using Bayesian inference to achieve fast convergence.", "pdf": "/pdf/f2330b850544ed7b0157ff0411638fd7ee8aefc0.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhang2021dynatune,\ntitle={DynaTune: Dynamic Tensor Program Optimization in Deep Neural Network Compilation},\nauthor={Minjia Zhang and Menghao Li and Chi Wang and Mingqin Li},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=GTGb3M_KcUl}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "GTGb3M_KcUl", "replyto": "GTGb3M_KcUl", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper174/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538148791, "tmdate": 1606915794396, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper174/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper174/-/Official_Review"}}}], "count": 17}