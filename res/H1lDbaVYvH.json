{"notes": [{"id": "H1lDbaVYvH", "original": "Hkxn-dvIPS", "number": 379, "cdate": 1569438975324, "ddate": null, "tcdate": 1569438975324, "tmdate": 1577168281107, "tddate": null, "forum": "H1lDbaVYvH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["gberseth@gmail.com", "dangengdg@berkeley.edu", "coline.devin@gmail.com", "dinesh.jayaraman123@gmail.com", "cbfinn@eecs.berkeley.edu", "svlevine@eecs.berkeley.edu"], "title": "SMiRL: Surprise Minimizing RL in Entropic Environments", "authors": ["Glen Berseth", "Daniel Geng", "Coline Devin", "Dinesh Jayaraman", "Chelsea Finn", "Sergey Levine"], "pdf": "/pdf/846d7f8d3f89640e31c6ee94de558ed1614b13ee.pdf", "TL;DR": "Learning emergent behavior by minimizing Bayesian surprise with RL in natural environments with entropy.", "abstract": "All living organisms struggle against the forces of nature to carve out niches where\nthey can maintain relative stasis. We propose that such a search for order amidst\nchaos might offer a unifying principle for the emergence of useful behaviors in\nartificial agents. We formalize this idea into an unsupervised reinforcement learning\nmethod called surprise minimizing RL (SMiRL). SMiRL trains an agent with the\nobjective of maximizing the probability of observed states under a model trained on\nall previously seen states. The resulting agents acquire several proactive behaviors\nto seek and maintain stable states such as balancing and damage avoidance, that\nare closely tied to the affordances of the environment and its prevailing sources\nof entropy, such as winds, earthquakes, and other agents.  We demonstrate that\nour surprise minimizing agents can successfully play Tetris, Doom, and control\na  humanoid  to  avoid  falls,  without  any  task-specific  reward  supervision.   We\nfurther show that SMiRL can be used as an unsupervised pre-training objective\nthat substantially accelerates subsequent reward-driven learning", "keywords": ["intrinsic motivation", "reinforcement learning", "unsurpervised RL"], "paperhash": "berseth|smirl_surprise_minimizing_rl_in_entropic_environments", "original_pdf": "/attachment/fa83ce9c6d3f8a2984c4f5c8c1dd933dc9b1e3a5.pdf", "_bibtex": "@misc{\nberseth2020smirl,\ntitle={{\\{}SM{\\}}iRL: Surprise Minimizing {\\{}RL{\\}} in Entropic Environments},\nauthor={Glen Berseth and Daniel Geng and Coline Devin and Dinesh Jayaraman and Chelsea Finn and Sergey Levine},\nyear={2020},\nurl={https://openreview.net/forum?id=H1lDbaVYvH}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "ywTg9HO2d", "original": null, "number": 1, "cdate": 1576798694769, "ddate": null, "tcdate": 1576798694769, "tmdate": 1576800940784, "tddate": null, "forum": "H1lDbaVYvH", "replyto": "H1lDbaVYvH", "invitation": "ICLR.cc/2020/Conference/Paper379/-/Decision", "content": {"decision": "Reject", "comment": "This paper proposes augmentation of the state exploration strategy that is interesting and has a potential to lead to improvement. However, the current presentation makes it difficult to properly assess that. In particular, the way the authors convey both the underlying intuition and its implementation is fairly vague and does not build confidence in the grounding of the underlying methodology.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["gberseth@gmail.com", "dangengdg@berkeley.edu", "coline.devin@gmail.com", "dinesh.jayaraman123@gmail.com", "cbfinn@eecs.berkeley.edu", "svlevine@eecs.berkeley.edu"], "title": "SMiRL: Surprise Minimizing RL in Entropic Environments", "authors": ["Glen Berseth", "Daniel Geng", "Coline Devin", "Dinesh Jayaraman", "Chelsea Finn", "Sergey Levine"], "pdf": "/pdf/846d7f8d3f89640e31c6ee94de558ed1614b13ee.pdf", "TL;DR": "Learning emergent behavior by minimizing Bayesian surprise with RL in natural environments with entropy.", "abstract": "All living organisms struggle against the forces of nature to carve out niches where\nthey can maintain relative stasis. We propose that such a search for order amidst\nchaos might offer a unifying principle for the emergence of useful behaviors in\nartificial agents. We formalize this idea into an unsupervised reinforcement learning\nmethod called surprise minimizing RL (SMiRL). SMiRL trains an agent with the\nobjective of maximizing the probability of observed states under a model trained on\nall previously seen states. The resulting agents acquire several proactive behaviors\nto seek and maintain stable states such as balancing and damage avoidance, that\nare closely tied to the affordances of the environment and its prevailing sources\nof entropy, such as winds, earthquakes, and other agents.  We demonstrate that\nour surprise minimizing agents can successfully play Tetris, Doom, and control\na  humanoid  to  avoid  falls,  without  any  task-specific  reward  supervision.   We\nfurther show that SMiRL can be used as an unsupervised pre-training objective\nthat substantially accelerates subsequent reward-driven learning", "keywords": ["intrinsic motivation", "reinforcement learning", "unsurpervised RL"], "paperhash": "berseth|smirl_surprise_minimizing_rl_in_entropic_environments", "original_pdf": "/attachment/fa83ce9c6d3f8a2984c4f5c8c1dd933dc9b1e3a5.pdf", "_bibtex": "@misc{\nberseth2020smirl,\ntitle={{\\{}SM{\\}}iRL: Surprise Minimizing {\\{}RL{\\}} in Entropic Environments},\nauthor={Glen Berseth and Daniel Geng and Coline Devin and Dinesh Jayaraman and Chelsea Finn and Sergey Levine},\nyear={2020},\nurl={https://openreview.net/forum?id=H1lDbaVYvH}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "H1lDbaVYvH", "replyto": "H1lDbaVYvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795705223, "tmdate": 1576800252951, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper379/-/Decision"}}}, {"id": "Bylmpdc2jB", "original": null, "number": 5, "cdate": 1573853370652, "ddate": null, "tcdate": 1573853370652, "tmdate": 1573860140565, "tddate": null, "forum": "H1lDbaVYvH", "replyto": "Ske_SVOntr", "invitation": "ICLR.cc/2020/Conference/Paper379/-/Official_Comment", "content": {"title": "Response to  the update", "comment": "Thank you for your update and helpful comments. We think there may be a misunderstanding of how we we calculate the augmented state.\n\nThe augmented state includes the state $s_t$, the parameter $\\theta_t$, and the timestep $t$.\nWe define $\\theta_t$ as the sufficient statistic over the history of states within an episode; $\\theta_t$ does not incorporate information from previous episodes. Given two identical states transitions $(s_t, \\theta_t, t),  a_t, (s_{t+1}, \\theta_{t+1}, t+1)$ at the same timestep t in two different episodes, $\\theta_{t+1}$ will be, by definition, updated in the same way. This is because $\\theta_t$ and $t$ are the sufficient statistics for the distribution over states used for the reward. \n\nBecause $\\theta_t$ is independent across episodes and because the augmented state includes the timestep $t$, the transition dynamics T($ (s_{t+1}, \\theta_{t+1}, t+1)| (s_t, \\theta_t, t),  a_t$) are constant over the full training procedure, and the environment is a true MDP. "}, "signatures": ["ICLR.cc/2020/Conference/Paper379/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper379/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["gberseth@gmail.com", "dangengdg@berkeley.edu", "coline.devin@gmail.com", "dinesh.jayaraman123@gmail.com", "cbfinn@eecs.berkeley.edu", "svlevine@eecs.berkeley.edu"], "title": "SMiRL: Surprise Minimizing RL in Entropic Environments", "authors": ["Glen Berseth", "Daniel Geng", "Coline Devin", "Dinesh Jayaraman", "Chelsea Finn", "Sergey Levine"], "pdf": "/pdf/846d7f8d3f89640e31c6ee94de558ed1614b13ee.pdf", "TL;DR": "Learning emergent behavior by minimizing Bayesian surprise with RL in natural environments with entropy.", "abstract": "All living organisms struggle against the forces of nature to carve out niches where\nthey can maintain relative stasis. We propose that such a search for order amidst\nchaos might offer a unifying principle for the emergence of useful behaviors in\nartificial agents. We formalize this idea into an unsupervised reinforcement learning\nmethod called surprise minimizing RL (SMiRL). SMiRL trains an agent with the\nobjective of maximizing the probability of observed states under a model trained on\nall previously seen states. The resulting agents acquire several proactive behaviors\nto seek and maintain stable states such as balancing and damage avoidance, that\nare closely tied to the affordances of the environment and its prevailing sources\nof entropy, such as winds, earthquakes, and other agents.  We demonstrate that\nour surprise minimizing agents can successfully play Tetris, Doom, and control\na  humanoid  to  avoid  falls,  without  any  task-specific  reward  supervision.   We\nfurther show that SMiRL can be used as an unsupervised pre-training objective\nthat substantially accelerates subsequent reward-driven learning", "keywords": ["intrinsic motivation", "reinforcement learning", "unsurpervised RL"], "paperhash": "berseth|smirl_surprise_minimizing_rl_in_entropic_environments", "original_pdf": "/attachment/fa83ce9c6d3f8a2984c4f5c8c1dd933dc9b1e3a5.pdf", "_bibtex": "@misc{\nberseth2020smirl,\ntitle={{\\{}SM{\\}}iRL: Surprise Minimizing {\\{}RL{\\}} in Entropic Environments},\nauthor={Glen Berseth and Daniel Geng and Coline Devin and Dinesh Jayaraman and Chelsea Finn and Sergey Levine},\nyear={2020},\nurl={https://openreview.net/forum?id=H1lDbaVYvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1lDbaVYvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper379/Authors", "ICLR.cc/2020/Conference/Paper379/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper379/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper379/Reviewers", "ICLR.cc/2020/Conference/Paper379/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper379/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper379/Authors|ICLR.cc/2020/Conference/Paper379/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504172322, "tmdate": 1576860535012, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper379/Authors", "ICLR.cc/2020/Conference/Paper379/Reviewers", "ICLR.cc/2020/Conference/Paper379/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper379/-/Official_Comment"}}}, {"id": "Ske_SVOntr", "original": null, "number": 3, "cdate": 1571746880044, "ddate": null, "tcdate": 1571746880044, "tmdate": 1573729634999, "tddate": null, "forum": "H1lDbaVYvH", "replyto": "H1lDbaVYvH", "invitation": "ICLR.cc/2020/Conference/Paper379/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #1", "review": "Summary\n\nThis paper proposes a novel form of surprise-minimizing intrinsic reward signal that leads to interesting behavior in the absence of an external reward signal. The proposed approach encourages an agent to visit states with high probability / density under a parametric marginal state distribution that is learned as the agent interacts with its environment. The method (dubbed SMiRL) is evaluated in visual and proprioceptive high-dimensional \"entropic\" benchmarks (that progress without the agent doing anything in order to prevent trivial solutions such as standing and never moving), and compared against two surprise-maximizing intrinsic motivation methods (ICM and RND) as well as to a reward-maximizing oracle. The experiments demonstrate that SMiRL can lead to more sensible behavior compared to ICM and RND in the chosen environments, and eventually recover the performance of a purely reward-maximizing agent. Also, SMiRL can be used for imitation learning by pre-training the parametric state distribution with data from a teacher. Finally, SMiRL shows the potential of speeding up reinforcement learning by using intrinsic motivation as an additional reward signal added to the external task-defining reward.\n\nQuality\n\nAs a practical paper, this work needs to be judged based on the quality of the experiments. I find the number of benchmarks and baselines sufficient. One major issue, that currently prevents me from voting for acceptance, is that experiments have not been conducted with enough seeds. I couldn't find any information in the paper regarding how many repetitions there are for each experiment. Also, most figures do not indicate any uncertainty measures (standard deviation, percentiles or the like)---some do, e.g. Figure (4), but it is not mentioned what type of uncertainty is depicted. One seed is certainly not enough to support the claims made by the authors---especially not the one that SMiRL can help improve RL in Figure (5). Figure (5a) clearly  does not draw a clear picture under one seed, and (5b) and (5c) require additional expert demonstrations. If experiments are repeated with more seeds and still support the claims, I am happy to increase my score to acceptance (presupposing that the discussion phase does not prevent acceptance for other reasons).\n\nClarity\n\nThe paper is clearly written and easy to follow. However, I do not like one aspect in the way the authors motivate their approach. The problem formulation starts with an MDP formulation. MDPs rely on a stationary reward signal and RL agents aim to optimize for future cumulative rewards based on these stationary reward signals. The authors propose to optimize for a non-stationary signal since the parametric state distribution changes over time. This itself is not uncommon and nothing controversial from a practical perspective. However, the statement that SMiRL agents seek to visit states that will change the parametric state distribution to obtain higher intrinsic reward in the future is controversial (see e.g. at the end of Section 2.1), because optimizing a non-stationary signal is outside the scope of the problem formulation. The reinforcement learning problem of maximizing intrinsic rewards does not know how the intrinsic reward signal is altered in the course of the future, i.e. how the parametric state distribution is updated. These statements should therefore be either adjusted accordingly, or the claims should be backed up theoretically rather than intuitively. On a minor note, I don't think that Figure (1) is necessary and the quote at the beginning of the paper might be better suited for a book chapter (but that is just my personal opinion).\n\nOriginality\n\nI find the simple idea presented by the authors to minimize rather than maximize surprise quite original. However, I do not have much experience in the domain of intrinsic motivation and leave the judgement of originality to the other reviewers and the area chair. Also, some references might be missing regarding learning with intrinsic reward (e.g. empowerment).\n\nSignificance\n\nThe fact that a simple intrinsic reward, as presented by the authors, can lead to interesting behavior, as demonstrated by the experiments, is quite significant. Unfortunately, the experiments are not significant from a statistical perspective which is why I do not recommend acceptance at this stage (as mentioned above, depending on how the authors address this issue and the discussion period, I might change to acceptance).\n\nUpdate\n\nThe authors have addressed my main concern regarding missing seeds. I therefore change to weak accept. But I am not happy how the authors responded to my concern regarding their motivation:\n1.) The formulation that tries to justify their reasoning as given in the rebuttal was absent in the first version of this paper, but something along this line would have been necessary since the argument relies on a non-standard MDP formulation.\n2.) I don't think the argumentation given in the rebuttal is correct. Imagine hypothetically the agent is in exactly the same augmented state s_t, a_t, \\theta_t at two different points in time, e.g. in the first episode and after multiple episodes. In both cases, the collected states seen so far are going to be different, hence the optimization objective for learning a marginal state distribution is different, hence the parameter updates are different, hence the transitions are not stationary.\nI do therefore encourage the authors to attenuate their wording.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper379/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper379/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["gberseth@gmail.com", "dangengdg@berkeley.edu", "coline.devin@gmail.com", "dinesh.jayaraman123@gmail.com", "cbfinn@eecs.berkeley.edu", "svlevine@eecs.berkeley.edu"], "title": "SMiRL: Surprise Minimizing RL in Entropic Environments", "authors": ["Glen Berseth", "Daniel Geng", "Coline Devin", "Dinesh Jayaraman", "Chelsea Finn", "Sergey Levine"], "pdf": "/pdf/846d7f8d3f89640e31c6ee94de558ed1614b13ee.pdf", "TL;DR": "Learning emergent behavior by minimizing Bayesian surprise with RL in natural environments with entropy.", "abstract": "All living organisms struggle against the forces of nature to carve out niches where\nthey can maintain relative stasis. We propose that such a search for order amidst\nchaos might offer a unifying principle for the emergence of useful behaviors in\nartificial agents. We formalize this idea into an unsupervised reinforcement learning\nmethod called surprise minimizing RL (SMiRL). SMiRL trains an agent with the\nobjective of maximizing the probability of observed states under a model trained on\nall previously seen states. The resulting agents acquire several proactive behaviors\nto seek and maintain stable states such as balancing and damage avoidance, that\nare closely tied to the affordances of the environment and its prevailing sources\nof entropy, such as winds, earthquakes, and other agents.  We demonstrate that\nour surprise minimizing agents can successfully play Tetris, Doom, and control\na  humanoid  to  avoid  falls,  without  any  task-specific  reward  supervision.   We\nfurther show that SMiRL can be used as an unsupervised pre-training objective\nthat substantially accelerates subsequent reward-driven learning", "keywords": ["intrinsic motivation", "reinforcement learning", "unsurpervised RL"], "paperhash": "berseth|smirl_surprise_minimizing_rl_in_entropic_environments", "original_pdf": "/attachment/fa83ce9c6d3f8a2984c4f5c8c1dd933dc9b1e3a5.pdf", "_bibtex": "@misc{\nberseth2020smirl,\ntitle={{\\{}SM{\\}}iRL: Surprise Minimizing {\\{}RL{\\}} in Entropic Environments},\nauthor={Glen Berseth and Daniel Geng and Coline Devin and Dinesh Jayaraman and Chelsea Finn and Sergey Levine},\nyear={2020},\nurl={https://openreview.net/forum?id=H1lDbaVYvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "H1lDbaVYvH", "replyto": "H1lDbaVYvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper379/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper379/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575777855284, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper379/Reviewers"], "noninvitees": [], "tcdate": 1570237753005, "tmdate": 1575777855297, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper379/-/Official_Review"}}}, {"id": "HyeojVRtoB", "original": null, "number": 4, "cdate": 1573672098750, "ddate": null, "tcdate": 1573672098750, "tmdate": 1573672098750, "tddate": null, "forum": "H1lDbaVYvH", "replyto": "Sklfr9LFKB", "invitation": "ICLR.cc/2020/Conference/Paper379/-/Official_Comment", "content": {"title": "Rebuttal Discussion", "comment": "Dear Reviewer, \n\nCould you let us know if our response has addressed the concerns raised in your review? We would be happy to provide further revisions or experiments to address any remaining issues and would appreciate a response from you on the points that we raised.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper379/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper379/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["gberseth@gmail.com", "dangengdg@berkeley.edu", "coline.devin@gmail.com", "dinesh.jayaraman123@gmail.com", "cbfinn@eecs.berkeley.edu", "svlevine@eecs.berkeley.edu"], "title": "SMiRL: Surprise Minimizing RL in Entropic Environments", "authors": ["Glen Berseth", "Daniel Geng", "Coline Devin", "Dinesh Jayaraman", "Chelsea Finn", "Sergey Levine"], "pdf": "/pdf/846d7f8d3f89640e31c6ee94de558ed1614b13ee.pdf", "TL;DR": "Learning emergent behavior by minimizing Bayesian surprise with RL in natural environments with entropy.", "abstract": "All living organisms struggle against the forces of nature to carve out niches where\nthey can maintain relative stasis. We propose that such a search for order amidst\nchaos might offer a unifying principle for the emergence of useful behaviors in\nartificial agents. We formalize this idea into an unsupervised reinforcement learning\nmethod called surprise minimizing RL (SMiRL). SMiRL trains an agent with the\nobjective of maximizing the probability of observed states under a model trained on\nall previously seen states. The resulting agents acquire several proactive behaviors\nto seek and maintain stable states such as balancing and damage avoidance, that\nare closely tied to the affordances of the environment and its prevailing sources\nof entropy, such as winds, earthquakes, and other agents.  We demonstrate that\nour surprise minimizing agents can successfully play Tetris, Doom, and control\na  humanoid  to  avoid  falls,  without  any  task-specific  reward  supervision.   We\nfurther show that SMiRL can be used as an unsupervised pre-training objective\nthat substantially accelerates subsequent reward-driven learning", "keywords": ["intrinsic motivation", "reinforcement learning", "unsurpervised RL"], "paperhash": "berseth|smirl_surprise_minimizing_rl_in_entropic_environments", "original_pdf": "/attachment/fa83ce9c6d3f8a2984c4f5c8c1dd933dc9b1e3a5.pdf", "_bibtex": "@misc{\nberseth2020smirl,\ntitle={{\\{}SM{\\}}iRL: Surprise Minimizing {\\{}RL{\\}} in Entropic Environments},\nauthor={Glen Berseth and Daniel Geng and Coline Devin and Dinesh Jayaraman and Chelsea Finn and Sergey Levine},\nyear={2020},\nurl={https://openreview.net/forum?id=H1lDbaVYvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1lDbaVYvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper379/Authors", "ICLR.cc/2020/Conference/Paper379/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper379/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper379/Reviewers", "ICLR.cc/2020/Conference/Paper379/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper379/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper379/Authors|ICLR.cc/2020/Conference/Paper379/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504172322, "tmdate": 1576860535012, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper379/Authors", "ICLR.cc/2020/Conference/Paper379/Reviewers", "ICLR.cc/2020/Conference/Paper379/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper379/-/Official_Comment"}}}, {"id": "BJgoqb0YoS", "original": null, "number": 3, "cdate": 1573671315386, "ddate": null, "tcdate": 1573671315386, "tmdate": 1573671315386, "tddate": null, "forum": "H1lDbaVYvH", "replyto": "Ske_SVOntr", "invitation": "ICLR.cc/2020/Conference/Paper379/-/Official_Comment", "content": {"title": "More random seeds", "comment": "We appreciate your time and comments on the work.\n\nThe main concern in this review is the lack of multiple random seeds for the experiments shown in the paper. While it seems we missed describing this in the paper, all of the humanoid robot examples were already averaged over 3 seeds and the standard deviation is shown in Figures 4a, 4b, 5b and 5c. We have collected data for 5 seeds over all other environments in the paper (Figure3, 5a). We have included this extra data in the paper and a description of the seeds and standard deviations. The findings from this more in-depth analysis has not altered any claims in the paper.\n\nThere was also concern raised by the review that the reward function is non-stationary. We emphasize that the RL algorithm in SMiRL is provided with a standard stationary MDP (except in the VAE setting), where the state is simply augmented with the parameters of the belief over states $\\theta$ and the timestep $t$. We emphasize that this MDP is indeed Markovian, and therefore it is reasonable to expect any convergent RL algorithm to converge to a near-optimal solution. Consider the augmented state transition $p(s_{t+1}, \\theta_{t+1}, t+1 | s_{t}, a_{t}, \\theta_{t}, t )$. This transition model does not change over time because the updates to $\\theta$ are deterministic when given $s_t$ and $t$. The reward function $R( s_{t}, \\theta_{t}, t )$ is also stationary: it is deterministic given $ s_{t}$ and $\\theta_{t}$. Because SMiRL uses RL in an MDP, we benefit from the same convergence properties as other RL methods. We have added details to make this more clear to the paper appendix.\n\nWe have also added references to empowerment in section 5 and included more theoretical justification of our method and its properties to the appendix.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper379/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper379/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["gberseth@gmail.com", "dangengdg@berkeley.edu", "coline.devin@gmail.com", "dinesh.jayaraman123@gmail.com", "cbfinn@eecs.berkeley.edu", "svlevine@eecs.berkeley.edu"], "title": "SMiRL: Surprise Minimizing RL in Entropic Environments", "authors": ["Glen Berseth", "Daniel Geng", "Coline Devin", "Dinesh Jayaraman", "Chelsea Finn", "Sergey Levine"], "pdf": "/pdf/846d7f8d3f89640e31c6ee94de558ed1614b13ee.pdf", "TL;DR": "Learning emergent behavior by minimizing Bayesian surprise with RL in natural environments with entropy.", "abstract": "All living organisms struggle against the forces of nature to carve out niches where\nthey can maintain relative stasis. We propose that such a search for order amidst\nchaos might offer a unifying principle for the emergence of useful behaviors in\nartificial agents. We formalize this idea into an unsupervised reinforcement learning\nmethod called surprise minimizing RL (SMiRL). SMiRL trains an agent with the\nobjective of maximizing the probability of observed states under a model trained on\nall previously seen states. The resulting agents acquire several proactive behaviors\nto seek and maintain stable states such as balancing and damage avoidance, that\nare closely tied to the affordances of the environment and its prevailing sources\nof entropy, such as winds, earthquakes, and other agents.  We demonstrate that\nour surprise minimizing agents can successfully play Tetris, Doom, and control\na  humanoid  to  avoid  falls,  without  any  task-specific  reward  supervision.   We\nfurther show that SMiRL can be used as an unsupervised pre-training objective\nthat substantially accelerates subsequent reward-driven learning", "keywords": ["intrinsic motivation", "reinforcement learning", "unsurpervised RL"], "paperhash": "berseth|smirl_surprise_minimizing_rl_in_entropic_environments", "original_pdf": "/attachment/fa83ce9c6d3f8a2984c4f5c8c1dd933dc9b1e3a5.pdf", "_bibtex": "@misc{\nberseth2020smirl,\ntitle={{\\{}SM{\\}}iRL: Surprise Minimizing {\\{}RL{\\}} in Entropic Environments},\nauthor={Glen Berseth and Daniel Geng and Coline Devin and Dinesh Jayaraman and Chelsea Finn and Sergey Levine},\nyear={2020},\nurl={https://openreview.net/forum?id=H1lDbaVYvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1lDbaVYvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper379/Authors", "ICLR.cc/2020/Conference/Paper379/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper379/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper379/Reviewers", "ICLR.cc/2020/Conference/Paper379/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper379/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper379/Authors|ICLR.cc/2020/Conference/Paper379/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504172322, "tmdate": 1576860535012, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper379/Authors", "ICLR.cc/2020/Conference/Paper379/Reviewers", "ICLR.cc/2020/Conference/Paper379/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper379/-/Official_Comment"}}}, {"id": "Bkxnt_gLor", "original": null, "number": 2, "cdate": 1573419140096, "ddate": null, "tcdate": 1573419140096, "tmdate": 1573419140096, "tddate": null, "forum": "H1lDbaVYvH", "replyto": "HJgOkLqitS", "invitation": "ICLR.cc/2020/Conference/Paper379/-/Official_Comment", "content": {"title": "Additional experiments", "comment": "We appreciate the comments on the work.\n\nRelated to the reviewer's curiosity experiments, we have performed these and have added the results to the paper. The question of combining surprise and curiosity has been something we have considered. While on the surface, SMiRL minimizes surprise and curiosity approaches like ICM maximize surprise, they are in fact, not mutually incompatible. SMiRL surprise minimization is episodic, so in effect, combining it with novelty-seeking exploration uses exploration bonuses to help find better strategies for minimizing surprise. On the treadmill environment, we added a new experiment that shows that ICM and SMiRL rewards can be combined to achieve even better results (Figure 4(a) right \u201cSMiRL + ICM\u201d and on the updated webpage \"Treadmill SMiRL + ICM\"). The combination of methods leads to increased initial learning speed and producing a walking gait on that task. \n\nWe have also run a version of the SMiRL as a stability reward experiment where $p(s)$ is not initialized with expert data (Figure 5(b,c) \u201cReward + SMiRL (no-expert)\u201d). In this configuration, SMiRL improves on the number of falls during training and average task reward.\n\nWe have also updated the paper to reflect the writing errors you found.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper379/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper379/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["gberseth@gmail.com", "dangengdg@berkeley.edu", "coline.devin@gmail.com", "dinesh.jayaraman123@gmail.com", "cbfinn@eecs.berkeley.edu", "svlevine@eecs.berkeley.edu"], "title": "SMiRL: Surprise Minimizing RL in Entropic Environments", "authors": ["Glen Berseth", "Daniel Geng", "Coline Devin", "Dinesh Jayaraman", "Chelsea Finn", "Sergey Levine"], "pdf": "/pdf/846d7f8d3f89640e31c6ee94de558ed1614b13ee.pdf", "TL;DR": "Learning emergent behavior by minimizing Bayesian surprise with RL in natural environments with entropy.", "abstract": "All living organisms struggle against the forces of nature to carve out niches where\nthey can maintain relative stasis. We propose that such a search for order amidst\nchaos might offer a unifying principle for the emergence of useful behaviors in\nartificial agents. We formalize this idea into an unsupervised reinforcement learning\nmethod called surprise minimizing RL (SMiRL). SMiRL trains an agent with the\nobjective of maximizing the probability of observed states under a model trained on\nall previously seen states. The resulting agents acquire several proactive behaviors\nto seek and maintain stable states such as balancing and damage avoidance, that\nare closely tied to the affordances of the environment and its prevailing sources\nof entropy, such as winds, earthquakes, and other agents.  We demonstrate that\nour surprise minimizing agents can successfully play Tetris, Doom, and control\na  humanoid  to  avoid  falls,  without  any  task-specific  reward  supervision.   We\nfurther show that SMiRL can be used as an unsupervised pre-training objective\nthat substantially accelerates subsequent reward-driven learning", "keywords": ["intrinsic motivation", "reinforcement learning", "unsurpervised RL"], "paperhash": "berseth|smirl_surprise_minimizing_rl_in_entropic_environments", "original_pdf": "/attachment/fa83ce9c6d3f8a2984c4f5c8c1dd933dc9b1e3a5.pdf", "_bibtex": "@misc{\nberseth2020smirl,\ntitle={{\\{}SM{\\}}iRL: Surprise Minimizing {\\{}RL{\\}} in Entropic Environments},\nauthor={Glen Berseth and Daniel Geng and Coline Devin and Dinesh Jayaraman and Chelsea Finn and Sergey Levine},\nyear={2020},\nurl={https://openreview.net/forum?id=H1lDbaVYvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1lDbaVYvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper379/Authors", "ICLR.cc/2020/Conference/Paper379/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper379/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper379/Reviewers", "ICLR.cc/2020/Conference/Paper379/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper379/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper379/Authors|ICLR.cc/2020/Conference/Paper379/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504172322, "tmdate": 1576860535012, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper379/Authors", "ICLR.cc/2020/Conference/Paper379/Reviewers", "ICLR.cc/2020/Conference/Paper379/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper379/-/Official_Comment"}}}, {"id": "SkgszfQzjS", "original": null, "number": 1, "cdate": 1573167634973, "ddate": null, "tcdate": 1573167634973, "tmdate": 1573167634973, "tddate": null, "forum": "H1lDbaVYvH", "replyto": "Sklfr9LFKB", "invitation": "ICLR.cc/2020/Conference/Paper379/-/Official_Comment", "content": {"title": "MDP definition and exploration.", "comment": "The main concern raised in the review is about the definition of \u201centropic\u201d and whether SMiRL learns in a well-defined MDP because of it. We do not use \u201centropic\u201d to mean that state transition probabilities change over time. Rather, it means that for any state in the environment, random disruptive perturbations may be applied to the state. In such settings, SMiRL seeks to visit state distributions $p(s)$ that are easy to preserve. We will include more details in the paper to clarify this property. Additionally, we emphasize that the RL algorithm in SMiRL is provided with a standard stationary MDP (except in the VAE setting, more on that below), where the state is simply augmented with the parameters of the belief over states $\\theta$ and the timestep $t$. We emphasize that this MDP is indeed Markovian, and therefore it is reasonable to expect any convergent RL algorithm to converge to a near-optimal solution. Consider the augmented state transition $p(s_{t+1}, \\theta_{t+1}, t+1 | s_{t}, a_{t}, \\theta_{t}, t )$. This transition model does not change over time because the updates to $\\theta$ are deterministic when given $s_t$ and $t$. The reward function $R( s_{t}, \\theta_{t}, t )$ is also stationary: it is deterministic given $ s_{t}$ and $\\theta_{t}$. Because SMiRL uses RL in an MDP, we benefit from the same convergence properties as other RL methods. This description has been added to the paper to assist in making the method more clear to the reader.\n\nHowever, the version of SMiRL that uses a representation learned from a VAE is not Markovian because the VAE parameters are not added to the state, and thus the reward function changes over time. We find that this does not hurt results, and note that many intrinsic reward methods such as ICM and RND also lack stationary reward functions.\n\nAnother concern in the review is whether SMiRL discourages exploration. SMiRL does not discourage exploration over its lifetime. When SMiRL is in a situation such that maintaining a stable $p_{\\theta_{t}}(s)$ is difficult, then SMiRL will explore to learn the actions necessary to reach a more maintainable state distribution. In the paper, we outline this concept in Figure 1, where a robot must explore and learn how to build a house so that it can protect itself from changing weather in the future. The environments we use in the paper have entropy and noise; as a result, the agent must learn how to actively explore in order to reach and maintain homeostasis/equilibrium while battling entropy. We will include this more in-depth details in the paper.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper379/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper379/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["gberseth@gmail.com", "dangengdg@berkeley.edu", "coline.devin@gmail.com", "dinesh.jayaraman123@gmail.com", "cbfinn@eecs.berkeley.edu", "svlevine@eecs.berkeley.edu"], "title": "SMiRL: Surprise Minimizing RL in Entropic Environments", "authors": ["Glen Berseth", "Daniel Geng", "Coline Devin", "Dinesh Jayaraman", "Chelsea Finn", "Sergey Levine"], "pdf": "/pdf/846d7f8d3f89640e31c6ee94de558ed1614b13ee.pdf", "TL;DR": "Learning emergent behavior by minimizing Bayesian surprise with RL in natural environments with entropy.", "abstract": "All living organisms struggle against the forces of nature to carve out niches where\nthey can maintain relative stasis. We propose that such a search for order amidst\nchaos might offer a unifying principle for the emergence of useful behaviors in\nartificial agents. We formalize this idea into an unsupervised reinforcement learning\nmethod called surprise minimizing RL (SMiRL). SMiRL trains an agent with the\nobjective of maximizing the probability of observed states under a model trained on\nall previously seen states. The resulting agents acquire several proactive behaviors\nto seek and maintain stable states such as balancing and damage avoidance, that\nare closely tied to the affordances of the environment and its prevailing sources\nof entropy, such as winds, earthquakes, and other agents.  We demonstrate that\nour surprise minimizing agents can successfully play Tetris, Doom, and control\na  humanoid  to  avoid  falls,  without  any  task-specific  reward  supervision.   We\nfurther show that SMiRL can be used as an unsupervised pre-training objective\nthat substantially accelerates subsequent reward-driven learning", "keywords": ["intrinsic motivation", "reinforcement learning", "unsurpervised RL"], "paperhash": "berseth|smirl_surprise_minimizing_rl_in_entropic_environments", "original_pdf": "/attachment/fa83ce9c6d3f8a2984c4f5c8c1dd933dc9b1e3a5.pdf", "_bibtex": "@misc{\nberseth2020smirl,\ntitle={{\\{}SM{\\}}iRL: Surprise Minimizing {\\{}RL{\\}} in Entropic Environments},\nauthor={Glen Berseth and Daniel Geng and Coline Devin and Dinesh Jayaraman and Chelsea Finn and Sergey Levine},\nyear={2020},\nurl={https://openreview.net/forum?id=H1lDbaVYvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1lDbaVYvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper379/Authors", "ICLR.cc/2020/Conference/Paper379/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper379/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper379/Reviewers", "ICLR.cc/2020/Conference/Paper379/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper379/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper379/Authors|ICLR.cc/2020/Conference/Paper379/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504172322, "tmdate": 1576860535012, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper379/Authors", "ICLR.cc/2020/Conference/Paper379/Reviewers", "ICLR.cc/2020/Conference/Paper379/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper379/-/Official_Comment"}}}, {"id": "Sklfr9LFKB", "original": null, "number": 1, "cdate": 1571543609710, "ddate": null, "tcdate": 1571543609710, "tmdate": 1572972602690, "tddate": null, "forum": "H1lDbaVYvH", "replyto": "H1lDbaVYvH", "invitation": "ICLR.cc/2020/Conference/Paper379/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The paper proposes a novel RL algorithm to minimize \u2018surprise\u2019, and the empirical results show the efficiency. The experiments are well-done jobs. However, there are several problems in other parts:\n1. Some terms in Section 2 are too general, such as \u2018surprise\u2019 and \u2018stable states\u2019. Please define these more precise.\n2. Please give an exact description of the MDP you consider about. It seems that the transition probabilities change by time as you mention \u2018entropic environments\u2019. So I guess it\u2019s a finite horizon MDP?\n3. What\u2019s the final situations of the policy and density function? Please give some theoretical results such as convergence or asymptotic properties of the policy and the density functions. At least, the results can be derived under simple MDP and density function settings.\n4. What\u2019s the relationship between the density function you mention in your paper and state distributions in RL?\n5. The agent trying to reach stable states sounds like exploration is discouraged. How do you explain? \u201cProvably Efficient Maximum Entropy Exploration\u201d by Elad et al. 2018 proposes to maximize negative log-likelihood of state distributions to encourage exploration, which is opposite to your setting. How do you think?\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper379/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper379/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["gberseth@gmail.com", "dangengdg@berkeley.edu", "coline.devin@gmail.com", "dinesh.jayaraman123@gmail.com", "cbfinn@eecs.berkeley.edu", "svlevine@eecs.berkeley.edu"], "title": "SMiRL: Surprise Minimizing RL in Entropic Environments", "authors": ["Glen Berseth", "Daniel Geng", "Coline Devin", "Dinesh Jayaraman", "Chelsea Finn", "Sergey Levine"], "pdf": "/pdf/846d7f8d3f89640e31c6ee94de558ed1614b13ee.pdf", "TL;DR": "Learning emergent behavior by minimizing Bayesian surprise with RL in natural environments with entropy.", "abstract": "All living organisms struggle against the forces of nature to carve out niches where\nthey can maintain relative stasis. We propose that such a search for order amidst\nchaos might offer a unifying principle for the emergence of useful behaviors in\nartificial agents. We formalize this idea into an unsupervised reinforcement learning\nmethod called surprise minimizing RL (SMiRL). SMiRL trains an agent with the\nobjective of maximizing the probability of observed states under a model trained on\nall previously seen states. The resulting agents acquire several proactive behaviors\nto seek and maintain stable states such as balancing and damage avoidance, that\nare closely tied to the affordances of the environment and its prevailing sources\nof entropy, such as winds, earthquakes, and other agents.  We demonstrate that\nour surprise minimizing agents can successfully play Tetris, Doom, and control\na  humanoid  to  avoid  falls,  without  any  task-specific  reward  supervision.   We\nfurther show that SMiRL can be used as an unsupervised pre-training objective\nthat substantially accelerates subsequent reward-driven learning", "keywords": ["intrinsic motivation", "reinforcement learning", "unsurpervised RL"], "paperhash": "berseth|smirl_surprise_minimizing_rl_in_entropic_environments", "original_pdf": "/attachment/fa83ce9c6d3f8a2984c4f5c8c1dd933dc9b1e3a5.pdf", "_bibtex": "@misc{\nberseth2020smirl,\ntitle={{\\{}SM{\\}}iRL: Surprise Minimizing {\\{}RL{\\}} in Entropic Environments},\nauthor={Glen Berseth and Daniel Geng and Coline Devin and Dinesh Jayaraman and Chelsea Finn and Sergey Levine},\nyear={2020},\nurl={https://openreview.net/forum?id=H1lDbaVYvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "H1lDbaVYvH", "replyto": "H1lDbaVYvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper379/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper379/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575777855284, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper379/Reviewers"], "noninvitees": [], "tcdate": 1570237753005, "tmdate": 1575777855297, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper379/-/Official_Review"}}}, {"id": "HJgOkLqitS", "original": null, "number": 2, "cdate": 1571689952124, "ddate": null, "tcdate": 1571689952124, "tmdate": 1572972602654, "tddate": null, "forum": "H1lDbaVYvH", "replyto": "H1lDbaVYvH", "invitation": "ICLR.cc/2020/Conference/Paper379/-/Official_Review", "content": {"rating": "6: Weak Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper proposes Surprise Minimizing RL (SMiRL), a conceptual framework for training a reinforcement learning agent to seek out states with high likelihood under a density model trained on visited states. They qualitatively and quantitatively explore various aspects of the behaviour of these agents and argue that they exhibit a variety of favourable properties. They also compare their surprise minimizing algorithm with a variety of novelty-seeking algorithms (which can be considered somewhat the opposite) and show that in certain cases surprise minimization can result in more desirable behaviour. Finally, they show that using surprise minimization as an auxiliary reward can speed learning in certain settings.\n\nMinimizing surprise with RL presents challenges because, as the authors point out, exploring a surprising state in the present might minimize surprise in the future if the surprising state can be maintained (and therefore made unsurprising). To formulate this notion of surprise as a meaningful RL problem it is necessary to include some representation of the state likelihood in the agent's state representation; so that the agent can learn how it's actions affect not only the environment state but it's own potential for future surprise. This paper takes a pragmatic approach to this by training a density model on the full set of visited states which are reset at the start of each of a series of training episodes. The agent's policy is then conditioned on the parameters of the density model and the current size of the dataset.\n\nI lean toward accepting this paper as an interesting initial step in applying the idea of surprise minimization to reinforcement learning. The proposed approach is simplistic in how they treat surprise, and therefore illustrative, but probably not practical. The experiments are also illustrative, and while it is clear that surprise minimization won't always generate useful behaviour, the paper doesn't overclaim in this regard. I found the motivation for being useful in natural environments, where maintaining some kind of homeostasis is often key, to be well presented. Given that a significant body of recent work focuses on novelty seeking as a means to guide exploration, I think it is a point worth making that there are many reasonable environments where the opposite behaviour is desirable.\n\nI also found the paper to be quite well written and enjoyable to read overall.\n\nGeneral Comments:\nSection 3, VizDoom: \"The observation space for consists...\"->\"The observation space consists...\"\nSection 2.3: \"...as a multivariate normal distribution...\" it looks like covariance is not accounted for so wouldn't it more accurately be representing each component as an independent normal distribution?\nSection 4.1: \"...fireball explorations.\"->\"...fireball explosions.\"?\nSection 4.1: \"...as shown in Figure Figure...\"->\"...as shown in Figure...\"\n\nQuestions for the authors:\n-Out of curiousity, what do the results look like when using SMiRL as a stability reward but without example trajectories?\n-SMiRL is largely pitched as an alternative to novelty-seeking methods. But it seems to me novelty-seeking could be usefully combined since as you point out SMiRL may still have to explore to find stable states. Do you see such a combination as feasible or are the two methods fundamentally opposed?"}, "signatures": ["ICLR.cc/2020/Conference/Paper379/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper379/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["gberseth@gmail.com", "dangengdg@berkeley.edu", "coline.devin@gmail.com", "dinesh.jayaraman123@gmail.com", "cbfinn@eecs.berkeley.edu", "svlevine@eecs.berkeley.edu"], "title": "SMiRL: Surprise Minimizing RL in Entropic Environments", "authors": ["Glen Berseth", "Daniel Geng", "Coline Devin", "Dinesh Jayaraman", "Chelsea Finn", "Sergey Levine"], "pdf": "/pdf/846d7f8d3f89640e31c6ee94de558ed1614b13ee.pdf", "TL;DR": "Learning emergent behavior by minimizing Bayesian surprise with RL in natural environments with entropy.", "abstract": "All living organisms struggle against the forces of nature to carve out niches where\nthey can maintain relative stasis. We propose that such a search for order amidst\nchaos might offer a unifying principle for the emergence of useful behaviors in\nartificial agents. We formalize this idea into an unsupervised reinforcement learning\nmethod called surprise minimizing RL (SMiRL). SMiRL trains an agent with the\nobjective of maximizing the probability of observed states under a model trained on\nall previously seen states. The resulting agents acquire several proactive behaviors\nto seek and maintain stable states such as balancing and damage avoidance, that\nare closely tied to the affordances of the environment and its prevailing sources\nof entropy, such as winds, earthquakes, and other agents.  We demonstrate that\nour surprise minimizing agents can successfully play Tetris, Doom, and control\na  humanoid  to  avoid  falls,  without  any  task-specific  reward  supervision.   We\nfurther show that SMiRL can be used as an unsupervised pre-training objective\nthat substantially accelerates subsequent reward-driven learning", "keywords": ["intrinsic motivation", "reinforcement learning", "unsurpervised RL"], "paperhash": "berseth|smirl_surprise_minimizing_rl_in_entropic_environments", "original_pdf": "/attachment/fa83ce9c6d3f8a2984c4f5c8c1dd933dc9b1e3a5.pdf", "_bibtex": "@misc{\nberseth2020smirl,\ntitle={{\\{}SM{\\}}iRL: Surprise Minimizing {\\{}RL{\\}} in Entropic Environments},\nauthor={Glen Berseth and Daniel Geng and Coline Devin and Dinesh Jayaraman and Chelsea Finn and Sergey Levine},\nyear={2020},\nurl={https://openreview.net/forum?id=H1lDbaVYvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "H1lDbaVYvH", "replyto": "H1lDbaVYvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper379/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper379/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575777855284, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper379/Reviewers"], "noninvitees": [], "tcdate": 1570237753005, "tmdate": 1575777855297, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper379/-/Official_Review"}}}], "count": 10}