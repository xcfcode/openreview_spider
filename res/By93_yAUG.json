{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1528124456059, "tcdate": 1518364849563, "number": 69, "cdate": 1518364849563, "id": "By93_yAUG", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "By93_yAUG", "signatures": ["~Siddhartha_Brahma1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "SufiSent - Universal Sentence Representations Using Suffix Encodings", "abstract": "Computing universal distributed representations of sentences is a fundamental task\nin natural language processing. We propose a method to learn such representations\nby encoding the suffixes of word sequences in a sentence and training on the\nStanford Natural Language Inference (SNLI) dataset. We demonstrate the effectiveness\nof our approach by evaluating it on the SentEval benchmark, improving\non existing approaches on several transfer tasks.", "paperhash": "brahma|sufisent_universal_sentence_representations_using_suffix_encodings", "keywords": ["universal sentence representation", "LSTM", "natural language inference"], "_bibtex": "@misc{\n  brahma2018sufisent,\n  title={SufiSent - Universal Sentence Representations Using Suffix Encodings},\n  author={Siddhartha Brahma},\n  year={2018},\n  url={https://openreview.net/forum?id=By93_yAUG}\n}", "authorids": ["sidbrahma@gmail.com"], "authors": ["Siddhartha Brahma"], "TL;DR": "Using LSTM encodings of both prefixes and suffixes gives better universal sentence representations.", "pdf": "/pdf/e9b153732ebe516b3dae32a9ad95ef9cae478b0b.pdf"}, "nonreaders": [], "details": {"replyCount": 6, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582788655, "tcdate": 1520630106808, "number": 1, "cdate": 1520630106808, "id": "rkmPFueFz", "invitation": "ICLR.cc/2018/Workshop/-/Paper69/Official_Review", "forum": "By93_yAUG", "replyto": "By93_yAUG", "signatures": ["ICLR.cc/2018/Workshop/Paper69/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper69/AnonReviewer3"], "content": {"title": "Interesting incremental work", "rating": "6: Marginally above acceptance threshold", "review": "The paper proposes a sentence representation method by encoding the suffixes of word sequence in a sentence with BiLSTMs. In the forward direction, the proposed model encodes both prefixes and suffixes. To show the effectiveness, the SufiSent models were trained and tested on SNLI and a transfer SentEval benchmark and achieved better performance than the baseline, BiLSTM-Max (Conneau et al. 2017). The paper compares several variants of SufiSent such as tied parameters and concatenation of vectors.\n\nThe paper proposes an interesting extension (though novelty is rather limited) to the original BiLSTM encoders. Content of the paper is well structured and easy to follow. \n\nPros: \n1. This is an interesting extension to the original BiLSTM encoders. \n2. Experiments show some improvement on the SNLI and the transfer tasks.\n\nCons:\n1. Some more analysis and discussion on the benefit of encoding suffixes would help readers understand the emperical improvement.\n2. The suffix encoding needs a total of N passes over progressively smaller suffixes of sentence. It may have a higher computation cost than the baseline. It would be interesting to provide some comparison on speed between baselines and the proposed models.\n\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SufiSent - Universal Sentence Representations Using Suffix Encodings", "abstract": "Computing universal distributed representations of sentences is a fundamental task\nin natural language processing. We propose a method to learn such representations\nby encoding the suffixes of word sequences in a sentence and training on the\nStanford Natural Language Inference (SNLI) dataset. We demonstrate the effectiveness\nof our approach by evaluating it on the SentEval benchmark, improving\non existing approaches on several transfer tasks.", "paperhash": "brahma|sufisent_universal_sentence_representations_using_suffix_encodings", "keywords": ["universal sentence representation", "LSTM", "natural language inference"], "_bibtex": "@misc{\n  brahma2018sufisent,\n  title={SufiSent - Universal Sentence Representations Using Suffix Encodings},\n  author={Siddhartha Brahma},\n  year={2018},\n  url={https://openreview.net/forum?id=By93_yAUG}\n}", "authorids": ["sidbrahma@gmail.com"], "authors": ["Siddhartha Brahma"], "TL;DR": "Using LSTM encodings of both prefixes and suffixes gives better universal sentence representations.", "pdf": "/pdf/e9b153732ebe516b3dae32a9ad95ef9cae478b0b.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582788463, "id": "ICLR.cc/2018/Workshop/-/Paper69/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper69/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper69/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper69/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper69/AnonReviewer1"], "reply": {"forum": "By93_yAUG", "replyto": "By93_yAUG", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper69/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper69/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582788463}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582709351, "tcdate": 1520689175591, "number": 2, "cdate": 1520689175591, "id": "Byl7eDbFf", "invitation": "ICLR.cc/2018/Workshop/-/Paper69/Official_Review", "forum": "By93_yAUG", "replyto": "By93_yAUG", "signatures": ["ICLR.cc/2018/Workshop/Paper69/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper69/AnonReviewer2"], "content": {"title": "Minor contribution with mixed results", "rating": "6: Marginally above acceptance threshold", "review": "This paper proposes to extend the bi-directional LSTM (BiLSTM) for suffixes encoding. In sum, this paper is quite clear, although I would like to see more motivation for the proposed approach. In my opinion, the only difference of this model compared to the BiLSTM is that the author presents information encoded in each point of a sentence twice (left-to-right and right-to-left context). It means also that the number of parameters is doubled compared to a traditional BiLSTM. Therefore, in order to have a fair comparison, the author should double the number of parameters of the BiLSTM too (Table 1).\n\nIn Table 2, the results are mixed. The author should provide more intuition and insights to readers to have a better understanding why. Furthermore, I suggest the author to add a BiLSTM with self-attention as another baseline system. \n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SufiSent - Universal Sentence Representations Using Suffix Encodings", "abstract": "Computing universal distributed representations of sentences is a fundamental task\nin natural language processing. We propose a method to learn such representations\nby encoding the suffixes of word sequences in a sentence and training on the\nStanford Natural Language Inference (SNLI) dataset. We demonstrate the effectiveness\nof our approach by evaluating it on the SentEval benchmark, improving\non existing approaches on several transfer tasks.", "paperhash": "brahma|sufisent_universal_sentence_representations_using_suffix_encodings", "keywords": ["universal sentence representation", "LSTM", "natural language inference"], "_bibtex": "@misc{\n  brahma2018sufisent,\n  title={SufiSent - Universal Sentence Representations Using Suffix Encodings},\n  author={Siddhartha Brahma},\n  year={2018},\n  url={https://openreview.net/forum?id=By93_yAUG}\n}", "authorids": ["sidbrahma@gmail.com"], "authors": ["Siddhartha Brahma"], "TL;DR": "Using LSTM encodings of both prefixes and suffixes gives better universal sentence representations.", "pdf": "/pdf/e9b153732ebe516b3dae32a9ad95ef9cae478b0b.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582788463, "id": "ICLR.cc/2018/Workshop/-/Paper69/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper69/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper69/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper69/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper69/AnonReviewer1"], "reply": {"forum": "By93_yAUG", "replyto": "By93_yAUG", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper69/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper69/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582788463}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582606433, "tcdate": 1520902817840, "number": 3, "cdate": 1520902817840, "id": "B19jGj4YG", "invitation": "ICLR.cc/2018/Workshop/-/Paper69/Official_Review", "forum": "By93_yAUG", "replyto": "By93_yAUG", "signatures": ["ICLR.cc/2018/Workshop/Paper69/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper69/AnonReviewer1"], "content": {"title": "Solid focused contribution", "rating": "7: Good paper, accept", "review": "This paper presents a simple modification to the standard BiLSTM architecture for sentence classification that results in increased performance over a number of tasks. The basic idea is to encode sentence suffixes using multiple passes through an LSTM (rather than just a single pass as in the backwards side of a BiLSTM). While the gains are quite small over the InferSent baseline on most tasks, overall I think the results are good enough to make NLP researchers think about trying it out to squeeze out some extra accuracy (although the computational inefficiency might make them think twice! it would be good to mention the training time in the paper). The paper does not attempt to explain *why* these suffix representations are better than standard BiLSTM ones, although three pages is not much room; nevertheless, I wonder if looking at examples for which your model is correct but InferSent is wrong would be informative.\n\nAdditionally, it would be nice to see mean/variance across multiple runs reported in the results tables, as there is really no reason to only run one experiment on these small datasets, and the reported accuracy differences are often miniscule. \n\nOverall, I would lean towards accepting this paper, as I think its empirical contribution is appropriate for a workshop submission despite an analysis of why the method works. ", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SufiSent - Universal Sentence Representations Using Suffix Encodings", "abstract": "Computing universal distributed representations of sentences is a fundamental task\nin natural language processing. We propose a method to learn such representations\nby encoding the suffixes of word sequences in a sentence and training on the\nStanford Natural Language Inference (SNLI) dataset. We demonstrate the effectiveness\nof our approach by evaluating it on the SentEval benchmark, improving\non existing approaches on several transfer tasks.", "paperhash": "brahma|sufisent_universal_sentence_representations_using_suffix_encodings", "keywords": ["universal sentence representation", "LSTM", "natural language inference"], "_bibtex": "@misc{\n  brahma2018sufisent,\n  title={SufiSent - Universal Sentence Representations Using Suffix Encodings},\n  author={Siddhartha Brahma},\n  year={2018},\n  url={https://openreview.net/forum?id=By93_yAUG}\n}", "authorids": ["sidbrahma@gmail.com"], "authors": ["Siddhartha Brahma"], "TL;DR": "Using LSTM encodings of both prefixes and suffixes gives better universal sentence representations.", "pdf": "/pdf/e9b153732ebe516b3dae32a9ad95ef9cae478b0b.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582788463, "id": "ICLR.cc/2018/Workshop/-/Paper69/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper69/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper69/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper69/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper69/AnonReviewer1"], "reply": {"forum": "By93_yAUG", "replyto": "By93_yAUG", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper69/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper69/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582788463}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521573563306, "tcdate": 1521573563306, "number": 89, "cdate": 1521573562915, "id": "SkXpRRCtf", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "By93_yAUG", "replyto": "By93_yAUG", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Accept", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "Congratulations, your paper was accepted to the ICLR workshop."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SufiSent - Universal Sentence Representations Using Suffix Encodings", "abstract": "Computing universal distributed representations of sentences is a fundamental task\nin natural language processing. We propose a method to learn such representations\nby encoding the suffixes of word sequences in a sentence and training on the\nStanford Natural Language Inference (SNLI) dataset. We demonstrate the effectiveness\nof our approach by evaluating it on the SentEval benchmark, improving\non existing approaches on several transfer tasks.", "paperhash": "brahma|sufisent_universal_sentence_representations_using_suffix_encodings", "keywords": ["universal sentence representation", "LSTM", "natural language inference"], "_bibtex": "@misc{\n  brahma2018sufisent,\n  title={SufiSent - Universal Sentence Representations Using Suffix Encodings},\n  author={Siddhartha Brahma},\n  year={2018},\n  url={https://openreview.net/forum?id=By93_yAUG}\n}", "authorids": ["sidbrahma@gmail.com"], "authors": ["Siddhartha Brahma"], "TL;DR": "Using LSTM encodings of both prefixes and suffixes gives better universal sentence representations.", "pdf": "/pdf/e9b153732ebe516b3dae32a9ad95ef9cae478b0b.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}, {"tddate": null, "ddate": null, "tmdate": 1520448249341, "tcdate": 1520448249341, "number": 2, "cdate": 1520448249341, "id": "Sk-W72adf", "invitation": "ICLR.cc/2018/Workshop/-/Paper69/Public_Comment", "forum": "By93_yAUG", "replyto": "r1w8runOz", "signatures": ["~Siddhartha_Brahma1"], "readers": ["everyone"], "writers": ["~Siddhartha_Brahma1"], "content": {"title": "Regarding the experiments and language understanding from SufiSent", "comment": "\nThank you for reading the paper. Both the questions are very pertinent. \n\n1. We use exactly the same training setup as used by the InferSent paper - namely SGD with initial learning rate of 0.1, which is decayed by 0.99 after every epoch if the validation accuracy improves over the current best or by 0.2 if the validation accuracy does not improve. The training is continued until the learning rate drops below 1e-5. The word embeddings are initialized using GloVe vectors and are not updated during training. We also use the same classification layer to make the results comparable (one fully connected layer of dimension 512; there is a typo in our paper submission which incorrectly mentions this as two). We also retrained the InferSent model in our setup, obtaining results comparable to the ones published in the paper, hence we decided to stick with the numbers from the paper. \n\nAs for the number of parameters, a SufiSent-Tied model (which uses the same LSTM for the prefixes and suffixes in one direction) has exactly the same number of parameters as a BiLSTM model with the same LSTM encoding dimension. In fact, the SufiSent-Tied is also our best overall model (see Table 1). A SufiSent model, which uses two different LSTMs for the prefixes and suffixes, has double the number of parameters. As shown in Figure 2, SufiSent has an upper hand over SufiSent-Tied at lower dimensions, but the latter becomes better with the increased capacity of the LSTMs at higher dimensions.\n \nIn summary, we have taken care to make our results as comparable as possible to the InferSent results and the SufiSent models show improvement over InferSent under the same training conditions. \n\n2. This is a very good question, and we will try to give our best understanding of why encoding suffixes is beneficial.\nOne way to think about the intermediate states in a BiLSTM encoder is to see each state as a distinct \u201cencoded view\u201d of the sentence. However, the fact that the suffixes are encoded in a reverse order, makes them not directly compatible with the prefix encodings. By encoding the suffixes, we can directly combine the prefix and suffix encodings (using a max in SufiSent), thereby giving richer encoded views. \n\nPerhaps a more convincing reason is the following. The $k$-th intermediate state in a LSTM encoding is constrained to be a function of the $k-1$-th state and the $k$-th word. What SufiSent is doing is resetting the state at each word and forcing the LSTM to \u201clearn from scratch\u201d. Another way to put it would be to think of SufiSent as a single LSTM being repeatedly fed a sentence with larger and larger prefixes dropped out, and then their final encodings being combined by max-pooling. We believe this gives it a better generalization ability as evidenced by the better results compared to a BiLSTM-Max/InferSent encoder on transfer tasks in SentEval.\n\nIn terms of language understanding, one important by-product of computing encodings for each suffix is that we have encodings for each subsequence of a sentence. If understanding a sentence implies identifying key parts and discarding others, these subsequence encodings could be combined in interesting ways to come up with an encoding of a sentence that is able to retain specific parts of the sentence, in a task specific manner. On the other hand, external knowledge like a parse tree could be used to combine the subsequence encodings, thereby producing a model with a strong inductive bias. We are actively investigating all these aspects of SufiSent.\n\n\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SufiSent - Universal Sentence Representations Using Suffix Encodings", "abstract": "Computing universal distributed representations of sentences is a fundamental task\nin natural language processing. We propose a method to learn such representations\nby encoding the suffixes of word sequences in a sentence and training on the\nStanford Natural Language Inference (SNLI) dataset. We demonstrate the effectiveness\nof our approach by evaluating it on the SentEval benchmark, improving\non existing approaches on several transfer tasks.", "paperhash": "brahma|sufisent_universal_sentence_representations_using_suffix_encodings", "keywords": ["universal sentence representation", "LSTM", "natural language inference"], "_bibtex": "@misc{\n  brahma2018sufisent,\n  title={SufiSent - Universal Sentence Representations Using Suffix Encodings},\n  author={Siddhartha Brahma},\n  year={2018},\n  url={https://openreview.net/forum?id=By93_yAUG}\n}", "authorids": ["sidbrahma@gmail.com"], "authors": ["Siddhartha Brahma"], "TL;DR": "Using LSTM encodings of both prefixes and suffixes gives better universal sentence representations.", "pdf": "/pdf/e9b153732ebe516b3dae32a9ad95ef9cae478b0b.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518712627143, "id": "ICLR.cc/2018/Workshop/-/Paper69/Public_Comment", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper69/Reviewers"], "reply": {"replyto": null, "forum": "By93_yAUG", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1518712627143}}}, {"tddate": null, "ddate": null, "tmdate": 1520366926805, "tcdate": 1520366926805, "number": 1, "cdate": 1520366926805, "id": "r1w8runOz", "invitation": "ICLR.cc/2018/Workshop/-/Paper69/Public_Comment", "forum": "By93_yAUG", "replyto": "By93_yAUG", "signatures": ["~Samuel_R._Bowman1"], "readers": ["everyone"], "writers": ["~Samuel_R._Bowman1"], "content": {"title": "Interesting result", "comment": "Two questions:\n\u2013 Have you tried training plain InferSent using your same training setup and the same number of parameters? There's always the risk with experiments like these that your results will have more to do with subtle training decisions like early stopping criterion or initialization than with the target change. \n\u2013 What do you think suffix encoding gets you? Is there some useful strategy for language understanding that's easier to learn with a suffix encoder than with a plain BiLSTM?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SufiSent - Universal Sentence Representations Using Suffix Encodings", "abstract": "Computing universal distributed representations of sentences is a fundamental task\nin natural language processing. We propose a method to learn such representations\nby encoding the suffixes of word sequences in a sentence and training on the\nStanford Natural Language Inference (SNLI) dataset. We demonstrate the effectiveness\nof our approach by evaluating it on the SentEval benchmark, improving\non existing approaches on several transfer tasks.", "paperhash": "brahma|sufisent_universal_sentence_representations_using_suffix_encodings", "keywords": ["universal sentence representation", "LSTM", "natural language inference"], "_bibtex": "@misc{\n  brahma2018sufisent,\n  title={SufiSent - Universal Sentence Representations Using Suffix Encodings},\n  author={Siddhartha Brahma},\n  year={2018},\n  url={https://openreview.net/forum?id=By93_yAUG}\n}", "authorids": ["sidbrahma@gmail.com"], "authors": ["Siddhartha Brahma"], "TL;DR": "Using LSTM encodings of both prefixes and suffixes gives better universal sentence representations.", "pdf": "/pdf/e9b153732ebe516b3dae32a9ad95ef9cae478b0b.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518712627143, "id": "ICLR.cc/2018/Workshop/-/Paper69/Public_Comment", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper69/Reviewers"], "reply": {"replyto": null, "forum": "By93_yAUG", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1518712627143}}}], "count": 7}