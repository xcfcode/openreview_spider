{"notes": [{"id": "ByeTHsAqtX", "original": "rJxl1oMKY7", "number": 125, "cdate": 1538087748571, "ddate": null, "tcdate": 1538087748571, "tmdate": 1545355410016, "tddate": null, "forum": "ByeTHsAqtX", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Gradient Descent Happens in a Tiny Subspace", "abstract": "We show that in a variety of large-scale deep learning scenarios the gradient dynamically converges to a very small subspace after a short period of training. The subspace is spanned by a few top eigenvectors of the Hessian (equal to the number of classes in the dataset), and is mostly preserved over long periods of training. A simple argument then suggests that gradient descent may happen mostly in this subspace. We give an example of this effect in a solvable model of classification, and we comment on possible implications for optimization and learning.", "keywords": ["Gradient Descent", "Hessian", "Deep Learning"], "authorids": ["guyg@ias.edu", "danr@fb.com", "edyer@google.com"], "authors": ["Guy Gur-Ari", "Daniel A. Roberts", "Ethan Dyer"], "TL;DR": "For classification problems with k classes, we show that the gradient tends to live in a tiny, slowly-evolving subspace spanned by the eigenvectors corresponding to the k-largest eigenvalues of the Hessian.", "pdf": "/pdf/1a5c9d880096c438ce4e5908c4b58ad58d158bee.pdf", "paperhash": "gurari|gradient_descent_happens_in_a_tiny_subspace", "_bibtex": "@misc{\ngur-ari2019gradient,\ntitle={Gradient Descent Happens in a Tiny Subspace},\nauthor={Guy Gur-Ari and Daniel A. Roberts and Ethan Dyer},\nyear={2019},\nurl={https://openreview.net/forum?id=ByeTHsAqtX},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "r1ehP-7yxV", "original": null, "number": 1, "cdate": 1544659300247, "ddate": null, "tcdate": 1544659300247, "tmdate": 1545354504102, "tddate": null, "forum": "ByeTHsAqtX", "replyto": "ByeTHsAqtX", "invitation": "ICLR.cc/2019/Conference/-/Paper125/Meta_Review", "content": {"metareview": "The paper is overally interesting and addresses an important problem, however reviewers ask for more rigorous empirical study and less restrictive settings.", "confidence": "5: The area chair is absolutely certain", "recommendation": "Reject", "title": "Summary review"}, "signatures": ["ICLR.cc/2019/Conference/Paper125/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper125/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Gradient Descent Happens in a Tiny Subspace", "abstract": "We show that in a variety of large-scale deep learning scenarios the gradient dynamically converges to a very small subspace after a short period of training. The subspace is spanned by a few top eigenvectors of the Hessian (equal to the number of classes in the dataset), and is mostly preserved over long periods of training. A simple argument then suggests that gradient descent may happen mostly in this subspace. We give an example of this effect in a solvable model of classification, and we comment on possible implications for optimization and learning.", "keywords": ["Gradient Descent", "Hessian", "Deep Learning"], "authorids": ["guyg@ias.edu", "danr@fb.com", "edyer@google.com"], "authors": ["Guy Gur-Ari", "Daniel A. Roberts", "Ethan Dyer"], "TL;DR": "For classification problems with k classes, we show that the gradient tends to live in a tiny, slowly-evolving subspace spanned by the eigenvectors corresponding to the k-largest eigenvalues of the Hessian.", "pdf": "/pdf/1a5c9d880096c438ce4e5908c4b58ad58d158bee.pdf", "paperhash": "gurari|gradient_descent_happens_in_a_tiny_subspace", "_bibtex": "@misc{\ngur-ari2019gradient,\ntitle={Gradient Descent Happens in a Tiny Subspace},\nauthor={Guy Gur-Ari and Daniel A. Roberts and Ethan Dyer},\nyear={2019},\nurl={https://openreview.net/forum?id=ByeTHsAqtX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper125/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353329292, "tddate": null, "super": null, "final": null, "reply": {"forum": "ByeTHsAqtX", "replyto": "ByeTHsAqtX", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper125/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper125/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper125/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353329292}}}, {"id": "SJgd8-z-l4", "original": null, "number": 1, "cdate": 1544786255681, "ddate": null, "tcdate": 1544786255681, "tmdate": 1544786255681, "tddate": null, "forum": "ByeTHsAqtX", "replyto": "ByeTHsAqtX", "invitation": "ICLR.cc/2019/Conference/-/Paper125/Public_Comment", "content": {"comment": "Hi,\n\nI am trying to reconcile these findings with those of https://arxiv.org/pdf/1804.08838.pdf where they find that the intrinsic dimensionality had more to do with the input dimension than the number of classes.\n\nOne of the differences between the two works is that your subspace is not randomly chosen but I was wondering if you had insights on the discrepancy between the two claims.", "title": "Link with \"MEASURING THE INTRINSIC DIMENSION OF OBJECTIVE LANDSCAPES\""}, "signatures": ["~Nicolas_Le_Roux2"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Nicolas_Le_Roux2", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Gradient Descent Happens in a Tiny Subspace", "abstract": "We show that in a variety of large-scale deep learning scenarios the gradient dynamically converges to a very small subspace after a short period of training. The subspace is spanned by a few top eigenvectors of the Hessian (equal to the number of classes in the dataset), and is mostly preserved over long periods of training. A simple argument then suggests that gradient descent may happen mostly in this subspace. We give an example of this effect in a solvable model of classification, and we comment on possible implications for optimization and learning.", "keywords": ["Gradient Descent", "Hessian", "Deep Learning"], "authorids": ["guyg@ias.edu", "danr@fb.com", "edyer@google.com"], "authors": ["Guy Gur-Ari", "Daniel A. Roberts", "Ethan Dyer"], "TL;DR": "For classification problems with k classes, we show that the gradient tends to live in a tiny, slowly-evolving subspace spanned by the eigenvectors corresponding to the k-largest eigenvalues of the Hessian.", "pdf": "/pdf/1a5c9d880096c438ce4e5908c4b58ad58d158bee.pdf", "paperhash": "gurari|gradient_descent_happens_in_a_tiny_subspace", "_bibtex": "@misc{\ngur-ari2019gradient,\ntitle={Gradient Descent Happens in a Tiny Subspace},\nauthor={Guy Gur-Ari and Daniel A. Roberts and Ethan Dyer},\nyear={2019},\nurl={https://openreview.net/forum?id=ByeTHsAqtX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper125/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311912968, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "ByeTHsAqtX", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper125/Authors", "ICLR.cc/2019/Conference/Paper125/Reviewers", "ICLR.cc/2019/Conference/Paper125/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper125/Authors", "ICLR.cc/2019/Conference/Paper125/Reviewers", "ICLR.cc/2019/Conference/Paper125/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311912968}}}, {"id": "rygrNWk80m", "original": null, "number": 4, "cdate": 1543004461159, "ddate": null, "tcdate": 1543004461159, "tmdate": 1543004461159, "tddate": null, "forum": "ByeTHsAqtX", "replyto": "Hke2kYg2nm", "invitation": "ICLR.cc/2019/Conference/-/Paper125/Official_Comment", "content": {"title": "Revised with a large amount of additional experimental evidence", "comment": "Thank you for your specific comments. We will address them here directly.\n\n1. It is true that having a large overlap does not immediately imply that the gradient lives in the top subspace. However, in all examples we checked, whenever the overlap was large the gradient did in fact live almost entirely in the top subspace. In the revised version we provided further empirical evidence of this relation, see Figure 11 in particular.\n\n2. Our measure of subspace overlap, equation (5), is a natural measure of overlap. Each subspace is defined by a projector onto that subspace. The Frobenius norm of the difference ||P-P'|| is perhaps the best known measure of distance between two subspaces (and is the natural metric on the Grassmanian, the manifold of linear subspaces ). Our equation (5) is simply related to this measure (analogous to the cosine distance for vectors, and literally the cosine distance for one dimensional vectors.), ||P-P'||_2 = 2k - 2Tr(P.P'). (We've updated the draft to make this clearer.) \n\nWe are interested in the overlap (Tr(P.P')/k) not the distance||P-P'|| since it measures the average fraction of a late-time vector that lives in the early-time subspace. Thus we decided on equation (5) as a very natural measure that also has this intuitive interpretation. As mentioned, we have clarified the relation to the distance measure ||P-P'|| in the text.\n\n3. When we use the term flat, we mean in terms of curvature. A direction where the Hessian vanishes but there is a nontrivial gradient is still flat, even though the parameters update in that direction. The point is that the amount the parameters will update will be constant. However, we appreciate that this may be confusing to readers, so we have added a clarifying footnote, footnote 1, regarding this point."}, "signatures": ["ICLR.cc/2019/Conference/Paper125/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper125/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper125/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Gradient Descent Happens in a Tiny Subspace", "abstract": "We show that in a variety of large-scale deep learning scenarios the gradient dynamically converges to a very small subspace after a short period of training. The subspace is spanned by a few top eigenvectors of the Hessian (equal to the number of classes in the dataset), and is mostly preserved over long periods of training. A simple argument then suggests that gradient descent may happen mostly in this subspace. We give an example of this effect in a solvable model of classification, and we comment on possible implications for optimization and learning.", "keywords": ["Gradient Descent", "Hessian", "Deep Learning"], "authorids": ["guyg@ias.edu", "danr@fb.com", "edyer@google.com"], "authors": ["Guy Gur-Ari", "Daniel A. Roberts", "Ethan Dyer"], "TL;DR": "For classification problems with k classes, we show that the gradient tends to live in a tiny, slowly-evolving subspace spanned by the eigenvectors corresponding to the k-largest eigenvalues of the Hessian.", "pdf": "/pdf/1a5c9d880096c438ce4e5908c4b58ad58d158bee.pdf", "paperhash": "gurari|gradient_descent_happens_in_a_tiny_subspace", "_bibtex": "@misc{\ngur-ari2019gradient,\ntitle={Gradient Descent Happens in a Tiny Subspace},\nauthor={Guy Gur-Ari and Daniel A. Roberts and Ethan Dyer},\nyear={2019},\nurl={https://openreview.net/forum?id=ByeTHsAqtX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper125/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621614685, "tddate": null, "super": null, "final": null, "reply": {"forum": "ByeTHsAqtX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper125/Authors", "ICLR.cc/2019/Conference/Paper125/Reviewers", "ICLR.cc/2019/Conference/Paper125/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper125/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper125/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper125/Authors|ICLR.cc/2019/Conference/Paper125/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper125/Reviewers", "ICLR.cc/2019/Conference/Paper125/Authors", "ICLR.cc/2019/Conference/Paper125/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621614685}}}, {"id": "HkghkW1ICQ", "original": null, "number": 3, "cdate": 1543004388316, "ddate": null, "tcdate": 1543004388316, "tmdate": 1543004388316, "tddate": null, "forum": "ByeTHsAqtX", "replyto": "BJxE_SOIhX", "invitation": "ICLR.cc/2019/Conference/-/Paper125/Official_Comment", "content": {"title": "Revised with much more evidence for number of classes", "comment": "We very much appreciate that you found our paper interesting and agree with our general arguments. We thank you for your very constructive feedback. Let us address your specific concerns below.\n\n1) First, we respectfully disagree that any noiseless k-class linear classifier has its gradient lie on a k-dimensional subspace. This can be verified in a more complicated toy model of a k-class classifier where the input is a mixture of k Gaussians. In the noiseless limit, there are k(k-1) nonzero eigenvalues, and it is only through complicatedly solving the dynamics that one can see how k(k-1) relaxes to k. (This result is outside the scope of this paper and is being written up as a follow up.) However, we have added some text referring to this future work (footnote 14) and also explain this point in the current paper. (Also note that k=2 is degenerate, and so one needs to analyze higher k to see this effect. Nevertheless, we still feel that the toy model is illustrative of the behavior of the parameters, gradient, and Hessian eigenvalues over training and helps one reason about thing---e.g. the expected behavior when changing learning rate---which we later confirm on deep models via experiment.)\n\nFor this reason, it's also not obvious that the final layer should only have rank k, and not rank k(k-1). (We should further note that you can study the k-class model with noise, and the noise lifts the rest of the Hessian eigenvalues to be nonzero.)\n\nOn the other hand, we agree that the Hessian of the final layer might dominate over the earlier ones. If the final layer is rank k, then the equations of back propagation mean that the gradient gets projected onto this lower rank subspace, regardless of the action of the other tensors. However, we think that this is an important phenomenon that deserves to be highlighted, since it controls very directly how the gradient can update the model.\n\n2) We very much agree that the eigenvalue distribution of the Hessian is important, and this was unfortunately relegated to Appendix B for space concerns (and since it was already explored in e.g. Sagun et al.). We have greatly expanded the discussion in this appendix. In particular, Appendix B shows an example eigenvalue distribution that highlights the k eigenvalue effect (Figure 4) and also a density plot of the top 1000 eigenvalues for CIFAR100 (Figure 5). The density plot is a histogram of log eigenvalue averaged over 200 realizations. It makes it very clear that there's some kind of change in the function describing the density distribution very near the mean 100th eigenvalue. In fact, this portion is fit very well by a Gaussian, which make it very suggestive to say that perhaps that the top-k distribution is log-normal---however far more evidence and thought would be needed in order to make this claim. In order to make our spectrum results clearer to readers, we have better highlighted Appendix B in the main text, and we have also expanded our discussion of this spectrum there.\n\nThe suggestion to plot the gradient over other size subspaces is a very good one, and we thank the reviewer for encouraging us to included these results. We have a very nice summary of these results in Figure 3, and we've written a new Appendix (labeled C, with the previous Appendix C bumped up to Appendix E) that's explicitly dedicated to this question. In particular Figures 9 and 10 contain the raw data summarized by Figure 3, showing that an important change happens at a subspace dimension that's one less than the number of classes in the dataset. In Figure 11, we show that the gradient overlaps across many of the different eigenvectors in the top subspace after some training, and that the gradient doesn't really extend to the next subspace. (Figure 1 also captures this point.)\n\nWe hope the reviewer finds these results to be as convincing as we do and that we have satisfied the criteria for being thorough."}, "signatures": ["ICLR.cc/2019/Conference/Paper125/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper125/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper125/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Gradient Descent Happens in a Tiny Subspace", "abstract": "We show that in a variety of large-scale deep learning scenarios the gradient dynamically converges to a very small subspace after a short period of training. The subspace is spanned by a few top eigenvectors of the Hessian (equal to the number of classes in the dataset), and is mostly preserved over long periods of training. A simple argument then suggests that gradient descent may happen mostly in this subspace. We give an example of this effect in a solvable model of classification, and we comment on possible implications for optimization and learning.", "keywords": ["Gradient Descent", "Hessian", "Deep Learning"], "authorids": ["guyg@ias.edu", "danr@fb.com", "edyer@google.com"], "authors": ["Guy Gur-Ari", "Daniel A. Roberts", "Ethan Dyer"], "TL;DR": "For classification problems with k classes, we show that the gradient tends to live in a tiny, slowly-evolving subspace spanned by the eigenvectors corresponding to the k-largest eigenvalues of the Hessian.", "pdf": "/pdf/1a5c9d880096c438ce4e5908c4b58ad58d158bee.pdf", "paperhash": "gurari|gradient_descent_happens_in_a_tiny_subspace", "_bibtex": "@misc{\ngur-ari2019gradient,\ntitle={Gradient Descent Happens in a Tiny Subspace},\nauthor={Guy Gur-Ari and Daniel A. Roberts and Ethan Dyer},\nyear={2019},\nurl={https://openreview.net/forum?id=ByeTHsAqtX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper125/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621614685, "tddate": null, "super": null, "final": null, "reply": {"forum": "ByeTHsAqtX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper125/Authors", "ICLR.cc/2019/Conference/Paper125/Reviewers", "ICLR.cc/2019/Conference/Paper125/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper125/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper125/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper125/Authors|ICLR.cc/2019/Conference/Paper125/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper125/Reviewers", "ICLR.cc/2019/Conference/Paper125/Authors", "ICLR.cc/2019/Conference/Paper125/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621614685}}}, {"id": "rJgLqx1I0Q", "original": null, "number": 2, "cdate": 1543004302256, "ddate": null, "tcdate": 1543004302256, "tmdate": 1543004302256, "tddate": null, "forum": "ByeTHsAqtX", "replyto": "rJesez7I3m", "invitation": "ICLR.cc/2019/Conference/-/Paper125/Official_Comment", "content": {"title": "Response to particular points", "comment": "We thank the reviewer for the kind words about our paper being interesting an easy to follow. We'd also like to respond to your individual comments.\n\n- We extensively investigated different learning rates over a few orders of magnitude, finding our results to be robust. Due to space and presentation issues, only one of these results is included (as the third element in Table 1). We've also included a plot, Figure 12 (a).\n\nThe behavior with respect to changing the learning rate is reasonably well captured by our Hessian toy model (equation 8). Since eta scales with time, decreasing the learning rate has the effect of increasing the magnitude of the large eigenvalues (which will decay over the course of training). (See, e.g. Figure 8.) We don't expect this to hold if the learning rate is too big (such that the model doesn't train), but seems robust as long as the model trains.\n\nSince it's clear that perhaps our way of summarizing experiments with a Table was perhaps confusing, we included Appendix D that shows the effect of a different learning rate on our results.\n\nResponses to particular points:\n\n- The spectrum of the Hessian was discussed in Appendix B, and we've expanded quite a bit on that discussion. Furthermore, in the main text when we derive our summary metric for the gradient being in the top subspace, equation (3), we discuss the typical Hessian spectrum (given results from Sagun et al. and our experiments) and explain why this can lead to the gradient living in a small subspace. However, it's not just enough to live in a small subspace independently across updates since the eigenvectors that span the top subspace might be mixing with the bulk eigenvectors. Our notion of \"living in a small subspace\" is meant to refer to one that is nearly preserved over the course of training. It is for this reason that we measure the overlap of subspaces at different time, finding that the top subspace is nearly preserved. (See Figures 2 and 3.)\n\n- We agree that it might be interesting to understand whether using natural gradient still leads to the same gradient and Hessian behavior. We would also like to emphasize that it's not necessarily obvious to us whether or not SGD is learning in the low-curvature directions or what role they play. For instance, it might be the case that they are a necessary part of the dynamical mechanism that creates the nearly-preserved subspace. If that were the case, for instance, then the approximate second-order algorithm we propose with equation (9) would fail due to projecting out important information.\n\n- We agree that the toy model is simple and is not the final story about why the subspace is preserved. (We have some upcoming work that studies the k-class extension. In this case, it's not true that the Hessian naturally has rank k. It starts as rank k(k-1) and then dynamically becomes rank k.) \n\nFurthermore, it is not true that the Hessian is constant over time (see, e.g. equation 18 for its time dependence), the eigenvalues decay like ~1/t, if t is the number of updates. As we commented on above, we find this analysis illuminating as it let us understand the effect of changing the learning rate (which we then confirmed extensively via experiment).\n\nWe understand that neural networks are not convex, but by simulating the k-class extension of our toy model (see section 3.1) and identifying similar behavior to the deep models we studied, we think of the toy model as a helpful illustrative example, and not the definitive explanation. We have tried to add text to make it clearer how we regard the toy model.\n\n-Finally, we agree that the preservation of the subspace is perhaps the most interesting part of this work. We have added an additional appendix (Appendix C) exploring this phenomenon in more detail, summarizing these results in Figure 3. We hope that the reviewer will appreciate these results in particular."}, "signatures": ["ICLR.cc/2019/Conference/Paper125/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper125/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper125/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Gradient Descent Happens in a Tiny Subspace", "abstract": "We show that in a variety of large-scale deep learning scenarios the gradient dynamically converges to a very small subspace after a short period of training. The subspace is spanned by a few top eigenvectors of the Hessian (equal to the number of classes in the dataset), and is mostly preserved over long periods of training. A simple argument then suggests that gradient descent may happen mostly in this subspace. We give an example of this effect in a solvable model of classification, and we comment on possible implications for optimization and learning.", "keywords": ["Gradient Descent", "Hessian", "Deep Learning"], "authorids": ["guyg@ias.edu", "danr@fb.com", "edyer@google.com"], "authors": ["Guy Gur-Ari", "Daniel A. Roberts", "Ethan Dyer"], "TL;DR": "For classification problems with k classes, we show that the gradient tends to live in a tiny, slowly-evolving subspace spanned by the eigenvectors corresponding to the k-largest eigenvalues of the Hessian.", "pdf": "/pdf/1a5c9d880096c438ce4e5908c4b58ad58d158bee.pdf", "paperhash": "gurari|gradient_descent_happens_in_a_tiny_subspace", "_bibtex": "@misc{\ngur-ari2019gradient,\ntitle={Gradient Descent Happens in a Tiny Subspace},\nauthor={Guy Gur-Ari and Daniel A. Roberts and Ethan Dyer},\nyear={2019},\nurl={https://openreview.net/forum?id=ByeTHsAqtX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper125/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621614685, "tddate": null, "super": null, "final": null, "reply": {"forum": "ByeTHsAqtX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper125/Authors", "ICLR.cc/2019/Conference/Paper125/Reviewers", "ICLR.cc/2019/Conference/Paper125/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper125/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper125/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper125/Authors|ICLR.cc/2019/Conference/Paper125/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper125/Reviewers", "ICLR.cc/2019/Conference/Paper125/Authors", "ICLR.cc/2019/Conference/Paper125/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621614685}}}, {"id": "S1gpLgJUA7", "original": null, "number": 1, "cdate": 1543004245460, "ddate": null, "tcdate": 1543004245460, "tmdate": 1543004245460, "tddate": null, "forum": "ByeTHsAqtX", "replyto": "ByeTHsAqtX", "invitation": "ICLR.cc/2019/Conference/-/Paper125/Official_Comment", "content": {"title": "Summary of revisions and responses", "comment": "We are grateful to the reviewers for their thoughtful comments. We have tried to address their concerns directly below. We have also updated the draft (mostly by vastly expanding the Appendices) to include additional analysis, experiments, and hopefully clarity.\n\nIn particular, we've including the following important experiment evidence addressing the specific concerns of the reviewers:\n\n-Figure 3 clearly shows that the relevant dimension for the subspace freezing is related to the number of classes in the dataset. Appendix C is a more in depth discussion of this, with additional experimental evidence.\n-Appendix B includes an extended discussion of the Hessian spectrum. Let us highlight some important plots. Figure 4 is an example spectrum for a single realization, with the eigenvalues in the top subspace now labeled. Figure 5 shows a realistic model on a dataset with a different number of classes than MNIST and CIFAR10 (CIFAR100) averaged over 200 realizations. The feature around the mean 100th eigenvalue is clear.\n-Figures 6 and 7 plot a particular eigenvector at different points in training, showing that such eigenvectors both evolve and are not dominated by any particular parameter or layer.\n-Figure 11 makes it clear that the gradient is not actually an eigenvector of the Hessian as some reviewers were concerned about. It also makes it clear that the gradient overlaps are nicely spread over the top subspace and evolving nontrivially in time.\n-Appendix D contains additional experimental results for different hyperparameters, including showing robustness to changing the learning rate as Reviewer 1 requested. \n-We've tried to add clarifying comments to the draft about the utility of the toy model, see e.g. footnote 14 in particular.\n-We also want to highlight that Figure 2 (as well as the new Figure 3 and Figure 10) contains results for the freezing of the subspace for the ResNet 18 and are consistent with the claims of the paper.\n\nAs a general comment, Reviewers 2 and 3 were concerned that the experimental evidence is lacking for our observed phenomenon. We hope this additions will help put those concerns to rest. We hadn't included these results before because we didn't appreciate that we can expand our Appendices in this way. Naturally, in the experimental phase of the project, we had conducted many more experiments than could possibly fit in the paper. For clarity, we had thought to focus on three illustrative examples, a fully-connected network, a simple convolutional network, and a ResNet-18. Rather than clutter the paper with an overabundance of plots, we developed a summary metric for our phenomenon, the overlap described in equation (3) and summarized a variety of additional experiments (which still do not cover the full number of experiments we performed). Of course when performing these additional experiments, we checked that the effect illustrated directly in Figure 1 also covers these other scenarios, and we offer the explanation around equation (4) as to why our summary metric (3) is a reasonable metric. Nevertheless, as we stated above, we created Appendix D with the fractional of gradient in top subspace plots for many of the experiments collected in Table 1. We hope these results will be useful in considering the robustness of the phenomena we discuss.\n\nFurthermore, all three reviewers found our toy model unsatisfying. We would like to highlight that we interpret the toy model to be illustrative (and was helpful in understanding how our results might change if we change hyperparameters such as learning rate) and not a definitive explanation. We find it helpful to have a model we can solve that also illustrates many of the phenomenon we see in the more complicated models. However, we have added text to the body making it clear that we don't think of the toy model as a complete explanation. Finally, we have added a reference (footnote 14) that talks about upcoming work, the k-class extension, giving one part of the result that explains why having k eigenvalues (instead of k(k-1) as should be naively expected by symmetries) is a nontrivial result.\n\nWe hope that these revisions are helpful in making our paper clearer and will convince the reviewers that our results are robust across many experiments. Please feel free to request additional clarifications or explanations."}, "signatures": ["ICLR.cc/2019/Conference/Paper125/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper125/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper125/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Gradient Descent Happens in a Tiny Subspace", "abstract": "We show that in a variety of large-scale deep learning scenarios the gradient dynamically converges to a very small subspace after a short period of training. The subspace is spanned by a few top eigenvectors of the Hessian (equal to the number of classes in the dataset), and is mostly preserved over long periods of training. A simple argument then suggests that gradient descent may happen mostly in this subspace. We give an example of this effect in a solvable model of classification, and we comment on possible implications for optimization and learning.", "keywords": ["Gradient Descent", "Hessian", "Deep Learning"], "authorids": ["guyg@ias.edu", "danr@fb.com", "edyer@google.com"], "authors": ["Guy Gur-Ari", "Daniel A. Roberts", "Ethan Dyer"], "TL;DR": "For classification problems with k classes, we show that the gradient tends to live in a tiny, slowly-evolving subspace spanned by the eigenvectors corresponding to the k-largest eigenvalues of the Hessian.", "pdf": "/pdf/1a5c9d880096c438ce4e5908c4b58ad58d158bee.pdf", "paperhash": "gurari|gradient_descent_happens_in_a_tiny_subspace", "_bibtex": "@misc{\ngur-ari2019gradient,\ntitle={Gradient Descent Happens in a Tiny Subspace},\nauthor={Guy Gur-Ari and Daniel A. Roberts and Ethan Dyer},\nyear={2019},\nurl={https://openreview.net/forum?id=ByeTHsAqtX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper125/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621614685, "tddate": null, "super": null, "final": null, "reply": {"forum": "ByeTHsAqtX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper125/Authors", "ICLR.cc/2019/Conference/Paper125/Reviewers", "ICLR.cc/2019/Conference/Paper125/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper125/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper125/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper125/Authors|ICLR.cc/2019/Conference/Paper125/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper125/Reviewers", "ICLR.cc/2019/Conference/Paper125/Authors", "ICLR.cc/2019/Conference/Paper125/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621614685}}}, {"id": "Hke2kYg2nm", "original": null, "number": 3, "cdate": 1541306595718, "ddate": null, "tcdate": 1541306595718, "tmdate": 1541534260725, "tddate": null, "forum": "ByeTHsAqtX", "replyto": "ByeTHsAqtX", "invitation": "ICLR.cc/2019/Conference/-/Paper125/Official_Review", "content": {"title": "This paper describes an interesting phenomenon, but some of the experimental evidence is a bit lacking. ", "review": "This paper describes an interesting phenomenon: that most of the learning happens in a small subspace. However, the experimental evidence presented in this paper is a bit lacking. The authors also cook up a toy example on which gradient descent exhibits similar behavior. Here are a few detailed comments:\n\n1. The Hessian overlap metric is suitable for showing the gradient lies in an invariant subspace of the Hessian, but does not show it lies in the dominant invariant subspace. \n2. There are well-established notions of distances between subspaces in linear algebra, and I suggest the authors comment on the connection between their notion of overlap between subspaces and these established notions.\n3. The authors make a few statements along the lines of ``the Hessian is small, so the objective is flat''. This is a bit misleading as it is possible for the gradient to be large but the Hessian to be small.", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper125/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Gradient Descent Happens in a Tiny Subspace", "abstract": "We show that in a variety of large-scale deep learning scenarios the gradient dynamically converges to a very small subspace after a short period of training. The subspace is spanned by a few top eigenvectors of the Hessian (equal to the number of classes in the dataset), and is mostly preserved over long periods of training. A simple argument then suggests that gradient descent may happen mostly in this subspace. We give an example of this effect in a solvable model of classification, and we comment on possible implications for optimization and learning.", "keywords": ["Gradient Descent", "Hessian", "Deep Learning"], "authorids": ["guyg@ias.edu", "danr@fb.com", "edyer@google.com"], "authors": ["Guy Gur-Ari", "Daniel A. Roberts", "Ethan Dyer"], "TL;DR": "For classification problems with k classes, we show that the gradient tends to live in a tiny, slowly-evolving subspace spanned by the eigenvectors corresponding to the k-largest eigenvalues of the Hessian.", "pdf": "/pdf/1a5c9d880096c438ce4e5908c4b58ad58d158bee.pdf", "paperhash": "gurari|gradient_descent_happens_in_a_tiny_subspace", "_bibtex": "@misc{\ngur-ari2019gradient,\ntitle={Gradient Descent Happens in a Tiny Subspace},\nauthor={Guy Gur-Ari and Daniel A. Roberts and Ethan Dyer},\nyear={2019},\nurl={https://openreview.net/forum?id=ByeTHsAqtX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper125/Official_Review", "cdate": 1542234532384, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "ByeTHsAqtX", "replyto": "ByeTHsAqtX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper125/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335654709, "tmdate": 1552335654709, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper125/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "rJesez7I3m", "original": null, "number": 1, "cdate": 1540923890941, "ddate": null, "tcdate": 1540923890941, "tmdate": 1541534260526, "tddate": null, "forum": "ByeTHsAqtX", "replyto": "ByeTHsAqtX", "invitation": "ICLR.cc/2019/Conference/-/Paper125/Official_Review", "content": {"title": "Interesting paper with supported experiments", "review": "This paper shows that gradient descent mostly happens in a tiny subspace which is spanned by the top eigenvectors of the Hessian. Empirical results are shown to support the claim. This finding is interesting and provides us some insights to design more efficient optimization algorithms. Overall, this paper is interesting and easy to follow. \n\nThe experiments in section 2 do a decent job supporting the claim that gradient descent happens in a tiny subspace and the subspace is mostly preserved over long periods of training. However, I would like to add a couple more points to the discussion: \n\n- It's not surprising that the magnitude of gradient is larger in the high curvature directions, which means that the learning would always first happen in top subspace if the learning rate is small enough. It would be interesting to tune the parameter of learning rate to see if the phenomena would occur across different learning rate (especially large learning rate).\n- The argument of gradient descent happening in a tiny space is quite obvious if the Hessian has only a few large eigenvalues. Therefore, it would be interesting to discuss the spectrum of the Hessian a little bit.\n- Contrary to plain gradient descent, natural gradient is able to learn low curvature directions (small eigenvalues). It would be interesting to show some experiments with natural gradient methods.\n\nFollowing section 2, the authors give a toy model to further backup their claims. However, I find the example is too restrictive and may not explain why the subspace would be preserved over the training. If I understand right, the loss function of the toy model is convex and Hessian is a constant over time. For this kind of toy model, the Hessian (or equivalently the Fisher matrix) only depends on the input distribution, so it's easy to see that the Hessian would be low-rank and preserved throughout the training. However, neural networks are highly non-convex, so it's unclear to me whether the implications of the toy model would generalize. I encourage the authors to analyze more complicated models. \n\nTo summarize, I think this paper is interesting and well-written. However, it lacks convincing explanation why the subspace would preserve over the training (to me, it's more interesting than the point that gradient descent happens in tiny subspace). Anyway, it is not completely reasonable to expect all such possible discussions to take place at once. ", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper125/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Gradient Descent Happens in a Tiny Subspace", "abstract": "We show that in a variety of large-scale deep learning scenarios the gradient dynamically converges to a very small subspace after a short period of training. The subspace is spanned by a few top eigenvectors of the Hessian (equal to the number of classes in the dataset), and is mostly preserved over long periods of training. A simple argument then suggests that gradient descent may happen mostly in this subspace. We give an example of this effect in a solvable model of classification, and we comment on possible implications for optimization and learning.", "keywords": ["Gradient Descent", "Hessian", "Deep Learning"], "authorids": ["guyg@ias.edu", "danr@fb.com", "edyer@google.com"], "authors": ["Guy Gur-Ari", "Daniel A. Roberts", "Ethan Dyer"], "TL;DR": "For classification problems with k classes, we show that the gradient tends to live in a tiny, slowly-evolving subspace spanned by the eigenvectors corresponding to the k-largest eigenvalues of the Hessian.", "pdf": "/pdf/1a5c9d880096c438ce4e5908c4b58ad58d158bee.pdf", "paperhash": "gurari|gradient_descent_happens_in_a_tiny_subspace", "_bibtex": "@misc{\ngur-ari2019gradient,\ntitle={Gradient Descent Happens in a Tiny Subspace},\nauthor={Guy Gur-Ari and Daniel A. Roberts and Ethan Dyer},\nyear={2019},\nurl={https://openreview.net/forum?id=ByeTHsAqtX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper125/Official_Review", "cdate": 1542234532384, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "ByeTHsAqtX", "replyto": "ByeTHsAqtX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper125/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335654709, "tmdate": 1552335654709, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper125/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "BJxE_SOIhX", "original": null, "number": 2, "cdate": 1540945260288, "ddate": null, "tcdate": 1540945260288, "tmdate": 1541534260324, "tddate": null, "forum": "ByeTHsAqtX", "replyto": "ByeTHsAqtX", "invitation": "ICLR.cc/2019/Conference/-/Paper125/Official_Review", "content": {"title": "Agreed with small subspace but not with number of classes. Needs a more thorough study.", "review": "The authors build on recent works that study the spectrum of the Hessian of deep networks (e.g. Sagun et al). Previous work argues that Hessian is approximately low-rank i.e. there are few large eigenvalues and many small eigenvalues. This work argues that after some training, large eigenvectors of the Hessian converges to a subspace and stays there.\n\nIntuitively, this papers message makes sense and is interesting. I also agree with the tiny subspace argument in the paper. However, I am not convinced by a couple of things and I believe further evidence is necessary. \n\n1) Authors claim the top subspace has same rank k (where k=number of classes) and backs this up with linear classifier with 2 class (toy model). It is clear that any noiseless k-class linear classifier has its gradient lie on a k dimensional subspace. Similarly, for a deep network, I agree with hessian of the final layer will be rank k however earlier layers can have different ranks as a function of complexity of the lower level representations. My worry is that perhaps the Hessian of the final layer is somehow dominating over the other ones. Ideally, I would like to see analysis of individual layers to reach a conclusion. Is first layer also approximately rank k?\n\n2) Similarly, we need to see the eigenvalue distribution of the Hessian. Say there is a single very large eigenvalue and all others are small. The same claim would still hold. So it is not clear if number of classes really plays a role from the available experiments. This can indeed happen is the classes are correlated for instance. Authors can perhaps plot gradient over top k/2 subspace to reveal if their claim is specific to number of classes.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper125/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Gradient Descent Happens in a Tiny Subspace", "abstract": "We show that in a variety of large-scale deep learning scenarios the gradient dynamically converges to a very small subspace after a short period of training. The subspace is spanned by a few top eigenvectors of the Hessian (equal to the number of classes in the dataset), and is mostly preserved over long periods of training. A simple argument then suggests that gradient descent may happen mostly in this subspace. We give an example of this effect in a solvable model of classification, and we comment on possible implications for optimization and learning.", "keywords": ["Gradient Descent", "Hessian", "Deep Learning"], "authorids": ["guyg@ias.edu", "danr@fb.com", "edyer@google.com"], "authors": ["Guy Gur-Ari", "Daniel A. Roberts", "Ethan Dyer"], "TL;DR": "For classification problems with k classes, we show that the gradient tends to live in a tiny, slowly-evolving subspace spanned by the eigenvectors corresponding to the k-largest eigenvalues of the Hessian.", "pdf": "/pdf/1a5c9d880096c438ce4e5908c4b58ad58d158bee.pdf", "paperhash": "gurari|gradient_descent_happens_in_a_tiny_subspace", "_bibtex": "@misc{\ngur-ari2019gradient,\ntitle={Gradient Descent Happens in a Tiny Subspace},\nauthor={Guy Gur-Ari and Daniel A. Roberts and Ethan Dyer},\nyear={2019},\nurl={https://openreview.net/forum?id=ByeTHsAqtX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper125/Official_Review", "cdate": 1542234532384, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "ByeTHsAqtX", "replyto": "ByeTHsAqtX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper125/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335654709, "tmdate": 1552335654709, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper125/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 10}