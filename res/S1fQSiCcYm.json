{"notes": [{"id": "S1fQSiCcYm", "original": "HkgqhPiF_m", "number": 73, "cdate": 1538087738937, "ddate": null, "tcdate": 1538087738937, "tmdate": 1551478715786, "tddate": null, "forum": "S1fQSiCcYm", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Understanding and Improving Interpolation in Autoencoders via an Adversarial Regularizer", "abstract": "Autoencoders provide a powerful framework for learning compressed representations by encoding all of the information needed to reconstruct a data point in a latent code. In some cases, autoencoders can \"interpolate\": By decoding the convex combination of the latent codes for two datapoints, the autoencoder can produce an output which semantically mixes characteristics from the datapoints. In this paper, we propose a regularization procedure which encourages interpolated outputs to appear more realistic by fooling a critic network which has been trained to recover the mixing coefficient from interpolated data. We then develop a simple benchmark task where we can quantitatively measure the extent to which various autoencoders can interpolate and show that our regularizer dramatically improves interpolation in this setting. We also demonstrate empirically that our regularizer produces latent codes which are more effective on downstream tasks, suggesting a possible link between interpolation abilities and learning useful representations.", "paperhash": "berthelot|understanding_and_improving_interpolation_in_autoencoders_via_an_adversarial_regularizer", "TL;DR": "We propose a regularizer that improves interpolation and autoencoders and show that it also improves the learned representation for downstream tasks.", "authorids": ["dberth@google.com", "craffel@gmail.com", "aurkor@google.com", "goodfellow@google.com"], "authors": ["David Berthelot*", "Colin Raffel*", "Aurko Roy", "Ian Goodfellow"], "keywords": ["autoencoders", "interpolation", "unsupervised learning", "representation learning", "adversarial learning"], "pdf": "/pdf/6f8c41dcd45651f410da3b3fbe9d0fbdfe7765cf.pdf", "_bibtex": "@inproceedings{\nberthelot*2018understanding,\ntitle={Understanding and Improving Interpolation in Autoencoders via an Adversarial Regularizer},\nauthor={David Berthelot* and Colin Raffel* and Aurko Roy and Ian Goodfellow},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1fQSiCcYm},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 22, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "rklsL14-x4", "original": null, "number": 1, "cdate": 1544793938727, "ddate": null, "tcdate": 1544793938727, "tmdate": 1545354472540, "tddate": null, "forum": "S1fQSiCcYm", "replyto": "S1fQSiCcYm", "invitation": "ICLR.cc/2019/Conference/-/Paper73/Meta_Review", "content": {"metareview": "The reviewers have reached a consensus that this paper is very interesting and add insights into interpolation in autoencoders.", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Accept (Poster)", "title": "Strong paper"}, "signatures": ["ICLR.cc/2019/Conference/Paper73/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper73/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understanding and Improving Interpolation in Autoencoders via an Adversarial Regularizer", "abstract": "Autoencoders provide a powerful framework for learning compressed representations by encoding all of the information needed to reconstruct a data point in a latent code. In some cases, autoencoders can \"interpolate\": By decoding the convex combination of the latent codes for two datapoints, the autoencoder can produce an output which semantically mixes characteristics from the datapoints. In this paper, we propose a regularization procedure which encourages interpolated outputs to appear more realistic by fooling a critic network which has been trained to recover the mixing coefficient from interpolated data. We then develop a simple benchmark task where we can quantitatively measure the extent to which various autoencoders can interpolate and show that our regularizer dramatically improves interpolation in this setting. We also demonstrate empirically that our regularizer produces latent codes which are more effective on downstream tasks, suggesting a possible link between interpolation abilities and learning useful representations.", "paperhash": "berthelot|understanding_and_improving_interpolation_in_autoencoders_via_an_adversarial_regularizer", "TL;DR": "We propose a regularizer that improves interpolation and autoencoders and show that it also improves the learned representation for downstream tasks.", "authorids": ["dberth@google.com", "craffel@gmail.com", "aurkor@google.com", "goodfellow@google.com"], "authors": ["David Berthelot*", "Colin Raffel*", "Aurko Roy", "Ian Goodfellow"], "keywords": ["autoencoders", "interpolation", "unsupervised learning", "representation learning", "adversarial learning"], "pdf": "/pdf/6f8c41dcd45651f410da3b3fbe9d0fbdfe7765cf.pdf", "_bibtex": "@inproceedings{\nberthelot*2018understanding,\ntitle={Understanding and Improving Interpolation in Autoencoders via an Adversarial Regularizer},\nauthor={David Berthelot* and Colin Raffel* and Aurko Roy and Ian Goodfellow},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1fQSiCcYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper73/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353345735, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1fQSiCcYm", "replyto": "S1fQSiCcYm", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper73/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper73/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper73/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353345735}}}, {"id": "H1egXXwd14", "original": null, "number": 19, "cdate": 1544217368377, "ddate": null, "tcdate": 1544217368377, "tmdate": 1544217368377, "tddate": null, "forum": "S1fQSiCcYm", "replyto": "H1l1iK8OyE", "invitation": "ICLR.cc/2019/Conference/-/Paper73/Official_Comment", "content": {"title": "Thanks!", "comment": "Thanks for engaging in discussion with us, suggesting additional experiments, and being open to updating your review."}, "signatures": ["ICLR.cc/2019/Conference/Paper73/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper73/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper73/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understanding and Improving Interpolation in Autoencoders via an Adversarial Regularizer", "abstract": "Autoencoders provide a powerful framework for learning compressed representations by encoding all of the information needed to reconstruct a data point in a latent code. In some cases, autoencoders can \"interpolate\": By decoding the convex combination of the latent codes for two datapoints, the autoencoder can produce an output which semantically mixes characteristics from the datapoints. In this paper, we propose a regularization procedure which encourages interpolated outputs to appear more realistic by fooling a critic network which has been trained to recover the mixing coefficient from interpolated data. We then develop a simple benchmark task where we can quantitatively measure the extent to which various autoencoders can interpolate and show that our regularizer dramatically improves interpolation in this setting. We also demonstrate empirically that our regularizer produces latent codes which are more effective on downstream tasks, suggesting a possible link between interpolation abilities and learning useful representations.", "paperhash": "berthelot|understanding_and_improving_interpolation_in_autoencoders_via_an_adversarial_regularizer", "TL;DR": "We propose a regularizer that improves interpolation and autoencoders and show that it also improves the learned representation for downstream tasks.", "authorids": ["dberth@google.com", "craffel@gmail.com", "aurkor@google.com", "goodfellow@google.com"], "authors": ["David Berthelot*", "Colin Raffel*", "Aurko Roy", "Ian Goodfellow"], "keywords": ["autoencoders", "interpolation", "unsupervised learning", "representation learning", "adversarial learning"], "pdf": "/pdf/6f8c41dcd45651f410da3b3fbe9d0fbdfe7765cf.pdf", "_bibtex": "@inproceedings{\nberthelot*2018understanding,\ntitle={Understanding and Improving Interpolation in Autoencoders via an Adversarial Regularizer},\nauthor={David Berthelot* and Colin Raffel* and Aurko Roy and Ian Goodfellow},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1fQSiCcYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper73/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621612785, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1fQSiCcYm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper73/Authors", "ICLR.cc/2019/Conference/Paper73/Reviewers", "ICLR.cc/2019/Conference/Paper73/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper73/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper73/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper73/Authors|ICLR.cc/2019/Conference/Paper73/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper73/Reviewers", "ICLR.cc/2019/Conference/Paper73/Authors", "ICLR.cc/2019/Conference/Paper73/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621612785}}}, {"id": "HJeuQMG5n7", "original": null, "number": 3, "cdate": 1541181983663, "ddate": null, "tcdate": 1541181983663, "tmdate": 1544215417823, "tddate": null, "forum": "S1fQSiCcYm", "replyto": "S1fQSiCcYm", "invitation": "ICLR.cc/2019/Conference/-/Paper73/Official_Review", "content": {"title": "Regularize interpolation or regularize manifold?", "review": "Main idea:\nThis paper investigates the desiderata for a successful interpolation:\n1) Interpolation looks realistic;\n2) The interpolation path is semantically smooth. \nAn adversarial regularizer is proposed to achieve 1), and in practice 2) may also satisfied.  \nTo evaluate the method, they introduce a synthetic dataset with line images and compare with different autoencoder methods without the interpolation regularization.\nFor real data, they show that the interpolation regularized autoencoder (i.e. ACAI) leads to a better unsupervised representation.\n\nQuestions:\n1. Do we really need every interpolated point to be realistic (i.e. similar to a data point in the train-set)? I believe that there exists an interpolation between two totally different objects can never be observed.  \n2. Do we need interpolation points to form a semantically smooth morphing? I guess this is a desired property for continuous generators, but it seems not necessary in general.\n3. The gamma in the 2nd term in (1) is confusing. If gamma = 1, I understand it forces to predict alpha = 0 since x is real. But if gamma < 1, the average in data space may be very blurry thus not realistic at all. How does gamma affect the optimization?\n4. ACAI looks very similar to LSGAN: by giving \"0\" label to real data and \"alpha\" label to fake data; in LSGAN, alpha = 1.\nHave you tested a LSGAN like regularizer? \n5. The baselines are not representative: since ACAI introduces an adversarial regularizer, you should compare with other GAN techniques induced regularizers, such as WGAN regularized autoencoder. \n\nAfter rebuttal:\nSee the long discussion below. I tend to believe that a good interpolation is not only a way to do sanity check but also a nice property to explicitly control in representation learning.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper73/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Understanding and Improving Interpolation in Autoencoders via an Adversarial Regularizer", "abstract": "Autoencoders provide a powerful framework for learning compressed representations by encoding all of the information needed to reconstruct a data point in a latent code. In some cases, autoencoders can \"interpolate\": By decoding the convex combination of the latent codes for two datapoints, the autoencoder can produce an output which semantically mixes characteristics from the datapoints. In this paper, we propose a regularization procedure which encourages interpolated outputs to appear more realistic by fooling a critic network which has been trained to recover the mixing coefficient from interpolated data. We then develop a simple benchmark task where we can quantitatively measure the extent to which various autoencoders can interpolate and show that our regularizer dramatically improves interpolation in this setting. We also demonstrate empirically that our regularizer produces latent codes which are more effective on downstream tasks, suggesting a possible link between interpolation abilities and learning useful representations.", "paperhash": "berthelot|understanding_and_improving_interpolation_in_autoencoders_via_an_adversarial_regularizer", "TL;DR": "We propose a regularizer that improves interpolation and autoencoders and show that it also improves the learned representation for downstream tasks.", "authorids": ["dberth@google.com", "craffel@gmail.com", "aurkor@google.com", "goodfellow@google.com"], "authors": ["David Berthelot*", "Colin Raffel*", "Aurko Roy", "Ian Goodfellow"], "keywords": ["autoencoders", "interpolation", "unsupervised learning", "representation learning", "adversarial learning"], "pdf": "/pdf/6f8c41dcd45651f410da3b3fbe9d0fbdfe7765cf.pdf", "_bibtex": "@inproceedings{\nberthelot*2018understanding,\ntitle={Understanding and Improving Interpolation in Autoencoders via an Adversarial Regularizer},\nauthor={David Berthelot* and Colin Raffel* and Aurko Roy and Ian Goodfellow},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1fQSiCcYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper73/Official_Review", "cdate": 1542234543943, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "S1fQSiCcYm", "replyto": "S1fQSiCcYm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper73/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335643105, "tmdate": 1552335643105, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper73/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "H1l1iK8OyE", "original": null, "number": 18, "cdate": 1544214934716, "ddate": null, "tcdate": 1544214934716, "tmdate": 1544214934716, "tddate": null, "forum": "S1fQSiCcYm", "replyto": "B1gHm_rw14", "invitation": "ICLR.cc/2019/Conference/-/Paper73/Official_Comment", "content": {"title": "Interesting results", "comment": "I appreciate your quick experiments for addressing my concerns!\n\nNow I'm convinced ACAI is a quite interesting method: \nIt seems very important for the critic to see reconstructions and interpolants only. I tend to believe this somehow smooth the latent space, while LRAE doesn't make a full use of the encoder since by increasing d_z the performance only increases marginally. \n\nI believe ACAI deserves more visibility to our community. "}, "signatures": ["ICLR.cc/2019/Conference/Paper73/AnonReviewer3"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper73/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper73/AnonReviewer3", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understanding and Improving Interpolation in Autoencoders via an Adversarial Regularizer", "abstract": "Autoencoders provide a powerful framework for learning compressed representations by encoding all of the information needed to reconstruct a data point in a latent code. In some cases, autoencoders can \"interpolate\": By decoding the convex combination of the latent codes for two datapoints, the autoencoder can produce an output which semantically mixes characteristics from the datapoints. In this paper, we propose a regularization procedure which encourages interpolated outputs to appear more realistic by fooling a critic network which has been trained to recover the mixing coefficient from interpolated data. We then develop a simple benchmark task where we can quantitatively measure the extent to which various autoencoders can interpolate and show that our regularizer dramatically improves interpolation in this setting. We also demonstrate empirically that our regularizer produces latent codes which are more effective on downstream tasks, suggesting a possible link between interpolation abilities and learning useful representations.", "paperhash": "berthelot|understanding_and_improving_interpolation_in_autoencoders_via_an_adversarial_regularizer", "TL;DR": "We propose a regularizer that improves interpolation and autoencoders and show that it also improves the learned representation for downstream tasks.", "authorids": ["dberth@google.com", "craffel@gmail.com", "aurkor@google.com", "goodfellow@google.com"], "authors": ["David Berthelot*", "Colin Raffel*", "Aurko Roy", "Ian Goodfellow"], "keywords": ["autoencoders", "interpolation", "unsupervised learning", "representation learning", "adversarial learning"], "pdf": "/pdf/6f8c41dcd45651f410da3b3fbe9d0fbdfe7765cf.pdf", "_bibtex": "@inproceedings{\nberthelot*2018understanding,\ntitle={Understanding and Improving Interpolation in Autoencoders via an Adversarial Regularizer},\nauthor={David Berthelot* and Colin Raffel* and Aurko Roy and Ian Goodfellow},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1fQSiCcYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper73/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621612785, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1fQSiCcYm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper73/Authors", "ICLR.cc/2019/Conference/Paper73/Reviewers", "ICLR.cc/2019/Conference/Paper73/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper73/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper73/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper73/Authors|ICLR.cc/2019/Conference/Paper73/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper73/Reviewers", "ICLR.cc/2019/Conference/Paper73/Authors", "ICLR.cc/2019/Conference/Paper73/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621612785}}}, {"id": "B1gHm_rw14", "original": null, "number": 17, "cdate": 1544144924593, "ddate": null, "tcdate": 1544144924593, "tmdate": 1544144924593, "tddate": null, "forum": "S1fQSiCcYm", "replyto": "SyefNNz8kN", "invitation": "ICLR.cc/2019/Conference/-/Paper73/Official_Comment", "content": {"title": "LSGAN regularized autoencoder results, part 2", "comment": "We have now tested an autoencoder using this regularizer on our representation learning experiments. As a reminder, we first train an autoencoder on MNIST, SVHN, and CIFAR-10. We then use the latent codes as a learned representation for a single-layer classifier and report the accuracy of the classifier on the test set. Denoting the LSGAN Regularized Autoencoder as \"LRAE\", we obtained the following results (with Baseline and ACAI results included for reference):\n\n                                     | Baseline | ACAI  | LRAE\nMNIST, d_z = 32          | 94.90      | 98.25 | 95.66 \nMNIST, d_z = 256        | 93.94      | 99.00 | 96.94\nSVHN, d_z = 32           | 26.21      | 34.47 | 22.49\nSVHN, d_z = 256         | 22.74      | 85.14 | 30.77\nCIFAR-10, d_z = 256   | 47.92      | 52.77 | 47.99\nCIFAR-10, d_z = 1024 | 51.62      | 63.99 | 50.26\n\nThe LRAE improves over the baseline in some cases, but not consistently. Since the additional loss term/critic in LRAE is satisfied by making reconstructions more realistic, we hypothesize that it does not change the structure of the latent space. This would explain why it does not generally improve representation learning performance. In contrast, ACAI has the specific goal of modifying the structure of the latent space by making interpolants appear more like reconstructions. This results in improved representation learning performance.\n\nWe believe that these additional experiments further strengthen our claim that improving interpolation behavior can also produced a better learned representation. We also believe this addresses your main concern: \n> My main concern still remains: is the good representation coming from a GAN regularized autoencoder (since your interpolation formulation is very similar to that of LSGAN) or because of the improved interpolation (then it's your contribution)? \nThe results of these experiments definitively show that the improved performance comes from the specific form and goal of ACAI and not simply from that our approach uses a critic. We hope this convinces you of the merit of our submission."}, "signatures": ["ICLR.cc/2019/Conference/Paper73/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper73/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper73/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understanding and Improving Interpolation in Autoencoders via an Adversarial Regularizer", "abstract": "Autoencoders provide a powerful framework for learning compressed representations by encoding all of the information needed to reconstruct a data point in a latent code. In some cases, autoencoders can \"interpolate\": By decoding the convex combination of the latent codes for two datapoints, the autoencoder can produce an output which semantically mixes characteristics from the datapoints. In this paper, we propose a regularization procedure which encourages interpolated outputs to appear more realistic by fooling a critic network which has been trained to recover the mixing coefficient from interpolated data. We then develop a simple benchmark task where we can quantitatively measure the extent to which various autoencoders can interpolate and show that our regularizer dramatically improves interpolation in this setting. We also demonstrate empirically that our regularizer produces latent codes which are more effective on downstream tasks, suggesting a possible link between interpolation abilities and learning useful representations.", "paperhash": "berthelot|understanding_and_improving_interpolation_in_autoencoders_via_an_adversarial_regularizer", "TL;DR": "We propose a regularizer that improves interpolation and autoencoders and show that it also improves the learned representation for downstream tasks.", "authorids": ["dberth@google.com", "craffel@gmail.com", "aurkor@google.com", "goodfellow@google.com"], "authors": ["David Berthelot*", "Colin Raffel*", "Aurko Roy", "Ian Goodfellow"], "keywords": ["autoencoders", "interpolation", "unsupervised learning", "representation learning", "adversarial learning"], "pdf": "/pdf/6f8c41dcd45651f410da3b3fbe9d0fbdfe7765cf.pdf", "_bibtex": "@inproceedings{\nberthelot*2018understanding,\ntitle={Understanding and Improving Interpolation in Autoencoders via an Adversarial Regularizer},\nauthor={David Berthelot* and Colin Raffel* and Aurko Roy and Ian Goodfellow},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1fQSiCcYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper73/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621612785, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1fQSiCcYm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper73/Authors", "ICLR.cc/2019/Conference/Paper73/Reviewers", "ICLR.cc/2019/Conference/Paper73/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper73/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper73/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper73/Authors|ICLR.cc/2019/Conference/Paper73/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper73/Reviewers", "ICLR.cc/2019/Conference/Paper73/Authors", "ICLR.cc/2019/Conference/Paper73/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621612785}}}, {"id": "SyefNNz8kN", "original": null, "number": 15, "cdate": 1544066089667, "ddate": null, "tcdate": 1544066089667, "tmdate": 1544143975401, "tddate": null, "forum": "S1fQSiCcYm", "replyto": "S1eavUPSJE", "invitation": "ICLR.cc/2019/Conference/-/Paper73/Official_Comment", "content": {"title": "LSGAN regularized autoencoder results, part 1", "comment": "We have implemented the LSGAN regularized AE as you described and have (anonymously) pushed the code to https://github.com/anonymous-iclr-2019/acai-iclr-2019/blob/master/lrae.py \nThe loss is implemented here: https://github.com/anonymous-iclr-2019/acai-iclr-2019/blob/master/lrae.py#L61\n\nWe have run this autoencoder on the lines dataset. We tried lambda in {0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1.0}. The best setting of lambda achieved a Mean Distance of 3.62e-3 and a Smoothness of 0.51. For high settings of lambda, the autoencoder collapses to producing a single output. For comparison, the baseline autoencoder (equivalent to setting lambda = 0) achieved Mean Distance of 6.88e-3 and a Smoothness of 0.44 (lower is better for both). It appears qualitatively and quantitatively that (on this task) including this additional loss term improves reconstruction quality (lowering the Mean Distance) but makes the interpolation quality slightly worse (lowering the Smoothness). The interpolations exhibit sudden jumps (similar to the VAE), hence the poor smoothness score. This follows our intuition - the regularizer you suggested will make reconstructions  closer to real data (i.e., more realistic) but doesn't have a mechanism to improve interpolations or change the structure of the latent space. For comparison, the ACAI regularized autoencoder achieves a Mean Distance 0.24e-3 and a Smoothness of 0.10.\n\nWe will now run the autoencoder on our representation learning experiments on real datasets and will report back with results."}, "signatures": ["ICLR.cc/2019/Conference/Paper73/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper73/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper73/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understanding and Improving Interpolation in Autoencoders via an Adversarial Regularizer", "abstract": "Autoencoders provide a powerful framework for learning compressed representations by encoding all of the information needed to reconstruct a data point in a latent code. In some cases, autoencoders can \"interpolate\": By decoding the convex combination of the latent codes for two datapoints, the autoencoder can produce an output which semantically mixes characteristics from the datapoints. In this paper, we propose a regularization procedure which encourages interpolated outputs to appear more realistic by fooling a critic network which has been trained to recover the mixing coefficient from interpolated data. We then develop a simple benchmark task where we can quantitatively measure the extent to which various autoencoders can interpolate and show that our regularizer dramatically improves interpolation in this setting. We also demonstrate empirically that our regularizer produces latent codes which are more effective on downstream tasks, suggesting a possible link between interpolation abilities and learning useful representations.", "paperhash": "berthelot|understanding_and_improving_interpolation_in_autoencoders_via_an_adversarial_regularizer", "TL;DR": "We propose a regularizer that improves interpolation and autoencoders and show that it also improves the learned representation for downstream tasks.", "authorids": ["dberth@google.com", "craffel@gmail.com", "aurkor@google.com", "goodfellow@google.com"], "authors": ["David Berthelot*", "Colin Raffel*", "Aurko Roy", "Ian Goodfellow"], "keywords": ["autoencoders", "interpolation", "unsupervised learning", "representation learning", "adversarial learning"], "pdf": "/pdf/6f8c41dcd45651f410da3b3fbe9d0fbdfe7765cf.pdf", "_bibtex": "@inproceedings{\nberthelot*2018understanding,\ntitle={Understanding and Improving Interpolation in Autoencoders via an Adversarial Regularizer},\nauthor={David Berthelot* and Colin Raffel* and Aurko Roy and Ian Goodfellow},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1fQSiCcYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper73/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621612785, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1fQSiCcYm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper73/Authors", "ICLR.cc/2019/Conference/Paper73/Reviewers", "ICLR.cc/2019/Conference/Paper73/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper73/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper73/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper73/Authors|ICLR.cc/2019/Conference/Paper73/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper73/Reviewers", "ICLR.cc/2019/Conference/Paper73/Authors", "ICLR.cc/2019/Conference/Paper73/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621612785}}}, {"id": "S1eavUPSJE", "original": null, "number": 14, "cdate": 1544021605264, "ddate": null, "tcdate": 1544021605264, "tmdate": 1544021605264, "tddate": null, "forum": "S1fQSiCcYm", "replyto": "rJlGBqmBy4", "invitation": "ICLR.cc/2019/Conference/-/Paper73/Official_Comment", "content": {"title": "Re: LSGAN regularized autoencoder", "comment": "Thanks very much for your clarifications. We now understand what you were describing as a baseline. A few comments on this -\n\n1) We will implement this approach and update this comment thread with the results. Unfortunately, we cannot update the paper draft anymore during the review period, so we will have to just copy results here and update the paper in subsequent drafts. We will also push the code for this approach to our anonymous repository so that you can verify that we are implementing what you've described.\n\n2) The difference between what you are describing and what we propose is that our critic only ever sees reconstructions and interpolants - it never sees real points. In what you described, we understand the goal to be to make reconstructions more realistic. We instead enforce that interpolants look like reconstructions, which we could expect to have a very different impact. Our paper is focused on improving interpolation quality rather than reconstruction quality - we do not expect our approach to improve reconstruction quality (compared to a baseline without the regularizer).\n\n3) We want to point out a distinction between pix2pix/cycleGAN and autoencoders. For completeness, we first define our understanding of pix2pix, CycleGAN, and an autoencoder below.\n- pix2pix consists of a generator which maps an input x to an output \\hat{y} = G(x). The discriminator tries to distinguish between pairs of (x, \\hat{y}) (generated pair) and (x, y) (real pairs).\n- CycleGAN contains two generators, one to map from x -> y and one for y -> x. Call the first one G(x) and the second F(y). Two discriminators D_x and D_y are trained to distinguish between outputs of F(x) vs. real x's and outputs of G(y) and real y's. The CycleGAN loss enforces that F(G(x)) = x and G(F(y)) = y and that G and F fool D_y and D_x respectively.\n- An autoencoder uses an encoder to map x to a latent z, and then from z back to the latent space x. It's typically trained to reconstruct x accurately. The latent z can be used for representation learning and semantic manipulation of data (such as interpolation). We introduce a regularizer which also encourages interpolants to appear similar to reconstructions.\nWe want to point out that neither pix2pix nor CycleGAN contain a latent code or an encoder/decoder, so we don't think of them as autoencoders. While CycleGAN does include a loss which encourages cycle-consistency, there is no latent code, and so there is no opportunity for interpolation or representation learning. We believe the primary similarity between CycleGAN and ACAI is that both use a discriminator to learn and minimize a divergence between implicit distributions, but to us this is a commonality to any model using a critic. We have some discussion of this in section 2.1 of our current draft, but we can expand this discussion to include a comparison to CycleGAN and pix2pix in future drafts."}, "signatures": ["ICLR.cc/2019/Conference/Paper73/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper73/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper73/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understanding and Improving Interpolation in Autoencoders via an Adversarial Regularizer", "abstract": "Autoencoders provide a powerful framework for learning compressed representations by encoding all of the information needed to reconstruct a data point in a latent code. In some cases, autoencoders can \"interpolate\": By decoding the convex combination of the latent codes for two datapoints, the autoencoder can produce an output which semantically mixes characteristics from the datapoints. In this paper, we propose a regularization procedure which encourages interpolated outputs to appear more realistic by fooling a critic network which has been trained to recover the mixing coefficient from interpolated data. We then develop a simple benchmark task where we can quantitatively measure the extent to which various autoencoders can interpolate and show that our regularizer dramatically improves interpolation in this setting. We also demonstrate empirically that our regularizer produces latent codes which are more effective on downstream tasks, suggesting a possible link between interpolation abilities and learning useful representations.", "paperhash": "berthelot|understanding_and_improving_interpolation_in_autoencoders_via_an_adversarial_regularizer", "TL;DR": "We propose a regularizer that improves interpolation and autoencoders and show that it also improves the learned representation for downstream tasks.", "authorids": ["dberth@google.com", "craffel@gmail.com", "aurkor@google.com", "goodfellow@google.com"], "authors": ["David Berthelot*", "Colin Raffel*", "Aurko Roy", "Ian Goodfellow"], "keywords": ["autoencoders", "interpolation", "unsupervised learning", "representation learning", "adversarial learning"], "pdf": "/pdf/6f8c41dcd45651f410da3b3fbe9d0fbdfe7765cf.pdf", "_bibtex": "@inproceedings{\nberthelot*2018understanding,\ntitle={Understanding and Improving Interpolation in Autoencoders via an Adversarial Regularizer},\nauthor={David Berthelot* and Colin Raffel* and Aurko Roy and Ian Goodfellow},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1fQSiCcYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper73/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621612785, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1fQSiCcYm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper73/Authors", "ICLR.cc/2019/Conference/Paper73/Reviewers", "ICLR.cc/2019/Conference/Paper73/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper73/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper73/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper73/Authors|ICLR.cc/2019/Conference/Paper73/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper73/Reviewers", "ICLR.cc/2019/Conference/Paper73/Authors", "ICLR.cc/2019/Conference/Paper73/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621612785}}}, {"id": "rJlGBqmBy4", "original": null, "number": 13, "cdate": 1544006202482, "ddate": null, "tcdate": 1544006202482, "tmdate": 1544006202482, "tddate": null, "forum": "S1fQSiCcYm", "replyto": "H1g-QchEJE", "invitation": "ICLR.cc/2019/Conference/-/Paper73/Official_Comment", "content": {"title": "Re: Can you describe in more detail what you mean by an LSGAN regularized autoencoder? ", "comment": "To improve AE by GAN is quite common due to CycleGAN (Zhu et al. 2017, Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks) and pix2pix (Isola et al. 2016, Image-to-Image Translation with Conditional Adversarial Nets). A LSGAN regularized AE is almost equivalent to a CycleGAN except that you only do a half cycle here:\n\nGiven a AE: x -> z -> \\hat{x} with parameterization \\hat{x} = G(z), z = F(x). The critic is trained by minimizing\nL_critic = (D(x) - 0)^2 + (D(\\hat{x}) - 1)^2\nand the AE is trained by minimizing\nL_AE = || x - \\hat{x} || + lambda * (D(\\hat{x}) - 0)^2\n\nThis is fairly similar to your objective function in my opinion. So I was asking for a comparison.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper73/AnonReviewer3"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper73/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper73/AnonReviewer3", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understanding and Improving Interpolation in Autoencoders via an Adversarial Regularizer", "abstract": "Autoencoders provide a powerful framework for learning compressed representations by encoding all of the information needed to reconstruct a data point in a latent code. In some cases, autoencoders can \"interpolate\": By decoding the convex combination of the latent codes for two datapoints, the autoencoder can produce an output which semantically mixes characteristics from the datapoints. In this paper, we propose a regularization procedure which encourages interpolated outputs to appear more realistic by fooling a critic network which has been trained to recover the mixing coefficient from interpolated data. We then develop a simple benchmark task where we can quantitatively measure the extent to which various autoencoders can interpolate and show that our regularizer dramatically improves interpolation in this setting. We also demonstrate empirically that our regularizer produces latent codes which are more effective on downstream tasks, suggesting a possible link between interpolation abilities and learning useful representations.", "paperhash": "berthelot|understanding_and_improving_interpolation_in_autoencoders_via_an_adversarial_regularizer", "TL;DR": "We propose a regularizer that improves interpolation and autoencoders and show that it also improves the learned representation for downstream tasks.", "authorids": ["dberth@google.com", "craffel@gmail.com", "aurkor@google.com", "goodfellow@google.com"], "authors": ["David Berthelot*", "Colin Raffel*", "Aurko Roy", "Ian Goodfellow"], "keywords": ["autoencoders", "interpolation", "unsupervised learning", "representation learning", "adversarial learning"], "pdf": "/pdf/6f8c41dcd45651f410da3b3fbe9d0fbdfe7765cf.pdf", "_bibtex": "@inproceedings{\nberthelot*2018understanding,\ntitle={Understanding and Improving Interpolation in Autoencoders via an Adversarial Regularizer},\nauthor={David Berthelot* and Colin Raffel* and Aurko Roy and Ian Goodfellow},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1fQSiCcYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper73/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621612785, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1fQSiCcYm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper73/Authors", "ICLR.cc/2019/Conference/Paper73/Reviewers", "ICLR.cc/2019/Conference/Paper73/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper73/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper73/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper73/Authors|ICLR.cc/2019/Conference/Paper73/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper73/Reviewers", "ICLR.cc/2019/Conference/Paper73/Authors", "ICLR.cc/2019/Conference/Paper73/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621612785}}}, {"id": "H1g-QchEJE", "original": null, "number": 12, "cdate": 1543977496544, "ddate": null, "tcdate": 1543977496544, "tmdate": 1543977496544, "tddate": null, "forum": "S1fQSiCcYm", "replyto": "H1gUty5Ey4", "invitation": "ICLR.cc/2019/Conference/-/Paper73/Official_Comment", "content": {"title": "Re: This is a hidden discussion after rebuttal ", "comment": "R3, thank you for noticing the comments were not public and making the discussion public.\n\n> I like your question: \"given an autoencoder which reconstructs well but interpolations poorly (our Baseline), can we improve the quality of its interpolations, and does improving the interpolation quality improve the representation learned?\"\n> This should be added to the paper with an emphasis.\n\nWe are glad this question clarified your understanding of the paper. Unfortunately, the time period for us to be able to make revisions to the paper is over, so we can't update the PDF. However, we can assure you we will include and emphasize (e.g. boldface) this text in an updated draft.\n\n> My main concern still remains: is the good representation coming from a GAN regularized autoencoder (since your interpolation formulation is very similar to that of LSGAN) or because of the improved interpolation (then it's your contribution)? \n> I found the experiments insufficient unless you compared with such a baseline (e.g. LSGAN regularized autoencoder) on representation learning.\n\nCan you describe in more detail what you mean by an LSGAN regularized autoencoder? Our model is quite different from a GAN, since it is an autoencoder and not a generative model (there is no way to draw samples from it). While it uses a critic and an adversarial learning process, it otherwise has very little in common with GANs. If you mean an autoencoder whose latent space is regularized by a critic, I think that baseline is represented by our inclusion of an AAE. If you have a specific model architecture or loss function in mind, we would be happy to include it in our experiments."}, "signatures": ["ICLR.cc/2019/Conference/Paper73/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper73/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper73/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understanding and Improving Interpolation in Autoencoders via an Adversarial Regularizer", "abstract": "Autoencoders provide a powerful framework for learning compressed representations by encoding all of the information needed to reconstruct a data point in a latent code. In some cases, autoencoders can \"interpolate\": By decoding the convex combination of the latent codes for two datapoints, the autoencoder can produce an output which semantically mixes characteristics from the datapoints. In this paper, we propose a regularization procedure which encourages interpolated outputs to appear more realistic by fooling a critic network which has been trained to recover the mixing coefficient from interpolated data. We then develop a simple benchmark task where we can quantitatively measure the extent to which various autoencoders can interpolate and show that our regularizer dramatically improves interpolation in this setting. We also demonstrate empirically that our regularizer produces latent codes which are more effective on downstream tasks, suggesting a possible link between interpolation abilities and learning useful representations.", "paperhash": "berthelot|understanding_and_improving_interpolation_in_autoencoders_via_an_adversarial_regularizer", "TL;DR": "We propose a regularizer that improves interpolation and autoencoders and show that it also improves the learned representation for downstream tasks.", "authorids": ["dberth@google.com", "craffel@gmail.com", "aurkor@google.com", "goodfellow@google.com"], "authors": ["David Berthelot*", "Colin Raffel*", "Aurko Roy", "Ian Goodfellow"], "keywords": ["autoencoders", "interpolation", "unsupervised learning", "representation learning", "adversarial learning"], "pdf": "/pdf/6f8c41dcd45651f410da3b3fbe9d0fbdfe7765cf.pdf", "_bibtex": "@inproceedings{\nberthelot*2018understanding,\ntitle={Understanding and Improving Interpolation in Autoencoders via an Adversarial Regularizer},\nauthor={David Berthelot* and Colin Raffel* and Aurko Roy and Ian Goodfellow},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1fQSiCcYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper73/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621612785, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1fQSiCcYm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper73/Authors", "ICLR.cc/2019/Conference/Paper73/Reviewers", "ICLR.cc/2019/Conference/Paper73/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper73/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper73/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper73/Authors|ICLR.cc/2019/Conference/Paper73/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper73/Reviewers", "ICLR.cc/2019/Conference/Paper73/Authors", "ICLR.cc/2019/Conference/Paper73/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621612785}}}, {"id": "H1gUty5Ey4", "original": null, "number": 11, "cdate": 1543966589668, "ddate": null, "tcdate": 1543966589668, "tmdate": 1543966589668, "tddate": null, "forum": "S1fQSiCcYm", "replyto": "ryx6nAJFaQ", "invitation": "ICLR.cc/2019/Conference/-/Paper73/Official_Comment", "content": {"title": "This is a hidden discussion after rebuttal", "comment": "Sorry that I didn't realize the discussion between the authors and me was private! I replied to AC's question which was private making everything private afterwards.\nI think it is worth an open discussion by more people. So I post the discussion here.\n\n*** By reviewer 3 ***\nThis is an interesting idea, but I'm still not sure its practicality for autoencoders. I will rephrase and elaborate my concerns:\n\n> R3: \"The baselines are not representative; you should compare with other GAN techniques induced regularizers, such as WGAN regularized autoencoder.\" \nA: \"WAE = AAE; Our paper includes the adversarial autoencoder as a baseline.\"\n\nI'm sorry the question was not clear. In fact, I meant to compare other GAN regularizers for the output of the decoder (AAE regularizes the code), which is quite common due to the popularity of CycleGAN, and it indeed improves significantly the quality of the outputs. \nAs I asked: should we \"regularize the interpolation or regularize the image of the decoder\"?\nI think the latter is the main desideratum for autoencoders. \nInterpolation regularizer is one way to achieve that; and the proposed ACAI in my opinion is a generalized LSGAN regularizer (may be the motivations are different).  But since there is no comparison between that and ACAI, I'm not sure if this interpolation extension plays an important role. \n\n> Regarding the philosophy of interpolation: \n1) Interpolation looks realistic;\n2) The interpolation path is semantically smooth,\n\nI am not sure if there is a clear connection between a good interpolation and a good representation learning, since there are good discrete representation learning and as the authors mentioned the denoising AE could perform better despite producing bad interpolations.\nMore experiments are needed to gain a deeper understanding. The evaluation of interpolation on a toy dataset is far from satisfactory.  \n\n\n*** By authors ***\nR3, thanks for clarifying your initial comments and for your additional discussion. We think there remains some misunderstanding about the scope and claims of our paper.\n\n1) Our paper focuses on autoencoders and representation learning, not generative models. GANs are generative models, and ACAI is a regularizer for autoencoders. While ACAI includes an adversarial training process and a \"critic\", it otherwise has very little in common with GANs and the resulting autoencoder is not a generative model. Similarly, while the loss function has some similarities with the LSGAN loss function (i.e., they both use a least-squared error loss), it has very little in common with an LSGAN because an LSGAN is a generative model and not an autoencoder or a technique for learning representations. We agree that it would be interesting to study the effect of regularizing the decoder of an autoencoder in similar ways to the generator in a GAN, but this is outside the scope of our paper. More specifically, GANs are not representation learning techniques, they are generative models; so, there is no way to test their representation learning capabilities (as is the focus of our paper).\n\n2) We do not claim that \"good interpolation implies a good representation and a good representation implies good interpolation\". In contrast, we ask \"given an autoencoder which reconstructs well but interpolations poorly (our Baseline), can we improve the quality of its interpolations, and does improving the interpolation quality improve the representation learned?\" Note that the first is a claim of causality, ours is a test of an intervention. As an aside, we are not the first to study or point out this potential connection; see e.g. \"Better Mixing via Deep Representations\" by Bengio et al.\n\nBased on this discussion, we have included some additional statements in our updated draft to make it clear what the scope and claims of our paper are. We hope this clarifies the intention of our paper.\n\n\n*** By authors ***\nR3, we believe have addressed your concerns and clarified some of your points. Do you have an updated impression of our paper? Thanks for your consideration.\n\n\n*** By reviewer 3: EXPERIMENT REQUEST ***\nI like your question: \"given an autoencoder which reconstructs well but interpolations poorly (our Baseline), can we improve the quality of its interpolations, and does improving the interpolation quality improve the representation learned?\"\nThis should be added to the paper with an emphasis.\n\nMy main concern still remains: is the good representation coming from a GAN regularized autoencoder (since your interpolation formulation is very similar to that of LSGAN) or because of the improved interpolation (then it's your contribution)? \nI found the experiments insufficient unless you compared with such a baseline (e.g. LSGAN regularized autoencoder) on representation learning."}, "signatures": ["ICLR.cc/2019/Conference/Paper73/AnonReviewer3"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper73/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper73/AnonReviewer3", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understanding and Improving Interpolation in Autoencoders via an Adversarial Regularizer", "abstract": "Autoencoders provide a powerful framework for learning compressed representations by encoding all of the information needed to reconstruct a data point in a latent code. In some cases, autoencoders can \"interpolate\": By decoding the convex combination of the latent codes for two datapoints, the autoencoder can produce an output which semantically mixes characteristics from the datapoints. In this paper, we propose a regularization procedure which encourages interpolated outputs to appear more realistic by fooling a critic network which has been trained to recover the mixing coefficient from interpolated data. We then develop a simple benchmark task where we can quantitatively measure the extent to which various autoencoders can interpolate and show that our regularizer dramatically improves interpolation in this setting. We also demonstrate empirically that our regularizer produces latent codes which are more effective on downstream tasks, suggesting a possible link between interpolation abilities and learning useful representations.", "paperhash": "berthelot|understanding_and_improving_interpolation_in_autoencoders_via_an_adversarial_regularizer", "TL;DR": "We propose a regularizer that improves interpolation and autoencoders and show that it also improves the learned representation for downstream tasks.", "authorids": ["dberth@google.com", "craffel@gmail.com", "aurkor@google.com", "goodfellow@google.com"], "authors": ["David Berthelot*", "Colin Raffel*", "Aurko Roy", "Ian Goodfellow"], "keywords": ["autoencoders", "interpolation", "unsupervised learning", "representation learning", "adversarial learning"], "pdf": "/pdf/6f8c41dcd45651f410da3b3fbe9d0fbdfe7765cf.pdf", "_bibtex": "@inproceedings{\nberthelot*2018understanding,\ntitle={Understanding and Improving Interpolation in Autoencoders via an Adversarial Regularizer},\nauthor={David Berthelot* and Colin Raffel* and Aurko Roy and Ian Goodfellow},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1fQSiCcYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper73/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621612785, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1fQSiCcYm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper73/Authors", "ICLR.cc/2019/Conference/Paper73/Reviewers", "ICLR.cc/2019/Conference/Paper73/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper73/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper73/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper73/Authors|ICLR.cc/2019/Conference/Paper73/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper73/Reviewers", "ICLR.cc/2019/Conference/Paper73/Authors", "ICLR.cc/2019/Conference/Paper73/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621612785}}}, {"id": "BJgfoaTGkE", "original": null, "number": 10, "cdate": 1543851418291, "ddate": null, "tcdate": 1543851418291, "tmdate": 1543851418291, "tddate": null, "forum": "S1fQSiCcYm", "replyto": "S1e8zyxY67", "invitation": "ICLR.cc/2019/Conference/-/Paper73/Official_Comment", "content": {"title": "Thanks and good luck", "comment": "I would like to thank the authors for addressing my feedback. This comforts me in the rating I gave to their paper. \nGood luck."}, "signatures": ["ICLR.cc/2019/Conference/Paper73/AnonReviewer1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper73/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper73/AnonReviewer1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understanding and Improving Interpolation in Autoencoders via an Adversarial Regularizer", "abstract": "Autoencoders provide a powerful framework for learning compressed representations by encoding all of the information needed to reconstruct a data point in a latent code. In some cases, autoencoders can \"interpolate\": By decoding the convex combination of the latent codes for two datapoints, the autoencoder can produce an output which semantically mixes characteristics from the datapoints. In this paper, we propose a regularization procedure which encourages interpolated outputs to appear more realistic by fooling a critic network which has been trained to recover the mixing coefficient from interpolated data. We then develop a simple benchmark task where we can quantitatively measure the extent to which various autoencoders can interpolate and show that our regularizer dramatically improves interpolation in this setting. We also demonstrate empirically that our regularizer produces latent codes which are more effective on downstream tasks, suggesting a possible link between interpolation abilities and learning useful representations.", "paperhash": "berthelot|understanding_and_improving_interpolation_in_autoencoders_via_an_adversarial_regularizer", "TL;DR": "We propose a regularizer that improves interpolation and autoencoders and show that it also improves the learned representation for downstream tasks.", "authorids": ["dberth@google.com", "craffel@gmail.com", "aurkor@google.com", "goodfellow@google.com"], "authors": ["David Berthelot*", "Colin Raffel*", "Aurko Roy", "Ian Goodfellow"], "keywords": ["autoencoders", "interpolation", "unsupervised learning", "representation learning", "adversarial learning"], "pdf": "/pdf/6f8c41dcd45651f410da3b3fbe9d0fbdfe7765cf.pdf", "_bibtex": "@inproceedings{\nberthelot*2018understanding,\ntitle={Understanding and Improving Interpolation in Autoencoders via an Adversarial Regularizer},\nauthor={David Berthelot* and Colin Raffel* and Aurko Roy and Ian Goodfellow},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1fQSiCcYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper73/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621612785, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1fQSiCcYm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper73/Authors", "ICLR.cc/2019/Conference/Paper73/Reviewers", "ICLR.cc/2019/Conference/Paper73/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper73/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper73/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper73/Authors|ICLR.cc/2019/Conference/Paper73/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper73/Reviewers", "ICLR.cc/2019/Conference/Paper73/Authors", "ICLR.cc/2019/Conference/Paper73/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621612785}}}, {"id": "HyeA07QzJN", "original": null, "number": 9, "cdate": 1543807958110, "ddate": null, "tcdate": 1543807958110, "tmdate": 1543807958110, "tddate": null, "forum": "S1fQSiCcYm", "replyto": "rJxR0Gt_Cm", "invitation": "ICLR.cc/2019/Conference/-/Paper73/Official_Comment", "content": {"title": "Re: Re: Still not convinced", "comment": "R3, we believe have addressed your concerns and clarified some of your points. Do you have an updated impression of our paper? Thanks for your consideration."}, "signatures": ["ICLR.cc/2019/Conference/Paper73/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper73/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper73/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understanding and Improving Interpolation in Autoencoders via an Adversarial Regularizer", "abstract": "Autoencoders provide a powerful framework for learning compressed representations by encoding all of the information needed to reconstruct a data point in a latent code. In some cases, autoencoders can \"interpolate\": By decoding the convex combination of the latent codes for two datapoints, the autoencoder can produce an output which semantically mixes characteristics from the datapoints. In this paper, we propose a regularization procedure which encourages interpolated outputs to appear more realistic by fooling a critic network which has been trained to recover the mixing coefficient from interpolated data. We then develop a simple benchmark task where we can quantitatively measure the extent to which various autoencoders can interpolate and show that our regularizer dramatically improves interpolation in this setting. We also demonstrate empirically that our regularizer produces latent codes which are more effective on downstream tasks, suggesting a possible link between interpolation abilities and learning useful representations.", "paperhash": "berthelot|understanding_and_improving_interpolation_in_autoencoders_via_an_adversarial_regularizer", "TL;DR": "We propose a regularizer that improves interpolation and autoencoders and show that it also improves the learned representation for downstream tasks.", "authorids": ["dberth@google.com", "craffel@gmail.com", "aurkor@google.com", "goodfellow@google.com"], "authors": ["David Berthelot*", "Colin Raffel*", "Aurko Roy", "Ian Goodfellow"], "keywords": ["autoencoders", "interpolation", "unsupervised learning", "representation learning", "adversarial learning"], "pdf": "/pdf/6f8c41dcd45651f410da3b3fbe9d0fbdfe7765cf.pdf", "_bibtex": "@inproceedings{\nberthelot*2018understanding,\ntitle={Understanding and Improving Interpolation in Autoencoders via an Adversarial Regularizer},\nauthor={David Berthelot* and Colin Raffel* and Aurko Roy and Ian Goodfellow},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1fQSiCcYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper73/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621612785, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1fQSiCcYm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper73/Authors", "ICLR.cc/2019/Conference/Paper73/Reviewers", "ICLR.cc/2019/Conference/Paper73/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper73/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper73/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper73/Authors|ICLR.cc/2019/Conference/Paper73/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper73/Reviewers", "ICLR.cc/2019/Conference/Paper73/Authors", "ICLR.cc/2019/Conference/Paper73/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621612785}}}, {"id": "rJxR0Gt_Cm", "original": null, "number": 8, "cdate": 1543176918262, "ddate": null, "tcdate": 1543176918262, "tmdate": 1543176918262, "tddate": null, "forum": "S1fQSiCcYm", "replyto": "Skl73yXdCQ", "invitation": "ICLR.cc/2019/Conference/-/Paper73/Official_Comment", "content": {"title": "Re: Still not convinced", "comment": "R3, thanks for clarifying your initial comments and for your additional discussion. We think there remains some misunderstanding about the scope and claims of our paper.\n\n1) Our paper focuses on autoencoders and representation learning, not generative models. GANs are generative models, and ACAI is a regularizer for autoencoders. While ACAI includes an adversarial training process and a \"critic\", it otherwise has very little in common with GANs and the resulting autoencoder is not a generative model. Similarly, while the loss function has some similarities with the LSGAN loss function (i.e., they both use a least-squared error loss), it has very little in common with an LSGAN because an LSGAN is a generative model and not an autoencoder or a technique for learning representations. We agree that it would be interesting to study the effect of regularizing the decoder of an autoencoder in similar ways to the generator in a GAN, but this is outside the scope of our paper. More specifically, GANs are not representation learning techniques, they are generative models; so, there is no way to test their representation learning capabilities (as is the focus of our paper).\n\n2) We do not claim that \"good interpolation implies a good representation and a good representation implies good interpolation\". In contrast, we ask \"given an autoencoder which reconstructs well but interpolations poorly (our Baseline), can we improve the quality of its interpolations, and does improving the interpolation quality improve the representation learned?\" Note that the first is a claim of causality, ours is a test of an intervention. As an aside, we are not the first to study or point out this potential connection; see e.g. \"Better Mixing via Deep Representations\" by Bengio et al.\n\nBased on this discussion, we have included some additional statements in our updated draft to make it clear what the scope and claims of our paper are. We hope this clarifies the intention of our paper."}, "signatures": ["ICLR.cc/2019/Conference/Paper73/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper73/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper73/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understanding and Improving Interpolation in Autoencoders via an Adversarial Regularizer", "abstract": "Autoencoders provide a powerful framework for learning compressed representations by encoding all of the information needed to reconstruct a data point in a latent code. In some cases, autoencoders can \"interpolate\": By decoding the convex combination of the latent codes for two datapoints, the autoencoder can produce an output which semantically mixes characteristics from the datapoints. In this paper, we propose a regularization procedure which encourages interpolated outputs to appear more realistic by fooling a critic network which has been trained to recover the mixing coefficient from interpolated data. We then develop a simple benchmark task where we can quantitatively measure the extent to which various autoencoders can interpolate and show that our regularizer dramatically improves interpolation in this setting. We also demonstrate empirically that our regularizer produces latent codes which are more effective on downstream tasks, suggesting a possible link between interpolation abilities and learning useful representations.", "paperhash": "berthelot|understanding_and_improving_interpolation_in_autoencoders_via_an_adversarial_regularizer", "TL;DR": "We propose a regularizer that improves interpolation and autoencoders and show that it also improves the learned representation for downstream tasks.", "authorids": ["dberth@google.com", "craffel@gmail.com", "aurkor@google.com", "goodfellow@google.com"], "authors": ["David Berthelot*", "Colin Raffel*", "Aurko Roy", "Ian Goodfellow"], "keywords": ["autoencoders", "interpolation", "unsupervised learning", "representation learning", "adversarial learning"], "pdf": "/pdf/6f8c41dcd45651f410da3b3fbe9d0fbdfe7765cf.pdf", "_bibtex": "@inproceedings{\nberthelot*2018understanding,\ntitle={Understanding and Improving Interpolation in Autoencoders via an Adversarial Regularizer},\nauthor={David Berthelot* and Colin Raffel* and Aurko Roy and Ian Goodfellow},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1fQSiCcYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper73/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621612785, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1fQSiCcYm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper73/Authors", "ICLR.cc/2019/Conference/Paper73/Reviewers", "ICLR.cc/2019/Conference/Paper73/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper73/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper73/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper73/Authors|ICLR.cc/2019/Conference/Paper73/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper73/Reviewers", "ICLR.cc/2019/Conference/Paper73/Authors", "ICLR.cc/2019/Conference/Paper73/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621612785}}}, {"id": "Skl73yXdCQ", "original": null, "number": 7, "cdate": 1543151530811, "ddate": null, "tcdate": 1543151530811, "tmdate": 1543151530811, "tddate": null, "forum": "S1fQSiCcYm", "replyto": "ByglXo5SRQ", "invitation": "ICLR.cc/2019/Conference/-/Paper73/Official_Comment", "content": {"title": "Still not convinced", "comment": "Dear authors and ACs,\n\nThis is an interesting idea, but I'm still not sure its practicality for autoencoders. I will rephrase and elaborate my concerns:\n\n> R3: \"The baselines are not representative; you should compare with other GAN techniques induced regularizers, such as WGAN regularized autoencoder.\" \nA: \"WAE = AAE; Our paper includes the adversarial autoencoder as a baseline.\"\n\nI'm sorry the question was not clear. In fact, I meant to compare other GAN regularizers for the output of the decoder (AAE regularizes the code), which is quite common due to the popularity of CycleGAN, and it indeed improves significantly the quality of the outputs. \nAs I asked: should we \"regularize the interpolation or regularize the image of the decoder\"?\nI think the latter is the main desideratum for autoencoders. \nInterpolation regularizer is one way to achieve that; and the proposed ACAI in my opinion is a generalized LSGAN regularizer (may be the motivations are different).  But since there is no comparison between that and ACAI, I'm not sure if this interpolation extension plays an important role. \n\n> Regarding the philosophy of interpolation: \n1) Interpolation looks realistic;\n2) The interpolation path is semantically smooth,\n\nI am not sure if there is a clear connection between a good interpolation and a good representation learning, since there are good discrete representation learning and as the authors mentioned the denoising AE could perform better despite producing bad interpolations.\nMore experiments are needed to gain a deeper understanding. The evaluation of interpolation on a toy dataset is far from satisfactory.  "}, "signatures": ["ICLR.cc/2019/Conference/Paper73/AnonReviewer3"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper73/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper73/AnonReviewer3", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understanding and Improving Interpolation in Autoencoders via an Adversarial Regularizer", "abstract": "Autoencoders provide a powerful framework for learning compressed representations by encoding all of the information needed to reconstruct a data point in a latent code. In some cases, autoencoders can \"interpolate\": By decoding the convex combination of the latent codes for two datapoints, the autoencoder can produce an output which semantically mixes characteristics from the datapoints. In this paper, we propose a regularization procedure which encourages interpolated outputs to appear more realistic by fooling a critic network which has been trained to recover the mixing coefficient from interpolated data. We then develop a simple benchmark task where we can quantitatively measure the extent to which various autoencoders can interpolate and show that our regularizer dramatically improves interpolation in this setting. We also demonstrate empirically that our regularizer produces latent codes which are more effective on downstream tasks, suggesting a possible link between interpolation abilities and learning useful representations.", "paperhash": "berthelot|understanding_and_improving_interpolation_in_autoencoders_via_an_adversarial_regularizer", "TL;DR": "We propose a regularizer that improves interpolation and autoencoders and show that it also improves the learned representation for downstream tasks.", "authorids": ["dberth@google.com", "craffel@gmail.com", "aurkor@google.com", "goodfellow@google.com"], "authors": ["David Berthelot*", "Colin Raffel*", "Aurko Roy", "Ian Goodfellow"], "keywords": ["autoencoders", "interpolation", "unsupervised learning", "representation learning", "adversarial learning"], "pdf": "/pdf/6f8c41dcd45651f410da3b3fbe9d0fbdfe7765cf.pdf", "_bibtex": "@inproceedings{\nberthelot*2018understanding,\ntitle={Understanding and Improving Interpolation in Autoencoders via an Adversarial Regularizer},\nauthor={David Berthelot* and Colin Raffel* and Aurko Roy and Ian Goodfellow},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1fQSiCcYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper73/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621612785, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1fQSiCcYm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper73/Authors", "ICLR.cc/2019/Conference/Paper73/Reviewers", "ICLR.cc/2019/Conference/Paper73/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper73/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper73/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper73/Authors|ICLR.cc/2019/Conference/Paper73/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper73/Reviewers", "ICLR.cc/2019/Conference/Paper73/Authors", "ICLR.cc/2019/Conference/Paper73/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621612785}}}, {"id": "SJgQLPtKTm", "original": null, "number": 1, "cdate": 1542195018722, "ddate": null, "tcdate": 1542195018722, "tmdate": 1542278999026, "tddate": null, "forum": "S1fQSiCcYm", "replyto": "S1fQSiCcYm", "invitation": "ICLR.cc/2019/Conference/-/Paper73/Public_Comment", "content": {"comment": "For example, if the proposed regularizer is applied to a VAE, does it help in getting better random samples by decoding z ~ N(0, 1)?\n\n", "title": "Does the technique help in getting better random samples?"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understanding and Improving Interpolation in Autoencoders via an Adversarial Regularizer", "abstract": "Autoencoders provide a powerful framework for learning compressed representations by encoding all of the information needed to reconstruct a data point in a latent code. In some cases, autoencoders can \"interpolate\": By decoding the convex combination of the latent codes for two datapoints, the autoencoder can produce an output which semantically mixes characteristics from the datapoints. In this paper, we propose a regularization procedure which encourages interpolated outputs to appear more realistic by fooling a critic network which has been trained to recover the mixing coefficient from interpolated data. We then develop a simple benchmark task where we can quantitatively measure the extent to which various autoencoders can interpolate and show that our regularizer dramatically improves interpolation in this setting. We also demonstrate empirically that our regularizer produces latent codes which are more effective on downstream tasks, suggesting a possible link between interpolation abilities and learning useful representations.", "paperhash": "berthelot|understanding_and_improving_interpolation_in_autoencoders_via_an_adversarial_regularizer", "TL;DR": "We propose a regularizer that improves interpolation and autoencoders and show that it also improves the learned representation for downstream tasks.", "authorids": ["dberth@google.com", "craffel@gmail.com", "aurkor@google.com", "goodfellow@google.com"], "authors": ["David Berthelot*", "Colin Raffel*", "Aurko Roy", "Ian Goodfellow"], "keywords": ["autoencoders", "interpolation", "unsupervised learning", "representation learning", "adversarial learning"], "pdf": "/pdf/6f8c41dcd45651f410da3b3fbe9d0fbdfe7765cf.pdf", "_bibtex": "@inproceedings{\nberthelot*2018understanding,\ntitle={Understanding and Improving Interpolation in Autoencoders via an Adversarial Regularizer},\nauthor={David Berthelot* and Colin Raffel* and Aurko Roy and Ian Goodfellow},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1fQSiCcYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper73/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311925074, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "S1fQSiCcYm", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper73/Authors", "ICLR.cc/2019/Conference/Paper73/Reviewers", "ICLR.cc/2019/Conference/Paper73/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper73/Authors", "ICLR.cc/2019/Conference/Paper73/Reviewers", "ICLR.cc/2019/Conference/Paper73/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311925074}}}, {"id": "r1g4OITYpX", "original": null, "number": 5, "cdate": 1542211180381, "ddate": null, "tcdate": 1542211180381, "tmdate": 1542211180381, "tddate": null, "forum": "S1fQSiCcYm", "replyto": "SJgQLPtKTm", "invitation": "ICLR.cc/2019/Conference/-/Paper73/Official_Comment", "content": {"title": "Re: Does the techinique help in getting better random samples? ", "comment": "Thanks for your question. In general we do not expect this regularizer to improve the sample quality of a given autoencoder, since the critic's primary objective is to discriminate between interpolants and reconstructions (not interpolants and \"real\" data). The goal instead is to take an autoencoder which already reconstructs well but interpolates poorly and improve the quality of the interpolations. The VAE typically has the opposite problem - it reconstructs poorly but interpolates smoothly. In other words, the latent space of the VAE is already \"continuous\" in some sense (due to the enforcement of the prior) but many regions in latent space map to \"unrealistic\" (i.e. blurry) outputs. So, we aren't sure whether our regularizer would improve VAE reconstructions. It would be pretty straightforward to try using our publicly-available code, though!"}, "signatures": ["ICLR.cc/2019/Conference/Paper73/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper73/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper73/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understanding and Improving Interpolation in Autoencoders via an Adversarial Regularizer", "abstract": "Autoencoders provide a powerful framework for learning compressed representations by encoding all of the information needed to reconstruct a data point in a latent code. In some cases, autoencoders can \"interpolate\": By decoding the convex combination of the latent codes for two datapoints, the autoencoder can produce an output which semantically mixes characteristics from the datapoints. In this paper, we propose a regularization procedure which encourages interpolated outputs to appear more realistic by fooling a critic network which has been trained to recover the mixing coefficient from interpolated data. We then develop a simple benchmark task where we can quantitatively measure the extent to which various autoencoders can interpolate and show that our regularizer dramatically improves interpolation in this setting. We also demonstrate empirically that our regularizer produces latent codes which are more effective on downstream tasks, suggesting a possible link between interpolation abilities and learning useful representations.", "paperhash": "berthelot|understanding_and_improving_interpolation_in_autoencoders_via_an_adversarial_regularizer", "TL;DR": "We propose a regularizer that improves interpolation and autoencoders and show that it also improves the learned representation for downstream tasks.", "authorids": ["dberth@google.com", "craffel@gmail.com", "aurkor@google.com", "goodfellow@google.com"], "authors": ["David Berthelot*", "Colin Raffel*", "Aurko Roy", "Ian Goodfellow"], "keywords": ["autoencoders", "interpolation", "unsupervised learning", "representation learning", "adversarial learning"], "pdf": "/pdf/6f8c41dcd45651f410da3b3fbe9d0fbdfe7765cf.pdf", "_bibtex": "@inproceedings{\nberthelot*2018understanding,\ntitle={Understanding and Improving Interpolation in Autoencoders via an Adversarial Regularizer},\nauthor={David Berthelot* and Colin Raffel* and Aurko Roy and Ian Goodfellow},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1fQSiCcYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper73/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621612785, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1fQSiCcYm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper73/Authors", "ICLR.cc/2019/Conference/Paper73/Reviewers", "ICLR.cc/2019/Conference/Paper73/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper73/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper73/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper73/Authors|ICLR.cc/2019/Conference/Paper73/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper73/Reviewers", "ICLR.cc/2019/Conference/Paper73/Authors", "ICLR.cc/2019/Conference/Paper73/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621612785}}}, {"id": "HkxOaxgtp7", "original": null, "number": 4, "cdate": 1542156479884, "ddate": null, "tcdate": 1542156479884, "tmdate": 1542156479884, "tddate": null, "forum": "S1fQSiCcYm", "replyto": "S1fQSiCcYm", "invitation": "ICLR.cc/2019/Conference/-/Paper73/Official_Comment", "content": {"title": "Updated draft", "comment": "Thanks to all of the reviewers for their feedback on our paper. We have addressed each reviewer's comments individually and have also uploaded an updated draft based on the suggestions. The changes include the following:\n- Clarified why smooth and realistic interpolations may potentially lead to better reconstructions in the introduction\n- Framed our objective as minimizing an adversarial divergence between reconstructions and interpolants\n- Clarified the second term of the critic loss involving \\gamma and gave additional justification for this term\n- Added comparison of the ACAI and LSGAN critic losses\n- Gave additional intuition as to how the critic could potentially regress \\alpha when only being shown a single image at a time\n- Referred to our lines images as \"greyscale\" rather than \"black and white\"\n- Noted that the AAE baseline we included has also subsequently been referred to as Wasserstein Autoencoder\n- Pointed out some cases where interpolations can be smooth and realistic despite interpolating between dissimilar points\n\nWe hope these changes address any concerns the reviewers have."}, "signatures": ["ICLR.cc/2019/Conference/Paper73/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper73/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper73/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understanding and Improving Interpolation in Autoencoders via an Adversarial Regularizer", "abstract": "Autoencoders provide a powerful framework for learning compressed representations by encoding all of the information needed to reconstruct a data point in a latent code. In some cases, autoencoders can \"interpolate\": By decoding the convex combination of the latent codes for two datapoints, the autoencoder can produce an output which semantically mixes characteristics from the datapoints. In this paper, we propose a regularization procedure which encourages interpolated outputs to appear more realistic by fooling a critic network which has been trained to recover the mixing coefficient from interpolated data. We then develop a simple benchmark task where we can quantitatively measure the extent to which various autoencoders can interpolate and show that our regularizer dramatically improves interpolation in this setting. We also demonstrate empirically that our regularizer produces latent codes which are more effective on downstream tasks, suggesting a possible link between interpolation abilities and learning useful representations.", "paperhash": "berthelot|understanding_and_improving_interpolation_in_autoencoders_via_an_adversarial_regularizer", "TL;DR": "We propose a regularizer that improves interpolation and autoencoders and show that it also improves the learned representation for downstream tasks.", "authorids": ["dberth@google.com", "craffel@gmail.com", "aurkor@google.com", "goodfellow@google.com"], "authors": ["David Berthelot*", "Colin Raffel*", "Aurko Roy", "Ian Goodfellow"], "keywords": ["autoencoders", "interpolation", "unsupervised learning", "representation learning", "adversarial learning"], "pdf": "/pdf/6f8c41dcd45651f410da3b3fbe9d0fbdfe7765cf.pdf", "_bibtex": "@inproceedings{\nberthelot*2018understanding,\ntitle={Understanding and Improving Interpolation in Autoencoders via an Adversarial Regularizer},\nauthor={David Berthelot* and Colin Raffel* and Aurko Roy and Ian Goodfellow},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1fQSiCcYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper73/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621612785, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1fQSiCcYm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper73/Authors", "ICLR.cc/2019/Conference/Paper73/Reviewers", "ICLR.cc/2019/Conference/Paper73/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper73/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper73/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper73/Authors|ICLR.cc/2019/Conference/Paper73/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper73/Reviewers", "ICLR.cc/2019/Conference/Paper73/Authors", "ICLR.cc/2019/Conference/Paper73/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621612785}}}, {"id": "BJl0L1xt6m", "original": null, "number": 3, "cdate": 1542156118241, "ddate": null, "tcdate": 1542156118241, "tmdate": 1542156118241, "tddate": null, "forum": "S1fQSiCcYm", "replyto": "B1gmB4Kv2m", "invitation": "ICLR.cc/2019/Conference/-/Paper73/Official_Comment", "content": {"title": "Re: An interesting regularized AE algorithm that improves interpolation in latent space ", "comment": "Thanks for your review and thoughtful analysis. To address each of your cons in turn:\n\n> The interplay of the adversarial network (between AE and critic) isn\u2019t very clear and can be improved.\n\nThe goal of the critic is to predict the interpolation mixing coefficient \\alpha; the goal of the autoencoder is to \"fool\" the critic into outputting \\alpha = 0. It can be useful to think of the critic as estimating a divergence between real and interpolated datapoints, and the autoencoder is trying to minimize this divergence. We have added some discussion of this to our paper.\n\n> Eq. 1, should x be x_1 or a new data other than x1 and x2?\n\nIt actually can be any real datapoint x - the second term can be computed separately from the first. We have clarified this in our updated draft.\n\n> The paper states that the 2nd term of Eq. 1 isn\u2019t crucial. If x is a new data (other than x1 or x2), how can the critic infer \\alpha without a reference to x1 or x2?\n\nThe critic must infer \\alpha from common artifacts of interpolated datapoints alone. This is best illustrated in Figure 3(a) - note that as the interpolation morphs from one endpoint to the other, the image becomes dimmer and closer to a \"dot\" in the middle of the image. In this case, it is easy to infer \\alpha based on the length and brightness of the line. This is exactly the kind of behavior that ACAI seeks to discourage, and we find it's effective in practice. We have added some additional discussion of this point to our paper.\n\n> The paper states that \u201cencouraging this behavior also produce semantically smooth interpolation \u2026\u201d. Besides the empirical evidences from data, it would be better to any some theoretical justifications.\n\nOur approach can be viewed in the framework of adversarial divergences, where the critic network is being used to estimate a divergence . Of course, the exact form of this divergence is not clear, but it does provide a connection to the GAN theory literature. We have made this connection explicit in our updated draft."}, "signatures": ["ICLR.cc/2019/Conference/Paper73/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper73/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper73/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understanding and Improving Interpolation in Autoencoders via an Adversarial Regularizer", "abstract": "Autoencoders provide a powerful framework for learning compressed representations by encoding all of the information needed to reconstruct a data point in a latent code. In some cases, autoencoders can \"interpolate\": By decoding the convex combination of the latent codes for two datapoints, the autoencoder can produce an output which semantically mixes characteristics from the datapoints. In this paper, we propose a regularization procedure which encourages interpolated outputs to appear more realistic by fooling a critic network which has been trained to recover the mixing coefficient from interpolated data. We then develop a simple benchmark task where we can quantitatively measure the extent to which various autoencoders can interpolate and show that our regularizer dramatically improves interpolation in this setting. We also demonstrate empirically that our regularizer produces latent codes which are more effective on downstream tasks, suggesting a possible link between interpolation abilities and learning useful representations.", "paperhash": "berthelot|understanding_and_improving_interpolation_in_autoencoders_via_an_adversarial_regularizer", "TL;DR": "We propose a regularizer that improves interpolation and autoencoders and show that it also improves the learned representation for downstream tasks.", "authorids": ["dberth@google.com", "craffel@gmail.com", "aurkor@google.com", "goodfellow@google.com"], "authors": ["David Berthelot*", "Colin Raffel*", "Aurko Roy", "Ian Goodfellow"], "keywords": ["autoencoders", "interpolation", "unsupervised learning", "representation learning", "adversarial learning"], "pdf": "/pdf/6f8c41dcd45651f410da3b3fbe9d0fbdfe7765cf.pdf", "_bibtex": "@inproceedings{\nberthelot*2018understanding,\ntitle={Understanding and Improving Interpolation in Autoencoders via an Adversarial Regularizer},\nauthor={David Berthelot* and Colin Raffel* and Aurko Roy and Ian Goodfellow},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1fQSiCcYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper73/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621612785, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1fQSiCcYm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper73/Authors", "ICLR.cc/2019/Conference/Paper73/Reviewers", "ICLR.cc/2019/Conference/Paper73/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper73/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper73/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper73/Authors|ICLR.cc/2019/Conference/Paper73/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper73/Reviewers", "ICLR.cc/2019/Conference/Paper73/Authors", "ICLR.cc/2019/Conference/Paper73/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621612785}}}, {"id": "S1e8zyxY67", "original": null, "number": 2, "cdate": 1542156045894, "ddate": null, "tcdate": 1542156045894, "tmdate": 1542156045894, "tddate": null, "forum": "S1fQSiCcYm", "replyto": "rJlQxJGchX", "invitation": "ICLR.cc/2019/Conference/-/Paper73/Official_Comment", "content": {"title": "Re: Review for \"Understanding and Improving Interpolation in AE via an Adversarial Regularizer - Interesting Paper with good results.", "comment": "Thanks for your review, we are glad you found the paper interesting and significant. To address your questions and comments:\n\n> For the critic Loss L_d in equation (1) , the authors mention that the \\gamma based second term (that should ensure that the critic outputs 0 for non-interpolated inputs and expose the critic to realistic data even if the AE reconstruction is poor)  does not seem to be crucial in your approach but stabilized the adversarial training. Could you somehow quantify this. It seems like stability of the adversarial training should be paramount to your method to make sure the AE learns a better latent representation. This comment, even though I assume it well-founded, seems a bit of a contradiction.\n\nWe agree that this comment should be expanded on, and we have done so in our updated draft. To clarify, when we say it \"helped stabilize the adversarial learning process\", we mean that a) it allowed us to use the same value of \\lambda across all of our experiments and still achieve good results and b) it resulted in smooth convergence of the autoencoder's loss. We note that stability of the adversarial learning process was not an issue in general, in the sense that stability across runs was not an issue and our model never \"collapsed\" to a bad solution.\n\n> For the Lines synthetic data. It was chosen to use a 32x32 image size with 16 points length lines. This configuration does quantize directly the angles your measures can distinguish. Below a certain angle differences (or delta), 2 angles must have the same pixel representation, i.e. exact overlapping lines. My question is simple: What is the smallest angle you can use/distinguish or, how many exact unique lines can you have? \n\nOur code for synthesizing line images uses anti-aliasing, so for example a line with angle 0.3 and another with angle 0.300001 will be rendered differently. As a result, the number of unique lines is actually up to floating point precision. We think some confusion about this probably stems from the fact that we referred to the line images as \"black-and-white\"; we have updated the language in the paper to say \"grayscale\"."}, "signatures": ["ICLR.cc/2019/Conference/Paper73/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper73/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper73/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understanding and Improving Interpolation in Autoencoders via an Adversarial Regularizer", "abstract": "Autoencoders provide a powerful framework for learning compressed representations by encoding all of the information needed to reconstruct a data point in a latent code. In some cases, autoencoders can \"interpolate\": By decoding the convex combination of the latent codes for two datapoints, the autoencoder can produce an output which semantically mixes characteristics from the datapoints. In this paper, we propose a regularization procedure which encourages interpolated outputs to appear more realistic by fooling a critic network which has been trained to recover the mixing coefficient from interpolated data. We then develop a simple benchmark task where we can quantitatively measure the extent to which various autoencoders can interpolate and show that our regularizer dramatically improves interpolation in this setting. We also demonstrate empirically that our regularizer produces latent codes which are more effective on downstream tasks, suggesting a possible link between interpolation abilities and learning useful representations.", "paperhash": "berthelot|understanding_and_improving_interpolation_in_autoencoders_via_an_adversarial_regularizer", "TL;DR": "We propose a regularizer that improves interpolation and autoencoders and show that it also improves the learned representation for downstream tasks.", "authorids": ["dberth@google.com", "craffel@gmail.com", "aurkor@google.com", "goodfellow@google.com"], "authors": ["David Berthelot*", "Colin Raffel*", "Aurko Roy", "Ian Goodfellow"], "keywords": ["autoencoders", "interpolation", "unsupervised learning", "representation learning", "adversarial learning"], "pdf": "/pdf/6f8c41dcd45651f410da3b3fbe9d0fbdfe7765cf.pdf", "_bibtex": "@inproceedings{\nberthelot*2018understanding,\ntitle={Understanding and Improving Interpolation in Autoencoders via an Adversarial Regularizer},\nauthor={David Berthelot* and Colin Raffel* and Aurko Roy and Ian Goodfellow},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1fQSiCcYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper73/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621612785, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1fQSiCcYm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper73/Authors", "ICLR.cc/2019/Conference/Paper73/Reviewers", "ICLR.cc/2019/Conference/Paper73/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper73/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper73/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper73/Authors|ICLR.cc/2019/Conference/Paper73/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper73/Reviewers", "ICLR.cc/2019/Conference/Paper73/Authors", "ICLR.cc/2019/Conference/Paper73/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621612785}}}, {"id": "ryx6nAJFaQ", "original": null, "number": 1, "cdate": 1542155956913, "ddate": null, "tcdate": 1542155956913, "tmdate": 1542155956913, "tddate": null, "forum": "S1fQSiCcYm", "replyto": "HJeuQMG5n7", "invitation": "ICLR.cc/2019/Conference/-/Paper73/Official_Comment", "content": {"title": "Re: Regularize interpolation or regularize manifold?", "comment": "Thanks for your thorough review and questions. We've answered your questions below and have updated our draft to clarify.\n\n> Do we really need every interpolated point to be realistic (i.e. similar to a data point in the train-set)? I believe that there exists an interpolation between two totally different objects can never be observed.  \n\nWe are interested in latent spaces where interpolations produce realistic outputs across the entirety of the interpolation because this suggests some form of continuity in the latent space (as illustrated in FIgure 1). Our paper asks whether this property also results in an improved representation for downstream tasks. If an intermediate point was not realistic, the latent space might not have this property.\nThanks for pointing out that in some cases it's not obvious that there is a smooth and realistic path between two datapoints. We think two good examples of this are in Figure 6, bottom, where we interpolate between different MNIST digits. We find that even though there is no real digit which is at the midpoint of, for example, a 2 and a 9, the midpoint of ACAI's interpolation still appears realistic. We have added a note about this to our paper.\n\n> Do we need interpolation points to form a semantically smooth morphing? I guess this is a desired property for continuous generators, but it seems not necessary in general.\n\nWe agree that smoothness is not required for high-quality learned features -- for example, the Denoising Autoencoder fared well on our classification experiments despite producing poor interpolations. However, we are interested in the opposite, namely whether the ability to perform latent-space manipulations like interpolation suggest a better learned representation. We have added some clarification of this point in our paper.\n\n> The gamma in the 2nd term in (1) is confusing. If gamma = 1, I understand it forces to predict alpha = 0 since x is real. But if gamma < 1, the average in data space may be very blurry thus not realistic at all. How does gamma affect the optimization?\n\nNote \\hat{x} is a reconstruction of x, so in practice \\gamma*x + (1 \u2212 \\gamma)*\\hat{x} will be quite similar to x as long as \\hat{x} is a reasonable reconstruction. In other words, we are not interpolating between two totally different points, so typically the blurriness you might expect from pixel-space mixing won't be present. We have added some additional discussion of gamma and this term to our paper.\n\n> ACAI looks very similar to LSGAN: by giving \"0\" label to real data and \"alpha\" label to fake data; in LSGAN, alpha = 1. Have you tested a LSGAN like regularizer? \n\nYou're right that the LSGAN loss function and our regularization term are similar in the sense that both measure a squared error between the critic's output and a scalar. The difference is that the LSGAN is designed for use on a GAN-based generative model; our regularizer is designed as a regularizer for an autoencoder. As a result, the scalar in the LSGAN objective is a fixed hyperparameter whereas we regress the interpolation amount \\alpha. We added some discussion of the LSGAN objective to our paper.\n\n> The baselines are not representative: since ACAI introduces an adversarial regularizer, you should compare with other GAN techniques induced regularizers, such as WGAN regularized autoencoder. \n\nNote that the Wasserstein Autoencoder (WAE) is actually equivalent to an adversarial autoencoder when using a GAN loss; in the WAE paper [1] they write \"When c is the squared cost and D_Z is the GAN objective, WAE coincides with adversarial auto-encoders\". Our paper includes the adversarial autoencoder as a baseline (labeled AAE in tables and described in Section 3.2, paragraph 4). We added a citation to [1] to clarify this.\n\n[1] Ilya Tolstikhin, Olivier Bousquet, Sylvain Gelly and Bernhard Schoelkopf. \"Wasserstein Auto-Encoders\", ICLR 2017.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper73/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper73/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper73/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understanding and Improving Interpolation in Autoencoders via an Adversarial Regularizer", "abstract": "Autoencoders provide a powerful framework for learning compressed representations by encoding all of the information needed to reconstruct a data point in a latent code. In some cases, autoencoders can \"interpolate\": By decoding the convex combination of the latent codes for two datapoints, the autoencoder can produce an output which semantically mixes characteristics from the datapoints. In this paper, we propose a regularization procedure which encourages interpolated outputs to appear more realistic by fooling a critic network which has been trained to recover the mixing coefficient from interpolated data. We then develop a simple benchmark task where we can quantitatively measure the extent to which various autoencoders can interpolate and show that our regularizer dramatically improves interpolation in this setting. We also demonstrate empirically that our regularizer produces latent codes which are more effective on downstream tasks, suggesting a possible link between interpolation abilities and learning useful representations.", "paperhash": "berthelot|understanding_and_improving_interpolation_in_autoencoders_via_an_adversarial_regularizer", "TL;DR": "We propose a regularizer that improves interpolation and autoencoders and show that it also improves the learned representation for downstream tasks.", "authorids": ["dberth@google.com", "craffel@gmail.com", "aurkor@google.com", "goodfellow@google.com"], "authors": ["David Berthelot*", "Colin Raffel*", "Aurko Roy", "Ian Goodfellow"], "keywords": ["autoencoders", "interpolation", "unsupervised learning", "representation learning", "adversarial learning"], "pdf": "/pdf/6f8c41dcd45651f410da3b3fbe9d0fbdfe7765cf.pdf", "_bibtex": "@inproceedings{\nberthelot*2018understanding,\ntitle={Understanding and Improving Interpolation in Autoencoders via an Adversarial Regularizer},\nauthor={David Berthelot* and Colin Raffel* and Aurko Roy and Ian Goodfellow},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1fQSiCcYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper73/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621612785, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1fQSiCcYm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper73/Authors", "ICLR.cc/2019/Conference/Paper73/Reviewers", "ICLR.cc/2019/Conference/Paper73/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper73/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper73/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper73/Authors|ICLR.cc/2019/Conference/Paper73/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper73/Reviewers", "ICLR.cc/2019/Conference/Paper73/Authors", "ICLR.cc/2019/Conference/Paper73/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621612785}}}, {"id": "rJlQxJGchX", "original": null, "number": 2, "cdate": 1541181162753, "ddate": null, "tcdate": 1541181162753, "tmdate": 1541534308272, "tddate": null, "forum": "S1fQSiCcYm", "replyto": "S1fQSiCcYm", "invitation": "ICLR.cc/2019/Conference/-/Paper73/Official_Review", "content": {"title": "Review for \"Understanding and Improving Interpolation in AE via an Adversarial Regularizer - Interesting Paper with good results.", "review": "Summary: The authors propose a new approach to encourage valid interpolation in Auto-Encoders (AE). It is based on a regularization procedure involving a critic network judging the realistic nature of reconstructed data point from its mixed latent representations by recovering the mixing coefficient. The authors show that this approach does indeed improve the quality of interpolated samples on few tasks. A synthetic tasks of lines interpolation (proposing new Mean Distance and Smoothness metric for this task), classification task (with a single-layer classifier) from the latent space representation and finally a clustering accuracy on the latent space. On the proposed regularization method seems to help significantly compared to commonly used AE architectures (Basic AE, Denoising AE, Variational AE, Adversarial AE and VQ-VAE).\n\nThis paper was a very interesting read, and the work seems to be of significance for the unsupervised learning community.\nIt was clearly written and conveys the contributions clearly and the experimental results and their interpretations seem valid.\n\nThe proposed approach of a critic based regularizer is a simple but seemingly important addition that contributes to improving interpolation in AE significantly and even show impact \"downstream tasks\" as the authors put it.\n\nFew comments/questions come to mind:\n\n- For the critic Loss L_d in equation (1) , the authors mention that the \\gamma based second term (that should ensure that the critic outputs 0 for non-interpolated inputs and expose the critic to realistic data even if the AE reconstruction is poor)  does not seem to be crucial in your approach but stabilized the adversarial training. Could you somehow quantify this. It seems like stability of the adversarial training should be paramount to your method to make sure the AE learns a better latent representation. This comment, even though I assume it well-founded, seems a bit of a contradiction.\n\n- For the Lines synthetic data. It was chosen to use a 32x32 image size with 16 points length lines. This configuration does quantize directly the angles your measures can distinguish. Below a certain angle differences (or delta), 2 angles must have the same pixel representation, i.e. exact overlapping lines. My question is simple: What is the smallest angle you can use/distinguish or, how many exact unique lines can you have? \n\nOverall this is a good paper that deserves publications.", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper73/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understanding and Improving Interpolation in Autoencoders via an Adversarial Regularizer", "abstract": "Autoencoders provide a powerful framework for learning compressed representations by encoding all of the information needed to reconstruct a data point in a latent code. In some cases, autoencoders can \"interpolate\": By decoding the convex combination of the latent codes for two datapoints, the autoencoder can produce an output which semantically mixes characteristics from the datapoints. In this paper, we propose a regularization procedure which encourages interpolated outputs to appear more realistic by fooling a critic network which has been trained to recover the mixing coefficient from interpolated data. We then develop a simple benchmark task where we can quantitatively measure the extent to which various autoencoders can interpolate and show that our regularizer dramatically improves interpolation in this setting. We also demonstrate empirically that our regularizer produces latent codes which are more effective on downstream tasks, suggesting a possible link between interpolation abilities and learning useful representations.", "paperhash": "berthelot|understanding_and_improving_interpolation_in_autoencoders_via_an_adversarial_regularizer", "TL;DR": "We propose a regularizer that improves interpolation and autoencoders and show that it also improves the learned representation for downstream tasks.", "authorids": ["dberth@google.com", "craffel@gmail.com", "aurkor@google.com", "goodfellow@google.com"], "authors": ["David Berthelot*", "Colin Raffel*", "Aurko Roy", "Ian Goodfellow"], "keywords": ["autoencoders", "interpolation", "unsupervised learning", "representation learning", "adversarial learning"], "pdf": "/pdf/6f8c41dcd45651f410da3b3fbe9d0fbdfe7765cf.pdf", "_bibtex": "@inproceedings{\nberthelot*2018understanding,\ntitle={Understanding and Improving Interpolation in Autoencoders via an Adversarial Regularizer},\nauthor={David Berthelot* and Colin Raffel* and Aurko Roy and Ian Goodfellow},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1fQSiCcYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper73/Official_Review", "cdate": 1542234543943, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "S1fQSiCcYm", "replyto": "S1fQSiCcYm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper73/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335643105, "tmdate": 1552335643105, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper73/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "B1gmB4Kv2m", "original": null, "number": 1, "cdate": 1541014587043, "ddate": null, "tcdate": 1541014587043, "tmdate": 1541534308066, "tddate": null, "forum": "S1fQSiCcYm", "replyto": "S1fQSiCcYm", "invitation": "ICLR.cc/2019/Conference/-/Paper73/Official_Review", "content": {"title": "An interesting regularized AE algorithm that improves interpolation in latent space", "review": "This paper proposed an adversarially regularized AE algorithm that improve interpolation in latent space. Specifically, a critic is used to predict the interpolation weight \\alpha and encourage the interpolated images to be more realistic. The paper verified the method on a newly proposed synthetic line benchmark and on downstream classification and clustering tasks.\n\nPros:\n1.\tA novel algorithm that promotes the interpolation ability of AE\n2.\tA new synthesized line benchmark to verify the interpolation ability of different AE variants\n3.\tStrong results on downstream classification and clustering tasks\n\nCons: \n1.\tThe interplay of the adversarial network (between AE and critic) isn\u2019t very clear and can be improved\n2.\tEq. 1, should x be x_1 or a new data other than x1 and x2?\n3.\tThe paper states that the 2nd term of Eq. 1 isn\u2019t crucial. If x is a new data (other than x1 or x2), how can the critic infer \\alpha without a reference to x1 or x2?\n4.\tThe paper states that \u201cencouraging this behavior also produce semantically smooth interpolation \u2026\u201d. Besides the empirical evidences from data, it would be better to any some theoretical justifications.\n", "rating": "9: Top 15% of accepted papers, strong accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper73/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understanding and Improving Interpolation in Autoencoders via an Adversarial Regularizer", "abstract": "Autoencoders provide a powerful framework for learning compressed representations by encoding all of the information needed to reconstruct a data point in a latent code. In some cases, autoencoders can \"interpolate\": By decoding the convex combination of the latent codes for two datapoints, the autoencoder can produce an output which semantically mixes characteristics from the datapoints. In this paper, we propose a regularization procedure which encourages interpolated outputs to appear more realistic by fooling a critic network which has been trained to recover the mixing coefficient from interpolated data. We then develop a simple benchmark task where we can quantitatively measure the extent to which various autoencoders can interpolate and show that our regularizer dramatically improves interpolation in this setting. We also demonstrate empirically that our regularizer produces latent codes which are more effective on downstream tasks, suggesting a possible link between interpolation abilities and learning useful representations.", "paperhash": "berthelot|understanding_and_improving_interpolation_in_autoencoders_via_an_adversarial_regularizer", "TL;DR": "We propose a regularizer that improves interpolation and autoencoders and show that it also improves the learned representation for downstream tasks.", "authorids": ["dberth@google.com", "craffel@gmail.com", "aurkor@google.com", "goodfellow@google.com"], "authors": ["David Berthelot*", "Colin Raffel*", "Aurko Roy", "Ian Goodfellow"], "keywords": ["autoencoders", "interpolation", "unsupervised learning", "representation learning", "adversarial learning"], "pdf": "/pdf/6f8c41dcd45651f410da3b3fbe9d0fbdfe7765cf.pdf", "_bibtex": "@inproceedings{\nberthelot*2018understanding,\ntitle={Understanding and Improving Interpolation in Autoencoders via an Adversarial Regularizer},\nauthor={David Berthelot* and Colin Raffel* and Aurko Roy and Ian Goodfellow},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1fQSiCcYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper73/Official_Review", "cdate": 1542234543943, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "S1fQSiCcYm", "replyto": "S1fQSiCcYm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper73/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335643105, "tmdate": 1552335643105, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper73/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 23}