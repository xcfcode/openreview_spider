{"notes": [{"id": "uxYjVEXx48i", "original": "GdLM09I_PGn", "number": 1259, "cdate": 1601308140835, "ddate": null, "tcdate": 1601308140835, "tmdate": 1614985711460, "tddate": null, "forum": "uxYjVEXx48i", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "An Examination of Preference-based Reinforcement Learning for Treatment Recommendation", "authorids": ["~Nan_Xu2", "~Nitin_Kamra1", "~Yan_Liu1"], "authors": ["Nan Xu", "Nitin Kamra", "Yan Liu"], "keywords": ["Preference-based Reinforcement Learning", "Treatment Recommendation", "healthcare"], "abstract": "Treatment recommendation is a complex multi-faceted problem with many conflicting objectives, e.g., optimizing the survival rate (or expected lifetime), mitigating negative impacts, reducing financial expenses and time costs, avoiding over-treatment, etc. While this complicates the hand-engineering of a reward function for learning treatment policies, fortunately, qualitative feedback from human experts is readily available and can be easily exploited.  Since direct estimation of rewards via inverse reinforcement learning is a challenging task and requires the existence of an optimal human policy, the field of treatment recommendation has recently witnessed the development of the preference-based ReinforcementLearning (PRL) framework, which infers a reward function from only qualitative and imperfect human feedback to ensure that a human expert\u2019s preferred policy has a higher expected return over a less preferred policy. In this paper, we first present an open simulation platform to model the progression of two diseases, namely Cancer and Sepsis, and the reactions of the affected individuals to the received treatment. Secondly, we investigate important problems in adopting preference-basedRL approaches for treatment recommendation, such as advantages of learning from preference over hand-engineered reward, addressing incomparable policies, reward interpretability, and agent design via simulated experiments. The designed simulation platform and insights obtained for preference-based RL approaches are beneficial for achieving the right trade-off between various human objectives during treatment recommendation.", "one-sentence_summary": "Develop a simulation platform and investigate preference-based reinforcement learning approaches for treatment recommendation", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xu|an_examination_of_preferencebased_reinforcement_learning_for_treatment_recommendation", "supplementary_material": "/attachment/2584da0e9892dbc00de335fbf80dab9fecaf2e9d.zip", "pdf": "/pdf/ab9d9eaf60be6f66ecde42086838f1fc3f3a6542.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=pMKUh7w7PE", "_bibtex": "@misc{\nxu2021an,\ntitle={An Examination of Preference-based Reinforcement Learning for Treatment Recommendation},\nauthor={Nan Xu and Nitin Kamra and Yan Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=uxYjVEXx48i}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 13, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "Y4xPi753NiR", "original": null, "number": 1, "cdate": 1610040431343, "ddate": null, "tcdate": 1610040431343, "tmdate": 1610474031374, "tddate": null, "forum": "uxYjVEXx48i", "replyto": "uxYjVEXx48i", "invitation": "ICLR.cc/2021/Conference/Paper1259/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "Reviewers agree that this work is promising. The paper is well-grounded in the literature and different aspects of the considered methods are investigated through a variety of experiments. Unfortunately, this paper does not provide sufficient details to allow the reader to understand what has been done nor how to adequately build from it. For example, details in the Appendix lack sufficient formalization of the equations or concepts used to train the preference-based agents. The paper would benefit from clarifications of the method, procedures, and equations used. Beyond that, a major concern lied within the evaluation of the simulated patients across different initializations. Provided that one of the proposed contributions of this paper is a robust simulation platform for RL research within healthcare, it would be important to report convincing results on the patient physiologies admitted by the simulator and characterizing the behaviors of policies learned using this simulator. Finally, issues regarding the structure of the paper, including the split between the main paper and the Appendix, should be resolved before this paper can be published. Notably, the authors should consider elevating important material from the Appendix into the main paper."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An Examination of Preference-based Reinforcement Learning for Treatment Recommendation", "authorids": ["~Nan_Xu2", "~Nitin_Kamra1", "~Yan_Liu1"], "authors": ["Nan Xu", "Nitin Kamra", "Yan Liu"], "keywords": ["Preference-based Reinforcement Learning", "Treatment Recommendation", "healthcare"], "abstract": "Treatment recommendation is a complex multi-faceted problem with many conflicting objectives, e.g., optimizing the survival rate (or expected lifetime), mitigating negative impacts, reducing financial expenses and time costs, avoiding over-treatment, etc. While this complicates the hand-engineering of a reward function for learning treatment policies, fortunately, qualitative feedback from human experts is readily available and can be easily exploited.  Since direct estimation of rewards via inverse reinforcement learning is a challenging task and requires the existence of an optimal human policy, the field of treatment recommendation has recently witnessed the development of the preference-based ReinforcementLearning (PRL) framework, which infers a reward function from only qualitative and imperfect human feedback to ensure that a human expert\u2019s preferred policy has a higher expected return over a less preferred policy. In this paper, we first present an open simulation platform to model the progression of two diseases, namely Cancer and Sepsis, and the reactions of the affected individuals to the received treatment. Secondly, we investigate important problems in adopting preference-basedRL approaches for treatment recommendation, such as advantages of learning from preference over hand-engineered reward, addressing incomparable policies, reward interpretability, and agent design via simulated experiments. The designed simulation platform and insights obtained for preference-based RL approaches are beneficial for achieving the right trade-off between various human objectives during treatment recommendation.", "one-sentence_summary": "Develop a simulation platform and investigate preference-based reinforcement learning approaches for treatment recommendation", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xu|an_examination_of_preferencebased_reinforcement_learning_for_treatment_recommendation", "supplementary_material": "/attachment/2584da0e9892dbc00de335fbf80dab9fecaf2e9d.zip", "pdf": "/pdf/ab9d9eaf60be6f66ecde42086838f1fc3f3a6542.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=pMKUh7w7PE", "_bibtex": "@misc{\nxu2021an,\ntitle={An Examination of Preference-based Reinforcement Learning for Treatment Recommendation},\nauthor={Nan Xu and Nitin Kamra and Yan Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=uxYjVEXx48i}\n}"}, "tags": [], "invitation": {"reply": {"forum": "uxYjVEXx48i", "replyto": "uxYjVEXx48i", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040431329, "tmdate": 1610474031358, "id": "ICLR.cc/2021/Conference/Paper1259/-/Decision"}}}, {"id": "RWI9jII764", "original": null, "number": 3, "cdate": 1603868706084, "ddate": null, "tcdate": 1603868706084, "tmdate": 1606805680533, "tddate": null, "forum": "uxYjVEXx48i", "replyto": "uxYjVEXx48i", "invitation": "ICLR.cc/2021/Conference/Paper1259/-/Official_Review", "content": {"title": "Interesting concepts and extensive experimental justification; Unfortunately incomplete and unclear technical development", "review": "#### **Summary**\nThis paper develops a preference-based framework for Reinforcement Learning in Healthcare, focusing on treatment recommendation. The paper highlights several factors where policies trained with preferences may be better suited for use in healthcare applications and extensively evaluates these factors using two simulated care scenarios. \n\n\n#### **Assessment**\nThe foundational concepts underlying this paper are strong and the authors make good points framing the potential advantage of using preference-based rewards over what is termed a \u201chandcrafted reward\u201d. Reward design is an open challenge in Reinforcement Learning, particularly in Healthcare settings, and this paper proposes an intriguing way to perhaps avoid explicitly designing a reward while still providing improved performance. However there are significant gaps in the technical development of the paper that greatly reduce the clarity and perceived significance of the presented work (several examples highlighted in the \u201cWeaknesses\u201d section below). Several claims are made in the introduction and set-up of the experiments that are not fully supported, discussed or justified. Finally, there are some major concerns about the suitability of the simulated environments to adequately evaluate the contributions of the proposed methods (more in the \u201cWeaknesses\u201d section below) in light of the applicability to real world dynamics.\n\n\n#### **Strengths**\n- The paper presents an appealing rationale for the use of preference-based RL in complex sequential decision making problems where reward design is a significant challenge. The paper is firmly grounded in relevant literature. There are several papers that I expected to be included (see Relevant Literature in the \u201cGeneral Comments\u201d section) but their omission is not a critical error.\n- The experimental investigation is ambitious and covers most, if not all, relevant questions regarding the utility of preference-based rewards in treatment recommendation. Several relevant baselines are used to compare and contrast the two proposed preference based RL approaches, highlighting the apparent strengths of the proposed method.\n- Specifically, I found the analysis and experiments about transferability and how the proposed approach handles \u201cincomparable policies\u201d to be really interesting. Only transferring the parameters of the estimated reward functions instead of the policies themselves is a really interesting idea and could stand a far more detailed and extensive study in its own right.\n- The introduction of additional simulated treatment domains will be a nice contribution to the community as there are only a small handful of stable simulators available within the RL for healthcare domain.\n\n\n#### **Weaknesses**\n\nI found several weaknesses in the presentation and development of the proposed approach. Throughout the paper concepts are not fully detailed and are only vaguely referred to, seemingly in expectation that the reader implicitly understands or knows what is meant. The major offense in this direction is that the \u201chuman\u201d preference-based rewards are never explicitly defined. It\u2019s not clear from the formulation how the returns or observations from the environments are used to compute the rewards under this framing. I read the paper and appendices several times to look for a formalization of these reward definitions and was disappointed to not find them. The closest I could come was in the clinical efficacy and \u201cother factors\u201d metrics. But I didn\u2019t feel that these could be the defined preferences because the authors continually draw a distinction between hand designed rewards and preference-based rewards determined by human intentions. \n\nAlong this point, there are continual references to human preferences or human feedback guiding reward design. However, isn\u2019t this just a human-in-the-loop version of handcrafting a reward? Also, it\u2019s unclear that a clinician will be able to interpret the neural network policies and learned reward representations to provide adequate guidance and preference determination. In the introduction the following statement is made: \u201cFortunately, qualitative feedback according to human\u2019s preferences can be easily obtained and efficiently leveraged\u2026\u201d This is unfortunately not true in regards to clinical decision making without resulting in overly biased evaluations. Individual clinicians differ in their interpretation of patient conditions as well as the appropriate route of treatment. Without belaboring the point, it\u2019s not clear how establishing the preferences between policies is not just a different form of handcrafting a reward function. Missing from the paper is a discussion about how these preferences would be obtained and integrated into the development of learning a policy. Based on the conclusion it appears that this wasn\u2019t actually incorporated in this paper leaving major questions about the proposed direction and claims made in the paper about handcrafted vs. preference based rewards. Pessimistically, the observed gains with AbRM and SbRM could be attributed to ensembling (not altogether novel given recent advances in offline RL utilizing multiple agents to deal with overestimation within the learned value functions) or more informative reward design. \n\nOn this point regarding overestimation. There are major concerns that the preference based reward and how the individual reward functions (and agents) are trained will largely suffer from overconfidence, a standard problem in RL. Simulators largely cover over this limitation because unrealistic, non-physical behaviors or actions are still supported. In practice, RL in healthcare will be used in offline, off-policy settings where overconfidence and extrapolation to actions not seen in the training set will lead to ineffective and, in the worst case, fatal treatment policies (see Gottesman, et al [2019]). Simulators can only get us so far without specific guarantees about how realistic and representative they are of clinical and physiological reality. These limitations are not discussed or acknowledged in the paper in any way.\n\n*Other points of weakness in the paper:*\n\n- There are extensive experiments performed to demonstrate the advantages of the proposed learning approach yet the discussion and presentation of the results is unfocused and difficult to follow in places. There are four experimental questions raised in the introduction and at the end of Section 4. The presented results do not address or answer these questions directly. A good place to do this would have been in a discussion section at the end of Section 5. This being said, as written, the fourth question is not really a question at all. It rather stands as a statement of what should be an objective of the paper, demonstrating how to build preference-based agents.\n- Section 3 mentions the development of a platform. This however was never discussed or introduced. Typically the concept of a simulation platform denotes a standard tool or API such as OpenAI Gym or Mujoco. This was not formalized and provided a sense that the authors were trying to make unsupported claims about the paper\u2019s significance.\n- How are the environments interacted with? There are no formalized definitions of what the state or action spaces are. It is never explained how long trajectories are (ie. how many treatment decisions are possibly made for an individual patient). What are the relevant parameters of variation within the simulated dynamics? Equations for the treatment dynamics of the cancer and sepsis simulators are presented without any description of what the variables represent. How are patient physiologies varied? (patient types are mentioned in the paper) How different are the physiological responses? Do they admit different policies (aside from the trivial behavior observed in the sepsis simulator)? Provided the lack of variation in the reported results over the different data sets (namely in Figures 2, 3, 4, etc), it appears that there\u2019s no variation nor differences between the training and validation sets. (Why wasn\u2019t the test set evaluations presented even though it\u2019s mentioned in the baselines?)\n- The parameterizations of the reward estimators $R$ are never described. How does performance change based on the \u201caccuracy\u201d of these estimators?\n- What does it mean to sample a patient/subject? Why is the number of patients fixed for each subset of the data? Are these numbers proxies for the number of training episodes used to learn and evaluate policies with? What does it mean to validate an RL policy when it appears policies are optimized from scratch in Figures 2, 4 and etc.?\n- What is the \u201cupper bound\u201d presented in Figure 1?\n- Why are there different time scales between Figure 2 (a) and (b)? Why isn\u2019t expected return plotted in Figure 2(b)?\n- There isn\u2019t a clear separation between the evaluated metrics and the rewards/expected returns. Of course if one increases, the other should as well, right? \n\n\n#### **General Comments**\nOverall, I found this paper to be incomplete despite the extensive experiments. There are some great ideas at the root of the proposed approach to learning policies for treatment recommendation but there was far too little technical development for me to be confident about the stated contributions. There were not enough clear explanations or definitions of important and focal components of the proposed approach. Also, it\u2019s not clear that the simulators are best suited for comparing the proposed approach with the baselines, the fact that the validation and training curves match exactly raise significant doubts that there is any variation between settings within the simulators. This being said, there are also minor concerns about the use of policy gradient approaches within a healthcare setting. Rollouts using inaccurate policies and approximate dynamics models will not be representative of real-world physiologies. Policies developed from non-physical behavior cannot be deployed in practice and are not at all reliable. While PG approaches are admissible in simulators, their suitability is extremely limited in real settings.\n\nUltimately, I feel that my opinion of the paper could be improved if sufficiently clear descriptions of the following concepts were provided:\n- What explicit preferences were used for extracting the rewards for training AbRM and SbRM.\n- Related, what are the target rewards for the learned reward functions $R$?\n- What is the loss function for the agent? (Alg 3., Line 10)\n- How are patient physiologies varied in the simulators? Are the differences meaningful (ie. do they provide unique policies)?\n\n\n\n**Relevant Literature**\nThere were a few pieces of prior work that I expected to be included in this paper given its focus on reward design and the definition of objectives. Within the healthcare space there has been some notable work done to address reward design for RL approaches. Recently, Prasad, et al [ACM CHIL; 2020] looked into admissible reward functions for acute care.\n\nIn the IRL space, reward design has been a large area of research. Among several good papers I wanted to highlight Hadfield-Menell, et al [NeurIPS; 2017] and Shah, et al [ICML; 2019].\n\nFor Multi-objective Markov Decision Processes, I would refer the authors to Lizotte and Laber [JMLR; 2016].\n\nFinally, Yu, et al [arxiv; 2019] put together a decent survey of RL in Healthcare that might help round out the setting of the proposed preference-based approach.\n\n\nPrasad, Niranjani, Barbara Engelhardt, and Finale Doshi-Velez. \"Defining admissible rewards for high-confidence policy evaluation in batch reinforcement learning.\" Proceedings of the ACM Conference on Health, Inference, and Learning. 2020.\n\nHadfield-Menell, Dylan, et al. \"Inverse reward design.\" Advances in neural information processing systems. 2017.\n\nShah, Rohin, et al. \"On the Feasibility of Learning, Rather than Assuming, Human Biases for Reward Inference.\" International Conference on Machine Learning. 2019.\n\nLizotte, Daniel J., and Eric B. Laber. \"Multi-objective Markov decision processes for data-driven decision support.\" The Journal of Machine Learning Research 17.1 (2016): 7378-7405.\n\nYu, Chao, Jiming Liu, and Shamim Nemati. \"Reinforcement learning in healthcare: A survey.\" arXiv preprint arXiv:1908.08796 (2019).", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1259/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1259/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An Examination of Preference-based Reinforcement Learning for Treatment Recommendation", "authorids": ["~Nan_Xu2", "~Nitin_Kamra1", "~Yan_Liu1"], "authors": ["Nan Xu", "Nitin Kamra", "Yan Liu"], "keywords": ["Preference-based Reinforcement Learning", "Treatment Recommendation", "healthcare"], "abstract": "Treatment recommendation is a complex multi-faceted problem with many conflicting objectives, e.g., optimizing the survival rate (or expected lifetime), mitigating negative impacts, reducing financial expenses and time costs, avoiding over-treatment, etc. While this complicates the hand-engineering of a reward function for learning treatment policies, fortunately, qualitative feedback from human experts is readily available and can be easily exploited.  Since direct estimation of rewards via inverse reinforcement learning is a challenging task and requires the existence of an optimal human policy, the field of treatment recommendation has recently witnessed the development of the preference-based ReinforcementLearning (PRL) framework, which infers a reward function from only qualitative and imperfect human feedback to ensure that a human expert\u2019s preferred policy has a higher expected return over a less preferred policy. In this paper, we first present an open simulation platform to model the progression of two diseases, namely Cancer and Sepsis, and the reactions of the affected individuals to the received treatment. Secondly, we investigate important problems in adopting preference-basedRL approaches for treatment recommendation, such as advantages of learning from preference over hand-engineered reward, addressing incomparable policies, reward interpretability, and agent design via simulated experiments. The designed simulation platform and insights obtained for preference-based RL approaches are beneficial for achieving the right trade-off between various human objectives during treatment recommendation.", "one-sentence_summary": "Develop a simulation platform and investigate preference-based reinforcement learning approaches for treatment recommendation", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xu|an_examination_of_preferencebased_reinforcement_learning_for_treatment_recommendation", "supplementary_material": "/attachment/2584da0e9892dbc00de335fbf80dab9fecaf2e9d.zip", "pdf": "/pdf/ab9d9eaf60be6f66ecde42086838f1fc3f3a6542.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=pMKUh7w7PE", "_bibtex": "@misc{\nxu2021an,\ntitle={An Examination of Preference-based Reinforcement Learning for Treatment Recommendation},\nauthor={Nan Xu and Nitin Kamra and Yan Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=uxYjVEXx48i}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "uxYjVEXx48i", "replyto": "uxYjVEXx48i", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1259/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538122814, "tmdate": 1606915781279, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1259/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1259/-/Official_Review"}}}, {"id": "pjow_Cbhvfj", "original": null, "number": 15, "cdate": 1606247306604, "ddate": null, "tcdate": 1606247306604, "tmdate": 1606247306604, "tddate": null, "forum": "uxYjVEXx48i", "replyto": "Ni49i3yV8Sh", "invitation": "ICLR.cc/2021/Conference/Paper1259/-/Official_Comment", "content": {"title": "read answers to main questions", "comment": "Thank you for your comprehensive reply. As I said in my review, you are doing solid work, but the original draft was deficient. I appreciate your effort, and many thanks for writing up the answers. I don't have any other questions to you. We will further discuss your submission with the other reviewers.\nAll the best."}, "signatures": ["ICLR.cc/2021/Conference/Paper1259/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1259/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An Examination of Preference-based Reinforcement Learning for Treatment Recommendation", "authorids": ["~Nan_Xu2", "~Nitin_Kamra1", "~Yan_Liu1"], "authors": ["Nan Xu", "Nitin Kamra", "Yan Liu"], "keywords": ["Preference-based Reinforcement Learning", "Treatment Recommendation", "healthcare"], "abstract": "Treatment recommendation is a complex multi-faceted problem with many conflicting objectives, e.g., optimizing the survival rate (or expected lifetime), mitigating negative impacts, reducing financial expenses and time costs, avoiding over-treatment, etc. While this complicates the hand-engineering of a reward function for learning treatment policies, fortunately, qualitative feedback from human experts is readily available and can be easily exploited.  Since direct estimation of rewards via inverse reinforcement learning is a challenging task and requires the existence of an optimal human policy, the field of treatment recommendation has recently witnessed the development of the preference-based ReinforcementLearning (PRL) framework, which infers a reward function from only qualitative and imperfect human feedback to ensure that a human expert\u2019s preferred policy has a higher expected return over a less preferred policy. In this paper, we first present an open simulation platform to model the progression of two diseases, namely Cancer and Sepsis, and the reactions of the affected individuals to the received treatment. Secondly, we investigate important problems in adopting preference-basedRL approaches for treatment recommendation, such as advantages of learning from preference over hand-engineered reward, addressing incomparable policies, reward interpretability, and agent design via simulated experiments. The designed simulation platform and insights obtained for preference-based RL approaches are beneficial for achieving the right trade-off between various human objectives during treatment recommendation.", "one-sentence_summary": "Develop a simulation platform and investigate preference-based reinforcement learning approaches for treatment recommendation", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xu|an_examination_of_preferencebased_reinforcement_learning_for_treatment_recommendation", "supplementary_material": "/attachment/2584da0e9892dbc00de335fbf80dab9fecaf2e9d.zip", "pdf": "/pdf/ab9d9eaf60be6f66ecde42086838f1fc3f3a6542.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=pMKUh7w7PE", "_bibtex": "@misc{\nxu2021an,\ntitle={An Examination of Preference-based Reinforcement Learning for Treatment Recommendation},\nauthor={Nan Xu and Nitin Kamra and Yan Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=uxYjVEXx48i}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "uxYjVEXx48i", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1259/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1259/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1259/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1259/Authors|ICLR.cc/2021/Conference/Paper1259/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1259/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923861779, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1259/-/Official_Comment"}}}, {"id": "mWEJum55EQp", "original": null, "number": 14, "cdate": 1606246761252, "ddate": null, "tcdate": 1606246761252, "tmdate": 1606246761252, "tddate": null, "forum": "uxYjVEXx48i", "replyto": "f9NugFT3Oup", "invitation": "ICLR.cc/2021/Conference/Paper1259/-/Official_Comment", "content": {"title": "read answers to small issues", "comment": "Thank you for taking the time to write these explanations. The paper would read much better if these details were in it. I hope that you will add them."}, "signatures": ["ICLR.cc/2021/Conference/Paper1259/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1259/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An Examination of Preference-based Reinforcement Learning for Treatment Recommendation", "authorids": ["~Nan_Xu2", "~Nitin_Kamra1", "~Yan_Liu1"], "authors": ["Nan Xu", "Nitin Kamra", "Yan Liu"], "keywords": ["Preference-based Reinforcement Learning", "Treatment Recommendation", "healthcare"], "abstract": "Treatment recommendation is a complex multi-faceted problem with many conflicting objectives, e.g., optimizing the survival rate (or expected lifetime), mitigating negative impacts, reducing financial expenses and time costs, avoiding over-treatment, etc. While this complicates the hand-engineering of a reward function for learning treatment policies, fortunately, qualitative feedback from human experts is readily available and can be easily exploited.  Since direct estimation of rewards via inverse reinforcement learning is a challenging task and requires the existence of an optimal human policy, the field of treatment recommendation has recently witnessed the development of the preference-based ReinforcementLearning (PRL) framework, which infers a reward function from only qualitative and imperfect human feedback to ensure that a human expert\u2019s preferred policy has a higher expected return over a less preferred policy. In this paper, we first present an open simulation platform to model the progression of two diseases, namely Cancer and Sepsis, and the reactions of the affected individuals to the received treatment. Secondly, we investigate important problems in adopting preference-basedRL approaches for treatment recommendation, such as advantages of learning from preference over hand-engineered reward, addressing incomparable policies, reward interpretability, and agent design via simulated experiments. The designed simulation platform and insights obtained for preference-based RL approaches are beneficial for achieving the right trade-off between various human objectives during treatment recommendation.", "one-sentence_summary": "Develop a simulation platform and investigate preference-based reinforcement learning approaches for treatment recommendation", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xu|an_examination_of_preferencebased_reinforcement_learning_for_treatment_recommendation", "supplementary_material": "/attachment/2584da0e9892dbc00de335fbf80dab9fecaf2e9d.zip", "pdf": "/pdf/ab9d9eaf60be6f66ecde42086838f1fc3f3a6542.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=pMKUh7w7PE", "_bibtex": "@misc{\nxu2021an,\ntitle={An Examination of Preference-based Reinforcement Learning for Treatment Recommendation},\nauthor={Nan Xu and Nitin Kamra and Yan Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=uxYjVEXx48i}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "uxYjVEXx48i", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1259/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1259/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1259/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1259/Authors|ICLR.cc/2021/Conference/Paper1259/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1259/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923861779, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1259/-/Official_Comment"}}}, {"id": "youwIir2Lea", "original": null, "number": 8, "cdate": 1606159772033, "ddate": null, "tcdate": 1606159772033, "tmdate": 1606159772033, "tddate": null, "forum": "uxYjVEXx48i", "replyto": "RWI9jII764", "invitation": "ICLR.cc/2021/Conference/Paper1259/-/Official_Comment", "content": {"title": "Some minor points to clarify", "comment": "**Observed gains with AbRM and SbRM**: Since two agents with different parameter initializations are used in the PRL framework, so for fair comparison, we also have a compared method called Single-objective RL (Ensemble) in Table 1 for Cancer and RL (Ensemble) in Figure 1, where two agents are trained separately and the one with better performance on the validation set is evaluated on the testing set.  \n\n**Off-policy settings**: learning policies from simulation and from observations are two branches of approaches to perform treatment recommendations. We also conduct additional experiments about off-policy learning and evaluation on Cancer treatment recommendation: Given trajectory histories of a policy gradient learner with 26.96% survival rate (behavioral), we train another DQN learner without interactions with the simulator (learner). We then test the learner in the simulator and find that the achieved survival rate has dropped to 16.12%. We further illustrate the inaccuracy of existing importance sampling approaches for off-policy evaluation in Appendix Fig. 10 (also see in the link: https://drive.google.com/file/d/1T2OA4QG9bvlSYgVkRsQExchkk_WhrGRU/view?usp=sharing ).  When testing the well-trained agents on simulators, the actual expected return mainly focuses on \u22121 since extremely low survival rate is achieved by agents learning policies from other policies. However, the expected returns estimated by Per-decision Importance Sampling (PDIS), Weighted Per-decision Importance Sampling (WPDIS), Doubly-robust (DR) and Weighted Doubly-robust(WDR) distribute quite differently from the ground-truth. Both of the above unreliable off-policy evaluations and inferior treatment outcomes resulted from policy learning based on observational data demonstrate the importance of the provided simulation platform in enabling optimal policy learning and ensuring accurate policy evaluations.\n\n**Simulation platform**: Typically, the agent receives the current time and state from the simulator, and then sends the action back. We will upload the simulator codes and document later. There is no particular domain shift between training and testing set, but distinctions exist among different sampled subjects. Some details about the simulator are as follows: \n* Cancer: there are two state features: tumor size and toxicity level; there are four possible actions to take: 0.1, 0.4, 0.7, 1.0 (representing the dosage amount). One subject experiences at most 6 time-step simulations if he can survive to the end, each representing one month in the real-world. State transition dynamics are formulated in Section 3.1. \n* Sepsis: there are 19 physiological state features, 8 of which are observable while the remaining 11 are unmeasurable conceptual variables; there are two possible actions to take, one is performing blood purification operations and the other is no operation. One subject experiences 1000 time-step simulation, each representing 0.1 h in real-world. There are 21 ODEs to represent state transitions, some of them are important and have been listed in Section 3.2 and A.3.2. The interaction network of inflammatory responses and hypothetic hemoadsorption mechanisms is also demonstrated in Figure 12 (Appendix).\n\n**Parameterization**: both agent and reward learner are parameterized by two-layer MLPs for Cancer (MDP) and two-layer LSTMs for Sepsis (POMDP). Figure 3 can reflect the relationship between reward estimator accuracy and policy performance. While preserving the incomparable policy for reward estimator (blue curves), the preference over incomparable policy becomes stable around 10th epoch (Fig.3 (a)), till which the policy learner has steady increasing survival rate (Fig.3(c)); after that the accuracy of reward estimator does not change, but the policy learner keeps on improving policy until it converges.\n\n**Sample a patient**: for both Cancer and Sepsis experiments, we can generate a random subject (initial tumor size and toxicity level for Cancer and state transition parameters for Sepsis). We set the training and testing set fixed for fair comparison. A large number of training set is beneficial for learning parameters in deep models and a large number of testing set is appropriate to evaluate the performance with small variance. Validation at different epochs during model training is designed for selecting the epoch with best validation results for testing.\n\n**Different time scales in Fig.2**: Training with or without a pre-trained reward model will converge at 10th epoch, so we only show the first 12 epochs to illustrate the performance gap between training with transfer and training without transfer.\n\n**Evaluation metrics and rewards/expected return**: Actually, the evaluation metric is the preference metric that guides the preference generation between two policies. Since the reward is inferred rather than pre-defined, then the two should have a positive proportional relationship if the reward estimator is well-trained. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1259/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1259/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An Examination of Preference-based Reinforcement Learning for Treatment Recommendation", "authorids": ["~Nan_Xu2", "~Nitin_Kamra1", "~Yan_Liu1"], "authors": ["Nan Xu", "Nitin Kamra", "Yan Liu"], "keywords": ["Preference-based Reinforcement Learning", "Treatment Recommendation", "healthcare"], "abstract": "Treatment recommendation is a complex multi-faceted problem with many conflicting objectives, e.g., optimizing the survival rate (or expected lifetime), mitigating negative impacts, reducing financial expenses and time costs, avoiding over-treatment, etc. While this complicates the hand-engineering of a reward function for learning treatment policies, fortunately, qualitative feedback from human experts is readily available and can be easily exploited.  Since direct estimation of rewards via inverse reinforcement learning is a challenging task and requires the existence of an optimal human policy, the field of treatment recommendation has recently witnessed the development of the preference-based ReinforcementLearning (PRL) framework, which infers a reward function from only qualitative and imperfect human feedback to ensure that a human expert\u2019s preferred policy has a higher expected return over a less preferred policy. In this paper, we first present an open simulation platform to model the progression of two diseases, namely Cancer and Sepsis, and the reactions of the affected individuals to the received treatment. Secondly, we investigate important problems in adopting preference-basedRL approaches for treatment recommendation, such as advantages of learning from preference over hand-engineered reward, addressing incomparable policies, reward interpretability, and agent design via simulated experiments. The designed simulation platform and insights obtained for preference-based RL approaches are beneficial for achieving the right trade-off between various human objectives during treatment recommendation.", "one-sentence_summary": "Develop a simulation platform and investigate preference-based reinforcement learning approaches for treatment recommendation", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xu|an_examination_of_preferencebased_reinforcement_learning_for_treatment_recommendation", "supplementary_material": "/attachment/2584da0e9892dbc00de335fbf80dab9fecaf2e9d.zip", "pdf": "/pdf/ab9d9eaf60be6f66ecde42086838f1fc3f3a6542.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=pMKUh7w7PE", "_bibtex": "@misc{\nxu2021an,\ntitle={An Examination of Preference-based Reinforcement Learning for Treatment Recommendation},\nauthor={Nan Xu and Nitin Kamra and Yan Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=uxYjVEXx48i}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "uxYjVEXx48i", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1259/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1259/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1259/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1259/Authors|ICLR.cc/2021/Conference/Paper1259/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1259/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923861779, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1259/-/Official_Comment"}}}, {"id": "GwS5RMTKiGL", "original": null, "number": 7, "cdate": 1606158956623, "ddate": null, "tcdate": 1606158956623, "tmdate": 1606158956623, "tddate": null, "forum": "uxYjVEXx48i", "replyto": "RWI9jII764", "invitation": "ICLR.cc/2021/Conference/Paper1259/-/Official_Comment", "content": {"title": "Loss function for the agent (Alg 3., Line 10) & How are patients physiologies varied in the simulators", "comment": "1. Loss function for the agent (Alg 3., Line 10)\n\nSince the preference-based reward is a non-stationary value approximated by a neural network, we implement agents with the policy gradient algorithm, which is robust to changes in the reward function [1, 2]. In policy gradient, we maximize the expected total reward by repeatedly estimating the gradient, and optimize it with gradient descent. Specifically, we subtract a baseline (determined by the current state only) from the expected return to avoid high variance in policy update.\n\n2. How are patients physiologies varied in the simulators\n\n- General Cancer and drug treatment simulation\n\nWe use the mathematical model proposed by [3] to simulate the general cancer evolution and drug treatment effects. There are two observations as state for each subject, i.e., tumor size and toxicity level. For state initialization, the tumor size and the toxicity level in the 0th month are generated independently from the uniform distribution $U (0, 2)$ (Section A.3.1).  Randomness also comes from the survival analysis (Section A.3.1), where the subject is likely to die intermediately.  Figure 4(b) also reflects the distinctions among patients after receiving treatment from policy gradient by observing the distribution of sum of tumor size and toxicity level (ranging from 0 to more than 5) at the end of simulation from x-axis. Similar distributions can be observed in Figure 4(c) for receiving treatments recommended by SbRM, 7(b) for receiving treatments recommended by AbRM. To further visualize the subject differences, we add one more plot in Appendix Figure 11 (also see in the link: https://drive.google.com/file/d/1actrpcPgtNr5CVrhBndB5RjCddUCCrz-/view?usp=sharing ), in order to show the value distribution of tumor size and toxicity level along simulation, either without or with treatment recommended by all 12 compared approaches.\n\n- Sepsis infection and blood purification simulation\n\nWe employ the mathematical model derived by [4] to simulate the acute inflammation process in response to an infection. We have eight longitudinal measurements of key cytokines and damage-related markers in blood of  23 CLP-induced septic rats. There are 19 physiological features that govern sepsis dynamics, 8 of which are observable while the remaining 11 are unmeasurable conceptual variables. All the state features are initialized by 0 except $CLP$ (0.9, cecal ligation and puncture) and $N_r$ (2.5e6, resting blood neutrophils). The parameters in the state transition functions (21 ODEs) characterizes the subject, and we assume they follow normal distribution with respective mean and standard deviation known from existing literature. We sample subjects individually by generating ODE parameters using Markov-Chain Monte Carlo (MCMC) sampling of their posterior parameter distributions. The parameters for each subject are accepted if they are compatible with their respective distributions and the observations in the resulted simulation are close to experimental data (measurements of 23 rats). Therefore, the transition parameters that distinguish one subject from the other should be quite different from each other due to the random initialization (follow their gaussian distributions) and iterative update during the MCMC sampling process. To further visualize the subject differences, we add one more plot in Appendix Figure 13 (also see in the link: https://drive.google.com/file/d/1HNY_Soo0r4fhcbQAx_mc5tYMC_XslS4A/view?usp=sharing ), in order to illustrate the variation of 19 state features during the simulation without treatment or with treatment recommended by SbRM. \n\n\n[1] Ho, Jonathan, and Stefano Ermon. \"Generative adversarial imitation learning.\" Advances in neural information processing systems. 2016.\n\n[2] Christiano, Paul F., et al. \"Deep reinforcement learning from human preferences.\" Advances in Neural Information Processing Systems. 2017.\n\n[3] Zhao, Yufan, Michael R. Kosorok, and Donglin Zeng. \"Reinforcement learning design for cancer clinical trials.\" Statistics in medicine 28.26 (2009): 3294-3315.\n\n[4] Song, Sang OK, et al. \"Ensemble models of neutrophil trafficking in severe sepsis.\" PLoS Comput Biol 8.3 (2012): e1002422.\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1259/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1259/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An Examination of Preference-based Reinforcement Learning for Treatment Recommendation", "authorids": ["~Nan_Xu2", "~Nitin_Kamra1", "~Yan_Liu1"], "authors": ["Nan Xu", "Nitin Kamra", "Yan Liu"], "keywords": ["Preference-based Reinforcement Learning", "Treatment Recommendation", "healthcare"], "abstract": "Treatment recommendation is a complex multi-faceted problem with many conflicting objectives, e.g., optimizing the survival rate (or expected lifetime), mitigating negative impacts, reducing financial expenses and time costs, avoiding over-treatment, etc. While this complicates the hand-engineering of a reward function for learning treatment policies, fortunately, qualitative feedback from human experts is readily available and can be easily exploited.  Since direct estimation of rewards via inverse reinforcement learning is a challenging task and requires the existence of an optimal human policy, the field of treatment recommendation has recently witnessed the development of the preference-based ReinforcementLearning (PRL) framework, which infers a reward function from only qualitative and imperfect human feedback to ensure that a human expert\u2019s preferred policy has a higher expected return over a less preferred policy. In this paper, we first present an open simulation platform to model the progression of two diseases, namely Cancer and Sepsis, and the reactions of the affected individuals to the received treatment. Secondly, we investigate important problems in adopting preference-basedRL approaches for treatment recommendation, such as advantages of learning from preference over hand-engineered reward, addressing incomparable policies, reward interpretability, and agent design via simulated experiments. The designed simulation platform and insights obtained for preference-based RL approaches are beneficial for achieving the right trade-off between various human objectives during treatment recommendation.", "one-sentence_summary": "Develop a simulation platform and investigate preference-based reinforcement learning approaches for treatment recommendation", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xu|an_examination_of_preferencebased_reinforcement_learning_for_treatment_recommendation", "supplementary_material": "/attachment/2584da0e9892dbc00de335fbf80dab9fecaf2e9d.zip", "pdf": "/pdf/ab9d9eaf60be6f66ecde42086838f1fc3f3a6542.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=pMKUh7w7PE", "_bibtex": "@misc{\nxu2021an,\ntitle={An Examination of Preference-based Reinforcement Learning for Treatment Recommendation},\nauthor={Nan Xu and Nitin Kamra and Yan Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=uxYjVEXx48i}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "uxYjVEXx48i", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1259/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1259/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1259/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1259/Authors|ICLR.cc/2021/Conference/Paper1259/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1259/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923861779, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1259/-/Official_Comment"}}}, {"id": "atOGTsdDzOf", "original": null, "number": 6, "cdate": 1606158574132, "ddate": null, "tcdate": 1606158574132, "tmdate": 1606158574132, "tddate": null, "forum": "uxYjVEXx48i", "replyto": "RWI9jII764", "invitation": "ICLR.cc/2021/Conference/Paper1259/-/Official_Comment", "content": {"title": "Explicit preferences to infer reward functions", "comment": "1. Obtaining preferences:\n\nIn section 5.3, we briefly describe the preferences to consider for Cancer and Sepsis treatment. We extract the most commonly used evaluation metrics from treatment recommendation literature and use them as preferences. \n\nOverall, two policies are compared firstly based on clinical efficacy, i.e., capability to save subject\u2019s life; if both two lead to survivals, then policy influences on subject\u2019s health conditions are considered for a further comparison. If no policy is preferred to the other after comparison, then the two are incomparable. \n\n| Disease | Preference                                                 | First comparison          | Second Comparison                                                  |\n|---------|------------------------------------------------------------|---------------------------|--------------------------------------------------------------------|\n| Cancer  | CE                                                         | Maximizing survival rate  | -                                                                  |\n|         | CE&OF-I                                                    | Maximizing survival rate  | Minimizing sum of tumor size and toxicity level                    |\n|         | CE&OF-II                                                   | Maximizing survival rate  | Minimizing the highest toxicity level, minimizing final tumor size |\n| Sepsis  | Optimizing clinical efficacy only                          | Maximizing survival rate  | -                                                                  |\n|         | Optimizing clinical efficacy & mitigating negative impacts | Maximizing survival rate  | Reducing number of operations                                      |---------|------------------------------------------------------------|---------------------------|--------------------------------------------------------------------|\n|\n\nWe elaborate how different preference concepts are utilized to compare two policies as follows:\n\n* Cancer: \n\n  * CE: Maximizing survival rate (results in main/Table 1). The policy saving the subject\u2019s life is preferred to the policy leading to subject\u2019s death. Otherwise, the two policies are incomparable.\n\n  * CE&OF-I: Maximizing survival rate and mitigating negative impacts in terms of minimizing sum of tumor size and toxicity level at the end of simulation (results in main/Table 1). The policy saving the subject\u2019s life is preferred to the policy leading to subject\u2019s death. If both lead to subject\u2019s survival, then the policy with a smaller sum value of tumor size and toxicity level is preferred. Otherwise, the two policies are incomparable.\n\n  * CE&OF-II: Maximizing survival rate and mitigating negative impacts in terms of minimizing the highest toxicity level and final tumor size during simulation (results in appendix/Table 2). The policy saving the subject\u2019s life is preferred to the policy leading to subject\u2019s death. If both lead to subject\u2019s survival, then the policy with a smaller highest toxicity level and a smaller final tumor size simultaneously is preferred. Otherwise, the two policies are incomparable.\n\n* Sepsis\n\n  * Optimizing clinical efficacy only: Maximizing survival rate (results in main/Fig.1(a)). The policy saving the subject\u2019s life is preferred to the policy leading to subject\u2019s death. Otherwise, the two policies are incomparable.\n\n  * Optimizing clinical efficacy & mitigating negative impacts: Maximizing survival rate and reducing number of operations (results in main/Fig.1(b)). The policy saving the subject\u2019s life is preferred to the policy leading to subject\u2019s death.  If both lead to subject\u2019s survival, then the policy with fewer times of operations is preferred. Otherwise, the two policies are incomparable.\n\n2. Utilizing preferences to infer reward:\n\nGiven two policies for one sampled subject (starting with the same initial state), we can obtain the preference between the two through the above procedure. Then we can infer the reward function as Equation (1) in Section 4.2: we are minimizing the loss of a binary classification task, in which one policy is predicted with label preferred or non-preferred. We also denote the policy comparison outcome given some preference metric by EVALUATEPREFERENCE($\\tau_1, \\tau_2$)  in Algorithm 1.\n\n3. Differences with handcrafted rewards:\n\nIn this paper, preference-based reward is parameterized and updated based on the preference between two policies. For a given state and action, we can infer the preference-based reward based on the learned reward function (either AbRM or SbRM). So the only prior knowledge we need from human is the metric to compare two policies. In our case, the preference metric is pre-defined (e.g., the policy maximizing survival rate and reducing operation times is preferred) and the preference generation process is automatic without human\u2019s interaction or involvement. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1259/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1259/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An Examination of Preference-based Reinforcement Learning for Treatment Recommendation", "authorids": ["~Nan_Xu2", "~Nitin_Kamra1", "~Yan_Liu1"], "authors": ["Nan Xu", "Nitin Kamra", "Yan Liu"], "keywords": ["Preference-based Reinforcement Learning", "Treatment Recommendation", "healthcare"], "abstract": "Treatment recommendation is a complex multi-faceted problem with many conflicting objectives, e.g., optimizing the survival rate (or expected lifetime), mitigating negative impacts, reducing financial expenses and time costs, avoiding over-treatment, etc. While this complicates the hand-engineering of a reward function for learning treatment policies, fortunately, qualitative feedback from human experts is readily available and can be easily exploited.  Since direct estimation of rewards via inverse reinforcement learning is a challenging task and requires the existence of an optimal human policy, the field of treatment recommendation has recently witnessed the development of the preference-based ReinforcementLearning (PRL) framework, which infers a reward function from only qualitative and imperfect human feedback to ensure that a human expert\u2019s preferred policy has a higher expected return over a less preferred policy. In this paper, we first present an open simulation platform to model the progression of two diseases, namely Cancer and Sepsis, and the reactions of the affected individuals to the received treatment. Secondly, we investigate important problems in adopting preference-basedRL approaches for treatment recommendation, such as advantages of learning from preference over hand-engineered reward, addressing incomparable policies, reward interpretability, and agent design via simulated experiments. The designed simulation platform and insights obtained for preference-based RL approaches are beneficial for achieving the right trade-off between various human objectives during treatment recommendation.", "one-sentence_summary": "Develop a simulation platform and investigate preference-based reinforcement learning approaches for treatment recommendation", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xu|an_examination_of_preferencebased_reinforcement_learning_for_treatment_recommendation", "supplementary_material": "/attachment/2584da0e9892dbc00de335fbf80dab9fecaf2e9d.zip", "pdf": "/pdf/ab9d9eaf60be6f66ecde42086838f1fc3f3a6542.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=pMKUh7w7PE", "_bibtex": "@misc{\nxu2021an,\ntitle={An Examination of Preference-based Reinforcement Learning for Treatment Recommendation},\nauthor={Nan Xu and Nitin Kamra and Yan Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=uxYjVEXx48i}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "uxYjVEXx48i", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1259/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1259/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1259/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1259/Authors|ICLR.cc/2021/Conference/Paper1259/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1259/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923861779, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1259/-/Official_Comment"}}}, {"id": "f9NugFT3Oup", "original": null, "number": 5, "cdate": 1606157251145, "ddate": null, "tcdate": 1606157251145, "tmdate": 1606157251145, "tddate": null, "forum": "uxYjVEXx48i", "replyto": "oT0-1khP2xL", "invitation": "ICLR.cc/2021/Conference/Paper1259/-/Official_Comment", "content": {"title": "Answers to small issues", "comment": "- Linearly weighted reward function induces negative interference between objectives: when multiple objectives are included in one optimization function, then it could happen when one objective is optimized while the other isn\u2019t.\n\n- Some notations: $a_1, a_2, b_1, b_2, m_1, m_2$ are hyper-parameters designed in the mathematical model proposed by Zhao et al. (2009). We adopt their defaulted values in our experiments.\n\n- Hill equations from state transitions in Section 3.2: Several molecular interactions in cellular systems exhibit sigmoidal response curve to variations in the input concentrations. Such a response curve is typically represented by Hill equation: $Y=I^{nH}/(K_{0.5}^{nH}+I^{nH})$, where $Y$ is the output response and $I$ is the input concentration. Hill equation involves two parameters, Hill Coefficient ($nH$) and half-saturation constant ($K_{0.5}$). While Hill coefficient characterizes the sensitivity of the response, the half-saturation constant quantifies the threshold concentration required for 50% output response.\n\n- Equations in line 5 and 8 from Algorithm 2: expected return computation, i.e., sum of discounted reward\n\n- Loss in Algorithm 2: as discussed in Section 5.4, there are cases when two policies are incomparable. Since no clear preference conclusion can be drawn between the two incomparable policies, the majority of existing work in preference learning disregarded them directly. After the performance analysis shown in Fig.3, we find that excluding incomparable pairs from the training set leaves the parameterized model exploring the preference space arbitrarily and inferring random preference over two policies although they are incomparable. To avoid arbitrary exploration in the preference space, we handle the incomparable pairs with a simple approach: treating both policies from the incomparable pair equally, i.e., $I(\\pi_m(s_i) \u227b \\pi_n(s_i)) = I(\\pi_n(s_i) \u227b \\pi_m(s_i)) = 0.5$. With the small but important augmentation to the preference indicator function, incomparable policies are efficiently utilized for better preference space exploration (preference approaching 0.5 as expected in Fig. 3a), more samples for preference model update (all the 10,000 samples from the training set participate in the loss function minimization in Fig. 3b), and much higher clinical efficacy (more than 30% survival rate achieved after the model converges in Fig. 3c)."}, "signatures": ["ICLR.cc/2021/Conference/Paper1259/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1259/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An Examination of Preference-based Reinforcement Learning for Treatment Recommendation", "authorids": ["~Nan_Xu2", "~Nitin_Kamra1", "~Yan_Liu1"], "authors": ["Nan Xu", "Nitin Kamra", "Yan Liu"], "keywords": ["Preference-based Reinforcement Learning", "Treatment Recommendation", "healthcare"], "abstract": "Treatment recommendation is a complex multi-faceted problem with many conflicting objectives, e.g., optimizing the survival rate (or expected lifetime), mitigating negative impacts, reducing financial expenses and time costs, avoiding over-treatment, etc. While this complicates the hand-engineering of a reward function for learning treatment policies, fortunately, qualitative feedback from human experts is readily available and can be easily exploited.  Since direct estimation of rewards via inverse reinforcement learning is a challenging task and requires the existence of an optimal human policy, the field of treatment recommendation has recently witnessed the development of the preference-based ReinforcementLearning (PRL) framework, which infers a reward function from only qualitative and imperfect human feedback to ensure that a human expert\u2019s preferred policy has a higher expected return over a less preferred policy. In this paper, we first present an open simulation platform to model the progression of two diseases, namely Cancer and Sepsis, and the reactions of the affected individuals to the received treatment. Secondly, we investigate important problems in adopting preference-basedRL approaches for treatment recommendation, such as advantages of learning from preference over hand-engineered reward, addressing incomparable policies, reward interpretability, and agent design via simulated experiments. The designed simulation platform and insights obtained for preference-based RL approaches are beneficial for achieving the right trade-off between various human objectives during treatment recommendation.", "one-sentence_summary": "Develop a simulation platform and investigate preference-based reinforcement learning approaches for treatment recommendation", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xu|an_examination_of_preferencebased_reinforcement_learning_for_treatment_recommendation", "supplementary_material": "/attachment/2584da0e9892dbc00de335fbf80dab9fecaf2e9d.zip", "pdf": "/pdf/ab9d9eaf60be6f66ecde42086838f1fc3f3a6542.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=pMKUh7w7PE", "_bibtex": "@misc{\nxu2021an,\ntitle={An Examination of Preference-based Reinforcement Learning for Treatment Recommendation},\nauthor={Nan Xu and Nitin Kamra and Yan Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=uxYjVEXx48i}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "uxYjVEXx48i", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1259/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1259/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1259/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1259/Authors|ICLR.cc/2021/Conference/Paper1259/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1259/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923861779, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1259/-/Official_Comment"}}}, {"id": "Ni49i3yV8Sh", "original": null, "number": 4, "cdate": 1606157185871, "ddate": null, "tcdate": 1606157185871, "tmdate": 1606157185871, "tddate": null, "forum": "uxYjVEXx48i", "replyto": "oT0-1khP2xL", "invitation": "ICLR.cc/2021/Conference/Paper1259/-/Official_Comment", "content": {"title": "Answers to main questions", "comment": "1. EVALUATEPREFERENCE procedure\n\nOnce we have determined the metric/rule that determines one policy is preferred to the other policy, then we can conduct EVALUATEPREFERENCE procedure automatically. Take the CE&OF-I case mentioned in Section 5.1 for Cancer treatment recommendation as an example. The goal of CE&OF-I is to maximize survival rate and mitigate negative impacts in terms of minimizing sum of tumor size and toxicity level at the end of simulation (results in main/Table 1). The policy saving the subject\u2019s life is preferred to the policy leading to the subject's death. If both lead to the subject's survival, then the policy with a smaller sum value of tumor size and toxicity level is preferred. Otherwise, the two policies are incomparable.\n\n2. Preference at individual states or at whole policy\n\nWe consider preference over policy rather than a single state for two reasons. Firstly, it is very difficult to conclude that one action is preferred to the other given a state in treatment recommendation, either in pre-defined metric way or consulting an expert.  That\u2019s also the reason when hand-crafted rewards are utilized for policy learning, intermediate steps are normally assigned with 0 reward and the final step is assigned with positive reward for positive outcome or negative reward for negative outcome. Secondly, we cannot compare two random states or state-action pairs directly since they may come from different subjects with different health conditions. For two policies starting with the same initial state, we can have a reasonable comparison by checking the outcomes of different treatment strategies after observing the whole policies on the same subject.\n\n3. How to work with real data and real human input\n\nState transition parameters in Sepsis experiments characterize each subject and are sampled using Markov-Chain Monte Carlo (MCMC) sampling of their posterior parameter distributions. The parameters for each subject are accepted if they are compatible with their respective distributions and the observations in the resulted simulation are close to collected experimental data from 23 rats. Suppose the simulator can emulate quite well the acute inflammation process in response to an infection, then human input is the metric to evaluate preference over two policies, such as the survival rate, the number of operations, etc.\n\n4. Comparison with hand-crafted reward\n\nTo have a fair comparison between preference-based reward and hand-crafter reward, we show performance of Grid-search Multi-objective RL for both Cancer and Sepsis (Table 1, Figure 6 and Figure 1), and grid-reward design for positive and negative outcomes for Cancer (Figure 5). \n\nIn Fig. 5 (Appendix), we first show the sensitivity of policy behaviors to small changes in handcrafted rewards leads to unstable clinical efficacy even when the relative importance of obtaining positive outcomes against negative ones keeps unchanged. The three heatmaps of the agent\u2019s performance in response to different reward scalars (121 combinations) reflect the difficulty in specifying an appropriate reward function to enable policy learning with the optimal clinical efficacy during treatment recommendation.\n\nIn Fig. 6 (Appendix), we further show the difficulty of selecting reward scalars for three factors \u2013 survival rate, last tumor size and maximum toxicity level \u2013 in the grid-search Multi-objective RL approach to appropriately prioritize the clinical efficacy over negative impacts. We can observe that policy learning from preference-based reward can have the highest survival rate compared with policies learning from all kinds of linear weighted sum of the three factors."}, "signatures": ["ICLR.cc/2021/Conference/Paper1259/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1259/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An Examination of Preference-based Reinforcement Learning for Treatment Recommendation", "authorids": ["~Nan_Xu2", "~Nitin_Kamra1", "~Yan_Liu1"], "authors": ["Nan Xu", "Nitin Kamra", "Yan Liu"], "keywords": ["Preference-based Reinforcement Learning", "Treatment Recommendation", "healthcare"], "abstract": "Treatment recommendation is a complex multi-faceted problem with many conflicting objectives, e.g., optimizing the survival rate (or expected lifetime), mitigating negative impacts, reducing financial expenses and time costs, avoiding over-treatment, etc. While this complicates the hand-engineering of a reward function for learning treatment policies, fortunately, qualitative feedback from human experts is readily available and can be easily exploited.  Since direct estimation of rewards via inverse reinforcement learning is a challenging task and requires the existence of an optimal human policy, the field of treatment recommendation has recently witnessed the development of the preference-based ReinforcementLearning (PRL) framework, which infers a reward function from only qualitative and imperfect human feedback to ensure that a human expert\u2019s preferred policy has a higher expected return over a less preferred policy. In this paper, we first present an open simulation platform to model the progression of two diseases, namely Cancer and Sepsis, and the reactions of the affected individuals to the received treatment. Secondly, we investigate important problems in adopting preference-basedRL approaches for treatment recommendation, such as advantages of learning from preference over hand-engineered reward, addressing incomparable policies, reward interpretability, and agent design via simulated experiments. The designed simulation platform and insights obtained for preference-based RL approaches are beneficial for achieving the right trade-off between various human objectives during treatment recommendation.", "one-sentence_summary": "Develop a simulation platform and investigate preference-based reinforcement learning approaches for treatment recommendation", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xu|an_examination_of_preferencebased_reinforcement_learning_for_treatment_recommendation", "supplementary_material": "/attachment/2584da0e9892dbc00de335fbf80dab9fecaf2e9d.zip", "pdf": "/pdf/ab9d9eaf60be6f66ecde42086838f1fc3f3a6542.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=pMKUh7w7PE", "_bibtex": "@misc{\nxu2021an,\ntitle={An Examination of Preference-based Reinforcement Learning for Treatment Recommendation},\nauthor={Nan Xu and Nitin Kamra and Yan Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=uxYjVEXx48i}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "uxYjVEXx48i", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1259/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1259/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1259/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1259/Authors|ICLR.cc/2021/Conference/Paper1259/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1259/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923861779, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1259/-/Official_Comment"}}}, {"id": "khmanJufZX_", "original": null, "number": 3, "cdate": 1606156652511, "ddate": null, "tcdate": 1606156652511, "tmdate": 1606156652511, "tddate": null, "forum": "uxYjVEXx48i", "replyto": "Xqw-jbJl9cX", "invitation": "ICLR.cc/2021/Conference/Paper1259/-/Official_Comment", "content": {"title": "Implementation details", "comment": "Overall, two policies are compared firstly based on clinical efficacy, i.e., capability to save subject\u2019s life; if both two lead to survivals, then policy influences on subject\u2019s health conditions are considered for a further comparison. If no policy is preferred to the other after comparison, then the two are incomparable. \n\nIn section 5.3, we briefly describe the preferences to consider for Cancer and Sepsis treatment. We elaborate here how different evaluation metrics of preferences are utilized to automatically compare two policies (by EVALUATEPREFERENCE function in Algorithm 1) as follows:\n\n1) Cancer: \n\n- CE: Maximizing survival rate (results in main/Table 1). The policy saving the subject\u2019s life is preferred to the policy leading to subject\u2019s death. Otherwise, the two policies are incomparable.\n\n- CE&OF-I: Maximizing survival rate and mitigating negative impacts in terms of minimizing sum of tumor size and toxicity level at the end of simulation (results in main/Table 1). The policy saving the subject\u2019s life is preferred to the policy leading to subject\u2019s death. If both lead to subject\u2019s survival, then the policy with a smaller sum value of tumor size and toxicity level is preferred. Otherwise, the two policies are incomparable.\n\n- CE&OF-II: Maximizing survival rate and mitigating negative impacts in terms of minimizing the highest toxicity level and final tumor size during simulation (results in appendix/Table 2). The policy saving the subject\u2019s life is preferred to the policy leading to subject\u2019s death. If both lead to subject\u2019s survival, then the policy with a smaller highest toxicity level and a smaller final tumor size simultaneously is preferred. Otherwise, the two policies are incomparable.\n\n2) Sepsis\n\n- Optimizing clinical efficacy only: Maximizing survival rate (results in main/Fig.1(a)). The policy saving the subject\u2019s life is preferred to the policy leading to subject\u2019s death. Otherwise, the two policies are incomparable.\n\n- Optimizing clinical efficacy & mitigating negative impacts: Maximizing survival rate and reducing number of operations (results in main/Fig.1(b)). The policy saving the subject\u2019s life is preferred to the policy leading to subject\u2019s death.  If both lead to subject\u2019s survival, then the policy with fewer times of operations is preferred. Otherwise, the two policies are incomparable.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1259/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1259/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An Examination of Preference-based Reinforcement Learning for Treatment Recommendation", "authorids": ["~Nan_Xu2", "~Nitin_Kamra1", "~Yan_Liu1"], "authors": ["Nan Xu", "Nitin Kamra", "Yan Liu"], "keywords": ["Preference-based Reinforcement Learning", "Treatment Recommendation", "healthcare"], "abstract": "Treatment recommendation is a complex multi-faceted problem with many conflicting objectives, e.g., optimizing the survival rate (or expected lifetime), mitigating negative impacts, reducing financial expenses and time costs, avoiding over-treatment, etc. While this complicates the hand-engineering of a reward function for learning treatment policies, fortunately, qualitative feedback from human experts is readily available and can be easily exploited.  Since direct estimation of rewards via inverse reinforcement learning is a challenging task and requires the existence of an optimal human policy, the field of treatment recommendation has recently witnessed the development of the preference-based ReinforcementLearning (PRL) framework, which infers a reward function from only qualitative and imperfect human feedback to ensure that a human expert\u2019s preferred policy has a higher expected return over a less preferred policy. In this paper, we first present an open simulation platform to model the progression of two diseases, namely Cancer and Sepsis, and the reactions of the affected individuals to the received treatment. Secondly, we investigate important problems in adopting preference-basedRL approaches for treatment recommendation, such as advantages of learning from preference over hand-engineered reward, addressing incomparable policies, reward interpretability, and agent design via simulated experiments. The designed simulation platform and insights obtained for preference-based RL approaches are beneficial for achieving the right trade-off between various human objectives during treatment recommendation.", "one-sentence_summary": "Develop a simulation platform and investigate preference-based reinforcement learning approaches for treatment recommendation", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xu|an_examination_of_preferencebased_reinforcement_learning_for_treatment_recommendation", "supplementary_material": "/attachment/2584da0e9892dbc00de335fbf80dab9fecaf2e9d.zip", "pdf": "/pdf/ab9d9eaf60be6f66ecde42086838f1fc3f3a6542.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=pMKUh7w7PE", "_bibtex": "@misc{\nxu2021an,\ntitle={An Examination of Preference-based Reinforcement Learning for Treatment Recommendation},\nauthor={Nan Xu and Nitin Kamra and Yan Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=uxYjVEXx48i}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "uxYjVEXx48i", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1259/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1259/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1259/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1259/Authors|ICLR.cc/2021/Conference/Paper1259/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1259/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923861779, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1259/-/Official_Comment"}}}, {"id": "SHytQImEmf5", "original": null, "number": 2, "cdate": 1606156221430, "ddate": null, "tcdate": 1606156221430, "tmdate": 1606156221430, "tddate": null, "forum": "uxYjVEXx48i", "replyto": "Xqw-jbJl9cX", "invitation": "ICLR.cc/2021/Conference/Paper1259/-/Official_Comment", "content": {"title": "Related preference learning work comparison", "comment": "We have cited Christiano\u2019s work but missed Ibarz\u2019s work in our paper. We didn\u2019t compare explicitly with the existing approach for learning from preferences since all of the preference-based reward estimators (including ours) follow the Bradley-Terry model (Bradley and Terry, 1952) for estimating score functions from pairwise preferences, and is the specialization of the Luce-Shephard choice rule (Luce, 2005; Shepard, 1957) to preferences over trajectory segments. Some latest work concerning learning reward functions based on ranking [1, 2] also adopted the same loss function for reward learning. The main differences of our work from Christiano\u2019s and Ibarz\u2019s work are two-folds: 1) we do not need explicit human involvement to annotate preference over policies during model training since the preferences are generated automatically based on some pre-defined metric, e.g., maximizing the survival rate, reducing the operation times for Sepsis experiments; 2) the majority of existing work disregarded incomparable pairs while we treat incomparable policies equally. As discussed in Section 5.4 and illustrations from Fig. 3, we find that incomparable policies are efficiently utilized for better preference space exploration (preference approaching 0.5 as expected in Fig. 3a), more samples for preference model update (all the 10,000 samples from the training set participate in the loss function minimization in Fig. 3b), and much higher clinical efficacy (more than 30% survival rate achieved after the model converges in Fig. 3c).\n\n[1] Brown, Daniel S., et al. \"Extrapolating beyond suboptimal demonstrations via inverse reinforcement learning from observations.\" arXiv preprint arXiv:1904.06387 (2019).\n\n[2] Brown, Daniel S., Wonjoon Goo, and Scott Niekum. \"Better-than-demonstrator imitation learning via automatically-ranked demonstrations.\" Conference on Robot Learning. 2020.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1259/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1259/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An Examination of Preference-based Reinforcement Learning for Treatment Recommendation", "authorids": ["~Nan_Xu2", "~Nitin_Kamra1", "~Yan_Liu1"], "authors": ["Nan Xu", "Nitin Kamra", "Yan Liu"], "keywords": ["Preference-based Reinforcement Learning", "Treatment Recommendation", "healthcare"], "abstract": "Treatment recommendation is a complex multi-faceted problem with many conflicting objectives, e.g., optimizing the survival rate (or expected lifetime), mitigating negative impacts, reducing financial expenses and time costs, avoiding over-treatment, etc. While this complicates the hand-engineering of a reward function for learning treatment policies, fortunately, qualitative feedback from human experts is readily available and can be easily exploited.  Since direct estimation of rewards via inverse reinforcement learning is a challenging task and requires the existence of an optimal human policy, the field of treatment recommendation has recently witnessed the development of the preference-based ReinforcementLearning (PRL) framework, which infers a reward function from only qualitative and imperfect human feedback to ensure that a human expert\u2019s preferred policy has a higher expected return over a less preferred policy. In this paper, we first present an open simulation platform to model the progression of two diseases, namely Cancer and Sepsis, and the reactions of the affected individuals to the received treatment. Secondly, we investigate important problems in adopting preference-basedRL approaches for treatment recommendation, such as advantages of learning from preference over hand-engineered reward, addressing incomparable policies, reward interpretability, and agent design via simulated experiments. The designed simulation platform and insights obtained for preference-based RL approaches are beneficial for achieving the right trade-off between various human objectives during treatment recommendation.", "one-sentence_summary": "Develop a simulation platform and investigate preference-based reinforcement learning approaches for treatment recommendation", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xu|an_examination_of_preferencebased_reinforcement_learning_for_treatment_recommendation", "supplementary_material": "/attachment/2584da0e9892dbc00de335fbf80dab9fecaf2e9d.zip", "pdf": "/pdf/ab9d9eaf60be6f66ecde42086838f1fc3f3a6542.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=pMKUh7w7PE", "_bibtex": "@misc{\nxu2021an,\ntitle={An Examination of Preference-based Reinforcement Learning for Treatment Recommendation},\nauthor={Nan Xu and Nitin Kamra and Yan Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=uxYjVEXx48i}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "uxYjVEXx48i", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1259/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1259/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1259/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1259/Authors|ICLR.cc/2021/Conference/Paper1259/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1259/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923861779, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1259/-/Official_Comment"}}}, {"id": "Xqw-jbJl9cX", "original": null, "number": 1, "cdate": 1603638841000, "ddate": null, "tcdate": 1603638841000, "tmdate": 1605024489198, "tddate": null, "forum": "uxYjVEXx48i", "replyto": "uxYjVEXx48i", "invitation": "ICLR.cc/2021/Conference/Paper1259/-/Official_Review", "content": {"title": "Lacks comparison with sota", "review": "This paper describes a preference based reinforcement learning framework for treatment recommendation tasks. The authors use a model-based environment to simulate the progression of disease and propose a preference based approach. They claim they can use this framework with any off-the-shelf RL algorithm to generate better survival results. The experiments are carried out with policy gradient. The authors also compared several different ways to design reward functions, and propose to use incomparable samples with a 0.5/0.5 distribution in the Bradley-Terry model, instead of discarding such samples, to improve the performance. The paper is generally well-written and easy-to-follow. Despite the technical contributions listed above, I would suggest rejection due to the following concerns.\n\n1. Innovation\nThe paper lacks citations and comparisons with several recent works in this field, such as [r1, r2]. The approach proposed is similar and the innovation seems marginal. I suggest the authors include a comparison of methodology and, if possible, experiments comparing their approach with state of the art preference based RL algorithms in the revision.\n\n2. Lack of implementation details\nThere are a few details I didn't find reading the manuscript, for example, how to query human responses on line 12 of Algorithm 1, what is the guideline for human experts to rate different treatment plans, implementation details and hyper-parameters used in the agent learning algorithm. The lack of such details makes it difficult for other researchers to reproduce the results shown in the paper.\n\n\n[r1] Christiano, Paul F., et al. \"Deep reinforcement learning from human preferences.\" Advances in Neural Information Processing Systems. 2017.\n[r2] Ibarz, Borja, et al. \"Reward learning from human preferences and demonstrations in Atari.\" Advances in Neural Information Processing Systems. 2018.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1259/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1259/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An Examination of Preference-based Reinforcement Learning for Treatment Recommendation", "authorids": ["~Nan_Xu2", "~Nitin_Kamra1", "~Yan_Liu1"], "authors": ["Nan Xu", "Nitin Kamra", "Yan Liu"], "keywords": ["Preference-based Reinforcement Learning", "Treatment Recommendation", "healthcare"], "abstract": "Treatment recommendation is a complex multi-faceted problem with many conflicting objectives, e.g., optimizing the survival rate (or expected lifetime), mitigating negative impacts, reducing financial expenses and time costs, avoiding over-treatment, etc. While this complicates the hand-engineering of a reward function for learning treatment policies, fortunately, qualitative feedback from human experts is readily available and can be easily exploited.  Since direct estimation of rewards via inverse reinforcement learning is a challenging task and requires the existence of an optimal human policy, the field of treatment recommendation has recently witnessed the development of the preference-based ReinforcementLearning (PRL) framework, which infers a reward function from only qualitative and imperfect human feedback to ensure that a human expert\u2019s preferred policy has a higher expected return over a less preferred policy. In this paper, we first present an open simulation platform to model the progression of two diseases, namely Cancer and Sepsis, and the reactions of the affected individuals to the received treatment. Secondly, we investigate important problems in adopting preference-basedRL approaches for treatment recommendation, such as advantages of learning from preference over hand-engineered reward, addressing incomparable policies, reward interpretability, and agent design via simulated experiments. The designed simulation platform and insights obtained for preference-based RL approaches are beneficial for achieving the right trade-off between various human objectives during treatment recommendation.", "one-sentence_summary": "Develop a simulation platform and investigate preference-based reinforcement learning approaches for treatment recommendation", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xu|an_examination_of_preferencebased_reinforcement_learning_for_treatment_recommendation", "supplementary_material": "/attachment/2584da0e9892dbc00de335fbf80dab9fecaf2e9d.zip", "pdf": "/pdf/ab9d9eaf60be6f66ecde42086838f1fc3f3a6542.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=pMKUh7w7PE", "_bibtex": "@misc{\nxu2021an,\ntitle={An Examination of Preference-based Reinforcement Learning for Treatment Recommendation},\nauthor={Nan Xu and Nitin Kamra and Yan Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=uxYjVEXx48i}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "uxYjVEXx48i", "replyto": "uxYjVEXx48i", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1259/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538122814, "tmdate": 1606915781279, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1259/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1259/-/Official_Review"}}}, {"id": "oT0-1khP2xL", "original": null, "number": 2, "cdate": 1603664300763, "ddate": null, "tcdate": 1603664300763, "tmdate": 1605024489136, "tddate": null, "forum": "uxYjVEXx48i", "replyto": "uxYjVEXx48i", "invitation": "ICLR.cc/2021/Conference/Paper1259/-/Official_Review", "content": {"title": "When reinforcement learning is considered for treatment recommendation, there is a trade-off between survival rate and other factors and side effects. In this paper, the authors use preferences to learn the reward that will guide the RL agents towards behaviour that matches the objectives that are used to assess agents' performance.", "review": "\nThis is a relatively comprehensive evaluation. The authors know the related literature, and a number of experiments are presented to show different aspects of the methods. The work is promising. Writing is great, and the authors can write beautiful sentences, but the overall structure has some flaws.\n\nI am not happy with the way the authors split their manuscript into the main paper and the appendix. The current version of the main paper is not self-contained, and it does not provide minimum explanation of the methods. I was very perplexed after reading the main part before I read the appendix because the core algorithm is not explained in the main part of the paper. It was hard to follow the experiments in sec. 5, when one is unsure how the methods work. The experiments section itself makes frequent jumps to the appendix. This content would be much more appropriate for a journal paper, in which most of the appendix would be nicely integrated with the main body.\n\nIt is not entirely clear what is the new contribution in this paper. When algorithm 1 is introduced in section 4.2, the authors do not say which components are new or how they extend the existing literature.\n\nThe EVALUATEPREFERENCE procedure in algorithm 1 is not defined, yet it is a very important component. The pseudocode of algorithm 1 indicates that entire trajectories are compared to elicit preferences, but this may lead to suboptimal behaviour. Note that if two policies are suboptimal, they may make suboptimal decisions in different states, so preferences at the level of individual states would be more appropriate. This important component of the algorithm is unclear to me in the paper. \n\nFollowing on the previous question, I should ask what the human expert would need to do if this algorithm was applied in practice. Specifically, what would be displayed to the human expert, and what the human expert would need to do or what they would need to compare and judge? The simulator (which would be the real environment) and the algorithms are tightly entangled in the appendix, and for this reason it is hard to imagine how this solution would work with real data and real human input.\n\nSince the authors' goal is to show that their method provides better rewards than the handcrafted rewards known in the existing literature, perhaps some discussion of those rewards could be added. How were those rewards derived? Did the human designers have the same objectives in mind? Could one derive a better handcrafted reward for these experiments? Sufficient evidence should be provided that the handcrafted rewards used in comparisons are not trivial. I am saying this because the preference-based reward is derived from preferences that match the quantities that constitute the objective of learning, so it is not that surprising that preference-based reward leads to better performance according to the corresponding objectives.\n\nSmall issues:\n\nIn sec. 1, the authors mention open access large-scale Electronic Health Records. Examples of such publicly available records should be provided.\n\nThe authors should clarify what they mean in this sentence \"the linearly weighted reward function induces negative interface between objectives\".\n\nSeveral symbols are not defined in section 2.1, e.g., a1, a2, m1, m2.\n\nFormatting on p. 3 is incorrect.\n\nWhat are \"hill\" equations on p. 3?\n\nIt would be good if the authors could explain equations that are in lines 5 and 8 in Algorithm 2. What would be an intuitive explanation of L in this algorithm? \n\n\n\nThis paper addresses a very important problem, and the results could be significant. I think that the future readers would be confused if this paper was accepted in this form. It would be sensible to turn it into a longer journal paper and clarify the key components.\n\n", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1259/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1259/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An Examination of Preference-based Reinforcement Learning for Treatment Recommendation", "authorids": ["~Nan_Xu2", "~Nitin_Kamra1", "~Yan_Liu1"], "authors": ["Nan Xu", "Nitin Kamra", "Yan Liu"], "keywords": ["Preference-based Reinforcement Learning", "Treatment Recommendation", "healthcare"], "abstract": "Treatment recommendation is a complex multi-faceted problem with many conflicting objectives, e.g., optimizing the survival rate (or expected lifetime), mitigating negative impacts, reducing financial expenses and time costs, avoiding over-treatment, etc. While this complicates the hand-engineering of a reward function for learning treatment policies, fortunately, qualitative feedback from human experts is readily available and can be easily exploited.  Since direct estimation of rewards via inverse reinforcement learning is a challenging task and requires the existence of an optimal human policy, the field of treatment recommendation has recently witnessed the development of the preference-based ReinforcementLearning (PRL) framework, which infers a reward function from only qualitative and imperfect human feedback to ensure that a human expert\u2019s preferred policy has a higher expected return over a less preferred policy. In this paper, we first present an open simulation platform to model the progression of two diseases, namely Cancer and Sepsis, and the reactions of the affected individuals to the received treatment. Secondly, we investigate important problems in adopting preference-basedRL approaches for treatment recommendation, such as advantages of learning from preference over hand-engineered reward, addressing incomparable policies, reward interpretability, and agent design via simulated experiments. The designed simulation platform and insights obtained for preference-based RL approaches are beneficial for achieving the right trade-off between various human objectives during treatment recommendation.", "one-sentence_summary": "Develop a simulation platform and investigate preference-based reinforcement learning approaches for treatment recommendation", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xu|an_examination_of_preferencebased_reinforcement_learning_for_treatment_recommendation", "supplementary_material": "/attachment/2584da0e9892dbc00de335fbf80dab9fecaf2e9d.zip", "pdf": "/pdf/ab9d9eaf60be6f66ecde42086838f1fc3f3a6542.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=pMKUh7w7PE", "_bibtex": "@misc{\nxu2021an,\ntitle={An Examination of Preference-based Reinforcement Learning for Treatment Recommendation},\nauthor={Nan Xu and Nitin Kamra and Yan Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=uxYjVEXx48i}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "uxYjVEXx48i", "replyto": "uxYjVEXx48i", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1259/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538122814, "tmdate": 1606915781279, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1259/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1259/-/Official_Review"}}}], "count": 14}