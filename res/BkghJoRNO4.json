{"notes": [{"id": "BkghJoRNO4", "original": "BkgVvZNQON", "number": 40, "cdate": 1553423075620, "ddate": null, "tcdate": 1553423075620, "tmdate": 1562082116776, "tddate": null, "forum": "BkghJoRNO4", "replyto": null, "invitation": "ICLR.cc/2019/Workshop/LLD/-/Blind_Submission", "content": {"title": "Cross-Linked Variational Autoencoders for Generalized Zero-Shot Learning", "authors": ["Edgar Sch\u00f6nfeld", "Sayna Ebrahimi", "Samarth Sinha", "Trevor Darrell", "Zeynep Akata"], "authorids": ["edgar.schoenfeld@bosch.com", "sayna@eecs.berkeley.edu", "samarth.sinha@mail.utoronto.ca", "trevor@eecs.berkeley.edu", "z.akata@uva.nl"], "keywords": ["generalized zero-shot learning", "zero-shot learning", "few-shot learning", "image classification"], "TL;DR": "We use VAEs to learn a shared latent space embedding between image features and attributes and thereby achieve state-of-the-art results in generalized zero-shot learning.", "abstract": "Most approaches in generalized zero-shot learning rely on cross-modal mapping between an image feature space and a class embedding space or on generating artificial image features. However, learning a shared cross-modal embedding by aligning the latent spaces of modality-specific autoencoders is shown to be promising in (generalized) zero-shot learning. While following the same direction, we also take artificial feature generation one step further and propose a model where a shared latent space of image features and class embeddings is learned by aligned variational autoencoders, for the purpose of generating latent features to train a softmax classifier. We evaluate our learned latent features on conventional benchmark datasets and establish a new state of the art on generalized zero-shot as well as on few-shot learning. Moreover, our results on ImageNet with various zero-shot splits show that our latent features generalize well in large-scale settings.", "pdf": "/pdf/2dca6b5142e07b5b8b22a705ca48f03b8b8b4f58.pdf", "paperhash": "sch\u00f6nfeld|crosslinked_variational_autoencoders_for_generalized_zeroshot_learning"}, "signatures": ["ICLR.cc/2019/Workshop/LLD"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/LLD"], "details": {"replyCount": 3, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/LLD/-/Blind_Submission", "cdate": 1548689671889, "reply": {"forum": null, "replyto": null, "readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2019/Workshop/LLD"]}, "signatures": {"values": ["ICLR.cc/2019/Workshop/LLD"]}, "content": {"authors": {"values-regex": ".*"}, "authorids": {"values-regex": ".*"}}}, "tcdate": 1548689671889, "tmdate": 1557933709646, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/LLD"], "invitees": ["~"], "signatures": ["ICLR.cc/2019/Workshop/LLD"], "details": {"writable": true}}}, "tauthor": "OpenReview.net"}, {"id": "rkeupP1wFN", "original": null, "number": 1, "cdate": 1554606016312, "ddate": null, "tcdate": 1554606016312, "tmdate": 1555512021543, "tddate": null, "forum": "BkghJoRNO4", "replyto": "BkghJoRNO4", "invitation": "ICLR.cc/2019/Workshop/LLD/-/Paper40/Official_Review", "content": {"title": "Interesting work on multi-modal generalized zero shot learning", "review": "Summary: The authors propose a VAE architecture that leverages multi-modal information, namely images and text in order to learn a matched latent space representation. The representation is created by minimizing two additional objectives in addition to the beta-VAE loss: the cross-reconstruction (CA) and the distribution-alignment (DA) regularizers. The learnt joint-posterior is then utilized to train a classifier in a GZSL setting. The method posed shows promising results as it beats SOTA in several benchmarks but should address the following points in the camera ready version.\n\nMajor:\n  - There is no motivation or reasoning specified for the L1 cross-reconstruction (CA) loss in Equation 2. If the decoder is a gaussian then the natural assumption is a likelihood proportional to the L2 loss. If there is indeed a Laplace-likelihood this should be clearly stated. If not, then a small ablation study / discussion demonstrating the difference between the L1 & L2  (and corresponding distributional assumptions) for the CA should be provided.\n\n  - For the classifier part of the model is the dimensionality of the softmax fixed? Or does it simply refer to which sample it is associated with as in few-shot learning? In addition, how is the classifier trained? I.e. does it use both \\mu's and \\Sigma's ? Are they concatenated? Passed through separate networks?\n\n  - The final loss does not show the dependence on the hyper-parameters that weight the different terms of the loss; specifically the L_{CA} term seems to be hyper-parameter free? Training of multi-objective VAEs critically relies on the scale of these hyper-parameters. Beta in the beta-VAE is also not specified. These are critical and should be described.\n\nMinor:\n  - are the image features extracted from a pre-trained Resnet-101 model or is the encoder a Resnet-101 model? This should be made clear.\n  - How many posterior samples are extracted for classification? How are they used? Is a classification made for each sample or is the latent representation averaged / concatenated and then classified? Why isn\u2019t just the mean used as is standard in a test-setting for VAEs?\n - title is missing the word \u201cShot\u201d", "rating": "4: Top 50% of accepted papers, clear accept", "confidence": "3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Workshop/LLD/Paper40/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/LLD/Paper40/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Cross-Linked Variational Autoencoders for Generalized Zero-Shot Learning", "authors": ["Edgar Sch\u00f6nfeld", "Sayna Ebrahimi", "Samarth Sinha", "Trevor Darrell", "Zeynep Akata"], "authorids": ["edgar.schoenfeld@bosch.com", "sayna@eecs.berkeley.edu", "samarth.sinha@mail.utoronto.ca", "trevor@eecs.berkeley.edu", "z.akata@uva.nl"], "keywords": ["generalized zero-shot learning", "zero-shot learning", "few-shot learning", "image classification"], "TL;DR": "We use VAEs to learn a shared latent space embedding between image features and attributes and thereby achieve state-of-the-art results in generalized zero-shot learning.", "abstract": "Most approaches in generalized zero-shot learning rely on cross-modal mapping between an image feature space and a class embedding space or on generating artificial image features. However, learning a shared cross-modal embedding by aligning the latent spaces of modality-specific autoencoders is shown to be promising in (generalized) zero-shot learning. While following the same direction, we also take artificial feature generation one step further and propose a model where a shared latent space of image features and class embeddings is learned by aligned variational autoencoders, for the purpose of generating latent features to train a softmax classifier. We evaluate our learned latent features on conventional benchmark datasets and establish a new state of the art on generalized zero-shot as well as on few-shot learning. Moreover, our results on ImageNet with various zero-shot splits show that our latent features generalize well in large-scale settings.", "pdf": "/pdf/2dca6b5142e07b5b8b22a705ca48f03b8b8b4f58.pdf", "paperhash": "sch\u00f6nfeld|crosslinked_variational_autoencoders_for_generalized_zeroshot_learning"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/LLD/-/Paper40/Official_Review", "cdate": 1553713415200, "expdate": 1555718400000, "duedate": 1554681600000, "reply": {"forum": "BkghJoRNO4", "replyto": "BkghJoRNO4", "writers": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2019/Workshop/LLD/Paper40/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2019/Workshop/LLD/Paper40/AnonReviewer[0-9]+"}, "readers": {"values": ["everyone"], "description": "The users who will be allowed to read the above content."}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["5: Top 15% of accepted papers, strong accept", "4: Top 50% of accepted papers, clear accept", "3: Marginally above acceptance threshold", "2: Marginally below acceptance threshold", "1: Strong rejection"], "required": true}, "confidence": {"order": 4, "value-radio": ["3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "2: The reviewer is fairly confident that the evaluation is correct", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "tcdate": 1553713415200, "tmdate": 1555511825439, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/LLD"], "invitees": ["ICLR.cc/2019/Workshop/LLD/Paper40/Reviewers"], "signatures": ["ICLR.cc/2019/Workshop/LLD"]}}}, {"id": "SkxNwZAPFN", "original": null, "number": 2, "cdate": 1554665819746, "ddate": null, "tcdate": 1554665819746, "tmdate": 1555512017778, "tddate": null, "forum": "BkghJoRNO4", "replyto": "BkghJoRNO4", "invitation": "ICLR.cc/2019/Workshop/LLD/-/Paper40/Official_Review", "content": {"title": "Unclear writing with seemingly SOTA results on generalized zero-shot learning tasks", "review": "Pros:\n- SOTA results\n\nCons:\n- generally written in an unclear way \n- Title should say \"zero-shot\"\n- The indices of the covariance matrices in Fig 1 appear flipped\n- Figure captions lack important information (for example, in Fig 1 there is no mention that the bottom VAE is for the class embeddings, and they also use c in the figure but c(y) in the text)\n- not clear whether the results in Fig 2 for ImageNet are over multiple seeds, no error bars.\n", "rating": "3: Marginally above acceptance threshold", "confidence": "1: The reviewer's evaluation is an educated guess"}, "signatures": ["ICLR.cc/2019/Workshop/LLD/Paper40/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/LLD/Paper40/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Cross-Linked Variational Autoencoders for Generalized Zero-Shot Learning", "authors": ["Edgar Sch\u00f6nfeld", "Sayna Ebrahimi", "Samarth Sinha", "Trevor Darrell", "Zeynep Akata"], "authorids": ["edgar.schoenfeld@bosch.com", "sayna@eecs.berkeley.edu", "samarth.sinha@mail.utoronto.ca", "trevor@eecs.berkeley.edu", "z.akata@uva.nl"], "keywords": ["generalized zero-shot learning", "zero-shot learning", "few-shot learning", "image classification"], "TL;DR": "We use VAEs to learn a shared latent space embedding between image features and attributes and thereby achieve state-of-the-art results in generalized zero-shot learning.", "abstract": "Most approaches in generalized zero-shot learning rely on cross-modal mapping between an image feature space and a class embedding space or on generating artificial image features. However, learning a shared cross-modal embedding by aligning the latent spaces of modality-specific autoencoders is shown to be promising in (generalized) zero-shot learning. While following the same direction, we also take artificial feature generation one step further and propose a model where a shared latent space of image features and class embeddings is learned by aligned variational autoencoders, for the purpose of generating latent features to train a softmax classifier. We evaluate our learned latent features on conventional benchmark datasets and establish a new state of the art on generalized zero-shot as well as on few-shot learning. Moreover, our results on ImageNet with various zero-shot splits show that our latent features generalize well in large-scale settings.", "pdf": "/pdf/2dca6b5142e07b5b8b22a705ca48f03b8b8b4f58.pdf", "paperhash": "sch\u00f6nfeld|crosslinked_variational_autoencoders_for_generalized_zeroshot_learning"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/LLD/-/Paper40/Official_Review", "cdate": 1553713415200, "expdate": 1555718400000, "duedate": 1554681600000, "reply": {"forum": "BkghJoRNO4", "replyto": "BkghJoRNO4", "writers": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2019/Workshop/LLD/Paper40/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2019/Workshop/LLD/Paper40/AnonReviewer[0-9]+"}, "readers": {"values": ["everyone"], "description": "The users who will be allowed to read the above content."}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["5: Top 15% of accepted papers, strong accept", "4: Top 50% of accepted papers, clear accept", "3: Marginally above acceptance threshold", "2: Marginally below acceptance threshold", "1: Strong rejection"], "required": true}, "confidence": {"order": 4, "value-radio": ["3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "2: The reviewer is fairly confident that the evaluation is correct", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "tcdate": 1553713415200, "tmdate": 1555511825439, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/LLD"], "invitees": ["ICLR.cc/2019/Workshop/LLD/Paper40/Reviewers"], "signatures": ["ICLR.cc/2019/Workshop/LLD"]}}}, {"id": "rklvbwkYKE", "original": null, "number": 1, "cdate": 1554736894849, "ddate": null, "tcdate": 1554736894849, "tmdate": 1555510986727, "tddate": null, "forum": "BkghJoRNO4", "replyto": "BkghJoRNO4", "invitation": "ICLR.cc/2019/Workshop/LLD/-/Paper40/Decision", "content": {"title": "Acceptance Decision", "decision": "Accept"}, "signatures": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Cross-Linked Variational Autoencoders for Generalized Zero-Shot Learning", "authors": ["Edgar Sch\u00f6nfeld", "Sayna Ebrahimi", "Samarth Sinha", "Trevor Darrell", "Zeynep Akata"], "authorids": ["edgar.schoenfeld@bosch.com", "sayna@eecs.berkeley.edu", "samarth.sinha@mail.utoronto.ca", "trevor@eecs.berkeley.edu", "z.akata@uva.nl"], "keywords": ["generalized zero-shot learning", "zero-shot learning", "few-shot learning", "image classification"], "TL;DR": "We use VAEs to learn a shared latent space embedding between image features and attributes and thereby achieve state-of-the-art results in generalized zero-shot learning.", "abstract": "Most approaches in generalized zero-shot learning rely on cross-modal mapping between an image feature space and a class embedding space or on generating artificial image features. However, learning a shared cross-modal embedding by aligning the latent spaces of modality-specific autoencoders is shown to be promising in (generalized) zero-shot learning. While following the same direction, we also take artificial feature generation one step further and propose a model where a shared latent space of image features and class embeddings is learned by aligned variational autoencoders, for the purpose of generating latent features to train a softmax classifier. We evaluate our learned latent features on conventional benchmark datasets and establish a new state of the art on generalized zero-shot as well as on few-shot learning. Moreover, our results on ImageNet with various zero-shot splits show that our latent features generalize well in large-scale settings.", "pdf": "/pdf/2dca6b5142e07b5b8b22a705ca48f03b8b8b4f58.pdf", "paperhash": "sch\u00f6nfeld|crosslinked_variational_autoencoders_for_generalized_zeroshot_learning"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/LLD/-/Paper40/Decision", "cdate": 1554736076674, "reply": {"forum": "BkghJoRNO4", "replyto": "BkghJoRNO4", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-regex": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "description": "How your identity will be displayed."}, "signatures": {"values": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"title": {"order": 1, "required": true, "value": "Acceptance Decision"}, "decision": {"order": 2, "required": true, "value-radio": ["Accept", "Reject"], "description": "Acceptance decision"}, "comment": {"order": 3, "required": false, "value-regex": "[\\S\\s]{0,5000}", "description": ""}}}, "tcdate": 1554736076674, "tmdate": 1555510962191, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/LLD"], "invitees": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "signatures": ["ICLR.cc/2019/Workshop/LLD"]}}}], "count": 4}