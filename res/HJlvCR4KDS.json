{"notes": [{"id": "HJlvCR4KDS", "original": "H1xffYcOPH", "number": 1435, "cdate": 1569439439495, "ddate": null, "tcdate": 1569439439495, "tmdate": 1577168243813, "tddate": null, "forum": "HJlvCR4KDS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Why Does the VQA Model Answer No?: Improving Reasoning through Visual and Linguistic Inference", "authors": ["Seungjun Jung", "Junyoung Byun", "Kyujin Shim", "Changick Kim"], "authorids": ["seungjun45@kaist.ac.kr", "bjyoung@kaist.ac.kr", "kjshim1028@kaist.ac.kr", "changick@kaist.ac.kr"], "keywords": ["Image Captioning", "Visual Question Answering", "Explainable A.I", "Beam Search", "Constrained Beam Search"], "abstract": "In order to make Visual Question Answering (VQA) explainable, previous studies not only visualize the attended region of a VQA model but also generate textual explanations for its answers. However, when the model\u2019s answer is \u2018no,\u2019 existing methods have difficulty in revealing detailed arguments that lead to that answer. In addition, previous methods are insufficient to provide logical bases, when the question requires common sense to answer. In this paper, we propose a novel textual explanation method to overcome the aforementioned limitations. First, we extract keywords that are essential to infer an answer from a question. Second, for a pre-trained explanation generator, we utilize a novel Variable-Constrained\nBeam Search (VCBS) algorithm to generate phrases that best describes the relationship between keywords in images. Then, we complete an explanation by feeding the phrase to the generator. Furthermore, if the answer to the question is \u201cyes\u201d or \u201cno,\u201d we apply Natural Langauge Inference (NLI) to identify whether contents of the question can be inferred from the explanation using common sense. Our user study, conducted in Amazon Mechanical Turk (MTurk), shows that our proposed method generates more reliable explanations compared to the previous methods. Moreover, by modifying the VQA model\u2019s answer through the output of the NLI model, we show that VQA performance increases by 1.1% from the original model.", "pdf": "/pdf/80da210e33deb537d6b6a14d56ffa8e3f98d580f.pdf", "paperhash": "jung|why_does_the_vqa_model_answer_no_improving_reasoning_through_visual_and_linguistic_inference", "original_pdf": "/attachment/5f76f46c6924afd4f0c8bf1ee6891d4394911165.pdf", "_bibtex": "@misc{\njung2020why,\ntitle={Why Does the {\\{}VQA{\\}} Model Answer No?: Improving Reasoning through Visual and Linguistic Inference},\nauthor={Seungjun Jung and Junyoung Byun and Kyujin Shim and Changick Kim},\nyear={2020},\nurl={https://openreview.net/forum?id=HJlvCR4KDS}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "pJMaaKC6sx", "original": null, "number": 1, "cdate": 1576798723214, "ddate": null, "tcdate": 1576798723214, "tmdate": 1576800913340, "tddate": null, "forum": "HJlvCR4KDS", "replyto": "HJlvCR4KDS", "invitation": "ICLR.cc/2020/Conference/Paper1435/-/Decision", "content": {"decision": "Reject", "comment": "This paper is good, with relatively positive support from the reviewers. However, there were also several legitimate issues raised, for example regarding the semantics of a negative answer and associated explanations. Though this paper cannot be accepted at this time, we hope the feedback here can help improve a future version, as all reviewers agree this is a valuable line of work.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Why Does the VQA Model Answer No?: Improving Reasoning through Visual and Linguistic Inference", "authors": ["Seungjun Jung", "Junyoung Byun", "Kyujin Shim", "Changick Kim"], "authorids": ["seungjun45@kaist.ac.kr", "bjyoung@kaist.ac.kr", "kjshim1028@kaist.ac.kr", "changick@kaist.ac.kr"], "keywords": ["Image Captioning", "Visual Question Answering", "Explainable A.I", "Beam Search", "Constrained Beam Search"], "abstract": "In order to make Visual Question Answering (VQA) explainable, previous studies not only visualize the attended region of a VQA model but also generate textual explanations for its answers. However, when the model\u2019s answer is \u2018no,\u2019 existing methods have difficulty in revealing detailed arguments that lead to that answer. In addition, previous methods are insufficient to provide logical bases, when the question requires common sense to answer. In this paper, we propose a novel textual explanation method to overcome the aforementioned limitations. First, we extract keywords that are essential to infer an answer from a question. Second, for a pre-trained explanation generator, we utilize a novel Variable-Constrained\nBeam Search (VCBS) algorithm to generate phrases that best describes the relationship between keywords in images. Then, we complete an explanation by feeding the phrase to the generator. Furthermore, if the answer to the question is \u201cyes\u201d or \u201cno,\u201d we apply Natural Langauge Inference (NLI) to identify whether contents of the question can be inferred from the explanation using common sense. Our user study, conducted in Amazon Mechanical Turk (MTurk), shows that our proposed method generates more reliable explanations compared to the previous methods. Moreover, by modifying the VQA model\u2019s answer through the output of the NLI model, we show that VQA performance increases by 1.1% from the original model.", "pdf": "/pdf/80da210e33deb537d6b6a14d56ffa8e3f98d580f.pdf", "paperhash": "jung|why_does_the_vqa_model_answer_no_improving_reasoning_through_visual_and_linguistic_inference", "original_pdf": "/attachment/5f76f46c6924afd4f0c8bf1ee6891d4394911165.pdf", "_bibtex": "@misc{\njung2020why,\ntitle={Why Does the {\\{}VQA{\\}} Model Answer No?: Improving Reasoning through Visual and Linguistic Inference},\nauthor={Seungjun Jung and Junyoung Byun and Kyujin Shim and Changick Kim},\nyear={2020},\nurl={https://openreview.net/forum?id=HJlvCR4KDS}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "HJlvCR4KDS", "replyto": "HJlvCR4KDS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795716263, "tmdate": 1576800266362, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1435/-/Decision"}}}, {"id": "H1x8vBV6tS", "original": null, "number": 2, "cdate": 1571796317939, "ddate": null, "tcdate": 1571796317939, "tmdate": 1574279997153, "tddate": null, "forum": "HJlvCR4KDS", "replyto": "HJlvCR4KDS", "invitation": "ICLR.cc/2020/Conference/Paper1435/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #3", "review": "\n I thank the authors for their response.  I would keep my score unchanged (i.e., 6 Weak Accept). \n\n-----------------------------------------------\n\nStrengths: \n- The paper enhances the beam search approach to generate explanations for answers to visual questions. The explanations are further used for verifying the yes/no answers.\n- The paper constructs a VCBS algorithm with novelties in allowing soft constrains of the generated beams. Since I have not worked on the constrained beam search before, thus it is hard for me to measure the novelty of this method.\n- The results of VQA 2.0 is pretty good. The accuracy of the Yes/No questions almost achieves the SotA systems. \n\n\nWeakness:\n- I have a question about the NLI system. Since the NLI is a three-way classifier where the answers would be \"Entail\", \"Contradictory\", and \"Neutral\".  What would the system do when the relationship is \"Neutral\"? For now, I think that it just give an answer of \"no\" but I am not sure whether it is correct.  For example, in Fig. 3, it shows an explanation (premise) of \"something (not the vegetable) on a plate\" and the hypo is \"there are vegetables on the plate\". Since the hypo is not necessary to contradict the premise, the relationship should be neutral. It does not directly provide evidence of the answer.\n\nComments:\n- Since more data are involved in training the explanation system, the proposed methods are not fairly compared with the BAN method. It would be better to mention this detail in the paper. \n\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1435/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1435/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Why Does the VQA Model Answer No?: Improving Reasoning through Visual and Linguistic Inference", "authors": ["Seungjun Jung", "Junyoung Byun", "Kyujin Shim", "Changick Kim"], "authorids": ["seungjun45@kaist.ac.kr", "bjyoung@kaist.ac.kr", "kjshim1028@kaist.ac.kr", "changick@kaist.ac.kr"], "keywords": ["Image Captioning", "Visual Question Answering", "Explainable A.I", "Beam Search", "Constrained Beam Search"], "abstract": "In order to make Visual Question Answering (VQA) explainable, previous studies not only visualize the attended region of a VQA model but also generate textual explanations for its answers. However, when the model\u2019s answer is \u2018no,\u2019 existing methods have difficulty in revealing detailed arguments that lead to that answer. In addition, previous methods are insufficient to provide logical bases, when the question requires common sense to answer. In this paper, we propose a novel textual explanation method to overcome the aforementioned limitations. First, we extract keywords that are essential to infer an answer from a question. Second, for a pre-trained explanation generator, we utilize a novel Variable-Constrained\nBeam Search (VCBS) algorithm to generate phrases that best describes the relationship between keywords in images. Then, we complete an explanation by feeding the phrase to the generator. Furthermore, if the answer to the question is \u201cyes\u201d or \u201cno,\u201d we apply Natural Langauge Inference (NLI) to identify whether contents of the question can be inferred from the explanation using common sense. Our user study, conducted in Amazon Mechanical Turk (MTurk), shows that our proposed method generates more reliable explanations compared to the previous methods. Moreover, by modifying the VQA model\u2019s answer through the output of the NLI model, we show that VQA performance increases by 1.1% from the original model.", "pdf": "/pdf/80da210e33deb537d6b6a14d56ffa8e3f98d580f.pdf", "paperhash": "jung|why_does_the_vqa_model_answer_no_improving_reasoning_through_visual_and_linguistic_inference", "original_pdf": "/attachment/5f76f46c6924afd4f0c8bf1ee6891d4394911165.pdf", "_bibtex": "@misc{\njung2020why,\ntitle={Why Does the {\\{}VQA{\\}} Model Answer No?: Improving Reasoning through Visual and Linguistic Inference},\nauthor={Seungjun Jung and Junyoung Byun and Kyujin Shim and Changick Kim},\nyear={2020},\nurl={https://openreview.net/forum?id=HJlvCR4KDS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJlvCR4KDS", "replyto": "HJlvCR4KDS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1435/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1435/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575954919058, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1435/Reviewers"], "noninvitees": [], "tcdate": 1570237737436, "tmdate": 1575954919069, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1435/-/Official_Review"}}}, {"id": "SyxclpUcsS", "original": null, "number": 3, "cdate": 1573706994305, "ddate": null, "tcdate": 1573706994305, "tmdate": 1573706994305, "tddate": null, "forum": "HJlvCR4KDS", "replyto": "H1x8vBV6tS", "invitation": "ICLR.cc/2020/Conference/Paper1435/-/Official_Comment", "content": {"title": "Response to Review #3", "comment": "We are obliged to get your suggestion and advice, and it is an honor to have your assistance.\n\n1) I have a question about the NLI system. Since the NLI is a three-way classifier where the answers would be \"Entail\", \"Contradictory\", and \"Neutral\". What would the system do when the relationship is \"Neutral\"? For now, I think that it just give an answer of \"no\" but I am not sure whether it is correct. For example, in Fig. 3, it shows an explanation (premise) of \"something (not the vegetable) on a plate\" and the hypo is \"there are vegetables on the plate\". Since the hypo is not necessary to contradict the premise, the relationship should be neutral. It does not directly provide evidence of the answer.\n\n\nFollowing the reviewer's thoughtful concern, we conducted additional experiments to confirm this. In this experiment, we set the VQA model\u2019s answer as \"yes\" if the class probability of the NLI model for \"Entail\" is greater than \"Contradictory,\" even though the NLI result is \"Neutral.\"\n\nAs the reviewer pointed out, we found that the answer is often \"yes,\" even when there is little correlation between premise and hypothesis. For this reason, we use the answer as \"yes\" only if the NLI result is \"Entail,\u201d as we first suggested in the paper.\n\np.s-\nIn Fig 3, the premise is \"a slice of pizza is sitting on a plate.\" \"something (not the vegetable) on a plate\" is a sentence added to visualize which keyword is absent in the image.\n\n2) Since more data are involved in training the explanation system, the proposed methods are not fairly compared with the BAN method. It would be better to mention this detail in the paper. \n\nThank you for your advice. We mentioned the reviewer's comment in Table 3. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1435/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1435/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Why Does the VQA Model Answer No?: Improving Reasoning through Visual and Linguistic Inference", "authors": ["Seungjun Jung", "Junyoung Byun", "Kyujin Shim", "Changick Kim"], "authorids": ["seungjun45@kaist.ac.kr", "bjyoung@kaist.ac.kr", "kjshim1028@kaist.ac.kr", "changick@kaist.ac.kr"], "keywords": ["Image Captioning", "Visual Question Answering", "Explainable A.I", "Beam Search", "Constrained Beam Search"], "abstract": "In order to make Visual Question Answering (VQA) explainable, previous studies not only visualize the attended region of a VQA model but also generate textual explanations for its answers. However, when the model\u2019s answer is \u2018no,\u2019 existing methods have difficulty in revealing detailed arguments that lead to that answer. In addition, previous methods are insufficient to provide logical bases, when the question requires common sense to answer. In this paper, we propose a novel textual explanation method to overcome the aforementioned limitations. First, we extract keywords that are essential to infer an answer from a question. Second, for a pre-trained explanation generator, we utilize a novel Variable-Constrained\nBeam Search (VCBS) algorithm to generate phrases that best describes the relationship between keywords in images. Then, we complete an explanation by feeding the phrase to the generator. Furthermore, if the answer to the question is \u201cyes\u201d or \u201cno,\u201d we apply Natural Langauge Inference (NLI) to identify whether contents of the question can be inferred from the explanation using common sense. Our user study, conducted in Amazon Mechanical Turk (MTurk), shows that our proposed method generates more reliable explanations compared to the previous methods. Moreover, by modifying the VQA model\u2019s answer through the output of the NLI model, we show that VQA performance increases by 1.1% from the original model.", "pdf": "/pdf/80da210e33deb537d6b6a14d56ffa8e3f98d580f.pdf", "paperhash": "jung|why_does_the_vqa_model_answer_no_improving_reasoning_through_visual_and_linguistic_inference", "original_pdf": "/attachment/5f76f46c6924afd4f0c8bf1ee6891d4394911165.pdf", "_bibtex": "@misc{\njung2020why,\ntitle={Why Does the {\\{}VQA{\\}} Model Answer No?: Improving Reasoning through Visual and Linguistic Inference},\nauthor={Seungjun Jung and Junyoung Byun and Kyujin Shim and Changick Kim},\nyear={2020},\nurl={https://openreview.net/forum?id=HJlvCR4KDS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJlvCR4KDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1435/Authors", "ICLR.cc/2020/Conference/Paper1435/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1435/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1435/Reviewers", "ICLR.cc/2020/Conference/Paper1435/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1435/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1435/Authors|ICLR.cc/2020/Conference/Paper1435/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504156053, "tmdate": 1576860549662, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1435/Authors", "ICLR.cc/2020/Conference/Paper1435/Reviewers", "ICLR.cc/2020/Conference/Paper1435/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1435/-/Official_Comment"}}}, {"id": "H1lLnhUqiH", "original": null, "number": 2, "cdate": 1573706925806, "ddate": null, "tcdate": 1573706925806, "tmdate": 1573706925806, "tddate": null, "forum": "HJlvCR4KDS", "replyto": "S1gXpfnhFB", "invitation": "ICLR.cc/2020/Conference/Paper1435/-/Official_Comment", "content": {"title": "Response to Review #2", "comment": "We are hugely thankful for your invaluable and thoughtful comments and support.\n\n1) However, the proposed approach also has noticeable weaknesses. It relies on external models or tools for natural language inference, and such inference does not take into account the visual context of the image. Also, the explanations generated from the proposed model only justify the answer but are not introspective, and they do not reflect the decision process of the target VQA model.\n\nWe agree with the issue the reviewer has pointed out. It's best to do inference using both visual and linguistic information at the same time, but unfortunately, we have not solved this problem yet. Therefore, we proposed a method of inference by using Visual and Linguistic information step by step. We also agree that explanations created in the proposed way are not introspective.  For this reason, we have revised the content of this paper as follows: (the proposed method not only produces more introspective explanations, ... ->  the proposed method not only produces more appropriate explanations, ...)\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1435/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1435/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Why Does the VQA Model Answer No?: Improving Reasoning through Visual and Linguistic Inference", "authors": ["Seungjun Jung", "Junyoung Byun", "Kyujin Shim", "Changick Kim"], "authorids": ["seungjun45@kaist.ac.kr", "bjyoung@kaist.ac.kr", "kjshim1028@kaist.ac.kr", "changick@kaist.ac.kr"], "keywords": ["Image Captioning", "Visual Question Answering", "Explainable A.I", "Beam Search", "Constrained Beam Search"], "abstract": "In order to make Visual Question Answering (VQA) explainable, previous studies not only visualize the attended region of a VQA model but also generate textual explanations for its answers. However, when the model\u2019s answer is \u2018no,\u2019 existing methods have difficulty in revealing detailed arguments that lead to that answer. In addition, previous methods are insufficient to provide logical bases, when the question requires common sense to answer. In this paper, we propose a novel textual explanation method to overcome the aforementioned limitations. First, we extract keywords that are essential to infer an answer from a question. Second, for a pre-trained explanation generator, we utilize a novel Variable-Constrained\nBeam Search (VCBS) algorithm to generate phrases that best describes the relationship between keywords in images. Then, we complete an explanation by feeding the phrase to the generator. Furthermore, if the answer to the question is \u201cyes\u201d or \u201cno,\u201d we apply Natural Langauge Inference (NLI) to identify whether contents of the question can be inferred from the explanation using common sense. Our user study, conducted in Amazon Mechanical Turk (MTurk), shows that our proposed method generates more reliable explanations compared to the previous methods. Moreover, by modifying the VQA model\u2019s answer through the output of the NLI model, we show that VQA performance increases by 1.1% from the original model.", "pdf": "/pdf/80da210e33deb537d6b6a14d56ffa8e3f98d580f.pdf", "paperhash": "jung|why_does_the_vqa_model_answer_no_improving_reasoning_through_visual_and_linguistic_inference", "original_pdf": "/attachment/5f76f46c6924afd4f0c8bf1ee6891d4394911165.pdf", "_bibtex": "@misc{\njung2020why,\ntitle={Why Does the {\\{}VQA{\\}} Model Answer No?: Improving Reasoning through Visual and Linguistic Inference},\nauthor={Seungjun Jung and Junyoung Byun and Kyujin Shim and Changick Kim},\nyear={2020},\nurl={https://openreview.net/forum?id=HJlvCR4KDS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJlvCR4KDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1435/Authors", "ICLR.cc/2020/Conference/Paper1435/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1435/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1435/Reviewers", "ICLR.cc/2020/Conference/Paper1435/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1435/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1435/Authors|ICLR.cc/2020/Conference/Paper1435/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504156053, "tmdate": 1576860549662, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1435/Authors", "ICLR.cc/2020/Conference/Paper1435/Reviewers", "ICLR.cc/2020/Conference/Paper1435/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1435/-/Official_Comment"}}}, {"id": "rJgxc285jS", "original": null, "number": 1, "cdate": 1573706887687, "ddate": null, "tcdate": 1573706887687, "tmdate": 1573706887687, "tddate": null, "forum": "HJlvCR4KDS", "replyto": "rJlJyunRKB", "invitation": "ICLR.cc/2020/Conference/Paper1435/-/Official_Comment", "content": {"title": "Response to Review #1", "comment": "We greatly appreciate your kind and detail review. We have tried our best to meet the comments. We will respond to your concerns below in the same order they were made.\n\n1) It actually generates relevant captions with respect to the question and answers instead of real explanations. It's true that correlated caption can sometimes serve as the explanation when they cover the same concept coincidently but, we can not use these captions to explain the reasoning process of VQA:\n\nThank you for your kind and detailed comment.\nWe stated on page 2 of the manuscript that the proposed method produces introspective explanations. However, we agree that, as the reviewer pointed out, explanations created in the proposed way are not introspective. For this reason, we have revised the content of this paper as follows: (the proposed method not only produces more introspective explanations, ... -> the proposed method not only produces more appropriate explanations, ...).\n\n2) The technique novelty of the proposed paper is also limited; the major novelty is the VCBS, which seems very similar to CBS. The only difference is VCBS adds relaxed parameters, which seems no technique novelty.\n\nVCBS differs from CBS in that it uses loosening parameters and the number of satisfied constraints ( | y and C | ). Therefore VCBS helps to identify the keywords that cause the VQA's answer to be \"no.\" It is the first attempt to determine the cause of \"no,\" which is extremely hard for existing methods. \n\nWe also leveraged the NLI model and ConceptNet to create explanations that require common sense. For these aforementioned reasons, we think our proposed scheme is novel enough. \n\n3) Most annotations in Algorithm 1 is also not explained, making the readers hard to follow the actual content.\n\nThank you for your thoughtful comment. We have revised the paper to make it easier for readers to understand the meaning of each notation in Algorithm 1.\n\n4) The proposed model, although tied the VQA words with the explanation words, suffers the same problems as the PJ-X model, which didn't consider the VQA attention at all.\n\nAlthough the BUTD is used as an explanation generator, the method presented in this paper (i.e., VCBS and NLI) can also utilize the PJ-X or the Multi-VQAE as an explanation generator. To illustrate this, we conducted an additional experiment using Multi-VQAE as an explanation generator and included sample results in the Appendix.\n As a result of replacing the explanation generator of our method to Multi-VQAE, we show that we can improve the explanations generated by the model (Multi-VQAE) while considering VQA attentions.\n\n\n5) The experiment is also weak, considering the results is conduct on 100 samples, there might be significant variance.\n\nThank you for your thoughtful consideration. We experimented with 100 additional samples and added the results to the Appendix. We have confirmed that we get results similar to the previous experiments.\n\n6) It's interesting the compared approach is learned based on a different dataset, which makes the results harder to compare. The NLI model results are interesting, but for a more fair comparison, I would expect the proposed method compare with a model trained with VQA and coco caption dataset, such as VQA-E.\n\nThank you again for your sincere advice. We understand what the reviewer is worried about. We post additional results using the Multi-VQAE as an explanation generator in the Appendix. As a result of applying our method to Multi-VQAE, we can see that more appropriate explanations are generated\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1435/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1435/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Why Does the VQA Model Answer No?: Improving Reasoning through Visual and Linguistic Inference", "authors": ["Seungjun Jung", "Junyoung Byun", "Kyujin Shim", "Changick Kim"], "authorids": ["seungjun45@kaist.ac.kr", "bjyoung@kaist.ac.kr", "kjshim1028@kaist.ac.kr", "changick@kaist.ac.kr"], "keywords": ["Image Captioning", "Visual Question Answering", "Explainable A.I", "Beam Search", "Constrained Beam Search"], "abstract": "In order to make Visual Question Answering (VQA) explainable, previous studies not only visualize the attended region of a VQA model but also generate textual explanations for its answers. However, when the model\u2019s answer is \u2018no,\u2019 existing methods have difficulty in revealing detailed arguments that lead to that answer. In addition, previous methods are insufficient to provide logical bases, when the question requires common sense to answer. In this paper, we propose a novel textual explanation method to overcome the aforementioned limitations. First, we extract keywords that are essential to infer an answer from a question. Second, for a pre-trained explanation generator, we utilize a novel Variable-Constrained\nBeam Search (VCBS) algorithm to generate phrases that best describes the relationship between keywords in images. Then, we complete an explanation by feeding the phrase to the generator. Furthermore, if the answer to the question is \u201cyes\u201d or \u201cno,\u201d we apply Natural Langauge Inference (NLI) to identify whether contents of the question can be inferred from the explanation using common sense. Our user study, conducted in Amazon Mechanical Turk (MTurk), shows that our proposed method generates more reliable explanations compared to the previous methods. Moreover, by modifying the VQA model\u2019s answer through the output of the NLI model, we show that VQA performance increases by 1.1% from the original model.", "pdf": "/pdf/80da210e33deb537d6b6a14d56ffa8e3f98d580f.pdf", "paperhash": "jung|why_does_the_vqa_model_answer_no_improving_reasoning_through_visual_and_linguistic_inference", "original_pdf": "/attachment/5f76f46c6924afd4f0c8bf1ee6891d4394911165.pdf", "_bibtex": "@misc{\njung2020why,\ntitle={Why Does the {\\{}VQA{\\}} Model Answer No?: Improving Reasoning through Visual and Linguistic Inference},\nauthor={Seungjun Jung and Junyoung Byun and Kyujin Shim and Changick Kim},\nyear={2020},\nurl={https://openreview.net/forum?id=HJlvCR4KDS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJlvCR4KDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1435/Authors", "ICLR.cc/2020/Conference/Paper1435/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1435/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1435/Reviewers", "ICLR.cc/2020/Conference/Paper1435/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1435/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1435/Authors|ICLR.cc/2020/Conference/Paper1435/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504156053, "tmdate": 1576860549662, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1435/Authors", "ICLR.cc/2020/Conference/Paper1435/Reviewers", "ICLR.cc/2020/Conference/Paper1435/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1435/-/Official_Comment"}}}, {"id": "S1gXpfnhFB", "original": null, "number": 1, "cdate": 1571762875410, "ddate": null, "tcdate": 1571762875410, "tmdate": 1572972469423, "tddate": null, "forum": "HJlvCR4KDS", "replyto": "HJlvCR4KDS", "invitation": "ICLR.cc/2020/Conference/Paper1435/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "\nThe paper proposes a novel method for explaining VQA systems. Different from most previous work, the proposed approach generates textual explanations based on three steps. First, it extracts keywords from the question. Then an explanation sentence is decoded (based on an RNN image captioner) through the proposed Variable-Constrained Beam Search (VCBS) algorithm to satisfy the keyword constraints. Finally, 3) checking through linguistic inference whether the explanation sentence can be used as a premise to infer the question and answer.\n\nI would recommend for acceptance. The paper proposes an alternative approach to VQA explanations, together with a few supporting algorithms such as VCBS. It is potentially helpful to future work on textual explanations and explainable AI in general.\n\nAt a high level, it is ambiguous to decide what is a reasonable explanation for many \u201cno\u201d answers. For example, one usually cannot provide stronger justification than \u201cthere is indeed no one\u201d or \u201cI don\u2019t see anyone\u201d to the question \u201cIs there anyone in the room\u201d with an answer \u201cno.\u201d The paper frames this explanation generation task as a linguistic inference task and checks entailment between the explanation and the question-answer pair. While it is debatable whether this is optimal, the proposed approach provides valuable insights on what constitutes a good explanation.\n\nHowever, the proposed approach also has noticeable weaknesses. \n\nIt relies on external models or tools for natural language inference, and such inference does not take into account the visual context of the image. Also, the explanations generated from the proposed model only justify the answer but are not introspective, and they do not reflect the decision process of the target VQA model."}, "signatures": ["ICLR.cc/2020/Conference/Paper1435/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1435/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Why Does the VQA Model Answer No?: Improving Reasoning through Visual and Linguistic Inference", "authors": ["Seungjun Jung", "Junyoung Byun", "Kyujin Shim", "Changick Kim"], "authorids": ["seungjun45@kaist.ac.kr", "bjyoung@kaist.ac.kr", "kjshim1028@kaist.ac.kr", "changick@kaist.ac.kr"], "keywords": ["Image Captioning", "Visual Question Answering", "Explainable A.I", "Beam Search", "Constrained Beam Search"], "abstract": "In order to make Visual Question Answering (VQA) explainable, previous studies not only visualize the attended region of a VQA model but also generate textual explanations for its answers. However, when the model\u2019s answer is \u2018no,\u2019 existing methods have difficulty in revealing detailed arguments that lead to that answer. In addition, previous methods are insufficient to provide logical bases, when the question requires common sense to answer. In this paper, we propose a novel textual explanation method to overcome the aforementioned limitations. First, we extract keywords that are essential to infer an answer from a question. Second, for a pre-trained explanation generator, we utilize a novel Variable-Constrained\nBeam Search (VCBS) algorithm to generate phrases that best describes the relationship between keywords in images. Then, we complete an explanation by feeding the phrase to the generator. Furthermore, if the answer to the question is \u201cyes\u201d or \u201cno,\u201d we apply Natural Langauge Inference (NLI) to identify whether contents of the question can be inferred from the explanation using common sense. Our user study, conducted in Amazon Mechanical Turk (MTurk), shows that our proposed method generates more reliable explanations compared to the previous methods. Moreover, by modifying the VQA model\u2019s answer through the output of the NLI model, we show that VQA performance increases by 1.1% from the original model.", "pdf": "/pdf/80da210e33deb537d6b6a14d56ffa8e3f98d580f.pdf", "paperhash": "jung|why_does_the_vqa_model_answer_no_improving_reasoning_through_visual_and_linguistic_inference", "original_pdf": "/attachment/5f76f46c6924afd4f0c8bf1ee6891d4394911165.pdf", "_bibtex": "@misc{\njung2020why,\ntitle={Why Does the {\\{}VQA{\\}} Model Answer No?: Improving Reasoning through Visual and Linguistic Inference},\nauthor={Seungjun Jung and Junyoung Byun and Kyujin Shim and Changick Kim},\nyear={2020},\nurl={https://openreview.net/forum?id=HJlvCR4KDS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJlvCR4KDS", "replyto": "HJlvCR4KDS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1435/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1435/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575954919058, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1435/Reviewers"], "noninvitees": [], "tcdate": 1570237737436, "tmdate": 1575954919069, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1435/-/Official_Review"}}}, {"id": "rJlJyunRKB", "original": null, "number": 3, "cdate": 1571895255201, "ddate": null, "tcdate": 1571895255201, "tmdate": 1572972469377, "tddate": null, "forum": "HJlvCR4KDS", "replyto": "HJlvCR4KDS", "invitation": "ICLR.cc/2020/Conference/Paper1435/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "Interesting results and analysis but lack of novelty and details. \n\nThis paper proposed a novel text explanation method which extracts keywords that are essential to infer an answer from question. The authors proposed VCBS based on CBS and use Natural language inference to identify the entailments when the answer is yes or no. Experiment results show better mean opinion score 100 random samples results compared with the previous method and better VQA binary classifications when flipping based on the NLI results. \n\nDifferent from [Park et. al. 2018], who collected the explanations, this paper directly use the coco captions dataset as the source for the explanations. One of my major concern about this paper is it actually generate relevant captions with respect to the question and answer instead of real explanations. It's true that correlated caption can sometimes serve as the explanation when they cover the same concept coincidently but, we can not use these captions to explain the reasoning process of VQA. \n\nThe technique novelty of the proposed paper is also limited, the major novelty is the VCBS, which seems very similar to CBS. The only difference is VCBS adds relaxed parameters, which seems no technique novelty. Most annotations in Algorithms 1 is also not explained, making the readers hard to follow the actual content. The proposed model, although tied the vqa words with the explanation words, it suffers the same problems as PJ-X model, which didn't consider the VQA attention at all. \n\nThe experiment is also weak, considering the results is conduct on 100 samples, there might be significant variance. It's interesting the compared approach is learned based on a different dataset, which makes the results harder to compare. The NLI model results are interesting but for a more fair comparison, I would expect the proposed method compare with a model trained with VQA and coco caption dataset, such as VQA-E. \n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1435/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1435/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Why Does the VQA Model Answer No?: Improving Reasoning through Visual and Linguistic Inference", "authors": ["Seungjun Jung", "Junyoung Byun", "Kyujin Shim", "Changick Kim"], "authorids": ["seungjun45@kaist.ac.kr", "bjyoung@kaist.ac.kr", "kjshim1028@kaist.ac.kr", "changick@kaist.ac.kr"], "keywords": ["Image Captioning", "Visual Question Answering", "Explainable A.I", "Beam Search", "Constrained Beam Search"], "abstract": "In order to make Visual Question Answering (VQA) explainable, previous studies not only visualize the attended region of a VQA model but also generate textual explanations for its answers. However, when the model\u2019s answer is \u2018no,\u2019 existing methods have difficulty in revealing detailed arguments that lead to that answer. In addition, previous methods are insufficient to provide logical bases, when the question requires common sense to answer. In this paper, we propose a novel textual explanation method to overcome the aforementioned limitations. First, we extract keywords that are essential to infer an answer from a question. Second, for a pre-trained explanation generator, we utilize a novel Variable-Constrained\nBeam Search (VCBS) algorithm to generate phrases that best describes the relationship between keywords in images. Then, we complete an explanation by feeding the phrase to the generator. Furthermore, if the answer to the question is \u201cyes\u201d or \u201cno,\u201d we apply Natural Langauge Inference (NLI) to identify whether contents of the question can be inferred from the explanation using common sense. Our user study, conducted in Amazon Mechanical Turk (MTurk), shows that our proposed method generates more reliable explanations compared to the previous methods. Moreover, by modifying the VQA model\u2019s answer through the output of the NLI model, we show that VQA performance increases by 1.1% from the original model.", "pdf": "/pdf/80da210e33deb537d6b6a14d56ffa8e3f98d580f.pdf", "paperhash": "jung|why_does_the_vqa_model_answer_no_improving_reasoning_through_visual_and_linguistic_inference", "original_pdf": "/attachment/5f76f46c6924afd4f0c8bf1ee6891d4394911165.pdf", "_bibtex": "@misc{\njung2020why,\ntitle={Why Does the {\\{}VQA{\\}} Model Answer No?: Improving Reasoning through Visual and Linguistic Inference},\nauthor={Seungjun Jung and Junyoung Byun and Kyujin Shim and Changick Kim},\nyear={2020},\nurl={https://openreview.net/forum?id=HJlvCR4KDS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJlvCR4KDS", "replyto": "HJlvCR4KDS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1435/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1435/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575954919058, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1435/Reviewers"], "noninvitees": [], "tcdate": 1570237737436, "tmdate": 1575954919069, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1435/-/Official_Review"}}}], "count": 8}