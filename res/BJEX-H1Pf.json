{"notes": [{"tddate": null, "ddate": null, "original": null, "tmdate": 1521573619442, "tcdate": 1521573619442, "number": 320, "cdate": 1521573619111, "id": "S1ilkJkqz", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "BJEX-H1Pf", "replyto": "BJEX-H1Pf", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Accept", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "This paper was invited to the workshop track based on reviews at the main conference."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Convolutional Sequence Modeling Revisited", "abstract": "Although both convolutional and recurrent architectures have a\nlong history in sequence prediction, the current \"default\" mindset in much of\nthe deep learning community is that generic sequence modeling is best handled\nusing recurrent networks.  Yet recent results indicate that convolutional architectures\ncan outperform recurrent networks on tasks such as audio synthesis and machine\ntranslation. Given a new sequence modeling task or dataset, which architecture\nshould a practitioner use? We conduct a systematic evaluation of generic\nconvolutional and recurrent architectures for sequence modeling.\nIn particular, the models are evaluated across a broad range of standard tasks that are\ncommonly used to benchmark recurrent networks. Our results indicate that a\nsimple convolutional architecture outperforms canonical recurrent networks\nsuch as LSTMs across a diverse range of tasks and datasets, while demonstrating\nlonger effective memory. We further show that thepotential \"infinite memory\" advantage \nthat RNNs have over TCNs is largely absent in practice: TCNs indeed exhibit longer \neffective history sizes than their recurrent counterparts.   As a whole, we argue that \nit may be time to (re)consider ConvNets as the default ``go to'' architecture for sequence\nmodeling.", "pdf": "/pdf/09cdf51a57ae8a3600ad7e9b5a27547433884ca3.pdf", "TL;DR": "Convolutional networks should be considered a natural starting point for sequence modeling tasks.", "paperhash": "bai|convolutional_sequence_modeling_revisited", "_bibtex": "@misc{\nbai2018convolutional,\ntitle={Convolutional Sequence Modeling Revisited},\nauthor={Shaojie Bai, J. Zico Kolter, Vladlen Koltun},\nyear={2018},\nurl={https://openreview.net/forum?id=rk8wKk-R-},\n}", "keywords": ["Temporal Convolutional Network", "Sequence Modeling", "Deep Learning"], "authors": ["Shaojie Bai", "J. Zico Kolter", "Vladlen Koltun"], "authorids": ["shaojieb@cs.cmu.edu", "zkolter@cs.cmu.edu", "vkoltun@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}, {"tddate": null, "ddate": null, "tmdate": 1518730175288, "tcdate": 1518453020873, "number": 151, "cdate": 1518453020873, "id": "BJEX-H1Pf", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "BJEX-H1Pf", "original": "rk8wKk-R-", "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "Convolutional Sequence Modeling Revisited", "abstract": "Although both convolutional and recurrent architectures have a\nlong history in sequence prediction, the current \"default\" mindset in much of\nthe deep learning community is that generic sequence modeling is best handled\nusing recurrent networks.  Yet recent results indicate that convolutional architectures\ncan outperform recurrent networks on tasks such as audio synthesis and machine\ntranslation. Given a new sequence modeling task or dataset, which architecture\nshould a practitioner use? We conduct a systematic evaluation of generic\nconvolutional and recurrent architectures for sequence modeling.\nIn particular, the models are evaluated across a broad range of standard tasks that are\ncommonly used to benchmark recurrent networks. Our results indicate that a\nsimple convolutional architecture outperforms canonical recurrent networks\nsuch as LSTMs across a diverse range of tasks and datasets, while demonstrating\nlonger effective memory. We further show that thepotential \"infinite memory\" advantage \nthat RNNs have over TCNs is largely absent in practice: TCNs indeed exhibit longer \neffective history sizes than their recurrent counterparts.   As a whole, we argue that \nit may be time to (re)consider ConvNets as the default ``go to'' architecture for sequence\nmodeling.", "pdf": "/pdf/09cdf51a57ae8a3600ad7e9b5a27547433884ca3.pdf", "TL;DR": "Convolutional networks should be considered a natural starting point for sequence modeling tasks.", "paperhash": "bai|convolutional_sequence_modeling_revisited", "_bibtex": "@misc{\nbai2018convolutional,\ntitle={Convolutional Sequence Modeling Revisited},\nauthor={Shaojie Bai, J. Zico Kolter, Vladlen Koltun},\nyear={2018},\nurl={https://openreview.net/forum?id=rk8wKk-R-},\n}", "keywords": ["Temporal Convolutional Network", "Sequence Modeling", "Deep Learning"], "authors": ["Shaojie Bai", "J. Zico Kolter", "Vladlen Koltun"], "authorids": ["shaojieb@cs.cmu.edu", "zkolter@cs.cmu.edu", "vkoltun@gmail.com"]}, "nonreaders": [], "details": {"replyCount": 1, "writable": false, "overwriting": [], "revisions": true, "tags": [], "original": {"tddate": null, "ddate": null, "tmdate": 1518730175288, "tcdate": 1509124445918, "number": 501, "cdate": 1518730175274, "id": "rk8wKk-R-", "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "forum": "rk8wKk-R-", "original": "HyVDYybAZ", "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference"], "content": {"title": "Convolutional Sequence Modeling Revisited", "abstract": "This paper revisits the problem of sequence modeling using convolutional \narchitectures.  Although both convolutional and recurrent architectures have a\nlong history in sequence prediction, the current \"default\" mindset in much of\nthe deep learning community is that generic sequence modeling is best handled\nusing recurrent networks.  The goal of this paper is to question this assumption. \nSpecifically, we consider a simple generic temporal convolution network (TCN),\nwhich adopts features from modern ConvNet architectures such as a dilations and \nresidual connections.  We show that on a variety of sequence modeling tasks,\nincluding many frequently used as benchmarks for evaluating recurrent networks,\nthe TCN outperforms baseline RNN methods (LSTMs, GRUs, and vanilla RNNs) and\nsometimes even highly specialized approaches.  We further show that the\npotential \"infinite memory\" advantage that RNNs have over TCNs is largely\nabsent in practice: TCNs indeed exhibit longer effective history sizes than their \nrecurrent counterparts.   As a whole, we argue that it may be time to (re)consider \nConvNets as the default \"go to\" architecture for sequence modeling.", "pdf": "/pdf/4f72202970a15efa844bae2729d7d0a4f2ed503a.pdf", "TL;DR": "We argue that convolutional networks should be considered the default starting point for sequence modeling tasks.", "paperhash": "bai|convolutional_sequence_modeling_revisited", "_bibtex": "@misc{\nbai2018convolutional,\ntitle={Convolutional Sequence Modeling Revisited},\nauthor={Shaojie Bai and J. Zico Kolter and Vladlen Koltun},\nyear={2018},\nurl={https://openreview.net/forum?id=rk8wKk-R-},\n}", "keywords": ["Temporal Convolutional Network", "Sequence Modeling", "Deep Learning"], "authors": ["Shaojie Bai", "J. Zico Kolter", "Vladlen Koltun"], "authorids": ["shaojieb@cs.cmu.edu", "zkolter@cs.cmu.edu", "vkoltun@gmail.com"]}, "nonreaders": []}, "originalWritable": false, "originalInvitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1506717071958, "id": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Conference"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference"]}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"authors": {"required": false, "order": 1, "values-regex": ".*", "description": "Comma separated list of author names, as they appear in the paper."}, "authorids": {"required": false, "order": 2, "values-regex": ".*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "cdate": 1506717071958}, "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}, "tauthor": "ICLR.cc/2018/Workshop"}], "count": 2}