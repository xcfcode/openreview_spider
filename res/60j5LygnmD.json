{"notes": [{"id": "60j5LygnmD", "original": "qW0BfGLi396", "number": 1967, "cdate": 1601308216671, "ddate": null, "tcdate": 1601308216671, "tmdate": 1616000862270, "tddate": null, "forum": "60j5LygnmD", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Meta-learning with negative learning rates", "authorids": ["~Alberto_Bernacchia1"], "authors": ["Alberto Bernacchia"], "keywords": ["Meta-learning"], "abstract": "Deep learning models require a large amount of data to perform well. When data is scarce for a target task, we can transfer the knowledge gained by training on similar tasks to quickly learn the target. A successful approach is meta-learning, or \"learning to learn\" a distribution of tasks, where \"learning\" is represented by an outer loop, and \"to learn\" by an inner loop of gradient descent. However, a number of recent empirical studies argue that the inner loop is unnecessary and more simple models work equally well or even better. We study the performance of MAML as a function of the learning rate of the inner loop, where zero learning rate implies that there is no inner loop. Using random matrix theory and exact solutions of linear models, we calculate an algebraic expression for the test loss of MAML applied to mixed linear regression and nonlinear regression with overparameterized models. Surprisingly, while the optimal learning rate for adaptation is positive, we find that the optimal learning rate for training is always negative, a setting that has never been considered before. Therefore, not only does the performance increase by decreasing the learning rate to zero, as suggested by recent work, but it can be increased even further by decreasing the learning rate to negative\nvalues. These results help clarify under what circumstances meta-learning performs best.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "bernacchia|metalearning_with_negative_learning_rates", "one-sentence_summary": "We show theoretically that the optimal inner learning rate of MAML during training is always negative in a family of models  ", "supplementary_material": "/attachment/e34c29f2a8252f67626a8ad3d84f86309d577ff8.zip", "pdf": "/pdf/d806f550e5ed35f48323af6699507966218a6eb8.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nbernacchia2021metalearning,\ntitle={Meta-learning with negative learning rates},\nauthor={Alberto Bernacchia},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=60j5LygnmD}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 14, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "ioBDoATV8V", "original": null, "number": 1, "cdate": 1610040396571, "ddate": null, "tcdate": 1610040396571, "tmdate": 1610473991845, "tddate": null, "forum": "60j5LygnmD", "replyto": "60j5LygnmD", "invitation": "ICLR.cc/2021/Conference/Paper1967/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Poster)", "comment": "While this paper would be significantly improved with experiments on real data, the reviewers all agreed that there is value in the ideas and simple experiments in this paper and all voted for acceptance after the discussion period.\n\nWe encourage the authors to consider adding an experimental evaluation in more realistic settings (e.g. with real data) in the final version of the paper."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Meta-learning with negative learning rates", "authorids": ["~Alberto_Bernacchia1"], "authors": ["Alberto Bernacchia"], "keywords": ["Meta-learning"], "abstract": "Deep learning models require a large amount of data to perform well. When data is scarce for a target task, we can transfer the knowledge gained by training on similar tasks to quickly learn the target. A successful approach is meta-learning, or \"learning to learn\" a distribution of tasks, where \"learning\" is represented by an outer loop, and \"to learn\" by an inner loop of gradient descent. However, a number of recent empirical studies argue that the inner loop is unnecessary and more simple models work equally well or even better. We study the performance of MAML as a function of the learning rate of the inner loop, where zero learning rate implies that there is no inner loop. Using random matrix theory and exact solutions of linear models, we calculate an algebraic expression for the test loss of MAML applied to mixed linear regression and nonlinear regression with overparameterized models. Surprisingly, while the optimal learning rate for adaptation is positive, we find that the optimal learning rate for training is always negative, a setting that has never been considered before. Therefore, not only does the performance increase by decreasing the learning rate to zero, as suggested by recent work, but it can be increased even further by decreasing the learning rate to negative\nvalues. These results help clarify under what circumstances meta-learning performs best.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "bernacchia|metalearning_with_negative_learning_rates", "one-sentence_summary": "We show theoretically that the optimal inner learning rate of MAML during training is always negative in a family of models  ", "supplementary_material": "/attachment/e34c29f2a8252f67626a8ad3d84f86309d577ff8.zip", "pdf": "/pdf/d806f550e5ed35f48323af6699507966218a6eb8.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nbernacchia2021metalearning,\ntitle={Meta-learning with negative learning rates},\nauthor={Alberto Bernacchia},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=60j5LygnmD}\n}"}, "tags": [], "invitation": {"reply": {"forum": "60j5LygnmD", "replyto": "60j5LygnmD", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040396556, "tmdate": 1610473991829, "id": "ICLR.cc/2021/Conference/Paper1967/-/Decision"}}}, {"id": "U22Azq1o6bG", "original": null, "number": 3, "cdate": 1604186694239, "ddate": null, "tcdate": 1604186694239, "tmdate": 1606509099332, "tddate": null, "forum": "60j5LygnmD", "replyto": "60j5LygnmD", "invitation": "ICLR.cc/2021/Conference/Paper1967/-/Official_Review", "content": {"title": "Interesting findings on negative learning rates in meta learning", "review": "Summary: \n\nThis paper uses random matrix theory to study meta-learning in mixed linear regression and finds that the optimal learning rate for the inner loop/training is negative in order to minimize the test/adaption error. The results are interesting and novel. However, there are some concerns regarding the practical relevance and presentation of the results. \n\nMajor comments:\n\n1. Implementation of negative learning rates in practice: This paper provides an interesting perspective that a negative learning rate could reduce the test error. My concern is with a negative learning rate, the training loss $\\mathcal{L}^{(i)}$ in Eq. (3)  increases and the algorithm may not converge (at least on the training sets). In practice, for example, on deep learning models motivated by this paper in the first paragraph, how do you decide when to stop training parameters $\\theta^{(i)}$ and $\\omega$? How could you use the results in this paper to provide some guidance? \n2. Presentation of the main results: I would suggest the authors formally state the results in theorems or propositions. Currently,  the main results are presented in Eq. (5), (9), (10), (15), and (16), that seem to be informal and need clarification. First, how is $\\bar{\\mathcal{L}}^{test}$ defined? Second, what does $\\simeq$ mean in this context? If it means ````\" approximately equal to,\" then what is the order of the estimation error? Third, the results are derived using mean-squared loss as shown in Eq. (25) and (28) in the appendix. It is helpful to be explicit about the loss function in the main text. Fourth, the loss function does not seem to have a regularization term. In the overparameterized regime, would the model suffer from overfitting? \n3. Experiments: Could you elaborate more on why the theory matches the experiment pretty well in Figures 1.a, 2.a, and 3.a, while not the case in Figures 1.b, 2.b, and 3.b (especially Figure 3.b)? If I understand correctly, the data generating process in the simulation follows the assumption in the main results. Is it because the estimation error in $\\bar{\\mathcal{L}}^{test}$ (the terms omitted in the RHS of $\\simeq$) is not negligible in finite samples? Also, is the curve in Figures 1.b and 3.b robust to the choice of parameters? It may be helpful to include a few other simulation setups in the appendix. ", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1967/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1967/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Meta-learning with negative learning rates", "authorids": ["~Alberto_Bernacchia1"], "authors": ["Alberto Bernacchia"], "keywords": ["Meta-learning"], "abstract": "Deep learning models require a large amount of data to perform well. When data is scarce for a target task, we can transfer the knowledge gained by training on similar tasks to quickly learn the target. A successful approach is meta-learning, or \"learning to learn\" a distribution of tasks, where \"learning\" is represented by an outer loop, and \"to learn\" by an inner loop of gradient descent. However, a number of recent empirical studies argue that the inner loop is unnecessary and more simple models work equally well or even better. We study the performance of MAML as a function of the learning rate of the inner loop, where zero learning rate implies that there is no inner loop. Using random matrix theory and exact solutions of linear models, we calculate an algebraic expression for the test loss of MAML applied to mixed linear regression and nonlinear regression with overparameterized models. Surprisingly, while the optimal learning rate for adaptation is positive, we find that the optimal learning rate for training is always negative, a setting that has never been considered before. Therefore, not only does the performance increase by decreasing the learning rate to zero, as suggested by recent work, but it can be increased even further by decreasing the learning rate to negative\nvalues. These results help clarify under what circumstances meta-learning performs best.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "bernacchia|metalearning_with_negative_learning_rates", "one-sentence_summary": "We show theoretically that the optimal inner learning rate of MAML during training is always negative in a family of models  ", "supplementary_material": "/attachment/e34c29f2a8252f67626a8ad3d84f86309d577ff8.zip", "pdf": "/pdf/d806f550e5ed35f48323af6699507966218a6eb8.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nbernacchia2021metalearning,\ntitle={Meta-learning with negative learning rates},\nauthor={Alberto Bernacchia},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=60j5LygnmD}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "60j5LygnmD", "replyto": "60j5LygnmD", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1967/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538106785, "tmdate": 1606915771629, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1967/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1967/-/Official_Review"}}}, {"id": "h3wpoZbNSZ", "original": null, "number": 4, "cdate": 1604623106042, "ddate": null, "tcdate": 1604623106042, "tmdate": 1606427402735, "tddate": null, "forum": "60j5LygnmD", "replyto": "60j5LygnmD", "invitation": "ICLR.cc/2021/Conference/Paper1967/-/Official_Review", "content": {"title": "Potentially interesting but still incomplete investigation of step-sizes in meta-learning.", "review": "Summary:\nThis paper studies meta-learning in the mixed linear regression setting, focusing on the effect of the within-task step-size on performance. For over-parameterized, under-parameterized, and NTK regimes they derive expressions for test-time loss that suggest that negative or close-to-zero learning rates are optimal, and provide experiments that closely match these results. However, some aspects of the mathematical approach are unclear, and the work's impact is limited without an investigation of the consequences of the analysis.\n\nStrengths:\n1. Understanding meta-learning in simple settings and focusing on the effect of learning rate are worthwhile goals.\n2. The authors provide experimental evidence closely following the outlined theoretical results.\n\nWeaknesses:\n1. The mathematical results in the paper are difficult to follow carefully. It is still unclear to me what objective is being solved by the meta-training procedure, and various derivations in the appendix seem non-rigorous, such as replacing denominator terms by expectations. Formal statements and some proof sketches would be helpful. See also Questions 1-4.\n2. No experimental investigation beyond the limited settings studied theoretically. The authors\u2019 investigation leads to a clear prescriptive suggestion\u2014use of negative within-task step-sizes during meta-training\u2014that should be investigated on actual data to get a sense of whether the analysis extends to practical settings. This would be useful to do in both linear settings (see e.g. the experiments in Denevi et al. (2019)) and for standard meta-learning tasks. See also Questions 5-6.\n3. The analysis is limited in not being able to handle representation learning or the case of label shuffling among tasks. Motivating empirical work such as the paper of Raghu et al. (2020) suggests that representation learning is a key component of being able to meta-learn without inner loop updates, while theoretical work (e.g. Saunshi et al. (2020)) suggests that the linear setting studied here cannot account for successful meta representation learning.\n\nQuestions:\n1. The paper seems to suggest the objective ignores task structure completely, but in this case how does the within-task step-size affect meta-training at all?\n2. How many steps of MAML are used in the analysis?\n3. The line \u201cAll of the above distributions apply independently to each task and dataset (training, validation, target, test).\u201d seems to suggest that data for the same task is drawn from different distributions - is this the case and if so how it this at all justifiable in the meta-learning setting?\n4. In what references can the Gaussian moment results (36-46) be found?\n5. How closely do experiments follow the analysis in non-asymptotic settings?\n6. In the NTK results, why not investigate agreement between theory and experiments using kernel matrices obtained from actual networks rather than probability distributions on symmetric matrices?\n\nNotes:\n1. \u201ctheir performance doesn\u2019t seem to stop improving when adding more data and computing resources\u201d - why would we expect it to?\n2. \u201cA meta-learning problem is solved by a bi-level optimization procedure: an outer loop optimizes meta-parameters across tasks, while an inner loop optimizes parameters within each task (Hospedales et al. (2020)).\u201d - not all algorithms do this, c.f. Reptile (Nichol et al., 2018).\n3. \u201crecent papers argue that a simple alternative to meta-learning is just good enough, in which the inner loop is removed entirely (Chen et al. (2020a), Tian et al. (2020), Dhillon et al. (2020), Chen et al. (2020b), Raghu et al. (2020)). This is surprising, because this approach merges all source tasks into a single big training set, and it does not even distinguish between different source tasks during training.\u201d - the paper of Raghu et al. (2020) does not remove the inner loop during training and still distinguishes between source tasks, since gradient updates still use data from the same task. Actually merging all data into a single big training set was shown to perform poorly by Finn et al. (2017).\n4. \u201cIn the problem of mixed linear regression, we prove that the optimal learning rate is always negative in overparameterized models. The same result holds in underparameterized models for small values of the learning rate.\u201d - the second sentence is unclear. The learning rate is negative when it is small?\n5. \u201cnone of these studies look at the effect of the learning rate\u201d - in the convex setting, Khodak et al. (2019) connect the within-task learning rate to the task similarity.\n6. \u201cNote that each task is characterized by a different distribution of the data, and we separate the training Dt and validation data Dv.\u201d  - it is unclear whether these distributions are empirical distributions or population distributions. If they are empirical distributions then minimizing (1) is not the \u201cgoal of meta-learning\u201d but an objective used to achieve the goal. If they are population distributions, are Dt and Dv different? Why and how?\n7. What does the equality-like symbol in (5) mean? \u201cAsymptotically equal to\u201d?\n8. Numerous equation reference numbers are missing parentheses.\n9. \u201clearning of the meta-parameter \u03c9 is performed by the outer loop\u201d - still unclear which objective is being used for this. (1)?\n\nReferences:\n- Denevi, Stamos, Ciliberto, Pontil. \u201cOnline-within-online meta-learning.\u201d NeurIPS 2019.\n- Khodak, Balcan, Talwalkar. \u201cAdaptive gradient-based meta-learning methods\u201d. NeurIPS 2019.\n- Nichol, Achiam, Schulman. \u201cOn first-order meta-learning algorithms.\u201d arXiv 2018.  \n\n# Update after rebuttal phase\n\nThank you to the authors for engaging with reviewer comments. I think the paper is much clearer now, and the additional results in Figures 5 and 6 indicate that the analysis may be relevant for practical meta-learning settings. I am not sure of the necessity of the new data-generating plot for mixed linear regression in Figure 1 (my uncertainty here was resolved with words); the authors might consider using the space for putting Figure 6 (currently in the appendix) in the main paper, or for additional experiments. Two more notes:  \n1. It may make the paper easier to read if the appendix were part of the same PDF as the main paper and not in the supplementary material.\n2. While the experiments are perhaps not difficult to reproduce, code would be helpful to the community.\n\nI am increasing my rating to a 6, as I believe the paper presents an interesting result with sufficient evidence. I am not giving a higher rating as I think the paper's impact would increase substantially with experiments on actual data, either in the mixed linear or deep net setting. For future versions of the paper, I encourage the authors to consider adding such results.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1967/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1967/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Meta-learning with negative learning rates", "authorids": ["~Alberto_Bernacchia1"], "authors": ["Alberto Bernacchia"], "keywords": ["Meta-learning"], "abstract": "Deep learning models require a large amount of data to perform well. When data is scarce for a target task, we can transfer the knowledge gained by training on similar tasks to quickly learn the target. A successful approach is meta-learning, or \"learning to learn\" a distribution of tasks, where \"learning\" is represented by an outer loop, and \"to learn\" by an inner loop of gradient descent. However, a number of recent empirical studies argue that the inner loop is unnecessary and more simple models work equally well or even better. We study the performance of MAML as a function of the learning rate of the inner loop, where zero learning rate implies that there is no inner loop. Using random matrix theory and exact solutions of linear models, we calculate an algebraic expression for the test loss of MAML applied to mixed linear regression and nonlinear regression with overparameterized models. Surprisingly, while the optimal learning rate for adaptation is positive, we find that the optimal learning rate for training is always negative, a setting that has never been considered before. Therefore, not only does the performance increase by decreasing the learning rate to zero, as suggested by recent work, but it can be increased even further by decreasing the learning rate to negative\nvalues. These results help clarify under what circumstances meta-learning performs best.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "bernacchia|metalearning_with_negative_learning_rates", "one-sentence_summary": "We show theoretically that the optimal inner learning rate of MAML during training is always negative in a family of models  ", "supplementary_material": "/attachment/e34c29f2a8252f67626a8ad3d84f86309d577ff8.zip", "pdf": "/pdf/d806f550e5ed35f48323af6699507966218a6eb8.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nbernacchia2021metalearning,\ntitle={Meta-learning with negative learning rates},\nauthor={Alberto Bernacchia},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=60j5LygnmD}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "60j5LygnmD", "replyto": "60j5LygnmD", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1967/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538106785, "tmdate": 1606915771629, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1967/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1967/-/Official_Review"}}}, {"id": "3im6G7h6Qc", "original": null, "number": 12, "cdate": 1606223548371, "ddate": null, "tcdate": 1606223548371, "tmdate": 1606241828621, "tddate": null, "forum": "60j5LygnmD", "replyto": "cibHotD7hKQ", "invitation": "ICLR.cc/2021/Conference/Paper1967/-/Official_Comment", "content": {"title": "Revised version", "comment": "We have now uploaded a new version of the paper. Going back to your comment #2, we improved the clarity of Section 3 by adding a figure (Figure 1) that shows the graphical model of data generation.\nSince you mentioned the concerns by AnonReviewer3, we have also made the following major changes:\n- The mathematical results are now stated as three formal theorems, in sections 5.1, 5.2, 5.3, that include the order of the approximation error and the required assumptions. We added three lemmas in the Appendix that are instrumental to compute such error (section 7.2).\n- We added an experiment of a two-layer feed-forward neural network applied to a nonlinear (quadratic) regression problem, in section 5.4, showing that a negative learning rate indeed works better in this simple problem.\n- We show additional experiments on additional parameter sets in Figure 6 in the Appendix, showing more evidence in favor of our theory.\n  "}, "signatures": ["ICLR.cc/2021/Conference/Paper1967/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1967/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Meta-learning with negative learning rates", "authorids": ["~Alberto_Bernacchia1"], "authors": ["Alberto Bernacchia"], "keywords": ["Meta-learning"], "abstract": "Deep learning models require a large amount of data to perform well. When data is scarce for a target task, we can transfer the knowledge gained by training on similar tasks to quickly learn the target. A successful approach is meta-learning, or \"learning to learn\" a distribution of tasks, where \"learning\" is represented by an outer loop, and \"to learn\" by an inner loop of gradient descent. However, a number of recent empirical studies argue that the inner loop is unnecessary and more simple models work equally well or even better. We study the performance of MAML as a function of the learning rate of the inner loop, where zero learning rate implies that there is no inner loop. Using random matrix theory and exact solutions of linear models, we calculate an algebraic expression for the test loss of MAML applied to mixed linear regression and nonlinear regression with overparameterized models. Surprisingly, while the optimal learning rate for adaptation is positive, we find that the optimal learning rate for training is always negative, a setting that has never been considered before. Therefore, not only does the performance increase by decreasing the learning rate to zero, as suggested by recent work, but it can be increased even further by decreasing the learning rate to negative\nvalues. These results help clarify under what circumstances meta-learning performs best.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "bernacchia|metalearning_with_negative_learning_rates", "one-sentence_summary": "We show theoretically that the optimal inner learning rate of MAML during training is always negative in a family of models  ", "supplementary_material": "/attachment/e34c29f2a8252f67626a8ad3d84f86309d577ff8.zip", "pdf": "/pdf/d806f550e5ed35f48323af6699507966218a6eb8.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nbernacchia2021metalearning,\ntitle={Meta-learning with negative learning rates},\nauthor={Alberto Bernacchia},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=60j5LygnmD}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "60j5LygnmD", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1967/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1967/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1967/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1967/Authors|ICLR.cc/2021/Conference/Paper1967/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1967/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923853719, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1967/-/Official_Comment"}}}, {"id": "DU0rKdhqogH", "original": null, "number": 11, "cdate": 1606223119878, "ddate": null, "tcdate": 1606223119878, "tmdate": 1606241801667, "tddate": null, "forum": "60j5LygnmD", "replyto": "U22Azq1o6bG", "invitation": "ICLR.cc/2021/Conference/Paper1967/-/Official_Comment", "content": {"title": "Revised version", "comment": "We have now uploaded a new version of the paper that addresses your points 2 and 3:\n- The mathematical results are now stated as three formal theorems, in sections 5.1, 5.2, 5.3, that include the order of the approximation error and the required assumptions. We added three lemmas in the Appendix that are instrumental to compute such error (section 7.2).\n- We show additional experiments on additional parameter sets in Figure 6 in the Appendix, showing more evidence in favor of our theory."}, "signatures": ["ICLR.cc/2021/Conference/Paper1967/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1967/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Meta-learning with negative learning rates", "authorids": ["~Alberto_Bernacchia1"], "authors": ["Alberto Bernacchia"], "keywords": ["Meta-learning"], "abstract": "Deep learning models require a large amount of data to perform well. When data is scarce for a target task, we can transfer the knowledge gained by training on similar tasks to quickly learn the target. A successful approach is meta-learning, or \"learning to learn\" a distribution of tasks, where \"learning\" is represented by an outer loop, and \"to learn\" by an inner loop of gradient descent. However, a number of recent empirical studies argue that the inner loop is unnecessary and more simple models work equally well or even better. We study the performance of MAML as a function of the learning rate of the inner loop, where zero learning rate implies that there is no inner loop. Using random matrix theory and exact solutions of linear models, we calculate an algebraic expression for the test loss of MAML applied to mixed linear regression and nonlinear regression with overparameterized models. Surprisingly, while the optimal learning rate for adaptation is positive, we find that the optimal learning rate for training is always negative, a setting that has never been considered before. Therefore, not only does the performance increase by decreasing the learning rate to zero, as suggested by recent work, but it can be increased even further by decreasing the learning rate to negative\nvalues. These results help clarify under what circumstances meta-learning performs best.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "bernacchia|metalearning_with_negative_learning_rates", "one-sentence_summary": "We show theoretically that the optimal inner learning rate of MAML during training is always negative in a family of models  ", "supplementary_material": "/attachment/e34c29f2a8252f67626a8ad3d84f86309d577ff8.zip", "pdf": "/pdf/d806f550e5ed35f48323af6699507966218a6eb8.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nbernacchia2021metalearning,\ntitle={Meta-learning with negative learning rates},\nauthor={Alberto Bernacchia},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=60j5LygnmD}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "60j5LygnmD", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1967/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1967/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1967/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1967/Authors|ICLR.cc/2021/Conference/Paper1967/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1967/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923853719, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1967/-/Official_Comment"}}}, {"id": "RtLNxDJOaHv", "original": null, "number": 10, "cdate": 1606222696549, "ddate": null, "tcdate": 1606222696549, "tmdate": 1606241779653, "tddate": null, "forum": "60j5LygnmD", "replyto": "wZZFoEIwn7F", "invitation": "ICLR.cc/2021/Conference/Paper1967/-/Official_Comment", "content": {"title": "Revised version", "comment": "Thank you for your additional suggestions to improve our paper.\nWe have now uploaded a new version of the paper addressing both your major and your minor concerns.\n\nMajor changes:\n- The mathematical results are now stated as three formal theorems, in sections 5.1, 5.2, 5.3, that include the order of the approximation error and the required assumptions. We added three lemmas in the Appendix that are instrumental to compute such error (section 7.2).\n- We added an experiment of a two-layer feed-forward neural network applied to a nonlinear (quadratic) regression problem, in section 5.4, showing that a negative learning rate indeed works better in this simple problem.\n- We show additional experiments on additional parameter sets in Figure 6 in the Appendix, showing more evidence in favor of our theory.\n- We added a figure (Figure 1 in section 3) to explain more clearly the data generating model.\n\nAdditional minor changes:\n- We understand your point now and agree, so we removed the sentence: \"This is surprising, because this approach merges all source tasks into a single big training set, and it does not even distinguish between different source tasks during training.\"\n- In the \"Related Work\" section, we added: \"The theoretical work of Khodak et al. (2019) connects the learning rate to task similarity.\"\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1967/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1967/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Meta-learning with negative learning rates", "authorids": ["~Alberto_Bernacchia1"], "authors": ["Alberto Bernacchia"], "keywords": ["Meta-learning"], "abstract": "Deep learning models require a large amount of data to perform well. When data is scarce for a target task, we can transfer the knowledge gained by training on similar tasks to quickly learn the target. A successful approach is meta-learning, or \"learning to learn\" a distribution of tasks, where \"learning\" is represented by an outer loop, and \"to learn\" by an inner loop of gradient descent. However, a number of recent empirical studies argue that the inner loop is unnecessary and more simple models work equally well or even better. We study the performance of MAML as a function of the learning rate of the inner loop, where zero learning rate implies that there is no inner loop. Using random matrix theory and exact solutions of linear models, we calculate an algebraic expression for the test loss of MAML applied to mixed linear regression and nonlinear regression with overparameterized models. Surprisingly, while the optimal learning rate for adaptation is positive, we find that the optimal learning rate for training is always negative, a setting that has never been considered before. Therefore, not only does the performance increase by decreasing the learning rate to zero, as suggested by recent work, but it can be increased even further by decreasing the learning rate to negative\nvalues. These results help clarify under what circumstances meta-learning performs best.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "bernacchia|metalearning_with_negative_learning_rates", "one-sentence_summary": "We show theoretically that the optimal inner learning rate of MAML during training is always negative in a family of models  ", "supplementary_material": "/attachment/e34c29f2a8252f67626a8ad3d84f86309d577ff8.zip", "pdf": "/pdf/d806f550e5ed35f48323af6699507966218a6eb8.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nbernacchia2021metalearning,\ntitle={Meta-learning with negative learning rates},\nauthor={Alberto Bernacchia},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=60j5LygnmD}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "60j5LygnmD", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1967/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1967/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1967/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1967/Authors|ICLR.cc/2021/Conference/Paper1967/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1967/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923853719, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1967/-/Official_Comment"}}}, {"id": "ZyVHL24PrT9", "original": null, "number": 2, "cdate": 1603915649388, "ddate": null, "tcdate": 1603915649388, "tmdate": 1606103007945, "tddate": null, "forum": "60j5LygnmD", "replyto": "60j5LygnmD", "invitation": "ICLR.cc/2021/Conference/Paper1967/-/Official_Review", "content": {"title": "Official Review", "review": "### Summary\nThe authors of this paper prove that the optimal learning rate for MAML is negative under mixed linear regression and nonlinear regression with overparametrized models. They verify that theoretic bounds align with numerical experiments.\n\n### Comments\n* As the authors note, \"theoretical work is still lagging behind,\" so theoretical research to explain advances in meta-learning is useful.\n* I think the clarity could be improved, especially in section 3. It was a bit difficult to understand notation and the exact setting. For instance, I think it'd be preferable to use a different Greek letter for the learning rates, rather than $\\alpha_t$ and $\\alpha_r$. \n* I performed a surface level check of the proofs, and the results look correct to me.\n* It would be nice to discuss more the practical implications of the results or whether there still exists a divide between theory and practice.\n\n### Recommendation / Justification\nI vote that the paper is marginally above the acceptance threshold. I think the results presented are interesting, but it is not clear to me what the implications are. It would be nice if the authors explored whether the setting they analyze aligns with more commonly used meta-learning tasks.\n\n### Questions\n* In the last paragraph of Section 3, are you referring to the learning rates at meta-training and meta-testing time?\n\n### Minor feedback \n* not only the performance -> not only [does] the performance\n* These results help clarifying -> These results help clarify\n* In general, I think it is better to reference equations as Equation 13 rather than 13.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1967/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1967/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Meta-learning with negative learning rates", "authorids": ["~Alberto_Bernacchia1"], "authors": ["Alberto Bernacchia"], "keywords": ["Meta-learning"], "abstract": "Deep learning models require a large amount of data to perform well. When data is scarce for a target task, we can transfer the knowledge gained by training on similar tasks to quickly learn the target. A successful approach is meta-learning, or \"learning to learn\" a distribution of tasks, where \"learning\" is represented by an outer loop, and \"to learn\" by an inner loop of gradient descent. However, a number of recent empirical studies argue that the inner loop is unnecessary and more simple models work equally well or even better. We study the performance of MAML as a function of the learning rate of the inner loop, where zero learning rate implies that there is no inner loop. Using random matrix theory and exact solutions of linear models, we calculate an algebraic expression for the test loss of MAML applied to mixed linear regression and nonlinear regression with overparameterized models. Surprisingly, while the optimal learning rate for adaptation is positive, we find that the optimal learning rate for training is always negative, a setting that has never been considered before. Therefore, not only does the performance increase by decreasing the learning rate to zero, as suggested by recent work, but it can be increased even further by decreasing the learning rate to negative\nvalues. These results help clarify under what circumstances meta-learning performs best.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "bernacchia|metalearning_with_negative_learning_rates", "one-sentence_summary": "We show theoretically that the optimal inner learning rate of MAML during training is always negative in a family of models  ", "supplementary_material": "/attachment/e34c29f2a8252f67626a8ad3d84f86309d577ff8.zip", "pdf": "/pdf/d806f550e5ed35f48323af6699507966218a6eb8.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nbernacchia2021metalearning,\ntitle={Meta-learning with negative learning rates},\nauthor={Alberto Bernacchia},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=60j5LygnmD}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "60j5LygnmD", "replyto": "60j5LygnmD", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1967/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538106785, "tmdate": 1606915771629, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1967/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1967/-/Official_Review"}}}, {"id": "cibHotD7hKQ", "original": null, "number": 9, "cdate": 1606102964216, "ddate": null, "tcdate": 1606102964216, "tmdate": 1606102964216, "tddate": null, "forum": "60j5LygnmD", "replyto": "gjgtfHnVYf6", "invitation": "ICLR.cc/2021/Conference/Paper1967/-/Official_Comment", "content": {"title": "Comments on response", "comment": "Thank you for the response. I have read through the other reviews and responses.\n\n> We made a few changes to the text to make it more clear. For example, we state explicitly which equations correspond to the outer and to the inner loop of meta-learning, we explain more clearly how the training and validation data are drawn, and we specify that we only consider one step of gradient descent in the inner loop. We have not changed notation as we are running out of symbols and we could not agree on which symbols to use.\n\nThanks, the new version reads a bit more clearly.\n\n> We are running additional experiments on non-linear regression, we hope that we will obtain some results before November 24th.\n\nGreat, I think the addition will strengthen the paper.\n\nAnonReviewer3 raises some valid, unresolved concerns, which I hope you will address before the end of the rebuttal period."}, "signatures": ["ICLR.cc/2021/Conference/Paper1967/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1967/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Meta-learning with negative learning rates", "authorids": ["~Alberto_Bernacchia1"], "authors": ["Alberto Bernacchia"], "keywords": ["Meta-learning"], "abstract": "Deep learning models require a large amount of data to perform well. When data is scarce for a target task, we can transfer the knowledge gained by training on similar tasks to quickly learn the target. A successful approach is meta-learning, or \"learning to learn\" a distribution of tasks, where \"learning\" is represented by an outer loop, and \"to learn\" by an inner loop of gradient descent. However, a number of recent empirical studies argue that the inner loop is unnecessary and more simple models work equally well or even better. We study the performance of MAML as a function of the learning rate of the inner loop, where zero learning rate implies that there is no inner loop. Using random matrix theory and exact solutions of linear models, we calculate an algebraic expression for the test loss of MAML applied to mixed linear regression and nonlinear regression with overparameterized models. Surprisingly, while the optimal learning rate for adaptation is positive, we find that the optimal learning rate for training is always negative, a setting that has never been considered before. Therefore, not only does the performance increase by decreasing the learning rate to zero, as suggested by recent work, but it can be increased even further by decreasing the learning rate to negative\nvalues. These results help clarify under what circumstances meta-learning performs best.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "bernacchia|metalearning_with_negative_learning_rates", "one-sentence_summary": "We show theoretically that the optimal inner learning rate of MAML during training is always negative in a family of models  ", "supplementary_material": "/attachment/e34c29f2a8252f67626a8ad3d84f86309d577ff8.zip", "pdf": "/pdf/d806f550e5ed35f48323af6699507966218a6eb8.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nbernacchia2021metalearning,\ntitle={Meta-learning with negative learning rates},\nauthor={Alberto Bernacchia},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=60j5LygnmD}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "60j5LygnmD", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1967/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1967/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1967/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1967/Authors|ICLR.cc/2021/Conference/Paper1967/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1967/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923853719, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1967/-/Official_Comment"}}}, {"id": "wZZFoEIwn7F", "original": null, "number": 8, "cdate": 1605805417760, "ddate": null, "tcdate": 1605805417760, "tmdate": 1605805574538, "tddate": null, "forum": "60j5LygnmD", "replyto": "4Is3BiVN0DS", "invitation": "ICLR.cc/2021/Conference/Paper1967/-/Official_Comment", "content": {"title": "Reviewer follow-up", "comment": "Thank you for the detailed response; I look forward to reading the revision and updating my evaluation accordingly. Below I\u2019ve followed up on a few of your responses to my original questions/comments.  \n\nWeaknesses:  \n3.1 - On the question of label shuffling in point 3, note that in the regression setting one can effectively have label shuffling by flipping signs of the target variables.  \n3.2 - On the question of representation learning in point 3, I believe representation learning should be understood as a nontrivial representation of the data that is then passed to a linear predictor, whereas here there is only a linear predictor without any nontrivial representation. Note that I do not believe this issue to be a flaw of the work, just a limitation.  \n\nQuestions:  \n1 - My question here referred to the motivating line \u201cthis approach merges all source tasks into a single big training set, and it does not even distinguish between different source tasks during training\u201d from the introduction; it does not seem accurate to me that the cited approaches do not distinguish between source tasks, since as far as I know gradients are still obtained by sampling data points from the same task.  \n4 - I think this is important detail that should be included and relevant theorems should be cited.  \n\nNotes:  \n2 - I agree that Reptile has an inner loop, but I am not sure it is accurate to say it uses bilevel optimization since there is no separation of within-task data into training and validation sets.  \n3 - As also noted in Question 1 above, I think my main concern here is with the \u201cdoes not even distinguish between different source tasks during training\u201d statement, which I do not believe is accurate.  \n5 - As I noted in my original comment, they connect the task similarity to the within-task learning rate, which is the inner loop learning rate you focus on. I believe the outer loop learning rate is discussed separately.  \n8 - Note that some readers still read on paper, not PDF. Adding Eq. also works."}, "signatures": ["ICLR.cc/2021/Conference/Paper1967/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1967/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Meta-learning with negative learning rates", "authorids": ["~Alberto_Bernacchia1"], "authors": ["Alberto Bernacchia"], "keywords": ["Meta-learning"], "abstract": "Deep learning models require a large amount of data to perform well. When data is scarce for a target task, we can transfer the knowledge gained by training on similar tasks to quickly learn the target. A successful approach is meta-learning, or \"learning to learn\" a distribution of tasks, where \"learning\" is represented by an outer loop, and \"to learn\" by an inner loop of gradient descent. However, a number of recent empirical studies argue that the inner loop is unnecessary and more simple models work equally well or even better. We study the performance of MAML as a function of the learning rate of the inner loop, where zero learning rate implies that there is no inner loop. Using random matrix theory and exact solutions of linear models, we calculate an algebraic expression for the test loss of MAML applied to mixed linear regression and nonlinear regression with overparameterized models. Surprisingly, while the optimal learning rate for adaptation is positive, we find that the optimal learning rate for training is always negative, a setting that has never been considered before. Therefore, not only does the performance increase by decreasing the learning rate to zero, as suggested by recent work, but it can be increased even further by decreasing the learning rate to negative\nvalues. These results help clarify under what circumstances meta-learning performs best.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "bernacchia|metalearning_with_negative_learning_rates", "one-sentence_summary": "We show theoretically that the optimal inner learning rate of MAML during training is always negative in a family of models  ", "supplementary_material": "/attachment/e34c29f2a8252f67626a8ad3d84f86309d577ff8.zip", "pdf": "/pdf/d806f550e5ed35f48323af6699507966218a6eb8.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nbernacchia2021metalearning,\ntitle={Meta-learning with negative learning rates},\nauthor={Alberto Bernacchia},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=60j5LygnmD}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "60j5LygnmD", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1967/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1967/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1967/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1967/Authors|ICLR.cc/2021/Conference/Paper1967/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1967/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923853719, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1967/-/Official_Comment"}}}, {"id": "I5pFqOs522x", "original": null, "number": 7, "cdate": 1605700308332, "ddate": null, "tcdate": 1605700308332, "tmdate": 1605700308332, "tddate": null, "forum": "60j5LygnmD", "replyto": "gymjZGCgL--", "invitation": "ICLR.cc/2021/Conference/Paper1967/-/Official_Comment", "content": {"title": "Answer to Reviewer #2", "comment": "We thank the reviewer for recognizing the value of our work.\nWe will include the references to Meta-SGD and LEO in the revised version of the paper.\nWe are also running additional experiments on non-linear regression, we hope that we will obtain some results before November 24th.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1967/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1967/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Meta-learning with negative learning rates", "authorids": ["~Alberto_Bernacchia1"], "authors": ["Alberto Bernacchia"], "keywords": ["Meta-learning"], "abstract": "Deep learning models require a large amount of data to perform well. When data is scarce for a target task, we can transfer the knowledge gained by training on similar tasks to quickly learn the target. A successful approach is meta-learning, or \"learning to learn\" a distribution of tasks, where \"learning\" is represented by an outer loop, and \"to learn\" by an inner loop of gradient descent. However, a number of recent empirical studies argue that the inner loop is unnecessary and more simple models work equally well or even better. We study the performance of MAML as a function of the learning rate of the inner loop, where zero learning rate implies that there is no inner loop. Using random matrix theory and exact solutions of linear models, we calculate an algebraic expression for the test loss of MAML applied to mixed linear regression and nonlinear regression with overparameterized models. Surprisingly, while the optimal learning rate for adaptation is positive, we find that the optimal learning rate for training is always negative, a setting that has never been considered before. Therefore, not only does the performance increase by decreasing the learning rate to zero, as suggested by recent work, but it can be increased even further by decreasing the learning rate to negative\nvalues. These results help clarify under what circumstances meta-learning performs best.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "bernacchia|metalearning_with_negative_learning_rates", "one-sentence_summary": "We show theoretically that the optimal inner learning rate of MAML during training is always negative in a family of models  ", "supplementary_material": "/attachment/e34c29f2a8252f67626a8ad3d84f86309d577ff8.zip", "pdf": "/pdf/d806f550e5ed35f48323af6699507966218a6eb8.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nbernacchia2021metalearning,\ntitle={Meta-learning with negative learning rates},\nauthor={Alberto Bernacchia},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=60j5LygnmD}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "60j5LygnmD", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1967/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1967/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1967/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1967/Authors|ICLR.cc/2021/Conference/Paper1967/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1967/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923853719, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1967/-/Official_Comment"}}}, {"id": "gjgtfHnVYf6", "original": null, "number": 6, "cdate": 1605635410638, "ddate": null, "tcdate": 1605635410638, "tmdate": 1605635410638, "tddate": null, "forum": "60j5LygnmD", "replyto": "ZyVHL24PrT9", "invitation": "ICLR.cc/2021/Conference/Paper1967/-/Official_Comment", "content": {"title": "Answer to Reviewer #1", "comment": "We thank the reviewer for recognizing that the \"theoretical research to explain advances in meta-learning is useful\".\nHere we answer the reviewer's comments and questions point by point and we highlight the changes to be made (revised version to be uploaded soon).\n\nComments\n- Thank you, we agree that theoretical research is useful.\n- We made a few changes to the text to make it more clear. For example, we state explicitly which equations correspond to the outer and to the inner loop of meta-learning, we explain more clearly how the training and validation data are drawn, and we specify that we only consider one step of gradient descent in the inner loop. We have not changed notation as we are running out of symbols and we could not agree on which symbols to use.\n- We also believe that proofs are correct, thank you. In the revised version of the paper we will present all mathematical results stated as formal theorems, as requested by other reviewers.\n- We are running additional experiments on non-linear regression, we hope that we will obtain some results before November 24th.\n\nQuestions\n- Yes. We realize that \"training\" and \"testing\" is confusing, so we changed the text by adding the \"meta-\" prefix to both.\n\nMinor Feedback\n- Done\n- Done\n- Done\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1967/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1967/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Meta-learning with negative learning rates", "authorids": ["~Alberto_Bernacchia1"], "authors": ["Alberto Bernacchia"], "keywords": ["Meta-learning"], "abstract": "Deep learning models require a large amount of data to perform well. When data is scarce for a target task, we can transfer the knowledge gained by training on similar tasks to quickly learn the target. A successful approach is meta-learning, or \"learning to learn\" a distribution of tasks, where \"learning\" is represented by an outer loop, and \"to learn\" by an inner loop of gradient descent. However, a number of recent empirical studies argue that the inner loop is unnecessary and more simple models work equally well or even better. We study the performance of MAML as a function of the learning rate of the inner loop, where zero learning rate implies that there is no inner loop. Using random matrix theory and exact solutions of linear models, we calculate an algebraic expression for the test loss of MAML applied to mixed linear regression and nonlinear regression with overparameterized models. Surprisingly, while the optimal learning rate for adaptation is positive, we find that the optimal learning rate for training is always negative, a setting that has never been considered before. Therefore, not only does the performance increase by decreasing the learning rate to zero, as suggested by recent work, but it can be increased even further by decreasing the learning rate to negative\nvalues. These results help clarify under what circumstances meta-learning performs best.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "bernacchia|metalearning_with_negative_learning_rates", "one-sentence_summary": "We show theoretically that the optimal inner learning rate of MAML during training is always negative in a family of models  ", "supplementary_material": "/attachment/e34c29f2a8252f67626a8ad3d84f86309d577ff8.zip", "pdf": "/pdf/d806f550e5ed35f48323af6699507966218a6eb8.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nbernacchia2021metalearning,\ntitle={Meta-learning with negative learning rates},\nauthor={Alberto Bernacchia},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=60j5LygnmD}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "60j5LygnmD", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1967/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1967/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1967/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1967/Authors|ICLR.cc/2021/Conference/Paper1967/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1967/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923853719, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1967/-/Official_Comment"}}}, {"id": "MHVrV9tBzVp", "original": null, "number": 5, "cdate": 1605635253828, "ddate": null, "tcdate": 1605635253828, "tmdate": 1605635277478, "tddate": null, "forum": "60j5LygnmD", "replyto": "U22Azq1o6bG", "invitation": "ICLR.cc/2021/Conference/Paper1967/-/Official_Comment", "content": {"title": "Answer to Reviewer #4", "comment": "We thank the reviewer for recognizing that the \"results are interesting and novel\".\nHere we answer the reviewer's comments point by point and we highlight the changes to be made (revised version to be uploaded soon).\nWe hope that the reviewer will consider the quality of the paper to have increased.\n\n1. The training loss in Eq.(2) does not increase, our procedure finds its unique minimum with respect to the meta-parameter $\\omega$. The negative learning rate is in the inner loop, Eq.(3). Since only one step of gradient descent is taken in the inner loop, there is no problem of convergence.\n\n2. In the revised version of the paper, before Novermber 24th, we will include all mathematical results stated as formal theorems. The average test loss is defined by Eq.32. We will provide the order of the estimation error in the revised paper. Overfitting in our model is expressed by the term $\\left|\\boldsymbol\\omega_0-\\mathbf{w}_0\\right|^2$ in Eq.5.\n\n3. The reviewer is correct, the error is due to higher orders. We will provide figures with results of theory/experiment on additional parameter sets in the appendix. Our theory holds in the asymptotic limit, either for extremely overparameterized or extremely underparameterized problems.\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1967/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1967/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Meta-learning with negative learning rates", "authorids": ["~Alberto_Bernacchia1"], "authors": ["Alberto Bernacchia"], "keywords": ["Meta-learning"], "abstract": "Deep learning models require a large amount of data to perform well. When data is scarce for a target task, we can transfer the knowledge gained by training on similar tasks to quickly learn the target. A successful approach is meta-learning, or \"learning to learn\" a distribution of tasks, where \"learning\" is represented by an outer loop, and \"to learn\" by an inner loop of gradient descent. However, a number of recent empirical studies argue that the inner loop is unnecessary and more simple models work equally well or even better. We study the performance of MAML as a function of the learning rate of the inner loop, where zero learning rate implies that there is no inner loop. Using random matrix theory and exact solutions of linear models, we calculate an algebraic expression for the test loss of MAML applied to mixed linear regression and nonlinear regression with overparameterized models. Surprisingly, while the optimal learning rate for adaptation is positive, we find that the optimal learning rate for training is always negative, a setting that has never been considered before. Therefore, not only does the performance increase by decreasing the learning rate to zero, as suggested by recent work, but it can be increased even further by decreasing the learning rate to negative\nvalues. These results help clarify under what circumstances meta-learning performs best.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "bernacchia|metalearning_with_negative_learning_rates", "one-sentence_summary": "We show theoretically that the optimal inner learning rate of MAML during training is always negative in a family of models  ", "supplementary_material": "/attachment/e34c29f2a8252f67626a8ad3d84f86309d577ff8.zip", "pdf": "/pdf/d806f550e5ed35f48323af6699507966218a6eb8.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nbernacchia2021metalearning,\ntitle={Meta-learning with negative learning rates},\nauthor={Alberto Bernacchia},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=60j5LygnmD}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "60j5LygnmD", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1967/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1967/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1967/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1967/Authors|ICLR.cc/2021/Conference/Paper1967/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1967/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923853719, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1967/-/Official_Comment"}}}, {"id": "4Is3BiVN0DS", "original": null, "number": 4, "cdate": 1605634576309, "ddate": null, "tcdate": 1605634576309, "tmdate": 1605634576309, "tddate": null, "forum": "60j5LygnmD", "replyto": "h3wpoZbNSZ", "invitation": "ICLR.cc/2021/Conference/Paper1967/-/Official_Comment", "content": {"title": "Answer to Reviewer #3", "comment": "We thank the reviewer for recognizing that \"meta-learning in simple settings is a worthwhile goal\", and for giving us several suggestions to improve the paper.\nHere we answer the reviewer's comments point by point and we highlight the changes to be made (revised version to be uploaded soon).\nWe hope that the reviewer will consider the quality of the paper to have increased.\n\nWeaknesses:\n1. In the revised version of the paper, before Novermber 24th, we will include all mathematical results stated as formal theorems.\n2. We are running additional experiments on non-linear regression, we hope that we will obtain some results before November 24th.\n3. By \"label shuffling\", we believe that the reviewer is referring to classification problems, where the set of classes are different across tasks. However, in this work we look at a regression problem only. In the context of regression, the work of Saunshi et al. (2020) is particularly interesting as they consider two-layer neural networks, where the output of the first layer is interpreted as the good representation. However, they study a different problem that has other limitations (e.g. they have only two tasks), and we believe that our study is also relevant for the general problem of \"representation learning\", since the general problem of finding a good meta-parameter across training tasks means finding a good representation for the meta-learning problem. \n\nQuestions:\n1. We are not sure what the reviewer is referring to here. Our objective in Eq.1 is a standard meta-learning objective, see e.g. Eqs. 5 and 6 in Hospedales et al. (2020).\n2. We use one step of MAML in all theory and experiment. We now state this explicitly in Section 3, before Eq.3: \"In this work we consider the simple case of a single gradient step\"\n3. This is our mistake. We now say: \"The generating parameter w is drawn once and kept fixed within a task, and drawn independently for different tasks.\"\n4. We could not find a reference with Gaussian moments up to all orders, so we calculated them ourselves using Isserlis Theorem and combinatorics.\n5. We will provide figures with results of theory/experiment on additional parameter sets in the appendix, showing examples in non-asymptotic setting.\n6. This is a good point, but unfortunately we are not able to do it in one week and it will the focus of our future work.\n\nNotes:\n1. One usually expects performance to improve with data and compute (except in pathological cases, see e.g. Nakkiran 2019), but the question is whether it improves significantly. Today everyone assumes that deep learning models keep improving significantly with data and compute, but this was not obvious 10 years ago.\n2. While we do not argue that all meta-learning algorithms have the inner/outer structure, we refer to Hospedales et al. (2020) for a definition of the meta-learning problem (as stated in the text). Furthermore, we believe that Reptile has also the inner/outer structure: Algorithm 1 of the Reptile paper shows the outer loop, and the inner loop is hidden in line 4 of the algorithm (\"Compute $\\tilde{\\phi}$ with $k$ steps of SGD or Adam.\")\n3. In the paper by Raghu et al. (2020), most of the experiments are on the ANIL model (Almost No Inner Loop, in which the inner loop runs only on the head of the network). However, some experiments are on the NIL model (No Inner Loop, in which the inner loop is removed entirely, by removing the head of the network during testing). They found that NIL performs as good or even better than ANIL.\n4. Here we mean that, when the optimal learning rate is small in magnitude, then it has to be negative, because the average test loss has a positive derivative at zero (see Eq.11). We modified the text now to: \"The same result holds in underparameterized models provided that the optimal learning rate is small in absolute value.\"\n5. We added the reference of Khodak et al. (2020) to our \"Related Work\" section. However, note that they study the learning rate of the outer loop (which we believe should never be negative), while we focus on the inner loop learning rate only.\n6. This is our mistake, Dt and Dv are population distribution. We now say: \"Within each task, the training Dt and validation data Dv are drawn independently from the same distribution.\n7. Yes. We acknowledge that this notation is not very clear, so we will use a formal notation in the revised version.\n8. In the entire paper, we don't use parentheses to reference equations. We believe that this notation is not ambiguous, because all equations are hyper-referenced, and can be located by a simple click on the pdf file. However, we now refer to every equation by \"Eq.\"\n9. Yes, we added a reference to the equation to make it clear.\n\nReferences:\nNakkiran (2019) \"More Data Can Hurt for Linear Regression: Sample-wise Double Descent.\" https://arxiv.org/abs/1912.07242 \nHospedales et al (2020) \"Meta-Learning in Neural Networks: A Survey.\" https://arxiv.org/abs/2004.05439"}, "signatures": ["ICLR.cc/2021/Conference/Paper1967/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1967/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Meta-learning with negative learning rates", "authorids": ["~Alberto_Bernacchia1"], "authors": ["Alberto Bernacchia"], "keywords": ["Meta-learning"], "abstract": "Deep learning models require a large amount of data to perform well. When data is scarce for a target task, we can transfer the knowledge gained by training on similar tasks to quickly learn the target. A successful approach is meta-learning, or \"learning to learn\" a distribution of tasks, where \"learning\" is represented by an outer loop, and \"to learn\" by an inner loop of gradient descent. However, a number of recent empirical studies argue that the inner loop is unnecessary and more simple models work equally well or even better. We study the performance of MAML as a function of the learning rate of the inner loop, where zero learning rate implies that there is no inner loop. Using random matrix theory and exact solutions of linear models, we calculate an algebraic expression for the test loss of MAML applied to mixed linear regression and nonlinear regression with overparameterized models. Surprisingly, while the optimal learning rate for adaptation is positive, we find that the optimal learning rate for training is always negative, a setting that has never been considered before. Therefore, not only does the performance increase by decreasing the learning rate to zero, as suggested by recent work, but it can be increased even further by decreasing the learning rate to negative\nvalues. These results help clarify under what circumstances meta-learning performs best.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "bernacchia|metalearning_with_negative_learning_rates", "one-sentence_summary": "We show theoretically that the optimal inner learning rate of MAML during training is always negative in a family of models  ", "supplementary_material": "/attachment/e34c29f2a8252f67626a8ad3d84f86309d577ff8.zip", "pdf": "/pdf/d806f550e5ed35f48323af6699507966218a6eb8.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nbernacchia2021metalearning,\ntitle={Meta-learning with negative learning rates},\nauthor={Alberto Bernacchia},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=60j5LygnmD}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "60j5LygnmD", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1967/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1967/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1967/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1967/Authors|ICLR.cc/2021/Conference/Paper1967/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1967/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923853719, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1967/-/Official_Comment"}}}, {"id": "gymjZGCgL--", "original": null, "number": 1, "cdate": 1603312208527, "ddate": null, "tcdate": 1603312208527, "tmdate": 1605024317258, "tddate": null, "forum": "60j5LygnmD", "replyto": "60j5LygnmD", "invitation": "ICLR.cc/2021/Conference/Paper1967/-/Official_Review", "content": {"title": "Important theoretical study of MAML with counterintuitive findings, sheds light on empirical observations", "review": "Summary:\nThis paper gives much needed attention to the theoretical underpinnings of modern meta-learning algorithms such as MAML; it introduces a novel formal argument, discovers surprising implications and follows through to show that such predictions hold experimentally, despite being counterintuitive.\n\n\nStrong points: \n- Novel analysis of a practically important aspect of MAML-like meta-learning algorithms: setting the learning rates for training and adaptation; surprising theoretical result (of negative learning rates being optimal in some cases during meta-training) is well evaluated in controlled conditions.\n- Honest discussion of limitations and good intuition is provided for applicability of the work. This is not too hard to do, but so many papers don\u2019t provide it. Great job!\n- Writing is clear enough, although the paper is dense.\n- The authors don\u2019t discuss previous empirical works (e.g. Meta-SGD/LEO) where meta-learning of the learning rate leads to negative inner-loop learning rates for some parameters, but such experiments actually provide further evidence to back up their claims, this time in SOTA deep models.\n\n\nWeak points:\n- Unfortunately, the paper includes only toy-task experiments, even by the standards of  meta-learning research.\n\n\nRecommendation and Rationale:\nI strongly support acceptance because this paper contains much needed fundamental work on theoretical underpinnings of modern meta-learning.\n", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1967/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1967/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Meta-learning with negative learning rates", "authorids": ["~Alberto_Bernacchia1"], "authors": ["Alberto Bernacchia"], "keywords": ["Meta-learning"], "abstract": "Deep learning models require a large amount of data to perform well. When data is scarce for a target task, we can transfer the knowledge gained by training on similar tasks to quickly learn the target. A successful approach is meta-learning, or \"learning to learn\" a distribution of tasks, where \"learning\" is represented by an outer loop, and \"to learn\" by an inner loop of gradient descent. However, a number of recent empirical studies argue that the inner loop is unnecessary and more simple models work equally well or even better. We study the performance of MAML as a function of the learning rate of the inner loop, where zero learning rate implies that there is no inner loop. Using random matrix theory and exact solutions of linear models, we calculate an algebraic expression for the test loss of MAML applied to mixed linear regression and nonlinear regression with overparameterized models. Surprisingly, while the optimal learning rate for adaptation is positive, we find that the optimal learning rate for training is always negative, a setting that has never been considered before. Therefore, not only does the performance increase by decreasing the learning rate to zero, as suggested by recent work, but it can be increased even further by decreasing the learning rate to negative\nvalues. These results help clarify under what circumstances meta-learning performs best.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "bernacchia|metalearning_with_negative_learning_rates", "one-sentence_summary": "We show theoretically that the optimal inner learning rate of MAML during training is always negative in a family of models  ", "supplementary_material": "/attachment/e34c29f2a8252f67626a8ad3d84f86309d577ff8.zip", "pdf": "/pdf/d806f550e5ed35f48323af6699507966218a6eb8.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nbernacchia2021metalearning,\ntitle={Meta-learning with negative learning rates},\nauthor={Alberto Bernacchia},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=60j5LygnmD}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "60j5LygnmD", "replyto": "60j5LygnmD", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1967/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538106785, "tmdate": 1606915771629, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1967/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1967/-/Official_Review"}}}], "count": 15}