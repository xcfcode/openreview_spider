{"notes": [{"ddate": null, "legacy_migration": true, "tmdate": 1392710760000, "tcdate": 1392710760000, "number": 1, "id": "XnKCX8DECzXS1", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "gg4nKrblw0gkf", "replyto": "wwlZDuMrHTDr6", "signatures": ["KyungHyun Cho"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "Dear Reviewer (16f7),\r\n\r\nWe thank you for the thorough and insightful comment. Allow us to respond to some of your comments below.\r\n\r\n'For models where AIS was usable (the DBN, the DBM and the RBM), the CSL results wildly differ from the AIS ones.'\r\n\r\nThe proposed CSL estimator does not only reflect the model's generative capability but also the MCMC sampler used to collect samples of the latent variables. We believe the higher variance (or more slowly converging) CSL estimates, compared to AIS, are due to the inefficiency, or poor mixing, of Gibbs sampling in well-trained RBMs. However, keep in mind that the use case (and motivation) for CSL was to estimate the likelihood of GSNs, for which AIS is not available and where mixing tends to be much better.\r\n\r\nAs we have already stated in our responses to the other reviewers' comments, the proposed CSL estimator seems to be the only one that can be used for models that have no explicit formula for computing the probability (either normalized or unnormalized). However, we agree that improving the variance of CSL is an important objective for future work and we are studying options.\r\n\r\n'Another caveat, unfortunately extremely difficult to avoid, is that the effectiveness of these methods can only be empirically proven on tiny models where mixing problems do not occur'\r\n\r\nWe agree, and this is a problem in general with any estimator.  For instance, even with AIS, any empirical evidence that compares it with the true value can be only given for tiny models.  However, we believe this does not and should not discourage research and development of new estimators, especially, considering that some generative models such as GSNs do not have any better alternative at the moment.\r\n\r\n'Binary MNIST is a very particular dataset and this experiment does not convince me that it is actually usable to compare models, especially of different types'\r\n\r\nWe agree with you that more experiments with different types of data may support our claim better. In a next version of the paper, the proposed estimators should be tested on other datasets. We have started experiments on the TFD dataset and will be able to add these results to the paper before the conference.\r\n\r\n'The propositions are not worth publication by themselves.'\r\n\r\nWe agree that the math in this paper is very simple. However, please consider the following contributions:\r\n\r\n(1) We improve over a previously available likelihood estimator (Breuleux et al) for models such as GSNs\r\n  - by sampling over h rather than over x, making CSL more efficient because each h can cover many x's in a way that should be better than a poorly informed kernel density (e.g. centered on a sampled x)\r\n  - by not requiring a bandwidth hyper-parameter to be tuned (just for the purpose of estimating the likelihood)\r\n(2) We study experimentally the properties of this estimator and compare it to exact and AIS estimates.\r\n(3) We introduce a biased variant and experiments find it to order models well. \r\n\r\n'CSL is only a lower bound on the true log probability of the data in expectation. This should be made clearer in the paper.'\r\n\r\nIndeed. We will make the text more clear in the revision.\r\n\r\n'The pseudo code ... is only useful to people who already understood the algorithm.'\r\n\r\nWe do not understand what you mean when writing that it is only useful to those who already understood the algorithm. We would appreciate if you could further explain the problem with the presented algorithm.  We will then make changes accordingly.\r\n\r\n'Could you give more details on the parameters for AIS? How many chains? How many intermediate temperatures? How does the computation time compare to CSL?'\r\n\r\nWe used 100 independent AIS runs with 30,000 chains each. The chains were unevenly distributed between the inverse temperature 0 (independent variables) and 1 (true model distribution) such that there were 10k chains between 0 and 0.5, another 10k chains between 0.5 and 0.9, and the remaining 10k chains between 0.9 and 1.  Hence, the computation required for AIS is roughly equivalent to computing the CSL estimates with 1.5 million samples, considering that the CSL need to compute the conditional probability for a test sample.  For instance, the time taken by the AIS estimator is somewhere between those taken by the CSL with 10k and 50k samples in Table 1."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Bounding the Test Log-Likelihood of Generative Models", "decision": "submitted, no decision", "abstract": "Several interesting generative learning algorithms involve a complex probability distribution over many random variables, involving intractable normalization constants or latent variable normalization. Some of them may even not have an analytic expression for the unnormalized probability function and no tractable approximation. This makes it difficult to estimate the quality of these models, once they have been trained, or to monitor their quality (e.g. for early stopping) while training. A previously proposed method is based on constructing a non-parametric density estimator of the model's probability function from samples generated by the model. We revisit this idea, propose a more efficient estimator, and prove that it provides a lower bound on the true test log-likelihood, and an unbiased estimator as the number of generated samples goes to infinity, although one that incorporates the effect of poor mixing (making the estimated likelihood worse, i.e., more conservative).", "pdf": "https://arxiv.org/abs/1311.6184", "paperhash": "bengio|bounding_the_test_loglikelihood_of_generative_models", "keywords": [], "conflicts": [], "authors": ["Yoshua Bengio", "Li Yao", "KyungHyun Cho"], "authorids": ["yoshua.bengio@gmail.com", "yaoli.email@gmail.com", "kyunghyun.cho@aalto.fi"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392141360000, "tcdate": 1392141360000, "number": 4, "id": "wwlZDuMrHTDr6", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "gg4nKrblw0gkf", "replyto": "gg4nKrblw0gkf", "signatures": ["anonymous reviewer 16f7"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Bounding the Test Log-Likelihood of Generative Models", "review": "In this paper, the authors propose a new way to estimate the probability of data under a probabilistic model from which sampling is hard but for which an efficient Markov chain procedure exists. They first present an asymptotically unbiased estimator, then a more efficient biased estimator.\r\n\r\nThe idea is undeniably interesting. Some of the most used generative models satisfy these constraints and being able to calculate the probability of data under these models is crucial to comparing them. However, the results presented in this paper are underwhelming. For models where AIS was usable (the DBN, the DBM and the RBM), the CSL results wildly differ from the AIS ones. Since the results on the small RBM (Table 2) give a clear advantage to AIS, I am inclined to believe these results more.\r\n\r\nAnother caveat, unfortunately extremely difficult to avoid, is that the effectiveness of these methods can only be empirically proven on tiny models where mixing problems do not occur. I really do not blame the authors for that but this really limits the potential impact of the method.\r\n\r\nExperiment in Figure 1 is also very light to conclude on the effectiveness of Biased CSL. Binary MNIST is a very particular dataset and this experiment does not convince me that it is actually usable to compare models, especially of different types.\r\n\r\nConclusion: this paper does not prove the effectiveness of the proposed method. The propositions are not worth publication by themselves.\r\n\r\nOther comments:\r\n- CSL is only a lower bound on the true log probability of the data in expectation. This should be made clearer in the paper.\r\n- The pseudo code should either be commented or removed entirely. As it is, it is only useful to people who already understood the algorithm.\r\n- Could you give more details on the parameters for AIS? How many chains? How many intermediate temperatures? How does the computation time compare to CSL?"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Bounding the Test Log-Likelihood of Generative Models", "decision": "submitted, no decision", "abstract": "Several interesting generative learning algorithms involve a complex probability distribution over many random variables, involving intractable normalization constants or latent variable normalization. Some of them may even not have an analytic expression for the unnormalized probability function and no tractable approximation. This makes it difficult to estimate the quality of these models, once they have been trained, or to monitor their quality (e.g. for early stopping) while training. A previously proposed method is based on constructing a non-parametric density estimator of the model's probability function from samples generated by the model. We revisit this idea, propose a more efficient estimator, and prove that it provides a lower bound on the true test log-likelihood, and an unbiased estimator as the number of generated samples goes to infinity, although one that incorporates the effect of poor mixing (making the estimated likelihood worse, i.e., more conservative).", "pdf": "https://arxiv.org/abs/1311.6184", "paperhash": "bengio|bounding_the_test_loglikelihood_of_generative_models", "keywords": [], "conflicts": [], "authors": ["Yoshua Bengio", "Li Yao", "KyungHyun Cho"], "authorids": ["yoshua.bengio@gmail.com", "yaoli.email@gmail.com", "kyunghyun.cho@aalto.fi"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391971980000, "tcdate": 1391971980000, "number": 3, "id": "YAwpMHz_OUAvC", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "gg4nKrblw0gkf", "replyto": "gg4nKrblw0gkf", "signatures": ["KyungHyun Cho"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "Dear Reviewer (0661),\r\n\r\nWe thank you for the thorough and insightful comment. Allow us to respond to some of your comments below.\r\n\r\n'it places the idea of used by Breuleux et al. in a rigorous framework'\r\n\r\nWe agree that the proposed method is closely related to that by Breuleux et al. However, we claim that it is an improvement over the method by Breuleux et al. in two ways:\r\n\r\n(1) the CSL is more efficient because each sample of latent variables h can cover many x's.\r\n(2) the CSL does not have any tuning parameter such as bandwidth.\r\n\r\n'it exhibits very substantial divergence from AIS on larger models, as shown in Table 1.'\r\n\r\nAs you have pointed out earlier in your review, the proposed CSL estimator does not only evaluate the model itself but also the sampling procedure. When 'mixing' by MCMC sampling is fast (as in GSN), the CSL estimate tend to converge quickly, while in the opposite case (as in RBM and DBN), the convergence is slow. \r\n\r\n'the number of samples that's needed in order to accurate compute a log probability grows exponentially with the entropy of the distribution'\r\n\r\nThis is true, but one thing to be noted is that we are not aware of any alternative approach that does not suffer from this problem, when an approach such as AIS is not applicable.  Furthermore, we believe the fact that the CSL estimator uses the samples of latent variables h which are in a more abstract space than the raw input space, may make sampling-based methods such as the proposed CSL greatly reduce the curse of dimensionality.\r\n\r\nNonetheless, we agree that this is where future work lies, and we are indeed currently exploring different ways of exploiting the presence of a high-level (deep) representation to make the problem of likelihood estimation much easier and its convergence faster. Much more work is needed before these new ideas can be proven right and this paper should instead be judged in comparison with the past published work."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Bounding the Test Log-Likelihood of Generative Models", "decision": "submitted, no decision", "abstract": "Several interesting generative learning algorithms involve a complex probability distribution over many random variables, involving intractable normalization constants or latent variable normalization. Some of them may even not have an analytic expression for the unnormalized probability function and no tractable approximation. This makes it difficult to estimate the quality of these models, once they have been trained, or to monitor their quality (e.g. for early stopping) while training. A previously proposed method is based on constructing a non-parametric density estimator of the model's probability function from samples generated by the model. We revisit this idea, propose a more efficient estimator, and prove that it provides a lower bound on the true test log-likelihood, and an unbiased estimator as the number of generated samples goes to infinity, although one that incorporates the effect of poor mixing (making the estimated likelihood worse, i.e., more conservative).", "pdf": "https://arxiv.org/abs/1311.6184", "paperhash": "bengio|bounding_the_test_loglikelihood_of_generative_models", "keywords": [], "conflicts": [], "authors": ["Yoshua Bengio", "Li Yao", "KyungHyun Cho"], "authorids": ["yoshua.bengio@gmail.com", "yaoli.email@gmail.com", "kyunghyun.cho@aalto.fi"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391971980000, "tcdate": 1391971980000, "number": 1, "id": "Zem0HS_9PVHWt", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "gg4nKrblw0gkf", "replyto": "2xMtxps-6j3_7", "signatures": ["KyungHyun Cho"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "Dear Reviewer (0661),\r\n\r\nWe thank you for the thorough and insightful comment. Allow us to respond to some of your comments below.\r\n\r\n'it places the idea of used by Breuleux et al. in a rigorous framework'\r\n\r\nWe agree that the proposed method is closely related to that by Breuleux et al. However, we claim that it is an improvement over the method by Breuleux et al. in two ways:\r\n\r\n(1) the CSL is more efficient because each sample of latent variables h can cover many x's.\r\n(2) the CSL does not have any tuning parameter such as bandwidth.\r\n\r\n'it exhibits very substantial divergence from AIS on larger models, as shown in Table 1.'\r\n\r\nAs you have pointed out earlier in your review, the proposed CSL estimator does not only evaluate the model itself but also the sampling procedure. When 'mixing' by MCMC sampling is fast (as in GSN), the CSL estimate tend to converge quickly, while in the opposite case (as in RBM and DBN), the convergence is slow. \r\n\r\n'the number of samples that's needed in order to accurate compute a log probability grows exponentially with the entropy of the distribution'\r\n\r\nThis is true, but one thing to be noted is that we are not aware of any alternative approach that does not suffer from this problem, when an approach such as AIS is not applicable.  Furthermore, we believe the fact that the CSL estimator uses the samples of latent variables h which are in a more abstract space than the raw input space, may make sampling-based methods such as the proposed CSL greatly reduce the curse of dimensionality.\r\n\r\nNonetheless, we agree that this is where future work lies, and we are indeed currently exploring different ways of exploiting the presence of a high-level (deep) representation to make the problem of likelihood estimation much easier and its convergence faster. Much more work is needed before these new ideas can be proven right and this paper should instead be judged in comparison with the past published work."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Bounding the Test Log-Likelihood of Generative Models", "decision": "submitted, no decision", "abstract": "Several interesting generative learning algorithms involve a complex probability distribution over many random variables, involving intractable normalization constants or latent variable normalization. Some of them may even not have an analytic expression for the unnormalized probability function and no tractable approximation. This makes it difficult to estimate the quality of these models, once they have been trained, or to monitor their quality (e.g. for early stopping) while training. A previously proposed method is based on constructing a non-parametric density estimator of the model's probability function from samples generated by the model. We revisit this idea, propose a more efficient estimator, and prove that it provides a lower bound on the true test log-likelihood, and an unbiased estimator as the number of generated samples goes to infinity, although one that incorporates the effect of poor mixing (making the estimated likelihood worse, i.e., more conservative).", "pdf": "https://arxiv.org/abs/1311.6184", "paperhash": "bengio|bounding_the_test_loglikelihood_of_generative_models", "keywords": [], "conflicts": [], "authors": ["Yoshua Bengio", "Li Yao", "KyungHyun Cho"], "authorids": ["yoshua.bengio@gmail.com", "yaoli.email@gmail.com", "kyunghyun.cho@aalto.fi"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391971980000, "tcdate": 1391971980000, "number": 1, "id": "xJqhJ8AlT4JyR", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "gg4nKrblw0gkf", "replyto": "kk5t-iaKmykSQ", "signatures": ["KyungHyun Cho"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "Dear Reviewer (60ea),\r\n\r\nWe thank you for the thorough and insightful comment. Allow us to respond to some of your comments below.\r\n\r\n'When would the authors expect the biased CSL method to be useful in practice?'\r\n\r\nAs shown in Fig. 1, the biased CSL well reflects the ordering of the performances of different models correctly, however, optimistic. This is true even in the case where only a single MCMC step was taken from each test sample. As stated in Sec. 6, we believe the biased CSL will be useful in comparing models when there is no alternative way to compute/approximate the log-likelihood (such as GSN).\r\n\r\n'which one is closer to the truth?'\r\n\r\nAs you have correctly mentioned, it is not possible to answer this exactly for those models in Table 1. However, the result in Table 2 suggests that with enough samples, both the CSL and the estimate using AIS approach the true log-likelhood closely.\r\n\r\n'Is there a reason that GSN is so much better?'\r\n\r\nOne important factor affecting the CSL estimate is the 'mixing' rate of MCMC chain. As has been shown earlier, the MCMC sampling by GSN mixes among different modes very quickly, potentially leading to more accurate CSL estimate with less number of samples. \r\n\r\n'an RBM trained with PCD is thought to have much better likelihood than an RBM trained with CD. Is this reflected in CSL estimates?'\r\n\r\nAs the samples generated from an RBM trained with CD are generally bad (most of them tend to be from suprious modes), we believe this will be well reflected in CSL estimates. Also, our preliminary experiment with CD revealed the same tendency (not included in the paper).\r\n\r\nThank for you for pointing out typos in the paper. We will correct them in the next version."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Bounding the Test Log-Likelihood of Generative Models", "decision": "submitted, no decision", "abstract": "Several interesting generative learning algorithms involve a complex probability distribution over many random variables, involving intractable normalization constants or latent variable normalization. Some of them may even not have an analytic expression for the unnormalized probability function and no tractable approximation. This makes it difficult to estimate the quality of these models, once they have been trained, or to monitor their quality (e.g. for early stopping) while training. A previously proposed method is based on constructing a non-parametric density estimator of the model's probability function from samples generated by the model. We revisit this idea, propose a more efficient estimator, and prove that it provides a lower bound on the true test log-likelihood, and an unbiased estimator as the number of generated samples goes to infinity, although one that incorporates the effect of poor mixing (making the estimated likelihood worse, i.e., more conservative).", "pdf": "https://arxiv.org/abs/1311.6184", "paperhash": "bengio|bounding_the_test_loglikelihood_of_generative_models", "keywords": [], "conflicts": [], "authors": ["Yoshua Bengio", "Li Yao", "KyungHyun Cho"], "authorids": ["yoshua.bengio@gmail.com", "yaoli.email@gmail.com", "kyunghyun.cho@aalto.fi"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391861580000, "tcdate": 1391861580000, "number": 2, "id": "2xMtxps-6j3_7", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "gg4nKrblw0gkf", "replyto": "gg4nKrblw0gkf", "signatures": ["anonymous reviewer 0661"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Bounding the Test Log-Likelihood of Generative Models", "review": "The paper proposes a method for estimating the log probability of any probabilistic\r\nmodel that can generate samples.  The method builds a local density estimator around\r\nthe samples using the model's conditional probability, \r\nwhich is used to evaluate the log probability of a test set.  An important\r\nselling point of the method is that it evaluates the probabilistic model and the \r\nsampling procedure jointly, and that it is asymptotically consistent, in the sense\r\nthat the estimates converge to the true likelihood as the number of samples approaches\r\ninfinity.\r\n\r\nThis work is quite novel, and it places the idea of used by Breuleux et al. in a \r\nrigorous framework.  Empirically, the method works well on small models, although\r\nit exhibits very substantial divergence from AIS on larger models, as shown in Table 1.\r\n\r\nPerhaps the greatest weakness of this method, which is worth discussing, is that\r\nthe number of samples that's needed in order to accurate compute a log probability\r\ngrows exponentially with the entropy of the distribution.  For an example, consider\r\nthe dataset consisting the concatenations of of 10 randomly chosen MNIST digits.  It is \r\nfairly clear any sample set <<< 10^10 will vastly underestimate the log probability\r\nof a perfectly good sample. That is unfortunate, because it means that the method will\r\nnot work well on complicated models of high entropy distributions, such as images or speech. \r\n\r\nThis weakness notwithstanding, the method is very adequate for model comparison.  \r\n\r\nTo summarize:  Pros:  interesting method for obtaining conservative underestimates of the log probability,\r\nworks with nearly any model.\r\n\r\nCons:  method's complexity is exponential in the distribution's entropy;  the proposed fix is no longer \r\nconservative."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Bounding the Test Log-Likelihood of Generative Models", "decision": "submitted, no decision", "abstract": "Several interesting generative learning algorithms involve a complex probability distribution over many random variables, involving intractable normalization constants or latent variable normalization. Some of them may even not have an analytic expression for the unnormalized probability function and no tractable approximation. This makes it difficult to estimate the quality of these models, once they have been trained, or to monitor their quality (e.g. for early stopping) while training. A previously proposed method is based on constructing a non-parametric density estimator of the model's probability function from samples generated by the model. We revisit this idea, propose a more efficient estimator, and prove that it provides a lower bound on the true test log-likelihood, and an unbiased estimator as the number of generated samples goes to infinity, although one that incorporates the effect of poor mixing (making the estimated likelihood worse, i.e., more conservative).", "pdf": "https://arxiv.org/abs/1311.6184", "paperhash": "bengio|bounding_the_test_loglikelihood_of_generative_models", "keywords": [], "conflicts": [], "authors": ["Yoshua Bengio", "Li Yao", "KyungHyun Cho"], "authorids": ["yoshua.bengio@gmail.com", "yaoli.email@gmail.com", "kyunghyun.cho@aalto.fi"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391849220000, "tcdate": 1391849220000, "number": 1, "id": "kk5t-iaKmykSQ", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "gg4nKrblw0gkf", "replyto": "gg4nKrblw0gkf", "signatures": ["anonymous reviewer 60ea"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Bounding the Test Log-Likelihood of Generative Models", "review": "This paper proposes an estimator for the log-likelihood of intractable models with latent variables. The approach is simple in that it has no free parameters, doesn\u2019t require an explicit likelihood, and only needs samples from the model. The approach is most useful for model comparison since the estimate is conservative rather than optimistic.\r\n\r\nI enjoyed reading this paper. The proposed method is quite novel and elegant and has the potential to be a useful tool for model comparison. One issue is that the estimator seems to require a large number of samples in order to converge, and this potentially exacerbated by increasing model size. As stated in the paper, this is likely to do with the convergence of MCMC within the model itself. One empirical test of this would be to compare the efficiency of the estimator with exact samples vs MCMC in e.g., a small RBM.\r\n\r\nThe biased CSL is also novel, but seems to be even more optimistic than AIS. The argument of the paper is based on the idea that we would prefer conservative estimates to optimistic estimates for model comparison. When would the authors expect the biased CSL method to be useful in practice? How many steps would be required before biased CSL matches AIS?\r\n\r\nMinor thoughts and some found typos below.\r\n\r\n1. In table 1 the AIS and CSL estimates are vastly different. One is optimistic and one is conservative - which one is closer to the truth? Is there a reason that GSN is so much better? Obviously the truth is impossible to determine, but it is clear that more samples are needed before the estimate converges.\r\n2. The RBM used in table 2 is quite small, using only 5 hidden units. 20 hidden units is slightly larger but still tractable. It would be good to see how the efficiency of the estimator is affected by model size.\r\n3. An RBM trained with PCD is thought to have much better likelihood than an RBM trained with CD. Is this reflected in CSL estimates?\r\n\r\nformulat -> formulate (section 1)\r\ncollecte -> collect (section 2)\r\nin -> in (Monte-Carlo estimator in section 4)\r\n30 steps -> 300 steps (or the the legend in Figure 1 has a typo, section 6)\r\nmode -> model (section 7)"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Bounding the Test Log-Likelihood of Generative Models", "decision": "submitted, no decision", "abstract": "Several interesting generative learning algorithms involve a complex probability distribution over many random variables, involving intractable normalization constants or latent variable normalization. Some of them may even not have an analytic expression for the unnormalized probability function and no tractable approximation. This makes it difficult to estimate the quality of these models, once they have been trained, or to monitor their quality (e.g. for early stopping) while training. A previously proposed method is based on constructing a non-parametric density estimator of the model's probability function from samples generated by the model. We revisit this idea, propose a more efficient estimator, and prove that it provides a lower bound on the true test log-likelihood, and an unbiased estimator as the number of generated samples goes to infinity, although one that incorporates the effect of poor mixing (making the estimated likelihood worse, i.e., more conservative).", "pdf": "https://arxiv.org/abs/1311.6184", "paperhash": "bengio|bounding_the_test_loglikelihood_of_generative_models", "keywords": [], "conflicts": [], "authors": ["Yoshua Bengio", "Li Yao", "KyungHyun Cho"], "authorids": ["yoshua.bengio@gmail.com", "yaoli.email@gmail.com", "kyunghyun.cho@aalto.fi"]}, "tags": [], "invitation": {}}}, {"replyto": null, "ddate": null, "legacy_migration": true, "tmdate": 1387430040000, "tcdate": 1387430040000, "number": 9, "id": "gg4nKrblw0gkf", "invitation": "ICLR.cc/2014/conference/-/submission", "forum": "gg4nKrblw0gkf", "signatures": ["yoshua.bengio@gmail.com"], "readers": ["everyone"], "content": {"title": "Bounding the Test Log-Likelihood of Generative Models", "decision": "submitted, no decision", "abstract": "Several interesting generative learning algorithms involve a complex probability distribution over many random variables, involving intractable normalization constants or latent variable normalization. Some of them may even not have an analytic expression for the unnormalized probability function and no tractable approximation. This makes it difficult to estimate the quality of these models, once they have been trained, or to monitor their quality (e.g. for early stopping) while training. A previously proposed method is based on constructing a non-parametric density estimator of the model's probability function from samples generated by the model. We revisit this idea, propose a more efficient estimator, and prove that it provides a lower bound on the true test log-likelihood, and an unbiased estimator as the number of generated samples goes to infinity, although one that incorporates the effect of poor mixing (making the estimated likelihood worse, i.e., more conservative).", "pdf": "https://arxiv.org/abs/1311.6184", "paperhash": "bengio|bounding_the_test_loglikelihood_of_generative_models", "keywords": [], "conflicts": [], "authors": ["Yoshua Bengio", "Li Yao", "KyungHyun Cho"], "authorids": ["yoshua.bengio@gmail.com", "yaoli.email@gmail.com", "kyunghyun.cho@aalto.fi"]}, "writers": [], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1369422751717, "tmdate": 1496674357195, "id": "ICLR.cc/2014/conference/-/submission", "writers": ["ICLR.cc/2014"], "signatures": ["OpenReview.net"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": []}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1377198751717, "cdate": 1496674357195}}}], "count": 8}