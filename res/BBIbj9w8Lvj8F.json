{"notes": [{"ddate": null, "legacy_migration": true, "tmdate": 1362994020000, "tcdate": 1362994020000, "number": 1, "id": "t-wFtMYSdpR8v", "invitation": "ICLR.cc/2013/-/submission/reply", "forum": "BBIbj9w8Lvj8F", "replyto": "y-XNy_0Refysb", "signatures": ["Judy Hoffman, Erik Rodner, Jeff Donahue, Trevor Darrell, Kate Saenko"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "Please see the comment below (from March 3rd).\r\n\r\nWe have updated the paper to incorporate your comments."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Learning of Domain-invariant Image Representations", "decision": "conferenceOral-iclr2013-conference", "abstract": "We present an algorithm that learns representations which explicitly compensate for domain mismatch and which can be efficiently realized as linear classifiers. Specifically, we form a linear transformation that maps features from the target (test) domain to the source (training) domain as part of training the classifier. We optimize both the transformation and classifier parameters jointly, and introduce an efficient cost function based on misclassification loss. Our method combines several features previously unavailable in a single algorithm: multi-class adaptation through representation learning, ability to map across heterogeneous feature spaces, and scalability to large datasets. We present experiments on several image datasets that demonstrate improved accuracy and computational advantages compared to previous approaches.", "pdf": "https://arxiv.org/abs/1301.3224", "paperhash": "hoffman|efficient_learning_of_domaininvariant_image_representations", "keywords": [], "conflicts": [], "authors": ["Judy Hoffman", "Erik Rodner", "Jeff Donahue", "Kate Saenko", "Trevor Darrell"], "authorids": ["jhoffman@eecs.berkeley.edu", "erik.rodner@uni-jena.de", "jdonahue@eecs.berkeley.edu", "saenko@cs.uml.edu", "trevordarrell@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362971820000, "tcdate": 1362971820000, "number": 1, "id": "FPpzPM-IHKPkZ", "invitation": "ICLR.cc/2013/-/submission/reply", "forum": "BBIbj9w8Lvj8F", "replyto": "Ua0HJI2r-Waro", "signatures": ["Judy Hoffman, Erik Rodner, Jeff Donahue, Trevor Darrell, Kate Saenko"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "Thank you for your feedback. We would like to start by clarifying a few points from your comments section. First, our first experiment (standard domain adaptation setting) SVM_t is the classifier learned from being trained with only the limited available data from the target domain. So, for example when we're looking at the shift between amazon to webcam (a->w) we have a lot of training data from amazon and a very small amount of the webcam dataset. SVM_t for this example would be an SVM trained on just the small amount of data from webcam. Note that in the new category experiment setting it is not possible to train SVM_t because there are some categories that have no labeled examples in the target. Second, for our last experiment, SVM_s does not (and should not) change as the number of points in the target is increased. SVM_s is an SVM classifier trained using only source data. In the figure it is represented by the dotted cyan line, which remains constant (at around 42%) as the number of labeled target examples grows. As a third point, if we did have a metric to determine the adaptability of a (source,target) domain pair then we could simply choose to use the source data which is most adaptable to our target data. However, [15] provides a metric to determine a 'distance' between the source and target subspace, not necessarily an adaptability metric. The two might be correlated depending on the adaptation algorithm you use. Namely, if a (source,target) pair are 'close' you might assume they are easily adaptable. But, with our method we learn a transformation between the two spaces, so it's possible for a (source,target) pair to initially be very different according to the metric from [15], but be very adaptable. For example: in [15] the metric said that Caltech was most similar to Amazon, followed by Webcam, followed by Dslr. However, if you look at Table 1 you see that we received higher accuracy when adapting between dslr->caltech then from webcam->caltech. So even though webcam was initially more similar to caltech than dslr to caltech, we find that dslr is more 'adaptable' to caltech.\r\n\r\nFinally, the idea of using more definite domains or even different modalities is very interesting to us and is something we are considering for future work. We do feel that the experiments we present do justify our claims that our algorithm performs comparable or better than state of the art techniques and is simultaneously applicable to a larger variety of possible adaptation scenarios."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Learning of Domain-invariant Image Representations", "decision": "conferenceOral-iclr2013-conference", "abstract": "We present an algorithm that learns representations which explicitly compensate for domain mismatch and which can be efficiently realized as linear classifiers. Specifically, we form a linear transformation that maps features from the target (test) domain to the source (training) domain as part of training the classifier. We optimize both the transformation and classifier parameters jointly, and introduce an efficient cost function based on misclassification loss. Our method combines several features previously unavailable in a single algorithm: multi-class adaptation through representation learning, ability to map across heterogeneous feature spaces, and scalability to large datasets. We present experiments on several image datasets that demonstrate improved accuracy and computational advantages compared to previous approaches.", "pdf": "https://arxiv.org/abs/1301.3224", "paperhash": "hoffman|efficient_learning_of_domaininvariant_image_representations", "keywords": [], "conflicts": [], "authors": ["Judy Hoffman", "Erik Rodner", "Jeff Donahue", "Kate Saenko", "Trevor Darrell"], "authorids": ["jhoffman@eecs.berkeley.edu", "erik.rodner@uni-jena.de", "jdonahue@eecs.berkeley.edu", "saenko@cs.uml.edu", "trevordarrell@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362971700000, "tcdate": 1362971700000, "number": 1, "id": "tWNADGgy0XWy2", "invitation": "ICLR.cc/2013/-/submission/reply", "forum": "BBIbj9w8Lvj8F", "replyto": "u3MkubcB_YIB0", "signatures": ["Judy Hoffman, Erik Rodner, Jeff Donahue, Trevor Darrell, Kate Saenko"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "Thank you for your review. In this paper we present a method that learns an asymmetric linear mapping between the source and target feature spaces. In general, the feature transformation learning can be kernelized (the optimization framework can be formulated as a standard QP). However, for this work we focus on the linear case because of it's scalability to a large number of data points. We show that using the linear framework we perform as well or better than other methods which learn a non-linear mapping. \r\n\r\nWe learn a transformation between the target and source points which can be expressed by the matrix W in our paper. In this paper, we use this matrix to compute the dot product in the source domain between theta_k and the transformed target points (Wx^t_i). However, if we think of W (an asymmetric matrix) as begin decomposed as W = A'B, then the dot product function can be interpreted as theta_k'A'Bx^t_i. In other words it could be interpreted as the dot product in some common latent space between source points transformed by A and target points transformed by B. We propose learning the W matrix rather than A,B directly so that we do not have to specify the dimension of the latent space."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Learning of Domain-invariant Image Representations", "decision": "conferenceOral-iclr2013-conference", "abstract": "We present an algorithm that learns representations which explicitly compensate for domain mismatch and which can be efficiently realized as linear classifiers. Specifically, we form a linear transformation that maps features from the target (test) domain to the source (training) domain as part of training the classifier. We optimize both the transformation and classifier parameters jointly, and introduce an efficient cost function based on misclassification loss. Our method combines several features previously unavailable in a single algorithm: multi-class adaptation through representation learning, ability to map across heterogeneous feature spaces, and scalability to large datasets. We present experiments on several image datasets that demonstrate improved accuracy and computational advantages compared to previous approaches.", "pdf": "https://arxiv.org/abs/1301.3224", "paperhash": "hoffman|efficient_learning_of_domaininvariant_image_representations", "keywords": [], "conflicts": [], "authors": ["Judy Hoffman", "Erik Rodner", "Jeff Donahue", "Kate Saenko", "Trevor Darrell"], "authorids": ["jhoffman@eecs.berkeley.edu", "erik.rodner@uni-jena.de", "jdonahue@eecs.berkeley.edu", "saenko@cs.uml.edu", "trevordarrell@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362783240000, "tcdate": 1362783240000, "number": 1, "id": "Ua0HJI2r-Waro", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "BBIbj9w8Lvj8F", "replyto": "BBIbj9w8Lvj8F", "signatures": ["anonymous reviewer 36a3"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Efficient Learning of Domain-invariant Image Representations", "review": "The paper presents a new method for learning domain invariant image\r\nrepresentations. The proposed approach simultaneously learns a linear\r\nmapping of the target features into the source domain and the\r\nparameters of a multi-class linear SVM classifier. Experimental\r\nevaluations show that the proposed approach performs similarly or\r\nbetter than previous art. The new algorithm presents computational\r\nadvantages with respect to previous approaches.\r\n\r\nThe paper is well written and clearly presented. It addresses an\r\ninteresting problem proposing that has received attention in recent\r\nyears. The proposed method is considerably simpler than competitive\r\napproaches with similar (or better) performance (in the setting of the\r\nreported experiments). The method is not very novel but manages to\r\nimprove some drawbacks of previous approaches.\r\n\r\nPros:\r\n\r\n- the proposed framework is fairly simple and the provided\r\nimplementation details makes it easy to reproduce\r\n- experimental evaluation is presented, comparing the proposed method\r\nwith several competing approaches. The amount of empirical evidence\r\nseems sufficient to back up the claims.\r\n\r\nCons:\r\n\r\n- Being this method general, I think that it would have been very good\r\nto include an example with more distinct source and target feature\r\nspaces (e.g. text categorization), or even better different\r\nmodalities.\r\n\r\n\r\nComments:\r\n\r\nIn the work [15], the authors propose a metric that measures the\r\nadaptability between a pair of source and target domains. In this\r\nsetting if several possible source domains are available, it selects\r\nthe best one. How could this be considered in your setting?\r\n\r\nIn the first experimental setting (standard domain adaptation\r\nproblem), I understand that the idea the experiment is to show how the\r\nlabeled data in the source domain can help to better classify the data\r\nin the target domain. It is not clear to me how the SVM trained with\r\ntraining data, SVM_t, of the target domain. Is this done only with the\r\nlimited set of labeled data in the target domain? What is the case for\r\nthe SVM_s?\r\n\r\nLooking to the last experimental setting, I suppose that the SVM_s\r\n(trained using source training data) also includes the transformed\r\ndata from the target domain. Otherwise, I don't understand how the\r\nperformance can increase by increasing the number of labeled target\r\nexamples."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Learning of Domain-invariant Image Representations", "decision": "conferenceOral-iclr2013-conference", "abstract": "We present an algorithm that learns representations which explicitly compensate for domain mismatch and which can be efficiently realized as linear classifiers. Specifically, we form a linear transformation that maps features from the target (test) domain to the source (training) domain as part of training the classifier. We optimize both the transformation and classifier parameters jointly, and introduce an efficient cost function based on misclassification loss. Our method combines several features previously unavailable in a single algorithm: multi-class adaptation through representation learning, ability to map across heterogeneous feature spaces, and scalability to large datasets. We present experiments on several image datasets that demonstrate improved accuracy and computational advantages compared to previous approaches.", "pdf": "https://arxiv.org/abs/1301.3224", "paperhash": "hoffman|efficient_learning_of_domaininvariant_image_representations", "keywords": [], "conflicts": [], "authors": ["Judy Hoffman", "Erik Rodner", "Jeff Donahue", "Kate Saenko", "Trevor Darrell"], "authorids": ["jhoffman@eecs.berkeley.edu", "erik.rodner@uni-jena.de", "jdonahue@eecs.berkeley.edu", "saenko@cs.uml.edu", "trevordarrell@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362393540000, "tcdate": 1362393540000, "number": 3, "id": "u3MkubcB_YIB0", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "BBIbj9w8Lvj8F", "replyto": "BBIbj9w8Lvj8F", "signatures": ["anonymous reviewer feb2"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Efficient Learning of Domain-invariant Image Representations", "review": "This paper proposes to make domain adaptation and multi-task learning easier by jointly learning the task-specific max-margin classifiers and a linear mapping from a new target space to the source space; the loss function encourages the mapped features to lie on the correct side of the hyperplanes learned for each task of the hyperplanes of the max-margin classifiers. Experiments show that the mapping performs as well or better as existing domain adaptation methods, but can scale to larger problems while many earlier approaches are too costly.\r\n\r\nOverall the paper is clear, well-crafted, and the context and previous work are well presented. The idea is appealing in its simplicity, and works well.\r\n\r\nPros: the idea is intuitive and well justified; it is appealing that the method is flexible and can tackle cases where labels are missing for some categories.\r\nThe paper is clear and well-written.\r\nExperimental results are convincing enough; while the results are not outperforming the state of the art (results are within the standard error of previously published performance), the authors' argument that their method is better suited to cases where domains are more different seems reasonable and backed by their experimental results.\r\n\r\nCons: this method would work only in cases where a simple general linear rotation of features would do a good job placing features in a favorable space.\r\nThe method also gives a privileged role to the source space, while methods that map features to a common latent space have more symmetry; the authors argue that it is hard to guess the optimal dimension of the latent space -- but their method simply constrains it to the size of the source space, so there is no guarantee that this would be any more optimal."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Learning of Domain-invariant Image Representations", "decision": "conferenceOral-iclr2013-conference", "abstract": "We present an algorithm that learns representations which explicitly compensate for domain mismatch and which can be efficiently realized as linear classifiers. Specifically, we form a linear transformation that maps features from the target (test) domain to the source (training) domain as part of training the classifier. We optimize both the transformation and classifier parameters jointly, and introduce an efficient cost function based on misclassification loss. Our method combines several features previously unavailable in a single algorithm: multi-class adaptation through representation learning, ability to map across heterogeneous feature spaces, and scalability to large datasets. We present experiments on several image datasets that demonstrate improved accuracy and computational advantages compared to previous approaches.", "pdf": "https://arxiv.org/abs/1301.3224", "paperhash": "hoffman|efficient_learning_of_domaininvariant_image_representations", "keywords": [], "conflicts": [], "authors": ["Judy Hoffman", "Erik Rodner", "Jeff Donahue", "Kate Saenko", "Trevor Darrell"], "authorids": ["jhoffman@eecs.berkeley.edu", "erik.rodner@uni-jena.de", "jdonahue@eecs.berkeley.edu", "saenko@cs.uml.edu", "trevordarrell@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362367200000, "tcdate": 1362367200000, "number": 4, "id": "JnShnsXduOpVA", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "BBIbj9w8Lvj8F", "replyto": "BBIbj9w8Lvj8F", "signatures": ["Judy Hoffman, Erik Rodner, Jeff Donahue, Trevor Darrell, Kate Saenko"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "Thank you for your feedback. We argue that the task of adapting representations across domains is one that is common to all representation learning challenges, including those based on deep architectures, metric learning methods, and max-margin transform learning. Our insight into this problem is to use the source classifier to inform the representation learned for the target data. Specifically, we jointly learn a source domain classifier and a representation for the target domain, such that the target points can be well classified in the source domain.\r\n\r\nWe present a specific algorithm using an SVM classifier and testing on visual domains, however the principles of our method are applicable to both a range of methods for learning and classification (beyond SVM) as well as a range of applications (beyond vision).\r\n\r\nIn addition, thank you for your comments section. We will clarify what is meant by asymmetric transform and modify the wording around equations (4-5) to reflect the math shown, which has soft constraints and no slack variables."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Learning of Domain-invariant Image Representations", "decision": "conferenceOral-iclr2013-conference", "abstract": "We present an algorithm that learns representations which explicitly compensate for domain mismatch and which can be efficiently realized as linear classifiers. Specifically, we form a linear transformation that maps features from the target (test) domain to the source (training) domain as part of training the classifier. We optimize both the transformation and classifier parameters jointly, and introduce an efficient cost function based on misclassification loss. Our method combines several features previously unavailable in a single algorithm: multi-class adaptation through representation learning, ability to map across heterogeneous feature spaces, and scalability to large datasets. We present experiments on several image datasets that demonstrate improved accuracy and computational advantages compared to previous approaches.", "pdf": "https://arxiv.org/abs/1301.3224", "paperhash": "hoffman|efficient_learning_of_domaininvariant_image_representations", "keywords": [], "conflicts": [], "authors": ["Judy Hoffman", "Erik Rodner", "Jeff Donahue", "Kate Saenko", "Trevor Darrell"], "authorids": ["jhoffman@eecs.berkeley.edu", "erik.rodner@uni-jena.de", "jdonahue@eecs.berkeley.edu", "saenko@cs.uml.edu", "trevordarrell@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362214260000, "tcdate": 1362214260000, "number": 2, "id": "y-XNy_0Refysb", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "BBIbj9w8Lvj8F", "replyto": "BBIbj9w8Lvj8F", "signatures": ["anonymous reviewer 9aa4"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Efficient Learning of Domain-invariant Image Representations", "review": "This paper focuses on multi-task learning across domains, where both the data generating distribution and the output labels can change between source and target domains. It presents a SVM-based model which jointly learns 1) affine hyperplanes that separate the classes in a common domain consisting of the source and the target projected to the source; and 2) a linear transformation mapping points from the target domain into the source domain.\r\n\r\nPositive points\r\n\r\n1) The method is dead simple and seems technically sound. To the best of my knowledge it's novel, but I'm not as familiar with the SVM literature - I am hoping that another reviewer comes from the SVM community and can better assess its novelty.\r\n2) The paper is well written and understandable\r\n3) The experiments seem thorough: several datasets and tasks are considered, the model is compared to various baselines. The model is shown to outperform contemporary domain adaption methods, generalize to novel test categories at test time (which many other methods cannot do) and can scale to large datasets.\r\n\r\nNegative points\r\n\r\nI have one major criticism: the paper doesn't seem really focused on representation learning - it's more a paper about a method for multi-task learning across domains which learns a (shallow, linear) mapping from source to target. I agree - it's a representation but there's no real analysis or focus on the representation itself - e.g. what is being captured by the representation. The method is totally valid, but I just get the sense that it's a paper that could fit well with CVPR or ICCV (i.e. a good vision paper) where the title says 'represention learning', and a few sentences highlight the 'representation' that's being learned, however the method nor the paper's focus is really on learning interesting representations. On one hand I question its suitability for ICLR and it's appeal to the community (compared to CVPR/ICCV, etc.) but on the other hand, I think it's great to encourage diversity in the papers/authors at the conference and having a more 'visiony'-feeling paper is not a bad thing.\r\n\r\nComments\r\n--------\r\n\r\nCan you state up front what is meant by the asymmetry of the transform (e.g. when it's first mentioned)? Later on in the paper it becomes clear that it has to do with the source and target having different feature dimensions but it wasn't obvious to me at the beginning of the paper. \r\n\r\nJust before Eq (4) and (5) it says that 'we begin by rewriting Eq 1-3 with soft constraints (slack)'. But where are the slack variables in Eq 4?"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Learning of Domain-invariant Image Representations", "decision": "conferenceOral-iclr2013-conference", "abstract": "We present an algorithm that learns representations which explicitly compensate for domain mismatch and which can be efficiently realized as linear classifiers. Specifically, we form a linear transformation that maps features from the target (test) domain to the source (training) domain as part of training the classifier. We optimize both the transformation and classifier parameters jointly, and introduce an efficient cost function based on misclassification loss. Our method combines several features previously unavailable in a single algorithm: multi-class adaptation through representation learning, ability to map across heterogeneous feature spaces, and scalability to large datasets. We present experiments on several image datasets that demonstrate improved accuracy and computational advantages compared to previous approaches.", "pdf": "https://arxiv.org/abs/1301.3224", "paperhash": "hoffman|efficient_learning_of_domaininvariant_image_representations", "keywords": [], "conflicts": [], "authors": ["Judy Hoffman", "Erik Rodner", "Jeff Donahue", "Kate Saenko", "Trevor Darrell"], "authorids": ["jhoffman@eecs.berkeley.edu", "erik.rodner@uni-jena.de", "jdonahue@eecs.berkeley.edu", "saenko@cs.uml.edu", "trevordarrell@gmail.com"]}, "tags": [], "invitation": {}}}, {"replyto": null, "ddate": null, "legacy_migration": true, "tmdate": 1358317800000, "tcdate": 1358317800000, "number": 8, "id": "BBIbj9w8Lvj8F", "invitation": "ICLR.cc/2013/conference/-/submission", "forum": "BBIbj9w8Lvj8F", "signatures": ["jhoffman@eecs.berkeley.edu"], "readers": ["everyone"], "content": {"title": "Efficient Learning of Domain-invariant Image Representations", "decision": "conferenceOral-iclr2013-conference", "abstract": "We present an algorithm that learns representations which explicitly compensate for domain mismatch and which can be efficiently realized as linear classifiers. Specifically, we form a linear transformation that maps features from the target (test) domain to the source (training) domain as part of training the classifier. We optimize both the transformation and classifier parameters jointly, and introduce an efficient cost function based on misclassification loss. Our method combines several features previously unavailable in a single algorithm: multi-class adaptation through representation learning, ability to map across heterogeneous feature spaces, and scalability to large datasets. We present experiments on several image datasets that demonstrate improved accuracy and computational advantages compared to previous approaches.", "pdf": "https://arxiv.org/abs/1301.3224", "paperhash": "hoffman|efficient_learning_of_domaininvariant_image_representations", "keywords": [], "conflicts": [], "authors": ["Judy Hoffman", "Erik Rodner", "Jeff Donahue", "Kate Saenko", "Trevor Darrell"], "authorids": ["jhoffman@eecs.berkeley.edu", "erik.rodner@uni-jena.de", "jdonahue@eecs.berkeley.edu", "saenko@cs.uml.edu", "trevordarrell@gmail.com"]}, "writers": [], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1369422751717, "tmdate": 1496673673639, "cdate": 1496673673639, "tcdate": 1496673673639, "id": "ICLR.cc/2013/conference/-/submission", "writers": ["ICLR.cc/2013"], "signatures": ["OpenReview.net"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": []}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1377198751717}}}], "count": 8}