{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1488958318384, "tcdate": 1477783199854, "number": 12, "id": "S1di0sfgl", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "S1di0sfgl", "signatures": ["~Junyoung_Chung1"], "readers": ["everyone"], "content": {"title": "Hierarchical Multiscale Recurrent Neural Networks", "abstract": "Learning both hierarchical and temporal representation has been among the long- standing challenges of recurrent neural networks. Multiscale recurrent neural networks have been considered as a promising approach to resolve this issue, yet there has been a lack of empirical evidence showing that this type of models can actually capture the temporal dependencies by discovering the latent hierarchical structure of the sequence. In this paper, we propose a novel multiscale approach, called the hierarchical multiscale recurrent neural network, that can capture the latent hierarchical structure in the sequence by encoding the temporal dependencies with different timescales using a novel update mechanism. We show some evidence that the proposed model can discover underlying hierarchical structure in the sequences without using explicit boundary information. We evaluate our proposed model on character-level language modelling and handwriting sequence generation.", "pdf": "/pdf/7b450081bcf1cf199061ff7348f79ed6d403ecd7.pdf", "TL;DR": "Propose a recurrent neural network architecture that can discover the underlying hierarchical structure in the temporal data.", "paperhash": "chung|hierarchical_multiscale_recurrent_neural_networks", "keywords": ["Natural language processing", "Deep learning"], "conflicts": ["umontreal.ca", "iro.umontreal.ca", "google.com", "nyu.edu"], "authors": ["Junyoung Chung", "Sungjin Ahn", "Yoshua Bengio"], "authorids": ["junyoung.chung@umontreal.ca", "sungjin.ahn@umontreal.ca", "yoshua.bengio@umontreal.ca"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396310057, "tcdate": 1486396310057, "number": 1, "id": "SkAcsG8de", "invitation": "ICLR.cc/2017/conference/-/paper12/acceptance", "forum": "S1di0sfgl", "replyto": "S1di0sfgl", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "This extension to RNNs is clearly motivated, and the details of the proposed method are sensible. The paper would have benefitted from more experiments such as those in Figure 5 teasing out the representations learned by this model.", "decision": "Accept (Poster)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hierarchical Multiscale Recurrent Neural Networks", "abstract": "Learning both hierarchical and temporal representation has been among the long- standing challenges of recurrent neural networks. Multiscale recurrent neural networks have been considered as a promising approach to resolve this issue, yet there has been a lack of empirical evidence showing that this type of models can actually capture the temporal dependencies by discovering the latent hierarchical structure of the sequence. In this paper, we propose a novel multiscale approach, called the hierarchical multiscale recurrent neural network, that can capture the latent hierarchical structure in the sequence by encoding the temporal dependencies with different timescales using a novel update mechanism. We show some evidence that the proposed model can discover underlying hierarchical structure in the sequences without using explicit boundary information. We evaluate our proposed model on character-level language modelling and handwriting sequence generation.", "pdf": "/pdf/7b450081bcf1cf199061ff7348f79ed6d403ecd7.pdf", "TL;DR": "Propose a recurrent neural network architecture that can discover the underlying hierarchical structure in the temporal data.", "paperhash": "chung|hierarchical_multiscale_recurrent_neural_networks", "keywords": ["Natural language processing", "Deep learning"], "conflicts": ["umontreal.ca", "iro.umontreal.ca", "google.com", "nyu.edu"], "authors": ["Junyoung Chung", "Sungjin Ahn", "Yoshua Bengio"], "authorids": ["junyoung.chung@umontreal.ca", "sungjin.ahn@umontreal.ca", "yoshua.bengio@umontreal.ca"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396310548, "id": "ICLR.cc/2017/conference/-/paper12/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "S1di0sfgl", "replyto": "S1di0sfgl", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396310548}}}, {"tddate": null, "tmdate": 1484890050255, "tcdate": 1484890050255, "number": 5, "id": "Hk9p1mJDe", "invitation": "ICLR.cc/2017/conference/-/paper12/public/comment", "forum": "S1di0sfgl", "replyto": "H1nGAGf4g", "signatures": ["~Junyoung_Chung1"], "readers": ["everyone"], "writers": ["~Junyoung_Chung1"], "content": {"title": "Response to AnonReviewer3", "comment": "Thank you for a constructive review. \n\n>>Empirical results on computational savings are not given\n\nIt is not entirely clear how we should implement the conditional computation in a mini-batch setting, however it is very straight forward when the batch_size is equal to 1 (e.g., at test time). We are planning to include this experiment in a near future. \n\n>>It's unclear whether better downstream performance is due to use of hierarchical information or due to the architecture changes acting as regularization, something which could hopefully be addressed.\n\nA\u00a0good generalization performance is a result that we obtain by having the structural inductive bias (i.e., the hierarchical multiscale) incorporated in the RNN architecture. This is similar to the receptive field structure implemented in the CNNs. If we see the CNN architecture as an architectural regularizer, our model also implements an architectural regularizer. It depends on how broadly we define the term 'regularizer'. It is a regularizer in the sense that it restricts the model search space to a specific class (hierarchical multiscale). "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hierarchical Multiscale Recurrent Neural Networks", "abstract": "Learning both hierarchical and temporal representation has been among the long- standing challenges of recurrent neural networks. Multiscale recurrent neural networks have been considered as a promising approach to resolve this issue, yet there has been a lack of empirical evidence showing that this type of models can actually capture the temporal dependencies by discovering the latent hierarchical structure of the sequence. In this paper, we propose a novel multiscale approach, called the hierarchical multiscale recurrent neural network, that can capture the latent hierarchical structure in the sequence by encoding the temporal dependencies with different timescales using a novel update mechanism. We show some evidence that the proposed model can discover underlying hierarchical structure in the sequences without using explicit boundary information. We evaluate our proposed model on character-level language modelling and handwriting sequence generation.", "pdf": "/pdf/7b450081bcf1cf199061ff7348f79ed6d403ecd7.pdf", "TL;DR": "Propose a recurrent neural network architecture that can discover the underlying hierarchical structure in the temporal data.", "paperhash": "chung|hierarchical_multiscale_recurrent_neural_networks", "keywords": ["Natural language processing", "Deep learning"], "conflicts": ["umontreal.ca", "iro.umontreal.ca", "google.com", "nyu.edu"], "authors": ["Junyoung Chung", "Sungjin Ahn", "Yoshua Bengio"], "authorids": ["junyoung.chung@umontreal.ca", "sungjin.ahn@umontreal.ca", "yoshua.bengio@umontreal.ca"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287761573, "id": "ICLR.cc/2017/conference/-/paper12/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "S1di0sfgl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper12/reviewers", "ICLR.cc/2017/conference/paper12/areachairs"], "cdate": 1485287761573}}}, {"tddate": null, "tmdate": 1484854618392, "tcdate": 1484801561736, "number": 4, "id": "HyzXUTpLl", "invitation": "ICLR.cc/2017/conference/-/paper12/public/comment", "forum": "S1di0sfgl", "replyto": "HkbfXUcEx", "signatures": ["~Junyoung_Chung1"], "readers": ["everyone"], "writers": ["~Junyoung_Chung1"], "content": {"title": "Use the states of the backward RNN as auxiliary information ", "comment": "We can think of two types of simple approach that exploit a backward RNN.\n\n1) building representation based on intrinsic hierarchical structure of the data.\nWe can think of two HM-LSTM, forward RNN and backward RNN, let these RNNs to not interfere into inference of each other.\nThen, we concatenate the hidden representations of these two HM-LSTMs and provide as input to the output module. \nA downside of this approach is that, the model cannot take the full advantage of using bidirectional RNN - building representation of the next level based on the combined representation (forward RNN and backward RNN) of the previous level.\n\n2) use future information to yield better segmentation.\nWe can run the backward RNN (can be standard RNN, e.g., LSTM-RNN) first and use the hidden states as auxiliary input to the forward RNN, which is HM-LSTM. The downside here is that, the backward RNN is not a HM-LSTM, it cannot take any advantage such as conditional computation and etc."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hierarchical Multiscale Recurrent Neural Networks", "abstract": "Learning both hierarchical and temporal representation has been among the long- standing challenges of recurrent neural networks. Multiscale recurrent neural networks have been considered as a promising approach to resolve this issue, yet there has been a lack of empirical evidence showing that this type of models can actually capture the temporal dependencies by discovering the latent hierarchical structure of the sequence. In this paper, we propose a novel multiscale approach, called the hierarchical multiscale recurrent neural network, that can capture the latent hierarchical structure in the sequence by encoding the temporal dependencies with different timescales using a novel update mechanism. We show some evidence that the proposed model can discover underlying hierarchical structure in the sequences without using explicit boundary information. We evaluate our proposed model on character-level language modelling and handwriting sequence generation.", "pdf": "/pdf/7b450081bcf1cf199061ff7348f79ed6d403ecd7.pdf", "TL;DR": "Propose a recurrent neural network architecture that can discover the underlying hierarchical structure in the temporal data.", "paperhash": "chung|hierarchical_multiscale_recurrent_neural_networks", "keywords": ["Natural language processing", "Deep learning"], "conflicts": ["umontreal.ca", "iro.umontreal.ca", "google.com", "nyu.edu"], "authors": ["Junyoung Chung", "Sungjin Ahn", "Yoshua Bengio"], "authorids": ["junyoung.chung@umontreal.ca", "sungjin.ahn@umontreal.ca", "yoshua.bengio@umontreal.ca"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287761573, "id": "ICLR.cc/2017/conference/-/paper12/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "S1di0sfgl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper12/reviewers", "ICLR.cc/2017/conference/paper12/areachairs"], "cdate": 1485287761573}}}, {"tddate": null, "tmdate": 1482478344950, "tcdate": 1482478344950, "number": 3, "id": "HkbfXUcEx", "invitation": "ICLR.cc/2017/conference/-/paper12/official/review", "forum": "S1di0sfgl", "replyto": "S1di0sfgl", "signatures": ["ICLR.cc/2017/conference/paper12/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper12/AnonReviewer1"], "content": {"title": "", "rating": "8: Top 50% of accepted papers, clear accept", "review": "This paper proposes a new multiscale recurrent neural network, where each layer has different time scale, and the scale is not fixed but variable and determined by a neural network. The method is elegantly formulated within a recurrent neural network framework, and shows the state-of-the-art performance on several benchmarks. The paper is well written.\n\nQuestion) Can you extend it to bidirectional RNN? \n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hierarchical Multiscale Recurrent Neural Networks", "abstract": "Learning both hierarchical and temporal representation has been among the long- standing challenges of recurrent neural networks. Multiscale recurrent neural networks have been considered as a promising approach to resolve this issue, yet there has been a lack of empirical evidence showing that this type of models can actually capture the temporal dependencies by discovering the latent hierarchical structure of the sequence. In this paper, we propose a novel multiscale approach, called the hierarchical multiscale recurrent neural network, that can capture the latent hierarchical structure in the sequence by encoding the temporal dependencies with different timescales using a novel update mechanism. We show some evidence that the proposed model can discover underlying hierarchical structure in the sequences without using explicit boundary information. We evaluate our proposed model on character-level language modelling and handwriting sequence generation.", "pdf": "/pdf/7b450081bcf1cf199061ff7348f79ed6d403ecd7.pdf", "TL;DR": "Propose a recurrent neural network architecture that can discover the underlying hierarchical structure in the temporal data.", "paperhash": "chung|hierarchical_multiscale_recurrent_neural_networks", "keywords": ["Natural language processing", "Deep learning"], "conflicts": ["umontreal.ca", "iro.umontreal.ca", "google.com", "nyu.edu"], "authors": ["Junyoung Chung", "Sungjin Ahn", "Yoshua Bengio"], "authorids": ["junyoung.chung@umontreal.ca", "sungjin.ahn@umontreal.ca", "yoshua.bengio@umontreal.ca"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512724389, "id": "ICLR.cc/2017/conference/-/paper12/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper12/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper12/AnonReviewer4", "ICLR.cc/2017/conference/paper12/AnonReviewer3", "ICLR.cc/2017/conference/paper12/AnonReviewer1"], "reply": {"forum": "S1di0sfgl", "replyto": "S1di0sfgl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper12/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper12/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512724389}}}, {"tddate": null, "tmdate": 1481940499605, "tcdate": 1481940499605, "number": 2, "id": "H1nGAGf4g", "invitation": "ICLR.cc/2017/conference/-/paper12/official/review", "forum": "S1di0sfgl", "replyto": "S1di0sfgl", "signatures": ["ICLR.cc/2017/conference/paper12/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper12/AnonReviewer3"], "content": {"title": "well evaluated and written paper, novel flush operation", "rating": "7: Good paper, accept", "review": "The paper proposes a modified RNN architecture with multiple layers, where higher layers are only passed lower layer states if a FLUSH operation is predicted, consisting of passing up the state and reseting the lower layer's state. In order to select one of three operations at each time step, the authors propose using the straight-through estimator with a slope-annealing trick during training. Empirical results and visualizations illustrate that the modified architecture performs well at boundary detection.\n\nPros:\n- Paper is well-motivated, exceptionally well-composed\n- Provides promising initial results on learning hierarchical representations through visualizations and thorough experiments on language modeling and handwriting generation\n- The annealing trick with the straight-through estimator also seems potentially useful for other tasks containing discrete variables, and the trade-off in the flush operation is innovative.\nCons:\n- In a couple cases the paper does not fully deliver. Empirical results on computational savings are not given, and hierarchy beyond a single level (where the data contains separators such as spaces and pen up/down) does not seem to be demonstrated.\n- It's unclear whether better downstream performance is due to use of hierarchical information or due to the architecture changes acting as regularization, something which could hopefully be addressed.\n\n\n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hierarchical Multiscale Recurrent Neural Networks", "abstract": "Learning both hierarchical and temporal representation has been among the long- standing challenges of recurrent neural networks. Multiscale recurrent neural networks have been considered as a promising approach to resolve this issue, yet there has been a lack of empirical evidence showing that this type of models can actually capture the temporal dependencies by discovering the latent hierarchical structure of the sequence. In this paper, we propose a novel multiscale approach, called the hierarchical multiscale recurrent neural network, that can capture the latent hierarchical structure in the sequence by encoding the temporal dependencies with different timescales using a novel update mechanism. We show some evidence that the proposed model can discover underlying hierarchical structure in the sequences without using explicit boundary information. We evaluate our proposed model on character-level language modelling and handwriting sequence generation.", "pdf": "/pdf/7b450081bcf1cf199061ff7348f79ed6d403ecd7.pdf", "TL;DR": "Propose a recurrent neural network architecture that can discover the underlying hierarchical structure in the temporal data.", "paperhash": "chung|hierarchical_multiscale_recurrent_neural_networks", "keywords": ["Natural language processing", "Deep learning"], "conflicts": ["umontreal.ca", "iro.umontreal.ca", "google.com", "nyu.edu"], "authors": ["Junyoung Chung", "Sungjin Ahn", "Yoshua Bengio"], "authorids": ["junyoung.chung@umontreal.ca", "sungjin.ahn@umontreal.ca", "yoshua.bengio@umontreal.ca"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512724389, "id": "ICLR.cc/2017/conference/-/paper12/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper12/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper12/AnonReviewer4", "ICLR.cc/2017/conference/paper12/AnonReviewer3", "ICLR.cc/2017/conference/paper12/AnonReviewer1"], "reply": {"forum": "S1di0sfgl", "replyto": "S1di0sfgl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper12/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper12/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512724389}}}, {"tddate": null, "tmdate": 1481931250792, "tcdate": 1481931250792, "number": 3, "id": "BJcxqefEg", "invitation": "ICLR.cc/2017/conference/-/paper12/public/comment", "forum": "S1di0sfgl", "replyto": "rJXMSs-Ng", "signatures": ["~Junyoung_Chung1"], "readers": ["everyone"], "writers": ["~Junyoung_Chung1"], "content": {"title": "Missing references", "comment": "Thank you for pointing out the missing references, I will fix the citation accordingly. Apologies to the authors of related work that we've missed."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hierarchical Multiscale Recurrent Neural Networks", "abstract": "Learning both hierarchical and temporal representation has been among the long- standing challenges of recurrent neural networks. Multiscale recurrent neural networks have been considered as a promising approach to resolve this issue, yet there has been a lack of empirical evidence showing that this type of models can actually capture the temporal dependencies by discovering the latent hierarchical structure of the sequence. In this paper, we propose a novel multiscale approach, called the hierarchical multiscale recurrent neural network, that can capture the latent hierarchical structure in the sequence by encoding the temporal dependencies with different timescales using a novel update mechanism. We show some evidence that the proposed model can discover underlying hierarchical structure in the sequences without using explicit boundary information. We evaluate our proposed model on character-level language modelling and handwriting sequence generation.", "pdf": "/pdf/7b450081bcf1cf199061ff7348f79ed6d403ecd7.pdf", "TL;DR": "Propose a recurrent neural network architecture that can discover the underlying hierarchical structure in the temporal data.", "paperhash": "chung|hierarchical_multiscale_recurrent_neural_networks", "keywords": ["Natural language processing", "Deep learning"], "conflicts": ["umontreal.ca", "iro.umontreal.ca", "google.com", "nyu.edu"], "authors": ["Junyoung Chung", "Sungjin Ahn", "Yoshua Bengio"], "authorids": ["junyoung.chung@umontreal.ca", "sungjin.ahn@umontreal.ca", "yoshua.bengio@umontreal.ca"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287761573, "id": "ICLR.cc/2017/conference/-/paper12/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "S1di0sfgl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper12/reviewers", "ICLR.cc/2017/conference/paper12/areachairs"], "cdate": 1485287761573}}}, {"tddate": null, "tmdate": 1481909514712, "tcdate": 1481909514712, "number": 1, "id": "rJXMSs-Ng", "invitation": "ICLR.cc/2017/conference/-/paper12/official/review", "forum": "S1di0sfgl", "replyto": "S1di0sfgl", "signatures": ["ICLR.cc/2017/conference/paper12/AnonReviewer4"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper12/AnonReviewer4"], "content": {"title": "Good paper", "rating": "8: Top 50% of accepted papers, clear accept", "review": "This paper proposes a novel variant of recurrent networks that is able to learn the hierarchy of information in sequential data (e.g., character->word). Their approach does not require boundary information to segment the sequence in meaningful groups (like in Chung et al., 2016).\n\nTheir model is organized as a set of layers that aim at capturing the information form different \u201clevel of abstraction\u201d. The lowest level activate the upper one and decide when to update it based on a controller (or state cell, called c). A key feature of their model is that c is a discrete variable, allowing potentially fast inference time. However, this makes their model more challenging to learn, leading to the use of the straight-through estimator by Hinton, 2012. \n\nThe experiment section is thorough and their model obtain competitive performance on several challenging tasks. The qualitative results show also that their model can capture natural boundaries.\n\nOverall this paper presents a strong and novel model with promising experimental results.\n\n\n\nOn a minor note, I have few remarks/complaints about the writing and the related work:\n\n- In the introduction:\n\u201cOne of the key principles of learning in deep neural networks as well as in the human brain\u201d : please provide evidence for the \u201chuman brain\u201d part of this claim.\n\u201cFor modelling temporal data, the recent resurgence of recurrent neural networks (RNN) has led to remarkable advances\u201d I believe you re missing Mikolov et al. 2010 in the references.\n\u201cin spite of the fact that hierarchical multiscale structures naturally exist in many temporal data\u201d: missing reference to Lin et al., 1996\n\n- in the related work:\n\u201cA more recent model, the clockwork RNN (CW-RNN) (Koutn\u00edk et al., 2014) extends the hierarchicalRNN (El Hihi & Bengio, 1995)\u201d : It extends the NARX model of Lin et al. 1996, not the El Hihi & Bengio, 1995.\nWhile the above models focus on online prediction problems, where a prediction needs to be made\u2026\u201d: I believe there is a lot of missing references, in particular to Socher\u2019s work or older recursive networks.\n\u201cThe norm of the gradient is clipped with a threshold of 1 (Pascanu et al., 2012)\u201d: this is not the first work using gradient clipping. I believe it was introduced in Mikolov et al., 2010.\n\nMissing references:\n\u201cRecurrent neural network based language model.\u201d, Mikolov et al. 2010\n\u201cLearning long-term dependencies in NARX recurrent neural networks\u201d, Lin et al. 1996\n\u201cSequence labelling in structured domains with hierarchical recurrent neural networks\u201c, Fernandez et al. 2007\n\u201cLearning sequential tasks by incrementally adding  higher  orders\u201d, Ring, 1993\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hierarchical Multiscale Recurrent Neural Networks", "abstract": "Learning both hierarchical and temporal representation has been among the long- standing challenges of recurrent neural networks. Multiscale recurrent neural networks have been considered as a promising approach to resolve this issue, yet there has been a lack of empirical evidence showing that this type of models can actually capture the temporal dependencies by discovering the latent hierarchical structure of the sequence. In this paper, we propose a novel multiscale approach, called the hierarchical multiscale recurrent neural network, that can capture the latent hierarchical structure in the sequence by encoding the temporal dependencies with different timescales using a novel update mechanism. We show some evidence that the proposed model can discover underlying hierarchical structure in the sequences without using explicit boundary information. We evaluate our proposed model on character-level language modelling and handwriting sequence generation.", "pdf": "/pdf/7b450081bcf1cf199061ff7348f79ed6d403ecd7.pdf", "TL;DR": "Propose a recurrent neural network architecture that can discover the underlying hierarchical structure in the temporal data.", "paperhash": "chung|hierarchical_multiscale_recurrent_neural_networks", "keywords": ["Natural language processing", "Deep learning"], "conflicts": ["umontreal.ca", "iro.umontreal.ca", "google.com", "nyu.edu"], "authors": ["Junyoung Chung", "Sungjin Ahn", "Yoshua Bengio"], "authorids": ["junyoung.chung@umontreal.ca", "sungjin.ahn@umontreal.ca", "yoshua.bengio@umontreal.ca"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512724389, "id": "ICLR.cc/2017/conference/-/paper12/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper12/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper12/AnonReviewer4", "ICLR.cc/2017/conference/paper12/AnonReviewer3", "ICLR.cc/2017/conference/paper12/AnonReviewer1"], "reply": {"forum": "S1di0sfgl", "replyto": "S1di0sfgl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper12/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper12/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512724389}}}, {"tddate": null, "tmdate": 1481536491701, "tcdate": 1481536491695, "number": 2, "id": "rJ4gNg2Xx", "invitation": "ICLR.cc/2017/conference/-/paper12/public/comment", "forum": "S1di0sfgl", "replyto": "r1EFUlDme", "signatures": ["~Junyoung_Chung1"], "readers": ["everyone"], "writers": ["~Junyoung_Chung1"], "content": {"title": "Penn Treebank size and toolkit", "comment": "-Penn Treebank dataset consists of 5.8M characters. \n-We used Theano to implement our models.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hierarchical Multiscale Recurrent Neural Networks", "abstract": "Learning both hierarchical and temporal representation has been among the long- standing challenges of recurrent neural networks. Multiscale recurrent neural networks have been considered as a promising approach to resolve this issue, yet there has been a lack of empirical evidence showing that this type of models can actually capture the temporal dependencies by discovering the latent hierarchical structure of the sequence. In this paper, we propose a novel multiscale approach, called the hierarchical multiscale recurrent neural network, that can capture the latent hierarchical structure in the sequence by encoding the temporal dependencies with different timescales using a novel update mechanism. We show some evidence that the proposed model can discover underlying hierarchical structure in the sequences without using explicit boundary information. We evaluate our proposed model on character-level language modelling and handwriting sequence generation.", "pdf": "/pdf/7b450081bcf1cf199061ff7348f79ed6d403ecd7.pdf", "TL;DR": "Propose a recurrent neural network architecture that can discover the underlying hierarchical structure in the temporal data.", "paperhash": "chung|hierarchical_multiscale_recurrent_neural_networks", "keywords": ["Natural language processing", "Deep learning"], "conflicts": ["umontreal.ca", "iro.umontreal.ca", "google.com", "nyu.edu"], "authors": ["Junyoung Chung", "Sungjin Ahn", "Yoshua Bengio"], "authorids": ["junyoung.chung@umontreal.ca", "sungjin.ahn@umontreal.ca", "yoshua.bengio@umontreal.ca"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287761573, "id": "ICLR.cc/2017/conference/-/paper12/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "S1di0sfgl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper12/reviewers", "ICLR.cc/2017/conference/paper12/areachairs"], "cdate": 1485287761573}}}, {"tddate": null, "tmdate": 1481352614705, "tcdate": 1481352614695, "number": 1, "id": "H1k3BmtXx", "invitation": "ICLR.cc/2017/conference/-/paper12/public/comment", "forum": "S1di0sfgl", "replyto": "rkfCh_M7e", "signatures": ["~Junyoung_Chung1"], "readers": ["everyone"], "writers": ["~Junyoung_Chung1"], "content": {"title": "Computational savings", "comment": "By design, our architecture ensures that a layer performs less computations than its previous (lower) layer.\n\n-More specifically, in our model, each time step consists of two sub-phases of computation: (1) operation selection based on the hidden states of its previous layer at the same time or the previous time of the same layer and (2) the execution of the chosen operation (e.g., UPDATE, COPY, FLUSH). \n\n-Our architecture performs (2) only when (1) says it is necessary. For example, we do not go to (2) if (1) decides it is time to do COPY. Computation of (1) is usually much lighter than execution of the UPDATE or FLUSH operation.\n\n-Traditional RNN architectures, however, perform the UPDATE operation at every time step.\n\n-Also, by design of our model, an UPDATE at a layer can happen only after *at least* one UPDATE is performed at its previous layer. Thus, the number of UPDATE of a higher layer is always lower than a lower layer.\n\n-This computational saving can easily be obtained when batch size = 1 (e.g., at test time). However, implementing this in mini-batch training may require more engineering because each sequence in the batch may have different sequences of operations. Thus, we did not implemented this in our mini-batch training. Note that obtaining speed-up is a secondary advantage of the proposed method, the primary contribution being obtaining the hierarchical latent structure."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hierarchical Multiscale Recurrent Neural Networks", "abstract": "Learning both hierarchical and temporal representation has been among the long- standing challenges of recurrent neural networks. Multiscale recurrent neural networks have been considered as a promising approach to resolve this issue, yet there has been a lack of empirical evidence showing that this type of models can actually capture the temporal dependencies by discovering the latent hierarchical structure of the sequence. In this paper, we propose a novel multiscale approach, called the hierarchical multiscale recurrent neural network, that can capture the latent hierarchical structure in the sequence by encoding the temporal dependencies with different timescales using a novel update mechanism. We show some evidence that the proposed model can discover underlying hierarchical structure in the sequences without using explicit boundary information. We evaluate our proposed model on character-level language modelling and handwriting sequence generation.", "pdf": "/pdf/7b450081bcf1cf199061ff7348f79ed6d403ecd7.pdf", "TL;DR": "Propose a recurrent neural network architecture that can discover the underlying hierarchical structure in the temporal data.", "paperhash": "chung|hierarchical_multiscale_recurrent_neural_networks", "keywords": ["Natural language processing", "Deep learning"], "conflicts": ["umontreal.ca", "iro.umontreal.ca", "google.com", "nyu.edu"], "authors": ["Junyoung Chung", "Sungjin Ahn", "Yoshua Bengio"], "authorids": ["junyoung.chung@umontreal.ca", "sungjin.ahn@umontreal.ca", "yoshua.bengio@umontreal.ca"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287761573, "id": "ICLR.cc/2017/conference/-/paper12/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "S1di0sfgl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper12/reviewers", "ICLR.cc/2017/conference/paper12/areachairs"], "cdate": 1485287761573}}}, {"tddate": null, "tmdate": 1481209681059, "tcdate": 1481209468235, "number": 2, "id": "r1EFUlDme", "invitation": "ICLR.cc/2017/conference/-/paper12/pre-review/question", "forum": "S1di0sfgl", "replyto": "S1di0sfgl", "signatures": ["ICLR.cc/2017/conference/paper12/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper12/AnonReviewer1"], "content": {"title": "data size of Penn Treebank and toolkit", "question": "It's better to mention how many characters in Penn Treebank similar to the descriptions of Text8 and Hutter Prize Wikipedia tasks.\n\nWhat kind of toolkit are you using to implement it?\nSome toolkit does not allow us to touch a time-directional forloop easily, and it would be better information to describe how you implement things to which toolkit."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hierarchical Multiscale Recurrent Neural Networks", "abstract": "Learning both hierarchical and temporal representation has been among the long- standing challenges of recurrent neural networks. Multiscale recurrent neural networks have been considered as a promising approach to resolve this issue, yet there has been a lack of empirical evidence showing that this type of models can actually capture the temporal dependencies by discovering the latent hierarchical structure of the sequence. In this paper, we propose a novel multiscale approach, called the hierarchical multiscale recurrent neural network, that can capture the latent hierarchical structure in the sequence by encoding the temporal dependencies with different timescales using a novel update mechanism. We show some evidence that the proposed model can discover underlying hierarchical structure in the sequences without using explicit boundary information. We evaluate our proposed model on character-level language modelling and handwriting sequence generation.", "pdf": "/pdf/7b450081bcf1cf199061ff7348f79ed6d403ecd7.pdf", "TL;DR": "Propose a recurrent neural network architecture that can discover the underlying hierarchical structure in the temporal data.", "paperhash": "chung|hierarchical_multiscale_recurrent_neural_networks", "keywords": ["Natural language processing", "Deep learning"], "conflicts": ["umontreal.ca", "iro.umontreal.ca", "google.com", "nyu.edu"], "authors": ["Junyoung Chung", "Sungjin Ahn", "Yoshua Bengio"], "authorids": ["junyoung.chung@umontreal.ca", "sungjin.ahn@umontreal.ca", "yoshua.bengio@umontreal.ca"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481209468859, "id": "ICLR.cc/2017/conference/-/paper12/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper12/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper12/AnonReviewer3", "ICLR.cc/2017/conference/paper12/AnonReviewer1"], "reply": {"forum": "S1di0sfgl", "replyto": "S1di0sfgl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper12/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper12/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481209468859}}}, {"tddate": null, "tmdate": 1480916170360, "tcdate": 1480916170352, "number": 1, "id": "rkfCh_M7e", "invitation": "ICLR.cc/2017/conference/-/paper12/pre-review/question", "forum": "S1di0sfgl", "replyto": "S1di0sfgl", "signatures": ["ICLR.cc/2017/conference/paper12/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper12/AnonReviewer3"], "content": {"title": "HM-RNN", "question": "How much computational savings did the described architecture yield, in particular as the number of LSTM layers increased?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hierarchical Multiscale Recurrent Neural Networks", "abstract": "Learning both hierarchical and temporal representation has been among the long- standing challenges of recurrent neural networks. Multiscale recurrent neural networks have been considered as a promising approach to resolve this issue, yet there has been a lack of empirical evidence showing that this type of models can actually capture the temporal dependencies by discovering the latent hierarchical structure of the sequence. In this paper, we propose a novel multiscale approach, called the hierarchical multiscale recurrent neural network, that can capture the latent hierarchical structure in the sequence by encoding the temporal dependencies with different timescales using a novel update mechanism. We show some evidence that the proposed model can discover underlying hierarchical structure in the sequences without using explicit boundary information. We evaluate our proposed model on character-level language modelling and handwriting sequence generation.", "pdf": "/pdf/7b450081bcf1cf199061ff7348f79ed6d403ecd7.pdf", "TL;DR": "Propose a recurrent neural network architecture that can discover the underlying hierarchical structure in the temporal data.", "paperhash": "chung|hierarchical_multiscale_recurrent_neural_networks", "keywords": ["Natural language processing", "Deep learning"], "conflicts": ["umontreal.ca", "iro.umontreal.ca", "google.com", "nyu.edu"], "authors": ["Junyoung Chung", "Sungjin Ahn", "Yoshua Bengio"], "authorids": ["junyoung.chung@umontreal.ca", "sungjin.ahn@umontreal.ca", "yoshua.bengio@umontreal.ca"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481209468859, "id": "ICLR.cc/2017/conference/-/paper12/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper12/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper12/AnonReviewer3", "ICLR.cc/2017/conference/paper12/AnonReviewer1"], "reply": {"forum": "S1di0sfgl", "replyto": "S1di0sfgl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper12/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper12/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481209468859}}}], "count": 12}