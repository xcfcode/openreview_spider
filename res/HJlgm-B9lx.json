{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396450868, "tcdate": 1486396450868, "number": 1, "id": "H1s73z8Ol", "invitation": "ICLR.cc/2017/conference/-/paper236/acceptance", "forum": "HJlgm-B9lx", "replyto": "HJlgm-B9lx", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "The consensus amongst reviewers' was that this paper, incorporating global context into classification, is not ready for publication. It provides no novelty over similar methods. The evaluation did not convince most of the reviewers. The paper seems peppered with unjustified and (as rather bluntly, but accurately, put by one reviewer) unscientific claims. Disappointingly, the authors did not respond to pre-review questions. Perhaps more understandably, they did not respond to the uniformly negative reviews of their paper to defend it. I see no reason to diverge from the reviewers' recommendation, and advocate rejection of this paper."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Understand: Incorporating Local Contexts with Global Attention for Sentiment Classification", "abstract": "Recurrent neural networks have shown their ability to construct sentence or paragraph representations. Variants such as LSTM overcome the problem of vanishing gradients to some degree, thus being able to model long-time dependency. Still, these recurrent based models lack the ability of capturing complex semantic compositions. To address this problem, we propose a model which can incorporate local contexts with the guide of global context attention. Both the local and global contexts are obtained through LSTM networks. The working procedure of this model is just like how we human beings read a text and then answer a related question. Empirical studies show that the proposed model can achieve state of the art on some benchmark datasets. Attention visualization also verifies our intuition. Meanwhile, this model does not need pretrained embeddings to get good results.", "pdf": "/pdf/a3ac57b00f02fd0352d0f8a972afa1a7d1169209.pdf", "TL;DR": "a global-local mutually representation-learning attention model for sentiment analysis", "paperhash": "yuan|learning_to_understand_incorporating_local_contexts_with_global_attention_for_sentiment_classification", "keywords": ["Natural language processing", "Deep learning", "Applications"], "conflicts": ["mails.tsinghua.edu.cn", "tsinghua.edu.cn"], "authors": ["Zhigang Yuan", "Yuting Hu", "Yongfeng Huang"], "authorids": ["yuanzg14@mails.tsinghua.edu.cn", "hu-yt12@mails.tsinghua.edu.cn", "yfhuang@tsinghua.edu.cn"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396451377, "id": "ICLR.cc/2017/conference/-/paper236/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "HJlgm-B9lx", "replyto": "HJlgm-B9lx", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396451377}}}, {"tddate": null, "tmdate": 1482554860693, "tcdate": 1482554860693, "number": 3, "id": "SyHgRujVl", "invitation": "ICLR.cc/2017/conference/-/paper236/official/review", "forum": "HJlgm-B9lx", "replyto": "HJlgm-B9lx", "signatures": ["ICLR.cc/2017/conference/paper236/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper236/AnonReviewer3"], "content": {"title": "below acceptance threshold", "rating": "3: Clear rejection", "review": "The authors did not bother responding or fixing any of the pre-review comments. Hence I repeat here:\n\nPlease do not make incredibly unscientific statements like this one:\n\"The working procedure of this model is just like how we human beings read a text and then answer a related question. \"\nReally, \"humans beings\" have an LSTM like model to read a text? Can you cite an actual neuroscience paper for such a claim? The answer is no, so please delete such statements from future drafts.\n\nGenerally, your experiments are about simple classification and the methods you're competing against are simple models like NB-SVM. So I would change the title, abstract ad introduction accordingly and not attempt hyperbole like \"Learning to Understand\" in the title.\n\nLastly, your attention level approach seems similar to dynamic memory networks by Kumar et al. they also have experiments for sentiment and it would be interesting to understand the differences to your model and compare to them.\n\nOther reviewers included further missing related work and fitting this paper into the context of current literature.\nGiven that no efforts were made to fix the pre-review questions and feedback, I doubt this will become ready in time for publication.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Understand: Incorporating Local Contexts with Global Attention for Sentiment Classification", "abstract": "Recurrent neural networks have shown their ability to construct sentence or paragraph representations. Variants such as LSTM overcome the problem of vanishing gradients to some degree, thus being able to model long-time dependency. Still, these recurrent based models lack the ability of capturing complex semantic compositions. To address this problem, we propose a model which can incorporate local contexts with the guide of global context attention. Both the local and global contexts are obtained through LSTM networks. The working procedure of this model is just like how we human beings read a text and then answer a related question. Empirical studies show that the proposed model can achieve state of the art on some benchmark datasets. Attention visualization also verifies our intuition. Meanwhile, this model does not need pretrained embeddings to get good results.", "pdf": "/pdf/a3ac57b00f02fd0352d0f8a972afa1a7d1169209.pdf", "TL;DR": "a global-local mutually representation-learning attention model for sentiment analysis", "paperhash": "yuan|learning_to_understand_incorporating_local_contexts_with_global_attention_for_sentiment_classification", "keywords": ["Natural language processing", "Deep learning", "Applications"], "conflicts": ["mails.tsinghua.edu.cn", "tsinghua.edu.cn"], "authors": ["Zhigang Yuan", "Yuting Hu", "Yongfeng Huang"], "authorids": ["yuanzg14@mails.tsinghua.edu.cn", "hu-yt12@mails.tsinghua.edu.cn", "yfhuang@tsinghua.edu.cn"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482554861281, "id": "ICLR.cc/2017/conference/-/paper236/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper236/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper236/AnonReviewer1", "ICLR.cc/2017/conference/paper236/AnonReviewer2", "ICLR.cc/2017/conference/paper236/AnonReviewer3"], "reply": {"forum": "HJlgm-B9lx", "replyto": "HJlgm-B9lx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper236/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper236/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482554861281}}}, {"tddate": null, "tmdate": 1482147884271, "tcdate": 1482147760981, "number": 2, "id": "HytnPSSNx", "invitation": "ICLR.cc/2017/conference/-/paper236/official/review", "forum": "HJlgm-B9lx", "replyto": "HJlgm-B9lx", "signatures": ["ICLR.cc/2017/conference/paper236/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper236/AnonReviewer2"], "content": {"title": "official review", "rating": "3: Clear rejection", "review": "The paper proposes to enhance the attention mechanism for sentiment classification by using global context computed by a Bi-LSTM. The proposed models outperform many existing models in the literature on 3 sentiment analysis datasets. \n\nThe key idea of using Bi-LSTM to compute global context for attention is actually not novel, as proposed several times in the literature, e.g., Luong et al (2015) and Shen & Lee (2016). Especially, Luong et al (2015) already proposed to combine global context with local context for attention.\n\nRegarding to the experiments, of course it would be nice if the model can work well without the need of tricks like dropout or pre-trained word embeddings. However, it would be even better if the model can work well using those tricks. The authors should show results of the models using those tricks and compare them to the results in the literature.  \n\n\nRef:\nLuong et al. Effective Approaches to Attention-based Neural Machine Translation. EMNLP 2015", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Understand: Incorporating Local Contexts with Global Attention for Sentiment Classification", "abstract": "Recurrent neural networks have shown their ability to construct sentence or paragraph representations. Variants such as LSTM overcome the problem of vanishing gradients to some degree, thus being able to model long-time dependency. Still, these recurrent based models lack the ability of capturing complex semantic compositions. To address this problem, we propose a model which can incorporate local contexts with the guide of global context attention. Both the local and global contexts are obtained through LSTM networks. The working procedure of this model is just like how we human beings read a text and then answer a related question. Empirical studies show that the proposed model can achieve state of the art on some benchmark datasets. Attention visualization also verifies our intuition. Meanwhile, this model does not need pretrained embeddings to get good results.", "pdf": "/pdf/a3ac57b00f02fd0352d0f8a972afa1a7d1169209.pdf", "TL;DR": "a global-local mutually representation-learning attention model for sentiment analysis", "paperhash": "yuan|learning_to_understand_incorporating_local_contexts_with_global_attention_for_sentiment_classification", "keywords": ["Natural language processing", "Deep learning", "Applications"], "conflicts": ["mails.tsinghua.edu.cn", "tsinghua.edu.cn"], "authors": ["Zhigang Yuan", "Yuting Hu", "Yongfeng Huang"], "authorids": ["yuanzg14@mails.tsinghua.edu.cn", "hu-yt12@mails.tsinghua.edu.cn", "yfhuang@tsinghua.edu.cn"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482554861281, "id": "ICLR.cc/2017/conference/-/paper236/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper236/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper236/AnonReviewer1", "ICLR.cc/2017/conference/paper236/AnonReviewer2", "ICLR.cc/2017/conference/paper236/AnonReviewer3"], "reply": {"forum": "HJlgm-B9lx", "replyto": "HJlgm-B9lx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper236/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper236/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482554861281}}}, {"tddate": null, "tmdate": 1482128527830, "tcdate": 1482128527830, "number": 1, "id": "Bkd9nxHEx", "invitation": "ICLR.cc/2017/conference/-/paper236/official/review", "forum": "HJlgm-B9lx", "replyto": "HJlgm-B9lx", "signatures": ["ICLR.cc/2017/conference/paper236/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper236/AnonReviewer1"], "content": {"title": "feedback", "rating": "4: Ok but not good enough - rejection", "review": "This paper presents a hierarchical attention-based method for document classification. \nThe main idea is to first run a bidirectional LSTM to get global context vector, and then run another attention-based bidirectional LSTM that uses the final hidden state from the first pass to weight local context vectors (TS-ATT). \nA simpler architecture that removes the first LSTM and uses the output of the second LSTM as the global context vector is also proposed (SS-ATT). \nExperiments on three datasets are presented, however the results are mostly not state-of-the-art.\n\nI think the idea is nice, but the experiment results are not convincing enough to justify this new model architecture. \nWhy is your Yelp 2013 dataset smaller than the original Tang et al, 2015 paper that has ~300k documents? \nI noticed your other datasets are also quite small. Is it because your model is difficult to scale to large datasets?\nYou should also include results from Tang et al., 2015 in Table 2 that achieves 65.1% accuracy on Yelp 2013 (why is your number so much lower?)\nI also suggest removing phrases such as \"Learning to Understand\" when presenting their model.\nOverall, I think that this submission is a better fit for the workshop.\n\nMinor comments:\n- gloal -> global\n- Not needing a pretrained embeddings, while of course nice, is not that big of a deal. Various models will work just fine without pretrained embeddings.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Understand: Incorporating Local Contexts with Global Attention for Sentiment Classification", "abstract": "Recurrent neural networks have shown their ability to construct sentence or paragraph representations. Variants such as LSTM overcome the problem of vanishing gradients to some degree, thus being able to model long-time dependency. Still, these recurrent based models lack the ability of capturing complex semantic compositions. To address this problem, we propose a model which can incorporate local contexts with the guide of global context attention. Both the local and global contexts are obtained through LSTM networks. The working procedure of this model is just like how we human beings read a text and then answer a related question. Empirical studies show that the proposed model can achieve state of the art on some benchmark datasets. Attention visualization also verifies our intuition. Meanwhile, this model does not need pretrained embeddings to get good results.", "pdf": "/pdf/a3ac57b00f02fd0352d0f8a972afa1a7d1169209.pdf", "TL;DR": "a global-local mutually representation-learning attention model for sentiment analysis", "paperhash": "yuan|learning_to_understand_incorporating_local_contexts_with_global_attention_for_sentiment_classification", "keywords": ["Natural language processing", "Deep learning", "Applications"], "conflicts": ["mails.tsinghua.edu.cn", "tsinghua.edu.cn"], "authors": ["Zhigang Yuan", "Yuting Hu", "Yongfeng Huang"], "authorids": ["yuanzg14@mails.tsinghua.edu.cn", "hu-yt12@mails.tsinghua.edu.cn", "yfhuang@tsinghua.edu.cn"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482554861281, "id": "ICLR.cc/2017/conference/-/paper236/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper236/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper236/AnonReviewer1", "ICLR.cc/2017/conference/paper236/AnonReviewer2", "ICLR.cc/2017/conference/paper236/AnonReviewer3"], "reply": {"forum": "HJlgm-B9lx", "replyto": "HJlgm-B9lx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper236/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper236/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482554861281}}}, {"tddate": null, "tmdate": 1480688619560, "tcdate": 1480688619556, "number": 3, "id": "Hk4eV-kXx", "invitation": "ICLR.cc/2017/conference/-/paper236/pre-review/question", "forum": "HJlgm-B9lx", "replyto": "HJlgm-B9lx", "signatures": ["ICLR.cc/2017/conference/paper236/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper236/AnonReviewer2"], "content": {"title": "Do you have more experimental results?", "question": "1. Of course it would be nice if the model can work well without the need of tricks like dropout or pre-trained word embeddings. However, it would be even better if the model can work well together with those tricks. Do you have any experimental results with using those tricks? \n\n2. I'm wondering whether the good results come from the global contexts or not. Do you have results without using global contexts? "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Understand: Incorporating Local Contexts with Global Attention for Sentiment Classification", "abstract": "Recurrent neural networks have shown their ability to construct sentence or paragraph representations. Variants such as LSTM overcome the problem of vanishing gradients to some degree, thus being able to model long-time dependency. Still, these recurrent based models lack the ability of capturing complex semantic compositions. To address this problem, we propose a model which can incorporate local contexts with the guide of global context attention. Both the local and global contexts are obtained through LSTM networks. The working procedure of this model is just like how we human beings read a text and then answer a related question. Empirical studies show that the proposed model can achieve state of the art on some benchmark datasets. Attention visualization also verifies our intuition. Meanwhile, this model does not need pretrained embeddings to get good results.", "pdf": "/pdf/a3ac57b00f02fd0352d0f8a972afa1a7d1169209.pdf", "TL;DR": "a global-local mutually representation-learning attention model for sentiment analysis", "paperhash": "yuan|learning_to_understand_incorporating_local_contexts_with_global_attention_for_sentiment_classification", "keywords": ["Natural language processing", "Deep learning", "Applications"], "conflicts": ["mails.tsinghua.edu.cn", "tsinghua.edu.cn"], "authors": ["Zhigang Yuan", "Yuting Hu", "Yongfeng Huang"], "authorids": ["yuanzg14@mails.tsinghua.edu.cn", "hu-yt12@mails.tsinghua.edu.cn", "yfhuang@tsinghua.edu.cn"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959389597, "id": "ICLR.cc/2017/conference/-/paper236/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper236/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper236/AnonReviewer1", "ICLR.cc/2017/conference/paper236/AnonReviewer3", "ICLR.cc/2017/conference/paper236/AnonReviewer2"], "reply": {"forum": "HJlgm-B9lx", "replyto": "HJlgm-B9lx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper236/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper236/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959389597}}}, {"tddate": null, "tmdate": 1480639762557, "tcdate": 1480639762552, "number": 2, "id": "BJozSSCGg", "invitation": "ICLR.cc/2017/conference/-/paper236/pre-review/question", "forum": "HJlgm-B9lx", "replyto": "HJlgm-B9lx", "signatures": ["ICLR.cc/2017/conference/paper236/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper236/AnonReviewer3"], "content": {"title": "feedback", "question": "Please do not make incredibly unscientific statements like this one:\n\"The working procedure of this model is just like how we human beings read a text and then answer a related question. \"\nReally, \"humans beings\" have an LSTM like model to read a text? Can you cite an actual neuroscience paper for such a ridiculous claim? The answer is no, so please delete such statements from future drafts.\n\nGenerally, your experiments are about simple classification and the methods you're competing against are simple models like NB-SVM. So I would change the title, abstract ad introduction accordingly and not attempt hyperbole like \"Learning to Understand\" in the title.\n\nLastly, your attention level approach seems similar to dynamic memory networks by kumar et al. they also have experiments for sentiment and itd be interesting to understand the differences to your model and compare to them."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Understand: Incorporating Local Contexts with Global Attention for Sentiment Classification", "abstract": "Recurrent neural networks have shown their ability to construct sentence or paragraph representations. Variants such as LSTM overcome the problem of vanishing gradients to some degree, thus being able to model long-time dependency. Still, these recurrent based models lack the ability of capturing complex semantic compositions. To address this problem, we propose a model which can incorporate local contexts with the guide of global context attention. Both the local and global contexts are obtained through LSTM networks. The working procedure of this model is just like how we human beings read a text and then answer a related question. Empirical studies show that the proposed model can achieve state of the art on some benchmark datasets. Attention visualization also verifies our intuition. Meanwhile, this model does not need pretrained embeddings to get good results.", "pdf": "/pdf/a3ac57b00f02fd0352d0f8a972afa1a7d1169209.pdf", "TL;DR": "a global-local mutually representation-learning attention model for sentiment analysis", "paperhash": "yuan|learning_to_understand_incorporating_local_contexts_with_global_attention_for_sentiment_classification", "keywords": ["Natural language processing", "Deep learning", "Applications"], "conflicts": ["mails.tsinghua.edu.cn", "tsinghua.edu.cn"], "authors": ["Zhigang Yuan", "Yuting Hu", "Yongfeng Huang"], "authorids": ["yuanzg14@mails.tsinghua.edu.cn", "hu-yt12@mails.tsinghua.edu.cn", "yfhuang@tsinghua.edu.cn"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959389597, "id": "ICLR.cc/2017/conference/-/paper236/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper236/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper236/AnonReviewer1", "ICLR.cc/2017/conference/paper236/AnonReviewer3", "ICLR.cc/2017/conference/paper236/AnonReviewer2"], "reply": {"forum": "HJlgm-B9lx", "replyto": "HJlgm-B9lx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper236/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper236/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959389597}}}, {"tddate": null, "tmdate": 1480620262732, "tcdate": 1480620262728, "number": 1, "id": "rk1eKlAzg", "invitation": "ICLR.cc/2017/conference/-/paper236/pre-review/question", "forum": "HJlgm-B9lx", "replyto": "HJlgm-B9lx", "signatures": ["ICLR.cc/2017/conference/paper236/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper236/AnonReviewer1"], "content": {"title": "prereview questions", "question": "- Could you please comment on the differences and similarities between your proposed model and Yang et al. NAACL, 2016 (Hierarchical Attention Networks for Document Classification)?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Understand: Incorporating Local Contexts with Global Attention for Sentiment Classification", "abstract": "Recurrent neural networks have shown their ability to construct sentence or paragraph representations. Variants such as LSTM overcome the problem of vanishing gradients to some degree, thus being able to model long-time dependency. Still, these recurrent based models lack the ability of capturing complex semantic compositions. To address this problem, we propose a model which can incorporate local contexts with the guide of global context attention. Both the local and global contexts are obtained through LSTM networks. The working procedure of this model is just like how we human beings read a text and then answer a related question. Empirical studies show that the proposed model can achieve state of the art on some benchmark datasets. Attention visualization also verifies our intuition. Meanwhile, this model does not need pretrained embeddings to get good results.", "pdf": "/pdf/a3ac57b00f02fd0352d0f8a972afa1a7d1169209.pdf", "TL;DR": "a global-local mutually representation-learning attention model for sentiment analysis", "paperhash": "yuan|learning_to_understand_incorporating_local_contexts_with_global_attention_for_sentiment_classification", "keywords": ["Natural language processing", "Deep learning", "Applications"], "conflicts": ["mails.tsinghua.edu.cn", "tsinghua.edu.cn"], "authors": ["Zhigang Yuan", "Yuting Hu", "Yongfeng Huang"], "authorids": ["yuanzg14@mails.tsinghua.edu.cn", "hu-yt12@mails.tsinghua.edu.cn", "yfhuang@tsinghua.edu.cn"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959389597, "id": "ICLR.cc/2017/conference/-/paper236/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper236/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper236/AnonReviewer1", "ICLR.cc/2017/conference/paper236/AnonReviewer3", "ICLR.cc/2017/conference/paper236/AnonReviewer2"], "reply": {"forum": "HJlgm-B9lx", "replyto": "HJlgm-B9lx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper236/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper236/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959389597}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1478280205559, "tcdate": 1478279448500, "number": 236, "id": "HJlgm-B9lx", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "HJlgm-B9lx", "signatures": ["~Zhigang_Yuan1"], "readers": ["everyone"], "content": {"title": "Learning to Understand: Incorporating Local Contexts with Global Attention for Sentiment Classification", "abstract": "Recurrent neural networks have shown their ability to construct sentence or paragraph representations. Variants such as LSTM overcome the problem of vanishing gradients to some degree, thus being able to model long-time dependency. Still, these recurrent based models lack the ability of capturing complex semantic compositions. To address this problem, we propose a model which can incorporate local contexts with the guide of global context attention. Both the local and global contexts are obtained through LSTM networks. The working procedure of this model is just like how we human beings read a text and then answer a related question. Empirical studies show that the proposed model can achieve state of the art on some benchmark datasets. Attention visualization also verifies our intuition. Meanwhile, this model does not need pretrained embeddings to get good results.", "pdf": "/pdf/a3ac57b00f02fd0352d0f8a972afa1a7d1169209.pdf", "TL;DR": "a global-local mutually representation-learning attention model for sentiment analysis", "paperhash": "yuan|learning_to_understand_incorporating_local_contexts_with_global_attention_for_sentiment_classification", "keywords": ["Natural language processing", "Deep learning", "Applications"], "conflicts": ["mails.tsinghua.edu.cn", "tsinghua.edu.cn"], "authors": ["Zhigang Yuan", "Yuting Hu", "Yongfeng Huang"], "authorids": ["yuanzg14@mails.tsinghua.edu.cn", "hu-yt12@mails.tsinghua.edu.cn", "yfhuang@tsinghua.edu.cn"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}], "count": 8}