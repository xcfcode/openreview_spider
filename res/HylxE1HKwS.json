{"notes": [{"id": "HylxE1HKwS", "original": "rye_3cndDB", "number": 1641, "cdate": 1569439528008, "ddate": null, "tcdate": 1569439528008, "tmdate": 1588193016433, "tddate": null, "forum": "HylxE1HKwS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["hancai@mit.edu", "ganchuang1990@gmail.com", "usedtobe@mit.edu", "zhangzk@mit.edu", "songhan@mit.edu"], "title": "Once-for-All: Train One Network and Specialize it for Efficient Deployment", "authors": ["Han Cai", "Chuang Gan", "Tianzhe Wang", "Zhekai Zhang", "Song Han"], "pdf": "/pdf/cbef2d6e1f85be9ef9e5ed7a525a648b463fc6b5.pdf", "TL;DR": "We introduce techniques to train a single once-for-all network that fits many hardware platforms.", "abstract": "We address the challenging problem of efficient inference across many devices and resource constraints, especially on edge devices.  Conventional approaches either manually design or use neural architecture search (NAS) to find a specialized neural network and train it from scratch for each case, which is computationally prohibitive (causing $CO_2$ emission as much as 5 cars' lifetime) thus unscalable. In this work, we propose to train a once-for-all (OFA) network that supports diverse architectural settings by decoupling training and search, to reduce the cost. We can quickly get a specialized sub-network by selecting from the OFA network without additional training. To efficiently train OFA networks, we also propose a novel progressive shrinking algorithm, a generalized pruning method that reduces the model size across many more dimensions than pruning (depth, width, kernel size, and resolution). It can obtain a surprisingly large number of sub-networks ($> 10^{19}$) that can fit different hardware platforms and latency constraints while maintaining the same level of accuracy as training independently. On diverse edge devices, OFA consistently outperforms state-of-the-art (SOTA) NAS methods (up to 4.0% ImageNet top1 accuracy improvement over MobileNetV3, or same accuracy but 1.5x faster than MobileNetV3, 2.6x faster than EfficientNet w.r.t measured latency) while reducing many orders of magnitude GPU hours and $CO_2$ emission. In particular, OFA achieves a new SOTA 80.0% ImageNet top-1 accuracy under the mobile setting ($<$600M MACs). OFA is the winning solution for the 3rd Low Power Computer Vision Challenge (LPCVC), DSP classification track and the 4th LPCVC, both classification track and detection track. Code and  50 pre-trained models (for many devices & many latency constraints) are released at https://github.com/mit-han-lab/once-for-all. ", "keywords": ["Efficient Deep Learning", "Specialized Neural Network Architecture", "AutoML"], "paperhash": "cai|onceforall_train_one_network_and_specialize_it_for_efficient_deployment", "code": "https://github.com/mit-han-lab/once-for-all", "_bibtex": "@inproceedings{\nCai2020Once-for-All:,\ntitle={Once-for-All: Train One Network and Specialize it for Efficient Deployment},\nauthor={Han Cai and Chuang Gan and Tianzhe Wang and Zhekai Zhang and Song Han},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HylxE1HKwS}\n}", "original_pdf": "/attachment/2a60bd911a60e9c9b6aa95cd711966c0bc8af57a.pdf"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 13, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "rxBB1m4Kk6", "original": null, "number": 1, "cdate": 1576798728583, "ddate": null, "tcdate": 1576798728583, "tmdate": 1576800907960, "tddate": null, "forum": "HylxE1HKwS", "replyto": "HylxE1HKwS", "invitation": "ICLR.cc/2020/Conference/Paper1641/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "The authors propose a new method for neural architecture search, except it's not exactly that because model training is separated from architecture, which is the main point of the paper. Once this network is trained, sub-networks can be distilled from it and used for specific tasks.\n\nThe paper as submitted missed certain details, but after this was pointed out by reviewers the details were satisfactorily described by the authors. \n\nThe idea of the paper is original and interesting. The paper is correct and, after the revisions by authors, complete. In my view, this is sufficient for acceptance.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["hancai@mit.edu", "ganchuang1990@gmail.com", "usedtobe@mit.edu", "zhangzk@mit.edu", "songhan@mit.edu"], "title": "Once-for-All: Train One Network and Specialize it for Efficient Deployment", "authors": ["Han Cai", "Chuang Gan", "Tianzhe Wang", "Zhekai Zhang", "Song Han"], "pdf": "/pdf/cbef2d6e1f85be9ef9e5ed7a525a648b463fc6b5.pdf", "TL;DR": "We introduce techniques to train a single once-for-all network that fits many hardware platforms.", "abstract": "We address the challenging problem of efficient inference across many devices and resource constraints, especially on edge devices.  Conventional approaches either manually design or use neural architecture search (NAS) to find a specialized neural network and train it from scratch for each case, which is computationally prohibitive (causing $CO_2$ emission as much as 5 cars' lifetime) thus unscalable. In this work, we propose to train a once-for-all (OFA) network that supports diverse architectural settings by decoupling training and search, to reduce the cost. We can quickly get a specialized sub-network by selecting from the OFA network without additional training. To efficiently train OFA networks, we also propose a novel progressive shrinking algorithm, a generalized pruning method that reduces the model size across many more dimensions than pruning (depth, width, kernel size, and resolution). It can obtain a surprisingly large number of sub-networks ($> 10^{19}$) that can fit different hardware platforms and latency constraints while maintaining the same level of accuracy as training independently. On diverse edge devices, OFA consistently outperforms state-of-the-art (SOTA) NAS methods (up to 4.0% ImageNet top1 accuracy improvement over MobileNetV3, or same accuracy but 1.5x faster than MobileNetV3, 2.6x faster than EfficientNet w.r.t measured latency) while reducing many orders of magnitude GPU hours and $CO_2$ emission. In particular, OFA achieves a new SOTA 80.0% ImageNet top-1 accuracy under the mobile setting ($<$600M MACs). OFA is the winning solution for the 3rd Low Power Computer Vision Challenge (LPCVC), DSP classification track and the 4th LPCVC, both classification track and detection track. Code and  50 pre-trained models (for many devices & many latency constraints) are released at https://github.com/mit-han-lab/once-for-all. ", "keywords": ["Efficient Deep Learning", "Specialized Neural Network Architecture", "AutoML"], "paperhash": "cai|onceforall_train_one_network_and_specialize_it_for_efficient_deployment", "code": "https://github.com/mit-han-lab/once-for-all", "_bibtex": "@inproceedings{\nCai2020Once-for-All:,\ntitle={Once-for-All: Train One Network and Specialize it for Efficient Deployment},\nauthor={Han Cai and Chuang Gan and Tianzhe Wang and Zhekai Zhang and Song Han},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HylxE1HKwS}\n}", "original_pdf": "/attachment/2a60bd911a60e9c9b6aa95cd711966c0bc8af57a.pdf"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "HylxE1HKwS", "replyto": "HylxE1HKwS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795704768, "tmdate": 1576800252414, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1641/-/Decision"}}}, {"id": "Bygt_gUQYr", "original": null, "number": 1, "cdate": 1571147889197, "ddate": null, "tcdate": 1571147889197, "tmdate": 1574072490015, "tddate": null, "forum": "HylxE1HKwS", "replyto": "HylxE1HKwS", "invitation": "ICLR.cc/2020/Conference/Paper1641/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #2", "review": "In this papers, the authors learn a Once-for-all net. This starts as a big neural network which is trained normally (albeit with input images of different resolutions). It is then fine-tuned while sampling sub-networks with progressively smaller kernels, then lower depth, then width (while still sampling larger networks occasionally, as it reads). This results in a network from which one can extract sub-networks for various resource constraints (latency, memory etc.) that perform well without a need for retraining.\n\nThis paper is well written, and the results are very good. However there are serious problems that need addressing.\n\nThe method as described *is not reproducible*. The scheduling of sampling subnetworks is alluded to on page 4, and that's it. It is essential that the authors include their exact subnet sampling schedule e.g. as pseudocode with hyperparameters. There is no point doing good work if other researchers cannot build off it. \n\nOn another reproducibility note, as far as I can tell, the original model isn't given. There would be no harm in adding this to the appendix. \n\nFigure 1 is misleading, as we don't find out until later in the paper that Once For All #25 means that each of these points was finetuned for a further 25 epochs (which on ImageNet is non-trivial). This defeats the narrative of the paper (once-for-all plus some fine-tuning isn't exactly once-for-all).\n\nIs there a reason why the progressive shrinking goes resolution->kernel->depth->width? Was this just the permutation that worked best? I would be curious as to why this is.\n\nFor elastic width, I wasn't sure why the \"channel sorting operation preserves the accuracy of larger sub-networks\". Could you please elaborate?\n\nKudos on adding CO2 emissions in Table 2, I hope this gets reported more often.\n\nIn the introduction, the authors talk about iPhones and then the hardware considered is Samsung and Google. A minor note, but it seems inconsistent.\n\nAnother minor note, in Table 2, (Strubell et al) should be out of the brackets, as it is part of the sentence.\n\nGiven that there are 10^19 subnetworks that can be sampled, it would be nice to see more than 3-4 appear on a plot. This makes it seem like they might have been cherry-picked. Sampling a few 100/1000 subnets and producing some Pareto curves would be both interesting and insightful.\n\nPros\n-------\n- Good results\n- Well written\n- Neat idea\n\nCons\n-------\n- Training details are obfuscated. This paper should not be accepted without them.\n- Very few subnetworks of the vast quantity that exist are observed.\n\nIn conclusion, I am giving this paper a weak reject, as it is currently impossible to reproduce, and as such, is of no use to the community. If the authors remedy this I will gladly raise my score.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"}, "signatures": ["ICLR.cc/2020/Conference/Paper1641/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1641/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["hancai@mit.edu", "ganchuang1990@gmail.com", "usedtobe@mit.edu", "zhangzk@mit.edu", "songhan@mit.edu"], "title": "Once-for-All: Train One Network and Specialize it for Efficient Deployment", "authors": ["Han Cai", "Chuang Gan", "Tianzhe Wang", "Zhekai Zhang", "Song Han"], "pdf": "/pdf/cbef2d6e1f85be9ef9e5ed7a525a648b463fc6b5.pdf", "TL;DR": "We introduce techniques to train a single once-for-all network that fits many hardware platforms.", "abstract": "We address the challenging problem of efficient inference across many devices and resource constraints, especially on edge devices.  Conventional approaches either manually design or use neural architecture search (NAS) to find a specialized neural network and train it from scratch for each case, which is computationally prohibitive (causing $CO_2$ emission as much as 5 cars' lifetime) thus unscalable. In this work, we propose to train a once-for-all (OFA) network that supports diverse architectural settings by decoupling training and search, to reduce the cost. We can quickly get a specialized sub-network by selecting from the OFA network without additional training. To efficiently train OFA networks, we also propose a novel progressive shrinking algorithm, a generalized pruning method that reduces the model size across many more dimensions than pruning (depth, width, kernel size, and resolution). It can obtain a surprisingly large number of sub-networks ($> 10^{19}$) that can fit different hardware platforms and latency constraints while maintaining the same level of accuracy as training independently. On diverse edge devices, OFA consistently outperforms state-of-the-art (SOTA) NAS methods (up to 4.0% ImageNet top1 accuracy improvement over MobileNetV3, or same accuracy but 1.5x faster than MobileNetV3, 2.6x faster than EfficientNet w.r.t measured latency) while reducing many orders of magnitude GPU hours and $CO_2$ emission. In particular, OFA achieves a new SOTA 80.0% ImageNet top-1 accuracy under the mobile setting ($<$600M MACs). OFA is the winning solution for the 3rd Low Power Computer Vision Challenge (LPCVC), DSP classification track and the 4th LPCVC, both classification track and detection track. Code and  50 pre-trained models (for many devices & many latency constraints) are released at https://github.com/mit-han-lab/once-for-all. ", "keywords": ["Efficient Deep Learning", "Specialized Neural Network Architecture", "AutoML"], "paperhash": "cai|onceforall_train_one_network_and_specialize_it_for_efficient_deployment", "code": "https://github.com/mit-han-lab/once-for-all", "_bibtex": "@inproceedings{\nCai2020Once-for-All:,\ntitle={Once-for-All: Train One Network and Specialize it for Efficient Deployment},\nauthor={Han Cai and Chuang Gan and Tianzhe Wang and Zhekai Zhang and Song Han},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HylxE1HKwS}\n}", "original_pdf": "/attachment/2a60bd911a60e9c9b6aa95cd711966c0bc8af57a.pdf"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HylxE1HKwS", "replyto": "HylxE1HKwS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1641/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1641/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575699813277, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1641/Reviewers"], "noninvitees": [], "tcdate": 1570237734418, "tmdate": 1575699813290, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1641/-/Official_Review"}}}, {"id": "S1g4HP_njB", "original": null, "number": 7, "cdate": 1573844795801, "ddate": null, "tcdate": 1573844795801, "tmdate": 1573848482635, "tddate": null, "forum": "HylxE1HKwS", "replyto": "HylxE1HKwS", "invitation": "ICLR.cc/2020/Conference/Paper1641/-/Official_Comment", "content": {"title": "Revision Uploaded", "comment": "We sincerely thank all reviewers for their constructive comments. We have revised our paper accordingly with the promised results and implementation details included. Please check out the new version! \n\nOur pre-trained model and training code are available at: \nhttps://drive.google.com/open?id=1GrLufnGc_3UYG6l7kBX3JYjUqPr8ZaUQ\n\n1. We updated the experiment section (Section 4) with the new results in the MobileNetV3 search space. OFA consistently outperforms MobileNetV3 on various mobile platforms and latency constraints. \n\n2. In Appendix A, we included a figure showing the relationship between the performance of the accuracy prediction model and the accuracy of selected sub-networks. \n\n3. We updated figure1 and included a new figure (figure 5) that shows the entire trade-off curves of OFA on mobile platforms. \n\n4. In Appendix C, we added a table showing the detailed architecture of the full network. \n\n5. In Appendix E, we included implementation details of the progressive shrinking algorithm.\n\nIf there are any additional comments on the paper or on the code, please don\u2019t hesitate to let us know. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1641/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1641/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["hancai@mit.edu", "ganchuang1990@gmail.com", "usedtobe@mit.edu", "zhangzk@mit.edu", "songhan@mit.edu"], "title": "Once-for-All: Train One Network and Specialize it for Efficient Deployment", "authors": ["Han Cai", "Chuang Gan", "Tianzhe Wang", "Zhekai Zhang", "Song Han"], "pdf": "/pdf/cbef2d6e1f85be9ef9e5ed7a525a648b463fc6b5.pdf", "TL;DR": "We introduce techniques to train a single once-for-all network that fits many hardware platforms.", "abstract": "We address the challenging problem of efficient inference across many devices and resource constraints, especially on edge devices.  Conventional approaches either manually design or use neural architecture search (NAS) to find a specialized neural network and train it from scratch for each case, which is computationally prohibitive (causing $CO_2$ emission as much as 5 cars' lifetime) thus unscalable. In this work, we propose to train a once-for-all (OFA) network that supports diverse architectural settings by decoupling training and search, to reduce the cost. We can quickly get a specialized sub-network by selecting from the OFA network without additional training. To efficiently train OFA networks, we also propose a novel progressive shrinking algorithm, a generalized pruning method that reduces the model size across many more dimensions than pruning (depth, width, kernel size, and resolution). It can obtain a surprisingly large number of sub-networks ($> 10^{19}$) that can fit different hardware platforms and latency constraints while maintaining the same level of accuracy as training independently. On diverse edge devices, OFA consistently outperforms state-of-the-art (SOTA) NAS methods (up to 4.0% ImageNet top1 accuracy improvement over MobileNetV3, or same accuracy but 1.5x faster than MobileNetV3, 2.6x faster than EfficientNet w.r.t measured latency) while reducing many orders of magnitude GPU hours and $CO_2$ emission. In particular, OFA achieves a new SOTA 80.0% ImageNet top-1 accuracy under the mobile setting ($<$600M MACs). OFA is the winning solution for the 3rd Low Power Computer Vision Challenge (LPCVC), DSP classification track and the 4th LPCVC, both classification track and detection track. Code and  50 pre-trained models (for many devices & many latency constraints) are released at https://github.com/mit-han-lab/once-for-all. ", "keywords": ["Efficient Deep Learning", "Specialized Neural Network Architecture", "AutoML"], "paperhash": "cai|onceforall_train_one_network_and_specialize_it_for_efficient_deployment", "code": "https://github.com/mit-han-lab/once-for-all", "_bibtex": "@inproceedings{\nCai2020Once-for-All:,\ntitle={Once-for-All: Train One Network and Specialize it for Efficient Deployment},\nauthor={Han Cai and Chuang Gan and Tianzhe Wang and Zhekai Zhang and Song Han},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HylxE1HKwS}\n}", "original_pdf": "/attachment/2a60bd911a60e9c9b6aa95cd711966c0bc8af57a.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HylxE1HKwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1641/Authors", "ICLR.cc/2020/Conference/Paper1641/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1641/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1641/Reviewers", "ICLR.cc/2020/Conference/Paper1641/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1641/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1641/Authors|ICLR.cc/2020/Conference/Paper1641/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504153021, "tmdate": 1576860560371, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1641/Authors", "ICLR.cc/2020/Conference/Paper1641/Reviewers", "ICLR.cc/2020/Conference/Paper1641/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1641/-/Official_Comment"}}}, {"id": "rkgITIzrjS", "original": null, "number": 6, "cdate": 1573361342159, "ddate": null, "tcdate": 1573361342159, "tmdate": 1573361751508, "tddate": null, "forum": "HylxE1HKwS", "replyto": "BklpzCV0FB", "invitation": "ICLR.cc/2020/Conference/Paper1641/-/Official_Comment", "content": {"title": "Our response to Reviewer #3", "comment": "Thanks very much for your constructive comments. \n1. Why training from large to small can prevent interference between sub-networks.\nTraining large sub-networks can also benefit small sub-networks to learn useful features. For example, after finishing the step of elastic kernel size, the sub-network (D=3, W=6, K=7, R=224) can already achieve 69.1% top-1 accuracy on ImageNet without any fine-tuning. This is consistent with previous observations in network pruning [1,2,3]. By training from large to small, both large sub-networks and small sub-networks can reuse previously learned knowledge (or features). Empirically, we find that it is helpful for the optimization of the shared weights with the goal of supporting large sub-networks and small sub-networks at the same time. \n\n2. Why subnetworks with weight sharing could achieve the same, or even better performances compared with those non shared counterparts.\nWe first want to clarify that we are not targeting at improving the accuracy of a specific sub-network for a **single** scenario; instead, we want to improve the accuracy-efficiency trade-off on **many** hardware platforms while reducing the total training cost. To avoid confusion about the goal of this paper, we will emphasize our main contribution and make it more clear in the revision.\n\nWe conjecture the reason for this result is that smaller sub-networks can benefit from getting the knowledge transferred from well-trained large sub-networks through inheriting weights from large sub-networks and knowledge distillation. \n\nRegarding separating the benefits of PS and the disadvantage of weight sharing (i.e., interfering), we want to clarify that weight sharing is an essential component of the OFA framework since it is prohibitive to download and store so many networks independently on resource-constrained edge devices. \n\n3. Code release.\nThank you for the suggestion. We definitely hope this work can be a useful tool for application purposes. We are currently cleaning the code. The training code and pre-trained models will be released anonymously in the OpenReview by Nov. 22. \n\nWe have also summarized all of our planned updates in our general response above. If there are any additional comments on the paper or on the planned updates, please don\u2019t hesitate to let us know. \n\n[1] Han, Song, et al. \"Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding.\" in ICLR 2016.\n[2] Liu, Zhuang, et al. \"Learning efficient convolutional networks through network slimming.\" in ICCV 2017.\n[3] He, Yihui, et al. \"Channel pruning for accelerating very deep neural networks.\" in ICCV 2017."}, "signatures": ["ICLR.cc/2020/Conference/Paper1641/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1641/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["hancai@mit.edu", "ganchuang1990@gmail.com", "usedtobe@mit.edu", "zhangzk@mit.edu", "songhan@mit.edu"], "title": "Once-for-All: Train One Network and Specialize it for Efficient Deployment", "authors": ["Han Cai", "Chuang Gan", "Tianzhe Wang", "Zhekai Zhang", "Song Han"], "pdf": "/pdf/cbef2d6e1f85be9ef9e5ed7a525a648b463fc6b5.pdf", "TL;DR": "We introduce techniques to train a single once-for-all network that fits many hardware platforms.", "abstract": "We address the challenging problem of efficient inference across many devices and resource constraints, especially on edge devices.  Conventional approaches either manually design or use neural architecture search (NAS) to find a specialized neural network and train it from scratch for each case, which is computationally prohibitive (causing $CO_2$ emission as much as 5 cars' lifetime) thus unscalable. In this work, we propose to train a once-for-all (OFA) network that supports diverse architectural settings by decoupling training and search, to reduce the cost. We can quickly get a specialized sub-network by selecting from the OFA network without additional training. To efficiently train OFA networks, we also propose a novel progressive shrinking algorithm, a generalized pruning method that reduces the model size across many more dimensions than pruning (depth, width, kernel size, and resolution). It can obtain a surprisingly large number of sub-networks ($> 10^{19}$) that can fit different hardware platforms and latency constraints while maintaining the same level of accuracy as training independently. On diverse edge devices, OFA consistently outperforms state-of-the-art (SOTA) NAS methods (up to 4.0% ImageNet top1 accuracy improvement over MobileNetV3, or same accuracy but 1.5x faster than MobileNetV3, 2.6x faster than EfficientNet w.r.t measured latency) while reducing many orders of magnitude GPU hours and $CO_2$ emission. In particular, OFA achieves a new SOTA 80.0% ImageNet top-1 accuracy under the mobile setting ($<$600M MACs). OFA is the winning solution for the 3rd Low Power Computer Vision Challenge (LPCVC), DSP classification track and the 4th LPCVC, both classification track and detection track. Code and  50 pre-trained models (for many devices & many latency constraints) are released at https://github.com/mit-han-lab/once-for-all. ", "keywords": ["Efficient Deep Learning", "Specialized Neural Network Architecture", "AutoML"], "paperhash": "cai|onceforall_train_one_network_and_specialize_it_for_efficient_deployment", "code": "https://github.com/mit-han-lab/once-for-all", "_bibtex": "@inproceedings{\nCai2020Once-for-All:,\ntitle={Once-for-All: Train One Network and Specialize it for Efficient Deployment},\nauthor={Han Cai and Chuang Gan and Tianzhe Wang and Zhekai Zhang and Song Han},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HylxE1HKwS}\n}", "original_pdf": "/attachment/2a60bd911a60e9c9b6aa95cd711966c0bc8af57a.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HylxE1HKwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1641/Authors", "ICLR.cc/2020/Conference/Paper1641/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1641/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1641/Reviewers", "ICLR.cc/2020/Conference/Paper1641/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1641/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1641/Authors|ICLR.cc/2020/Conference/Paper1641/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504153021, "tmdate": 1576860560371, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1641/Authors", "ICLR.cc/2020/Conference/Paper1641/Reviewers", "ICLR.cc/2020/Conference/Paper1641/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1641/-/Official_Comment"}}}, {"id": "SyefqEGrsr", "original": null, "number": 3, "cdate": 1573360777970, "ddate": null, "tcdate": 1573360777970, "tmdate": 1573361700999, "tddate": null, "forum": "HylxE1HKwS", "replyto": "HylxE1HKwS", "invitation": "ICLR.cc/2020/Conference/Paper1641/-/Official_Comment", "content": {"title": "Our general response", "comment": "We sincerely thank all reviewers for their comments. We summarize our planned updates as follows:\n\n1. We will apply our method to the same architecture space as MobileNetV3. The new results will be included by Nov. 15. \n\n2. We will add a figure in the appendix by Nov. 15, showing the relationship between the performance of the accuracy prediction model and the accuracy of selected sub-networks. \n\n3. We will update our figures showing the entire trade-off curves with many points (rather than a few points) of OFA on different hardware platforms by Nov. 15. \n\n4. We will add the detailed architecture of the full model in the appendix. \n\n5. For reproduction, we will include a detailed description of our training details by Nov. 15. We are working on cleaning the code. The training code and pre-trained models will be released anonymously in the OpenReview by Nov. 22. \n\nIf there are any additional comments on the paper or on the planned updates, please don\u2019t hesitate to let us know. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1641/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1641/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["hancai@mit.edu", "ganchuang1990@gmail.com", "usedtobe@mit.edu", "zhangzk@mit.edu", "songhan@mit.edu"], "title": "Once-for-All: Train One Network and Specialize it for Efficient Deployment", "authors": ["Han Cai", "Chuang Gan", "Tianzhe Wang", "Zhekai Zhang", "Song Han"], "pdf": "/pdf/cbef2d6e1f85be9ef9e5ed7a525a648b463fc6b5.pdf", "TL;DR": "We introduce techniques to train a single once-for-all network that fits many hardware platforms.", "abstract": "We address the challenging problem of efficient inference across many devices and resource constraints, especially on edge devices.  Conventional approaches either manually design or use neural architecture search (NAS) to find a specialized neural network and train it from scratch for each case, which is computationally prohibitive (causing $CO_2$ emission as much as 5 cars' lifetime) thus unscalable. In this work, we propose to train a once-for-all (OFA) network that supports diverse architectural settings by decoupling training and search, to reduce the cost. We can quickly get a specialized sub-network by selecting from the OFA network without additional training. To efficiently train OFA networks, we also propose a novel progressive shrinking algorithm, a generalized pruning method that reduces the model size across many more dimensions than pruning (depth, width, kernel size, and resolution). It can obtain a surprisingly large number of sub-networks ($> 10^{19}$) that can fit different hardware platforms and latency constraints while maintaining the same level of accuracy as training independently. On diverse edge devices, OFA consistently outperforms state-of-the-art (SOTA) NAS methods (up to 4.0% ImageNet top1 accuracy improvement over MobileNetV3, or same accuracy but 1.5x faster than MobileNetV3, 2.6x faster than EfficientNet w.r.t measured latency) while reducing many orders of magnitude GPU hours and $CO_2$ emission. In particular, OFA achieves a new SOTA 80.0% ImageNet top-1 accuracy under the mobile setting ($<$600M MACs). OFA is the winning solution for the 3rd Low Power Computer Vision Challenge (LPCVC), DSP classification track and the 4th LPCVC, both classification track and detection track. Code and  50 pre-trained models (for many devices & many latency constraints) are released at https://github.com/mit-han-lab/once-for-all. ", "keywords": ["Efficient Deep Learning", "Specialized Neural Network Architecture", "AutoML"], "paperhash": "cai|onceforall_train_one_network_and_specialize_it_for_efficient_deployment", "code": "https://github.com/mit-han-lab/once-for-all", "_bibtex": "@inproceedings{\nCai2020Once-for-All:,\ntitle={Once-for-All: Train One Network and Specialize it for Efficient Deployment},\nauthor={Han Cai and Chuang Gan and Tianzhe Wang and Zhekai Zhang and Song Han},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HylxE1HKwS}\n}", "original_pdf": "/attachment/2a60bd911a60e9c9b6aa95cd711966c0bc8af57a.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HylxE1HKwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1641/Authors", "ICLR.cc/2020/Conference/Paper1641/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1641/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1641/Reviewers", "ICLR.cc/2020/Conference/Paper1641/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1641/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1641/Authors|ICLR.cc/2020/Conference/Paper1641/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504153021, "tmdate": 1576860560371, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1641/Authors", "ICLR.cc/2020/Conference/Paper1641/Reviewers", "ICLR.cc/2020/Conference/Paper1641/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1641/-/Official_Comment"}}}, {"id": "rJeuZIGHiB", "original": null, "number": 5, "cdate": 1573361151526, "ddate": null, "tcdate": 1573361151526, "tmdate": 1573361151526, "tddate": null, "forum": "HylxE1HKwS", "replyto": "Bygt_gUQYr", "invitation": "ICLR.cc/2020/Conference/Paper1641/-/Official_Comment", "content": {"title": "Our response to Reviewer #2", "comment": "Thanks very much for your constructive and detailed comments. We will fix the typos and remove \u201cOnce for All #25\u201d from figure 1. \n1. Training details and code release.\nFor reproduction, we will add a detailed and clear description of our training details by Nov. 15. We are also working on cleaning the code. The training code and pre-trained models will be posted anonymously in the OpenReview by Nov. 22.\n\n2. Sample more sub-networks and produce some Pareto curves.\nThat\u2019s a great idea. Thanks for the suggestion. We will update our figures showing the entire trade-off curves rather than a few points by Nov. 15. \n\n3. Adding the original model in the appendix.\nThank you for the suggestion. We will add a figure showing the detailed architecture of the full (original) model in the appendix.\n\n4. Why the progressive shrinking goes resolution->kernel->depth->width.\nThe order is determined based on the difficulty of each task. Intuitively, we hope the model to complete easy tasks first and then handle more difficult tasks, similar to the idea of curriculum learning. \n\n5. Why the channel sorting operation preserves the accuracy of larger sub-networks.\nWhen performing the channel sorting operation on a specific layer, we first sort the input dimension of the layer according to their importance (i.e., L1 norm). Then the output dimension of the previous layer is reorganized accordingly to make sure the functionality of large sub-networks does not change. \n\nWe have also summarized all of our planned updates in our general response above. If there are any additional comments on the paper or on the planned updates, please don\u2019t hesitate to let us know. "}, "signatures": ["ICLR.cc/2020/Conference/Paper1641/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1641/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["hancai@mit.edu", "ganchuang1990@gmail.com", "usedtobe@mit.edu", "zhangzk@mit.edu", "songhan@mit.edu"], "title": "Once-for-All: Train One Network and Specialize it for Efficient Deployment", "authors": ["Han Cai", "Chuang Gan", "Tianzhe Wang", "Zhekai Zhang", "Song Han"], "pdf": "/pdf/cbef2d6e1f85be9ef9e5ed7a525a648b463fc6b5.pdf", "TL;DR": "We introduce techniques to train a single once-for-all network that fits many hardware platforms.", "abstract": "We address the challenging problem of efficient inference across many devices and resource constraints, especially on edge devices.  Conventional approaches either manually design or use neural architecture search (NAS) to find a specialized neural network and train it from scratch for each case, which is computationally prohibitive (causing $CO_2$ emission as much as 5 cars' lifetime) thus unscalable. In this work, we propose to train a once-for-all (OFA) network that supports diverse architectural settings by decoupling training and search, to reduce the cost. We can quickly get a specialized sub-network by selecting from the OFA network without additional training. To efficiently train OFA networks, we also propose a novel progressive shrinking algorithm, a generalized pruning method that reduces the model size across many more dimensions than pruning (depth, width, kernel size, and resolution). It can obtain a surprisingly large number of sub-networks ($> 10^{19}$) that can fit different hardware platforms and latency constraints while maintaining the same level of accuracy as training independently. On diverse edge devices, OFA consistently outperforms state-of-the-art (SOTA) NAS methods (up to 4.0% ImageNet top1 accuracy improvement over MobileNetV3, or same accuracy but 1.5x faster than MobileNetV3, 2.6x faster than EfficientNet w.r.t measured latency) while reducing many orders of magnitude GPU hours and $CO_2$ emission. In particular, OFA achieves a new SOTA 80.0% ImageNet top-1 accuracy under the mobile setting ($<$600M MACs). OFA is the winning solution for the 3rd Low Power Computer Vision Challenge (LPCVC), DSP classification track and the 4th LPCVC, both classification track and detection track. Code and  50 pre-trained models (for many devices & many latency constraints) are released at https://github.com/mit-han-lab/once-for-all. ", "keywords": ["Efficient Deep Learning", "Specialized Neural Network Architecture", "AutoML"], "paperhash": "cai|onceforall_train_one_network_and_specialize_it_for_efficient_deployment", "code": "https://github.com/mit-han-lab/once-for-all", "_bibtex": "@inproceedings{\nCai2020Once-for-All:,\ntitle={Once-for-All: Train One Network and Specialize it for Efficient Deployment},\nauthor={Han Cai and Chuang Gan and Tianzhe Wang and Zhekai Zhang and Song Han},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HylxE1HKwS}\n}", "original_pdf": "/attachment/2a60bd911a60e9c9b6aa95cd711966c0bc8af57a.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HylxE1HKwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1641/Authors", "ICLR.cc/2020/Conference/Paper1641/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1641/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1641/Reviewers", "ICLR.cc/2020/Conference/Paper1641/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1641/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1641/Authors|ICLR.cc/2020/Conference/Paper1641/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504153021, "tmdate": 1576860560371, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1641/Authors", "ICLR.cc/2020/Conference/Paper1641/Reviewers", "ICLR.cc/2020/Conference/Paper1641/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1641/-/Official_Comment"}}}, {"id": "Skx8DSMHoH", "original": null, "number": 4, "cdate": 1573360990319, "ddate": null, "tcdate": 1573360990319, "tmdate": 1573360990319, "tddate": null, "forum": "HylxE1HKwS", "replyto": "HygXKK8RKB", "invitation": "ICLR.cc/2020/Conference/Paper1641/-/Official_Comment", "content": {"title": "Our response to Reviewer #1", "comment": "Thanks very much for your constructive comments.\n1. Performance of the accuracy prediction model and how it influences the final selection.\nWe will add a figure in the appendix by Nov. 15, showing the relationship between the performance of the accuracy prediction model and the accuracy of selected sub-networks. \n\n2. Comparison to MobileNetV3 in Table 2.\nThanks for the suggestion. We agree that it is essential to compare our model to MobileNetV3 which gives the current SOTA performances on mobile platforms. To have an Apple-to-Apple comparison with it, we will apply our method to the same architecture space as MobileNetV3. The new results will be included by Nov. 15. \n\nWe have also summarized all of our planned updates in our general response above. If there are any additional comments on the paper or on the planned updates, please don\u2019t hesitate to let us know. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1641/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1641/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["hancai@mit.edu", "ganchuang1990@gmail.com", "usedtobe@mit.edu", "zhangzk@mit.edu", "songhan@mit.edu"], "title": "Once-for-All: Train One Network and Specialize it for Efficient Deployment", "authors": ["Han Cai", "Chuang Gan", "Tianzhe Wang", "Zhekai Zhang", "Song Han"], "pdf": "/pdf/cbef2d6e1f85be9ef9e5ed7a525a648b463fc6b5.pdf", "TL;DR": "We introduce techniques to train a single once-for-all network that fits many hardware platforms.", "abstract": "We address the challenging problem of efficient inference across many devices and resource constraints, especially on edge devices.  Conventional approaches either manually design or use neural architecture search (NAS) to find a specialized neural network and train it from scratch for each case, which is computationally prohibitive (causing $CO_2$ emission as much as 5 cars' lifetime) thus unscalable. In this work, we propose to train a once-for-all (OFA) network that supports diverse architectural settings by decoupling training and search, to reduce the cost. We can quickly get a specialized sub-network by selecting from the OFA network without additional training. To efficiently train OFA networks, we also propose a novel progressive shrinking algorithm, a generalized pruning method that reduces the model size across many more dimensions than pruning (depth, width, kernel size, and resolution). It can obtain a surprisingly large number of sub-networks ($> 10^{19}$) that can fit different hardware platforms and latency constraints while maintaining the same level of accuracy as training independently. On diverse edge devices, OFA consistently outperforms state-of-the-art (SOTA) NAS methods (up to 4.0% ImageNet top1 accuracy improvement over MobileNetV3, or same accuracy but 1.5x faster than MobileNetV3, 2.6x faster than EfficientNet w.r.t measured latency) while reducing many orders of magnitude GPU hours and $CO_2$ emission. In particular, OFA achieves a new SOTA 80.0% ImageNet top-1 accuracy under the mobile setting ($<$600M MACs). OFA is the winning solution for the 3rd Low Power Computer Vision Challenge (LPCVC), DSP classification track and the 4th LPCVC, both classification track and detection track. Code and  50 pre-trained models (for many devices & many latency constraints) are released at https://github.com/mit-han-lab/once-for-all. ", "keywords": ["Efficient Deep Learning", "Specialized Neural Network Architecture", "AutoML"], "paperhash": "cai|onceforall_train_one_network_and_specialize_it_for_efficient_deployment", "code": "https://github.com/mit-han-lab/once-for-all", "_bibtex": "@inproceedings{\nCai2020Once-for-All:,\ntitle={Once-for-All: Train One Network and Specialize it for Efficient Deployment},\nauthor={Han Cai and Chuang Gan and Tianzhe Wang and Zhekai Zhang and Song Han},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HylxE1HKwS}\n}", "original_pdf": "/attachment/2a60bd911a60e9c9b6aa95cd711966c0bc8af57a.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HylxE1HKwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1641/Authors", "ICLR.cc/2020/Conference/Paper1641/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1641/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1641/Reviewers", "ICLR.cc/2020/Conference/Paper1641/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1641/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1641/Authors|ICLR.cc/2020/Conference/Paper1641/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504153021, "tmdate": 1576860560371, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1641/Authors", "ICLR.cc/2020/Conference/Paper1641/Reviewers", "ICLR.cc/2020/Conference/Paper1641/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1641/-/Official_Comment"}}}, {"id": "BJgb4Ezrjr", "original": null, "number": 2, "cdate": 1573360681494, "ddate": null, "tcdate": 1573360681494, "tmdate": 1573360681494, "tddate": null, "forum": "HylxE1HKwS", "replyto": "r1l72hZWir", "invitation": "ICLR.cc/2020/Conference/Paper1641/-/Official_Comment", "content": {"title": "Thanks for suggesting a related paper. We will add a reference to the paper in the revision.", "comment": "Hi Jason, \n\nRegarding your question about distillation, the teacher model does not share weights with the OFA network. Specifically, after training the full network, one copy of the full network weights is used as the teacher model, and another copy of the full network weights is used for further training to support smaller sub-networks. Therefore, training smaller sub-networks does not affect the teacher model. \n\nBest,\nAuthors"}, "signatures": ["ICLR.cc/2020/Conference/Paper1641/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1641/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["hancai@mit.edu", "ganchuang1990@gmail.com", "usedtobe@mit.edu", "zhangzk@mit.edu", "songhan@mit.edu"], "title": "Once-for-All: Train One Network and Specialize it for Efficient Deployment", "authors": ["Han Cai", "Chuang Gan", "Tianzhe Wang", "Zhekai Zhang", "Song Han"], "pdf": "/pdf/cbef2d6e1f85be9ef9e5ed7a525a648b463fc6b5.pdf", "TL;DR": "We introduce techniques to train a single once-for-all network that fits many hardware platforms.", "abstract": "We address the challenging problem of efficient inference across many devices and resource constraints, especially on edge devices.  Conventional approaches either manually design or use neural architecture search (NAS) to find a specialized neural network and train it from scratch for each case, which is computationally prohibitive (causing $CO_2$ emission as much as 5 cars' lifetime) thus unscalable. In this work, we propose to train a once-for-all (OFA) network that supports diverse architectural settings by decoupling training and search, to reduce the cost. We can quickly get a specialized sub-network by selecting from the OFA network without additional training. To efficiently train OFA networks, we also propose a novel progressive shrinking algorithm, a generalized pruning method that reduces the model size across many more dimensions than pruning (depth, width, kernel size, and resolution). It can obtain a surprisingly large number of sub-networks ($> 10^{19}$) that can fit different hardware platforms and latency constraints while maintaining the same level of accuracy as training independently. On diverse edge devices, OFA consistently outperforms state-of-the-art (SOTA) NAS methods (up to 4.0% ImageNet top1 accuracy improvement over MobileNetV3, or same accuracy but 1.5x faster than MobileNetV3, 2.6x faster than EfficientNet w.r.t measured latency) while reducing many orders of magnitude GPU hours and $CO_2$ emission. In particular, OFA achieves a new SOTA 80.0% ImageNet top-1 accuracy under the mobile setting ($<$600M MACs). OFA is the winning solution for the 3rd Low Power Computer Vision Challenge (LPCVC), DSP classification track and the 4th LPCVC, both classification track and detection track. Code and  50 pre-trained models (for many devices & many latency constraints) are released at https://github.com/mit-han-lab/once-for-all. ", "keywords": ["Efficient Deep Learning", "Specialized Neural Network Architecture", "AutoML"], "paperhash": "cai|onceforall_train_one_network_and_specialize_it_for_efficient_deployment", "code": "https://github.com/mit-han-lab/once-for-all", "_bibtex": "@inproceedings{\nCai2020Once-for-All:,\ntitle={Once-for-All: Train One Network and Specialize it for Efficient Deployment},\nauthor={Han Cai and Chuang Gan and Tianzhe Wang and Zhekai Zhang and Song Han},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HylxE1HKwS}\n}", "original_pdf": "/attachment/2a60bd911a60e9c9b6aa95cd711966c0bc8af57a.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HylxE1HKwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1641/Authors", "ICLR.cc/2020/Conference/Paper1641/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1641/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1641/Reviewers", "ICLR.cc/2020/Conference/Paper1641/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1641/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1641/Authors|ICLR.cc/2020/Conference/Paper1641/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504153021, "tmdate": 1576860560371, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1641/Authors", "ICLR.cc/2020/Conference/Paper1641/Reviewers", "ICLR.cc/2020/Conference/Paper1641/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1641/-/Official_Comment"}}}, {"id": "r1l72hZWir", "original": null, "number": 2, "cdate": 1573096618671, "ddate": null, "tcdate": 1573096618671, "tmdate": 1573096946169, "tddate": null, "forum": "HylxE1HKwS", "replyto": "HylxE1HKwS", "invitation": "ICLR.cc/2020/Conference/Paper1641/-/Public_Comment", "content": {"title": "Interesting work and a missing reference", "comment": "Hi, thanks for the interesting work that advances the progress of efficient deep learning. \n\nI especially find the idea of progressive shrinking intriguing. It is said that the smaller sub-networks distill knowledge from larger sub-networks.. Since all sub-networks share the same weights, wouldn't training smaller sub-networks change the prediction behavior of larger sub-networks (making them unreliable for distillation)?\n\nThere is a missing reference to a related work that also similarly focuses on multiple efficiency configurations using a single model without retraining. It would be good if the authors could acknowledge it.\n- Stochastic Downsampling for Cost-Adjustable Inference and Improved Regularization in Convolutional Networks, CVPR 2018.\n"}, "signatures": ["~Jason_Kuen1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Jason_Kuen1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["hancai@mit.edu", "ganchuang1990@gmail.com", "usedtobe@mit.edu", "zhangzk@mit.edu", "songhan@mit.edu"], "title": "Once-for-All: Train One Network and Specialize it for Efficient Deployment", "authors": ["Han Cai", "Chuang Gan", "Tianzhe Wang", "Zhekai Zhang", "Song Han"], "pdf": "/pdf/cbef2d6e1f85be9ef9e5ed7a525a648b463fc6b5.pdf", "TL;DR": "We introduce techniques to train a single once-for-all network that fits many hardware platforms.", "abstract": "We address the challenging problem of efficient inference across many devices and resource constraints, especially on edge devices.  Conventional approaches either manually design or use neural architecture search (NAS) to find a specialized neural network and train it from scratch for each case, which is computationally prohibitive (causing $CO_2$ emission as much as 5 cars' lifetime) thus unscalable. In this work, we propose to train a once-for-all (OFA) network that supports diverse architectural settings by decoupling training and search, to reduce the cost. We can quickly get a specialized sub-network by selecting from the OFA network without additional training. To efficiently train OFA networks, we also propose a novel progressive shrinking algorithm, a generalized pruning method that reduces the model size across many more dimensions than pruning (depth, width, kernel size, and resolution). It can obtain a surprisingly large number of sub-networks ($> 10^{19}$) that can fit different hardware platforms and latency constraints while maintaining the same level of accuracy as training independently. On diverse edge devices, OFA consistently outperforms state-of-the-art (SOTA) NAS methods (up to 4.0% ImageNet top1 accuracy improvement over MobileNetV3, or same accuracy but 1.5x faster than MobileNetV3, 2.6x faster than EfficientNet w.r.t measured latency) while reducing many orders of magnitude GPU hours and $CO_2$ emission. In particular, OFA achieves a new SOTA 80.0% ImageNet top-1 accuracy under the mobile setting ($<$600M MACs). OFA is the winning solution for the 3rd Low Power Computer Vision Challenge (LPCVC), DSP classification track and the 4th LPCVC, both classification track and detection track. Code and  50 pre-trained models (for many devices & many latency constraints) are released at https://github.com/mit-han-lab/once-for-all. ", "keywords": ["Efficient Deep Learning", "Specialized Neural Network Architecture", "AutoML"], "paperhash": "cai|onceforall_train_one_network_and_specialize_it_for_efficient_deployment", "code": "https://github.com/mit-han-lab/once-for-all", "_bibtex": "@inproceedings{\nCai2020Once-for-All:,\ntitle={Once-for-All: Train One Network and Specialize it for Efficient Deployment},\nauthor={Han Cai and Chuang Gan and Tianzhe Wang and Zhekai Zhang and Song Han},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HylxE1HKwS}\n}", "original_pdf": "/attachment/2a60bd911a60e9c9b6aa95cd711966c0bc8af57a.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HylxE1HKwS", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504191879, "tmdate": 1576860593388, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper1641/Authors", "ICLR.cc/2020/Conference/Paper1641/Reviewers", "ICLR.cc/2020/Conference/Paper1641/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1641/-/Public_Comment"}}}, {"id": "BklpzCV0FB", "original": null, "number": 2, "cdate": 1571864085393, "ddate": null, "tcdate": 1571864085393, "tmdate": 1572972442418, "tddate": null, "forum": "HylxE1HKwS", "replyto": "HylxE1HKwS", "invitation": "ICLR.cc/2020/Conference/Paper1641/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper tries to tackle the problem of searching best architectures for specialized resource constraint deployment scenarios. The authors basically take a two-step approach: First train a large network including all the small networks with weight sharing and some specially designed trick (e.g., progressive shrinking). Second, use prediction based NAS method to learn the performance/inference prediction module, from which the good sub architecture corresponding to a particular scenario is obtained. The experiments show that the proposed method is promising.\n\nPros:\n\n\n1. It is an interesting new paradigm that tries to solve AutoML for different deployment scenarios \u201conce for all\u201d.  AFAIK there is no prior works thinking in this way.\n2. It is useful and encouraging to see the proposed method achieves satisfactory performances, on par with the current best method specially designed for different deployment environment, while the computational cost is reduced by a large margin. \n3. Paper is clearly written and easy to understand.\n\nCons:\n\n1. The motivation towards \u201cprogressive shrinking (PS)\u201d is not that clear. It seems natural to train a large network, and from it to train sub structures, since overparameterization helps NN training. However, it is hard to imagine that training from large to small could eliminate the \u201cinterfering\u201d of subnetworks, let alone \u201cwhile maintaining the same accuracy as independently trained networks\u201d.  To me it is neither theoretically nor empirically supported (Please note the training of subnetworks definitely affect the learnt weights of the big one through weight sharing). In particular, the subnetworks with weight sharing could achieve the same, or even better performances compared with those non shared counterparts, which seems too good to be true.\n    1. A possible explanation might be that the overparameterization brings additional gain in the optimization process of each small network, especially with the help of knowledge distillation. If that is true, an additional ablation study should be done to separate the benefits of PS, and the disadvantage of weight sharing (i.e., interfering). \n2. I see no statements about code release. If a clear, and TIMELY code (for the SEARCH phase, not only for the Eval phase) release could be done, then at least from the perspective of application, the impact of this paper could be further enhanced.\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1641/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1641/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["hancai@mit.edu", "ganchuang1990@gmail.com", "usedtobe@mit.edu", "zhangzk@mit.edu", "songhan@mit.edu"], "title": "Once-for-All: Train One Network and Specialize it for Efficient Deployment", "authors": ["Han Cai", "Chuang Gan", "Tianzhe Wang", "Zhekai Zhang", "Song Han"], "pdf": "/pdf/cbef2d6e1f85be9ef9e5ed7a525a648b463fc6b5.pdf", "TL;DR": "We introduce techniques to train a single once-for-all network that fits many hardware platforms.", "abstract": "We address the challenging problem of efficient inference across many devices and resource constraints, especially on edge devices.  Conventional approaches either manually design or use neural architecture search (NAS) to find a specialized neural network and train it from scratch for each case, which is computationally prohibitive (causing $CO_2$ emission as much as 5 cars' lifetime) thus unscalable. In this work, we propose to train a once-for-all (OFA) network that supports diverse architectural settings by decoupling training and search, to reduce the cost. We can quickly get a specialized sub-network by selecting from the OFA network without additional training. To efficiently train OFA networks, we also propose a novel progressive shrinking algorithm, a generalized pruning method that reduces the model size across many more dimensions than pruning (depth, width, kernel size, and resolution). It can obtain a surprisingly large number of sub-networks ($> 10^{19}$) that can fit different hardware platforms and latency constraints while maintaining the same level of accuracy as training independently. On diverse edge devices, OFA consistently outperforms state-of-the-art (SOTA) NAS methods (up to 4.0% ImageNet top1 accuracy improvement over MobileNetV3, or same accuracy but 1.5x faster than MobileNetV3, 2.6x faster than EfficientNet w.r.t measured latency) while reducing many orders of magnitude GPU hours and $CO_2$ emission. In particular, OFA achieves a new SOTA 80.0% ImageNet top-1 accuracy under the mobile setting ($<$600M MACs). OFA is the winning solution for the 3rd Low Power Computer Vision Challenge (LPCVC), DSP classification track and the 4th LPCVC, both classification track and detection track. Code and  50 pre-trained models (for many devices & many latency constraints) are released at https://github.com/mit-han-lab/once-for-all. ", "keywords": ["Efficient Deep Learning", "Specialized Neural Network Architecture", "AutoML"], "paperhash": "cai|onceforall_train_one_network_and_specialize_it_for_efficient_deployment", "code": "https://github.com/mit-han-lab/once-for-all", "_bibtex": "@inproceedings{\nCai2020Once-for-All:,\ntitle={Once-for-All: Train One Network and Specialize it for Efficient Deployment},\nauthor={Han Cai and Chuang Gan and Tianzhe Wang and Zhekai Zhang and Song Han},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HylxE1HKwS}\n}", "original_pdf": "/attachment/2a60bd911a60e9c9b6aa95cd711966c0bc8af57a.pdf"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HylxE1HKwS", "replyto": "HylxE1HKwS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1641/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1641/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575699813277, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1641/Reviewers"], "noninvitees": [], "tcdate": 1570237734418, "tmdate": 1575699813290, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1641/-/Official_Review"}}}, {"id": "HygXKK8RKB", "original": null, "number": 3, "cdate": 1571871099049, "ddate": null, "tcdate": 1571871099049, "tmdate": 1572972442375, "tddate": null, "forum": "HylxE1HKwS", "replyto": "HylxE1HKwS", "invitation": "ICLR.cc/2020/Conference/Paper1641/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "In this manuscript, authors propose an OFA NAS framework. They train a supernet first and then finetune the elastic version of the large network. After training, the sub-networks derived from the supernet can be applied for different scenarios directly without retraining. The motivation is clear and interesting. My concerns are as follows.\n1.\tWhen sampling sub-networks, a prediction model is applied to predict the accuracy of networks. It is interesting to show the accuracy of the prediction model itself and how it will influence the final selection.\n2.\tThe results compared in Table 2 are outdated. Authors should at least add the result of MobileNetV3."}, "signatures": ["ICLR.cc/2020/Conference/Paper1641/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1641/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["hancai@mit.edu", "ganchuang1990@gmail.com", "usedtobe@mit.edu", "zhangzk@mit.edu", "songhan@mit.edu"], "title": "Once-for-All: Train One Network and Specialize it for Efficient Deployment", "authors": ["Han Cai", "Chuang Gan", "Tianzhe Wang", "Zhekai Zhang", "Song Han"], "pdf": "/pdf/cbef2d6e1f85be9ef9e5ed7a525a648b463fc6b5.pdf", "TL;DR": "We introduce techniques to train a single once-for-all network that fits many hardware platforms.", "abstract": "We address the challenging problem of efficient inference across many devices and resource constraints, especially on edge devices.  Conventional approaches either manually design or use neural architecture search (NAS) to find a specialized neural network and train it from scratch for each case, which is computationally prohibitive (causing $CO_2$ emission as much as 5 cars' lifetime) thus unscalable. In this work, we propose to train a once-for-all (OFA) network that supports diverse architectural settings by decoupling training and search, to reduce the cost. We can quickly get a specialized sub-network by selecting from the OFA network without additional training. To efficiently train OFA networks, we also propose a novel progressive shrinking algorithm, a generalized pruning method that reduces the model size across many more dimensions than pruning (depth, width, kernel size, and resolution). It can obtain a surprisingly large number of sub-networks ($> 10^{19}$) that can fit different hardware platforms and latency constraints while maintaining the same level of accuracy as training independently. On diverse edge devices, OFA consistently outperforms state-of-the-art (SOTA) NAS methods (up to 4.0% ImageNet top1 accuracy improvement over MobileNetV3, or same accuracy but 1.5x faster than MobileNetV3, 2.6x faster than EfficientNet w.r.t measured latency) while reducing many orders of magnitude GPU hours and $CO_2$ emission. In particular, OFA achieves a new SOTA 80.0% ImageNet top-1 accuracy under the mobile setting ($<$600M MACs). OFA is the winning solution for the 3rd Low Power Computer Vision Challenge (LPCVC), DSP classification track and the 4th LPCVC, both classification track and detection track. Code and  50 pre-trained models (for many devices & many latency constraints) are released at https://github.com/mit-han-lab/once-for-all. ", "keywords": ["Efficient Deep Learning", "Specialized Neural Network Architecture", "AutoML"], "paperhash": "cai|onceforall_train_one_network_and_specialize_it_for_efficient_deployment", "code": "https://github.com/mit-han-lab/once-for-all", "_bibtex": "@inproceedings{\nCai2020Once-for-All:,\ntitle={Once-for-All: Train One Network and Specialize it for Efficient Deployment},\nauthor={Han Cai and Chuang Gan and Tianzhe Wang and Zhekai Zhang and Song Han},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HylxE1HKwS}\n}", "original_pdf": "/attachment/2a60bd911a60e9c9b6aa95cd711966c0bc8af57a.pdf"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HylxE1HKwS", "replyto": "HylxE1HKwS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1641/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1641/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575699813277, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1641/Reviewers"], "noninvitees": [], "tcdate": 1570237734418, "tmdate": 1575699813290, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1641/-/Official_Review"}}}, {"id": "ByxzZ83IYH", "original": null, "number": 1, "cdate": 1571370490188, "ddate": null, "tcdate": 1571370490188, "tmdate": 1571370599452, "tddate": null, "forum": "HylxE1HKwS", "replyto": "BJe47svjOB", "invitation": "ICLR.cc/2020/Conference/Paper1641/-/Official_Comment", "content": {"comment": "Hi Rudy,\n\nThanks for your interest. We will release the code after the double-blind review. \n\nWe use 150 epochs to train the full (large) network before switching to the elastic version. In training the elastic version, the full (large) network is not trained. We set the learning rate for fine-tuning as 0.04 (1/10 initial learning rate). We choose the hyper-parameters by cross-validation (learning rates around 0.04 give stable results; increasing the number of epochs can usually improve the results).\n\nRegarding your second question, we do not claim that OFA produces sub-networks that outperform the individually trained ones.  Our main contribution is to reduce the total cost of handling **many** deployment scenarios (hardware platforms and constraints), which is crucial for real-world applications, rather than targeting a **single** scenario. Therefore, the key advantage of OFA is that OFA can efficiently specialize for different deployment scenarios while individually trained models cannot. Even independently training the sub-network with distillation (using the same teacher network as OFA), the accuracy slightly improved from 74.3% to 74.7%, which is still at the same level as OFA produced sub-network (74.8%). \n\nBest,\nAuthors", "title": "Open Source & Motivation"}, "signatures": ["ICLR.cc/2020/Conference/Paper1641/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1641/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["hancai@mit.edu", "ganchuang1990@gmail.com", "usedtobe@mit.edu", "zhangzk@mit.edu", "songhan@mit.edu"], "title": "Once-for-All: Train One Network and Specialize it for Efficient Deployment", "authors": ["Han Cai", "Chuang Gan", "Tianzhe Wang", "Zhekai Zhang", "Song Han"], "pdf": "/pdf/cbef2d6e1f85be9ef9e5ed7a525a648b463fc6b5.pdf", "TL;DR": "We introduce techniques to train a single once-for-all network that fits many hardware platforms.", "abstract": "We address the challenging problem of efficient inference across many devices and resource constraints, especially on edge devices.  Conventional approaches either manually design or use neural architecture search (NAS) to find a specialized neural network and train it from scratch for each case, which is computationally prohibitive (causing $CO_2$ emission as much as 5 cars' lifetime) thus unscalable. In this work, we propose to train a once-for-all (OFA) network that supports diverse architectural settings by decoupling training and search, to reduce the cost. We can quickly get a specialized sub-network by selecting from the OFA network without additional training. To efficiently train OFA networks, we also propose a novel progressive shrinking algorithm, a generalized pruning method that reduces the model size across many more dimensions than pruning (depth, width, kernel size, and resolution). It can obtain a surprisingly large number of sub-networks ($> 10^{19}$) that can fit different hardware platforms and latency constraints while maintaining the same level of accuracy as training independently. On diverse edge devices, OFA consistently outperforms state-of-the-art (SOTA) NAS methods (up to 4.0% ImageNet top1 accuracy improvement over MobileNetV3, or same accuracy but 1.5x faster than MobileNetV3, 2.6x faster than EfficientNet w.r.t measured latency) while reducing many orders of magnitude GPU hours and $CO_2$ emission. In particular, OFA achieves a new SOTA 80.0% ImageNet top-1 accuracy under the mobile setting ($<$600M MACs). OFA is the winning solution for the 3rd Low Power Computer Vision Challenge (LPCVC), DSP classification track and the 4th LPCVC, both classification track and detection track. Code and  50 pre-trained models (for many devices & many latency constraints) are released at https://github.com/mit-han-lab/once-for-all. ", "keywords": ["Efficient Deep Learning", "Specialized Neural Network Architecture", "AutoML"], "paperhash": "cai|onceforall_train_one_network_and_specialize_it_for_efficient_deployment", "code": "https://github.com/mit-han-lab/once-for-all", "_bibtex": "@inproceedings{\nCai2020Once-for-All:,\ntitle={Once-for-All: Train One Network and Specialize it for Efficient Deployment},\nauthor={Han Cai and Chuang Gan and Tianzhe Wang and Zhekai Zhang and Song Han},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HylxE1HKwS}\n}", "original_pdf": "/attachment/2a60bd911a60e9c9b6aa95cd711966c0bc8af57a.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HylxE1HKwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1641/Authors", "ICLR.cc/2020/Conference/Paper1641/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1641/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1641/Reviewers", "ICLR.cc/2020/Conference/Paper1641/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1641/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1641/Authors|ICLR.cc/2020/Conference/Paper1641/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504153021, "tmdate": 1576860560371, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1641/Authors", "ICLR.cc/2020/Conference/Paper1641/Reviewers", "ICLR.cc/2020/Conference/Paper1641/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1641/-/Official_Comment"}}}, {"id": "BJe47svjOB", "original": null, "number": 1, "cdate": 1570630427701, "ddate": null, "tcdate": 1570630427701, "tmdate": 1570630427701, "tddate": null, "forum": "HylxE1HKwS", "replyto": "HylxE1HKwS", "invitation": "ICLR.cc/2020/Conference/Paper1641/-/Public_Comment", "content": {"comment": "Dear authors,\n\nThis is a really interesting work that one can train a large network such that the sub-networks within the large network still work really well, which appears to be even better than the individual trained ones!\n\nI'm trying to implement this idea and would like to know the specific hyper-parameters used in the paper. Specifically, how long do you train the large network before switching to the elastic version. In training the elastic version, do you still train the large network? If so, how are their losses combined? When training the elastic version gradually, what is the specific learning rate since the paper mentioned that you train with small learning rate so that the large network wouldn't deviate too much from the pre-trained weights. Also, can you elaborate on how the number of epochs and learning rate used to fine-tune each of the elastic space chosen and how they affect the results?\n\nAnother question I have is that you mentioned using distillation for the large network to distill the sub-networks, do you do the same for the independent trained models? Specifically, in Table 1, do you use knowledge distillation for the independent trained models? If not, I think it is not clear if the proposed OFA network indeed produce sub-networks that outperform the individually trained ones. \n\nThanks,\nRudy", "title": "Interesting work"}, "signatures": ["~Rudy_Chin1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Rudy_Chin1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["hancai@mit.edu", "ganchuang1990@gmail.com", "usedtobe@mit.edu", "zhangzk@mit.edu", "songhan@mit.edu"], "title": "Once-for-All: Train One Network and Specialize it for Efficient Deployment", "authors": ["Han Cai", "Chuang Gan", "Tianzhe Wang", "Zhekai Zhang", "Song Han"], "pdf": "/pdf/cbef2d6e1f85be9ef9e5ed7a525a648b463fc6b5.pdf", "TL;DR": "We introduce techniques to train a single once-for-all network that fits many hardware platforms.", "abstract": "We address the challenging problem of efficient inference across many devices and resource constraints, especially on edge devices.  Conventional approaches either manually design or use neural architecture search (NAS) to find a specialized neural network and train it from scratch for each case, which is computationally prohibitive (causing $CO_2$ emission as much as 5 cars' lifetime) thus unscalable. In this work, we propose to train a once-for-all (OFA) network that supports diverse architectural settings by decoupling training and search, to reduce the cost. We can quickly get a specialized sub-network by selecting from the OFA network without additional training. To efficiently train OFA networks, we also propose a novel progressive shrinking algorithm, a generalized pruning method that reduces the model size across many more dimensions than pruning (depth, width, kernel size, and resolution). It can obtain a surprisingly large number of sub-networks ($> 10^{19}$) that can fit different hardware platforms and latency constraints while maintaining the same level of accuracy as training independently. On diverse edge devices, OFA consistently outperforms state-of-the-art (SOTA) NAS methods (up to 4.0% ImageNet top1 accuracy improvement over MobileNetV3, or same accuracy but 1.5x faster than MobileNetV3, 2.6x faster than EfficientNet w.r.t measured latency) while reducing many orders of magnitude GPU hours and $CO_2$ emission. In particular, OFA achieves a new SOTA 80.0% ImageNet top-1 accuracy under the mobile setting ($<$600M MACs). OFA is the winning solution for the 3rd Low Power Computer Vision Challenge (LPCVC), DSP classification track and the 4th LPCVC, both classification track and detection track. Code and  50 pre-trained models (for many devices & many latency constraints) are released at https://github.com/mit-han-lab/once-for-all. ", "keywords": ["Efficient Deep Learning", "Specialized Neural Network Architecture", "AutoML"], "paperhash": "cai|onceforall_train_one_network_and_specialize_it_for_efficient_deployment", "code": "https://github.com/mit-han-lab/once-for-all", "_bibtex": "@inproceedings{\nCai2020Once-for-All:,\ntitle={Once-for-All: Train One Network and Specialize it for Efficient Deployment},\nauthor={Han Cai and Chuang Gan and Tianzhe Wang and Zhekai Zhang and Song Han},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HylxE1HKwS}\n}", "original_pdf": "/attachment/2a60bd911a60e9c9b6aa95cd711966c0bc8af57a.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HylxE1HKwS", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504191879, "tmdate": 1576860593388, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper1641/Authors", "ICLR.cc/2020/Conference/Paper1641/Reviewers", "ICLR.cc/2020/Conference/Paper1641/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1641/-/Public_Comment"}}}], "count": 14}