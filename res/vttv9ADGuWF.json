{"notes": [{"id": "vttv9ADGuWF", "original": "08RLcidrmL_", "number": 3584, "cdate": 1601308398369, "ddate": null, "tcdate": 1601308398369, "tmdate": 1614985750012, "tddate": null, "forum": "vttv9ADGuWF", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Certified robustness against physically-realizable patch attack via randomized cropping", "authorids": ["~Wan-Yi_Lin1", "~Fatemeh_Sheikholeslami1", "~jinghao_shi1", "~Leslie_Rice1", "~J_Zico_Kolter1"], "authors": ["Wan-Yi Lin", "Fatemeh Sheikholeslami", "jinghao shi", "Leslie Rice", "J Zico Kolter"], "keywords": ["adversarial machine learning", "certifiable defense", "patch attack"], "abstract": "This paper studies a certifiable defense against adversarial patch attacks on image classification. Our approach classifies random crops from the original image independently and the original image is classified as the vote over these crops.  This process minimizes changes to the training process, as only the crop classification model needs to be trained, and can be trained in a standard manner without explicit adversarial training.  Leveraging the fact that a patch attack can only influence some pixels of the image, we derive certified robustness bounds on the resulting classification. Our method is particularly effective when realistic physical transformations are applied to the adversarial patch, such as affine transformations. Such transformations occur naturally when an adversarial patch is physically introduced to a scene. Our method improves upon the current state of the art in defending against patch attacks on CIFAR10 and ImageNet, both in terms of certified accuracy and inference time.", "one-sentence_summary": "This paper studies certifiable defense against adversarial patch attacks on image classification using random sub-regions of the original image.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lin|certified_robustness_against_physicallyrealizable_patch_attack_via_randomized_cropping", "pdf": "/pdf/71e9d68eb2a9a96b809252a936cdab1fc1779d1a.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=RAoyaoQJF", "_bibtex": "@misc{\nlin2021certified,\ntitle={Certified robustness against physically-realizable patch attack via randomized cropping},\nauthor={Wan-Yi Lin and Fatemeh Sheikholeslami and jinghao shi and Leslie Rice and J Zico Kolter},\nyear={2021},\nurl={https://openreview.net/forum?id=vttv9ADGuWF}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "HprP3Cit20r", "original": null, "number": 1, "cdate": 1610040385888, "ddate": null, "tcdate": 1610040385888, "tmdate": 1610473979420, "tddate": null, "forum": "vttv9ADGuWF", "replyto": "vttv9ADGuWF", "invitation": "ICLR.cc/2021/Conference/Paper3584/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "The paper provides a simple prediction procedure to defend against (rectangular) patch attacks, and also a method to obtain some random estimates of the certified robustness of the method. The simplicity of the method is certainly appreciated. On the other hand, there are a number of issues preventing the acceptance of this paper. The main problem is that the paper deals with a randomized predictor, yet the certification guarantee developed for deterministic predictors is applied. This leads to several problems, starting from the target being undefined to unfair comparisons. While the authors made an attempt to address this in the rebuttal, more work is needed to properly settle this issue."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Certified robustness against physically-realizable patch attack via randomized cropping", "authorids": ["~Wan-Yi_Lin1", "~Fatemeh_Sheikholeslami1", "~jinghao_shi1", "~Leslie_Rice1", "~J_Zico_Kolter1"], "authors": ["Wan-Yi Lin", "Fatemeh Sheikholeslami", "jinghao shi", "Leslie Rice", "J Zico Kolter"], "keywords": ["adversarial machine learning", "certifiable defense", "patch attack"], "abstract": "This paper studies a certifiable defense against adversarial patch attacks on image classification. Our approach classifies random crops from the original image independently and the original image is classified as the vote over these crops.  This process minimizes changes to the training process, as only the crop classification model needs to be trained, and can be trained in a standard manner without explicit adversarial training.  Leveraging the fact that a patch attack can only influence some pixels of the image, we derive certified robustness bounds on the resulting classification. Our method is particularly effective when realistic physical transformations are applied to the adversarial patch, such as affine transformations. Such transformations occur naturally when an adversarial patch is physically introduced to a scene. Our method improves upon the current state of the art in defending against patch attacks on CIFAR10 and ImageNet, both in terms of certified accuracy and inference time.", "one-sentence_summary": "This paper studies certifiable defense against adversarial patch attacks on image classification using random sub-regions of the original image.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lin|certified_robustness_against_physicallyrealizable_patch_attack_via_randomized_cropping", "pdf": "/pdf/71e9d68eb2a9a96b809252a936cdab1fc1779d1a.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=RAoyaoQJF", "_bibtex": "@misc{\nlin2021certified,\ntitle={Certified robustness against physically-realizable patch attack via randomized cropping},\nauthor={Wan-Yi Lin and Fatemeh Sheikholeslami and jinghao shi and Leslie Rice and J Zico Kolter},\nyear={2021},\nurl={https://openreview.net/forum?id=vttv9ADGuWF}\n}"}, "tags": [], "invitation": {"reply": {"forum": "vttv9ADGuWF", "replyto": "vttv9ADGuWF", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040385874, "tmdate": 1610473979403, "id": "ICLR.cc/2021/Conference/Paper3584/-/Decision"}}}, {"id": "fxO6iTYgR2", "original": null, "number": 3, "cdate": 1603897139515, "ddate": null, "tcdate": 1603897139515, "tmdate": 1606921258791, "tddate": null, "forum": "vttv9ADGuWF", "replyto": "vttv9ADGuWF", "invitation": "ICLR.cc/2021/Conference/Paper3584/-/Official_Review", "content": {"title": "The paper proposed a simple way to defense adversarial patch attacks.", "review": "The method can be basically summarized as a majority voting of crops of an image. Moreover, a new certification of the proposed method is introduced, not similar to the conventional adversarial robustness certification on perturbation under $\\ell_p$ ball , the method using simple geometry and probability problem to certify the results instead of any relaxation of the neural networks.\nHowever, the paper is poorly written, and many experimental setups are missing.\n\nUpdate after rebuttal:\nAfter reading the rebuttal, I don't think my questions are addressed very well, especially about the confidence probability, $p_c$. The certification is defined as a guaranteed yes/no problem but the $p_c$ will relax the certification to a probabilistic problem. Also, PatchGuard with patch transformation is out of scope for the original paper, so I think the experimental results in Section 4.1 and 4.2 are more like a fair comparison. However,  refer to the results in table 3, the proposed method yields worse performance than PG-DRS although the computational cost is saved. Hence, I will keep my rating.\n\nPros:\n\n+The proposed method is quite general and intuitive.\n\n+Using a simple geometry and probability model, the proposed method can certify the robustness of the adversarial patch attack in a very efficient way.\n\nCons:\n\n-What's the training time of the crop classifier $g$ compare with other baselines?\n\n-The whole system highly depends on the test accuracy of $g$. What's the result if no attack involved in?\n\n-From the experimental results, the proposed method has margin improvement compare with PG+DRS, and the basic idea of the certification algorithm also follows PG+DRS. So, the contribution is not enough for ICLR.\n\n-The hyperparameter $p_c$ is set as 0.95 in the experiment. However, the certification needs guaranteed results. So, if the $p_c = 0.95$, is that means there is a 5% possibility that this image can be attacked? Although this may not change the results so much, the ablation study of this parameter setting should be performed to make the comparison fairly.\n\n-Figure 2 (right) seems to use the wrong plot.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3584/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3584/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Certified robustness against physically-realizable patch attack via randomized cropping", "authorids": ["~Wan-Yi_Lin1", "~Fatemeh_Sheikholeslami1", "~jinghao_shi1", "~Leslie_Rice1", "~J_Zico_Kolter1"], "authors": ["Wan-Yi Lin", "Fatemeh Sheikholeslami", "jinghao shi", "Leslie Rice", "J Zico Kolter"], "keywords": ["adversarial machine learning", "certifiable defense", "patch attack"], "abstract": "This paper studies a certifiable defense against adversarial patch attacks on image classification. Our approach classifies random crops from the original image independently and the original image is classified as the vote over these crops.  This process minimizes changes to the training process, as only the crop classification model needs to be trained, and can be trained in a standard manner without explicit adversarial training.  Leveraging the fact that a patch attack can only influence some pixels of the image, we derive certified robustness bounds on the resulting classification. Our method is particularly effective when realistic physical transformations are applied to the adversarial patch, such as affine transformations. Such transformations occur naturally when an adversarial patch is physically introduced to a scene. Our method improves upon the current state of the art in defending against patch attacks on CIFAR10 and ImageNet, both in terms of certified accuracy and inference time.", "one-sentence_summary": "This paper studies certifiable defense against adversarial patch attacks on image classification using random sub-regions of the original image.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lin|certified_robustness_against_physicallyrealizable_patch_attack_via_randomized_cropping", "pdf": "/pdf/71e9d68eb2a9a96b809252a936cdab1fc1779d1a.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=RAoyaoQJF", "_bibtex": "@misc{\nlin2021certified,\ntitle={Certified robustness against physically-realizable patch attack via randomized cropping},\nauthor={Wan-Yi Lin and Fatemeh Sheikholeslami and jinghao shi and Leslie Rice and J Zico Kolter},\nyear={2021},\nurl={https://openreview.net/forum?id=vttv9ADGuWF}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "vttv9ADGuWF", "replyto": "vttv9ADGuWF", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3584/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538073236, "tmdate": 1606915768384, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3584/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3584/-/Official_Review"}}}, {"id": "LMoxIS39q5k", "original": null, "number": 4, "cdate": 1604355794644, "ddate": null, "tcdate": 1604355794644, "tmdate": 1606748268924, "tddate": null, "forum": "vttv9ADGuWF", "replyto": "vttv9ADGuWF", "invitation": "ICLR.cc/2021/Conference/Paper3584/-/Official_Review", "content": {"title": "An elegant defense. But text and discussion must be improved.", "review": "### Summary\n\nThe paper proposes a robust neural network architecture to defend against\nadversarial patch attacks --where the attacker can freely modify a small patch\nof size $s_{patch}$ of the input images-- by training the net on random\ncrops of a pre-defined size $s_{crop}$ (in general $\\neq s_{patch}$), and, at\ntest time, use majority voting over several such random crops for every image.\nThey show that, as long as $s_{patch}$ is small ($\\leq 2.4\\%$ of total image),\nthese architectures achieve usual and **certified** robust accuracies that are\ncompetitive with the SOTA among the techniques and architectures that were\ntailored to certifiably defend against such attacks.\n\n\n### Overall evaluation\n\nThe defense is elegant for its simplicity and seems efficient, both in terms of\nperformance for small attack patches (which, arguably, is the most interesting\ncase), and in terms of computational speed/complexity (for small enough sample\nsize of crops). But the paper is still unclear, vague and/or too\napproximate in some parts (see points below). It seems to have been written in\na rush (Fig.2 on the right is obviously not the one intended) and some\nadditional experiments and/or discussions would be useful to complement the\nevaluation of the current method (see point 8. below). So, overall, I don't\nrecommend publication in the current state but am inclined to reconsider when I\nwill see appropriate changes in the paper.\n\n\n### Detailed remarks/questions\n\n1. Since you repeatedly refer to de-randomized smoothing and PatchGuard in the\n   text (and their combination), I suggest adding a short self-contained\n   description (in appendix with a reference) at the beginning (or in appendix\n   with a reference at the beginning) for the non-informed readers.\n2. I suggest to clearly mention that what is certified is the robust **test**\n   accuracy, not the distributional robust accuracy.\n3. I think that you are sampling the crops **uniformly** at random **with**\n   replacement, but I don't recall reading this explicitly in the text. Anyway,\n   it would be good to compare both approaches (with and without replacement),\n   at least in appendix.\n4. The description of Table 1 is in Sec.4.1 with title \"Without patch\n   transformation\", yet the caption says (and Table 2 confirms) that it shows\n   results **with** patch transformations. Which one is right? And if it's with\n   patch transformation, please add the same table for the setting without patch\n   transformation (at least to appendix).\n5. Eq. 6: capital $N$ in $C^N_i$ should be $n$.\n6. Please do an additional pass of proof-reading\n7. Sec 3.2: the description of how to compute the certifiable robustness bound\n   could be **significantly** improved and seems to be written in a big rush. In\n   particular, you never explicitly say which formula is used to compute the\n   robust bounds.\n8. _Crop size vs patch size_: The trade-off between crop size and attack patch\n   size is discussed in a rush at the end of the experiments (Sec. 4.2): it\n   deserves attention and an intuition explanation of the trade-off much earlier\n   in the paper, f.ex. when explaining the overall attack. Also, the paper could\n   benefit a lot from a finer analysis of the dependence of the optimal crop-size\n   on the attack's patch size (and probably on the typical size of \"relevant\n   information\" in the images). This could be either empirical (studying the\n   optimal crop-size as a function of the patch size) or more theoretical (with an\n   image model, or by using the average size of relevant objects in the image).\n   Also, it could shed more light on why your attack works better on ImageNet than\n   on CIFAR10 (probably because the typical size of the relevant parts of the\n   images are different in both datasets).\n9. _Sec. 3.1 \u00a7Training randomized cropping classifier_: the sentence \"the only\n   trainable part of the randomized cropping classifier is the crop\n   classifier\" deserves a proper Proposition/Theorem (with precise and explicit\n   hypotheses, s.a., f.ex. the assumption that crops are sampled **uniformly**\n   at random with replacement) and a proper, well-delimited proof. \n\n------------------------------------\n\nUpdate after rebuttal\n-----------------------------\n\nThe updated version is clearly better than the first one. However, the concerns\nof the other reviewers regarding the randomness of $p_c$ (and therefore of the\ncertification method) have convinced me that there are, indeed, some further\nclarifications and discussions needed prior to the publication, which is why I\nwill keep my initial recommendation.\n\nTo be more precise, the root issue here seems to be that the proposed\nclassifier is not deterministic, which means that the standard definitions of\nadversarial examples and adversarial accuracy do not apply and therefore, that\nthe problem that you try to solve is unclear and/or not well defined.  In\nparticular: what is it that gets certified?  what does it mean to get\ncertified? and, more generally, is the word \"certified\" really appropriate in\nthis context?\n\nHowever, whether an analysis of the distribution of $n_{2to1}$ and $p_c$ will\nbe needed (as asked by other reviewers) might depend on how the authors will\ndefine adversarial vulnerability in the random setting, and what they try to\ncertify. Let me explain what I mean.\n\nA reasonable start might be to define adversarial risk as\n$$\n    E_{(x,y)} E_{\\phi} \\mathcal{L}(\\phi(x), y) \\ ,  \\tag{1}\n$$\nwhich is the usual definition, but with an additional expectation over the\nvariability of the classifier $\\phi$.  Adversarial accuracy would then be the\nadversarial risk for the 0-1-loss $\\mathcal{L}_{0-1}$. Then the authors could, f.ex., set as goal\nto construct a (provably) unbiased estimate of this quantity.\n\nThe advantage of such a method is that one doesn't forget the fact that, what\nwe actually want to certify is this \"distributional\" robustness (i.e. where\nexpectation is taken over the true underlying, unknown distribution), not the\nrobustness on the test set. Even methods that have a non-random certification\nprocess (so-called \"provable robustness guarantees\") will never be able to\ncertify this quantity: they'll only deliver certificates on test example. The\n\"certified robustness on the test set\" that they yield is also just a random\nvariable which we hope \"generalizes to\" (1). Reviewers almost never ask authors\nto analyze/certify this generalization gap. Similarly, here, one could see the\nrandomness over $n_{2to1}$ and $p_c$ as just another source of randomness\ncontributing to the variability of the generalization gap, in which case, maybe\nno rigorous analysis could be acceptable, as long as it is clear what the\nauthors want to certify (unbiasedness of the estimator, f.ex.). Therefore,\nwhether this source of randomness could or should be explicitly captured/used\nby the authors' method is, I think, a question of how the authors frame the\nproblem and their goal.\n\n### Minor points:\n\n- even the revised version still contains quite a few grammatical errors,\n  especially in the new/re-worked sections, where many articles (\"the\", \"a\")\n  are missing.\n\n- End of p.5, \"to maximize number of certified robust images, the randomized\n  cropping classifier should maximize n2to1, which is equivalent to maximizing\n  classification accuracy of $g_\\theta$\": not sure about this equivalence.\n  Maximizing the classification accuracy is equivalent to maximizing n1, not\n  necessarily n1-n2.\n\n- are DRS and PG also probabilistic certifications (i.e. certifying robustness\n  with some probability, f.ex. pc>.95)? This should be clearly said in the text\n  and the captions, especially since it would make the comparison a bit unfair\n  if their certification were 100% sure. (This issue is obviously related to my\n  previous major remark on randomness.)\n\n- the name \"worst-case certified accuracy\" in the caption of Table 1 is very\n  unclear at that point. It becomes clear in Sec. 4.3, but you refer to Table 1\n  in Sec. 4.1 already. So this term should be clearly explained in the caption,\n  or there should be a clear reference to the relevant part in the text.\n\n- don't always re-cite Levine&Feizi and Xiang et al. every time you mention\n  de-randomized smoothing and patch guard. Cite them the first time, and then\n  say that you'll refer to de-randomized smoothing and patch guard as DRS and\n  PG in the rest of the text.\n\n- in the conclusion: \"This paper proposes a new architecture for defense\n  against\" -> \"This paper proposes a new defense against\". (You are not really\n  proposing a new architecture.)", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3584/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3584/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Certified robustness against physically-realizable patch attack via randomized cropping", "authorids": ["~Wan-Yi_Lin1", "~Fatemeh_Sheikholeslami1", "~jinghao_shi1", "~Leslie_Rice1", "~J_Zico_Kolter1"], "authors": ["Wan-Yi Lin", "Fatemeh Sheikholeslami", "jinghao shi", "Leslie Rice", "J Zico Kolter"], "keywords": ["adversarial machine learning", "certifiable defense", "patch attack"], "abstract": "This paper studies a certifiable defense against adversarial patch attacks on image classification. Our approach classifies random crops from the original image independently and the original image is classified as the vote over these crops.  This process minimizes changes to the training process, as only the crop classification model needs to be trained, and can be trained in a standard manner without explicit adversarial training.  Leveraging the fact that a patch attack can only influence some pixels of the image, we derive certified robustness bounds on the resulting classification. Our method is particularly effective when realistic physical transformations are applied to the adversarial patch, such as affine transformations. Such transformations occur naturally when an adversarial patch is physically introduced to a scene. Our method improves upon the current state of the art in defending against patch attacks on CIFAR10 and ImageNet, both in terms of certified accuracy and inference time.", "one-sentence_summary": "This paper studies certifiable defense against adversarial patch attacks on image classification using random sub-regions of the original image.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lin|certified_robustness_against_physicallyrealizable_patch_attack_via_randomized_cropping", "pdf": "/pdf/71e9d68eb2a9a96b809252a936cdab1fc1779d1a.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=RAoyaoQJF", "_bibtex": "@misc{\nlin2021certified,\ntitle={Certified robustness against physically-realizable patch attack via randomized cropping},\nauthor={Wan-Yi Lin and Fatemeh Sheikholeslami and jinghao shi and Leslie Rice and J Zico Kolter},\nyear={2021},\nurl={https://openreview.net/forum?id=vttv9ADGuWF}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "vttv9ADGuWF", "replyto": "vttv9ADGuWF", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3584/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538073236, "tmdate": 1606915768384, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3584/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3584/-/Official_Review"}}}, {"id": "a6Fvd-HxG4Z", "original": null, "number": 2, "cdate": 1603744742195, "ddate": null, "tcdate": 1603744742195, "tmdate": 1606740583576, "tddate": null, "forum": "vttv9ADGuWF", "replyto": "vttv9ADGuWF", "invitation": "ICLR.cc/2021/Conference/Paper3584/-/Official_Review", "content": {"title": "Review", "review": "Update after the rebuttal:\n\nI read other reviews and response from the authors and I decided to keep my score. Overall, I think paper still needs more\nwork. For example, incorporating details on confidence into the main paper and not just a section in the appendix is quite\nimportant as otherwise the paper is misleading.\n\n==================================================\n\n-> Summary: \nIn this paper, authors propose a new certified defense against adversarial patches. They propose a model\nwhich samples different patches from the original image, performs classification of these patches using neural network classifier and performs a majority vote to compute the output label. The guarantee is obtained by computing a probability that none of the sampled patches intersect adversarial patch.\n\n-> Reasons for score:\n\nI vote for rejecting this paper. The main issue I have is that their probabilistic guarantees lack confidence intervals and \ntherefore it is not clear how meaningful they are. Further issue is that the method works better than prior work only if \ncertain image transformations are applied, such as rotation. Yet, this critical part is not described formally: what is rotated, entire image or just a patch? Same holds for aspect ratio. Due to these problems, I cannot recommend acceptance for this paper.\n\n-> Pros:\n\nI think explanations of the method are easy to follow and writing is solid, with some parts that require more clarifications.\n\n-> Cons:\n\nThe biggest problem I have with this paper is that authors do not consider confidence intervals for their guarantees. In particular, probability p_c computed in Equation 6 is tied to the particular n patches that were sampled which determines the value of n_{2to1}. This means that n_{2to1} is random variable and p_c that authors compute holds for just one sampled value of n_{2to1}. So for this bound to be useful, we need some form of confidence interval. For example, guarantee could be that with 99% probability p_c is interval [p_c_low, p_c_high]. And then, only if p_c_low is greater than some probability threshold (which is set to 95% in Section 4), we can report that this sample is certified. If I misunderstood something, it would be great if authors can clarify what kind of guarantee is provided.\n\nAdditionally, I find method used here relatively trivial and I think more technical contributions are necessary (e.g. computing confidence intervals above). In terms of empirical results, it seems that this method works better than prior work only in the case of additional image transformations and otherwise it performs worse or same. As this is the critical thing, I think authors should provide more explanations on how are these transformations applied, more formally instead of just describing it.\n\n\n-> Questions:\n\n- In Equation 6, summation is over i in the set {0, n_{2to1}}. Should this summation be over 0 <= i < n_{2to1} instead?\n- Can you clarify what is the difference between the experiments in Table 1 and Table 2? If I understand correctly, the experiments in Table 1 have additional transformation that is applied besides the adversarial patch? \n- Can you write mathematical formulation of transformations in Section 4.2? I am not sure whether these transformations are applied over the entire image or only over the patch, so it would be good to write more formally what exactly is the transformation.\n\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3584/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3584/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Certified robustness against physically-realizable patch attack via randomized cropping", "authorids": ["~Wan-Yi_Lin1", "~Fatemeh_Sheikholeslami1", "~jinghao_shi1", "~Leslie_Rice1", "~J_Zico_Kolter1"], "authors": ["Wan-Yi Lin", "Fatemeh Sheikholeslami", "jinghao shi", "Leslie Rice", "J Zico Kolter"], "keywords": ["adversarial machine learning", "certifiable defense", "patch attack"], "abstract": "This paper studies a certifiable defense against adversarial patch attacks on image classification. Our approach classifies random crops from the original image independently and the original image is classified as the vote over these crops.  This process minimizes changes to the training process, as only the crop classification model needs to be trained, and can be trained in a standard manner without explicit adversarial training.  Leveraging the fact that a patch attack can only influence some pixels of the image, we derive certified robustness bounds on the resulting classification. Our method is particularly effective when realistic physical transformations are applied to the adversarial patch, such as affine transformations. Such transformations occur naturally when an adversarial patch is physically introduced to a scene. Our method improves upon the current state of the art in defending against patch attacks on CIFAR10 and ImageNet, both in terms of certified accuracy and inference time.", "one-sentence_summary": "This paper studies certifiable defense against adversarial patch attacks on image classification using random sub-regions of the original image.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lin|certified_robustness_against_physicallyrealizable_patch_attack_via_randomized_cropping", "pdf": "/pdf/71e9d68eb2a9a96b809252a936cdab1fc1779d1a.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=RAoyaoQJF", "_bibtex": "@misc{\nlin2021certified,\ntitle={Certified robustness against physically-realizable patch attack via randomized cropping},\nauthor={Wan-Yi Lin and Fatemeh Sheikholeslami and jinghao shi and Leslie Rice and J Zico Kolter},\nyear={2021},\nurl={https://openreview.net/forum?id=vttv9ADGuWF}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "vttv9ADGuWF", "replyto": "vttv9ADGuWF", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3584/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538073236, "tmdate": 1606915768384, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3584/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3584/-/Official_Review"}}}, {"id": "g74BP57QAVF", "original": null, "number": 1, "cdate": 1603463151929, "ddate": null, "tcdate": 1603463151929, "tmdate": 1606732028516, "tddate": null, "forum": "vttv9ADGuWF", "replyto": "vttv9ADGuWF", "invitation": "ICLR.cc/2021/Conference/Paper3584/-/Official_Review", "content": {"title": "Surprisingly simple approach to an important problem", "review": "The authors propose a surprisingly simple statistical defense, that can certify the robustness of a classifier against patch attacks. This is achieved by randomly sampling small rectangular subregions of the perturbed image and classify these samples individually. \n\nThe paper is mostly well written. Certification methods, able to handle patch attacks, are of significant interest in real world applications like autonomous driving. The paper is charming because of the simplicity. However, some questions remain unanswered and the experimental evaluation is not very thorough, hence this paper is borderline. \n\nQuestions:\n- Have the authors considered some kind of adversarial training? \n- As far as i understood the paper, the certification method is probabilistic due to the probabilistic intersection of sampled subregions with the adversarial patch. Could this be circumvented by defining a fixed set of subframes that get classified? Then the number of overlapping subregions could be calculated precisely in advance and the certification would not be probabilistic. \n- How does the right figure in Figure 2 look like? The current one seems to be the wrong one. \n- In Table 1, Table 2 and Table 3, the authors provide certification rates. Could the authors also provide how many images of the not certified ones can be successfully attacked?\n- When is $p_c$ as in Equation 6 considered to be close enough to 1, is it $0.95$ as in Section 4?\n- How do the certification rates change for larger/smaller adversarial patches?\n\nComments:\n- $C_i^N$ in Equation 6 should be defined. \n- A vertical line between the CIFAR10 and the ImageNet results in Table 2 & 3 would improve readability.\n\nI am willing to increase my score if my questions and concerns are addressed. \n\n-------------------\n\nAfter reading the authors response, i think that this work would benefit from an experimental comparison of their random method against a method relying on a deterministic crop selection, even if the certification rates of the deterministic method are inferior, because the certificates both methods are yielding are different: The resulting certificates from the deterministic method would be deterministic instead of probabilistic. Hence i will retain my score. ", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3584/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3584/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Certified robustness against physically-realizable patch attack via randomized cropping", "authorids": ["~Wan-Yi_Lin1", "~Fatemeh_Sheikholeslami1", "~jinghao_shi1", "~Leslie_Rice1", "~J_Zico_Kolter1"], "authors": ["Wan-Yi Lin", "Fatemeh Sheikholeslami", "jinghao shi", "Leslie Rice", "J Zico Kolter"], "keywords": ["adversarial machine learning", "certifiable defense", "patch attack"], "abstract": "This paper studies a certifiable defense against adversarial patch attacks on image classification. Our approach classifies random crops from the original image independently and the original image is classified as the vote over these crops.  This process minimizes changes to the training process, as only the crop classification model needs to be trained, and can be trained in a standard manner without explicit adversarial training.  Leveraging the fact that a patch attack can only influence some pixels of the image, we derive certified robustness bounds on the resulting classification. Our method is particularly effective when realistic physical transformations are applied to the adversarial patch, such as affine transformations. Such transformations occur naturally when an adversarial patch is physically introduced to a scene. Our method improves upon the current state of the art in defending against patch attacks on CIFAR10 and ImageNet, both in terms of certified accuracy and inference time.", "one-sentence_summary": "This paper studies certifiable defense against adversarial patch attacks on image classification using random sub-regions of the original image.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lin|certified_robustness_against_physicallyrealizable_patch_attack_via_randomized_cropping", "pdf": "/pdf/71e9d68eb2a9a96b809252a936cdab1fc1779d1a.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=RAoyaoQJF", "_bibtex": "@misc{\nlin2021certified,\ntitle={Certified robustness against physically-realizable patch attack via randomized cropping},\nauthor={Wan-Yi Lin and Fatemeh Sheikholeslami and jinghao shi and Leslie Rice and J Zico Kolter},\nyear={2021},\nurl={https://openreview.net/forum?id=vttv9ADGuWF}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "vttv9ADGuWF", "replyto": "vttv9ADGuWF", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3584/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538073236, "tmdate": 1606915768384, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3584/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3584/-/Official_Review"}}}, {"id": "PXvrmhG5MZ_", "original": null, "number": 7, "cdate": 1606263430314, "ddate": null, "tcdate": 1606263430314, "tmdate": 1606263430314, "tddate": null, "forum": "vttv9ADGuWF", "replyto": "g74BP57QAVF", "invitation": "ICLR.cc/2021/Conference/Paper3584/-/Official_Comment", "content": {"title": "We revised the paper to improve readability, and ran experiments as suggested", "comment": "Thank you for the positive and helpful review. We have addressed the comments in the updated manuscript. Below please find responses for the questions:\n\nWhile adversarial training is a strong empirical defense against adversarial attack, it does not provide robustness guarantees, i.e., given a clean/unperturbed image, an adversarially-trained classifier cannot certify if adversarial attack can or cannot change the predicted class on this image. On the other hand, our method can certify if the predicted class of a given image can or cannot be changed under patch attack. We have considered adversarially-training $f_{\\theta}$, and it does improve empirical performance but decreases certification probability. This is because certification probability increases as the number of crops from \\textit{clean} image being classified correctly increases, but adversarial training decreases performance on clean samples and consequently decreases certification probability. \n\nA deterministic crop selection covering the whole image was considered, but we found that random selection is easier to select the number of crops, while having comparable certified accuracy.\n\nWe list the percentage of images that cannot be certified but can be successfully attacked by image-specific patches in the below table with the corresponding overall accuracy. We train our image-specific patch by training an adversarial patch which is placed at the center of the image for each image. We train each patch for 100 steps with step size 0.02. Please note that although some images are still correctly classified under image-specific patch attack, there can still exist a patch that can change its classification output. Due to time constraints, we only show CIFAR with patch size 2.4\\% and ImageNet with patch size 2\\% with no patch transformation.\n\n___\n CIFAR10 2.4\\% patch                     ||  ImageNet 2\\% patch\n___\n| ours | DRS | PG+DRS / PG+Bagnets || ours | DRS  | PG+DRS / PG+Bagnets\n___\nattack success rate      | 83.2  | 94.3 |92.1 / 87.3                        ||78.5 | 84.9  |83.2 / 79.3   \nclassification accuracy| 60.3  | 59.5 |61.4 / 40.4                        ||34.4 | 27.0  |30.1 / 31.2\n       \n\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3584/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3584/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Certified robustness against physically-realizable patch attack via randomized cropping", "authorids": ["~Wan-Yi_Lin1", "~Fatemeh_Sheikholeslami1", "~jinghao_shi1", "~Leslie_Rice1", "~J_Zico_Kolter1"], "authors": ["Wan-Yi Lin", "Fatemeh Sheikholeslami", "jinghao shi", "Leslie Rice", "J Zico Kolter"], "keywords": ["adversarial machine learning", "certifiable defense", "patch attack"], "abstract": "This paper studies a certifiable defense against adversarial patch attacks on image classification. Our approach classifies random crops from the original image independently and the original image is classified as the vote over these crops.  This process minimizes changes to the training process, as only the crop classification model needs to be trained, and can be trained in a standard manner without explicit adversarial training.  Leveraging the fact that a patch attack can only influence some pixels of the image, we derive certified robustness bounds on the resulting classification. Our method is particularly effective when realistic physical transformations are applied to the adversarial patch, such as affine transformations. Such transformations occur naturally when an adversarial patch is physically introduced to a scene. Our method improves upon the current state of the art in defending against patch attacks on CIFAR10 and ImageNet, both in terms of certified accuracy and inference time.", "one-sentence_summary": "This paper studies certifiable defense against adversarial patch attacks on image classification using random sub-regions of the original image.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lin|certified_robustness_against_physicallyrealizable_patch_attack_via_randomized_cropping", "pdf": "/pdf/71e9d68eb2a9a96b809252a936cdab1fc1779d1a.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=RAoyaoQJF", "_bibtex": "@misc{\nlin2021certified,\ntitle={Certified robustness against physically-realizable patch attack via randomized cropping},\nauthor={Wan-Yi Lin and Fatemeh Sheikholeslami and jinghao shi and Leslie Rice and J Zico Kolter},\nyear={2021},\nurl={https://openreview.net/forum?id=vttv9ADGuWF}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "vttv9ADGuWF", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3584/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3584/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3584/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3584/Authors|ICLR.cc/2021/Conference/Paper3584/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3584/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923835992, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3584/-/Official_Comment"}}}, {"id": "CJYhMpM-tix", "original": null, "number": 6, "cdate": 1606261883965, "ddate": null, "tcdate": 1606261883965, "tmdate": 1606261883965, "tddate": null, "forum": "vttv9ADGuWF", "replyto": "a6Fvd-HxG4Z", "invitation": "ICLR.cc/2021/Conference/Paper3584/-/Official_Comment", "content": {"title": "We added confidence level experiment in Appendix D", "comment": "Thank you for the helpful review. We have made corrections to Eq. 6 and added experiments and discussion of confidence level in Appendix D.\n\nCertified accuracy listed in Table 1. (and Table 3) are when the adversarial path undergoes affine transformation, while adversarial patches in Table 2 are all square with edges align with image coordinate axes. For example, a 2.4\\% patch for CIFAR10 is always a 5x5  square and its edges align with image coordinates, i.e., edges of the patch are parallel to image edges for Table 1; a 2.4\\% patch for CIFAR10 can be a 2x12 or 3x8 rectangle for Table 2."}, "signatures": ["ICLR.cc/2021/Conference/Paper3584/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3584/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Certified robustness against physically-realizable patch attack via randomized cropping", "authorids": ["~Wan-Yi_Lin1", "~Fatemeh_Sheikholeslami1", "~jinghao_shi1", "~Leslie_Rice1", "~J_Zico_Kolter1"], "authors": ["Wan-Yi Lin", "Fatemeh Sheikholeslami", "jinghao shi", "Leslie Rice", "J Zico Kolter"], "keywords": ["adversarial machine learning", "certifiable defense", "patch attack"], "abstract": "This paper studies a certifiable defense against adversarial patch attacks on image classification. Our approach classifies random crops from the original image independently and the original image is classified as the vote over these crops.  This process minimizes changes to the training process, as only the crop classification model needs to be trained, and can be trained in a standard manner without explicit adversarial training.  Leveraging the fact that a patch attack can only influence some pixels of the image, we derive certified robustness bounds on the resulting classification. Our method is particularly effective when realistic physical transformations are applied to the adversarial patch, such as affine transformations. Such transformations occur naturally when an adversarial patch is physically introduced to a scene. Our method improves upon the current state of the art in defending against patch attacks on CIFAR10 and ImageNet, both in terms of certified accuracy and inference time.", "one-sentence_summary": "This paper studies certifiable defense against adversarial patch attacks on image classification using random sub-regions of the original image.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lin|certified_robustness_against_physicallyrealizable_patch_attack_via_randomized_cropping", "pdf": "/pdf/71e9d68eb2a9a96b809252a936cdab1fc1779d1a.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=RAoyaoQJF", "_bibtex": "@misc{\nlin2021certified,\ntitle={Certified robustness against physically-realizable patch attack via randomized cropping},\nauthor={Wan-Yi Lin and Fatemeh Sheikholeslami and jinghao shi and Leslie Rice and J Zico Kolter},\nyear={2021},\nurl={https://openreview.net/forum?id=vttv9ADGuWF}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "vttv9ADGuWF", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3584/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3584/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3584/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3584/Authors|ICLR.cc/2021/Conference/Paper3584/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3584/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923835992, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3584/-/Official_Comment"}}}, {"id": "2UfUt8xrZtn", "original": null, "number": 5, "cdate": 1606261470448, "ddate": null, "tcdate": 1606261470448, "tmdate": 1606261470448, "tddate": null, "forum": "vttv9ADGuWF", "replyto": "fxO6iTYgR2", "invitation": "ICLR.cc/2021/Conference/Paper3584/-/Official_Comment", "content": {"title": "We have added ablation study of $p_c$ and revised the manuscript for improved readability ", "comment": "Thank you for the helpful review. We've added training time to the first paragraph of Section 4. Clean accuracy are listed in Table 1 and a discussion on clean accuracy is added to Section 4.1. The ablation study of $p_c$ is added to Table 3.\n\nWe respectfully disagree with the reviewer that our method has only marginal improvement over PG+DRS -- without patch transformation, we achieve comparable certification accuracy as PG+DRS while the inference time is more than a magnitude faster; with patch transformation, our worst-case certification accuracy is 4 times better than PG+DRS. This comes from our basic idea of using crops instead of ablating regions of an image."}, "signatures": ["ICLR.cc/2021/Conference/Paper3584/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3584/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Certified robustness against physically-realizable patch attack via randomized cropping", "authorids": ["~Wan-Yi_Lin1", "~Fatemeh_Sheikholeslami1", "~jinghao_shi1", "~Leslie_Rice1", "~J_Zico_Kolter1"], "authors": ["Wan-Yi Lin", "Fatemeh Sheikholeslami", "jinghao shi", "Leslie Rice", "J Zico Kolter"], "keywords": ["adversarial machine learning", "certifiable defense", "patch attack"], "abstract": "This paper studies a certifiable defense against adversarial patch attacks on image classification. Our approach classifies random crops from the original image independently and the original image is classified as the vote over these crops.  This process minimizes changes to the training process, as only the crop classification model needs to be trained, and can be trained in a standard manner without explicit adversarial training.  Leveraging the fact that a patch attack can only influence some pixels of the image, we derive certified robustness bounds on the resulting classification. Our method is particularly effective when realistic physical transformations are applied to the adversarial patch, such as affine transformations. Such transformations occur naturally when an adversarial patch is physically introduced to a scene. Our method improves upon the current state of the art in defending against patch attacks on CIFAR10 and ImageNet, both in terms of certified accuracy and inference time.", "one-sentence_summary": "This paper studies certifiable defense against adversarial patch attacks on image classification using random sub-regions of the original image.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lin|certified_robustness_against_physicallyrealizable_patch_attack_via_randomized_cropping", "pdf": "/pdf/71e9d68eb2a9a96b809252a936cdab1fc1779d1a.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=RAoyaoQJF", "_bibtex": "@misc{\nlin2021certified,\ntitle={Certified robustness against physically-realizable patch attack via randomized cropping},\nauthor={Wan-Yi Lin and Fatemeh Sheikholeslami and jinghao shi and Leslie Rice and J Zico Kolter},\nyear={2021},\nurl={https://openreview.net/forum?id=vttv9ADGuWF}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "vttv9ADGuWF", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3584/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3584/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3584/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3584/Authors|ICLR.cc/2021/Conference/Paper3584/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3584/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923835992, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3584/-/Official_Comment"}}}, {"id": "GTFigNVUns", "original": null, "number": 4, "cdate": 1606261235495, "ddate": null, "tcdate": 1606261235495, "tmdate": 1606261235495, "tddate": null, "forum": "vttv9ADGuWF", "replyto": "LMoxIS39q5k", "invitation": "ICLR.cc/2021/Conference/Paper3584/-/Official_Comment", "content": {"title": "We've added suggested analysis and extra experiments in Appendix, and revised the manuscript for improved readability ", "comment": "Thank you for the thorough and helpful review. We've added a brief overview of De-randomized smoothing and PatchGuard in Appendix A, pseudo code explaining how certification probability is computed in Appendix B, and derivations and experimental results for uniform sampling without replacement in Appendix C. We've also expanded discussion on how certification rate is reported at test time in the \"metric\" paragraph in Section 4 and the effect of crop size vs patch size in Section 4.4. We analyze all three components of $f_{\\theta}$ in the \"Training randomized cropping classifier\" paragraph of Section 3.1 and show that $f_{\\theta}$ has the same set of trainable parameters as $g_{\\theta}$. We had several passes of revising the manuscript to improve the readability."}, "signatures": ["ICLR.cc/2021/Conference/Paper3584/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3584/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Certified robustness against physically-realizable patch attack via randomized cropping", "authorids": ["~Wan-Yi_Lin1", "~Fatemeh_Sheikholeslami1", "~jinghao_shi1", "~Leslie_Rice1", "~J_Zico_Kolter1"], "authors": ["Wan-Yi Lin", "Fatemeh Sheikholeslami", "jinghao shi", "Leslie Rice", "J Zico Kolter"], "keywords": ["adversarial machine learning", "certifiable defense", "patch attack"], "abstract": "This paper studies a certifiable defense against adversarial patch attacks on image classification. Our approach classifies random crops from the original image independently and the original image is classified as the vote over these crops.  This process minimizes changes to the training process, as only the crop classification model needs to be trained, and can be trained in a standard manner without explicit adversarial training.  Leveraging the fact that a patch attack can only influence some pixels of the image, we derive certified robustness bounds on the resulting classification. Our method is particularly effective when realistic physical transformations are applied to the adversarial patch, such as affine transformations. Such transformations occur naturally when an adversarial patch is physically introduced to a scene. Our method improves upon the current state of the art in defending against patch attacks on CIFAR10 and ImageNet, both in terms of certified accuracy and inference time.", "one-sentence_summary": "This paper studies certifiable defense against adversarial patch attacks on image classification using random sub-regions of the original image.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lin|certified_robustness_against_physicallyrealizable_patch_attack_via_randomized_cropping", "pdf": "/pdf/71e9d68eb2a9a96b809252a936cdab1fc1779d1a.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=RAoyaoQJF", "_bibtex": "@misc{\nlin2021certified,\ntitle={Certified robustness against physically-realizable patch attack via randomized cropping},\nauthor={Wan-Yi Lin and Fatemeh Sheikholeslami and jinghao shi and Leslie Rice and J Zico Kolter},\nyear={2021},\nurl={https://openreview.net/forum?id=vttv9ADGuWF}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "vttv9ADGuWF", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3584/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3584/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3584/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3584/Authors|ICLR.cc/2021/Conference/Paper3584/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3584/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923835992, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3584/-/Official_Comment"}}}], "count": 10}