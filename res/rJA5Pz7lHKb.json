{"notes": [{"id": "rJA5Pz7lHKb", "original": "GSpK0SO4DmE", "number": 2584, "cdate": 1601308286042, "ddate": null, "tcdate": 1601308286042, "tmdate": 1611607670322, "tddate": null, "forum": "rJA5Pz7lHKb", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Improved Autoregressive Modeling with Distribution Smoothing", "authorids": ["~Chenlin_Meng1", "~Jiaming_Song1", "~Yang_Song1", "~Shengjia_Zhao1", "~Stefano_Ermon1"], "authors": ["Chenlin Meng", "Jiaming Song", "Yang Song", "Shengjia Zhao", "Stefano Ermon"], "keywords": ["generative models", "autoregressive models"], "abstract": "While autoregressive models excel at image compression, their sample quality is often lacking. Inspired by randomized smoothing for adversarial defense, we incorporate randomized smoothing techniques into autoregressive generative modeling. We first model a smoothed version of the data distribution and then recover the data distribution by learning to reverse the smoothing process. We demonstrate empirically on a 1-d dataset that by appropriately choosing the smoothing level, we can keep the proposed process relatively easier to model than directly learning a data distribution with a high Lipschitz constant. Since autoregressive generative modeling consists of a sequence of 1-d density estimation problems, we believe the same arguments can be generalized to an autoregressive model. This seemingly simple procedure drastically improves the sample quality of existing autoregressive models on several synthetic and real-world datasets while obtaining competitive likelihoods on synthetic datasets.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "meng|improved_autoregressive_modeling_with_distribution_smoothing", "pdf": "/pdf/577d16e60a71a84127c2aae7d49e2f36e1975360.pdf", "venueid": "ICLR.cc/2021/Conference", "venue": "ICLR 2021 Oral", "_bibtex": "@inproceedings{\nmeng2021improved,\ntitle={Improved Autoregressive Modeling with Distribution Smoothing},\nauthor={Chenlin Meng and Jiaming Song and Yang Song and Shengjia Zhao and Stefano Ermon},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=rJA5Pz7lHKb}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "nRKhyHWd-2a", "original": null, "number": 1, "cdate": 1610667284358, "ddate": null, "tcdate": 1610667284358, "tmdate": 1611037494504, "tddate": null, "forum": "rJA5Pz7lHKb", "replyto": "rJA5Pz7lHKb", "invitation": "ICLR.cc/2021/Conference/Paper2584/-/Comment", "content": {"title": "NLL calculation in table 1 and section 5", "comment": "Thanks for your insightful paper. \n\nHowever, I have a small question about facts presented in Section 4.2 2-D synthetic datasets. The paper estimates the density through modeling p(noise x) p(x | noise x), then how can we calculate the exact negative log-likelihoods shown in Table 1?  To calculate p(x), we need to marginalize noise x,  which is intractable by integrating p(noise x, x).\n\n\nUpdate: The p(x) is an approximation. There is  q(noise |x) which could be a good proposal distribution for important sampling and also can be approximated by ELBO. "}, "signatures": ["~Qinsheng_Zhang1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "~Qinsheng_Zhang1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improved Autoregressive Modeling with Distribution Smoothing", "authorids": ["~Chenlin_Meng1", "~Jiaming_Song1", "~Yang_Song1", "~Shengjia_Zhao1", "~Stefano_Ermon1"], "authors": ["Chenlin Meng", "Jiaming Song", "Yang Song", "Shengjia Zhao", "Stefano Ermon"], "keywords": ["generative models", "autoregressive models"], "abstract": "While autoregressive models excel at image compression, their sample quality is often lacking. Inspired by randomized smoothing for adversarial defense, we incorporate randomized smoothing techniques into autoregressive generative modeling. We first model a smoothed version of the data distribution and then recover the data distribution by learning to reverse the smoothing process. We demonstrate empirically on a 1-d dataset that by appropriately choosing the smoothing level, we can keep the proposed process relatively easier to model than directly learning a data distribution with a high Lipschitz constant. Since autoregressive generative modeling consists of a sequence of 1-d density estimation problems, we believe the same arguments can be generalized to an autoregressive model. This seemingly simple procedure drastically improves the sample quality of existing autoregressive models on several synthetic and real-world datasets while obtaining competitive likelihoods on synthetic datasets.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "meng|improved_autoregressive_modeling_with_distribution_smoothing", "pdf": "/pdf/577d16e60a71a84127c2aae7d49e2f36e1975360.pdf", "venueid": "ICLR.cc/2021/Conference", "venue": "ICLR 2021 Oral", "_bibtex": "@inproceedings{\nmeng2021improved,\ntitle={Improved Autoregressive Modeling with Distribution Smoothing},\nauthor={Chenlin Meng and Jiaming Song and Yang Song and Shengjia Zhao and Stefano Ermon},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=rJA5Pz7lHKb}\n}"}, "tags": [], "invitation": {"reply": {"forum": "rJA5Pz7lHKb", "readers": {"values": ["everyone"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2021/Conference/Paper2584/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2584/Authors|ICLR.cc/2021/Conference/Paper2584/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs"}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}}, "multiReply": true, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["everyone"], "tcdate": 1610649445748, "tmdate": 1610649445748, "id": "ICLR.cc/2021/Conference/Paper2584/-/Comment"}}}, {"id": "246-NASfjI", "original": null, "number": 2, "cdate": 1610937470405, "ddate": null, "tcdate": 1610937470405, "tmdate": 1610937644708, "tddate": null, "forum": "rJA5Pz7lHKb", "replyto": "nRKhyHWd-2a", "invitation": "ICLR.cc/2021/Conference/Paper2584/-/Comment", "content": {"title": "I guess it is possible to marginalize in 2-d toy data", "comment": "and in high-dim they report ELBO."}, "signatures": ["~Zhisheng_Xiao1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "~Zhisheng_Xiao1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improved Autoregressive Modeling with Distribution Smoothing", "authorids": ["~Chenlin_Meng1", "~Jiaming_Song1", "~Yang_Song1", "~Shengjia_Zhao1", "~Stefano_Ermon1"], "authors": ["Chenlin Meng", "Jiaming Song", "Yang Song", "Shengjia Zhao", "Stefano Ermon"], "keywords": ["generative models", "autoregressive models"], "abstract": "While autoregressive models excel at image compression, their sample quality is often lacking. Inspired by randomized smoothing for adversarial defense, we incorporate randomized smoothing techniques into autoregressive generative modeling. We first model a smoothed version of the data distribution and then recover the data distribution by learning to reverse the smoothing process. We demonstrate empirically on a 1-d dataset that by appropriately choosing the smoothing level, we can keep the proposed process relatively easier to model than directly learning a data distribution with a high Lipschitz constant. Since autoregressive generative modeling consists of a sequence of 1-d density estimation problems, we believe the same arguments can be generalized to an autoregressive model. This seemingly simple procedure drastically improves the sample quality of existing autoregressive models on several synthetic and real-world datasets while obtaining competitive likelihoods on synthetic datasets.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "meng|improved_autoregressive_modeling_with_distribution_smoothing", "pdf": "/pdf/577d16e60a71a84127c2aae7d49e2f36e1975360.pdf", "venueid": "ICLR.cc/2021/Conference", "venue": "ICLR 2021 Oral", "_bibtex": "@inproceedings{\nmeng2021improved,\ntitle={Improved Autoregressive Modeling with Distribution Smoothing},\nauthor={Chenlin Meng and Jiaming Song and Yang Song and Shengjia Zhao and Stefano Ermon},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=rJA5Pz7lHKb}\n}"}, "tags": [], "invitation": {"reply": {"forum": "rJA5Pz7lHKb", "readers": {"values": ["everyone"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2021/Conference/Paper2584/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2584/Authors|ICLR.cc/2021/Conference/Paper2584/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs"}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}}, "multiReply": true, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["everyone"], "tcdate": 1610649445748, "tmdate": 1610649445748, "id": "ICLR.cc/2021/Conference/Paper2584/-/Comment"}}}, {"id": "nbd4ifViwmA", "original": null, "number": 1, "cdate": 1610040375808, "ddate": null, "tcdate": 1610040375808, "tmdate": 1610473967995, "tddate": null, "forum": "rJA5Pz7lHKb", "replyto": "rJA5Pz7lHKb", "invitation": "ICLR.cc/2021/Conference/Paper2584/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Oral)", "comment": "All reviewers recommend acceptance. Some concerns were raised about the precision of theorem 2 (now renamed to proposition 1), as well as the analysis of hyperparameter choices and quantitative evaluation, which I believe the authors have adequately addressed. Based on a suggestion of reviewer 1, experiments with flow-based models were also added, which demonstrates that the method is not strictly tied to autoregressive models. Personally, I was also curious about the connection between noise injection and quantisation, which the authors responded to by adding a paragraph discussing this connection in the manuscript.\n\nI would recommend that the authors also add the kernel inception distance (KID) results reported in the comments to the manuscript.\n\nThis work stands out to me in that it combines a relatively simple, easy to understand idea with nice results, which is a trait of many impactful papers. I will therefore join the reviewers in recommending acceptance."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improved Autoregressive Modeling with Distribution Smoothing", "authorids": ["~Chenlin_Meng1", "~Jiaming_Song1", "~Yang_Song1", "~Shengjia_Zhao1", "~Stefano_Ermon1"], "authors": ["Chenlin Meng", "Jiaming Song", "Yang Song", "Shengjia Zhao", "Stefano Ermon"], "keywords": ["generative models", "autoregressive models"], "abstract": "While autoregressive models excel at image compression, their sample quality is often lacking. Inspired by randomized smoothing for adversarial defense, we incorporate randomized smoothing techniques into autoregressive generative modeling. We first model a smoothed version of the data distribution and then recover the data distribution by learning to reverse the smoothing process. We demonstrate empirically on a 1-d dataset that by appropriately choosing the smoothing level, we can keep the proposed process relatively easier to model than directly learning a data distribution with a high Lipschitz constant. Since autoregressive generative modeling consists of a sequence of 1-d density estimation problems, we believe the same arguments can be generalized to an autoregressive model. This seemingly simple procedure drastically improves the sample quality of existing autoregressive models on several synthetic and real-world datasets while obtaining competitive likelihoods on synthetic datasets.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "meng|improved_autoregressive_modeling_with_distribution_smoothing", "pdf": "/pdf/577d16e60a71a84127c2aae7d49e2f36e1975360.pdf", "venueid": "ICLR.cc/2021/Conference", "venue": "ICLR 2021 Oral", "_bibtex": "@inproceedings{\nmeng2021improved,\ntitle={Improved Autoregressive Modeling with Distribution Smoothing},\nauthor={Chenlin Meng and Jiaming Song and Yang Song and Shengjia Zhao and Stefano Ermon},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=rJA5Pz7lHKb}\n}"}, "tags": [], "invitation": {"reply": {"forum": "rJA5Pz7lHKb", "replyto": "rJA5Pz7lHKb", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040375793, "tmdate": 1610473967979, "id": "ICLR.cc/2021/Conference/Paper2584/-/Decision"}}}, {"id": "BgtnXaLSxF9", "original": null, "number": 2, "cdate": 1603627918904, "ddate": null, "tcdate": 1603627918904, "tmdate": 1606768505669, "tddate": null, "forum": "rJA5Pz7lHKb", "replyto": "rJA5Pz7lHKb", "invitation": "ICLR.cc/2021/Conference/Paper2584/-/Official_Review", "content": {"title": "Advances on autoregressive models. Well-written, simple idea with good results.", "review": "**Summary.** Autoregressive models have demonstrate their potential utility for modeling images and other types of complex data with high flexibility (particularly in density estimation). However, its sampling ability is not that good as explained in the paper. Authors show that one of the main weaknesses of autoregressive models comes from the propagation of mistakes due to the mismatch of conditionals. Inspired in the promising results of randomized smoothing in adversarial models (Cohen et al. 2019), authors propose a similar strategy. The addition of Gaussian noise and posterior modeling of the smoother data makes easier to the autoregressive density to capture the true data distribution. The benefits of this strategy are empirically proved and shown in the experiments.\n\n**Strengths.** The quality of writing is high and the presentation of the paper facilitates the process of reading. I have to say that I enjoyed while reviewing it. The analysis and description of problems for sampling from autoregressive models is completely understandable to me and I agree with the manifold hypothesis held. \n\nResults with the \u201csharp\u201d multimodal data looks reliable to me and I believe that the smoother process can also reduce the lipschitz constant as stated in Theorem 1. Until pp. 5, nothing is said about the data denoising process, so one could initially think that there is no way to recover the target density without noise, but authors also did an effort on this. Good point. It is important to remark that the randomized smoothing process can be reverted once learning finishes. \n\nAdditionally, I particularly like how authors first present the idea on 1-d examples, later in the experiments, the method is validated with 2-d rings and finally, as stated in the introduction, with different image datasets. \n\nFinally, I did not find any similar work that mixes the idea of smoothing for improving autoregressive modelling. \n\n**Weaknesses, Questions & Recommendations.**\nTo me, there are 3 main points of weakness:\n[W1]. A lack of analysis about the optimal noise for randomized smoothing.\n[W2]. Why just Gaussian noise, what if data is discrete, could we do this with another type of noise?\n[W3]. Comments about denoising are included a bit late in the manuscript. I think that authors should remark that this is a reversible process.\n\nMy main questions are:\n[Q1]. In section 2.2, I do not see why data closer to the manifold should have larger first order derivatives or even infinity. Is this a bit counter intuitive, or not? Like, better positioned, worse gradient values?\n[Q2]. Is the 1/N term in the global likelihood expression of 1st paragraph of section 2 correct?\n[Q3]. If I do not appropriately choose the \\sigma parameter for smoothing, do I have the risk of not capturing some modes of the original data? I have the opinion that adding too much or too less noise to data could \u201cmask\u201d modes and something could be lost. Am I correct? Did authors empirically analyzed this in the experiments?\n[Q4]. How could we assess that conditionals are now better fitted than before?\n\nA few recommendations for improvement:\n[Rec1]. I would explain a bit more the manifold hypothesis of section 2.2, maybe a diagram or figure would help for quicker comprehension of the problem.\n[Rec2]. Some acronym for \u201crandomized smoothing\u201d would help in the 1st paragraph of section 3.1. To avoid repetitive expressions.\n\n**Reasons for score.** I liked the idea, think that the paper is well written and I trust the results presented by the authors. Despite the randomized smoothing strategy is rather simple, it seems to work particularly well. For this reason I tend to vote for accept. If I not set a higher score, it is because a bit more of analysis on the optimal sigma, distribution for smoothing and lipschitz constant could have been included. \n\n**Post-rebuttal update.** Thanks to the authors for their response to all my questions and comments. I also read the updated version of the manuscript, which is clearly improved and the rest of reviews and comments by the AC. Looking to that, I agree with the rest of reviewers about the quality of the paper, so I raised my score and I recommend to accept it.\n\n", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2584/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2584/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improved Autoregressive Modeling with Distribution Smoothing", "authorids": ["~Chenlin_Meng1", "~Jiaming_Song1", "~Yang_Song1", "~Shengjia_Zhao1", "~Stefano_Ermon1"], "authors": ["Chenlin Meng", "Jiaming Song", "Yang Song", "Shengjia Zhao", "Stefano Ermon"], "keywords": ["generative models", "autoregressive models"], "abstract": "While autoregressive models excel at image compression, their sample quality is often lacking. Inspired by randomized smoothing for adversarial defense, we incorporate randomized smoothing techniques into autoregressive generative modeling. We first model a smoothed version of the data distribution and then recover the data distribution by learning to reverse the smoothing process. We demonstrate empirically on a 1-d dataset that by appropriately choosing the smoothing level, we can keep the proposed process relatively easier to model than directly learning a data distribution with a high Lipschitz constant. Since autoregressive generative modeling consists of a sequence of 1-d density estimation problems, we believe the same arguments can be generalized to an autoregressive model. This seemingly simple procedure drastically improves the sample quality of existing autoregressive models on several synthetic and real-world datasets while obtaining competitive likelihoods on synthetic datasets.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "meng|improved_autoregressive_modeling_with_distribution_smoothing", "pdf": "/pdf/577d16e60a71a84127c2aae7d49e2f36e1975360.pdf", "venueid": "ICLR.cc/2021/Conference", "venue": "ICLR 2021 Oral", "_bibtex": "@inproceedings{\nmeng2021improved,\ntitle={Improved Autoregressive Modeling with Distribution Smoothing},\nauthor={Chenlin Meng and Jiaming Song and Yang Song and Shengjia Zhao and Stefano Ermon},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=rJA5Pz7lHKb}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "rJA5Pz7lHKb", "replyto": "rJA5Pz7lHKb", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2584/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538093041, "tmdate": 1606915765689, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2584/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2584/-/Official_Review"}}}, {"id": "vWh4UgeXY6u", "original": null, "number": 3, "cdate": 1603821855154, "ddate": null, "tcdate": 1603821855154, "tmdate": 1606391182762, "tddate": null, "forum": "rJA5Pz7lHKb", "replyto": "rJA5Pz7lHKb", "invitation": "ICLR.cc/2021/Conference/Paper2584/-/Official_Review", "content": {"title": "Interesting empirical results but might benefit from more careful discussion.", "review": "#### SHORT DESCRIPTION \nThis paper proposes a two-stage generative modeling approach, first learning a distribution over noised data, then learning the original data distribution conditioned on this noised data. The paper demonstrates that this leads to improved sample quality compared to fitting the data distribution directly.\n\n\n#### DISCUSSION\nOverall, I like this paper: it's a straightforward idea, decently motivated and fairly well described, and has good supporting empirical evaluation. I didn't expect the sampling performance to improve substantially by adding just a single denoising step, and I think demonstrating this is a good contribution. However, I think the paper could be improved by some more careful discussion, and a better placement in the literature.\n\n\"Theorem 2 shows that our smoothing process provides a regularization effect on the original objective... This regularization effect can, intuitively, increase the generalization capability of the model.\" How does the extra term in the theorem lead to a regularization effect? Why does this 'intuitively' increase the generalization capability of the model? Unless I'm mistaken, the added term is (up to a constant) the Laplacian of the log-likelihood w.r.t. data. The objective maximizes this on average across observed data, which intuitively minimizes the 'curvature' or 'steepness' of the log-likelihood at observed data, thus presumably smoothing the maximum likelihood solution. This Laplacian term also appears in the score matching objective presented in Theorem 1 of 'Estimation of Non-Normalized Statistical Models by Score Matching, Hyvarinen 2005', where it is minimized instead of maximized. There are also known connections between score matching and denoising methods e.g. 'Optimal Approximation of Signal Priors, Hyvarinen 2006', and 'A Connection Between Score Matching and Denoising Autoencoders, Vincent 2011', which you've cited in passing later. Much of this material and how it relates to the objective in Theorem 2 might be discussed in more depth rather than passing over it as simply a 'regularization term'.   \n\n\"Our approach is related to two-stage VAE (Dai & Wipf, 2019) which introduces a second VAE to correct the errors made by the first VAE.\" I'm not sure I agree with this. The idea of the two-stage VAE in that paper is to clean up mismatch between the aggregate posterior q(z) and the prior p(z). On the other hand, your variational model is identical to the canonical VAE setup: x is data, z is noised data, the 'posterior' q(z | x) is fixed and adds Gaussian noise, the 'prior' p(z) is a powerful autoregressive model, and the observation likelihood p(x | z) is another powerful autoregressive model (the canonical VAE would have learned q(z | x), fixed simple p(z), and learned but simple p(x | z)). This is one of the reasons 'VAE' can a confusing term when used to describe latent variable generative models in general: assuming something should be 'encoded' and 'decoded' can sometimes obfuscate the actual probabilistic model. What you propose in this paper might generically be called a 'denoising VAE', but again that's maybe not the most accurate description. I think the most closely related work is probably the denoising diffusion and denoising score-matching approaches which have received attention recently and which you've mentioned, but you could also think of it as turning a denoising autoencoder into a generative model. In any case, I think a more careful discussion of these points would be beneficial for the paper.\n \nFinally, the approach isn't really tied to autoregressive models, apart from the motivation given in terms of smoothing 1D distributions. It's fairly likely that the same idea could readily be applied to e.g. normalizing flows and that it would work well there also, so it would have been nice to see experiments featuring flows included here. This would be especially useful since your best-performing two-stage method takes autoregressive models, which are already slow samplers, and effectively doubles their sampling time. \n\n\n#### EXTRA NOTES \nMaybe be careful with the word 'spurious' in the intro - I know what you mean, but samples from a model are by definition typical samples from that model, and there's nothing spurious about them. They're only questionable when compared to data. Similarly: \"The \u201cerroneous\u201d sample x\u02c6, in some sense, resembles an adversarial example, i.e., an input that causes the model to make mistakes.\" This seems to be implying that samples generated by a model are somehow pathological. By virtue of the fact that they are generated by the model, they are *by definition* typical samples from the model. There is nothing pathological whatsoever about them. Why the model specification and fitting procedure have resulted in such samples, and whether the samples resemble training data or not, is another issue entirely.\n\n'However, this approach bounds the capacity of the model by limiting the number of modes for each conditional.' All models have limited capacity -- what is particularly bad about the capacity of an autoregressive model being limited in this way? Do we have reason to believe inability to cover multiple nodes is a common bottleneck?\n\nFigure 2 & Figure 3: Axis ticks are too small (and there probably too many), whole figure could be made bigger (this would also help with legends cutting off a lot of the plots).\n\n'Proofs' for theorems 1 and 2 should be referred to in the main text. Theorem 2 should also have log p(x) on the LHS? \n\nFigure 5 caption: \"All the samples are not conditioned on class labels.\" -> None of the samples are conditioned on class labels.\n\nWhat exactly is being inpainted in Figures 7 (a) and (b)? \n\nSince a central claim of the paper is that the method results in improved sample quality, it might be good to add the Kernel Inception Distance (\"Demystifying MMD GANs\" Binkowski et al 2018) which has many favourable properties over FID, and is really no more difficult to compute.  \n\n#### CONCLUSION \nOverall, I think this paper is a nice submission, and would like to see it accepted given a few tweaks. \n\nUPDATE: I've upped my score to a 7, and would like to see the paper accepted. ", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2584/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2584/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improved Autoregressive Modeling with Distribution Smoothing", "authorids": ["~Chenlin_Meng1", "~Jiaming_Song1", "~Yang_Song1", "~Shengjia_Zhao1", "~Stefano_Ermon1"], "authors": ["Chenlin Meng", "Jiaming Song", "Yang Song", "Shengjia Zhao", "Stefano Ermon"], "keywords": ["generative models", "autoregressive models"], "abstract": "While autoregressive models excel at image compression, their sample quality is often lacking. Inspired by randomized smoothing for adversarial defense, we incorporate randomized smoothing techniques into autoregressive generative modeling. We first model a smoothed version of the data distribution and then recover the data distribution by learning to reverse the smoothing process. We demonstrate empirically on a 1-d dataset that by appropriately choosing the smoothing level, we can keep the proposed process relatively easier to model than directly learning a data distribution with a high Lipschitz constant. Since autoregressive generative modeling consists of a sequence of 1-d density estimation problems, we believe the same arguments can be generalized to an autoregressive model. This seemingly simple procedure drastically improves the sample quality of existing autoregressive models on several synthetic and real-world datasets while obtaining competitive likelihoods on synthetic datasets.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "meng|improved_autoregressive_modeling_with_distribution_smoothing", "pdf": "/pdf/577d16e60a71a84127c2aae7d49e2f36e1975360.pdf", "venueid": "ICLR.cc/2021/Conference", "venue": "ICLR 2021 Oral", "_bibtex": "@inproceedings{\nmeng2021improved,\ntitle={Improved Autoregressive Modeling with Distribution Smoothing},\nauthor={Chenlin Meng and Jiaming Song and Yang Song and Shengjia Zhao and Stefano Ermon},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=rJA5Pz7lHKb}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "rJA5Pz7lHKb", "replyto": "rJA5Pz7lHKb", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2584/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538093041, "tmdate": 1606915765689, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2584/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2584/-/Official_Review"}}}, {"id": "X-UCVdAoyN", "original": null, "number": 6, "cdate": 1606254299444, "ddate": null, "tcdate": 1606254299444, "tmdate": 1606286577440, "tddate": null, "forum": "rJA5Pz7lHKb", "replyto": "BgtnXaLSxF9", "invitation": "ICLR.cc/2021/Conference/Paper2584/-/Official_Comment", "content": {"title": "We use the median value of the Euclidean distance between two data points in a dataset for choosing the smoothing distribution on images (Saremi & Hyvarinen, 2019; Garreau et al., 2017). ", "comment": "We thank Reviewer 3 for the constructive feedback and writing suggestions! We have uploaded the revision to address the feedback.\n\n**Q: A lack of analysis about the optimal noise for randomized smoothing.**\n\n**A:** We discussed the selection of noise in section 3.3. In general, we believe the optimal noise depends on the task. For instance, if our goal is to obtain better likelihoods, optimizing the noise distribution using ELBO could be a way of finding the optimal noise for randomized smoothing. However, if our goal is to generate high quality image samples, training the noise distribution via ELBO might not be the best option since better likelihoods do not necessarily imply better sample qualities (A note on the evaluation of generative models, Theis et al. (2015)). In practice, we also find that optimizing the noise using ELBO is not able to provide high quality image samples. \n\nAs there is not an efficient objective function which we can directly optimize on for improved sample quality, we use heuristics from (Saremi & Hyvarinen, 2019; Garreau et al., 2017) and select the noise distribution according to the median value of the Euclidean distance between two data points in a dataset. We find selecting the smoothing distribution with heuristics is able to drastically improve the image sample quality. We believe if there exists efficient objective functions for sample quality just as ELBO for likelihoods, we can directly optimize the objective function to obtain the optimal noise distribution.\n\n**Q: Why just Gaussian noise, what if data is discrete, could we do this with another type of noise?**\n\n**A:** In section 4.1, we study the performance of different types of noise including uniform, Laplace and Gaussian noise on a 1-d toy dataset. We find that in the 1-d example, all the three types of noise are able to work reasonably well, with Gaussian noise slightly outperforming the other two noise distributions. Since state-of-the-art models such as NCSN (Song & Ermon (2019; 2020)) and denoising diffusion probabilistic model (Ho et al., 2020) also use Gaussian noise, this motivates us to use Gaussian noise for the later experiments. We believe the optimal type of noise for other discrete data could be different and domain knowledge may be required to design the optimal noise distribution. \n\n**Q: Comments about denoising are included a bit late in the manuscript.**\n\n**A:** We have included comments about denoising in the introduction.\n\n**Q: In section 2.2, I do not see why data closer to the manifold should have larger first order derivatives or even infinity.** \n\n**A:** We have included an example on the manifold hypothesis in section 2.2. We also add a figure to provide intuition. \n\n**Q: Is the 1/N term in the global likelihood expression of 1st paragraph of section 2 correct?**\n\n**A:** Yes, the 1/N term is correct. It is an unbiased Monte Carlo estimation of the expectation which is equivalent to sampling mean.\n\n**Q: If I do not appropriately choose the sigma parameter for smoothing, do I have the risk of not capturing the original data well?**\n\n**A:** Yes, if the sigma parameter for smoothing is not chosen appropriately, the performance of our method can be affected negatively. The rightmost panel in Figure 3 (Figure 4 in the revision) has demonstrated this point where we compute ELBO for various sigmas on three types of smoothing distributions and we observe a reverse U-shape correlation between the variance of the smoothing distribution and ELBO (see section 4.1 for more details). We also have a related discussion in section 3.3. More specifically, when the smoothing distribution has a zero variance, no noise is added to the data distribution. In this case, modeling the smoothed distribution would be equivalent to modeling $p_{data}(\\textbf x)$, which is the same as the original method. When the smoothing distribution has an infinite variance, modeling the denoising model is equivalent to directly modeling $p_{data}(\\textbf x)$, which is also the same as the original modeling method. \n\n**Q: How could we assess that conditionals are now better fitted than before?**\n\n**A:** We believe that the visualization and loss curves in the 1-d example in Figure 4 (used to be Figure 3), 2-d example in Figure 5 (used to be Figure 4), and the improved image sample quality in image experiments can provide insights that the model can capture the conditionals reasonably well. \n\n**Q: Some acronyms for \u201crandomized smoothing\u201d in section 3.1**\n\n**A:** We have used \u201csmoothing\u201d as the acronym for \u201crandomized smoothing\u201d in section 3.1 in the revision."}, "signatures": ["ICLR.cc/2021/Conference/Paper2584/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2584/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improved Autoregressive Modeling with Distribution Smoothing", "authorids": ["~Chenlin_Meng1", "~Jiaming_Song1", "~Yang_Song1", "~Shengjia_Zhao1", "~Stefano_Ermon1"], "authors": ["Chenlin Meng", "Jiaming Song", "Yang Song", "Shengjia Zhao", "Stefano Ermon"], "keywords": ["generative models", "autoregressive models"], "abstract": "While autoregressive models excel at image compression, their sample quality is often lacking. Inspired by randomized smoothing for adversarial defense, we incorporate randomized smoothing techniques into autoregressive generative modeling. We first model a smoothed version of the data distribution and then recover the data distribution by learning to reverse the smoothing process. We demonstrate empirically on a 1-d dataset that by appropriately choosing the smoothing level, we can keep the proposed process relatively easier to model than directly learning a data distribution with a high Lipschitz constant. Since autoregressive generative modeling consists of a sequence of 1-d density estimation problems, we believe the same arguments can be generalized to an autoregressive model. This seemingly simple procedure drastically improves the sample quality of existing autoregressive models on several synthetic and real-world datasets while obtaining competitive likelihoods on synthetic datasets.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "meng|improved_autoregressive_modeling_with_distribution_smoothing", "pdf": "/pdf/577d16e60a71a84127c2aae7d49e2f36e1975360.pdf", "venueid": "ICLR.cc/2021/Conference", "venue": "ICLR 2021 Oral", "_bibtex": "@inproceedings{\nmeng2021improved,\ntitle={Improved Autoregressive Modeling with Distribution Smoothing},\nauthor={Chenlin Meng and Jiaming Song and Yang Song and Shengjia Zhao and Stefano Ermon},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=rJA5Pz7lHKb}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "rJA5Pz7lHKb", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2584/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2584/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2584/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2584/Authors|ICLR.cc/2021/Conference/Paper2584/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2584/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923846664, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2584/-/Official_Comment"}}}, {"id": "EujIwmWPZkd", "original": null, "number": 5, "cdate": 1606252824020, "ddate": null, "tcdate": 1606252824020, "tmdate": 1606285962548, "tddate": null, "forum": "rJA5Pz7lHKb", "replyto": "vWh4UgeXY6u", "invitation": "ICLR.cc/2021/Conference/Paper2584/-/Official_Comment", "content": {"title": "Our method can also be applied to other models like normalizing flow models.", "comment": "We thank Reviewer 1 for the constructive feedback and writing suggestions! We have uploaded the revision to address the feedback.\n\n**Q: How does the extra term in Theorem 2 lead to a regularization effect? Why does this 'intuitively' increase the generalization capability of the model?**\n\n**A:** We have incorporated your feedback into the discussion on Theorem 2 (Proposition 1 in the revision). We have also added its connection to score matching and other denoising methods in the discussion.\n\n**Q: The idea of Two-stage VAE (Dai & Wipf, 2019) might not be directly related to our methods.**\n\n**A:** We agree that the idea and motivation of two-stage VAE are different from ours. We have removed the discussion on two-stage VAE in the revision.\n\n**Q: Applying the same randomized smoothing method to other models like normalizing flow models.**\n\n**A:** Yes, this method is not limited to autoregressive models. It can also be applied to other models like normalizing flow models. In the revision, we introduce section 5, a new section which shows on 2D synthetic datasets that for RealNVP (Dinh et al, (2016)), the \u201crandomized smoothing\u201d approach is also able to generate better samples (according to human observers) than the original method and obtain competitive likelihoods while using comparable number of parameters.\n\n**Q: 'However, this approach bounds the capacity of the model by limiting the number of modes for each conditional.' All models have limited capacity -- what is particularly bad about the capacity of an autoregressive model being limited in this way? Do we have reason to believe inability to cover multiple nodes is a common bottleneck?**\n\n**A:** In Figure 4 (used to be Figure 3), we show on a 1-d example that when the capacity of the model is limited in this way, the model will not perform well if there are more modes in the distribution than that of the model. In this case, the model will assign much higher density to the low density region due to the mode covering property of MLE, which could cause unrealistic samples compared to the data distribution. Even when the model is able to generate enough modes to model the target distribution (see Baseline(8) in Figure 4), the model can still fail to capture the data distribution if the data density has sharp transitions (e.g. a high Lipschitz constant). Since an autoregressive model decomposes a joint distribution into 1-d conditional distributions, the same analysis on this 1-d example could also be applicable to the modeling of each univariate conditional $p(x_i|\\textbf x_{<i})$ in autoregressive modeling. \n\nMoreover, autoregressive modeling also suffers from compounding error issues during sampling time, meaning that a failure in capturing $p(x_i|\\textbf x_{<i})$ could cause $x_i$ to be sampled from a low density region from $p(x_i|\\textbf x_{<i})$, introducing difficulty to sample the later pixels correctly, eventually producing a sample that is unlikely under the data distribution. Empirically, our methods have shown to improve the sample quality of PixelCNN++ by introducing more modes to the univariate conditionals in a flexible way so that the low density regions can also be captured in an improved way, which also aligns with our claims.\n\n**Q:  \u201cSpurious\u201d and \u201cerroneous\u201d are not the right words of describing samples generated by models.**\n\n**A:** We have changed \u201cspurious\u201d and \u201cerroneous\u201d  to \u201cquestionable\u201d and \u201cunrealistic\u201d respectively.\n\n**Q: Figure 2 and Figure 3 axis ticks are too small and the whole figure could be made bigger. Figure 5 caption can be improved.**\n\n**A:** We have modified the figures as well as their captions in the revision. \n\n**Q: Proofs for theorems should be referred to in the main text. Theorem 2 should also have log p(x) on the LHS.**\n\n**A:** We have added reference to the proofs of theorems in the main text. Yes, it should be \u201clog p(x)\u201d instead of \u201cp(x)\u201d in Theorem 2, we have fixed the typo in the revision and we thank the reviewer for pointing that out!\n\n**Q: KID scores on image samples.**\n\n**A:**  We evaluated the KID score of our method and compared it to the original PixelCNN++. The mean (standard deviation) KID scores are as follows:\n\nOur method: 0.022 (0.001), the original: 0.046 (0.002). \nThe KID score of our model is much better than the original PixelCNN++ baseline. \nAs a comparison, according to \u201cDemystifying MMD GANs\u201d (Binkowski et al 2018), WGAN-GP has KID: 0.026 (0.001)\nCram\u00e9r GAN has KID: 0.028 (0.001).\nWe believe this result has also demonstrated the effectiveness of our method in terms of improving image sample quality.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2584/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2584/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improved Autoregressive Modeling with Distribution Smoothing", "authorids": ["~Chenlin_Meng1", "~Jiaming_Song1", "~Yang_Song1", "~Shengjia_Zhao1", "~Stefano_Ermon1"], "authors": ["Chenlin Meng", "Jiaming Song", "Yang Song", "Shengjia Zhao", "Stefano Ermon"], "keywords": ["generative models", "autoregressive models"], "abstract": "While autoregressive models excel at image compression, their sample quality is often lacking. Inspired by randomized smoothing for adversarial defense, we incorporate randomized smoothing techniques into autoregressive generative modeling. We first model a smoothed version of the data distribution and then recover the data distribution by learning to reverse the smoothing process. We demonstrate empirically on a 1-d dataset that by appropriately choosing the smoothing level, we can keep the proposed process relatively easier to model than directly learning a data distribution with a high Lipschitz constant. Since autoregressive generative modeling consists of a sequence of 1-d density estimation problems, we believe the same arguments can be generalized to an autoregressive model. This seemingly simple procedure drastically improves the sample quality of existing autoregressive models on several synthetic and real-world datasets while obtaining competitive likelihoods on synthetic datasets.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "meng|improved_autoregressive_modeling_with_distribution_smoothing", "pdf": "/pdf/577d16e60a71a84127c2aae7d49e2f36e1975360.pdf", "venueid": "ICLR.cc/2021/Conference", "venue": "ICLR 2021 Oral", "_bibtex": "@inproceedings{\nmeng2021improved,\ntitle={Improved Autoregressive Modeling with Distribution Smoothing},\nauthor={Chenlin Meng and Jiaming Song and Yang Song and Shengjia Zhao and Stefano Ermon},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=rJA5Pz7lHKb}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "rJA5Pz7lHKb", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2584/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2584/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2584/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2584/Authors|ICLR.cc/2021/Conference/Paper2584/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2584/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923846664, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2584/-/Official_Comment"}}}, {"id": "8OCuyR1mS7p", "original": null, "number": 7, "cdate": 1606255022179, "ddate": null, "tcdate": 1606255022179, "tmdate": 1606278249816, "tddate": null, "forum": "rJA5Pz7lHKb", "replyto": "ysYRKsGUOCA", "invitation": "ICLR.cc/2021/Conference/Paper2584/-/Official_Comment", "content": {"title": "Detailed analysis on 2-d synthetic datasets is included. Improved figures in the revision.", "comment": "We thank you for the constructive feedback and writing suggestions! We have uploaded the revision to address the feedback.\n\n**Q: Intersection of rings on Olympics datasets and single-step (gradient-based) denoising on 2-d datasets.**\n\n**A:** We believe the reason why the intersections of the rings are not modeled well is because for a smoothed data sample $\\tilde x$ near the intersections of the ring, the ground truth $p(x|\\tilde x)$ is a complicated distribution that can be hard to be captured by the simple \"denoising\" model we used, which only has two mixture of logistics at each dimension. If we increase the flexibility of the denoising model, the intersection can be modeled in a better way with our method. We have updated the results in section 4.2 and showed an example with increased mixtures of logistics components for both the baseline (now 6 mixture of logistics, used to be 5) and our methods (now 3 mixture of logistics, used to be 2).\n\nIn Appendix B.2, we have added experiments where we compared the performance of our method when using 2, 3 and 4 mixtures of logistics for the MADE model with the baseline MADE model using 5, 6 and 7 mixtures of logistics. We show that by only increasing the mixture components from **2 to 3**, our method can capture the intersections of the Olympics distribution in an improved way. **The baseline MADE model, on the other hand, still has trouble modeling the two sides of the rings when using 7 mixtures of logistics** on the Olympics dataset. This shows the effectiveness of our methods. We have included more analysis in Appendix B.\n\nWe also provide single-step denoising results on 2-d datasets in Appendix B.2. However, the denoising results are not very good when the smoothing distribution has a relatively large variance since single-step denoising use $\\mathbb {E}[x|\\tilde x]$ as a substitute for the denoised $\\tilde x$, which might not be ideal in practice.\n\n**Q: Not clear what debiasing methods are used.**\n\n**A:** We have clarified the text and provide \u201csingle-step denoising\u201d (gradient based denoising) results in Appendix B.\n\n**Q: The authors should not mention denoising and inpainting applications if there is no quantitative assessment. What is the corrupted input in Figure 7?**\n\n**A:** We have moved the denoising and inpainting experiments to Appendix C.2. In Figure 7 (now Figure 14 in the revision), the bottom half of the image is being inpainted. We have clarified the text and included an image which shows the corrupted input (see Appendix C.2).\n\n**Q: Both theorems are provided with nice demonstrations, the authors should refer to the demonstration in the core text of the article.**\n\n**A:** We have added references to the proofs in the main article.\n\n**Q: Add small arrows in Table 2 and typos in the paper.**\n\n**A:** We have added small arrows to Table 2. We have also fixed the typo in the revision.\n\n**Q: Figure 3 : Right panel : What are the 3 shaded curves? The x-axis should be specified \u2018Variance of $q(\\tilde x|x)$\u2019.**\n\n**A:** The shaded curves were the loss curves without smoothing the loss values. We have updated the plots (Figure 4 in the revision) by only showing the loss curves after smoothing (the loss values). We have also updated the x-axis according to the feedback.\n\n**Q: It does not seem hard to conclusively determine the best way of choosing $q(\\tilde{x}|x)$; if you want good samples, then it has to be set from heuristic; if you want good likelihood, then it can be directly optimized**\n\n**A:** We agree.  By saying  \u201cit is hard to conclusively determine\u201d, we mean it can be hard to design the heuristic that works best in practice."}, "signatures": ["ICLR.cc/2021/Conference/Paper2584/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2584/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improved Autoregressive Modeling with Distribution Smoothing", "authorids": ["~Chenlin_Meng1", "~Jiaming_Song1", "~Yang_Song1", "~Shengjia_Zhao1", "~Stefano_Ermon1"], "authors": ["Chenlin Meng", "Jiaming Song", "Yang Song", "Shengjia Zhao", "Stefano Ermon"], "keywords": ["generative models", "autoregressive models"], "abstract": "While autoregressive models excel at image compression, their sample quality is often lacking. Inspired by randomized smoothing for adversarial defense, we incorporate randomized smoothing techniques into autoregressive generative modeling. We first model a smoothed version of the data distribution and then recover the data distribution by learning to reverse the smoothing process. We demonstrate empirically on a 1-d dataset that by appropriately choosing the smoothing level, we can keep the proposed process relatively easier to model than directly learning a data distribution with a high Lipschitz constant. Since autoregressive generative modeling consists of a sequence of 1-d density estimation problems, we believe the same arguments can be generalized to an autoregressive model. This seemingly simple procedure drastically improves the sample quality of existing autoregressive models on several synthetic and real-world datasets while obtaining competitive likelihoods on synthetic datasets.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "meng|improved_autoregressive_modeling_with_distribution_smoothing", "pdf": "/pdf/577d16e60a71a84127c2aae7d49e2f36e1975360.pdf", "venueid": "ICLR.cc/2021/Conference", "venue": "ICLR 2021 Oral", "_bibtex": "@inproceedings{\nmeng2021improved,\ntitle={Improved Autoregressive Modeling with Distribution Smoothing},\nauthor={Chenlin Meng and Jiaming Song and Yang Song and Shengjia Zhao and Stefano Ermon},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=rJA5Pz7lHKb}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "rJA5Pz7lHKb", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2584/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2584/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2584/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2584/Authors|ICLR.cc/2021/Conference/Paper2584/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2584/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923846664, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2584/-/Official_Comment"}}}, {"id": "CSHXlcl3MJp", "original": null, "number": 4, "cdate": 1606252109012, "ddate": null, "tcdate": 1606252109012, "tmdate": 1606258836084, "tddate": null, "forum": "rJA5Pz7lHKb", "replyto": "n2nNvRoTi71", "invitation": "ICLR.cc/2021/Conference/Paper2584/-/Official_Comment", "content": {"title": "Our smoothing model is unconditioned on class labels during training.", "comment": "We would like to first thank Reviewer 2 for the constructive feedback and writing suggestions! We have uploaded the revision to address the feedback.\n\n**Q: \"q(noisy x| x) is simply zero\" not being precise.**\n\n**A:** We have rephrased \"q(noisy x| x) is simply zero\" to \u201ca distribution with all its mass at x\u201d in the revision.\n\n**Q: Theorem 2 contains a typo: the left side of the equation should be \"log p\" instead of \"p\".**\n\n**A:** Yes, it should be \u201clog p\u201d instead of \u201cp\u201d. We have fixed this typo in Theorem 2 (Proposition 1 in the revision).\n\n**Q: \"Theorem 2\" might be more appropriately called simply a \"Proposition\" instead of a \"Theorem\".**\n\n**A:** We have changed Theorem 2 to Proposition 1, since the core idea is related to the one in (Bishop, 1995).\n\n**Q: Clarification of Figure 5 columns.**\n\n**A:** We have clarified the column number by adding the column number to Figure 5 (Figure 6 in the revision) and updating the caption.\n\n**Q: The regions of images being inpainted.**\n\n**A:** The bottom half of the image is being impainted. We have included text and figures to show the inpainted region. In the revision, we moved the \u201cimage inpainting\u201d experiments to Appendix C.2.\n\n**Q: Figure 9 says \"unconditioned\", but it's about p(x | noisy x). Wouldn't that be the opposite of \"unconditioned\"?**\n\n**A:** \u201cUnconditioned samples\u201d in Figure 9 (now Figure 17) means the model is not conditioned on class labels. The opposite of \"unconditioned\" are models conditioned on class labels during training, which would have a positive effect on sample quality. We apologize for the confusion and we have clarified the image captions in the revision."}, "signatures": ["ICLR.cc/2021/Conference/Paper2584/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2584/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improved Autoregressive Modeling with Distribution Smoothing", "authorids": ["~Chenlin_Meng1", "~Jiaming_Song1", "~Yang_Song1", "~Shengjia_Zhao1", "~Stefano_Ermon1"], "authors": ["Chenlin Meng", "Jiaming Song", "Yang Song", "Shengjia Zhao", "Stefano Ermon"], "keywords": ["generative models", "autoregressive models"], "abstract": "While autoregressive models excel at image compression, their sample quality is often lacking. Inspired by randomized smoothing for adversarial defense, we incorporate randomized smoothing techniques into autoregressive generative modeling. We first model a smoothed version of the data distribution and then recover the data distribution by learning to reverse the smoothing process. We demonstrate empirically on a 1-d dataset that by appropriately choosing the smoothing level, we can keep the proposed process relatively easier to model than directly learning a data distribution with a high Lipschitz constant. Since autoregressive generative modeling consists of a sequence of 1-d density estimation problems, we believe the same arguments can be generalized to an autoregressive model. This seemingly simple procedure drastically improves the sample quality of existing autoregressive models on several synthetic and real-world datasets while obtaining competitive likelihoods on synthetic datasets.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "meng|improved_autoregressive_modeling_with_distribution_smoothing", "pdf": "/pdf/577d16e60a71a84127c2aae7d49e2f36e1975360.pdf", "venueid": "ICLR.cc/2021/Conference", "venue": "ICLR 2021 Oral", "_bibtex": "@inproceedings{\nmeng2021improved,\ntitle={Improved Autoregressive Modeling with Distribution Smoothing},\nauthor={Chenlin Meng and Jiaming Song and Yang Song and Shengjia Zhao and Stefano Ermon},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=rJA5Pz7lHKb}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "rJA5Pz7lHKb", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2584/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2584/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2584/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2584/Authors|ICLR.cc/2021/Conference/Paper2584/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2584/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923846664, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2584/-/Official_Comment"}}}, {"id": "ysYRKsGUOCA", "original": null, "number": 1, "cdate": 1603550805258, "ddate": null, "tcdate": 1603550805258, "tmdate": 1605024176431, "tddate": null, "forum": "rJA5Pz7lHKb", "replyto": "rJA5Pz7lHKb", "invitation": "ICLR.cc/2021/Conference/Paper2584/-/Official_Review", "content": {"title": "Good paper, well motivated and strongly grounded in theory", "review": "#### Summary of the paper :\nThe authors propose to improve the sample quality of autoregressive models. The authors propose to (1) - smooth the input data distribution leveraging methods that have shown success in adversarial defense, (2) recover input distribution by learning to reverse the smoothing process. The authors first demonstrate the efficiency of their method on 1d toy-problem, and extend the demonstration to more complex datasets such as MNIST, CIFAR-10 and CelebA with application such as image generation, inpaintting and denoising.\n\n#### Pros :\n* The idea to leverage a method previously used for adversarial defense to density estimation is interesting and novel.\n* The paper is well motivated through the manifold hypothesis approximation (which results in densities with high Lipschitz constants) and compounding errors.\n* The theory is strong \n\n#### Cons :\n* The experiments on denoising and inpainting are only qualitative and suffer from a lack of quantitative evaluation.\n\n#### Recommandation :\nThe article is clear, well motivated, and have a strong theoretical grounding. Therefore I would tend to accept the article.\n\n#### Detailed comments :\n\n* The experiment on 2d synthetic datasets (especially the olympic dataset) should be discussed more thoroughly. First, it is not clear that the proposed model is generating better sample than the MADE baseline on this specific dataset. Second, the intersection between rings, in the olympic dataset, seems to be much poorly modeled with the proposed approach compared to the MADE baseline. What is the reason ?\n \n* In the section 3.2 the authors are introducing 2 different debiasing methods (either a denoising step or another autoregressive model). In the rest of the article it is not clear which of the two methods the authors are using. In addition, in the 2d toy-problem (i.e. ring and olympic) as the authors are choosing a gaussian smoothing both debiasing methods are usable. Therefore it would be interesting to show both methods and to describe thoroughly the differences (in addition, it might provide an answer to my previous point). \n\n* The authors should not mention denoising and inpainting applications if there is no quantitative assessment (at least in appendix)\u2026 For the inpainting part, the corrupted input are not even shown (which part of the image has been predicted). The denoising and inpainting experiments sounds like it\u2019s been rushed\u2026 \n\n#### Typos and suggestions to improve the paper :\n* Minor : Both theorems are provided with nice demonstrations, then the authors should refer to the demonstration in the core text of the article (e.g. see Appendix A).\n* Minor : Add small arrows in Table 2 to indicate that Inception score is better when lower, and opposite for FID\n* Typo : page 5, section 3.3, paragraph 2 : relative \u2014> relatively\n* Figure3 : Right panel : What are the 3 shaded curves ? This should be shown in the legend or at least in the caption\n* Figure3 : Right panel: In the x-axis it should be specified \u2018Variance of q(x^{\\tilde} \\mid x)\n* Page 7 : paragraph 1 : 'Thus, it is hard to conclusively determine what is the best way of choosing q(x \u0303|x).\u2019 \u2014> I  think the authors actually give the key to properly choose the noise level (i.e. variance). It seems to depend on the task : if one wants to generate  good samples, then the variance has to be set by heuristic. If one needs a good likelihood (e.g. for subsequent downstream tasks) then the variance could be optimized.\n* Figure 6 :  On my understanding, the part \u2018denoising\u2019 is redundant with the section image generation. It is interesting to mention the denoising application, but I am not convinced of the utility of the figure 6.\n* Figure 7 : What is the corrupted input ? Which part of the input has been masked ??\n", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2584/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2584/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improved Autoregressive Modeling with Distribution Smoothing", "authorids": ["~Chenlin_Meng1", "~Jiaming_Song1", "~Yang_Song1", "~Shengjia_Zhao1", "~Stefano_Ermon1"], "authors": ["Chenlin Meng", "Jiaming Song", "Yang Song", "Shengjia Zhao", "Stefano Ermon"], "keywords": ["generative models", "autoregressive models"], "abstract": "While autoregressive models excel at image compression, their sample quality is often lacking. Inspired by randomized smoothing for adversarial defense, we incorporate randomized smoothing techniques into autoregressive generative modeling. We first model a smoothed version of the data distribution and then recover the data distribution by learning to reverse the smoothing process. We demonstrate empirically on a 1-d dataset that by appropriately choosing the smoothing level, we can keep the proposed process relatively easier to model than directly learning a data distribution with a high Lipschitz constant. Since autoregressive generative modeling consists of a sequence of 1-d density estimation problems, we believe the same arguments can be generalized to an autoregressive model. This seemingly simple procedure drastically improves the sample quality of existing autoregressive models on several synthetic and real-world datasets while obtaining competitive likelihoods on synthetic datasets.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "meng|improved_autoregressive_modeling_with_distribution_smoothing", "pdf": "/pdf/577d16e60a71a84127c2aae7d49e2f36e1975360.pdf", "venueid": "ICLR.cc/2021/Conference", "venue": "ICLR 2021 Oral", "_bibtex": "@inproceedings{\nmeng2021improved,\ntitle={Improved Autoregressive Modeling with Distribution Smoothing},\nauthor={Chenlin Meng and Jiaming Song and Yang Song and Shengjia Zhao and Stefano Ermon},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=rJA5Pz7lHKb}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "rJA5Pz7lHKb", "replyto": "rJA5Pz7lHKb", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2584/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538093041, "tmdate": 1606915765689, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2584/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2584/-/Official_Review"}}}, {"id": "n2nNvRoTi71", "original": null, "number": 4, "cdate": 1603895662859, "ddate": null, "tcdate": 1603895662859, "tmdate": 1605024176289, "tddate": null, "forum": "rJA5Pz7lHKb", "replyto": "rJA5Pz7lHKb", "invitation": "ICLR.cc/2021/Conference/Paper2584/-/Official_Review", "content": {"title": "Good approach. Nice proof-of-concept. Straightforward paper.", "review": "Summary : This paper proposes an approach to modeling distributions based on a two-step process which involves sampling a noisy x first and then applying a denoising function. The theory is grounded in other works in the literature that use the connection between denoising and the gradient of log p(x) with respect to x.\n\nI enjoyed reading the paper and I think that the authors are definitely working in an exciting area. The authors do a good job in section 3.3 \"Tradeoff in modeling\" to explain clearly that everything about this \"two step\" method relies on striking the right balance between the tasks of modeling p(noisy x) and p(x | noisy x). When one is trivial, the other is very hard.\n\nEverything about this paper hinges on the fact that the authors are learning p(noisy x). If they didn't, this whole paper would be trivial and a rather useless exercise. It took me a few passes to realize that they were indeed learning p(noisy x). This is such an important thing, and I think it might be worth insisting a bit more on this. Otherwise it's easy to look at the pictures and to conclude that they are taking x from the training distribution, adding a small amount of noise, and then showing that they can remove the noise. The authors are doing more than that, and this is why I like this paper.\n\n\nSome more specific comments about the text.\n\nSection 3.3 has \"q(noisy x| x) is simply zero\" when it's in fact a distribution with all its mass around x; not around zero. We understand what the authors meant by the context, but I recommend rephrasing this.\n\nI like figure 1-2 for how they illustrate the concepts well.\n\nI like that the authors took the time to write Theorem 1 instead of hand waving to appeal to how reasonable result feels.\n\nTheorem 2 contains a typo in its statement. The left side of the equation should be \"log p\" instead of \"p\".\n\nI could argue that \"Theorem 2\" might be more appropriately called simply a \"Proposition\" instead of a \"Theorem\", but I will leave it up to the authors to decide that. However, using the \\approx notation without clarifying its precise meaning is unbecoming for a theorem. Here the authors intend to communicate something that there is an equality, but with an extra term o(epsilon^2) on the right-hand side, as epsilon->0, with that epsilon having some role in the definition of q(noisy x | x). I think this deserves to be part of the statement of this theorem. Plenty of things are \"almost equal\" to something else, so the precise meaning matters when stating a theorem.\n\nFigure 5 refers to columns in a way that doesn't feel natural to me. To my eye, column 2 looks bad, column 3 is the best, and column 4 looks the worse. I suspect that the description of the figure either treats the first column as \"Column 0\", or it starts counting \"column 1\" after skipping the leftmost column, or we just don't have the same visual appreciation of thumbnails (especially when they contain wild pixels like column 2 does).\n\nInpainting at the bottom of page 7 doesn't really say what regions of the image are cut out to be inpainted.\n\nFigure 9 says \"unconditioned\", but it's about p(x | noisy x). Wouldn't that be the opposite of \"unconditioned\"?", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2584/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2584/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improved Autoregressive Modeling with Distribution Smoothing", "authorids": ["~Chenlin_Meng1", "~Jiaming_Song1", "~Yang_Song1", "~Shengjia_Zhao1", "~Stefano_Ermon1"], "authors": ["Chenlin Meng", "Jiaming Song", "Yang Song", "Shengjia Zhao", "Stefano Ermon"], "keywords": ["generative models", "autoregressive models"], "abstract": "While autoregressive models excel at image compression, their sample quality is often lacking. Inspired by randomized smoothing for adversarial defense, we incorporate randomized smoothing techniques into autoregressive generative modeling. We first model a smoothed version of the data distribution and then recover the data distribution by learning to reverse the smoothing process. We demonstrate empirically on a 1-d dataset that by appropriately choosing the smoothing level, we can keep the proposed process relatively easier to model than directly learning a data distribution with a high Lipschitz constant. Since autoregressive generative modeling consists of a sequence of 1-d density estimation problems, we believe the same arguments can be generalized to an autoregressive model. This seemingly simple procedure drastically improves the sample quality of existing autoregressive models on several synthetic and real-world datasets while obtaining competitive likelihoods on synthetic datasets.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "meng|improved_autoregressive_modeling_with_distribution_smoothing", "pdf": "/pdf/577d16e60a71a84127c2aae7d49e2f36e1975360.pdf", "venueid": "ICLR.cc/2021/Conference", "venue": "ICLR 2021 Oral", "_bibtex": "@inproceedings{\nmeng2021improved,\ntitle={Improved Autoregressive Modeling with Distribution Smoothing},\nauthor={Chenlin Meng and Jiaming Song and Yang Song and Shengjia Zhao and Stefano Ermon},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=rJA5Pz7lHKb}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "rJA5Pz7lHKb", "replyto": "rJA5Pz7lHKb", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2584/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538093041, "tmdate": 1606915765689, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2584/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2584/-/Official_Review"}}}], "count": 12}