{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1528124426033, "tcdate": 1518472018075, "number": 309, "cdate": 1518472018075, "id": "HycIjFkPM", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "HycIjFkPM", "signatures": ["~Tim_Tsz-Kit_Lau1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "A Proximal Block Coordinate Descent Algorithm for Deep Neural Network Training", "abstract": "Training deep neural networks (DNNs) efficiently is a challenge due to the associated highly nonconvex optimization. The backpropagation (backprop) algorithm has long been the most widely used algorithm for gradient computation of parameters of DNNs and is used along with gradient descent-type algorithms for this optimization task. Recent work have shown the efficiency of block coordinate descent (BCD) type methods empirically for training DNNs. In view of this, we propose a novel algorithm based on the BCD method for training DNNs and provide its global convergence results built upon the powerful framework of the Kurdyka-Lojasiewicz (KL) property. Numerical experiments on standard datasets demonstrate its competitive efficiency against standard optimizers with backprop. ", "paperhash": "lau|a_proximal_block_coordinate_descent_algorithm_for_deep_neural_network_training", "keywords": ["Block coordinate descent", "nonconvex optimization", "Kurdyka-Lojasiewicz property", "deep neural network training"], "_bibtex": "@misc{\n  lau2018a,\n  title={A Proximal Block Coordinate Descent Algorithm for Deep Neural Network Training},\n  author={Tim Tsz-Kit Lau and Jinshan Zeng and Baoyuan Wu and Yuan Yao},\n  year={2018},\n  url={https://openreview.net/forum?id=HycIjFkPM}\n}", "authorids": ["timlautk@gmail.com", "jsh.zeng@gmail.com", "wubaoyuan1987@gmail.com", "yuany@ust.hk"], "authors": ["Tim Tsz-Kit Lau", "Jinshan Zeng", "Baoyuan Wu", "Yuan Yao"], "TL;DR": "An efficient block coordinate descent algorithm is proposed for training deep neural networks with convergence guarantees built upon the powerful framework of the Kurdyka-Lojasiewicz (KL) property according to a block multiconvex formulation of the training objective, whose competitive efficiency is demonstrated using the MNIST and CIFAR-10 datasets.", "pdf": "/pdf/5826800fb1f92b04ae5b7bc7802a7441946cb3f9.pdf"}, "nonreaders": [], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582898316, "tcdate": 1520501125190, "number": 1, "cdate": 1520501125190, "id": "r1pY-tR_f", "invitation": "ICLR.cc/2018/Workshop/-/Paper309/Official_Review", "forum": "HycIjFkPM", "replyto": "HycIjFkPM", "signatures": ["ICLR.cc/2018/Workshop/Paper309/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper309/AnonReviewer3"], "content": {"title": "efficiency of block coordinate descent for training DNNs", "rating": "7: Good paper, accept", "review": "Training deep neural networks (DNNs) can generally be recast as a nonconvex optimization problem. Many existing methods with guarantees work well for convex cases but rarely for nonconvex cases. This paper exploits the proximal point algorithm combining with the coordinate descent technique to reduce the computational complexity. As far as I see, the numerical experiments look good. However, I am not an expert in this field.", "confidence": "1: The reviewer's evaluation is an educated guess"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Proximal Block Coordinate Descent Algorithm for Deep Neural Network Training", "abstract": "Training deep neural networks (DNNs) efficiently is a challenge due to the associated highly nonconvex optimization. The backpropagation (backprop) algorithm has long been the most widely used algorithm for gradient computation of parameters of DNNs and is used along with gradient descent-type algorithms for this optimization task. Recent work have shown the efficiency of block coordinate descent (BCD) type methods empirically for training DNNs. In view of this, we propose a novel algorithm based on the BCD method for training DNNs and provide its global convergence results built upon the powerful framework of the Kurdyka-Lojasiewicz (KL) property. Numerical experiments on standard datasets demonstrate its competitive efficiency against standard optimizers with backprop. ", "paperhash": "lau|a_proximal_block_coordinate_descent_algorithm_for_deep_neural_network_training", "keywords": ["Block coordinate descent", "nonconvex optimization", "Kurdyka-Lojasiewicz property", "deep neural network training"], "_bibtex": "@misc{\n  lau2018a,\n  title={A Proximal Block Coordinate Descent Algorithm for Deep Neural Network Training},\n  author={Tim Tsz-Kit Lau and Jinshan Zeng and Baoyuan Wu and Yuan Yao},\n  year={2018},\n  url={https://openreview.net/forum?id=HycIjFkPM}\n}", "authorids": ["timlautk@gmail.com", "jsh.zeng@gmail.com", "wubaoyuan1987@gmail.com", "yuany@ust.hk"], "authors": ["Tim Tsz-Kit Lau", "Jinshan Zeng", "Baoyuan Wu", "Yuan Yao"], "TL;DR": "An efficient block coordinate descent algorithm is proposed for training deep neural networks with convergence guarantees built upon the powerful framework of the Kurdyka-Lojasiewicz (KL) property according to a block multiconvex formulation of the training objective, whose competitive efficiency is demonstrated using the MNIST and CIFAR-10 datasets.", "pdf": "/pdf/5826800fb1f92b04ae5b7bc7802a7441946cb3f9.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582898132, "id": "ICLR.cc/2018/Workshop/-/Paper309/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper309/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper309/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper309/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper309/AnonReviewer1"], "reply": {"forum": "HycIjFkPM", "replyto": "HycIjFkPM", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper309/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper309/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582898132}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582895352, "tcdate": 1520512975299, "number": 2, "cdate": 1520512975299, "id": "Syw0J20dG", "invitation": "ICLR.cc/2018/Workshop/-/Paper309/Official_Review", "forum": "HycIjFkPM", "replyto": "HycIjFkPM", "signatures": ["ICLR.cc/2018/Workshop/Paper309/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper309/AnonReviewer2"], "content": {"title": "The paper presents a BCD proximal algorithm for minimizing a DNN-type objective. The convergence analysis is based on known results applied to this particular setting where the functions are known to satisfy the KL inequality (e.g. semialgebraic or definable functions).", "rating": "6: Marginally above acceptance threshold", "review": "The paper presents a BCD proximal algorithm for minimizing a DNN-type objective. The convergence analysis is based on known results applied to this particular setting where the functions are known to satisfy the KL inequality (e.g. semialgebraic or definable functions). The paper suffers from many issues:\n1. The novelty is very limited (essentially applying known convergence results by checking the corresponding assumptions).\n2. Key references are missing (e.g.  the work of Sabach, Teboulle, Bolte and many others).\n3. Many material is taken from others' work (e.g. material on semialgebraic geometry).\n4. Proposition 1 is trivial.\n5. Theorem 2 is originally due to the work of Bolte et al. as well as to Frankel et al.\n6. The passage from (1) to (2) is NOT rigorous as stated. Seen as a penalty method, for it to work, \\gamma_l should be appropriately decreasing. From the Lagrangian \"duality\" perspective, existing and boundedness of \\gamma_l must be proved.\n7. In DNN, stochastic versions are applied and it would have been much more interesting to blend the stated analysis with stochastic coordinate descent.\n  ", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Proximal Block Coordinate Descent Algorithm for Deep Neural Network Training", "abstract": "Training deep neural networks (DNNs) efficiently is a challenge due to the associated highly nonconvex optimization. The backpropagation (backprop) algorithm has long been the most widely used algorithm for gradient computation of parameters of DNNs and is used along with gradient descent-type algorithms for this optimization task. Recent work have shown the efficiency of block coordinate descent (BCD) type methods empirically for training DNNs. In view of this, we propose a novel algorithm based on the BCD method for training DNNs and provide its global convergence results built upon the powerful framework of the Kurdyka-Lojasiewicz (KL) property. Numerical experiments on standard datasets demonstrate its competitive efficiency against standard optimizers with backprop. ", "paperhash": "lau|a_proximal_block_coordinate_descent_algorithm_for_deep_neural_network_training", "keywords": ["Block coordinate descent", "nonconvex optimization", "Kurdyka-Lojasiewicz property", "deep neural network training"], "_bibtex": "@misc{\n  lau2018a,\n  title={A Proximal Block Coordinate Descent Algorithm for Deep Neural Network Training},\n  author={Tim Tsz-Kit Lau and Jinshan Zeng and Baoyuan Wu and Yuan Yao},\n  year={2018},\n  url={https://openreview.net/forum?id=HycIjFkPM}\n}", "authorids": ["timlautk@gmail.com", "jsh.zeng@gmail.com", "wubaoyuan1987@gmail.com", "yuany@ust.hk"], "authors": ["Tim Tsz-Kit Lau", "Jinshan Zeng", "Baoyuan Wu", "Yuan Yao"], "TL;DR": "An efficient block coordinate descent algorithm is proposed for training deep neural networks with convergence guarantees built upon the powerful framework of the Kurdyka-Lojasiewicz (KL) property according to a block multiconvex formulation of the training objective, whose competitive efficiency is demonstrated using the MNIST and CIFAR-10 datasets.", "pdf": "/pdf/5826800fb1f92b04ae5b7bc7802a7441946cb3f9.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582898132, "id": "ICLR.cc/2018/Workshop/-/Paper309/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper309/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper309/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper309/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper309/AnonReviewer1"], "reply": {"forum": "HycIjFkPM", "replyto": "HycIjFkPM", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper309/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper309/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582898132}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582598212, "tcdate": 1521001731634, "number": 3, "cdate": 1521001731634, "id": "By2brX8KM", "invitation": "ICLR.cc/2018/Workshop/-/Paper309/Official_Review", "forum": "HycIjFkPM", "replyto": "HycIjFkPM", "signatures": ["ICLR.cc/2018/Workshop/Paper309/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper309/AnonReviewer1"], "content": {"title": "Review", "rating": "8: Top 50% of accepted papers, clear accept", "review": "This paper proposes a proximal BCD algorithm for training DNNs, and provide its global convergence results using KL property. My understanding is that the algorithm itself is not very novel. However, its application to DNN is new. I skipped the theoretical analysis in the appendix, and it looks correct and interesting. I thus recommend the paper to be accepted. \n\nI suggest the authors list a pseudo algorithm for the algorithm part for easier presentation, and the comparison between the algorithm and other existing algorithms should be better strengthen.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Proximal Block Coordinate Descent Algorithm for Deep Neural Network Training", "abstract": "Training deep neural networks (DNNs) efficiently is a challenge due to the associated highly nonconvex optimization. The backpropagation (backprop) algorithm has long been the most widely used algorithm for gradient computation of parameters of DNNs and is used along with gradient descent-type algorithms for this optimization task. Recent work have shown the efficiency of block coordinate descent (BCD) type methods empirically for training DNNs. In view of this, we propose a novel algorithm based on the BCD method for training DNNs and provide its global convergence results built upon the powerful framework of the Kurdyka-Lojasiewicz (KL) property. Numerical experiments on standard datasets demonstrate its competitive efficiency against standard optimizers with backprop. ", "paperhash": "lau|a_proximal_block_coordinate_descent_algorithm_for_deep_neural_network_training", "keywords": ["Block coordinate descent", "nonconvex optimization", "Kurdyka-Lojasiewicz property", "deep neural network training"], "_bibtex": "@misc{\n  lau2018a,\n  title={A Proximal Block Coordinate Descent Algorithm for Deep Neural Network Training},\n  author={Tim Tsz-Kit Lau and Jinshan Zeng and Baoyuan Wu and Yuan Yao},\n  year={2018},\n  url={https://openreview.net/forum?id=HycIjFkPM}\n}", "authorids": ["timlautk@gmail.com", "jsh.zeng@gmail.com", "wubaoyuan1987@gmail.com", "yuany@ust.hk"], "authors": ["Tim Tsz-Kit Lau", "Jinshan Zeng", "Baoyuan Wu", "Yuan Yao"], "TL;DR": "An efficient block coordinate descent algorithm is proposed for training deep neural networks with convergence guarantees built upon the powerful framework of the Kurdyka-Lojasiewicz (KL) property according to a block multiconvex formulation of the training objective, whose competitive efficiency is demonstrated using the MNIST and CIFAR-10 datasets.", "pdf": "/pdf/5826800fb1f92b04ae5b7bc7802a7441946cb3f9.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582898132, "id": "ICLR.cc/2018/Workshop/-/Paper309/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper309/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper309/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper309/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper309/AnonReviewer1"], "reply": {"forum": "HycIjFkPM", "replyto": "HycIjFkPM", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper309/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper309/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582898132}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521573554312, "tcdate": 1521573554312, "number": 49, "cdate": 1521573553965, "id": "r1q2AR0FG", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "HycIjFkPM", "replyto": "HycIjFkPM", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Accept", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "Congratulations, your paper was accepted to the ICLR workshop."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Proximal Block Coordinate Descent Algorithm for Deep Neural Network Training", "abstract": "Training deep neural networks (DNNs) efficiently is a challenge due to the associated highly nonconvex optimization. The backpropagation (backprop) algorithm has long been the most widely used algorithm for gradient computation of parameters of DNNs and is used along with gradient descent-type algorithms for this optimization task. Recent work have shown the efficiency of block coordinate descent (BCD) type methods empirically for training DNNs. In view of this, we propose a novel algorithm based on the BCD method for training DNNs and provide its global convergence results built upon the powerful framework of the Kurdyka-Lojasiewicz (KL) property. Numerical experiments on standard datasets demonstrate its competitive efficiency against standard optimizers with backprop. ", "paperhash": "lau|a_proximal_block_coordinate_descent_algorithm_for_deep_neural_network_training", "keywords": ["Block coordinate descent", "nonconvex optimization", "Kurdyka-Lojasiewicz property", "deep neural network training"], "_bibtex": "@misc{\n  lau2018a,\n  title={A Proximal Block Coordinate Descent Algorithm for Deep Neural Network Training},\n  author={Tim Tsz-Kit Lau and Jinshan Zeng and Baoyuan Wu and Yuan Yao},\n  year={2018},\n  url={https://openreview.net/forum?id=HycIjFkPM}\n}", "authorids": ["timlautk@gmail.com", "jsh.zeng@gmail.com", "wubaoyuan1987@gmail.com", "yuany@ust.hk"], "authors": ["Tim Tsz-Kit Lau", "Jinshan Zeng", "Baoyuan Wu", "Yuan Yao"], "TL;DR": "An efficient block coordinate descent algorithm is proposed for training deep neural networks with convergence guarantees built upon the powerful framework of the Kurdyka-Lojasiewicz (KL) property according to a block multiconvex formulation of the training objective, whose competitive efficiency is demonstrated using the MNIST and CIFAR-10 datasets.", "pdf": "/pdf/5826800fb1f92b04ae5b7bc7802a7441946cb3f9.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}], "count": 5}