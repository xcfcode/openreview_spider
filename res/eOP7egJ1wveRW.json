{"notes": [{"ddate": null, "legacy_migration": true, "tmdate": 1392879180000, "tcdate": 1392879180000, "number": 1, "id": "ccXQi_g3QhiMR", "invitation": "ICLR.cc/2014/-/submission/workshop/reply", "forum": "eOP7egJ1wveRW", "replyto": "OOxLKAd6LBO_C", "signatures": ["David Eigen"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "Thanks for your comments.\r\n\r\nTables 1 and 2 have comparisons for 1-layer MoE on the last lines of each table.\r\n\r\nRe: scalability:  We are currently using an all-experts mixture in this work, so haven't realized any computational gains yet.  However, the fact that the DMoE factorizes is an interesting result that we think is a promising step towards partitioning these networks efficiently.\r\n\r\nFor training curves, the paper is somewhat packed as it is, and already twice the recommended length for workshop submissions, so it seems infeasible to include these.  The reported results are on the final train/test split using fixed numbers of epochs validated beforehand."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Factored Representations in a Deep Mixture of Experts", "decision": "submitted, no decision", "abstract": "Mixtures of Experts combine the outputs of several 'expert' networks, each of which specializes in a different part of the input space. This is achieved by training a 'gating' network that maps each input to a distribution over the experts. Such models show promise for building larger networks that are still cheap to compute at test time, and more parallelizable at training time. In this this work, we extend the Mixture of Experts to a stacked model, the Deep Mixture of Experts, with multiple sets of gating and experts. This exponentially increases the number of effective experts by associating each input with a combination of experts at each layer, yet maintains a modest model size. On a randomly translated version of the MNIST dataset, we find that the Deep Mixture of Experts automatically learns to develop location-dependent ('where') experts at the first layer, and class-specific ('what') experts at the second layer. In addition, we see that the different combinations are in use when the model is applied to a dataset of speech monophones. These demonstrate effective use of all expert combinations.", "pdf": "https://arxiv.org/abs/1312.4314", "paperhash": "eigen|learning_factored_representations_in_a_deep_mixture_of_experts", "keywords": [], "conflicts": [], "authors": ["David Eigen", "Marc'Aurelio Ranzato", "Ilya Sutskever"], "authorids": ["deigen@cs.nyu.edu", "ranzato@google.com", "ilya.at.cs@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392878160000, "tcdate": 1392878160000, "number": 1, "id": "ccU6RwPFLaROG", "invitation": "ICLR.cc/2014/-/submission/workshop/reply", "forum": "eOP7egJ1wveRW", "replyto": "--5uYip1KdY1B", "signatures": ["David Eigen"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "Thank you for your comments and suggestions.  We now include DNN baselines for Jittered MNIST.  In response to your question re: which error rate, it is the phone error.  This has been updated in the new version as well."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Factored Representations in a Deep Mixture of Experts", "decision": "submitted, no decision", "abstract": "Mixtures of Experts combine the outputs of several 'expert' networks, each of which specializes in a different part of the input space. This is achieved by training a 'gating' network that maps each input to a distribution over the experts. Such models show promise for building larger networks that are still cheap to compute at test time, and more parallelizable at training time. In this this work, we extend the Mixture of Experts to a stacked model, the Deep Mixture of Experts, with multiple sets of gating and experts. This exponentially increases the number of effective experts by associating each input with a combination of experts at each layer, yet maintains a modest model size. On a randomly translated version of the MNIST dataset, we find that the Deep Mixture of Experts automatically learns to develop location-dependent ('where') experts at the first layer, and class-specific ('what') experts at the second layer. In addition, we see that the different combinations are in use when the model is applied to a dataset of speech monophones. These demonstrate effective use of all expert combinations.", "pdf": "https://arxiv.org/abs/1312.4314", "paperhash": "eigen|learning_factored_representations_in_a_deep_mixture_of_experts", "keywords": [], "conflicts": [], "authors": ["David Eigen", "Marc'Aurelio Ranzato", "Ilya Sutskever"], "authorids": ["deigen@cs.nyu.edu", "ranzato@google.com", "ilya.at.cs@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392877980000, "tcdate": 1392877980000, "number": 1, "id": "xxuVPAmBVc4BE", "invitation": "ICLR.cc/2014/-/submission/workshop/reply", "forum": "eOP7egJ1wveRW", "replyto": "T29y23Xay3UVQ", "signatures": ["David Eigen"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "Thank you for your comments.  In response to your questions:\r\n\r\n'does more (than two) layers of mixtures still help'\r\n\r\nWe did not try more than two layers yet.\r\n\r\n\r\n'reason for using the Jittered MNIST instead of the MNIST itself'\r\n\r\nJittering places digits at different spatial locations, which the first layer learns to factor out.  By jittering the dataset ourselves, we can explicitly measure this effect, as shown in Fig 2."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Factored Representations in a Deep Mixture of Experts", "decision": "submitted, no decision", "abstract": "Mixtures of Experts combine the outputs of several 'expert' networks, each of which specializes in a different part of the input space. This is achieved by training a 'gating' network that maps each input to a distribution over the experts. Such models show promise for building larger networks that are still cheap to compute at test time, and more parallelizable at training time. In this this work, we extend the Mixture of Experts to a stacked model, the Deep Mixture of Experts, with multiple sets of gating and experts. This exponentially increases the number of effective experts by associating each input with a combination of experts at each layer, yet maintains a modest model size. On a randomly translated version of the MNIST dataset, we find that the Deep Mixture of Experts automatically learns to develop location-dependent ('where') experts at the first layer, and class-specific ('what') experts at the second layer. In addition, we see that the different combinations are in use when the model is applied to a dataset of speech monophones. These demonstrate effective use of all expert combinations.", "pdf": "https://arxiv.org/abs/1312.4314", "paperhash": "eigen|learning_factored_representations_in_a_deep_mixture_of_experts", "keywords": [], "conflicts": [], "authors": ["David Eigen", "Marc'Aurelio Ranzato", "Ilya Sutskever"], "authorids": ["deigen@cs.nyu.edu", "ranzato@google.com", "ilya.at.cs@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392877860000, "tcdate": 1392877860000, "number": 1, "id": "__vZdXgmZXdMz", "invitation": "ICLR.cc/2014/-/submission/workshop/reply", "forum": "eOP7egJ1wveRW", "replyto": "3fVm9U8jmI9ZW", "signatures": ["David Eigen"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "Thank you for your review.  Responding to your various points:\r\n\r\n'A comparison against the fully connected DNN on the two tasks is needed'\r\n\r\nFor Jittered MNIST, we ran these baselines and are including the results.  For Monophone Speech, there are unfortunately some IP issues that prevent us from running this now -- however, there are still the second layer single-expert and concatenated-experts baselines.\r\n\r\n\r\n'It\u2019s not clear whether the claimed computation reduction is true'\r\n\r\nIn this work, we use the all-experts mixture and have no computational reductions yet.  We feel the fact that the model factorizes is a promising result in this direction, however.  This was explained in the discussion, but we will be more explicit about this in the introduction as well.\r\n\r\n\r\n'The concatenation trick improved the result on the MNIST.'\r\n\r\nThis concatenation was actually intended as a baseline target that the mixture should not be able to beat, since it concatenates the experts' outputs instead of superimposing them (this also increases the number of parameters in the final softmax layer).  We demonstrate that the DMoE falls in between this and the single-expert baseline -- it is best to be as close as possible to the concatenated experts bound.  This is explained at the bottom of page 3.\r\n\r\n\r\n'The models on row 2 and 3 are identical in Table 2 but the results are different'\r\n\r\nThanks for pointing this out; these used two different sized gating networks at the second mixture layer (50 and 20 hiddens).  We now include all the gating network sizes in these tables."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Factored Representations in a Deep Mixture of Experts", "decision": "submitted, no decision", "abstract": "Mixtures of Experts combine the outputs of several 'expert' networks, each of which specializes in a different part of the input space. This is achieved by training a 'gating' network that maps each input to a distribution over the experts. Such models show promise for building larger networks that are still cheap to compute at test time, and more parallelizable at training time. In this this work, we extend the Mixture of Experts to a stacked model, the Deep Mixture of Experts, with multiple sets of gating and experts. This exponentially increases the number of effective experts by associating each input with a combination of experts at each layer, yet maintains a modest model size. On a randomly translated version of the MNIST dataset, we find that the Deep Mixture of Experts automatically learns to develop location-dependent ('where') experts at the first layer, and class-specific ('what') experts at the second layer. In addition, we see that the different combinations are in use when the model is applied to a dataset of speech monophones. These demonstrate effective use of all expert combinations.", "pdf": "https://arxiv.org/abs/1312.4314", "paperhash": "eigen|learning_factored_representations_in_a_deep_mixture_of_experts", "keywords": [], "conflicts": [], "authors": ["David Eigen", "Marc'Aurelio Ranzato", "Ilya Sutskever"], "authorids": ["deigen@cs.nyu.edu", "ranzato@google.com", "ilya.at.cs@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391843280000, "tcdate": 1391843280000, "number": 4, "id": "--5uYip1KdY1B", "invitation": "ICLR.cc/2014/-/submission/workshop/review", "forum": "eOP7egJ1wveRW", "replyto": "eOP7egJ1wveRW", "signatures": ["anonymous reviewer 3af9"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Learning Factored Representations in a Deep Mixture of Experts", "review": "The paper introduce a deep mixture of experts model which contains multiple layers each of them contains multiple experts and a gating network. The idea is nice and the presentation is clear but the experiments lack proper, needed, comparisons with baseline systems for the Jittered MNIST and the monophone speech datasets.\r\n\r\nAs the authors mentioned in conclusion, the experiments use all experts for all data points which doesn\u2019t achieve the main purpose of the papers, i.e. faster training and testing. It is important to show how does this system perform against a deep NN baseline with the same number of parameters in terms of accuracy and training time per epoch.\r\nRegarding the speech task. What is the error you are presenting in Table 2, is it the Phone or Frame error rate?"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Factored Representations in a Deep Mixture of Experts", "decision": "submitted, no decision", "abstract": "Mixtures of Experts combine the outputs of several 'expert' networks, each of which specializes in a different part of the input space. This is achieved by training a 'gating' network that maps each input to a distribution over the experts. Such models show promise for building larger networks that are still cheap to compute at test time, and more parallelizable at training time. In this this work, we extend the Mixture of Experts to a stacked model, the Deep Mixture of Experts, with multiple sets of gating and experts. This exponentially increases the number of effective experts by associating each input with a combination of experts at each layer, yet maintains a modest model size. On a randomly translated version of the MNIST dataset, we find that the Deep Mixture of Experts automatically learns to develop location-dependent ('where') experts at the first layer, and class-specific ('what') experts at the second layer. In addition, we see that the different combinations are in use when the model is applied to a dataset of speech monophones. These demonstrate effective use of all expert combinations.", "pdf": "https://arxiv.org/abs/1312.4314", "paperhash": "eigen|learning_factored_representations_in_a_deep_mixture_of_experts", "keywords": [], "conflicts": [], "authors": ["David Eigen", "Marc'Aurelio Ranzato", "Ilya Sutskever"], "authorids": ["deigen@cs.nyu.edu", "ranzato@google.com", "ilya.at.cs@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391636100000, "tcdate": 1391636100000, "number": 2, "id": "OOxLKAd6LBO_C", "invitation": "ICLR.cc/2014/-/submission/workshop/review", "forum": "eOP7egJ1wveRW", "replyto": "eOP7egJ1wveRW", "signatures": ["Liangliang Cao"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "I am interested in the topic of this paper but my impression after reading is still that deep MOE is hard to train and we need to know a number of tricks including constrained training and fine tuning.  I would expect it would be harder to train deeper models (say, 4 or 5 layers)\r\n\r\nSeveral suggestions:\r\n- About experimental comparison. If I understand correctly, Table 1 and 2 only compare performances from several configurations of 2-layer MOE. I would be interesting to see how much better compared with basic MOE (1-layer).\r\n\r\n- About Jordan and Jacob's HMOE. Section 2 reviews the differences between DMOE and HMOE. Which model is more scalable? I am curious about the comparison with HMOE on both accuracy and speed.\r\n\r\n- Training + testing accuracy. I like that the current submission conveys more information by reporting the performance of training and testing. However, it will be even more interesting to report the curves of two errors during SGD training. Also I am a little confused: are you using a validation set with SGD? How is the performance on validation set during training?"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Factored Representations in a Deep Mixture of Experts", "decision": "submitted, no decision", "abstract": "Mixtures of Experts combine the outputs of several 'expert' networks, each of which specializes in a different part of the input space. This is achieved by training a 'gating' network that maps each input to a distribution over the experts. Such models show promise for building larger networks that are still cheap to compute at test time, and more parallelizable at training time. In this this work, we extend the Mixture of Experts to a stacked model, the Deep Mixture of Experts, with multiple sets of gating and experts. This exponentially increases the number of effective experts by associating each input with a combination of experts at each layer, yet maintains a modest model size. On a randomly translated version of the MNIST dataset, we find that the Deep Mixture of Experts automatically learns to develop location-dependent ('where') experts at the first layer, and class-specific ('what') experts at the second layer. In addition, we see that the different combinations are in use when the model is applied to a dataset of speech monophones. These demonstrate effective use of all expert combinations.", "pdf": "https://arxiv.org/abs/1312.4314", "paperhash": "eigen|learning_factored_representations_in_a_deep_mixture_of_experts", "keywords": [], "conflicts": [], "authors": ["David Eigen", "Marc'Aurelio Ranzato", "Ilya Sutskever"], "authorids": ["deigen@cs.nyu.edu", "ranzato@google.com", "ilya.at.cs@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391016120000, "tcdate": 1391016120000, "number": 3, "id": "T29y23Xay3UVQ", "invitation": "ICLR.cc/2014/-/submission/workshop/review", "forum": "eOP7egJ1wveRW", "replyto": "eOP7egJ1wveRW", "signatures": ["anonymous reviewer c87d"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Learning Factored Representations in a Deep Mixture of Experts", "review": "The paper extends the concept of mixtures of experts to multiple  layers of experts. Well, at least in theory - in practise authors stopped their experiments at only two such layers - which somehow invalidates the use of buzzy 'deep' word in the title - does more (than two) layers of mixtures still help?\r\n\r\nThe clue idea is to collaboratively optimise different sub-networks representing either experts or gating networks. Authors propose also 'the trick' to effectively learn mixing networks by preserving too rapid selection of dominant experts at the beginning of training stage.\r\n\r\nIt's hard to deduce whether presented idea gives a real advantage over, for example, usual -- one or two hidden layers feed-forward networks with the same total number of parameters. Perhaps, I am also missing something important here -- but was there any good reason for using the Jittered MNIST instead of the MNIST itself? In the end both are just toy benchmarks while the latter gives you the ability to cite and compare your work to other many other reported results. If you did that, not doing some basic baselines by yourself would be OK.\r\n\r\nI've got similar comments to the monophone voice classification. On the top I've already written for MNIST I do not see the need to use simplified  proprietary database. It would be better to do the experiments in TIMIT benchmark and then cite other works that reports frame accuracy (where a single frame is a monophone) so the reader could get a bit wider picture of how your work fits into broader perspective.\r\n\r\nAnyway, idea is sufficiently novel and interesting and I am in favour of  accept. Perhaps the authors could at least improve MNIST experimental aspect."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Factored Representations in a Deep Mixture of Experts", "decision": "submitted, no decision", "abstract": "Mixtures of Experts combine the outputs of several 'expert' networks, each of which specializes in a different part of the input space. This is achieved by training a 'gating' network that maps each input to a distribution over the experts. Such models show promise for building larger networks that are still cheap to compute at test time, and more parallelizable at training time. In this this work, we extend the Mixture of Experts to a stacked model, the Deep Mixture of Experts, with multiple sets of gating and experts. This exponentially increases the number of effective experts by associating each input with a combination of experts at each layer, yet maintains a modest model size. On a randomly translated version of the MNIST dataset, we find that the Deep Mixture of Experts automatically learns to develop location-dependent ('where') experts at the first layer, and class-specific ('what') experts at the second layer. In addition, we see that the different combinations are in use when the model is applied to a dataset of speech monophones. These demonstrate effective use of all expert combinations.", "pdf": "https://arxiv.org/abs/1312.4314", "paperhash": "eigen|learning_factored_representations_in_a_deep_mixture_of_experts", "keywords": [], "conflicts": [], "authors": ["David Eigen", "Marc'Aurelio Ranzato", "Ilya Sutskever"], "authorids": ["deigen@cs.nyu.edu", "ranzato@google.com", "ilya.at.cs@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1389855180000, "tcdate": 1389855180000, "number": 1, "id": "3fVm9U8jmI9ZW", "invitation": "ICLR.cc/2014/-/submission/workshop/review", "forum": "eOP7egJ1wveRW", "replyto": "eOP7egJ1wveRW", "signatures": ["anonymous reviewer 4f75"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Learning Factored Representations in a Deep Mixture of Experts", "review": "This paper extends the mixture-of-experts (MoE) model by stacking several blocks of the MoEs to form a deep MoE. In this model, each mixture weight is implemented with a gating network. The mixtures at each block is different. The whole deep MoE is trained jointly using the stochastic gradient descent algorithm. The motivation of the work is to reduce the decoding time by exploiting the structure imposed in the MoE model. The model was evaluated on the MNIST and speech monophone classification tasks.\r\n\r\nThe idea of deep MoE is interesting and, although not difficult to come out, is novel. I found the fact that the first and second blocks focus on distinguishing different patterns is particularly interesting. \r\n\r\nHowever, I feel that the effectiveness and the benefit of the model is not supported by the evidence presented in the paper. \r\n\r\n1.\tIt\u2019s not clear how or whether the proposed deep MoE can beat the fully connected normal DNNs if the same number of the model parameters are used (or even when deep MoEs use more parameters). A comparison against the fully connected DNN on the two tasks is needed. In many cases we don\u2019t want to sacrifice accuracy for small speed improvement. \r\n2.\tIt\u2019s not clear whether the claimed computation reduction is true. It would be desirable if a comparison on the computation cost between the deep MoE and the fully connected conventional DNN is provided when both the number of classes is small (say 10) and large (say 1K-10K). The comparison should also consider the fact that the sparseness pattern in the deep MoE is random and unknown beforehand may not save computation at all when SIMD instructions are used.\r\n3.\tIt is also unclear whether deep MoE performs better than the single-block MoE. It appears to me, according to the results presented, the deep MoE actually performs worse. The concatenation trick improved the result on the MNIST. However, from my experience, the gain is more likely from the concatenation of the hidden features instead of the deep architecture used.\r\n\r\nThere is also a minor presentation issue. The models on row 2 and 3 are identical in Table 2 but the results are different. What is the difference between these two models?"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Factored Representations in a Deep Mixture of Experts", "decision": "submitted, no decision", "abstract": "Mixtures of Experts combine the outputs of several 'expert' networks, each of which specializes in a different part of the input space. This is achieved by training a 'gating' network that maps each input to a distribution over the experts. Such models show promise for building larger networks that are still cheap to compute at test time, and more parallelizable at training time. In this this work, we extend the Mixture of Experts to a stacked model, the Deep Mixture of Experts, with multiple sets of gating and experts. This exponentially increases the number of effective experts by associating each input with a combination of experts at each layer, yet maintains a modest model size. On a randomly translated version of the MNIST dataset, we find that the Deep Mixture of Experts automatically learns to develop location-dependent ('where') experts at the first layer, and class-specific ('what') experts at the second layer. In addition, we see that the different combinations are in use when the model is applied to a dataset of speech monophones. These demonstrate effective use of all expert combinations.", "pdf": "https://arxiv.org/abs/1312.4314", "paperhash": "eigen|learning_factored_representations_in_a_deep_mixture_of_experts", "keywords": [], "conflicts": [], "authors": ["David Eigen", "Marc'Aurelio Ranzato", "Ilya Sutskever"], "authorids": ["deigen@cs.nyu.edu", "ranzato@google.com", "ilya.at.cs@gmail.com"]}, "tags": [], "invitation": {}}}, {"replyto": null, "ddate": null, "legacy_migration": true, "tmdate": 1387517040000, "tcdate": 1387517040000, "number": 18, "id": "eOP7egJ1wveRW", "invitation": "ICLR.cc/2014/workshop/-/submission", "forum": "eOP7egJ1wveRW", "signatures": ["deigen@cs.nyu.edu"], "readers": ["everyone"], "content": {"title": "Learning Factored Representations in a Deep Mixture of Experts", "decision": "submitted, no decision", "abstract": "Mixtures of Experts combine the outputs of several 'expert' networks, each of which specializes in a different part of the input space. This is achieved by training a 'gating' network that maps each input to a distribution over the experts. Such models show promise for building larger networks that are still cheap to compute at test time, and more parallelizable at training time. In this this work, we extend the Mixture of Experts to a stacked model, the Deep Mixture of Experts, with multiple sets of gating and experts. This exponentially increases the number of effective experts by associating each input with a combination of experts at each layer, yet maintains a modest model size. On a randomly translated version of the MNIST dataset, we find that the Deep Mixture of Experts automatically learns to develop location-dependent ('where') experts at the first layer, and class-specific ('what') experts at the second layer. In addition, we see that the different combinations are in use when the model is applied to a dataset of speech monophones. These demonstrate effective use of all expert combinations.", "pdf": "https://arxiv.org/abs/1312.4314", "paperhash": "eigen|learning_factored_representations_in_a_deep_mixture_of_experts", "keywords": [], "conflicts": [], "authors": ["David Eigen", "Marc'Aurelio Ranzato", "Ilya Sutskever"], "authorids": ["deigen@cs.nyu.edu", "ranzato@google.com", "ilya.at.cs@gmail.com"]}, "writers": [], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1369422751717, "tmdate": 1496674357014, "id": "ICLR.cc/2014/workshop/-/submission", "writers": ["ICLR.cc/2014"], "signatures": ["OpenReview.net"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": []}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1377198751717, "cdate": 1496674357014}}}], "count": 9}