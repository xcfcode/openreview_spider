{"notes": [{"id": "H1g6kaVKvH", "original": "H1xng_iHDS", "number": 318, "cdate": 1569438948772, "ddate": null, "tcdate": 1569438948772, "tmdate": 1577168230599, "tddate": null, "forum": "H1g6kaVKvH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"abstract": "Current deep neural networks can achieve remarkable performance on a single task. However, when the deep neural network is continually trained on a sequence of tasks, it seems to gradually forget the previous learned knowledge.  This phenomenon is referred to as catastrophic forgetting and motivates the field called lifelong learning.  The central question in lifelong learning is how to enable deep neural networks to maintain performance on old tasks while learning a new task. In this paper, we introduce a novel and effective lifelong learning algorithm, called MixEd stochastic GrAdient (MEGA), which allows deep neural networks to acquire the ability of retaining performance on old tasks while learning new tasks. MEGA modulates the balance between old tasks and the new task by integrating the current gradient with the gradient computed on a small reference episodic memory.  Extensive  experimental  results  show  that  the  proposed  MEGA  algorithm significantly advances the state-of-the-art on all four commonly used life-long learning benchmarks, reducing the error by up to 18%.", "title": "Learning with Long-term Remembering: Following the Lead of Mixed Stochastic Gradient", "code": "https://drive.google.com/file/d/1ZKrIpkf1VoZrotMZ9MyALKIbbB_qEvg5/view?usp=sharing", "keywords": ["lifelong learning", "continual learning"], "pdf": "/pdf/88e73de8e94a7b9e4c474cb98a3b01488736fbd1.pdf", "authors": ["Yunhui Guo", "Mingrui Liu", "Tianbao Yang", "Tajana Rosing"], "TL;DR": "A novel and effective lifelong learning algorithm which achieves the state-of-the-art results on several benchmarks.", "authorids": ["yug185@eng.ucsd.edu", "mingrui-liu@uiowa.edu", "tianbao-yang@uiowa.edu", "tajana@ucsd.edu"], "paperhash": "guo|learning_with_longterm_remembering_following_the_lead_of_mixed_stochastic_gradient", "original_pdf": "/attachment/8d21e424cf1fdbef12668ecb67a1c256ce6126c3.pdf", "_bibtex": "@misc{\nguo2020learning,\ntitle={Learning with Long-term Remembering: Following the Lead of Mixed Stochastic Gradient},\nauthor={Yunhui Guo and Mingrui Liu and Tianbao Yang and Tajana Rosing},\nyear={2020},\nurl={https://openreview.net/forum?id=H1g6kaVKvH}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "LUTNx7tR4", "original": null, "number": 1, "cdate": 1576798693166, "ddate": null, "tcdate": 1576798693166, "tmdate": 1576800942248, "tddate": null, "forum": "H1g6kaVKvH", "replyto": "H1g6kaVKvH", "invitation": "ICLR.cc/2020/Conference/Paper318/-/Decision", "content": {"decision": "Reject", "comment": "The submission is concerned with the catastrophic forgetting problem of continual learning, and proposes a gradient-based method which uses buffers of data seen previously to integrate the angles of the gradients and thereby mitigate forgetting. Empirical results are given on several benchmarks. \n\nThe reviewers were impressed with the thorough validation and strong results, but noticed that the much simpler MEGA-D baseline did almost as well. Given this, they were not convinced that the proposed approach was necessary. Although the authors provided a strong rebuttal and an additional ablation, the reviewers did not feel that their concerns were met.\n\nMy recommendation is to reject the submission at this time.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"abstract": "Current deep neural networks can achieve remarkable performance on a single task. However, when the deep neural network is continually trained on a sequence of tasks, it seems to gradually forget the previous learned knowledge.  This phenomenon is referred to as catastrophic forgetting and motivates the field called lifelong learning.  The central question in lifelong learning is how to enable deep neural networks to maintain performance on old tasks while learning a new task. In this paper, we introduce a novel and effective lifelong learning algorithm, called MixEd stochastic GrAdient (MEGA), which allows deep neural networks to acquire the ability of retaining performance on old tasks while learning new tasks. MEGA modulates the balance between old tasks and the new task by integrating the current gradient with the gradient computed on a small reference episodic memory.  Extensive  experimental  results  show  that  the  proposed  MEGA  algorithm significantly advances the state-of-the-art on all four commonly used life-long learning benchmarks, reducing the error by up to 18%.", "title": "Learning with Long-term Remembering: Following the Lead of Mixed Stochastic Gradient", "code": "https://drive.google.com/file/d/1ZKrIpkf1VoZrotMZ9MyALKIbbB_qEvg5/view?usp=sharing", "keywords": ["lifelong learning", "continual learning"], "pdf": "/pdf/88e73de8e94a7b9e4c474cb98a3b01488736fbd1.pdf", "authors": ["Yunhui Guo", "Mingrui Liu", "Tianbao Yang", "Tajana Rosing"], "TL;DR": "A novel and effective lifelong learning algorithm which achieves the state-of-the-art results on several benchmarks.", "authorids": ["yug185@eng.ucsd.edu", "mingrui-liu@uiowa.edu", "tianbao-yang@uiowa.edu", "tajana@ucsd.edu"], "paperhash": "guo|learning_with_longterm_remembering_following_the_lead_of_mixed_stochastic_gradient", "original_pdf": "/attachment/8d21e424cf1fdbef12668ecb67a1c256ce6126c3.pdf", "_bibtex": "@misc{\nguo2020learning,\ntitle={Learning with Long-term Remembering: Following the Lead of Mixed Stochastic Gradient},\nauthor={Yunhui Guo and Mingrui Liu and Tianbao Yang and Tajana Rosing},\nyear={2020},\nurl={https://openreview.net/forum?id=H1g6kaVKvH}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "H1g6kaVKvH", "replyto": "H1g6kaVKvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795722692, "tmdate": 1576800274051, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper318/-/Decision"}}}, {"id": "Skl8qrtstB", "original": null, "number": 1, "cdate": 1571685773585, "ddate": null, "tcdate": 1571685773585, "tmdate": 1574432052681, "tddate": null, "forum": "H1g6kaVKvH", "replyto": "H1g6kaVKvH", "invitation": "ICLR.cc/2020/Conference/Paper318/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #3", "review": "UPDATE:\n\nI thank the authors for proactively engaging with the review process and improving the paper.\n\nAfter considering the other reviews and discussions with other reviewers, I also share the concern that the simple MEGA-D baseline performs very well, with little additional gain from the full MEGA approach (only on the many permutations case that was introduced in the rebuttal). Unfortunately, it doesn't look like this point has been fully addressed.\n\nI know this is part of the contribution, and I am certainly an advocate of simple techniques that yield strong results. However, as it stands, this baseline is only mentioned briefly, with a single paragraph in the method section and a single paragraph on evaluation. Given the strength of the result, I think a lot more of the paper should be devoted to understanding the merits of this simple method and evaluating how it relates to the proposed angle-based approach.\n\nI am also a bit confused by the new baseline; given that the memory and current batch are both used for the MEGA-C case, I think the explanation of how this differs from the full case could be clearer.\n\nAs such, I must regrettably change my score to a 3. I think this paper has potential; and with a bit more analysis and clarity on the above points, could be a good submission. I encourage the authors to address these for a future publication. \n\n==============================================================\n\nThis paper describes an approach to perform continual learning by maintaining an episodic memory / coreset of old examples and learning a linear weighting function between the gradients from new and old samples. A very simple direct method is proposed, as well as an angle-based approach. In the latter, projected gradient ascent is used to find an optimal rotation angle for the current data gradient such that the resulting direction also aligns well with the gradients computed from the memory buffer. This appears to generalise previous work (such as GEM and A-GEM), and a new metric of long-term remembering (LTR) is also introduced.\n\nThe experiments are comprehensive and compelling.\nThe paper is clearly written and easy to follow, and I think it could be quite a good contribution to the conference.\n\nI have some questions and concerns that I think should be addressed first:\n1) If I understand correctly, Algorithm 1 seems to indicate that every single batch is added to the memory buffer - I assume this is an error, as it is suggested throughout the paper that only a small buffer is used. How is the memory buffer updated?\n2) It is unclear how much memory is required for this approach and whether this is consistent with previous approaches. An ablation over memory size would help with this (ideally with comparison to other episodic memory-based approaches); and a discussion on memory use of different methods is needed.\n3) With the direct approach, it seems odd to specify a loss threshold of zero to determine that the current task performance is high. What loss is being used? Further, how does the direct approach relate to eg. GEM/A-GEM/MER in terms of weighting between old and new samples?\n4) The related work section is quite thin, and there are several other works that could be cited; currently they seem to be focused on just \"gradient-similarity based continual learning\" with a few other continual learning works.\n5) The paper states that progressive networks increase in memory super-linearly, but I don't believe this is the case; it would be linear or sub-linear, given that new tasks would typically benefit from forward transfer and require fewer additional units.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper318/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper318/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"abstract": "Current deep neural networks can achieve remarkable performance on a single task. However, when the deep neural network is continually trained on a sequence of tasks, it seems to gradually forget the previous learned knowledge.  This phenomenon is referred to as catastrophic forgetting and motivates the field called lifelong learning.  The central question in lifelong learning is how to enable deep neural networks to maintain performance on old tasks while learning a new task. In this paper, we introduce a novel and effective lifelong learning algorithm, called MixEd stochastic GrAdient (MEGA), which allows deep neural networks to acquire the ability of retaining performance on old tasks while learning new tasks. MEGA modulates the balance between old tasks and the new task by integrating the current gradient with the gradient computed on a small reference episodic memory.  Extensive  experimental  results  show  that  the  proposed  MEGA  algorithm significantly advances the state-of-the-art on all four commonly used life-long learning benchmarks, reducing the error by up to 18%.", "title": "Learning with Long-term Remembering: Following the Lead of Mixed Stochastic Gradient", "code": "https://drive.google.com/file/d/1ZKrIpkf1VoZrotMZ9MyALKIbbB_qEvg5/view?usp=sharing", "keywords": ["lifelong learning", "continual learning"], "pdf": "/pdf/88e73de8e94a7b9e4c474cb98a3b01488736fbd1.pdf", "authors": ["Yunhui Guo", "Mingrui Liu", "Tianbao Yang", "Tajana Rosing"], "TL;DR": "A novel and effective lifelong learning algorithm which achieves the state-of-the-art results on several benchmarks.", "authorids": ["yug185@eng.ucsd.edu", "mingrui-liu@uiowa.edu", "tianbao-yang@uiowa.edu", "tajana@ucsd.edu"], "paperhash": "guo|learning_with_longterm_remembering_following_the_lead_of_mixed_stochastic_gradient", "original_pdf": "/attachment/8d21e424cf1fdbef12668ecb67a1c256ce6126c3.pdf", "_bibtex": "@misc{\nguo2020learning,\ntitle={Learning with Long-term Remembering: Following the Lead of Mixed Stochastic Gradient},\nauthor={Yunhui Guo and Mingrui Liu and Tianbao Yang and Tajana Rosing},\nyear={2020},\nurl={https://openreview.net/forum?id=H1g6kaVKvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "H1g6kaVKvH", "replyto": "H1g6kaVKvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper318/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper318/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575449446388, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper318/Reviewers"], "noninvitees": [], "tcdate": 1570237753868, "tmdate": 1575449446399, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper318/-/Official_Review"}}}, {"id": "BJgJKvDaYB", "original": null, "number": 2, "cdate": 1571809143313, "ddate": null, "tcdate": 1571809143313, "tmdate": 1574372770974, "tddate": null, "forum": "H1g6kaVKvH", "replyto": "H1g6kaVKvH", "invitation": "ICLR.cc/2020/Conference/Paper318/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #2", "review": "========================== Summary of paper ==========================\nThe paper proposes a new loss balancing approach for lifelong learning. The proposed method dynamically balances the old task loss (from an episodic memory) and the new task loss based on their magnitude. The paper outperforms state-of-the-art method (that don't use extra attribute info), and is straightforward to understand.\n\n========================== Decision ==========================\nI vote for rejecting the paper. The paper adds a very simple dynamic loss balancing to a joint loss, which has limited novelty, yet does not discuss its relationship with loss balancing in multitask learning. Although the method outperforms state-of-the-art, a very simple baseline of their method also outperforms state-of-the-art, making the contribution ineffective. The writing of the paper may also benefit from further edits.\n\n========================== Pros/Cons ==========================\n+ The paper outperforms state-of-the-art.\n+ The method is simple to explain and straightforward to implement.\n+ Proposes a weighted variant of forgetting measure (although the purpose is not justified -- need to discuss what use case would make one prefer this weighted variant better than the original).\n- The paper is not well-placed in the literature. Not until the bottom of page 5 can readers see very closely related methods, and despite the similarity, little is discussed about the difference. To improve, relationships with existing multitask learning weight balancing methods (e.g. https://arxiv.org/abs/1810.04650, https://arxiv.org/abs/1705.07115, https://arxiv.org/abs/1711.02257) should also be discussed, and maybe compared to.\n- Motivation to use the proposed loss balancing rather than that of very similar methods (e.g. GEM/A-GEM) is lacking (\"These approaches cannot capture the dynamics in the lifelong learning process\", but GEM's balancing is also dynamic)\n- Solving for beta using optimization (eq. 10), while (eq. 9) should have an explicit, close-form solution.\n- Writing sometimes is confusing, uses absolute language, or using claims unsupported by evidence.\n    - (1) page 3 \"In this case, the weights are only optimized for the current task while ignoring previous tasks which leads to catastrophic forgetting.\" ignores the existence of regularization-based methods such as EWC\n    - (2) bottom of page 3 \"alpha1 and alpha2 should be adjusted adaptively\" does not have experiment results supporting it (especially considering GEM is also adaptive)\n    - (3) \"l_ref(w; \u03b6) = 0 implies that there is almost no catastrophic forgetting\" claim is problematic. Overfitting to the episodic memory is a common problem.\n    - (3) page 3 xi and zeta not clearly defined. \n    - (4) brings up NP-hardness while it is seldom of interest in this field.\n    - (5) missing \"\u2207\" on the denominator in (eq. 7).\n- Experiment:\n    - The only ablation (the direct approach) is statistically indistinguishable from the proposed method. This also outperforms state-of-the-art, while it should not be. One can only assume that the experiment is problematic.\n    - Completely uses hyperparameter from an unpublished paper.\n    - Claims state-of-the-art, yet omits a state-of-the-art variant in cited A-GEM paper (with joint-embedding). This should be discussed even if the comparison is unfair.\n\n========================== Improving the paper ==========================\n- Rewrite the introduction so it is clear the paper is an improvement over A-GEM with a better loss balancing, and focus the motivation and side experiments on why this is important for lifelong learning.\n- Clean up writing.\n- Explain why the direct approach, while being a basic loss balancing method, outperform state-of-the-art GEM greatly. Perhaps a set of ablation studies can help.\n- Replace optimizing for beta as solving for beta.\n- Comparison with other loss balancing papers.\n- Clarify state-of-the-art comparison.\n\n========================== After rebuttal ==========================\nI appreciate the authors addressing my concerns about the placement in the literature; it looks clearer now. However, the rebuttal did not address well the question why the simple MEGA-D variant outperforms prior state-of-the-art besides promising to publish code. The new ablation study is especially difficult to understand, since the first set of results are MEGA without using both memory and new data, but it seems to me that MEGA *needs* the memory data to work, so I have no idea what is being ablated here, and I also do not understand how this answers the question why MEGA-D performs so well. I will not be changing my rating.\n\nOther issues:\n(1) Citing A-GEM: please update the bib file so it shows the ICLR version instead of arXiv.\n(2) Solving eq. 9: Here you go: https://imgur.com/a/b4QCVlv\nI literally had this problem on one of my high school exams. Feel free to use this in any future version.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper318/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper318/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"abstract": "Current deep neural networks can achieve remarkable performance on a single task. However, when the deep neural network is continually trained on a sequence of tasks, it seems to gradually forget the previous learned knowledge.  This phenomenon is referred to as catastrophic forgetting and motivates the field called lifelong learning.  The central question in lifelong learning is how to enable deep neural networks to maintain performance on old tasks while learning a new task. In this paper, we introduce a novel and effective lifelong learning algorithm, called MixEd stochastic GrAdient (MEGA), which allows deep neural networks to acquire the ability of retaining performance on old tasks while learning new tasks. MEGA modulates the balance between old tasks and the new task by integrating the current gradient with the gradient computed on a small reference episodic memory.  Extensive  experimental  results  show  that  the  proposed  MEGA  algorithm significantly advances the state-of-the-art on all four commonly used life-long learning benchmarks, reducing the error by up to 18%.", "title": "Learning with Long-term Remembering: Following the Lead of Mixed Stochastic Gradient", "code": "https://drive.google.com/file/d/1ZKrIpkf1VoZrotMZ9MyALKIbbB_qEvg5/view?usp=sharing", "keywords": ["lifelong learning", "continual learning"], "pdf": "/pdf/88e73de8e94a7b9e4c474cb98a3b01488736fbd1.pdf", "authors": ["Yunhui Guo", "Mingrui Liu", "Tianbao Yang", "Tajana Rosing"], "TL;DR": "A novel and effective lifelong learning algorithm which achieves the state-of-the-art results on several benchmarks.", "authorids": ["yug185@eng.ucsd.edu", "mingrui-liu@uiowa.edu", "tianbao-yang@uiowa.edu", "tajana@ucsd.edu"], "paperhash": "guo|learning_with_longterm_remembering_following_the_lead_of_mixed_stochastic_gradient", "original_pdf": "/attachment/8d21e424cf1fdbef12668ecb67a1c256ce6126c3.pdf", "_bibtex": "@misc{\nguo2020learning,\ntitle={Learning with Long-term Remembering: Following the Lead of Mixed Stochastic Gradient},\nauthor={Yunhui Guo and Mingrui Liu and Tianbao Yang and Tajana Rosing},\nyear={2020},\nurl={https://openreview.net/forum?id=H1g6kaVKvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "H1g6kaVKvH", "replyto": "H1g6kaVKvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper318/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper318/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575449446388, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper318/Reviewers"], "noninvitees": [], "tcdate": 1570237753868, "tmdate": 1575449446399, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper318/-/Official_Review"}}}, {"id": "HJgMB6EjiH", "original": null, "number": 10, "cdate": 1573764410482, "ddate": null, "tcdate": 1573764410482, "tmdate": 1573764410482, "tddate": null, "forum": "H1g6kaVKvH", "replyto": "HJlUkYQsoS", "invitation": "ICLR.cc/2020/Conference/Paper318/-/Official_Comment", "content": {"title": "Thanks for your responses! We provide more clarifications below.", "comment": "- With the direct approach, it seems odd to specify a loss threshold of zero to determine that the current task performance is high. What loss is being used?\n\nA: We use the cross-entropy loss calculated on the batch of the current task. Since the training is online, the current batch can be viewed as a random sampled batch from the distribution of the current task [1], the loss on the current batch  can be seen as the generalization error of the current model. So a loss of zero can serve as a good indicator that the current task performance is high.  \n\n- A: It is certainly true that we can add fewer units for new tasks due to forward transfer. The statement is based on the idea of original work on progressive networks [3] which initialize a new network each time for each new task.\"\n\nYeas, if the network is of equal size for each task, then it's linear. However, in progressive networks, there are additional adapters or lateral connections as called in the paper to connect the new network with old ones. So the growth is much more than linear. The authors of Progressive Neural Network also pointed out this in Appendix B of the paper [2].\n\n[1] Nicolo Cesa-Bianchi, Alex Conconi, and Claudio Gentile. On the generalization ability of on-line\nlearning algorithms. IEEE Transactions on Information Theory, 50(9):2050\u20132057, 2004.\n\n[2] Rusu, Andrei A., et al. \"Progressive neural networks.\" arXiv preprint arXiv:1606.04671 (2016)."}, "signatures": ["ICLR.cc/2020/Conference/Paper318/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper318/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"abstract": "Current deep neural networks can achieve remarkable performance on a single task. However, when the deep neural network is continually trained on a sequence of tasks, it seems to gradually forget the previous learned knowledge.  This phenomenon is referred to as catastrophic forgetting and motivates the field called lifelong learning.  The central question in lifelong learning is how to enable deep neural networks to maintain performance on old tasks while learning a new task. In this paper, we introduce a novel and effective lifelong learning algorithm, called MixEd stochastic GrAdient (MEGA), which allows deep neural networks to acquire the ability of retaining performance on old tasks while learning new tasks. MEGA modulates the balance between old tasks and the new task by integrating the current gradient with the gradient computed on a small reference episodic memory.  Extensive  experimental  results  show  that  the  proposed  MEGA  algorithm significantly advances the state-of-the-art on all four commonly used life-long learning benchmarks, reducing the error by up to 18%.", "title": "Learning with Long-term Remembering: Following the Lead of Mixed Stochastic Gradient", "code": "https://drive.google.com/file/d/1ZKrIpkf1VoZrotMZ9MyALKIbbB_qEvg5/view?usp=sharing", "keywords": ["lifelong learning", "continual learning"], "pdf": "/pdf/88e73de8e94a7b9e4c474cb98a3b01488736fbd1.pdf", "authors": ["Yunhui Guo", "Mingrui Liu", "Tianbao Yang", "Tajana Rosing"], "TL;DR": "A novel and effective lifelong learning algorithm which achieves the state-of-the-art results on several benchmarks.", "authorids": ["yug185@eng.ucsd.edu", "mingrui-liu@uiowa.edu", "tianbao-yang@uiowa.edu", "tajana@ucsd.edu"], "paperhash": "guo|learning_with_longterm_remembering_following_the_lead_of_mixed_stochastic_gradient", "original_pdf": "/attachment/8d21e424cf1fdbef12668ecb67a1c256ce6126c3.pdf", "_bibtex": "@misc{\nguo2020learning,\ntitle={Learning with Long-term Remembering: Following the Lead of Mixed Stochastic Gradient},\nauthor={Yunhui Guo and Mingrui Liu and Tianbao Yang and Tajana Rosing},\nyear={2020},\nurl={https://openreview.net/forum?id=H1g6kaVKvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1g6kaVKvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper318/Authors", "ICLR.cc/2020/Conference/Paper318/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper318/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper318/Reviewers", "ICLR.cc/2020/Conference/Paper318/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper318/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper318/Authors|ICLR.cc/2020/Conference/Paper318/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504173198, "tmdate": 1576860554964, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper318/Authors", "ICLR.cc/2020/Conference/Paper318/Reviewers", "ICLR.cc/2020/Conference/Paper318/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper318/-/Official_Comment"}}}, {"id": "HJlUkYQsoS", "original": null, "number": 9, "cdate": 1573759198509, "ddate": null, "tcdate": 1573759198509, "tmdate": 1573759198509, "tddate": null, "forum": "H1g6kaVKvH", "replyto": "SJgNXibrjr", "invitation": "ICLR.cc/2020/Conference/Paper318/-/Official_Comment", "content": {"title": "Could you provide some further clarifications?", "comment": "I thank the authors for their response; there are still a few questions remaining.\n\n- With the direct approach, it seems odd to specify a loss threshold of zero to determine that the current task performance is high. What loss is being used?\n\nTo clarify this question; what specific form does the loss take, such that a threshold of zero makes sense to gauge high performance? Eg. a mean-squared error loss would be strictly non-negative.\n\n\n\"A: It is certainly true that we can add fewer units for new tasks due to forward transfer. The statement is based on the idea of original work on progressive networks [3] which initialize a new network each time for each new task.\"\n\nEven in the case of a new network of equal size for each task, this would be linear; so I'm still not sure how the super-linearity claim can be justified."}, "signatures": ["ICLR.cc/2020/Conference/Paper318/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper318/AnonReviewer3", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"abstract": "Current deep neural networks can achieve remarkable performance on a single task. However, when the deep neural network is continually trained on a sequence of tasks, it seems to gradually forget the previous learned knowledge.  This phenomenon is referred to as catastrophic forgetting and motivates the field called lifelong learning.  The central question in lifelong learning is how to enable deep neural networks to maintain performance on old tasks while learning a new task. In this paper, we introduce a novel and effective lifelong learning algorithm, called MixEd stochastic GrAdient (MEGA), which allows deep neural networks to acquire the ability of retaining performance on old tasks while learning new tasks. MEGA modulates the balance between old tasks and the new task by integrating the current gradient with the gradient computed on a small reference episodic memory.  Extensive  experimental  results  show  that  the  proposed  MEGA  algorithm significantly advances the state-of-the-art on all four commonly used life-long learning benchmarks, reducing the error by up to 18%.", "title": "Learning with Long-term Remembering: Following the Lead of Mixed Stochastic Gradient", "code": "https://drive.google.com/file/d/1ZKrIpkf1VoZrotMZ9MyALKIbbB_qEvg5/view?usp=sharing", "keywords": ["lifelong learning", "continual learning"], "pdf": "/pdf/88e73de8e94a7b9e4c474cb98a3b01488736fbd1.pdf", "authors": ["Yunhui Guo", "Mingrui Liu", "Tianbao Yang", "Tajana Rosing"], "TL;DR": "A novel and effective lifelong learning algorithm which achieves the state-of-the-art results on several benchmarks.", "authorids": ["yug185@eng.ucsd.edu", "mingrui-liu@uiowa.edu", "tianbao-yang@uiowa.edu", "tajana@ucsd.edu"], "paperhash": "guo|learning_with_longterm_remembering_following_the_lead_of_mixed_stochastic_gradient", "original_pdf": "/attachment/8d21e424cf1fdbef12668ecb67a1c256ce6126c3.pdf", "_bibtex": "@misc{\nguo2020learning,\ntitle={Learning with Long-term Remembering: Following the Lead of Mixed Stochastic Gradient},\nauthor={Yunhui Guo and Mingrui Liu and Tianbao Yang and Tajana Rosing},\nyear={2020},\nurl={https://openreview.net/forum?id=H1g6kaVKvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1g6kaVKvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper318/Authors", "ICLR.cc/2020/Conference/Paper318/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper318/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper318/Reviewers", "ICLR.cc/2020/Conference/Paper318/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper318/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper318/Authors|ICLR.cc/2020/Conference/Paper318/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504173198, "tmdate": 1576860554964, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper318/Authors", "ICLR.cc/2020/Conference/Paper318/Reviewers", "ICLR.cc/2020/Conference/Paper318/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper318/-/Official_Comment"}}}, {"id": "SygxF2XrsS", "original": null, "number": 7, "cdate": 1573366903599, "ddate": null, "tcdate": 1573366903599, "tmdate": 1573446658150, "tddate": null, "forum": "H1g6kaVKvH", "replyto": "H1g6kaVKvH", "invitation": "ICLR.cc/2020/Conference/Paper318/-/Official_Comment", "content": {"title": "Updated version of the paper is uploaded", "comment": "Dear Reviewers,\n\nWe have incorporated all the suggestions and comments into the updated draft.  We add more related works and additional experiments. Due to the space limitation, we add the ablation studies in the Appendix. Please let us know if you have more suggestions or comments. We are happy to include them to improve the quality and readability of the paper.\n\nBest,\n\nPaper318 Authors"}, "signatures": ["ICLR.cc/2020/Conference/Paper318/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper318/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"abstract": "Current deep neural networks can achieve remarkable performance on a single task. However, when the deep neural network is continually trained on a sequence of tasks, it seems to gradually forget the previous learned knowledge.  This phenomenon is referred to as catastrophic forgetting and motivates the field called lifelong learning.  The central question in lifelong learning is how to enable deep neural networks to maintain performance on old tasks while learning a new task. In this paper, we introduce a novel and effective lifelong learning algorithm, called MixEd stochastic GrAdient (MEGA), which allows deep neural networks to acquire the ability of retaining performance on old tasks while learning new tasks. MEGA modulates the balance between old tasks and the new task by integrating the current gradient with the gradient computed on a small reference episodic memory.  Extensive  experimental  results  show  that  the  proposed  MEGA  algorithm significantly advances the state-of-the-art on all four commonly used life-long learning benchmarks, reducing the error by up to 18%.", "title": "Learning with Long-term Remembering: Following the Lead of Mixed Stochastic Gradient", "code": "https://drive.google.com/file/d/1ZKrIpkf1VoZrotMZ9MyALKIbbB_qEvg5/view?usp=sharing", "keywords": ["lifelong learning", "continual learning"], "pdf": "/pdf/88e73de8e94a7b9e4c474cb98a3b01488736fbd1.pdf", "authors": ["Yunhui Guo", "Mingrui Liu", "Tianbao Yang", "Tajana Rosing"], "TL;DR": "A novel and effective lifelong learning algorithm which achieves the state-of-the-art results on several benchmarks.", "authorids": ["yug185@eng.ucsd.edu", "mingrui-liu@uiowa.edu", "tianbao-yang@uiowa.edu", "tajana@ucsd.edu"], "paperhash": "guo|learning_with_longterm_remembering_following_the_lead_of_mixed_stochastic_gradient", "original_pdf": "/attachment/8d21e424cf1fdbef12668ecb67a1c256ce6126c3.pdf", "_bibtex": "@misc{\nguo2020learning,\ntitle={Learning with Long-term Remembering: Following the Lead of Mixed Stochastic Gradient},\nauthor={Yunhui Guo and Mingrui Liu and Tianbao Yang and Tajana Rosing},\nyear={2020},\nurl={https://openreview.net/forum?id=H1g6kaVKvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1g6kaVKvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper318/Authors", "ICLR.cc/2020/Conference/Paper318/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper318/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper318/Reviewers", "ICLR.cc/2020/Conference/Paper318/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper318/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper318/Authors|ICLR.cc/2020/Conference/Paper318/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504173198, "tmdate": 1576860554964, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper318/Authors", "ICLR.cc/2020/Conference/Paper318/Reviewers", "ICLR.cc/2020/Conference/Paper318/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper318/-/Official_Comment"}}}, {"id": "H1x5fqWSsB", "original": null, "number": 3, "cdate": 1573358097952, "ddate": null, "tcdate": 1573358097952, "tmdate": 1573371042955, "tddate": null, "forum": "H1g6kaVKvH", "replyto": "rJx7qtA4jS", "invitation": "ICLR.cc/2020/Conference/Paper318/-/Official_Comment", "content": {"title": "Reply to Review 1 (Part 2) ", "comment": "Hope the illustration below can better explain the relation between GEM/A-GEM/MER, MEGA and MEGA-D\n\n\nGEM/A-GEM/MER $\\rightarrow^1$ MEGA $\\rightarrow^2$ MEGA-D\n\n1. Better modification of the current gradient direction by maximizing correlation as in Eq 7 and better balancing with losses on current task and old tasks.\n\n2. Simple variant of MEGA which balances current task and old tasks by only leveraging loss information.\n\n\nMore clarifications of MEGA-D and A-GEM/GEM:\n\nwe want to further point out that it is not clear how to directly apply MEGA-D to A-GEM/GEM, i.e., introduce the loss information to A-GEM/GEM.  Since the gradient direction modification rule of A-GEM/GEM in some sense is \"hard-coded\". \n\nIn A-GEM/GEM, if the angle between the current gradient and the reference gradient is acute, then there is no change to the current gradient. On the other hand, if the angle between the current gradient and the reference gradient is obtuse, then project the current gradient to be perpendicular to the reference gradient to the obtain the projected gradient.  In both cases,  we cannot directly multiply the loss (or the ratio of the current loss and reference loss as in the direct approach)  to the current gradient or the projected gradient, since the loss (or the ratio of the current loss and reference loss as in the direct approach)  can only change the magnitude of the current gradient or the projected gradient but not the direction.  In MEGA-D, the final gradient is a linear combination of the current gradient and the reference gradient, so it is very natural to use loss to balance the two gradients. The direction of the final gradient obtained by MEGA-D can vary based on the ratio of the current loss and reference loss.\n\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper318/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper318/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"abstract": "Current deep neural networks can achieve remarkable performance on a single task. However, when the deep neural network is continually trained on a sequence of tasks, it seems to gradually forget the previous learned knowledge.  This phenomenon is referred to as catastrophic forgetting and motivates the field called lifelong learning.  The central question in lifelong learning is how to enable deep neural networks to maintain performance on old tasks while learning a new task. In this paper, we introduce a novel and effective lifelong learning algorithm, called MixEd stochastic GrAdient (MEGA), which allows deep neural networks to acquire the ability of retaining performance on old tasks while learning new tasks. MEGA modulates the balance between old tasks and the new task by integrating the current gradient with the gradient computed on a small reference episodic memory.  Extensive  experimental  results  show  that  the  proposed  MEGA  algorithm significantly advances the state-of-the-art on all four commonly used life-long learning benchmarks, reducing the error by up to 18%.", "title": "Learning with Long-term Remembering: Following the Lead of Mixed Stochastic Gradient", "code": "https://drive.google.com/file/d/1ZKrIpkf1VoZrotMZ9MyALKIbbB_qEvg5/view?usp=sharing", "keywords": ["lifelong learning", "continual learning"], "pdf": "/pdf/88e73de8e94a7b9e4c474cb98a3b01488736fbd1.pdf", "authors": ["Yunhui Guo", "Mingrui Liu", "Tianbao Yang", "Tajana Rosing"], "TL;DR": "A novel and effective lifelong learning algorithm which achieves the state-of-the-art results on several benchmarks.", "authorids": ["yug185@eng.ucsd.edu", "mingrui-liu@uiowa.edu", "tianbao-yang@uiowa.edu", "tajana@ucsd.edu"], "paperhash": "guo|learning_with_longterm_remembering_following_the_lead_of_mixed_stochastic_gradient", "original_pdf": "/attachment/8d21e424cf1fdbef12668ecb67a1c256ce6126c3.pdf", "_bibtex": "@misc{\nguo2020learning,\ntitle={Learning with Long-term Remembering: Following the Lead of Mixed Stochastic Gradient},\nauthor={Yunhui Guo and Mingrui Liu and Tianbao Yang and Tajana Rosing},\nyear={2020},\nurl={https://openreview.net/forum?id=H1g6kaVKvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1g6kaVKvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper318/Authors", "ICLR.cc/2020/Conference/Paper318/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper318/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper318/Reviewers", "ICLR.cc/2020/Conference/Paper318/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper318/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper318/Authors|ICLR.cc/2020/Conference/Paper318/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504173198, "tmdate": 1576860554964, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper318/Authors", "ICLR.cc/2020/Conference/Paper318/Reviewers", "ICLR.cc/2020/Conference/Paper318/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper318/-/Official_Comment"}}}, {"id": "rJefEaWBor", "original": null, "number": 6, "cdate": 1573358889587, "ddate": null, "tcdate": 1573358889587, "tmdate": 1573359279794, "tddate": null, "forum": "H1g6kaVKvH", "replyto": "BJgJKvDaYB", "invitation": "ICLR.cc/2020/Conference/Paper318/-/Official_Comment", "content": {"title": "Reply to Reviewer 2 (Part 2)", "comment": "Q4: Relationship with existing multitask learning weight balancing methods should also be discussed.\n\nA: Thanks for pointing out this. We will include the references you mentioned and discuss them in the revised version. We also want to highlight the difference between our approach and the approaches in multitask learning. Lifelong learning is an online learning paradigm while multi-task learning is offline. Multi-task learning aims at optimizing the network to produce multiple predictive outputs for different tasks on the same dataset. However in lifelong learning, each task is a dataset. The most closely related work in the mentioned reference is GradNorm https://arxiv.org/abs/1711.02257, the main idea of GradNorm is to balance tasks such as depth estimation and segmentation based on the magnitude of the gradients, however in the proposed MEGA and also in GEM [1], we focus on modifying the gradient direction to overcome catastrophic forgetting in lifelong learning. \n\n\nQ5: Solving for beta using optimization (eq. 10), while (eq. 9) should have an explicit, close-form solution.\n\nA: The objective function of (eq. 9) is a possibly non-convex function in terms of $\\beta$. It is impossible to get a general closed-form solution to the best of our knowledge. The best we can do is to derive closed-form solution if $\\ell_t=\\ell_{\\text{ref}}$ since in this case the function is concave. For example, the objective of (8) is concave and the optimal solution of $\\beta$ is $\\tilde{\\theta}/2$.\n\nQ6: Writing sometimes is confusing.\n\n(1) This sentence is only applied to the optimization problem formulated in Eq 4. And we did not intend that it can be generalized to other type of lifelong learning methods. \n\n(2) \u201cbottom of page 3 \"alpha1 and alpha2 should be adjusted adaptively\" does not have experiment results supporting it (especially considering GEM is also adaptive)\u201d\n\nA: Please refer to the answer of Q3 in Part 1.\n\n(3) \u201c \"l_ref(w; \u03b6) = 0 implies that there is almost no catastrophic forgetting\" claim is problematic. Overfitting to the episodic memory is a common problem \u201d\n\nA: We use l_ref(w; \u03b6) = 0 as a proxy to assess the performance of the current model on old tasks. We agree that there can be overfitting when the memory size is small. Since we only compute the loss on a sampled batch from the episodic memory, this can alleviate the overfitting issue to some extent. Empirically we found the loss on the episodic memory can still serve as a good indicator to balance old tasks and the current task as shown in the ablation studies in the answer to Q3 in Part 1.\n\n(4) \u201cpage 3 xi and zeta not clearly defined.\u201d \n\nA:  xi and zeta are random variables which characterize the random samples. This is a commonly usage in stochastic optimization [3].\n\n(5) \u201cbrings up NP-hardness while it is seldom of interest in this field.\u201d\n\nA: We use NP-hard to motivate the necessity to use stochastic gradient descent to solve the problem. We will remove this in the updated draft. \n\n(6) \u201cmissing \"\u2207\" on the denominator in (eq. 7).\u201d\n\nA: Thanks for pointing out this. We will make a correction in the updated draft. \n\n\nQ7: Experiments\n\n(1) The only ablation (the direct approach) is statistically indistinguishable from the proposed method. This also outperforms state-of-the-art, while it should not be. One can only assume that the experiment is problematic.\n\nA: The direct approach is a variant of the MEGA (can also be regarded as an ablation as the reviewer pointed out) which can be easily implemented. Our experiments follow the exact same setting as the previous work [2] for a fair comparison. We open source our code and the results can be easily replicated.  If there is any incorrectness in the experiment, we welcome the reviewer to point out. \n\n(2) Completely uses hyperparameter from an unpublished paper.\n\nA: We reuse the hyperparameters from A-GEM [2] which is published in ICLR 2019.\n\n(3) Claims state-of-the-art, yet omits a state-of-the-art variant in cited A-GEM paper (with joint-embedding).\n\nA: The joint-embedding utilizes attribute information.  For the state-of-the-art comparison, we only consider the label of the examples without any other information. This is applied to all the methods considered in the paper to allow a fair comparison. \n\n[1] Lopez-Paz, David, and Marc'Aurelio Ranzato. \"Gradient episodic memory for continual learning.\" NeurIPS 2017.\n[2] Chaudhry, Arslan, et al. \"Efficient lifelong learning with a-gem.\" ICLR 2019.\n[3] Johnson, Rie, and Tong Zhang. \"Accelerating stochastic gradient descent using predictive variance reduction.\" Advances in neural information processing systems. 2013.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper318/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper318/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"abstract": "Current deep neural networks can achieve remarkable performance on a single task. However, when the deep neural network is continually trained on a sequence of tasks, it seems to gradually forget the previous learned knowledge.  This phenomenon is referred to as catastrophic forgetting and motivates the field called lifelong learning.  The central question in lifelong learning is how to enable deep neural networks to maintain performance on old tasks while learning a new task. In this paper, we introduce a novel and effective lifelong learning algorithm, called MixEd stochastic GrAdient (MEGA), which allows deep neural networks to acquire the ability of retaining performance on old tasks while learning new tasks. MEGA modulates the balance between old tasks and the new task by integrating the current gradient with the gradient computed on a small reference episodic memory.  Extensive  experimental  results  show  that  the  proposed  MEGA  algorithm significantly advances the state-of-the-art on all four commonly used life-long learning benchmarks, reducing the error by up to 18%.", "title": "Learning with Long-term Remembering: Following the Lead of Mixed Stochastic Gradient", "code": "https://drive.google.com/file/d/1ZKrIpkf1VoZrotMZ9MyALKIbbB_qEvg5/view?usp=sharing", "keywords": ["lifelong learning", "continual learning"], "pdf": "/pdf/88e73de8e94a7b9e4c474cb98a3b01488736fbd1.pdf", "authors": ["Yunhui Guo", "Mingrui Liu", "Tianbao Yang", "Tajana Rosing"], "TL;DR": "A novel and effective lifelong learning algorithm which achieves the state-of-the-art results on several benchmarks.", "authorids": ["yug185@eng.ucsd.edu", "mingrui-liu@uiowa.edu", "tianbao-yang@uiowa.edu", "tajana@ucsd.edu"], "paperhash": "guo|learning_with_longterm_remembering_following_the_lead_of_mixed_stochastic_gradient", "original_pdf": "/attachment/8d21e424cf1fdbef12668ecb67a1c256ce6126c3.pdf", "_bibtex": "@misc{\nguo2020learning,\ntitle={Learning with Long-term Remembering: Following the Lead of Mixed Stochastic Gradient},\nauthor={Yunhui Guo and Mingrui Liu and Tianbao Yang and Tajana Rosing},\nyear={2020},\nurl={https://openreview.net/forum?id=H1g6kaVKvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1g6kaVKvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper318/Authors", "ICLR.cc/2020/Conference/Paper318/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper318/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper318/Reviewers", "ICLR.cc/2020/Conference/Paper318/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper318/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper318/Authors|ICLR.cc/2020/Conference/Paper318/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504173198, "tmdate": 1576860554964, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper318/Authors", "ICLR.cc/2020/Conference/Paper318/Reviewers", "ICLR.cc/2020/Conference/Paper318/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper318/-/Official_Comment"}}}, {"id": "Syx8F3WSoB", "original": null, "number": 5, "cdate": 1573358718130, "ddate": null, "tcdate": 1573358718130, "tmdate": 1573358718130, "tddate": null, "forum": "H1g6kaVKvH", "replyto": "BJgJKvDaYB", "invitation": "ICLR.cc/2020/Conference/Paper318/-/Official_Comment", "content": {"title": "Reply to Reviewer 2 (Part 1)", "comment": "We greatly appreciate the time and effort of the reviewer to point out the issues of the current draft. We will incorporate the comments in the final manuscript to improve the quality and readability of the paper. \n\nQ1: The purpose the weighted variant of forgetting measure\n\nA:  The proposed LTR measures if the method can maintain its performance on the tasks trained initially. The purpose of the proposed measure is to access the accuracy drop on each task during the learning process. \n\nQ2: Not well-placed in the literature.\n\nA: Thanks for pointing out this. We will add more discussions on related works in the beginning of the draft to clearly show the differences between the proposed method and existing methods to improve the readability of the paper.\n\nQ3: Motivation to use the proposed loss balancing rather than that of very similar methods (e.g. GEM/A-GEM).\n\nA:  Thanks for the question. We agree that the sentence \u201cthese approaches cannot capture the dynamics\u201d is not very accurate, what we want to emphasize is that we explicitly consider the loss on the current task and old tasks to achieve a better balance.  Below we add discussions on the motivations of the proposed approach and explain why our method can achieve higher accuracy than previous approaches. \n\nFirst, we explicitly maximize the correlation between the mixed stochastic gradient g_mix and the current gradient g_cur (calculated on current data), and the correlation between the mixed stochastic gradient g_mix and the reference gradient g_ref (calculated on memory data) as in Eq (7). Intuitively, the direction of g_mix will not bias towards the current task or old tasks  (since it is easy to see that the angle between g_mix and g_cur, and the angle between g_mix and g_ref are both acute except the edge case where the angle between g_cur and g_ref is 180 degree). By following g_mix, each update tends to degrade the loss on both current task and old tasks.   While in previous works (GEM [1], A-GEM[2]), the corresponding g_mix is found by projecting the current gradient to be perpendicular to g_ref when angle between them is obtuse (in this case, g_mix $\\cdot$ g_ref = 0). Intuitively, both GEM and A-GEM put more emphasis on the current task (when g_mix is perpendicular to g_ref, the update following the direction of g_mix will not directly reduce the loss on old tasks [1]). Although GEM and A-GEM also adaptively modify the direction of the current gradient, our approach can achieve a better tradeoff between the performance on current task  and old tasks based on above reasoning. \n\n We did an ablation study on just adding this source, \n############################################################################\nThe results of only considering the first source (maximizing correlation):\nOn MNIST:    91.15\t\t\tOn CIFAR:   58.04\nOn CUB:       68.6\t\t\t        On AWA:      47.95\n\n############################################################################\nResults of A-GEM [2]:\n\nOn MNIST:    89.32\t\t\t\tOn CIFAR:   61.28\nOn CUB:        61.82\t\t\t\tOn AWA:      44.95\n############################################################################\n\nSecond, we introduce the loss on the current batch and on the sampled batch from episodic memory. This allows us to better balance the performance on the current task and old tasks. Since it is preferable to put more emphasis on the tasks (task) which achieve(s) low performance. \n############################################################################\nThe results of combining two sources:\nOn MNIST:  91.21  \t\t\tOn CIFAR: 66.12\nOn CUB:     80.58\t\t\t        On AWA:    54.28\n############################################################################\n\nIt can seen that both of the two sources contribute to the improvement.  We will add the ablations and discussions of each source in the final draft to clearly demonstrate why our approach is better than previous approaches.\n\nIn summary, we first propose a better approach to adaptively modify the direction of the gradient.  Second, we introduce the losses on current task and on the episodic memory to achieve a better balancing.  Hopefully the visualization below can better explain the relation between GEM/A-GEM/MER, MEGA and MEGA-D,\n\nGEM/A-GEM/MER $\\rightarrow^1$ MEGA $\\rightarrow^2$ MEGA-D\n\n1. Better modification of the current gradient direction by maximizing correlation as in Eq 7 and better balancing with losses on current task and old tasks.\n\n2. Simple variant of MEGA which balances current task and old tasks by only leveraging loss information.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper318/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper318/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"abstract": "Current deep neural networks can achieve remarkable performance on a single task. However, when the deep neural network is continually trained on a sequence of tasks, it seems to gradually forget the previous learned knowledge.  This phenomenon is referred to as catastrophic forgetting and motivates the field called lifelong learning.  The central question in lifelong learning is how to enable deep neural networks to maintain performance on old tasks while learning a new task. In this paper, we introduce a novel and effective lifelong learning algorithm, called MixEd stochastic GrAdient (MEGA), which allows deep neural networks to acquire the ability of retaining performance on old tasks while learning new tasks. MEGA modulates the balance between old tasks and the new task by integrating the current gradient with the gradient computed on a small reference episodic memory.  Extensive  experimental  results  show  that  the  proposed  MEGA  algorithm significantly advances the state-of-the-art on all four commonly used life-long learning benchmarks, reducing the error by up to 18%.", "title": "Learning with Long-term Remembering: Following the Lead of Mixed Stochastic Gradient", "code": "https://drive.google.com/file/d/1ZKrIpkf1VoZrotMZ9MyALKIbbB_qEvg5/view?usp=sharing", "keywords": ["lifelong learning", "continual learning"], "pdf": "/pdf/88e73de8e94a7b9e4c474cb98a3b01488736fbd1.pdf", "authors": ["Yunhui Guo", "Mingrui Liu", "Tianbao Yang", "Tajana Rosing"], "TL;DR": "A novel and effective lifelong learning algorithm which achieves the state-of-the-art results on several benchmarks.", "authorids": ["yug185@eng.ucsd.edu", "mingrui-liu@uiowa.edu", "tianbao-yang@uiowa.edu", "tajana@ucsd.edu"], "paperhash": "guo|learning_with_longterm_remembering_following_the_lead_of_mixed_stochastic_gradient", "original_pdf": "/attachment/8d21e424cf1fdbef12668ecb67a1c256ce6126c3.pdf", "_bibtex": "@misc{\nguo2020learning,\ntitle={Learning with Long-term Remembering: Following the Lead of Mixed Stochastic Gradient},\nauthor={Yunhui Guo and Mingrui Liu and Tianbao Yang and Tajana Rosing},\nyear={2020},\nurl={https://openreview.net/forum?id=H1g6kaVKvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1g6kaVKvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper318/Authors", "ICLR.cc/2020/Conference/Paper318/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper318/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper318/Reviewers", "ICLR.cc/2020/Conference/Paper318/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper318/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper318/Authors|ICLR.cc/2020/Conference/Paper318/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504173198, "tmdate": 1576860554964, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper318/Authors", "ICLR.cc/2020/Conference/Paper318/Reviewers", "ICLR.cc/2020/Conference/Paper318/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper318/-/Official_Comment"}}}, {"id": "SJgNXibrjr", "original": null, "number": 4, "cdate": 1573358363576, "ddate": null, "tcdate": 1573358363576, "tmdate": 1573358363576, "tddate": null, "forum": "H1g6kaVKvH", "replyto": "Skl8qrtstB", "invitation": "ICLR.cc/2020/Conference/Paper318/-/Official_Comment", "content": {"title": "Reply to reviewer 3", "comment": "We thank the reviewer for the constructive comments.\n\nQ1: How is the memory buffer updated\uff1f\nA: For a fair comparison with GEM [1] and AGEM [2], we update the memory buffer in the same way, i.e., the examples are chosen uniformly at random for each task as in [2]. The buffer can be regarded as a queue, when the buffer is full, old samples are discarded. The current notation is a little misleading, and we will correct it in the updated draft. \n\n\nQ2: Not clear how much memory is required for this approach and whether it is consistent with previous approaches.\nA: Thanks for the suggestions. In the current work, for all the methods we use the same memory size for a fair comparison which is also the setting used in [2]. It is definitely interesting to vary the memory size to see the change of performance of different methods. Below we add results of changing memory size on Permuted MNIST (17 tasks and 60000 examples per task), and compare our approach (MEGA) to A-GEM [2]. We will update the results in the final draft and also conduct similar experiments on other datasets. \n\nMemory size:        850  \t1700\t\t2550\t \t4250\t\n\nMEGA:\t\t       86.13     \t88.56   \t        89.62 \t\t91.21 \n\nA-GEM \t               85.60\t88.19\t\t88.97\t\t89.32\n\n\nWe can see that the proposed MEGA consistently outperform A-GEM regardless of the memory size.\n\nQ3: What loss is used in the direct approach? How does the direct approach relate to eg. GEM/A-GEM/MER in terms of weighting between old and new samples?\nA:  We use the loss on the current batch and a sampled batch from the memory to achieve a better balancing than previous works. Intuitively, our approach put more emphasis on the tasks which achieve a lower performance. The direct approach (MEGA-D) directly leverages the losses to balance the current gradient and the gradient computed on the episodic memory. However, in GEM/A-GEM/MER, the modification of the direction of the current gradient only based on the angle between the gradient and the reference gradient which does not consider the loss information. The connection between GEM/A-GEM/MER, MEGA and MEGA-D can be clearly explained as below,\n\nGEM/A-GEM/MER $\\rightarrow^1$ MEGA $\\rightarrow^2$ MEGA-D\n\n1. Better modification of the current gradient direction by maximizing correlation as in Eq 7 and better balancing with losses on current task and old tasks.\n\n2. Simple variant of MEGA which balances current task and old tasks by only leveraging loss information.\n\n\nQ4: Related work.\nA: We will cite more recent works on lifelong learning in the related work section in the updated draft. \n\nQ5: Progressive networks increase in memory super-linearly?\nA: It is certainly true that we can add fewer units for new tasks due to forward transfer. The statement is based on the idea of original work on progressive networks [3] which initialize a new network each time for each new task. \n\n\n[1] Lopez-Paz, David, and Marc'Aurelio Ranzato. \"Gradient episodic memory for continual learning.\" NeurIPS 2017.\n[2] Chaudhry, Arslan, et al. \"Efficient lifelong learning with a-gem.\" ICLR 2019.\n[3] Rusu, Andrei A., et al. \"Progressive neural networks.\" arXiv preprint arXiv:1606.04671 (2016).\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper318/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper318/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"abstract": "Current deep neural networks can achieve remarkable performance on a single task. However, when the deep neural network is continually trained on a sequence of tasks, it seems to gradually forget the previous learned knowledge.  This phenomenon is referred to as catastrophic forgetting and motivates the field called lifelong learning.  The central question in lifelong learning is how to enable deep neural networks to maintain performance on old tasks while learning a new task. In this paper, we introduce a novel and effective lifelong learning algorithm, called MixEd stochastic GrAdient (MEGA), which allows deep neural networks to acquire the ability of retaining performance on old tasks while learning new tasks. MEGA modulates the balance between old tasks and the new task by integrating the current gradient with the gradient computed on a small reference episodic memory.  Extensive  experimental  results  show  that  the  proposed  MEGA  algorithm significantly advances the state-of-the-art on all four commonly used life-long learning benchmarks, reducing the error by up to 18%.", "title": "Learning with Long-term Remembering: Following the Lead of Mixed Stochastic Gradient", "code": "https://drive.google.com/file/d/1ZKrIpkf1VoZrotMZ9MyALKIbbB_qEvg5/view?usp=sharing", "keywords": ["lifelong learning", "continual learning"], "pdf": "/pdf/88e73de8e94a7b9e4c474cb98a3b01488736fbd1.pdf", "authors": ["Yunhui Guo", "Mingrui Liu", "Tianbao Yang", "Tajana Rosing"], "TL;DR": "A novel and effective lifelong learning algorithm which achieves the state-of-the-art results on several benchmarks.", "authorids": ["yug185@eng.ucsd.edu", "mingrui-liu@uiowa.edu", "tianbao-yang@uiowa.edu", "tajana@ucsd.edu"], "paperhash": "guo|learning_with_longterm_remembering_following_the_lead_of_mixed_stochastic_gradient", "original_pdf": "/attachment/8d21e424cf1fdbef12668ecb67a1c256ce6126c3.pdf", "_bibtex": "@misc{\nguo2020learning,\ntitle={Learning with Long-term Remembering: Following the Lead of Mixed Stochastic Gradient},\nauthor={Yunhui Guo and Mingrui Liu and Tianbao Yang and Tajana Rosing},\nyear={2020},\nurl={https://openreview.net/forum?id=H1g6kaVKvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1g6kaVKvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper318/Authors", "ICLR.cc/2020/Conference/Paper318/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper318/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper318/Reviewers", "ICLR.cc/2020/Conference/Paper318/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper318/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper318/Authors|ICLR.cc/2020/Conference/Paper318/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504173198, "tmdate": 1576860554964, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper318/Authors", "ICLR.cc/2020/Conference/Paper318/Reviewers", "ICLR.cc/2020/Conference/Paper318/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper318/-/Official_Comment"}}}, {"id": "rJx7qtA4jS", "original": null, "number": 1, "cdate": 1573345675238, "ddate": null, "tcdate": 1573345675238, "tmdate": 1573357986782, "tddate": null, "forum": "H1g6kaVKvH", "replyto": "rke-ypoCFS", "invitation": "ICLR.cc/2020/Conference/Paper318/-/Official_Comment", "content": {"title": "Reply to Review 1 (Part 1)", "comment": "We thank the reviewer for your valuable comments and insightful questions. Next, we answer  the questions in detail. \n\nQ1: Where are the gains are coming from?\n\nA:  Thanks for the question. We definitely agree that this is an important question which has not been addressed well in the current draft.  In summary, the benefits of our approach come from two sources.  \n\nFirst, we explicitly maximize the correlation between the mixed stochastic gradient g_mix and the current gradient g_cur (calculated on current data), and the correlation between the mixed stochastic gradient g_mix and the reference gradient g_ref (calculated on memory data) as in Eq (7). Intuitively, the direction of g_mix will not bias towards the current task or old tasks  (since it is easy to see that the angle between g_mix and g_cur, and the angle between g_mix and g_ref are both acute except the edge case where the angle between g_cur and g_ref is 180 degree). By following g_mix, each update tends to degrade the loss on both current task and old tasks.   While in previous works (GEM [1], A-GEM[2]), the corresponding g_mix is found by projecting the current gradient to be perpendicular to g_ref when angle between them is obtuse (in this case, g_mix $\\cdot$ g_ref = 0). Intuitively, both GEM and A-GEM put more emphasis on the current task (when g_mix is perpendicular to g_ref, the update following the direction of g_mix will not directly reduce the loss on old tasks [1]).  We did an ablation study on just adding this source,\n\n############################################################################\nThe results of only considering the first source (maximizing correlation):\nOn MNIST:    91.15\t\t\tOn CIFAR:   58.04\nOn CUB:       68.6\t\t\tOn AWA:      47.95\n\n############################################################################\nResults of A-GEM [2]:\n\nOn MNIST:    89.32\t\t\t\tOn CIFAR:   61.28\nOn CUB:       61.82\t\t\t\tOn AWA:      44.95\n############################################################################\n\nSecond, we introduce the loss on the current batch and on the sampled batch from episodic memory. This allows us to better balance the performance on the current task and old tasks. Since it is preferable to put more emphasis on the tasks (task) which achieve(s) lower performance. \n\n############################################################################\nThe results of combining two sources:\nOn MNIST:  91.21  \t\t\tOn CIFAR: 66.12\nOn CUB:     80.58\t\t\t        On AWA:    54.28\n############################################################################\n\nIt can seen that both of the two sources contribute to the improvement.  We will add the ablations and discussions of each source in the final draft to clearly demonstrate why our approach is better than previous approaches.\n\n\nQ2: The performance of the direct approach.\nA:  Thanks for the question. \n\nWe need to point out that both MEGA and MEGA-D are our contributions and previous works such as GEM [1] and A-GEM [2] did not propose similar approaches.  For MEGA-D, our intention is to provide a simple variant of MEGA which can be easily implemented and can serve as an off-the-shelf solution for lifelong learning tasks.  For more difficult lifelong learning tasks, we found MEGA still performs better than MEGA-D. We did an additional experiment on Many Permutations on MNIST (100 tasks and 200 examples per task) below,\n\nAverage accuracy over 5 five runs on Many Permutations with MNIST (100 tasks and 200 examples per task) :\n\nMEGA-D: 56.52 \u00b1 0.43\t\tMEGA: 62.48 \u00b1 0.51\n\nWe will include the updated results in the draft.\n\n\nQ3: Experiments.\nA: We use the public code released by [2] to produce all the results if the method is available in the code, and our results match the results reported in [2].  For MER, we also adopt the code published by the original author. Unfortunately, we cannot find the results on CIFAR in the original MER paper.  To obtain results of MER on CIFAR, we reuse the hyperparameters of A-GEM [2], the same hyperparameters are also applied to our methods.  For MNIST Permutations, MER adopt a different setting from [2] (MER) which only consider 1000 examples per task and their best result is 85.50. We tried to run MER on our setting where each task has 60000 examples, but unfortunately MER cannot scale to such sample size since the model is trained one example at a time based on the meta-learning procedure. The best we can do is to run MER on Many Permutations (100 tasks and 200 examples per task) for MNIST.\n\n[1] Lopez-Paz, David, and Marc'Aurelio Ranzato. \"Gradient episodic memory for continual learning.\" NeurIPS 2017.\n[2] Chaudhry, Arslan, et al. \"Efficient lifelong learning with a-gem.\" ICLR 2019.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper318/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper318/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"abstract": "Current deep neural networks can achieve remarkable performance on a single task. However, when the deep neural network is continually trained on a sequence of tasks, it seems to gradually forget the previous learned knowledge.  This phenomenon is referred to as catastrophic forgetting and motivates the field called lifelong learning.  The central question in lifelong learning is how to enable deep neural networks to maintain performance on old tasks while learning a new task. In this paper, we introduce a novel and effective lifelong learning algorithm, called MixEd stochastic GrAdient (MEGA), which allows deep neural networks to acquire the ability of retaining performance on old tasks while learning new tasks. MEGA modulates the balance between old tasks and the new task by integrating the current gradient with the gradient computed on a small reference episodic memory.  Extensive  experimental  results  show  that  the  proposed  MEGA  algorithm significantly advances the state-of-the-art on all four commonly used life-long learning benchmarks, reducing the error by up to 18%.", "title": "Learning with Long-term Remembering: Following the Lead of Mixed Stochastic Gradient", "code": "https://drive.google.com/file/d/1ZKrIpkf1VoZrotMZ9MyALKIbbB_qEvg5/view?usp=sharing", "keywords": ["lifelong learning", "continual learning"], "pdf": "/pdf/88e73de8e94a7b9e4c474cb98a3b01488736fbd1.pdf", "authors": ["Yunhui Guo", "Mingrui Liu", "Tianbao Yang", "Tajana Rosing"], "TL;DR": "A novel and effective lifelong learning algorithm which achieves the state-of-the-art results on several benchmarks.", "authorids": ["yug185@eng.ucsd.edu", "mingrui-liu@uiowa.edu", "tianbao-yang@uiowa.edu", "tajana@ucsd.edu"], "paperhash": "guo|learning_with_longterm_remembering_following_the_lead_of_mixed_stochastic_gradient", "original_pdf": "/attachment/8d21e424cf1fdbef12668ecb67a1c256ce6126c3.pdf", "_bibtex": "@misc{\nguo2020learning,\ntitle={Learning with Long-term Remembering: Following the Lead of Mixed Stochastic Gradient},\nauthor={Yunhui Guo and Mingrui Liu and Tianbao Yang and Tajana Rosing},\nyear={2020},\nurl={https://openreview.net/forum?id=H1g6kaVKvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1g6kaVKvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper318/Authors", "ICLR.cc/2020/Conference/Paper318/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper318/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper318/Reviewers", "ICLR.cc/2020/Conference/Paper318/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper318/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper318/Authors|ICLR.cc/2020/Conference/Paper318/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504173198, "tmdate": 1576860554964, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper318/Authors", "ICLR.cc/2020/Conference/Paper318/Reviewers", "ICLR.cc/2020/Conference/Paper318/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper318/-/Official_Comment"}}}, {"id": "rke-ypoCFS", "original": null, "number": 3, "cdate": 1571892440772, "ddate": null, "tcdate": 1571892440772, "tmdate": 1572972610409, "tddate": null, "forum": "H1g6kaVKvH", "replyto": "H1g6kaVKvH", "invitation": "ICLR.cc/2020/Conference/Paper318/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "I am very torn about this paper as the experiments are thorough and the results are quite significant. The results are on four popular benchmarks and they also test on Many Permutations, which is an important finding as well.  While the angle based approach presented here is interesting and intuitively appealing, I am not really sure whether it is a needed generalization beyond past approaches. I don't really have a strong intuition for where the gains are coming from. As such, I think the paper would be much stronger if it could intuitively or theoretically get at why it improves on past approaches. \n\nActually, one of my biggest concerns is how good the performance of the direct approach is. The direct approach seems like a straightforward extension of experience replay with a special learning rate for current examples vs. buffer examples that is adaptive to the relative loss.  While it is potentially interesting that this approach works so well, it is not positioned as the selling point of the paper as currently written. Additionally, this is kind of an orthogonal contribution to past papers leveraging experience replay, which could potentially be modified and improved based on this approach. I wonder if the authors tried modifying any of the baseline approaches in this way as well. \n\nA small note on the experiments: the authors closely follow settings from past work and at times it can be unclear which results were implemented in this paper and which in past papers. I don't think that you have addressed what hyperparameters you chose for MER. It also seems to perform worse than past reported results on CIFAR and is not reported despite the best past performance on MNIST Permutations. "}, "signatures": ["ICLR.cc/2020/Conference/Paper318/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper318/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"abstract": "Current deep neural networks can achieve remarkable performance on a single task. However, when the deep neural network is continually trained on a sequence of tasks, it seems to gradually forget the previous learned knowledge.  This phenomenon is referred to as catastrophic forgetting and motivates the field called lifelong learning.  The central question in lifelong learning is how to enable deep neural networks to maintain performance on old tasks while learning a new task. In this paper, we introduce a novel and effective lifelong learning algorithm, called MixEd stochastic GrAdient (MEGA), which allows deep neural networks to acquire the ability of retaining performance on old tasks while learning new tasks. MEGA modulates the balance between old tasks and the new task by integrating the current gradient with the gradient computed on a small reference episodic memory.  Extensive  experimental  results  show  that  the  proposed  MEGA  algorithm significantly advances the state-of-the-art on all four commonly used life-long learning benchmarks, reducing the error by up to 18%.", "title": "Learning with Long-term Remembering: Following the Lead of Mixed Stochastic Gradient", "code": "https://drive.google.com/file/d/1ZKrIpkf1VoZrotMZ9MyALKIbbB_qEvg5/view?usp=sharing", "keywords": ["lifelong learning", "continual learning"], "pdf": "/pdf/88e73de8e94a7b9e4c474cb98a3b01488736fbd1.pdf", "authors": ["Yunhui Guo", "Mingrui Liu", "Tianbao Yang", "Tajana Rosing"], "TL;DR": "A novel and effective lifelong learning algorithm which achieves the state-of-the-art results on several benchmarks.", "authorids": ["yug185@eng.ucsd.edu", "mingrui-liu@uiowa.edu", "tianbao-yang@uiowa.edu", "tajana@ucsd.edu"], "paperhash": "guo|learning_with_longterm_remembering_following_the_lead_of_mixed_stochastic_gradient", "original_pdf": "/attachment/8d21e424cf1fdbef12668ecb67a1c256ce6126c3.pdf", "_bibtex": "@misc{\nguo2020learning,\ntitle={Learning with Long-term Remembering: Following the Lead of Mixed Stochastic Gradient},\nauthor={Yunhui Guo and Mingrui Liu and Tianbao Yang and Tajana Rosing},\nyear={2020},\nurl={https://openreview.net/forum?id=H1g6kaVKvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "H1g6kaVKvH", "replyto": "H1g6kaVKvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper318/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper318/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575449446388, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper318/Reviewers"], "noninvitees": [], "tcdate": 1570237753868, "tmdate": 1575449446399, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper318/-/Official_Review"}}}], "count": 13}