{"notes": [{"id": "S1gmvyHFDS", "original": "SkeXksT_wB", "number": 1760, "cdate": 1569439578889, "ddate": null, "tcdate": 1569439578889, "tmdate": 1577168236665, "tddate": null, "forum": "S1gmvyHFDS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["j.hayes@cs.ucl.ac.uk", "dvij@google.com", "yutianc@google.com", "sedielem@google.com", "pushmeet@google.com", "ncasagrande@google.com"], "title": "Provenance detection through learning transformation-resilient watermarking", "authors": ["Jamie Hayes", "Krishnamurthy Dvijotham", "Yutian Chen", "Sander Dieleman", "Pushmeet Kohli", "Norman Casagrande"], "pdf": "/pdf/8da5d919cb4e9e00a81f1bd4b375742021d6c27c.pdf", "TL;DR": "Develop a method to detect the provenance of signals that have undergone adversarial transformations.", "abstract": "Advancements in deep generative models have made it possible to synthesize images, videos and audio signals that are hard to distinguish from natural signals, creating opportunities for potential abuse of these capabilities. This motivates the problem of tracking the provenance of signals, i.e., being able to determine the original source of a signal. Watermarking the signal at the time of signal creation is a potential solution, but current techniques are brittle and watermark detection mechanisms can easily be bypassed by doing some post-processing (cropping images, shifting pitch in the audio etc.). In this paper, we introduce ReSWAT (Resilient Signal Watermarking via Adversarial Training), a framework for learning transformation-resilient watermark detectors that are able to detect a watermark even after a signal has been through several post-processing transformations. Our detection method can be applied to domains with continuous data representations such as images, videos or sound signals. Experiments on watermarking image and audio signals show that our method can reliably detect the provenance of a synthetic signal, even if the signal has been through several post-processing transformations, and improve upon related work in this setting. Furthermore, we show that for specific kinds of transformations (perturbations bounded in the $\\ell_2$ norm), we can even get formal guarantees on the ability of our model to detect the watermark.  We provide qualitative examples of watermarked image and audio samples in the anonymous code submission link.", "code": "https://drive.google.com/open?id=1c-qqHfTr3uMQSIuTR_Z8qZv0uTVrkH5m", "keywords": ["watermarking", "provenance detection"], "paperhash": "hayes|provenance_detection_through_learning_transformationresilient_watermarking", "original_pdf": "/attachment/4561c44462fc2c64c8b5a2240cf28a7a19628b41.pdf", "_bibtex": "@misc{\nhayes2020provenance,\ntitle={Provenance detection through learning transformation-resilient watermarking},\nauthor={Jamie Hayes and Krishnamurthy Dvijotham and Yutian Chen and Sander Dieleman and Pushmeet Kohli and Norman Casagrande},\nyear={2020},\nurl={https://openreview.net/forum?id=S1gmvyHFDS}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "KKPWxNnVBQ", "original": null, "number": 1, "cdate": 1576798731816, "ddate": null, "tcdate": 1576798731816, "tmdate": 1576800904626, "tddate": null, "forum": "S1gmvyHFDS", "replyto": "S1gmvyHFDS", "invitation": "ICLR.cc/2020/Conference/Paper1760/-/Decision", "content": {"decision": "Reject", "comment": "This paper offers an interesting and potentially useful approach to robust watermarking.  The reviewers are divided on the significance of the method.  The most senior and experienced reviewer was the most negative.  On balance, my assessment of this paper is borderline; given the number of more highly ranked papers in my pile, that means I have to assign \"reject\".", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["j.hayes@cs.ucl.ac.uk", "dvij@google.com", "yutianc@google.com", "sedielem@google.com", "pushmeet@google.com", "ncasagrande@google.com"], "title": "Provenance detection through learning transformation-resilient watermarking", "authors": ["Jamie Hayes", "Krishnamurthy Dvijotham", "Yutian Chen", "Sander Dieleman", "Pushmeet Kohli", "Norman Casagrande"], "pdf": "/pdf/8da5d919cb4e9e00a81f1bd4b375742021d6c27c.pdf", "TL;DR": "Develop a method to detect the provenance of signals that have undergone adversarial transformations.", "abstract": "Advancements in deep generative models have made it possible to synthesize images, videos and audio signals that are hard to distinguish from natural signals, creating opportunities for potential abuse of these capabilities. This motivates the problem of tracking the provenance of signals, i.e., being able to determine the original source of a signal. Watermarking the signal at the time of signal creation is a potential solution, but current techniques are brittle and watermark detection mechanisms can easily be bypassed by doing some post-processing (cropping images, shifting pitch in the audio etc.). In this paper, we introduce ReSWAT (Resilient Signal Watermarking via Adversarial Training), a framework for learning transformation-resilient watermark detectors that are able to detect a watermark even after a signal has been through several post-processing transformations. Our detection method can be applied to domains with continuous data representations such as images, videos or sound signals. Experiments on watermarking image and audio signals show that our method can reliably detect the provenance of a synthetic signal, even if the signal has been through several post-processing transformations, and improve upon related work in this setting. Furthermore, we show that for specific kinds of transformations (perturbations bounded in the $\\ell_2$ norm), we can even get formal guarantees on the ability of our model to detect the watermark.  We provide qualitative examples of watermarked image and audio samples in the anonymous code submission link.", "code": "https://drive.google.com/open?id=1c-qqHfTr3uMQSIuTR_Z8qZv0uTVrkH5m", "keywords": ["watermarking", "provenance detection"], "paperhash": "hayes|provenance_detection_through_learning_transformationresilient_watermarking", "original_pdf": "/attachment/4561c44462fc2c64c8b5a2240cf28a7a19628b41.pdf", "_bibtex": "@misc{\nhayes2020provenance,\ntitle={Provenance detection through learning transformation-resilient watermarking},\nauthor={Jamie Hayes and Krishnamurthy Dvijotham and Yutian Chen and Sander Dieleman and Pushmeet Kohli and Norman Casagrande},\nyear={2020},\nurl={https://openreview.net/forum?id=S1gmvyHFDS}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "S1gmvyHFDS", "replyto": "S1gmvyHFDS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795720299, "tmdate": 1576800271103, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1760/-/Decision"}}}, {"id": "rJe59wI7sS", "original": null, "number": 3, "cdate": 1573246866009, "ddate": null, "tcdate": 1573246866009, "tmdate": 1573562846840, "tddate": null, "forum": "S1gmvyHFDS", "replyto": "ryeRqiZaKH", "invitation": "ICLR.cc/2020/Conference/Paper1760/-/Official_Comment", "content": {"title": "Thank you for your detailed review", "comment": "We thank the reviewer for their review and detailed feedback. We will attempt to address each of the highlighted concerns.\n\n1. Re: how the detector and watermarking mechanisms are jointly obtained: We apologize for the confusion around this. Indeed, in our scheme the watermark generation and watermark detection are jointly optimized. We only refer to a fixed watermark below Definition 2.1 as an instructive motivating example for how we can train the detector, before explaining how the joint optimization is solved in Section 3. We can think of the game of learning both a watermark and a watermark detection model as adversarial training, where the classification task is to recognise the watermark. At each step of training, given an input, x, we find a perturbation (via gradient descent) that would cause the detector model to classify this input as watermarked, we then add this perturbation to the input, x+\\delta, and then update the detection model based on how strongly it classified x as not watermarked, and classified x+\\delta as watermarked. \n\n2. Re: motivation: The reviewer is entirely correct that deep fake creators would not want to add a watermark to their creations. Our threat model is designed to protect an individual or organization against abuse of data or media released by them. For example, consider an organization that has a corpus of photos/audio samples/videos, or a generative model that they allow the public access to query. These samples could then be used to create deep fakes by another party. If the organization has watermarked these samples with our scheme, the watermark is designed to be robust to post-processing transformations that an adversary might introduce, and so the watermark will persist when the deep fake creator manipulates the sample. The organization can then verify if the deep fake was originally created using samples that they own.\n\n3. Re: transferability: An attacker that has access to content that contains both non-watermarked and watermarked versions, is currently outside of our threat model. As far as we can tell, this is a relatively uncommon assumption in zero-bit watermarking, as if an attacker has a procedure to identify if a piece of content was or was not watermarked they have practically already broken the scheme. Furthermore, to increase the cost of this detection attack, in potential practical applications an organization can limit the exposure to watermarked content by prohibiting any single party from making a large number of queries to access content. Our experiments in Section 4.2.2 are designed to show that an attacker that does not have this information, but has other information about the watermarking procedure, cannot break the scheme. The attacker has full knowledge of the architecture of the detector model, the algorithm that is used to create watermarks and the detector, hyperparameters used in the algorithm, and the exact training set used, and so represents a relatively strong attack.\n\n4. Re: certified robustness: Section 4.2.3 attempts to show that a detector model that has been trained with post-processing transformations (eq. 1) is more robust than a model trained without post-processing transformations (eq. 1 with max over T omitted from the formulation). The certification procedure provides a certified region of robustness to Gaussian noise (correspondingly an L2 norm ball). Since Gaussian noise is included in the set of post-processing transformations, the model that has been trained to be robust to these transformations is naturally more robust to Gaussian noise (and so has a larger certified area with respect to the L2 norm) than the model that has not been trained on these set of transformations.\n\n5. Re: Validity of comparing with Broken Arrows: Please refer to our response to Blind Reviewer 1 regarding the concern of the comparison with Broken Arrows.\n\n6. Re: No numerical results in Section 4.2.4: The results on the SVHN test set is a single number, reporting that no mistakes were made by the detector. Results on BigGAN also had a false positive and false negative rate of <1.2% when attacked with various post-processing transformations. We have updated the paper now detailing these results.\n\nThank you again for the helpful review, we would be more than happy to act on any specific recommendations that the reviewer would consider important to revising the score for this paper.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1760/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1760/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["j.hayes@cs.ucl.ac.uk", "dvij@google.com", "yutianc@google.com", "sedielem@google.com", "pushmeet@google.com", "ncasagrande@google.com"], "title": "Provenance detection through learning transformation-resilient watermarking", "authors": ["Jamie Hayes", "Krishnamurthy Dvijotham", "Yutian Chen", "Sander Dieleman", "Pushmeet Kohli", "Norman Casagrande"], "pdf": "/pdf/8da5d919cb4e9e00a81f1bd4b375742021d6c27c.pdf", "TL;DR": "Develop a method to detect the provenance of signals that have undergone adversarial transformations.", "abstract": "Advancements in deep generative models have made it possible to synthesize images, videos and audio signals that are hard to distinguish from natural signals, creating opportunities for potential abuse of these capabilities. This motivates the problem of tracking the provenance of signals, i.e., being able to determine the original source of a signal. Watermarking the signal at the time of signal creation is a potential solution, but current techniques are brittle and watermark detection mechanisms can easily be bypassed by doing some post-processing (cropping images, shifting pitch in the audio etc.). In this paper, we introduce ReSWAT (Resilient Signal Watermarking via Adversarial Training), a framework for learning transformation-resilient watermark detectors that are able to detect a watermark even after a signal has been through several post-processing transformations. Our detection method can be applied to domains with continuous data representations such as images, videos or sound signals. Experiments on watermarking image and audio signals show that our method can reliably detect the provenance of a synthetic signal, even if the signal has been through several post-processing transformations, and improve upon related work in this setting. Furthermore, we show that for specific kinds of transformations (perturbations bounded in the $\\ell_2$ norm), we can even get formal guarantees on the ability of our model to detect the watermark.  We provide qualitative examples of watermarked image and audio samples in the anonymous code submission link.", "code": "https://drive.google.com/open?id=1c-qqHfTr3uMQSIuTR_Z8qZv0uTVrkH5m", "keywords": ["watermarking", "provenance detection"], "paperhash": "hayes|provenance_detection_through_learning_transformationresilient_watermarking", "original_pdf": "/attachment/4561c44462fc2c64c8b5a2240cf28a7a19628b41.pdf", "_bibtex": "@misc{\nhayes2020provenance,\ntitle={Provenance detection through learning transformation-resilient watermarking},\nauthor={Jamie Hayes and Krishnamurthy Dvijotham and Yutian Chen and Sander Dieleman and Pushmeet Kohli and Norman Casagrande},\nyear={2020},\nurl={https://openreview.net/forum?id=S1gmvyHFDS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1gmvyHFDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1760/Authors", "ICLR.cc/2020/Conference/Paper1760/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1760/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1760/Reviewers", "ICLR.cc/2020/Conference/Paper1760/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1760/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1760/Authors|ICLR.cc/2020/Conference/Paper1760/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504151282, "tmdate": 1576860552574, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1760/Authors", "ICLR.cc/2020/Conference/Paper1760/Reviewers", "ICLR.cc/2020/Conference/Paper1760/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1760/-/Official_Comment"}}}, {"id": "H1gouI8QoH", "original": null, "number": 2, "cdate": 1573246579352, "ddate": null, "tcdate": 1573246579352, "tmdate": 1573250155636, "tddate": null, "forum": "S1gmvyHFDS", "replyto": "SyeSWv57cB", "invitation": "ICLR.cc/2020/Conference/Paper1760/-/Official_Comment", "content": {"title": "Thank you for your comments", "comment": "We thank the reviewer for their careful review and thoughtful feedback. We agree that an exciting direction for future work would be to compare this scheme to other key-based watermarking schemes. In this work, we decided to focus on zero-bit watermarking as it most naturally aligns with the goal of provenance detection. However, it is true that the scheme could be extended in the future to multi-bit watermarking schemes and in that case it would be natural to compare against other key-based multi-bit watermarking schemes. Please refer to our response to Blind Reviewer 1 regarding the concern of the comparison with Broken Arrows.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1760/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1760/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["j.hayes@cs.ucl.ac.uk", "dvij@google.com", "yutianc@google.com", "sedielem@google.com", "pushmeet@google.com", "ncasagrande@google.com"], "title": "Provenance detection through learning transformation-resilient watermarking", "authors": ["Jamie Hayes", "Krishnamurthy Dvijotham", "Yutian Chen", "Sander Dieleman", "Pushmeet Kohli", "Norman Casagrande"], "pdf": "/pdf/8da5d919cb4e9e00a81f1bd4b375742021d6c27c.pdf", "TL;DR": "Develop a method to detect the provenance of signals that have undergone adversarial transformations.", "abstract": "Advancements in deep generative models have made it possible to synthesize images, videos and audio signals that are hard to distinguish from natural signals, creating opportunities for potential abuse of these capabilities. This motivates the problem of tracking the provenance of signals, i.e., being able to determine the original source of a signal. Watermarking the signal at the time of signal creation is a potential solution, but current techniques are brittle and watermark detection mechanisms can easily be bypassed by doing some post-processing (cropping images, shifting pitch in the audio etc.). In this paper, we introduce ReSWAT (Resilient Signal Watermarking via Adversarial Training), a framework for learning transformation-resilient watermark detectors that are able to detect a watermark even after a signal has been through several post-processing transformations. Our detection method can be applied to domains with continuous data representations such as images, videos or sound signals. Experiments on watermarking image and audio signals show that our method can reliably detect the provenance of a synthetic signal, even if the signal has been through several post-processing transformations, and improve upon related work in this setting. Furthermore, we show that for specific kinds of transformations (perturbations bounded in the $\\ell_2$ norm), we can even get formal guarantees on the ability of our model to detect the watermark.  We provide qualitative examples of watermarked image and audio samples in the anonymous code submission link.", "code": "https://drive.google.com/open?id=1c-qqHfTr3uMQSIuTR_Z8qZv0uTVrkH5m", "keywords": ["watermarking", "provenance detection"], "paperhash": "hayes|provenance_detection_through_learning_transformationresilient_watermarking", "original_pdf": "/attachment/4561c44462fc2c64c8b5a2240cf28a7a19628b41.pdf", "_bibtex": "@misc{\nhayes2020provenance,\ntitle={Provenance detection through learning transformation-resilient watermarking},\nauthor={Jamie Hayes and Krishnamurthy Dvijotham and Yutian Chen and Sander Dieleman and Pushmeet Kohli and Norman Casagrande},\nyear={2020},\nurl={https://openreview.net/forum?id=S1gmvyHFDS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1gmvyHFDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1760/Authors", "ICLR.cc/2020/Conference/Paper1760/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1760/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1760/Reviewers", "ICLR.cc/2020/Conference/Paper1760/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1760/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1760/Authors|ICLR.cc/2020/Conference/Paper1760/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504151282, "tmdate": 1576860552574, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1760/Authors", "ICLR.cc/2020/Conference/Paper1760/Reviewers", "ICLR.cc/2020/Conference/Paper1760/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1760/-/Official_Comment"}}}, {"id": "BygcMIUmjr", "original": null, "number": 1, "cdate": 1573246481625, "ddate": null, "tcdate": 1573246481625, "tmdate": 1573246481625, "tddate": null, "forum": "S1gmvyHFDS", "replyto": "rklOwGk0YH", "invitation": "ICLR.cc/2020/Conference/Paper1760/-/Official_Comment", "content": {"title": "Thank you for your comments", "comment": "We thank the reviewer for their careful review and thoughtful feedback. \n\nA concern was raised in the reviews that since Broken Arrows was proposed ten years ago, it might not reflect the state of the art. We had performed an extensive literature survey of zero-bit watermarking algorithms during our research. We used Broken Arrows to compare against our method for the following reasons:\n1. To the best of our knowledge, no publicly available zero-bit watermarking scheme has been published that claims superiority over Broken Arrows and is still used as the state-of-the-art watermarking method by other works. For example, Quiring et al. (2018) [1] used Broken Arrows in their recent work (see Section 4.1 in [1]).\n2. Broken Arrows won an international competition designed for defending against watermarking attacks [2]. \n3. It has provable robustness to Gaussian noise attacks (see Furon & Bas (2008) [3]).\n\n[1] Quiring, Erwin, Daniel Arp, and Konrad Rieck. \"Forgotten siblings: Unifying attacks on machine learning and digital watermarking.\" 2018 IEEE European Symposium on Security and Privacy (EuroS&P). IEEE, 2018.\n[2] http://bows2.ec-lille.fr/\n[3] Furon, Teddy, and Patrick Bas. \"Broken arrows.\" EURASIP Journal on Information Security 2008.1 (2008): 597040.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1760/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1760/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["j.hayes@cs.ucl.ac.uk", "dvij@google.com", "yutianc@google.com", "sedielem@google.com", "pushmeet@google.com", "ncasagrande@google.com"], "title": "Provenance detection through learning transformation-resilient watermarking", "authors": ["Jamie Hayes", "Krishnamurthy Dvijotham", "Yutian Chen", "Sander Dieleman", "Pushmeet Kohli", "Norman Casagrande"], "pdf": "/pdf/8da5d919cb4e9e00a81f1bd4b375742021d6c27c.pdf", "TL;DR": "Develop a method to detect the provenance of signals that have undergone adversarial transformations.", "abstract": "Advancements in deep generative models have made it possible to synthesize images, videos and audio signals that are hard to distinguish from natural signals, creating opportunities for potential abuse of these capabilities. This motivates the problem of tracking the provenance of signals, i.e., being able to determine the original source of a signal. Watermarking the signal at the time of signal creation is a potential solution, but current techniques are brittle and watermark detection mechanisms can easily be bypassed by doing some post-processing (cropping images, shifting pitch in the audio etc.). In this paper, we introduce ReSWAT (Resilient Signal Watermarking via Adversarial Training), a framework for learning transformation-resilient watermark detectors that are able to detect a watermark even after a signal has been through several post-processing transformations. Our detection method can be applied to domains with continuous data representations such as images, videos or sound signals. Experiments on watermarking image and audio signals show that our method can reliably detect the provenance of a synthetic signal, even if the signal has been through several post-processing transformations, and improve upon related work in this setting. Furthermore, we show that for specific kinds of transformations (perturbations bounded in the $\\ell_2$ norm), we can even get formal guarantees on the ability of our model to detect the watermark.  We provide qualitative examples of watermarked image and audio samples in the anonymous code submission link.", "code": "https://drive.google.com/open?id=1c-qqHfTr3uMQSIuTR_Z8qZv0uTVrkH5m", "keywords": ["watermarking", "provenance detection"], "paperhash": "hayes|provenance_detection_through_learning_transformationresilient_watermarking", "original_pdf": "/attachment/4561c44462fc2c64c8b5a2240cf28a7a19628b41.pdf", "_bibtex": "@misc{\nhayes2020provenance,\ntitle={Provenance detection through learning transformation-resilient watermarking},\nauthor={Jamie Hayes and Krishnamurthy Dvijotham and Yutian Chen and Sander Dieleman and Pushmeet Kohli and Norman Casagrande},\nyear={2020},\nurl={https://openreview.net/forum?id=S1gmvyHFDS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1gmvyHFDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1760/Authors", "ICLR.cc/2020/Conference/Paper1760/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1760/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1760/Reviewers", "ICLR.cc/2020/Conference/Paper1760/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1760/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1760/Authors|ICLR.cc/2020/Conference/Paper1760/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504151282, "tmdate": 1576860552574, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1760/Authors", "ICLR.cc/2020/Conference/Paper1760/Reviewers", "ICLR.cc/2020/Conference/Paper1760/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1760/-/Official_Comment"}}}, {"id": "ryeRqiZaKH", "original": null, "number": 1, "cdate": 1571785621712, "ddate": null, "tcdate": 1571785621712, "tmdate": 1572972427073, "tddate": null, "forum": "S1gmvyHFDS", "replyto": "S1gmvyHFDS", "invitation": "ICLR.cc/2020/Conference/Paper1760/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The paper proposes a watermark design algorithm that is based on an adversarial training paradigm where a watermark detector and a watermark generator are jointly trained to maximize the likelihood of watermark detection while bounding the manipulation of individual pixels. \n\nI have several concerns about the clarity of the paper, its applicability to the motivating problem, and the completeness of the results that motivate my \"reject\" recommendation.\n\nThe description does not make it clear how the detector and watermark mechanism are jointly obtained in (1) - intuitively, a detector should know the type of watermark to be observed in order to pose a feasible detection model. The discussion focuses on \"fixed detector\" and \"fixed watermark\" cases only, but it is not clear how an iterative optimization would be initialized, and no discussion of how to set up the problem appears in the manuscript. \n\nThe motivation for the paper is also unclear. The authors discuss the need to be able to detect deep fakes, which is a relevant and meritorious problem. However, it seems that the application of the proposed approach in this setting would require deep fake creators to add watermarks to their creations, which would be a naive assumption. Watermarks are used to demonstrate provenance, but deep fake creators would not want to have such provenance verified.\n\nThe watermark transferability premise seems to rely on the random initialization of the underlying deep learning watermark detector. One could posit that if a third party is interested in creating false positives, they could train the deep learning network using a database of watermarked and non-watermarked images. Another concern here is that the formulation provided in (3) would potentially lead to low transferability for the specific classifiers provided there, but one does not know which classifier an adversary would construct - or why an adversary would construct an adversary that does not have knowledge of the specific watermark being used, or of samples of the watermark. It would have been interesting to see if it is possible to construct a good watermark detector this way (given enough training data being available). It would also be good to have some intuition as to why this robustness formulation also improves \"certified robustness\".\n\nFinally, the performance comparison only considers a 10-year old watermarking algorithm; some discussion as to why this is sufficient should have been included.\n\nMinor comments\nThe norm infinity subindex is missing from the minimization conditions in (1).\nFigure 3 caption: watermark's -> watermarks\nIt is not clear why no numerical results were given for Section 4.2.4."}, "signatures": ["ICLR.cc/2020/Conference/Paper1760/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1760/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["j.hayes@cs.ucl.ac.uk", "dvij@google.com", "yutianc@google.com", "sedielem@google.com", "pushmeet@google.com", "ncasagrande@google.com"], "title": "Provenance detection through learning transformation-resilient watermarking", "authors": ["Jamie Hayes", "Krishnamurthy Dvijotham", "Yutian Chen", "Sander Dieleman", "Pushmeet Kohli", "Norman Casagrande"], "pdf": "/pdf/8da5d919cb4e9e00a81f1bd4b375742021d6c27c.pdf", "TL;DR": "Develop a method to detect the provenance of signals that have undergone adversarial transformations.", "abstract": "Advancements in deep generative models have made it possible to synthesize images, videos and audio signals that are hard to distinguish from natural signals, creating opportunities for potential abuse of these capabilities. This motivates the problem of tracking the provenance of signals, i.e., being able to determine the original source of a signal. Watermarking the signal at the time of signal creation is a potential solution, but current techniques are brittle and watermark detection mechanisms can easily be bypassed by doing some post-processing (cropping images, shifting pitch in the audio etc.). In this paper, we introduce ReSWAT (Resilient Signal Watermarking via Adversarial Training), a framework for learning transformation-resilient watermark detectors that are able to detect a watermark even after a signal has been through several post-processing transformations. Our detection method can be applied to domains with continuous data representations such as images, videos or sound signals. Experiments on watermarking image and audio signals show that our method can reliably detect the provenance of a synthetic signal, even if the signal has been through several post-processing transformations, and improve upon related work in this setting. Furthermore, we show that for specific kinds of transformations (perturbations bounded in the $\\ell_2$ norm), we can even get formal guarantees on the ability of our model to detect the watermark.  We provide qualitative examples of watermarked image and audio samples in the anonymous code submission link.", "code": "https://drive.google.com/open?id=1c-qqHfTr3uMQSIuTR_Z8qZv0uTVrkH5m", "keywords": ["watermarking", "provenance detection"], "paperhash": "hayes|provenance_detection_through_learning_transformationresilient_watermarking", "original_pdf": "/attachment/4561c44462fc2c64c8b5a2240cf28a7a19628b41.pdf", "_bibtex": "@misc{\nhayes2020provenance,\ntitle={Provenance detection through learning transformation-resilient watermarking},\nauthor={Jamie Hayes and Krishnamurthy Dvijotham and Yutian Chen and Sander Dieleman and Pushmeet Kohli and Norman Casagrande},\nyear={2020},\nurl={https://openreview.net/forum?id=S1gmvyHFDS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "S1gmvyHFDS", "replyto": "S1gmvyHFDS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1760/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1760/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575917728670, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1760/Reviewers"], "noninvitees": [], "tcdate": 1570237732686, "tmdate": 1575917728683, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1760/-/Official_Review"}}}, {"id": "rklOwGk0YH", "original": null, "number": 2, "cdate": 1571840608054, "ddate": null, "tcdate": 1571840608054, "tmdate": 1572972427028, "tddate": null, "forum": "S1gmvyHFDS", "replyto": "S1gmvyHFDS", "invitation": "ICLR.cc/2020/Conference/Paper1760/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The authors introduce ReSWAT, a method for transformation-resilient watermarking of images via adversarial training. The high level idea is to learn a watermark/detector pair (W,D). W can be any transformation (in this paper, an l-infty bounded perturbation) that imputes an imperceptible distortion to a given input, while D is a detector that distinguishes watermarked from non-watermarked images. There is an additional requirement that the detector should be robust to simple transformations such as rotations, cropping, flipping, and contrast enhancement. \n\nThe authors pose a min-max style learning problem for learning (W,D) that leads to a natural adversarial training scheme. In particular, it can be viewed as creating an adversarial defense to the Expectations-over-transformations attack of Athalye et al. Experimental results on a bunch of different datasets confirm that the method works,\n\nThe paper is well-written and the contributions are clear. In terms of conceptual or theoretical novelty, the paper is limited (the method essentially boils down to regular adversarial training with a slightly non-standard loss function) but the connection to watermarking seems novel and is nicely executed.\n\nMy only concern is the lack of comparisons with baselines. I am not a digital forensics expert -- so I don't know what the state of the art is -- but the only comparisons made are with the Broken Arrows (BA) watermarking , which seems to be over 10 years old, so I am not sure how to evaluate the results."}, "signatures": ["ICLR.cc/2020/Conference/Paper1760/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1760/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["j.hayes@cs.ucl.ac.uk", "dvij@google.com", "yutianc@google.com", "sedielem@google.com", "pushmeet@google.com", "ncasagrande@google.com"], "title": "Provenance detection through learning transformation-resilient watermarking", "authors": ["Jamie Hayes", "Krishnamurthy Dvijotham", "Yutian Chen", "Sander Dieleman", "Pushmeet Kohli", "Norman Casagrande"], "pdf": "/pdf/8da5d919cb4e9e00a81f1bd4b375742021d6c27c.pdf", "TL;DR": "Develop a method to detect the provenance of signals that have undergone adversarial transformations.", "abstract": "Advancements in deep generative models have made it possible to synthesize images, videos and audio signals that are hard to distinguish from natural signals, creating opportunities for potential abuse of these capabilities. This motivates the problem of tracking the provenance of signals, i.e., being able to determine the original source of a signal. Watermarking the signal at the time of signal creation is a potential solution, but current techniques are brittle and watermark detection mechanisms can easily be bypassed by doing some post-processing (cropping images, shifting pitch in the audio etc.). In this paper, we introduce ReSWAT (Resilient Signal Watermarking via Adversarial Training), a framework for learning transformation-resilient watermark detectors that are able to detect a watermark even after a signal has been through several post-processing transformations. Our detection method can be applied to domains with continuous data representations such as images, videos or sound signals. Experiments on watermarking image and audio signals show that our method can reliably detect the provenance of a synthetic signal, even if the signal has been through several post-processing transformations, and improve upon related work in this setting. Furthermore, we show that for specific kinds of transformations (perturbations bounded in the $\\ell_2$ norm), we can even get formal guarantees on the ability of our model to detect the watermark.  We provide qualitative examples of watermarked image and audio samples in the anonymous code submission link.", "code": "https://drive.google.com/open?id=1c-qqHfTr3uMQSIuTR_Z8qZv0uTVrkH5m", "keywords": ["watermarking", "provenance detection"], "paperhash": "hayes|provenance_detection_through_learning_transformationresilient_watermarking", "original_pdf": "/attachment/4561c44462fc2c64c8b5a2240cf28a7a19628b41.pdf", "_bibtex": "@misc{\nhayes2020provenance,\ntitle={Provenance detection through learning transformation-resilient watermarking},\nauthor={Jamie Hayes and Krishnamurthy Dvijotham and Yutian Chen and Sander Dieleman and Pushmeet Kohli and Norman Casagrande},\nyear={2020},\nurl={https://openreview.net/forum?id=S1gmvyHFDS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "S1gmvyHFDS", "replyto": "S1gmvyHFDS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1760/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1760/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575917728670, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1760/Reviewers"], "noninvitees": [], "tcdate": 1570237732686, "tmdate": 1575917728683, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1760/-/Official_Review"}}}, {"id": "SyeSWv57cB", "original": null, "number": 3, "cdate": 1572214525133, "ddate": null, "tcdate": 1572214525133, "tmdate": 1572972426985, "tddate": null, "forum": "S1gmvyHFDS", "replyto": "S1gmvyHFDS", "invitation": "ICLR.cc/2020/Conference/Paper1760/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper is about a novel method to add watermarks to images and audio that is highly robust to several transformations that is closely related to gan methods. The idea is that the watermark signal is learned concurrently to the detector network, which share similarities to a generator and detector networks. Five standard attack transformations are considered and a specific optimization to reduce the transferability of the watermark is considered. The method is compared against Broken arrows on Cifar10 and Imagenet. It shows similar or better performance for Gaussian noise attack given the same amount of perturbance allowed in a signal while very much better performance for the other attacks. Going beyond the five attacks, the paper also includes an estimation of the probability of confidence of finding watermarked images given a fixed l2 norm radius. Finally the method is also tested on audio on a proprietary dataset with a deepspeaker architecture which still shows very good performance and it is confirmed by a human evaluation where participants found the watermarked audio not significantly worse or degraded.\n\nThe paper is well written and of excellent presentation. The proposed method is novel and goes in the interesting direction of learning watermarks using adversarial training techniques. Experiments shows that the method has good performance but it is only compared to Broken Arrows (i.e. a zero bit watermarking) which is the state of the art of this type of watermarking. It would have been interesting a comparison to other key based watermarking methods to also deeply evaluate the signal transformation attacks. All in all, I found the paper to be significant and I would like to see it accepted."}, "signatures": ["ICLR.cc/2020/Conference/Paper1760/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1760/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["j.hayes@cs.ucl.ac.uk", "dvij@google.com", "yutianc@google.com", "sedielem@google.com", "pushmeet@google.com", "ncasagrande@google.com"], "title": "Provenance detection through learning transformation-resilient watermarking", "authors": ["Jamie Hayes", "Krishnamurthy Dvijotham", "Yutian Chen", "Sander Dieleman", "Pushmeet Kohli", "Norman Casagrande"], "pdf": "/pdf/8da5d919cb4e9e00a81f1bd4b375742021d6c27c.pdf", "TL;DR": "Develop a method to detect the provenance of signals that have undergone adversarial transformations.", "abstract": "Advancements in deep generative models have made it possible to synthesize images, videos and audio signals that are hard to distinguish from natural signals, creating opportunities for potential abuse of these capabilities. This motivates the problem of tracking the provenance of signals, i.e., being able to determine the original source of a signal. Watermarking the signal at the time of signal creation is a potential solution, but current techniques are brittle and watermark detection mechanisms can easily be bypassed by doing some post-processing (cropping images, shifting pitch in the audio etc.). In this paper, we introduce ReSWAT (Resilient Signal Watermarking via Adversarial Training), a framework for learning transformation-resilient watermark detectors that are able to detect a watermark even after a signal has been through several post-processing transformations. Our detection method can be applied to domains with continuous data representations such as images, videos or sound signals. Experiments on watermarking image and audio signals show that our method can reliably detect the provenance of a synthetic signal, even if the signal has been through several post-processing transformations, and improve upon related work in this setting. Furthermore, we show that for specific kinds of transformations (perturbations bounded in the $\\ell_2$ norm), we can even get formal guarantees on the ability of our model to detect the watermark.  We provide qualitative examples of watermarked image and audio samples in the anonymous code submission link.", "code": "https://drive.google.com/open?id=1c-qqHfTr3uMQSIuTR_Z8qZv0uTVrkH5m", "keywords": ["watermarking", "provenance detection"], "paperhash": "hayes|provenance_detection_through_learning_transformationresilient_watermarking", "original_pdf": "/attachment/4561c44462fc2c64c8b5a2240cf28a7a19628b41.pdf", "_bibtex": "@misc{\nhayes2020provenance,\ntitle={Provenance detection through learning transformation-resilient watermarking},\nauthor={Jamie Hayes and Krishnamurthy Dvijotham and Yutian Chen and Sander Dieleman and Pushmeet Kohli and Norman Casagrande},\nyear={2020},\nurl={https://openreview.net/forum?id=S1gmvyHFDS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "S1gmvyHFDS", "replyto": "S1gmvyHFDS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1760/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1760/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575917728670, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1760/Reviewers"], "noninvitees": [], "tcdate": 1570237732686, "tmdate": 1575917728683, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1760/-/Official_Review"}}}], "count": 8}