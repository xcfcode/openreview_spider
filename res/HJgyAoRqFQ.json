{"notes": [{"id": "HJgyAoRqFQ", "original": "rkxjtqpctX", "number": 857, "cdate": 1538087879004, "ddate": null, "tcdate": 1538087879004, "tmdate": 1545355439898, "tddate": null, "forum": "HJgyAoRqFQ", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "State-Denoised Recurrent Neural Networks", "abstract": "Recurrent neural networks (RNNs) are difficult to train on sequence processing tasks, not only because input noise may be amplified through feedback, but also because any inaccuracy in the weights has similar consequences as input noise. We describe a method for denoising the hidden state during training to achieve more robust representations thereby improving generalization performance. Attractor dynamics are incorporated into the hidden state to `clean up' representations at each step of a sequence. The attractor dynamics are trained through an auxillary denoising loss to recover previously experienced hidden states from noisy versions of those states. This state-denoised recurrent neural network (SDRNN) performs multiple steps of internal processing for each external sequence step. On a range of tasks, we show that the SDRNN outperforms a generic RNN as well as a variant of the SDRNN with attractor dynamics on the hidden state but without the auxillary loss. We argue that attractor dynamics---and corresponding connectivity constraints---are an essential component of the deep learning arsenal and should be invoked not only for recurrent networks but also for improving deep feedforward nets and intertask transfer.", "keywords": ["recurrent nets", "attractor nets", "denoising", "sequence processing"], "authorids": ["mozer@colorado.edu", "denis.kazakov@colorado.edu", "rob@imagen.ai"], "authors": ["Michael C. Mozer", "Denis Kazakov", "Robert V. Lindsey"], "TL;DR": "We propose a mechanism for denoising the internal state of an RNN to improve generalization performance.", "pdf": "/pdf/e739554c0f3d38f53d82206aafb0f5335ecb5305.pdf", "paperhash": "mozer|statedenoised_recurrent_neural_networks", "_bibtex": "@misc{\nmozer2019statedenoised,\ntitle={State-Denoised Recurrent Neural Networks},\nauthor={Michael C. Mozer and Denis Kazakov and Robert V. Lindsey},\nyear={2019},\nurl={https://openreview.net/forum?id=HJgyAoRqFQ},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "S1lkDRr8xN", "original": null, "number": 1, "cdate": 1545129559267, "ddate": null, "tcdate": 1545129559267, "tmdate": 1545354477464, "tddate": null, "forum": "HJgyAoRqFQ", "replyto": "HJgyAoRqFQ", "invitation": "ICLR.cc/2019/Conference/-/Paper857/Meta_Review", "content": {"metareview": "The paper is well written and develops a novel and original architecture and technique for RNNs to learn attractors for their hidden states (based on an auxiliary denoising training of an attractor network). All reviewers and AC found the idea very interesting and a promising direction of research for RNNs. However all also agreed that the experimental validation was currently too limited, in type and size of task and data, as in scope. Reviewers demand experimental comparisons with other (simpler) denoising / regularization techniques; more in depth experimental validation and analysis of the state-denoising behaviour; as well as experiments on larger datasets and more ambitious tasks. ", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Reject", "title": "Promising novel idea for RNN training, with too limited experiments"}, "signatures": ["ICLR.cc/2019/Conference/Paper857/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper857/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "State-Denoised Recurrent Neural Networks", "abstract": "Recurrent neural networks (RNNs) are difficult to train on sequence processing tasks, not only because input noise may be amplified through feedback, but also because any inaccuracy in the weights has similar consequences as input noise. We describe a method for denoising the hidden state during training to achieve more robust representations thereby improving generalization performance. Attractor dynamics are incorporated into the hidden state to `clean up' representations at each step of a sequence. The attractor dynamics are trained through an auxillary denoising loss to recover previously experienced hidden states from noisy versions of those states. This state-denoised recurrent neural network (SDRNN) performs multiple steps of internal processing for each external sequence step. On a range of tasks, we show that the SDRNN outperforms a generic RNN as well as a variant of the SDRNN with attractor dynamics on the hidden state but without the auxillary loss. We argue that attractor dynamics---and corresponding connectivity constraints---are an essential component of the deep learning arsenal and should be invoked not only for recurrent networks but also for improving deep feedforward nets and intertask transfer.", "keywords": ["recurrent nets", "attractor nets", "denoising", "sequence processing"], "authorids": ["mozer@colorado.edu", "denis.kazakov@colorado.edu", "rob@imagen.ai"], "authors": ["Michael C. Mozer", "Denis Kazakov", "Robert V. Lindsey"], "TL;DR": "We propose a mechanism for denoising the internal state of an RNN to improve generalization performance.", "pdf": "/pdf/e739554c0f3d38f53d82206aafb0f5335ecb5305.pdf", "paperhash": "mozer|statedenoised_recurrent_neural_networks", "_bibtex": "@misc{\nmozer2019statedenoised,\ntitle={State-Denoised Recurrent Neural Networks},\nauthor={Michael C. Mozer and Denis Kazakov and Robert V. Lindsey},\nyear={2019},\nurl={https://openreview.net/forum?id=HJgyAoRqFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper857/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353060041, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJgyAoRqFQ", "replyto": "HJgyAoRqFQ", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper857/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper857/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper857/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353060041}}}, {"id": "rkgctaSiCQ", "original": null, "number": 4, "cdate": 1543359874350, "ddate": null, "tcdate": 1543359874350, "tmdate": 1543359874350, "tddate": null, "forum": "HJgyAoRqFQ", "replyto": "S1gsJSsy2m", "invitation": "ICLR.cc/2019/Conference/-/Paper857/Official_Comment", "content": {"title": "Response to AnonReviewer1", "comment": "Your major point is appreciated, but we worry we have misled readers by using the term 'noise' in a fast and loose manner. Certainly corruption to the hidden state due to untrained or poorly trained weights is _not_ anything close to Gaussian. We see that we have been misleading in suggesting that our denoising training procedure is designed to eliminate noise, especially in the context of training it on inputs with added Gaussian noise. What the training procedure actually does  is to establish (nonGaussian) attractor manifolds that cause a set of hidden states to be clustered together. (See our Response to AnonReviewer3 for additional details.)  It is this clustering of states where we believe the attractor dynamics are valuable.  Denoising facilitates this clustering. The advantage of clustering with attractor dynamics over something like K-Means is that (1) attractor dynamics are flexible in terms of the number and shape of clusters, (2) we can compute gradients through attractor dynamics.  While we certainly can and should conduct simulations with other regularizers, we are highly confident that they will not have the same property as our attractor net denoising.\n\nYou asked whether it matters whether the 'c' variable is used as a bias rather than initial condition. It is absolutely essential for c to be a bias as it has a persistent effect on a final state. In Figure 3, treating c as a bias achieves a type of skip connection (see blue lines) that facilitates back propagation.  Finally, note that even in Hopfield nets (and certainly in Boltzmann machines), the external input must serve as a persistent bias that helps to shape energy landscapes.  We did some experiments in which c is _also_ used as the initial state, but doing so did not affect the results.\n\nThe Hopfield net training algorithm (Hebbian learning) is simpler than our loss-minimizing training procedure. But the Hopfield algorithm does not support hidden attractor state.  We will note this in subsequent revisions."}, "signatures": ["ICLR.cc/2019/Conference/Paper857/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper857/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper857/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "State-Denoised Recurrent Neural Networks", "abstract": "Recurrent neural networks (RNNs) are difficult to train on sequence processing tasks, not only because input noise may be amplified through feedback, but also because any inaccuracy in the weights has similar consequences as input noise. We describe a method for denoising the hidden state during training to achieve more robust representations thereby improving generalization performance. Attractor dynamics are incorporated into the hidden state to `clean up' representations at each step of a sequence. The attractor dynamics are trained through an auxillary denoising loss to recover previously experienced hidden states from noisy versions of those states. This state-denoised recurrent neural network (SDRNN) performs multiple steps of internal processing for each external sequence step. On a range of tasks, we show that the SDRNN outperforms a generic RNN as well as a variant of the SDRNN with attractor dynamics on the hidden state but without the auxillary loss. We argue that attractor dynamics---and corresponding connectivity constraints---are an essential component of the deep learning arsenal and should be invoked not only for recurrent networks but also for improving deep feedforward nets and intertask transfer.", "keywords": ["recurrent nets", "attractor nets", "denoising", "sequence processing"], "authorids": ["mozer@colorado.edu", "denis.kazakov@colorado.edu", "rob@imagen.ai"], "authors": ["Michael C. Mozer", "Denis Kazakov", "Robert V. Lindsey"], "TL;DR": "We propose a mechanism for denoising the internal state of an RNN to improve generalization performance.", "pdf": "/pdf/e739554c0f3d38f53d82206aafb0f5335ecb5305.pdf", "paperhash": "mozer|statedenoised_recurrent_neural_networks", "_bibtex": "@misc{\nmozer2019statedenoised,\ntitle={State-Denoised Recurrent Neural Networks},\nauthor={Michael C. Mozer and Denis Kazakov and Robert V. Lindsey},\nyear={2019},\nurl={https://openreview.net/forum?id=HJgyAoRqFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper857/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621624815, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJgyAoRqFQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper857/Authors", "ICLR.cc/2019/Conference/Paper857/Reviewers", "ICLR.cc/2019/Conference/Paper857/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper857/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper857/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper857/Authors|ICLR.cc/2019/Conference/Paper857/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper857/Reviewers", "ICLR.cc/2019/Conference/Paper857/Authors", "ICLR.cc/2019/Conference/Paper857/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621624815}}}, {"id": "Ske6I8SiAQ", "original": null, "number": 3, "cdate": 1543358036808, "ddate": null, "tcdate": 1543358036808, "tmdate": 1543358136348, "tddate": null, "forum": "HJgyAoRqFQ", "replyto": "BJgkzBas37", "invitation": "ICLR.cc/2019/Conference/-/Paper857/Official_Comment", "content": {"title": "Responses to AnonReviewer3", "comment": "We anonymized the 1994 citation because one of the current paper authors was a co-author on the 1994 paper.\n\nThank you for your spot-on summary of what denoising is intended to do by reference to Hopfield nets. Hopfield nets can take a corrupted or partial input and reconstruct the stored memory. An important property of Hopfield nets is that if two stored memories are very close, they can combine into a manifold that contains both memories (and other similar states), thereby performing a type of implicit clustering.  It is this clustering of nearby states that we leverage when we train the attractor net on the set of hidden states reached by the RNN. By mapping similar hidden states to the same attractor or attractor manifold, we impose a bias on the network to ignore small variations in the hidden state. This bias is valuable for symbolic tasks and imposes a form of regularization  early in training.\n\nYou asked about time scale:  The time scale of change to the hidden state over the elements of a sequence is completely orthogonal to the time scale of attractor dynamics. In Figure 3, the time scale of hidden state evolution is represented by the columns and the time scale of the attractor net evolution is represented by the green rows of neurons.\n\nConcerning the Pascanu et al. (arXiv 1312.6026) paper: Our SDRNN and RNN+A architectures  certainly fall into the class of models with deep hidden-to-hidden transitions, as described by Pascanu. As Pascanu noted, these models do not train well without skip connections. Indeed, our approach also leverages skip connections of a sort (as represented by the blue edges in Figure 3). However, the skip connections and architecture depth are not in and of themselves sufficient:  our RNN+A fails, although it has both, whereas our SDRNN success, because it has the auxillary denoising loss. We will add citations to Pascanu."}, "signatures": ["ICLR.cc/2019/Conference/Paper857/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper857/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper857/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "State-Denoised Recurrent Neural Networks", "abstract": "Recurrent neural networks (RNNs) are difficult to train on sequence processing tasks, not only because input noise may be amplified through feedback, but also because any inaccuracy in the weights has similar consequences as input noise. We describe a method for denoising the hidden state during training to achieve more robust representations thereby improving generalization performance. Attractor dynamics are incorporated into the hidden state to `clean up' representations at each step of a sequence. The attractor dynamics are trained through an auxillary denoising loss to recover previously experienced hidden states from noisy versions of those states. This state-denoised recurrent neural network (SDRNN) performs multiple steps of internal processing for each external sequence step. On a range of tasks, we show that the SDRNN outperforms a generic RNN as well as a variant of the SDRNN with attractor dynamics on the hidden state but without the auxillary loss. We argue that attractor dynamics---and corresponding connectivity constraints---are an essential component of the deep learning arsenal and should be invoked not only for recurrent networks but also for improving deep feedforward nets and intertask transfer.", "keywords": ["recurrent nets", "attractor nets", "denoising", "sequence processing"], "authorids": ["mozer@colorado.edu", "denis.kazakov@colorado.edu", "rob@imagen.ai"], "authors": ["Michael C. Mozer", "Denis Kazakov", "Robert V. Lindsey"], "TL;DR": "We propose a mechanism for denoising the internal state of an RNN to improve generalization performance.", "pdf": "/pdf/e739554c0f3d38f53d82206aafb0f5335ecb5305.pdf", "paperhash": "mozer|statedenoised_recurrent_neural_networks", "_bibtex": "@misc{\nmozer2019statedenoised,\ntitle={State-Denoised Recurrent Neural Networks},\nauthor={Michael C. Mozer and Denis Kazakov and Robert V. Lindsey},\nyear={2019},\nurl={https://openreview.net/forum?id=HJgyAoRqFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper857/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621624815, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJgyAoRqFQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper857/Authors", "ICLR.cc/2019/Conference/Paper857/Reviewers", "ICLR.cc/2019/Conference/Paper857/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper857/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper857/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper857/Authors|ICLR.cc/2019/Conference/Paper857/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper857/Reviewers", "ICLR.cc/2019/Conference/Paper857/Authors", "ICLR.cc/2019/Conference/Paper857/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621624815}}}, {"id": "HJx7C-SoCQ", "original": null, "number": 2, "cdate": 1543356874549, "ddate": null, "tcdate": 1543356874549, "tmdate": 1543356874549, "tddate": null, "forum": "HJgyAoRqFQ", "replyto": "Bye5F_Hb6Q", "invitation": "ICLR.cc/2019/Conference/-/Paper857/Official_Comment", "content": {"title": "Responses to AnonReviewer2", "comment": "It would indeed be interesting to have more insight into the factors contributing to the simulations in Section 2.1 (Figure 2). However, we limited our investigation of this stand-alone attractor net because it is based on randomly placed target states in the input/output space, and this assumption is most certainly violated when the target states are constrained by task context (as they are when the attractor net is incorporated into the sequence-processing net).\n\nWe have to agree with you that our experiments are modest in size, and we've done more data set exploration since the submission deadline. We have found that tasks with a nontrivial symbolic component (e.g., language processing) benefit the most, and other large-scale problems typically do not.\n\nConcerning  clarification of step 3 of the training procedure: In step 3, we give a pointer to section 2.1 which describes the denoising training procedure in detail.\n\nConcerning computational cost of denoising: As you suspect, this method adds significant more overhead to training. Our initial goal was to understand if and under what circumstances the architecture yields better generalization.\n\nConcerning RNN+A: the attractor component still has weight constraints that ensure attractor dynamics. Our main goal with RNN+A was to show that the architecture alone is inadequate to obtain good results; rather, the mix of training objectives is critical."}, "signatures": ["ICLR.cc/2019/Conference/Paper857/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper857/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper857/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "State-Denoised Recurrent Neural Networks", "abstract": "Recurrent neural networks (RNNs) are difficult to train on sequence processing tasks, not only because input noise may be amplified through feedback, but also because any inaccuracy in the weights has similar consequences as input noise. We describe a method for denoising the hidden state during training to achieve more robust representations thereby improving generalization performance. Attractor dynamics are incorporated into the hidden state to `clean up' representations at each step of a sequence. The attractor dynamics are trained through an auxillary denoising loss to recover previously experienced hidden states from noisy versions of those states. This state-denoised recurrent neural network (SDRNN) performs multiple steps of internal processing for each external sequence step. On a range of tasks, we show that the SDRNN outperforms a generic RNN as well as a variant of the SDRNN with attractor dynamics on the hidden state but without the auxillary loss. We argue that attractor dynamics---and corresponding connectivity constraints---are an essential component of the deep learning arsenal and should be invoked not only for recurrent networks but also for improving deep feedforward nets and intertask transfer.", "keywords": ["recurrent nets", "attractor nets", "denoising", "sequence processing"], "authorids": ["mozer@colorado.edu", "denis.kazakov@colorado.edu", "rob@imagen.ai"], "authors": ["Michael C. Mozer", "Denis Kazakov", "Robert V. Lindsey"], "TL;DR": "We propose a mechanism for denoising the internal state of an RNN to improve generalization performance.", "pdf": "/pdf/e739554c0f3d38f53d82206aafb0f5335ecb5305.pdf", "paperhash": "mozer|statedenoised_recurrent_neural_networks", "_bibtex": "@misc{\nmozer2019statedenoised,\ntitle={State-Denoised Recurrent Neural Networks},\nauthor={Michael C. Mozer and Denis Kazakov and Robert V. Lindsey},\nyear={2019},\nurl={https://openreview.net/forum?id=HJgyAoRqFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper857/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621624815, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJgyAoRqFQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper857/Authors", "ICLR.cc/2019/Conference/Paper857/Reviewers", "ICLR.cc/2019/Conference/Paper857/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper857/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper857/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper857/Authors|ICLR.cc/2019/Conference/Paper857/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper857/Reviewers", "ICLR.cc/2019/Conference/Paper857/Authors", "ICLR.cc/2019/Conference/Paper857/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621624815}}}, {"id": "Bye5F_Hb6Q", "original": null, "number": 3, "cdate": 1541654658462, "ddate": null, "tcdate": 1541654658462, "tmdate": 1541654658462, "tddate": null, "forum": "HJgyAoRqFQ", "replyto": "HJgyAoRqFQ", "invitation": "ICLR.cc/2019/Conference/-/Paper857/Official_Review", "content": {"title": "Interesting use of denoising based on attractor dynamics in RNNs, but weak experimental validation.", "review": "The authors propose to embed in a recurrent neural network (RNN) a multistage subnetwork that is trained to denoise its own state. This is done with an additional denoising cost term that essentially encourages the recurrent subnetwork to suppress noise during as the recurrence is unfolded in time.\nThe authors first demonstrate the denoising properties of this architecture, and then demonstrate its performance on a series of tasks combining it with regular tanh and GRU recurrent units.\n\nThe paper is clear and the main idea is rather interesting, but the presented experimental validations are arguably weak. The demonstration of the denoising properties of the network is rather superficial, in the sense that it does not give much insight into the functioning of the architecture, despite that presumably being the main goal of the section. In particular, it is not clear where the non-monotonic change in denoising as a function of network size comes from. Based on the attractor neural network literature that the authors cite at the beginning of the paper, it could be due to either the presence of spurious attractors, the absence of fixed-point attractors or the fact that the attractor network is trained above capacity. But the authors never go into a detailed analysis that could reveal the detailed functioning of their architecture and merely mention the hypothesis that for large networks denoising performance would decrease because of overfitting.\nAs for the experiments that are presented in the rest of the paper, while relevant, the types of tasks and datasets on which the proposed architecture is being tested are rather small.\n\nHere are some more specific comments and questions:\n- It would help to clarify the training procedure to explicitly mention in step 3 that training proceeds on sequences with added noise.\n- It is not clear how many times in each experiments step 3 is being repeated for each mini-batch, i.e. what computational overhead is required for the training of the SDRNN compared to a regular RNN.  \n- It is not clear whether the recurrent neural net called RNN with attractors (RNN+A) is indeed an attractor neural network. Does the state of the network indeed always converge to an attractor?\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper857/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "State-Denoised Recurrent Neural Networks", "abstract": "Recurrent neural networks (RNNs) are difficult to train on sequence processing tasks, not only because input noise may be amplified through feedback, but also because any inaccuracy in the weights has similar consequences as input noise. We describe a method for denoising the hidden state during training to achieve more robust representations thereby improving generalization performance. Attractor dynamics are incorporated into the hidden state to `clean up' representations at each step of a sequence. The attractor dynamics are trained through an auxillary denoising loss to recover previously experienced hidden states from noisy versions of those states. This state-denoised recurrent neural network (SDRNN) performs multiple steps of internal processing for each external sequence step. On a range of tasks, we show that the SDRNN outperforms a generic RNN as well as a variant of the SDRNN with attractor dynamics on the hidden state but without the auxillary loss. We argue that attractor dynamics---and corresponding connectivity constraints---are an essential component of the deep learning arsenal and should be invoked not only for recurrent networks but also for improving deep feedforward nets and intertask transfer.", "keywords": ["recurrent nets", "attractor nets", "denoising", "sequence processing"], "authorids": ["mozer@colorado.edu", "denis.kazakov@colorado.edu", "rob@imagen.ai"], "authors": ["Michael C. Mozer", "Denis Kazakov", "Robert V. Lindsey"], "TL;DR": "We propose a mechanism for denoising the internal state of an RNN to improve generalization performance.", "pdf": "/pdf/e739554c0f3d38f53d82206aafb0f5335ecb5305.pdf", "paperhash": "mozer|statedenoised_recurrent_neural_networks", "_bibtex": "@misc{\nmozer2019statedenoised,\ntitle={State-Denoised Recurrent Neural Networks},\nauthor={Michael C. Mozer and Denis Kazakov and Robert V. Lindsey},\nyear={2019},\nurl={https://openreview.net/forum?id=HJgyAoRqFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper857/Official_Review", "cdate": 1542234361142, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HJgyAoRqFQ", "replyto": "HJgyAoRqFQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper857/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335818077, "tmdate": 1552335818077, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper857/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "BJgkzBas37", "original": null, "number": 2, "cdate": 1541293319390, "ddate": null, "tcdate": 1541293319390, "tmdate": 1541533632176, "tddate": null, "forum": "HJgyAoRqFQ", "replyto": "HJgyAoRqFQ", "invitation": "ICLR.cc/2019/Conference/-/Paper857/Official_Review", "content": {"title": "Interesting submission, though more analysis could help", "review": "I think overall I appreciate the idea behind the work. I think the work is quite novel, and it also connects to bodies of literature (hopfield networks -- attractors based and more mainstream GRU/LSTM nets). Here are some notes that I have: \n\n1) There is a citation to anonymous 1994 -- not sure if it helps with anything. Is this work published? Why not, 1994 is quite a bit ago! I can\u2019t see any reason why from 1994 until now this should stay anonymous. \n\n2) Intuitively I like the idea of denoising. Though not sure exact what is denoised and towards what? In particular for hopfield networks (and I think most of the body of work that this paper points to), the idea is you have a set of sequences that you want to *memorize*. So you build a point attractor for each of this sequence, such that when starting the dynamical system in the vicinity of the point attractor (in its basin of attraction) then you converge to it (remembering the wanted sequence). Going back to this work, what is this sequence of patterns that we want to remember? More explicitly, for SDRNN you do backprop to get the h you would want and that make that a target (second loss) of the attractor net. But I'm confused about timescale. If h is not stable for a longer time, do you really converge on the attractor net ? Do we have evidence of that? Is this even meaningful early on in training, it feels like it should hurt.\n\n3) Connecting to this, I would really love to see more analysis, going beyond measuring entropy. How do we now this is not just more capacity and the auxiliary loss just helps the optimization. Particularly since the problems are synthetic, not large scale more analysis should be possible.  How does this compare to training the simple RNN but with gaussian noise on h (to learn to be robust to it). Can we control for capacity between RNN and SDRNN? \n\n4) To that point there is this work (not citet as far as I can tell) : https://arxiv.org/abs/1312.6026. It does have a structure somewhat similar, though none of the denoising perspective or the auxiliary loss used in this work. However the work points out that if you make the network deep in a similar way to how it was done here even though technically it is a more powerful model, gradients do not propagate well. The solution was skip connection. In the baseline that was run you do not have skip connections, and the auxiliary loss might play the role of what skip connections or a more powerful optimizer would have played. ", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper857/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "State-Denoised Recurrent Neural Networks", "abstract": "Recurrent neural networks (RNNs) are difficult to train on sequence processing tasks, not only because input noise may be amplified through feedback, but also because any inaccuracy in the weights has similar consequences as input noise. We describe a method for denoising the hidden state during training to achieve more robust representations thereby improving generalization performance. Attractor dynamics are incorporated into the hidden state to `clean up' representations at each step of a sequence. The attractor dynamics are trained through an auxillary denoising loss to recover previously experienced hidden states from noisy versions of those states. This state-denoised recurrent neural network (SDRNN) performs multiple steps of internal processing for each external sequence step. On a range of tasks, we show that the SDRNN outperforms a generic RNN as well as a variant of the SDRNN with attractor dynamics on the hidden state but without the auxillary loss. We argue that attractor dynamics---and corresponding connectivity constraints---are an essential component of the deep learning arsenal and should be invoked not only for recurrent networks but also for improving deep feedforward nets and intertask transfer.", "keywords": ["recurrent nets", "attractor nets", "denoising", "sequence processing"], "authorids": ["mozer@colorado.edu", "denis.kazakov@colorado.edu", "rob@imagen.ai"], "authors": ["Michael C. Mozer", "Denis Kazakov", "Robert V. Lindsey"], "TL;DR": "We propose a mechanism for denoising the internal state of an RNN to improve generalization performance.", "pdf": "/pdf/e739554c0f3d38f53d82206aafb0f5335ecb5305.pdf", "paperhash": "mozer|statedenoised_recurrent_neural_networks", "_bibtex": "@misc{\nmozer2019statedenoised,\ntitle={State-Denoised Recurrent Neural Networks},\nauthor={Michael C. Mozer and Denis Kazakov and Robert V. Lindsey},\nyear={2019},\nurl={https://openreview.net/forum?id=HJgyAoRqFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper857/Official_Review", "cdate": 1542234361142, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HJgyAoRqFQ", "replyto": "HJgyAoRqFQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper857/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335818077, "tmdate": 1552335818077, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper857/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "S1gsJSsy2m", "original": null, "number": 1, "cdate": 1540498659331, "ddate": null, "tcdate": 1540498659331, "tmdate": 1541533631970, "tddate": null, "forum": "HJgyAoRqFQ", "replyto": "HJgyAoRqFQ", "invitation": "ICLR.cc/2019/Conference/-/Paper857/Official_Review", "content": {"title": "Review for State-Denoised Recurrent Neural Networks", "review": "In this paper the authors develop the clever idea to use attractor networks, inspired by Hopfield nets, to \u201cdenoise\u201d a recurrent neural network.  The idea is that for every normal step of an RNN, one induces an additional \"dimension\" of recurrency in order to create attractor dynamics around that particular hidden state. The authors introduce their idea and run some basic experiments. This paper is well written and the idea is novel (to me) and worthy of exploration.  Unfortunately, the experiments are seriously lacking in my opinion, as I believe *the major focus* of those experiments should be comparisons to other denoising / regularization techniques.\n\nMAJOR\n\nThe point is taken that RNNs are susceptible to noise due to iterated application of the function. In my experience, countering noise (in the sense of gaussian noise added) isn\u2019t a huge problem in practice because there are many regularization methodologies to handle it. This leads me to the point that I think the experiments need to compare across a number of regularization techniques.  The paper is motivated by discussion of noise, \u201cnoise robustness is a highly desirable property in neural networks\u201d, and the experiments show improved performance on smaller datasets, all of which speak to regularization. So I believe comparisons with regularization techniques are pretty important here. \n\nMODERATE\n\nThere is some motivation at the beginning of this piece, in particular about language, and does not contain citations, but should.\n\n\u201cTraining is in complete batches to avoid the noise of mini-batch training.\u201d  Please explain, I guess this is not a type of noise that the method handles? \n\nWhat about problems that require graded responses, which is likely anything requiring integration? For example,  what happens in the majority task if the inputs were switched to a non-discrete version, where one must hold analog numbers?\n\n\nMINOR\n\nAny discussion about the (presumably dramatic) increase in training time due to the attractor dynamics unrolling + additional batching due to noise vectors (if I understood correctly)?\n\nWhat are your confidence intervals over?  Presumably, we\u2019d like to get confidence over multiple network instantiations.\n\nPg 1. Articulated neural network? \n\n\nQUESTIONS\n\nDoes using a the \u2018c\u2019 variable as a bias instead of an initial condition really matter? \n\nHow does supervised training via eqn (4) relate to the classic training of Hopfield nets? I assume not at all, but it would be useful to clarify?\n\nWhat RNN architecture did you use in the Figure 5 simulations (tanh vanilla RNN or GRU?)\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper857/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "State-Denoised Recurrent Neural Networks", "abstract": "Recurrent neural networks (RNNs) are difficult to train on sequence processing tasks, not only because input noise may be amplified through feedback, but also because any inaccuracy in the weights has similar consequences as input noise. We describe a method for denoising the hidden state during training to achieve more robust representations thereby improving generalization performance. Attractor dynamics are incorporated into the hidden state to `clean up' representations at each step of a sequence. The attractor dynamics are trained through an auxillary denoising loss to recover previously experienced hidden states from noisy versions of those states. This state-denoised recurrent neural network (SDRNN) performs multiple steps of internal processing for each external sequence step. On a range of tasks, we show that the SDRNN outperforms a generic RNN as well as a variant of the SDRNN with attractor dynamics on the hidden state but without the auxillary loss. We argue that attractor dynamics---and corresponding connectivity constraints---are an essential component of the deep learning arsenal and should be invoked not only for recurrent networks but also for improving deep feedforward nets and intertask transfer.", "keywords": ["recurrent nets", "attractor nets", "denoising", "sequence processing"], "authorids": ["mozer@colorado.edu", "denis.kazakov@colorado.edu", "rob@imagen.ai"], "authors": ["Michael C. Mozer", "Denis Kazakov", "Robert V. Lindsey"], "TL;DR": "We propose a mechanism for denoising the internal state of an RNN to improve generalization performance.", "pdf": "/pdf/e739554c0f3d38f53d82206aafb0f5335ecb5305.pdf", "paperhash": "mozer|statedenoised_recurrent_neural_networks", "_bibtex": "@misc{\nmozer2019statedenoised,\ntitle={State-Denoised Recurrent Neural Networks},\nauthor={Michael C. Mozer and Denis Kazakov and Robert V. Lindsey},\nyear={2019},\nurl={https://openreview.net/forum?id=HJgyAoRqFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper857/Official_Review", "cdate": 1542234361142, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HJgyAoRqFQ", "replyto": "HJgyAoRqFQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper857/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335818077, "tmdate": 1552335818077, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper857/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 8}