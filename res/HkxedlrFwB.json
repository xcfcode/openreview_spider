{"notes": [{"id": "HkxedlrFwB", "original": "ByxMP3gFvB", "number": 2385, "cdate": 1569439848212, "ddate": null, "tcdate": 1569439848212, "tmdate": 1577168228449, "tddate": null, "forum": "HkxedlrFwB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["angetato@gmail.com", "nkambou@gmail.com"], "title": "Accelerating First-Order Optimization Algorithms", "authors": ["Ange Tato", "Roger Nkambou"], "pdf": "/pdf/9ec81028a2ea77d7cd04f9f201d7aa714e644909.pdf", "TL;DR": "Accelerating First-Order Optimization Algorithms", "abstract": "Several stochastic optimization algorithms are currently available. In most cases, selecting the best optimizer for a given problem is not an easy task. Therefore, instead of looking for yet another \u2019absolute\u2019 best optimizer, accelerating existing ones according to the context might prove more effective. This paper presents a simple and intuitive technique to accelerate first-order optimization algorithms. When applied to first-order optimization algorithms, it converges much more quickly and achieves lower function/loss values when compared to traditional algorithms. The proposed solution modifies the update rule, based on the variation of the direction of the gradient during training. Several tests were conducted with SGD, AdaGrad, Adam and AMSGrad on three public datasets. Results clearly show that the proposed technique, has the potential to improve the performance of existing optimization algorithms.", "code": "https://github.com/angetato/Custom-Optimizer-on-Keras", "keywords": ["Neural Networks", "Gradient Descent", "First order optimization"], "paperhash": "tato|accelerating_firstorder_optimization_algorithms", "original_pdf": "/attachment/9ec81028a2ea77d7cd04f9f201d7aa714e644909.pdf", "_bibtex": "@misc{\ntato2020accelerating,\ntitle={Accelerating First-Order Optimization Algorithms},\nauthor={Ange Tato and Roger Nkambou},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxedlrFwB}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "DFXmJ0Bk3S", "original": null, "number": 1, "cdate": 1576798747752, "ddate": null, "tcdate": 1576798747752, "tmdate": 1576800888316, "tddate": null, "forum": "HkxedlrFwB", "replyto": "HkxedlrFwB", "invitation": "ICLR.cc/2020/Conference/Paper2385/-/Decision", "content": {"decision": "Reject", "comment": "All reviewers recommend rejection, and the authors have not provided a response.\n", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["angetato@gmail.com", "nkambou@gmail.com"], "title": "Accelerating First-Order Optimization Algorithms", "authors": ["Ange Tato", "Roger Nkambou"], "pdf": "/pdf/9ec81028a2ea77d7cd04f9f201d7aa714e644909.pdf", "TL;DR": "Accelerating First-Order Optimization Algorithms", "abstract": "Several stochastic optimization algorithms are currently available. In most cases, selecting the best optimizer for a given problem is not an easy task. Therefore, instead of looking for yet another \u2019absolute\u2019 best optimizer, accelerating existing ones according to the context might prove more effective. This paper presents a simple and intuitive technique to accelerate first-order optimization algorithms. When applied to first-order optimization algorithms, it converges much more quickly and achieves lower function/loss values when compared to traditional algorithms. The proposed solution modifies the update rule, based on the variation of the direction of the gradient during training. Several tests were conducted with SGD, AdaGrad, Adam and AMSGrad on three public datasets. Results clearly show that the proposed technique, has the potential to improve the performance of existing optimization algorithms.", "code": "https://github.com/angetato/Custom-Optimizer-on-Keras", "keywords": ["Neural Networks", "Gradient Descent", "First order optimization"], "paperhash": "tato|accelerating_firstorder_optimization_algorithms", "original_pdf": "/attachment/9ec81028a2ea77d7cd04f9f201d7aa714e644909.pdf", "_bibtex": "@misc{\ntato2020accelerating,\ntitle={Accelerating First-Order Optimization Algorithms},\nauthor={Ange Tato and Roger Nkambou},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxedlrFwB}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "HkxedlrFwB", "replyto": "HkxedlrFwB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795724905, "tmdate": 1576800276631, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2385/-/Decision"}}}, {"id": "Hkgw-7intr", "original": null, "number": 1, "cdate": 1571758846865, "ddate": null, "tcdate": 1571758846865, "tmdate": 1572972345074, "tddate": null, "forum": "HkxedlrFwB", "replyto": "HkxedlrFwB", "invitation": "ICLR.cc/2020/Conference/Paper2385/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper describes a technique to speed up optimizers that rely on gradient\ninformation to find the optimum value of a function. The authors describe and\njustify their method and show its promise in an empirical evaluation.\n\nThe proposed method sounds interesting and promising, but the empirical\nevaluation is unclear. In particular, details are missing on the exact\nexperimental setup and some of the presented results are unconvincing. I refer\nto the results of the basic function optimization (Figure 1), which shows that\nseveral of the considered optimizers are unable to even get close to the optimum\nof x^2 after several hundred iterations. It seems that this is extremely easy\nfunction to optimize -- why are the considered optimizers performing so poorly\non it? How were the hyperparameters of the optimizers set? This presumably\naffects the other results presented in the paper as well, and puts the\nimprovement of the proposed method in question.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2385/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2385/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["angetato@gmail.com", "nkambou@gmail.com"], "title": "Accelerating First-Order Optimization Algorithms", "authors": ["Ange Tato", "Roger Nkambou"], "pdf": "/pdf/9ec81028a2ea77d7cd04f9f201d7aa714e644909.pdf", "TL;DR": "Accelerating First-Order Optimization Algorithms", "abstract": "Several stochastic optimization algorithms are currently available. In most cases, selecting the best optimizer for a given problem is not an easy task. Therefore, instead of looking for yet another \u2019absolute\u2019 best optimizer, accelerating existing ones according to the context might prove more effective. This paper presents a simple and intuitive technique to accelerate first-order optimization algorithms. When applied to first-order optimization algorithms, it converges much more quickly and achieves lower function/loss values when compared to traditional algorithms. The proposed solution modifies the update rule, based on the variation of the direction of the gradient during training. Several tests were conducted with SGD, AdaGrad, Adam and AMSGrad on three public datasets. Results clearly show that the proposed technique, has the potential to improve the performance of existing optimization algorithms.", "code": "https://github.com/angetato/Custom-Optimizer-on-Keras", "keywords": ["Neural Networks", "Gradient Descent", "First order optimization"], "paperhash": "tato|accelerating_firstorder_optimization_algorithms", "original_pdf": "/attachment/9ec81028a2ea77d7cd04f9f201d7aa714e644909.pdf", "_bibtex": "@misc{\ntato2020accelerating,\ntitle={Accelerating First-Order Optimization Algorithms},\nauthor={Ange Tato and Roger Nkambou},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxedlrFwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HkxedlrFwB", "replyto": "HkxedlrFwB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2385/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2385/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575752342487, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2385/Reviewers"], "noninvitees": [], "tcdate": 1570237723574, "tmdate": 1575752342500, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2385/-/Official_Review"}}}, {"id": "HklyD34pKS", "original": null, "number": 2, "cdate": 1571798102730, "ddate": null, "tcdate": 1571798102730, "tmdate": 1572972345029, "tddate": null, "forum": "HkxedlrFwB", "replyto": "HkxedlrFwB", "invitation": "ICLR.cc/2020/Conference/Paper2385/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The authors present an acceleration technique for first-order optimization algorithms by comparing the directions of gradients in consecutive steps, which works for SGD, Adam, and AMSGrad.  Empirically it seems to work well with some standard evaluations with CNN for MNIST and CIFAR10 and LSTM for IMDB, beating the non-accelerated versions in convergence speed. However there are some issues with the parameter choice and proofs. Below are my specific comments: \n\n1. Setting the parameter S seems to be difficult and problem-dependent. S controls the size of the region near the optimum where the algorithm falls back to the non-accelerated version. But S depends on the size of the gradient, which is problem-dependent. If we need to tune S for the algorithm to work well on a particular dataset, then it defeats the purpose of acceleration in the first place. \n\n2. The setting of S also depends on batch size if mini-batch stochastic gradient algorithms are used. In the update rules S is compared against |g_t-1 - g_t|, and this quantity is directly related to the variance of gradients, which in term depends on the batch size. This makes it even more difficult to set a priori. \n\n3. What is k in Theorem 2? In the line above Theorem 2, why is it the case that the gradient at x_T-1 is k times the gradient at x_T? Also, if we compare equations 2 and 3, the regret bound for the `accelerated' version is k times worse than the original non-accelerated SGD. How could this happen? \n\n4. In the proofs in the Appendix I see no mention of the parameter S, which is very strange since it is part of the update condition. The size of S affects the convergence, as shown in Figure 4. It is odd to have a regret bound in Theorem 3 that is completely independent of S. \n\nUnless the authors can address these issues I don't think the current paper is suitable for publication yet. \n\n\n\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2385/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2385/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["angetato@gmail.com", "nkambou@gmail.com"], "title": "Accelerating First-Order Optimization Algorithms", "authors": ["Ange Tato", "Roger Nkambou"], "pdf": "/pdf/9ec81028a2ea77d7cd04f9f201d7aa714e644909.pdf", "TL;DR": "Accelerating First-Order Optimization Algorithms", "abstract": "Several stochastic optimization algorithms are currently available. In most cases, selecting the best optimizer for a given problem is not an easy task. Therefore, instead of looking for yet another \u2019absolute\u2019 best optimizer, accelerating existing ones according to the context might prove more effective. This paper presents a simple and intuitive technique to accelerate first-order optimization algorithms. When applied to first-order optimization algorithms, it converges much more quickly and achieves lower function/loss values when compared to traditional algorithms. The proposed solution modifies the update rule, based on the variation of the direction of the gradient during training. Several tests were conducted with SGD, AdaGrad, Adam and AMSGrad on three public datasets. Results clearly show that the proposed technique, has the potential to improve the performance of existing optimization algorithms.", "code": "https://github.com/angetato/Custom-Optimizer-on-Keras", "keywords": ["Neural Networks", "Gradient Descent", "First order optimization"], "paperhash": "tato|accelerating_firstorder_optimization_algorithms", "original_pdf": "/attachment/9ec81028a2ea77d7cd04f9f201d7aa714e644909.pdf", "_bibtex": "@misc{\ntato2020accelerating,\ntitle={Accelerating First-Order Optimization Algorithms},\nauthor={Ange Tato and Roger Nkambou},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxedlrFwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HkxedlrFwB", "replyto": "HkxedlrFwB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2385/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2385/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575752342487, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2385/Reviewers"], "noninvitees": [], "tcdate": 1570237723574, "tmdate": 1575752342500, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2385/-/Official_Review"}}}, {"id": "H1eOoyPTFr", "original": null, "number": 3, "cdate": 1571807135892, "ddate": null, "tcdate": 1571807135892, "tmdate": 1572972344983, "tddate": null, "forum": "HkxedlrFwB", "replyto": "HkxedlrFwB", "invitation": "ICLR.cc/2020/Conference/Paper2385/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary:\n\nThis paper presents an adaptive method which can be used alongside existing accelerated gradient methods. The paper is difficult to read due to mistakes and poorly defined mathematical notation. I believe that the paper is missing reference to related methods. The theoretical analysis in the paper is difficult to follow and provides little insight into the benefits of the proposed approach.\n\nOverview:\n\nThere are many mistakes throughout the paper which have made it difficult to read.\n\nOverall, I felt that this paper was missing a discussion of the effect of stochasticity on the proposed method. The issue with measuring the variation in the gradient direction is that in regimes where the gradient noise is dominating the signal the gradient direction at each time step is poorly correlated with overall optimization progress --- thus it seems intuitively ineffective to rely on the gradient direction to adjust the algorithm.\n\n1)  At the bottom of page 2, the authors write \"knowing that we do not have any knowledge of what this function looks like\". While minor, I would point out that we are able to compute local statistics of the function and so certainly we have _some_ knowledge.\n\n2) The authors claim that no techniques exist which use the variation of the direction of the gradient. One such example is in [1] which uses (in one case) the variation of the gradient direction to determine and appropriate time to restart the momentum computation. \n\n3) In section 3.2, the Adam moment computation is missing a \"diag\". Assuming that AMRSGrad is AMSGrad (mistyped), then this term is incorrect and matches Adam.\n\n4) There are many mistakes in the Algorithm 1 box.\n\n- The wrong $F$ is used in the input (should be $\\mathcal{F}$).\n- The algorithm takes as input a sequence of functions ($\\phi, \\psi$) which are not used.\n- Within the if statement, $gm_t = g_t + m_t$. I believe this should be an $m_{t-1}$. It is not clear what the vector $gm$ is exactly, and then $\\dot{g}$ is used afterwards which is also not defined.\n- The algorithm checks for $|m_{t-1} - g_t| > S$ while the text uses $|g_{t-1} - g_t| > S$.\n\n5) The first line of section 3.3 is quite worrying: \"We assume that if we are able to prove that modifying one optimizer with the proposed method does not alter its convergence, then the same applies for the other optimizers\". This seems like a dangerous assumption to make and should at the very least be carefully verified empirically. Following this, I am not sure what the authors mean by \"deterministic\" and \"non-deterministic\" methods.\n\n6) I do not understand the claim above Theorem 2 that $\\nabla f(x_{T-1}) = k \\nabla f(x_T)$. Under what conditions does this hold and how is $k$ computed? If I understand correctly, the bound provided in Theorem 2 is worse than that given for gradient descent. Moreover, the bound does not depend on the hyperparameter $S$ introduced in Algorithm 1 and provides limited insights into the method. I could not find a proof of Theorem 2 in the paper or appendix.\n\n7) There are serious flaws with the experimental evaluation in this paper.\n\na) There is no tuning over hyperparameter settings for any of the optimizers.\n\nb) The basic problems are very limited, even for toy problems. The 1D deterministic quadratic tells us very little about the performance of the optimizer. And the 1D cubic problem is particularly confusing. Unless I am mistaken, the gradient will always have the same sign (3x^2) and thus the acceleration condition will never be triggered.\n\nc) I believe that Figure 2 explores stochastic optimization problems which as discussed at top is a crucial evaluation. Unfortunately, due to lack of parameter tuning it is difficult to infer much about the comparison between the methods.\n\nd) Figure 4 compares performance variation over changing the threshold. The y-axis scale across each plot changes making the comparison unnecessarily difficult --- the scale should be the same.\n\nMinor:\n\n- TYPO Line 2, \"minimize ---,\"\n- End of intro, MNIST and CIFAR not cited while IMDB is. Citation uses citet not citep.\n- Bottom of page 2, \"\"\n- Top of section 3.2, \"The pseudo code of our the method\"\n\nReferences:\n\n[1] Adaptive Restart for Accelerated Gradient Schemes, Brendan O'Donoghue and Emmanuel Candes\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2385/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2385/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["angetato@gmail.com", "nkambou@gmail.com"], "title": "Accelerating First-Order Optimization Algorithms", "authors": ["Ange Tato", "Roger Nkambou"], "pdf": "/pdf/9ec81028a2ea77d7cd04f9f201d7aa714e644909.pdf", "TL;DR": "Accelerating First-Order Optimization Algorithms", "abstract": "Several stochastic optimization algorithms are currently available. In most cases, selecting the best optimizer for a given problem is not an easy task. Therefore, instead of looking for yet another \u2019absolute\u2019 best optimizer, accelerating existing ones according to the context might prove more effective. This paper presents a simple and intuitive technique to accelerate first-order optimization algorithms. When applied to first-order optimization algorithms, it converges much more quickly and achieves lower function/loss values when compared to traditional algorithms. The proposed solution modifies the update rule, based on the variation of the direction of the gradient during training. Several tests were conducted with SGD, AdaGrad, Adam and AMSGrad on three public datasets. Results clearly show that the proposed technique, has the potential to improve the performance of existing optimization algorithms.", "code": "https://github.com/angetato/Custom-Optimizer-on-Keras", "keywords": ["Neural Networks", "Gradient Descent", "First order optimization"], "paperhash": "tato|accelerating_firstorder_optimization_algorithms", "original_pdf": "/attachment/9ec81028a2ea77d7cd04f9f201d7aa714e644909.pdf", "_bibtex": "@misc{\ntato2020accelerating,\ntitle={Accelerating First-Order Optimization Algorithms},\nauthor={Ange Tato and Roger Nkambou},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxedlrFwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HkxedlrFwB", "replyto": "HkxedlrFwB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2385/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2385/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575752342487, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2385/Reviewers"], "noninvitees": [], "tcdate": 1570237723574, "tmdate": 1575752342500, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2385/-/Official_Review"}}}], "count": 5}