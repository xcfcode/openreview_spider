{"notes": [{"id": "ZW0yXJyNmoG", "original": "iXc6xNYABp-", "number": 1684, "cdate": 1601308186413, "ddate": null, "tcdate": 1601308186413, "tmdate": 1613306822369, "tddate": null, "forum": "ZW0yXJyNmoG", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Taming GANs with Lookahead-Minmax", "authorids": ["~Tatjana_Chavdarova2", "~Matteo_Pagliardini1", "~Sebastian_U_Stich1", "~Fran\u00e7ois_Fleuret2", "~Martin_Jaggi1"], "authors": ["Tatjana Chavdarova", "Matteo Pagliardini", "Sebastian U Stich", "Fran\u00e7ois Fleuret", "Martin Jaggi"], "keywords": ["Minmax", "Generative Adversarial Networks"], "abstract": "Generative Adversarial Networks are notoriously challenging to train. The underlying minmax optimization is highly susceptible to the variance of the stochastic gradient and the rotational component of the associated game vector field. To tackle these challenges, we propose the Lookahead algorithm for minmax optimization, originally developed for single objective minimization only. The backtracking step of our Lookahead\u2013minmax naturally handles the rotational game dynamics, a property which was identified to be key for enabling gradient ascent descent methods to converge on challenging examples often analyzed in the literature. Moreover, it implicitly handles high variance without using large mini-batches, known to be essential for reaching state of the art performance. Experimental results on MNIST, SVHN, CIFAR-10, and ImageNet demonstrate a clear advantage of combining Lookahead\u2013minmax with Adam or extragradient, in terms of performance and improved stability, for negligible memory and computational cost. Using 30-fold fewer parameters and 16-fold smaller minibatches we outperform the reported performance of the class-dependent BigGAN on CIFAR-10 by obtaining FID of 12.19 without using the class labels, bringing state-of-the-art GAN training within reach of common computational resources.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chavdarova|taming_gans_with_lookaheadminmax", "one-sentence_summary": "A novel optimizer for GANs and games in general. ", "supplementary_material": "/attachment/574497e5b2553a4afc26c607586d795bd1ae28cd.zip", "pdf": "/pdf/96012e3aefba2bb5211c854f40b60a16024e2ada.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nchavdarova2021taming,\ntitle={Taming {\\{}GAN{\\}}s with Lookahead-Minmax},\nauthor={Tatjana Chavdarova and Matteo Pagliardini and Sebastian U Stich and Fran{\\c{c}}ois Fleuret and Martin Jaggi},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ZW0yXJyNmoG}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 13, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "X-YZ24g5mnI", "original": null, "number": 1, "cdate": 1610040354392, "ddate": null, "tcdate": 1610040354392, "tmdate": 1610473943785, "tddate": null, "forum": "ZW0yXJyNmoG", "replyto": "ZW0yXJyNmoG", "invitation": "ICLR.cc/2021/Conference/Paper1684/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Poster)", "comment": "The reviews were a bit mixed, with a general consensus towards acceptance. The authors were one of the first to extend lookahead to minimax optimization, and demonstrated its potential through thorough experiments. The theoretical results were not as strong or at least not very well presented. Overall, the authors made interesting contributions and this work is of general interest to the ICLR audience. Please consider further polishing the draft according to the reviewers' comments. The AC would also like to draw the authors' attention to the following issues discovered in an independent assessment:\n\n(a) As the reviewers mentioned, how lookahead-minimax addresses rotational dynamics is not clearly presented. The current justification is a bit handwaving and speculative. \n\n(b) Please consider rewriting Section 3. If there is some new results on the minimization problem, state the results in a theorem and include all assumptions clearly and precisely. This is also useful for other people to reference your result. As the authors themselves pointed out, this result falls quite short of explaining or motivation lookahead. \n\n(c) Theorem 1, add e.g. in the citation before (Bertsekas, 1999). Theorem 2, in its current form, is quite weak in two aspects: (a) without checking its proof one can already see how to derive it in 1 line or 2. (b) if the base optimizer already converges, what is the point of having lookahead to converge as well? The potentially different convergence rate should be one's target here. It is certainly fine for the authors to not fully justify their proposed algorithm, as long as the authors (hopefully) are at least aware of the issues.\n\n(d) Section 4 is a bit disappointing as one would have expected the authors to derive some qualitative results here (also raised by some reviewers)."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Taming GANs with Lookahead-Minmax", "authorids": ["~Tatjana_Chavdarova2", "~Matteo_Pagliardini1", "~Sebastian_U_Stich1", "~Fran\u00e7ois_Fleuret2", "~Martin_Jaggi1"], "authors": ["Tatjana Chavdarova", "Matteo Pagliardini", "Sebastian U Stich", "Fran\u00e7ois Fleuret", "Martin Jaggi"], "keywords": ["Minmax", "Generative Adversarial Networks"], "abstract": "Generative Adversarial Networks are notoriously challenging to train. The underlying minmax optimization is highly susceptible to the variance of the stochastic gradient and the rotational component of the associated game vector field. To tackle these challenges, we propose the Lookahead algorithm for minmax optimization, originally developed for single objective minimization only. The backtracking step of our Lookahead\u2013minmax naturally handles the rotational game dynamics, a property which was identified to be key for enabling gradient ascent descent methods to converge on challenging examples often analyzed in the literature. Moreover, it implicitly handles high variance without using large mini-batches, known to be essential for reaching state of the art performance. Experimental results on MNIST, SVHN, CIFAR-10, and ImageNet demonstrate a clear advantage of combining Lookahead\u2013minmax with Adam or extragradient, in terms of performance and improved stability, for negligible memory and computational cost. Using 30-fold fewer parameters and 16-fold smaller minibatches we outperform the reported performance of the class-dependent BigGAN on CIFAR-10 by obtaining FID of 12.19 without using the class labels, bringing state-of-the-art GAN training within reach of common computational resources.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chavdarova|taming_gans_with_lookaheadminmax", "one-sentence_summary": "A novel optimizer for GANs and games in general. ", "supplementary_material": "/attachment/574497e5b2553a4afc26c607586d795bd1ae28cd.zip", "pdf": "/pdf/96012e3aefba2bb5211c854f40b60a16024e2ada.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nchavdarova2021taming,\ntitle={Taming {\\{}GAN{\\}}s with Lookahead-Minmax},\nauthor={Tatjana Chavdarova and Matteo Pagliardini and Sebastian U Stich and Fran{\\c{c}}ois Fleuret and Martin Jaggi},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ZW0yXJyNmoG}\n}"}, "tags": [], "invitation": {"reply": {"forum": "ZW0yXJyNmoG", "replyto": "ZW0yXJyNmoG", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040354376, "tmdate": 1610473943766, "id": "ICLR.cc/2021/Conference/Paper1684/-/Decision"}}}, {"id": "TjBFmBLybZ_", "original": null, "number": 1, "cdate": 1603164000051, "ddate": null, "tcdate": 1603164000051, "tmdate": 1606324779854, "tddate": null, "forum": "ZW0yXJyNmoG", "replyto": "ZW0yXJyNmoG", "invitation": "ICLR.cc/2021/Conference/Paper1684/-/Official_Review", "content": {"title": "Strong empirical results on standard benchmarks, but no theoretical result provided. ", "review": "Summary: This work extends the recently proposed lookahead optimizer (which was designed for single-objective optimization) to minimax optimization, particularly GAN training. The authors claim that the backtracking step in lookahead optimizer alleviates the notorious rotational behavior in GAN dynamics. Moreover, the authors argue that the lookahead optimizer implicitly handles the high variance in the small-batch setting. Both arguments are backed up by toy experiments on stochastic bilinear games.  Finally, on standard image datasets, the lookahead minimax algorithm outperforms some popular algorithms and achieves state-of-the-art performance on CIFAR-10.\n\nReview: This paper is well-written and explains the main idea in a clear and effective manner. The empirical results of the introduced lookahead minimax algorithm are quite impressive given its simplicity. However, the lookahead minimax dynamics is not well-understood and no theoretical justification is provided in the paper. The bilinear game might not reflect the real GAN training as standard GDA never diverges in real GAN training. I tend to give a score of 7 or even higher, but I'm not satisfied with the current explanation of the performance gain. I think a better understanding of the dynamics is needed and I will increase my rating if the authors could improve the paper accordingly.\n\nHere are detailed comments:\n\n1. In the third paragraph of the introduction, the authors mentioned that empirically GANs often converge to a locally stable stationary point that is not a differential Nash equilibrium. Actually, this phenomenon occurs even for the simplest quadratic minimax games, see for example [1, 2, 3]. Moreover, Nash equilibrium might not even exist in general, Stackelberg equilibrium [2, 4] is probably a better solution notion.\n\n2. The authors provide some theoretical results of the lookahead optimizer, but for single-objective optimization. I suggest the authors moving this section to the Appendix (as this section seems irrelevant) and replace it with the analysis of lookahead for minimax games. In particular, the authors argue many times in the paper that the lookahead algorithm is able to handle the rotational dynamics well, I think the authors should be able to verify that theoretically on quadratic minimax games if the argument is right.\n\n3. The bilinear game might not reflect the real training dynamics of GANs. In bilinear games, simultaneous GDA with finite step sizes simply diverges, but it is not the case in practice for GAN training. To this end, strongly-convex strongly-concave minimax games might be better to model the underlying GAN dynamics. Strongly-convex strongly-concave quadratic minimax games shall be as simple as the bilinear cases and some theoretical analyses and simulation should be easy to do. \n\n4. In the paper, the authors chose Extragradient as one of their baselines. However, after inspecting the code, I find much fewer iterations are used for Extragradient. I understand the authors would like to keep the total gradient queries the same over different algorithms, but there is a similar algorithm (Optimistic gradient descent ascent [5, 6]) with only one gradient query per step and perform similarly with Extragradient. In this sense, I think it is fair to only use fewer iterations for Extragradient. \n\n5. For stochastic games (especially for SB-G), there is a better version of Extragradient which handles the variance well, see [7] for details.\n\n6. In the original lookahead paper, it was mentioned that the slow weights can be understood as the exponential moving average (EMA) of the fast weights. In particular, the authors of the original lookahead paper did some analysis on a noisy quadratic model and showed lookahead reduces variance. In that simple noisy quadratic model, exponential moving average can effectively reduce variance, see section 3.4 of [8] for details. I think it is probably worth testing EMA on the toy experiments and mentioning it in the paper. I notice that the authors have already included EMA in section 5 and EMA improves the performance a lot. To this end, I think the advantages of lookahead can be potentially decomposed into two parts (though more works need to be done): (1) it suppresses the rotational part in game dynamics (I encourage the authors to analyze the spectrum of the dynamics); (2) it implicitly does exponential moving average and hence reduces the variance.\n\nOverall, I think this paper is very strong on the empirical results. However, analysis on the dynamics of the lookahead algorithm is lacking. I will be happy to increase my rating if the authors could resolve some of my comments.\n\n--------------------\n**In the rebuttal, the authors resolved most of my concerns, therefore I increased my score to 7 as promised.**\n\n\nReference:\n[1] On Finding Local Nash Equilibria (and Only Local Nash Equilibria) in Zero-Sum Games, 2019.\n\n[2] On Solving Minimax Optimization Locally: A Follow-the-Ridge Approach, 2020.\n\n[3] GANs May Have No Nash Equilibria, 2020.\n\n[4] Convergence of learning dynamics in Stackelberg games, 2019.\n\n[5] Training GANs with Optimism, 2018.\n\n[6] A Unified Analysis of Extra-gradient and Optimistic Gradient Methods for Saddle Point Problems: Proximal Point Approach, 2019.\n\n[7] Revisiting Stochastic Extragradient, 2019.\n\n[8] Which Algorithmic Choices Matter at Which Batch Sizes? Insights From a Noisy Quadratic Model, 2019.", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper1684/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1684/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Taming GANs with Lookahead-Minmax", "authorids": ["~Tatjana_Chavdarova2", "~Matteo_Pagliardini1", "~Sebastian_U_Stich1", "~Fran\u00e7ois_Fleuret2", "~Martin_Jaggi1"], "authors": ["Tatjana Chavdarova", "Matteo Pagliardini", "Sebastian U Stich", "Fran\u00e7ois Fleuret", "Martin Jaggi"], "keywords": ["Minmax", "Generative Adversarial Networks"], "abstract": "Generative Adversarial Networks are notoriously challenging to train. The underlying minmax optimization is highly susceptible to the variance of the stochastic gradient and the rotational component of the associated game vector field. To tackle these challenges, we propose the Lookahead algorithm for minmax optimization, originally developed for single objective minimization only. The backtracking step of our Lookahead\u2013minmax naturally handles the rotational game dynamics, a property which was identified to be key for enabling gradient ascent descent methods to converge on challenging examples often analyzed in the literature. Moreover, it implicitly handles high variance without using large mini-batches, known to be essential for reaching state of the art performance. Experimental results on MNIST, SVHN, CIFAR-10, and ImageNet demonstrate a clear advantage of combining Lookahead\u2013minmax with Adam or extragradient, in terms of performance and improved stability, for negligible memory and computational cost. Using 30-fold fewer parameters and 16-fold smaller minibatches we outperform the reported performance of the class-dependent BigGAN on CIFAR-10 by obtaining FID of 12.19 without using the class labels, bringing state-of-the-art GAN training within reach of common computational resources.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chavdarova|taming_gans_with_lookaheadminmax", "one-sentence_summary": "A novel optimizer for GANs and games in general. ", "supplementary_material": "/attachment/574497e5b2553a4afc26c607586d795bd1ae28cd.zip", "pdf": "/pdf/96012e3aefba2bb5211c854f40b60a16024e2ada.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nchavdarova2021taming,\ntitle={Taming {\\{}GAN{\\}}s with Lookahead-Minmax},\nauthor={Tatjana Chavdarova and Matteo Pagliardini and Sebastian U Stich and Fran{\\c{c}}ois Fleuret and Martin Jaggi},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ZW0yXJyNmoG}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "ZW0yXJyNmoG", "replyto": "ZW0yXJyNmoG", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1684/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538113080, "tmdate": 1606915759622, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1684/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1684/-/Official_Review"}}}, {"id": "xORLdQvrRPG", "original": null, "number": 9, "cdate": 1606294310722, "ddate": null, "tcdate": 1606294310722, "tmdate": 1606294310722, "tddate": null, "forum": "ZW0yXJyNmoG", "replyto": "FUcS_kI2BXa", "invitation": "ICLR.cc/2021/Conference/Paper1684/-/Official_Comment", "content": {"title": "Update on detailed comment 5", "comment": "We\u2019d like to note that [1] mentions the SB-G problem in the motivation, but does not compare the one-sample-EG on this problem empirically (the empirical evaluation contains a different bilinear game). We tested it on SB-G and noticed that taking the same sample for the extrapolation for the experiment in Fig.4: (i) does not make the diverging methods in Fig.4 converge, and (ii) for minibatch size 64, it speeds up slightly the otherwise converging Extra-Adam,  and it did not speed up Extra-Gradient.\n\n[1] Revisiting Stochastic Extragradient, 2019.\n\n[2] Reducing Noise in GAN Training with Variance Reduced Extragradient, 2019\n\nPlease let us know if you have further comments, thanks!"}, "signatures": ["ICLR.cc/2021/Conference/Paper1684/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1684/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Taming GANs with Lookahead-Minmax", "authorids": ["~Tatjana_Chavdarova2", "~Matteo_Pagliardini1", "~Sebastian_U_Stich1", "~Fran\u00e7ois_Fleuret2", "~Martin_Jaggi1"], "authors": ["Tatjana Chavdarova", "Matteo Pagliardini", "Sebastian U Stich", "Fran\u00e7ois Fleuret", "Martin Jaggi"], "keywords": ["Minmax", "Generative Adversarial Networks"], "abstract": "Generative Adversarial Networks are notoriously challenging to train. The underlying minmax optimization is highly susceptible to the variance of the stochastic gradient and the rotational component of the associated game vector field. To tackle these challenges, we propose the Lookahead algorithm for minmax optimization, originally developed for single objective minimization only. The backtracking step of our Lookahead\u2013minmax naturally handles the rotational game dynamics, a property which was identified to be key for enabling gradient ascent descent methods to converge on challenging examples often analyzed in the literature. Moreover, it implicitly handles high variance without using large mini-batches, known to be essential for reaching state of the art performance. Experimental results on MNIST, SVHN, CIFAR-10, and ImageNet demonstrate a clear advantage of combining Lookahead\u2013minmax with Adam or extragradient, in terms of performance and improved stability, for negligible memory and computational cost. Using 30-fold fewer parameters and 16-fold smaller minibatches we outperform the reported performance of the class-dependent BigGAN on CIFAR-10 by obtaining FID of 12.19 without using the class labels, bringing state-of-the-art GAN training within reach of common computational resources.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chavdarova|taming_gans_with_lookaheadminmax", "one-sentence_summary": "A novel optimizer for GANs and games in general. ", "supplementary_material": "/attachment/574497e5b2553a4afc26c607586d795bd1ae28cd.zip", "pdf": "/pdf/96012e3aefba2bb5211c854f40b60a16024e2ada.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nchavdarova2021taming,\ntitle={Taming {\\{}GAN{\\}}s with Lookahead-Minmax},\nauthor={Tatjana Chavdarova and Matteo Pagliardini and Sebastian U Stich and Fran{\\c{c}}ois Fleuret and Martin Jaggi},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ZW0yXJyNmoG}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ZW0yXJyNmoG", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1684/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1684/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1684/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1684/Authors|ICLR.cc/2021/Conference/Paper1684/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1684/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923856915, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1684/-/Official_Comment"}}}, {"id": "ZnTJtCj8Law", "original": null, "number": 8, "cdate": 1606247767911, "ddate": null, "tcdate": 1606247767911, "tmdate": 1606247767911, "tddate": null, "forum": "ZW0yXJyNmoG", "replyto": "OYX3jUxF4C1", "invitation": "ICLR.cc/2021/Conference/Paper1684/-/Official_Comment", "content": {"title": "Our source code is provided", "comment": "Thanks a lot, the code is provided [here](https://anonymous.4open.science/r/e062872f-2ff7-4aa8-9c74-59c215098577/) (anonymously for the moment)."}, "signatures": ["ICLR.cc/2021/Conference/Paper1684/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1684/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Taming GANs with Lookahead-Minmax", "authorids": ["~Tatjana_Chavdarova2", "~Matteo_Pagliardini1", "~Sebastian_U_Stich1", "~Fran\u00e7ois_Fleuret2", "~Martin_Jaggi1"], "authors": ["Tatjana Chavdarova", "Matteo Pagliardini", "Sebastian U Stich", "Fran\u00e7ois Fleuret", "Martin Jaggi"], "keywords": ["Minmax", "Generative Adversarial Networks"], "abstract": "Generative Adversarial Networks are notoriously challenging to train. The underlying minmax optimization is highly susceptible to the variance of the stochastic gradient and the rotational component of the associated game vector field. To tackle these challenges, we propose the Lookahead algorithm for minmax optimization, originally developed for single objective minimization only. The backtracking step of our Lookahead\u2013minmax naturally handles the rotational game dynamics, a property which was identified to be key for enabling gradient ascent descent methods to converge on challenging examples often analyzed in the literature. Moreover, it implicitly handles high variance without using large mini-batches, known to be essential for reaching state of the art performance. Experimental results on MNIST, SVHN, CIFAR-10, and ImageNet demonstrate a clear advantage of combining Lookahead\u2013minmax with Adam or extragradient, in terms of performance and improved stability, for negligible memory and computational cost. Using 30-fold fewer parameters and 16-fold smaller minibatches we outperform the reported performance of the class-dependent BigGAN on CIFAR-10 by obtaining FID of 12.19 without using the class labels, bringing state-of-the-art GAN training within reach of common computational resources.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chavdarova|taming_gans_with_lookaheadminmax", "one-sentence_summary": "A novel optimizer for GANs and games in general. ", "supplementary_material": "/attachment/574497e5b2553a4afc26c607586d795bd1ae28cd.zip", "pdf": "/pdf/96012e3aefba2bb5211c854f40b60a16024e2ada.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nchavdarova2021taming,\ntitle={Taming {\\{}GAN{\\}}s with Lookahead-Minmax},\nauthor={Tatjana Chavdarova and Matteo Pagliardini and Sebastian U Stich and Fran{\\c{c}}ois Fleuret and Martin Jaggi},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ZW0yXJyNmoG}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ZW0yXJyNmoG", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1684/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1684/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1684/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1684/Authors|ICLR.cc/2021/Conference/Paper1684/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1684/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923856915, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1684/-/Official_Comment"}}}, {"id": "OYX3jUxF4C1", "original": null, "number": 7, "cdate": 1605953139834, "ddate": null, "tcdate": 1605953139834, "tmdate": 1605953139834, "tddate": null, "forum": "ZW0yXJyNmoG", "replyto": "zjkQPHyZIhB", "invitation": "ICLR.cc/2021/Conference/Paper1684/-/Official_Comment", "content": {"title": "A decent contribution", "comment": "Thanks for the authors for updating the paper, I find the results on GAN  more convincing now. I am happy to raise my score to Accept. Given the instability of training, if the main results could be made reproducible, that would be better. "}, "signatures": ["ICLR.cc/2021/Conference/Paper1684/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1684/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Taming GANs with Lookahead-Minmax", "authorids": ["~Tatjana_Chavdarova2", "~Matteo_Pagliardini1", "~Sebastian_U_Stich1", "~Fran\u00e7ois_Fleuret2", "~Martin_Jaggi1"], "authors": ["Tatjana Chavdarova", "Matteo Pagliardini", "Sebastian U Stich", "Fran\u00e7ois Fleuret", "Martin Jaggi"], "keywords": ["Minmax", "Generative Adversarial Networks"], "abstract": "Generative Adversarial Networks are notoriously challenging to train. The underlying minmax optimization is highly susceptible to the variance of the stochastic gradient and the rotational component of the associated game vector field. To tackle these challenges, we propose the Lookahead algorithm for minmax optimization, originally developed for single objective minimization only. The backtracking step of our Lookahead\u2013minmax naturally handles the rotational game dynamics, a property which was identified to be key for enabling gradient ascent descent methods to converge on challenging examples often analyzed in the literature. Moreover, it implicitly handles high variance without using large mini-batches, known to be essential for reaching state of the art performance. Experimental results on MNIST, SVHN, CIFAR-10, and ImageNet demonstrate a clear advantage of combining Lookahead\u2013minmax with Adam or extragradient, in terms of performance and improved stability, for negligible memory and computational cost. Using 30-fold fewer parameters and 16-fold smaller minibatches we outperform the reported performance of the class-dependent BigGAN on CIFAR-10 by obtaining FID of 12.19 without using the class labels, bringing state-of-the-art GAN training within reach of common computational resources.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chavdarova|taming_gans_with_lookaheadminmax", "one-sentence_summary": "A novel optimizer for GANs and games in general. ", "supplementary_material": "/attachment/574497e5b2553a4afc26c607586d795bd1ae28cd.zip", "pdf": "/pdf/96012e3aefba2bb5211c854f40b60a16024e2ada.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nchavdarova2021taming,\ntitle={Taming {\\{}GAN{\\}}s with Lookahead-Minmax},\nauthor={Tatjana Chavdarova and Matteo Pagliardini and Sebastian U Stich and Fran{\\c{c}}ois Fleuret and Martin Jaggi},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ZW0yXJyNmoG}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ZW0yXJyNmoG", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1684/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1684/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1684/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1684/Authors|ICLR.cc/2021/Conference/Paper1684/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1684/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923856915, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1684/-/Official_Comment"}}}, {"id": "zjkQPHyZIhB", "original": null, "number": 6, "cdate": 1605910756320, "ddate": null, "tcdate": 1605910756320, "tmdate": 1605910756320, "tddate": null, "forum": "ZW0yXJyNmoG", "replyto": "Skq46OsJvHJ", "invitation": "ICLR.cc/2021/Conference/Paper1684/-/Official_Comment", "content": {"title": "Update on Sec.3 ", "comment": "Due to the extended page limit (relative to the initial submission), we'd keep Sec. 3 as we consider it an important clarification, and we added extended theoretical discussions concerning games in Sec. 4 (see last revised version).  Moreover, we modified the bullet points for our contributions inline with this modification.\n\nPlease let us know if your concerns are addressed."}, "signatures": ["ICLR.cc/2021/Conference/Paper1684/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1684/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Taming GANs with Lookahead-Minmax", "authorids": ["~Tatjana_Chavdarova2", "~Matteo_Pagliardini1", "~Sebastian_U_Stich1", "~Fran\u00e7ois_Fleuret2", "~Martin_Jaggi1"], "authors": ["Tatjana Chavdarova", "Matteo Pagliardini", "Sebastian U Stich", "Fran\u00e7ois Fleuret", "Martin Jaggi"], "keywords": ["Minmax", "Generative Adversarial Networks"], "abstract": "Generative Adversarial Networks are notoriously challenging to train. The underlying minmax optimization is highly susceptible to the variance of the stochastic gradient and the rotational component of the associated game vector field. To tackle these challenges, we propose the Lookahead algorithm for minmax optimization, originally developed for single objective minimization only. The backtracking step of our Lookahead\u2013minmax naturally handles the rotational game dynamics, a property which was identified to be key for enabling gradient ascent descent methods to converge on challenging examples often analyzed in the literature. Moreover, it implicitly handles high variance without using large mini-batches, known to be essential for reaching state of the art performance. Experimental results on MNIST, SVHN, CIFAR-10, and ImageNet demonstrate a clear advantage of combining Lookahead\u2013minmax with Adam or extragradient, in terms of performance and improved stability, for negligible memory and computational cost. Using 30-fold fewer parameters and 16-fold smaller minibatches we outperform the reported performance of the class-dependent BigGAN on CIFAR-10 by obtaining FID of 12.19 without using the class labels, bringing state-of-the-art GAN training within reach of common computational resources.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chavdarova|taming_gans_with_lookaheadminmax", "one-sentence_summary": "A novel optimizer for GANs and games in general. ", "supplementary_material": "/attachment/574497e5b2553a4afc26c607586d795bd1ae28cd.zip", "pdf": "/pdf/96012e3aefba2bb5211c854f40b60a16024e2ada.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nchavdarova2021taming,\ntitle={Taming {\\{}GAN{\\}}s with Lookahead-Minmax},\nauthor={Tatjana Chavdarova and Matteo Pagliardini and Sebastian U Stich and Fran{\\c{c}}ois Fleuret and Martin Jaggi},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ZW0yXJyNmoG}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ZW0yXJyNmoG", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1684/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1684/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1684/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1684/Authors|ICLR.cc/2021/Conference/Paper1684/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1684/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923856915, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1684/-/Official_Comment"}}}, {"id": "-CPMhutE7jA", "original": null, "number": 5, "cdate": 1605910391921, "ddate": null, "tcdate": 1605910391921, "tmdate": 1605910391921, "tddate": null, "forum": "ZW0yXJyNmoG", "replyto": "S3N-JFtTcH", "invitation": "ICLR.cc/2021/Conference/Paper1684/-/Official_Comment", "content": {"title": "Thanks for your valuable comments!", "comment": "Thanks a lot for your review and recommendations! While we aim to keep this paper primarily empirical as you noted, to address your main concern we improved our Sec. 4 (see last revised version), where, starting from a simple bilinear game we show analytically that: *(i)* GDA diverges on this example, while in contrast *(ii)* Alg.1 combined with GDA can converge for this example for some alpha. We then show that Alg.1 converges locally when combined with a converging base optimizer (Thm. 2 in the revised version). "}, "signatures": ["ICLR.cc/2021/Conference/Paper1684/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1684/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Taming GANs with Lookahead-Minmax", "authorids": ["~Tatjana_Chavdarova2", "~Matteo_Pagliardini1", "~Sebastian_U_Stich1", "~Fran\u00e7ois_Fleuret2", "~Martin_Jaggi1"], "authors": ["Tatjana Chavdarova", "Matteo Pagliardini", "Sebastian U Stich", "Fran\u00e7ois Fleuret", "Martin Jaggi"], "keywords": ["Minmax", "Generative Adversarial Networks"], "abstract": "Generative Adversarial Networks are notoriously challenging to train. The underlying minmax optimization is highly susceptible to the variance of the stochastic gradient and the rotational component of the associated game vector field. To tackle these challenges, we propose the Lookahead algorithm for minmax optimization, originally developed for single objective minimization only. The backtracking step of our Lookahead\u2013minmax naturally handles the rotational game dynamics, a property which was identified to be key for enabling gradient ascent descent methods to converge on challenging examples often analyzed in the literature. Moreover, it implicitly handles high variance without using large mini-batches, known to be essential for reaching state of the art performance. Experimental results on MNIST, SVHN, CIFAR-10, and ImageNet demonstrate a clear advantage of combining Lookahead\u2013minmax with Adam or extragradient, in terms of performance and improved stability, for negligible memory and computational cost. Using 30-fold fewer parameters and 16-fold smaller minibatches we outperform the reported performance of the class-dependent BigGAN on CIFAR-10 by obtaining FID of 12.19 without using the class labels, bringing state-of-the-art GAN training within reach of common computational resources.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chavdarova|taming_gans_with_lookaheadminmax", "one-sentence_summary": "A novel optimizer for GANs and games in general. ", "supplementary_material": "/attachment/574497e5b2553a4afc26c607586d795bd1ae28cd.zip", "pdf": "/pdf/96012e3aefba2bb5211c854f40b60a16024e2ada.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nchavdarova2021taming,\ntitle={Taming {\\{}GAN{\\}}s with Lookahead-Minmax},\nauthor={Tatjana Chavdarova and Matteo Pagliardini and Sebastian U Stich and Fran{\\c{c}}ois Fleuret and Martin Jaggi},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ZW0yXJyNmoG}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ZW0yXJyNmoG", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1684/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1684/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1684/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1684/Authors|ICLR.cc/2021/Conference/Paper1684/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1684/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923856915, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1684/-/Official_Comment"}}}, {"id": "9xIKwMngbVx", "original": null, "number": 4, "cdate": 1605910248491, "ddate": null, "tcdate": 1605910248491, "tmdate": 1605910248491, "tddate": null, "forum": "ZW0yXJyNmoG", "replyto": "ujcNKpKJ5Js", "invitation": "ICLR.cc/2021/Conference/Paper1684/-/Official_Comment", "content": {"title": "Thanks for your feedback and for pointing out our typos!", "comment": "Thanks a lot for catching the typos!\n> The contributions are a little bit confusing. The first contribution listed in the introduction (and Section 3) is an improved convergence guarantee, which seems an independent contribution. What is the relation between this theoretical result and the lookahead-minmax algorithm for GANs? Does the following analysis and conclusions rely on this improvement? After reading the paper, I found the rest of the paper does not necessarily build on the new convergence guarantee. I think it might be clearer to mention the most significant contribution first.\n\nThanks for pointing this out! We re-ordered that contribution as second, and we added to the same bullet point our theoretical insight on lookahead-minmax (added in Sec. 4 of our latest revised version). While the latter is more related to the rest of the paper, we find the former contribution important to clarify (although it focuses on minimization).\n\n> Some related works are missing. Gidel et al. 2019b presents the negative momentum method, which leads to a similar \u201cbacktracking step\u201d in the vector field the gradients. CLC-GAN [Xu et al. arXiv:1909.13188] proposes to stabilize GANs\u2019 training dynamics by interpreting the gradient flow as a dynamic system and manage it with closed-loop control, which is also related to this paper.\n\nThanks for your recommendation, we added these references.\n\n> The experiments are conducted on 4 representative datasets. However, the image sizes (32*32) do not match the state-of-the-art GANs, which typically can generate larger images (e.g. Mescheder et al. 2018). One concern is whether the proposed method works well on larger datasets such as LSUN and ImageNet with higher resolutions. It would be more convincing if the authors can provide results on larger datasets.\n\nWhile we don\u2019t have the computation budget to run such experiments*, our extensive benchmark also addresses the case of high sample variability by considering 32x32 ImageNet. While it would be interesting to see large resolution results we consider our findings (over different base-optimizer--see also results in App.) relevant for a large portion of the ICLR audience (who don\u2019t have such resources or use smaller scale setups). Also, see Sec.4 of our last revised version, which provides further insights into why Alg.1 helps for games --which combined with the compelling empirical results increases the chances that the performance gain could follow for larger-resolution setups.\n\n\\* By quoting BigGAN's authors \"On 8xV100: this script takes 15 days to train to 150k iterations\u201d (referring to 128x128 pixel resolution), we estimate a cost of 15days:  24h x 8 GPUS x 2.5 = 7200$ on Google Cloud for a *single* experiment.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1684/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1684/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Taming GANs with Lookahead-Minmax", "authorids": ["~Tatjana_Chavdarova2", "~Matteo_Pagliardini1", "~Sebastian_U_Stich1", "~Fran\u00e7ois_Fleuret2", "~Martin_Jaggi1"], "authors": ["Tatjana Chavdarova", "Matteo Pagliardini", "Sebastian U Stich", "Fran\u00e7ois Fleuret", "Martin Jaggi"], "keywords": ["Minmax", "Generative Adversarial Networks"], "abstract": "Generative Adversarial Networks are notoriously challenging to train. The underlying minmax optimization is highly susceptible to the variance of the stochastic gradient and the rotational component of the associated game vector field. To tackle these challenges, we propose the Lookahead algorithm for minmax optimization, originally developed for single objective minimization only. The backtracking step of our Lookahead\u2013minmax naturally handles the rotational game dynamics, a property which was identified to be key for enabling gradient ascent descent methods to converge on challenging examples often analyzed in the literature. Moreover, it implicitly handles high variance without using large mini-batches, known to be essential for reaching state of the art performance. Experimental results on MNIST, SVHN, CIFAR-10, and ImageNet demonstrate a clear advantage of combining Lookahead\u2013minmax with Adam or extragradient, in terms of performance and improved stability, for negligible memory and computational cost. Using 30-fold fewer parameters and 16-fold smaller minibatches we outperform the reported performance of the class-dependent BigGAN on CIFAR-10 by obtaining FID of 12.19 without using the class labels, bringing state-of-the-art GAN training within reach of common computational resources.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chavdarova|taming_gans_with_lookaheadminmax", "one-sentence_summary": "A novel optimizer for GANs and games in general. ", "supplementary_material": "/attachment/574497e5b2553a4afc26c607586d795bd1ae28cd.zip", "pdf": "/pdf/96012e3aefba2bb5211c854f40b60a16024e2ada.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nchavdarova2021taming,\ntitle={Taming {\\{}GAN{\\}}s with Lookahead-Minmax},\nauthor={Tatjana Chavdarova and Matteo Pagliardini and Sebastian U Stich and Fran{\\c{c}}ois Fleuret and Martin Jaggi},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ZW0yXJyNmoG}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ZW0yXJyNmoG", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1684/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1684/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1684/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1684/Authors|ICLR.cc/2021/Conference/Paper1684/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1684/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923856915, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1684/-/Official_Comment"}}}, {"id": "FUcS_kI2BXa", "original": null, "number": 3, "cdate": 1605909767367, "ddate": null, "tcdate": 1605909767367, "tmdate": 1605910000133, "tddate": null, "forum": "ZW0yXJyNmoG", "replyto": "TjBFmBLybZ_", "invitation": "ICLR.cc/2021/Conference/Paper1684/-/Official_Comment", "content": {"title": "Thanks for the great review!", "comment": "Many thanks for your extensive review and detailed comments. Below we answer separately to your detailed comments.\n\n1. In our understanding, recent works give a game-theoretic interpretation of these points (as diff. Stackelberg Equilibria - DSE), however, as these are a superset of DNEs it remains unclear if the performance of the leader at DSE (on average) is as good as at DNEs (and approximate DNEs are guaranteed to exist [Daskalakis, Skoulakis, Zampetakis, Sep.2020]). We remain brief in this section by only mentioning the needed info (which we consider valid) to present our empirical result that LA converges to a point which has no rotation-Fig. 5 (relevant to show, as to our knowledge, such a case hasn\u2019t been observed so far in GAN training). We added a new section in the supplementary (App. C, referenced from the 3rd paragraph of Sec. 2), where we also discuss the references you listed.\n\n2. We aim to keep this paper mainly empirical for clarity, while still clarifying that besides that Lookahead-Minmax has been shown to work well in practice there is no result (to our knowledge) on improved upper bound on its convergence rate (which we consider important to clarify). Given the extra page allowed (relative to the initial submission), we\u2019d like to keep this result in the main paper. \nTo address your main concern--verifying theoretically our argument on \u2018handles well rotations\u2019, please see our revised section 4 and App. B. (also related to your last question 6), thanks! \n\n3. Thanks for the suggestion, we considered a quadratic function from the FtR-Approach in reference [2], and we did a spectral analysis comparing Extragradient (EG), GDA, and their Lookahead-Minmax counterparts LA-EG, and LA-GDA. We computed the spectral radius of those operators for a wide range of pairs of learning rates for each player and saved the parameters giving us the best contraction. In figure 10, we plot the distance to the optimum as a function of the number of passes, for the best contraction. Our results are in line with our bilinear experiments, as (i) Lookahead-Minmax methods have the best contraction when compared to GDA and EG, and (ii) Lookahead-Minmax improves the contraction of the base optimizer. \nWe think the bilinear game (BG) is relevant as it has high rotation--which standard minimization algorithms don\u2019t handle well (and SB-G has both high rotation and noise), is widely studied,  and finally, we find it empirically relevant as: (i) although alt-GDA does not diverge at the beginning of the training (as on BG), it diverges notably earlier than EG)  (ii) we see empirical relevance as pointed out in Fig. 2. \n\n4. As both EG and Optimistic Gradient Descent Ascent (OGDA) can be seen as approximating Proximal Point Method [Mokhtari et al. 2019] we selected one of these. We added OGDA in Fig. 3, where OGDA exploits well the fact that the gradient is smooth for the bilinear game (to approximate EG with past gradient), and outperforms EG when the x-axis is the gradient queries. For the larger-scale experiments, we selected EG and trained all methods for a fixed number of parameter updates (not gradient queries, so we think OGDA can perform as good or worse as the reported results for EG).\n\n[Mokhtari et al.] A Unified Analysis of Extra-gradient and Optimistic Gradient Methods for Saddle Point Problems: Proximal Point Approach, 2019\n\n5. Thanks a lot for the reference, we\u2019ll try to add it in our next revision.   \n\n6. We added the result of EMA on SB-G in the App. D.3. We do not show EMA for toy-experiments in the main paper, as for convex-concave objective EMA is less interesting than the uniform average (UA) (one can show that UA of GDA on a bounded domain converges due to convexity, which to our knowledge cannot be shown for EMA). On the other hand, UA is not interesting for DNNs, as in a highly non-convex non-concave DNN setting we can no longer rely on convexity for UA to converge, and EMA is shown to perform notably better than UA in practice (consistent with our UA results in App.), see discussions on \u2018last iterate convergence\u2019 in [1,2]. \nThanks a lot for your recommendation to analyze the spectrum of the dynamics! See our revised section 4 and  App. B.\n\n\n[1] Reducing Noise in GAN Training with Variance Reduced Extragradient, 2019 \n\n[2] Stochastic Hamiltonian Gradient Methods for Smooth Games, 2020\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1684/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1684/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Taming GANs with Lookahead-Minmax", "authorids": ["~Tatjana_Chavdarova2", "~Matteo_Pagliardini1", "~Sebastian_U_Stich1", "~Fran\u00e7ois_Fleuret2", "~Martin_Jaggi1"], "authors": ["Tatjana Chavdarova", "Matteo Pagliardini", "Sebastian U Stich", "Fran\u00e7ois Fleuret", "Martin Jaggi"], "keywords": ["Minmax", "Generative Adversarial Networks"], "abstract": "Generative Adversarial Networks are notoriously challenging to train. The underlying minmax optimization is highly susceptible to the variance of the stochastic gradient and the rotational component of the associated game vector field. To tackle these challenges, we propose the Lookahead algorithm for minmax optimization, originally developed for single objective minimization only. The backtracking step of our Lookahead\u2013minmax naturally handles the rotational game dynamics, a property which was identified to be key for enabling gradient ascent descent methods to converge on challenging examples often analyzed in the literature. Moreover, it implicitly handles high variance without using large mini-batches, known to be essential for reaching state of the art performance. Experimental results on MNIST, SVHN, CIFAR-10, and ImageNet demonstrate a clear advantage of combining Lookahead\u2013minmax with Adam or extragradient, in terms of performance and improved stability, for negligible memory and computational cost. Using 30-fold fewer parameters and 16-fold smaller minibatches we outperform the reported performance of the class-dependent BigGAN on CIFAR-10 by obtaining FID of 12.19 without using the class labels, bringing state-of-the-art GAN training within reach of common computational resources.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chavdarova|taming_gans_with_lookaheadminmax", "one-sentence_summary": "A novel optimizer for GANs and games in general. ", "supplementary_material": "/attachment/574497e5b2553a4afc26c607586d795bd1ae28cd.zip", "pdf": "/pdf/96012e3aefba2bb5211c854f40b60a16024e2ada.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nchavdarova2021taming,\ntitle={Taming {\\{}GAN{\\}}s with Lookahead-Minmax},\nauthor={Tatjana Chavdarova and Matteo Pagliardini and Sebastian U Stich and Fran{\\c{c}}ois Fleuret and Martin Jaggi},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ZW0yXJyNmoG}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ZW0yXJyNmoG", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1684/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1684/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1684/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1684/Authors|ICLR.cc/2021/Conference/Paper1684/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1684/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923856915, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1684/-/Official_Comment"}}}, {"id": "Skq46OsJvHJ", "original": null, "number": 2, "cdate": 1605477896782, "ddate": null, "tcdate": 1605477896782, "tmdate": 1605477896782, "tddate": null, "forum": "ZW0yXJyNmoG", "replyto": "Mnpk9cNQLXc", "invitation": "ICLR.cc/2021/Conference/Paper1684/-/Official_Comment", "content": {"title": "Thanks for the thorough comments", "comment": "\n> There seems to me some inconsistency in the numerical results in Tab. 1 and Fig. 6. Are the same hyper-parameters used for LA-Alt-GAN on ImageNet? The median diverges in (c) of Fig. 6 while in Table 1, the FID is around 14.37? This is hard for me to interpret as the total number of iterations are the same, which is 500k.\n\nThanks for the question. Table 1 reports the *best* scores ever obtained per method while computing the computationally-expensive IS and FID scores once every few thousands of parameter updates (and then averaging these over 5 seeds)--as reported in all related works that we are aware of [all refs in Table 2]. Note that contrary to classification, GAN training often diverges on real-world applications (also with 5:1 update ratio of D:G, see our paragraph on stability), and GAN methods in practice store a copy of the best performing model obtained during the training and return this model. We updated the caption to make this clearer. Thus, the result of 14.37 is the mean of the best FIDs of the LA-AltGAN methods obtained over 500k iterations (not at the 500K iter).\n\n> In terms of writing, I would recommend to focus on the minmax problem. In this sense, the section 3 is a bit distracting as it is about the optimization problem. The Algorithm 5 (page 19) about LA-AltGAN is important to understand the results, so it would be better to explain it more in the main body of the paper. Also what is LA-ExtraGrad algorithm? How Adam is applied in these algorithms? These proposed variants are not very clear to the reader. Also is the number of passes the same as the number of iterations?\n\nWe added explicit descriptions of both LA-EG and LA-GAN-Adam in the appendix (Alg. 4 & 5), and we re-wrote Alg.1 more generally. By using a \u201clooser\u201d pseudo-code of Alg.1 in the paper, we aim to point out the general \u201cmeta/wrapper\u201d nature of LA--Minmax: the fact that it can be combined with any \"base\u201d-optimizer. Please see our revised beginning of Section 4 and Alg.1. We are considering your recommendation regarding Section 3 and hope to address it in our next revision (before the author response deadline). \nThe number of passes is different from the number of iterations, where the former is normalized by the computation and denotes one cycle of forward+backward pass, or alternatively can be expressed as the number of \u201cgradient queries\u201d (this term is taken from the optimization literature, and the SVRE reference). In other words, using parameter updates as the x-axis could be misleading as for example, extragradient uses extra passes (gradient queries) per parameter update. The number of passes is thus a fair indicator of the computational complexity (as wall-clock time measurements can be relatively noisier).  Due to lack of space, we added this description on the \u201cnumber of passes\u201d in the Appendix (and we reference it from Section 4.1 in the main paper).\n\n> To make the results in Fig 5. more conclusive, I would recommend to zoom into the eigenvalues of LA-AltGAN, to see how small the imagery parts of the eigenvalues are. This is an interesting observation that it seems not very conclusive to show that LA-GAN shows no rotations.\n\nThe eigenvalues of LA-AltGAN have zero imaginary part (they are not zoomed out). Interestingly, to our knowledge, this case has not been observed in (Berard et al \u201819)--which potentially opens up directions.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1684/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1684/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Taming GANs with Lookahead-Minmax", "authorids": ["~Tatjana_Chavdarova2", "~Matteo_Pagliardini1", "~Sebastian_U_Stich1", "~Fran\u00e7ois_Fleuret2", "~Martin_Jaggi1"], "authors": ["Tatjana Chavdarova", "Matteo Pagliardini", "Sebastian U Stich", "Fran\u00e7ois Fleuret", "Martin Jaggi"], "keywords": ["Minmax", "Generative Adversarial Networks"], "abstract": "Generative Adversarial Networks are notoriously challenging to train. The underlying minmax optimization is highly susceptible to the variance of the stochastic gradient and the rotational component of the associated game vector field. To tackle these challenges, we propose the Lookahead algorithm for minmax optimization, originally developed for single objective minimization only. The backtracking step of our Lookahead\u2013minmax naturally handles the rotational game dynamics, a property which was identified to be key for enabling gradient ascent descent methods to converge on challenging examples often analyzed in the literature. Moreover, it implicitly handles high variance without using large mini-batches, known to be essential for reaching state of the art performance. Experimental results on MNIST, SVHN, CIFAR-10, and ImageNet demonstrate a clear advantage of combining Lookahead\u2013minmax with Adam or extragradient, in terms of performance and improved stability, for negligible memory and computational cost. Using 30-fold fewer parameters and 16-fold smaller minibatches we outperform the reported performance of the class-dependent BigGAN on CIFAR-10 by obtaining FID of 12.19 without using the class labels, bringing state-of-the-art GAN training within reach of common computational resources.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chavdarova|taming_gans_with_lookaheadminmax", "one-sentence_summary": "A novel optimizer for GANs and games in general. ", "supplementary_material": "/attachment/574497e5b2553a4afc26c607586d795bd1ae28cd.zip", "pdf": "/pdf/96012e3aefba2bb5211c854f40b60a16024e2ada.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nchavdarova2021taming,\ntitle={Taming {\\{}GAN{\\}}s with Lookahead-Minmax},\nauthor={Tatjana Chavdarova and Matteo Pagliardini and Sebastian U Stich and Fran{\\c{c}}ois Fleuret and Martin Jaggi},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ZW0yXJyNmoG}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ZW0yXJyNmoG", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1684/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1684/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1684/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1684/Authors|ICLR.cc/2021/Conference/Paper1684/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1684/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923856915, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1684/-/Official_Comment"}}}, {"id": "ujcNKpKJ5Js", "original": null, "number": 2, "cdate": 1603700670504, "ddate": null, "tcdate": 1603700670504, "tmdate": 1605024383275, "tddate": null, "forum": "ZW0yXJyNmoG", "replyto": "ZW0yXJyNmoG", "invitation": "ICLR.cc/2021/Conference/Paper1684/-/Official_Review", "content": {"title": "Official Blind Review #1", "review": "##### Summary:\n\nThis paper proposes a lookahead-minmax algorithm for optimizing minmax problems such as GANs, which updates the parameters (of both the generator and the discriminator) with the extrapolation. With a bilinear example, the authors show that the use of lookahead-minimax allows for convergence in cases where other methods does not, and yields good performance under high variance. Experiments of generative performance on several well-known public datasets demonstrates the effectiveness of the proposed method.\n\n##### Reasons for score:\n\nThe idea is intuitive and easy to follow. My major concern is about the scale of the experiments.\n\n---\n##### Pros: \n \n1. The paper is well-motivated and the idea is easy to understand. Figure 1 clearly demonstrates how lookahead-minmax can address the rotational nature in GAN\u2019s training, which is further confirmed in Figure 5.\n \n2. The advantages of the proposed lookahead-minmax is demonstrated with a bilinear example. For me, this helps to understand the merits of the proposed method.\n\n3. The paper provides comprehensive experiments and describes the experimental settings in details. \n \n##### Cons: \n \n1. The contributions are a little bit confusing. The first contribution listed in the introduction (and Section 3) is an improved convergence guarantee, which seems an independent contribution. What is the relation between this theoretical result and the lookahead-minmax algorithm for GANs? Does the following analysis and conclusions rely on this improvement? After reading the paper, I found the rest of the paper does not necessarily build on the new convergence guarantee. I think it might be clearer to mention the most significant contribution first.\n  \n2. Some related works are missing. Gidel et al. 2019b presents the negative momentum method, which leads to a similar \u201cbacktracking step\u201d in the vector field the gradients. CLC-GAN [Xu et al. arXiv:1909.13188] proposes to stabilize GANs\u2019 training dynamics by interpreting the gradient flow as a dynamic system and manage it with closed-loop control, which is also related to this paper.\n \n3. The experiments are conducted on 4 representative datasets. However, the image sizes (32*32) do not match the state-of-the-art GANs, which typically can generate larger images (e.g. Mescheder et al. 2018). One concern is whether the proposed method works well on larger datasets such as LSUN and ImageNet with higher resolutions. It would be more convincing if the authors can provide results on larger datasets.\n  \n---\n##### Some typos: \n(1) In Eqn. (JVF): the second entry in the JVF $\\nabla_{\\phi}\\mathcal{L}^{\\theta}$ -> $\\nabla_{\\phi}\\mathcal{L}^{\\phi}$\n\n(2) In the line below Eqn. (SB-G): It seems that $b$ and $c$ are of dimension $d \\times n$.\n\n(3) In paragraph of \u201cBenchmark on CIFAR-10 \u2026\u201d of Page 7, \u201c3.5 times larger model then ours\u201d -> \u201c3.5 times larger model than ours\u201d\n\n(4) In the same paragraph mentioned in (3), the references to tables are inconsistent. Both \u201cTable 2\u201d and \u201cTab. 2\u201d are used.\n\n(5) In the last line of Page 7, \u201cLA\u2014GAN\u201d em dash -> en dash\n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1684/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1684/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Taming GANs with Lookahead-Minmax", "authorids": ["~Tatjana_Chavdarova2", "~Matteo_Pagliardini1", "~Sebastian_U_Stich1", "~Fran\u00e7ois_Fleuret2", "~Martin_Jaggi1"], "authors": ["Tatjana Chavdarova", "Matteo Pagliardini", "Sebastian U Stich", "Fran\u00e7ois Fleuret", "Martin Jaggi"], "keywords": ["Minmax", "Generative Adversarial Networks"], "abstract": "Generative Adversarial Networks are notoriously challenging to train. The underlying minmax optimization is highly susceptible to the variance of the stochastic gradient and the rotational component of the associated game vector field. To tackle these challenges, we propose the Lookahead algorithm for minmax optimization, originally developed for single objective minimization only. The backtracking step of our Lookahead\u2013minmax naturally handles the rotational game dynamics, a property which was identified to be key for enabling gradient ascent descent methods to converge on challenging examples often analyzed in the literature. Moreover, it implicitly handles high variance without using large mini-batches, known to be essential for reaching state of the art performance. Experimental results on MNIST, SVHN, CIFAR-10, and ImageNet demonstrate a clear advantage of combining Lookahead\u2013minmax with Adam or extragradient, in terms of performance and improved stability, for negligible memory and computational cost. Using 30-fold fewer parameters and 16-fold smaller minibatches we outperform the reported performance of the class-dependent BigGAN on CIFAR-10 by obtaining FID of 12.19 without using the class labels, bringing state-of-the-art GAN training within reach of common computational resources.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chavdarova|taming_gans_with_lookaheadminmax", "one-sentence_summary": "A novel optimizer for GANs and games in general. ", "supplementary_material": "/attachment/574497e5b2553a4afc26c607586d795bd1ae28cd.zip", "pdf": "/pdf/96012e3aefba2bb5211c854f40b60a16024e2ada.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nchavdarova2021taming,\ntitle={Taming {\\{}GAN{\\}}s with Lookahead-Minmax},\nauthor={Tatjana Chavdarova and Matteo Pagliardini and Sebastian U Stich and Fran{\\c{c}}ois Fleuret and Martin Jaggi},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ZW0yXJyNmoG}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "ZW0yXJyNmoG", "replyto": "ZW0yXJyNmoG", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1684/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538113080, "tmdate": 1606915759622, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1684/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1684/-/Official_Review"}}}, {"id": "Mnpk9cNQLXc", "original": null, "number": 3, "cdate": 1603827022642, "ddate": null, "tcdate": 1603827022642, "tmdate": 1605024383145, "tddate": null, "forum": "ZW0yXJyNmoG", "replyto": "ZW0yXJyNmoG", "invitation": "ICLR.cc/2021/Conference/Paper1684/-/Official_Review", "content": {"title": "I find that the results on the bilinear game are convincing, but the results on GANs are much less convincing.  ", "review": "This paper adapts a recently introduced Lookhead method in optimization to improve the training of minmax problems such as GANs. The main challenge is to address the variance of stochastic gradients and the rotational component in the Jacobian of the gradients. The algorithm proposed in this paper shows improvements over existing methods, in terms of convergence rate. The stability of existing algorithms is a major issue when applying GANs on image dataset, the proposed Lookhead minmax method also improves the stability. I find that the results on the bilinear game are convincing, but the results on GANs are much less convincing.  \n\nThere seems to me some inconsistency in the numerical results in Tab. 1 and Fig. 6. Are the same hyper-parameters used for LA-Alt-GAN on ImageNet? The median diverges in (c) of Fig. 6 while in Table 1, the FID is around 14.37? This is hard for me to interpret as the total number of iterations are the same, which is 500k.\u00a0\n\nIn terms of writing, I would recommend to focus on the minmax problem. In this sense, the section 3 is a bit distracting as it is about the optimization problem. The Algorithm 5 (page 19) about LA-AltGAN is important to understand the results, so it would be better to explain it more in the main body of the paper. Also what is LA-ExtraGrad algorithm? How Adam is applied in these algorithms? These proposed variants are not very clear to the reader. Also is the number of passes the same as the number of iterations? \n\nTo make the results in Fig 5. more conclusive, I would recommend to zoom into the eigenvalues of LA-AltGAN, to see how small the imagery parts of the eigenvalues are. This is an interesting observation that it seems not very conclusive to show that LA-GAN shows no rotations.\n\nOverall, I think both the results and the writing need to be improved. \n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1684/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1684/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Taming GANs with Lookahead-Minmax", "authorids": ["~Tatjana_Chavdarova2", "~Matteo_Pagliardini1", "~Sebastian_U_Stich1", "~Fran\u00e7ois_Fleuret2", "~Martin_Jaggi1"], "authors": ["Tatjana Chavdarova", "Matteo Pagliardini", "Sebastian U Stich", "Fran\u00e7ois Fleuret", "Martin Jaggi"], "keywords": ["Minmax", "Generative Adversarial Networks"], "abstract": "Generative Adversarial Networks are notoriously challenging to train. The underlying minmax optimization is highly susceptible to the variance of the stochastic gradient and the rotational component of the associated game vector field. To tackle these challenges, we propose the Lookahead algorithm for minmax optimization, originally developed for single objective minimization only. The backtracking step of our Lookahead\u2013minmax naturally handles the rotational game dynamics, a property which was identified to be key for enabling gradient ascent descent methods to converge on challenging examples often analyzed in the literature. Moreover, it implicitly handles high variance without using large mini-batches, known to be essential for reaching state of the art performance. Experimental results on MNIST, SVHN, CIFAR-10, and ImageNet demonstrate a clear advantage of combining Lookahead\u2013minmax with Adam or extragradient, in terms of performance and improved stability, for negligible memory and computational cost. Using 30-fold fewer parameters and 16-fold smaller minibatches we outperform the reported performance of the class-dependent BigGAN on CIFAR-10 by obtaining FID of 12.19 without using the class labels, bringing state-of-the-art GAN training within reach of common computational resources.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chavdarova|taming_gans_with_lookaheadminmax", "one-sentence_summary": "A novel optimizer for GANs and games in general. ", "supplementary_material": "/attachment/574497e5b2553a4afc26c607586d795bd1ae28cd.zip", "pdf": "/pdf/96012e3aefba2bb5211c854f40b60a16024e2ada.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nchavdarova2021taming,\ntitle={Taming {\\{}GAN{\\}}s with Lookahead-Minmax},\nauthor={Tatjana Chavdarova and Matteo Pagliardini and Sebastian U Stich and Fran{\\c{c}}ois Fleuret and Martin Jaggi},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ZW0yXJyNmoG}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "ZW0yXJyNmoG", "replyto": "ZW0yXJyNmoG", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1684/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538113080, "tmdate": 1606915759622, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1684/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1684/-/Official_Review"}}}, {"id": "S3N-JFtTcH", "original": null, "number": 4, "cdate": 1603841261156, "ddate": null, "tcdate": 1603841261156, "tmdate": 1605024383081, "tddate": null, "forum": "ZW0yXJyNmoG", "replyto": "ZW0yXJyNmoG", "invitation": "ICLR.cc/2021/Conference/Paper1684/-/Official_Review", "content": {"title": "This paper treats with algorithmic schemes that improve the training of GANs. These iterative methods are experimentally evaluate in a extensive manner. ", "review": "The authors are focusing in this paper on the development of LookAhead mechanism. Their contributions are the following:\n\n1. They provide an improvement upon an already existing convergence rate for the minimization case.\n2. They propose the application of the LookAhead mechanism (under the necessary adjustments) for the more general min/max framework.\n3. These results are extensively justified by various numerical experiments.\n\nThe paper is well-written and easy to follow. Moreover, the core idea of applying the LookAhead for mix-max problems, as far as my knowledge goes, is  novel. In addition, without being an expert, the experiments seem promising and extensive.\nThat said, my concern would be regarding the theoretical part of the paper. I fully understand that the paper is primarily experimentally oriented. However some more theoretical analysis would be useful. More precisely, the theoretical justification for the improved convergence part for the minimization part seems to be more like a sketch rather than a rigorous mathematical proof. Overall, I suggest that it would be very interesting to see some provable theoretical guarantees starting with the  paper's motivation example that of the bilinear game.", "rating": "7: Good paper, accept", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2021/Conference/Paper1684/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1684/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Taming GANs with Lookahead-Minmax", "authorids": ["~Tatjana_Chavdarova2", "~Matteo_Pagliardini1", "~Sebastian_U_Stich1", "~Fran\u00e7ois_Fleuret2", "~Martin_Jaggi1"], "authors": ["Tatjana Chavdarova", "Matteo Pagliardini", "Sebastian U Stich", "Fran\u00e7ois Fleuret", "Martin Jaggi"], "keywords": ["Minmax", "Generative Adversarial Networks"], "abstract": "Generative Adversarial Networks are notoriously challenging to train. The underlying minmax optimization is highly susceptible to the variance of the stochastic gradient and the rotational component of the associated game vector field. To tackle these challenges, we propose the Lookahead algorithm for minmax optimization, originally developed for single objective minimization only. The backtracking step of our Lookahead\u2013minmax naturally handles the rotational game dynamics, a property which was identified to be key for enabling gradient ascent descent methods to converge on challenging examples often analyzed in the literature. Moreover, it implicitly handles high variance without using large mini-batches, known to be essential for reaching state of the art performance. Experimental results on MNIST, SVHN, CIFAR-10, and ImageNet demonstrate a clear advantage of combining Lookahead\u2013minmax with Adam or extragradient, in terms of performance and improved stability, for negligible memory and computational cost. Using 30-fold fewer parameters and 16-fold smaller minibatches we outperform the reported performance of the class-dependent BigGAN on CIFAR-10 by obtaining FID of 12.19 without using the class labels, bringing state-of-the-art GAN training within reach of common computational resources.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chavdarova|taming_gans_with_lookaheadminmax", "one-sentence_summary": "A novel optimizer for GANs and games in general. ", "supplementary_material": "/attachment/574497e5b2553a4afc26c607586d795bd1ae28cd.zip", "pdf": "/pdf/96012e3aefba2bb5211c854f40b60a16024e2ada.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nchavdarova2021taming,\ntitle={Taming {\\{}GAN{\\}}s with Lookahead-Minmax},\nauthor={Tatjana Chavdarova and Matteo Pagliardini and Sebastian U Stich and Fran{\\c{c}}ois Fleuret and Martin Jaggi},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ZW0yXJyNmoG}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "ZW0yXJyNmoG", "replyto": "ZW0yXJyNmoG", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1684/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538113080, "tmdate": 1606915759622, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1684/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1684/-/Official_Review"}}}], "count": 14}