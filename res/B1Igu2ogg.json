{"notes": [{"tddate": null, "tmdate": 1495149665579, "tcdate": 1495149665579, "number": 14, "id": "S19DnijlZ", "invitation": "ICLR.cc/2017/conference/-/paper578/public/comment", "forum": "B1Igu2ogg", "replyto": "B1Igu2ogg", "signatures": ["~Minmin_Chen1"], "readers": ["everyone"], "writers": ["~Minmin_Chen1"], "nonreaders": ["m.chen@criteo.com"], "content": {"title": "Better word embeddings", "comment": "Extended the paper with experiments on the word relationship dataset, showing Doc2VecC generates better word embeddings in comparison to Word2Vec or Paragraph Vectors. "}, "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Vector Representation for Documents through Corruption", "abstract": "We present an efficient document representation learning framework, Document Vector through Corruption (Doc2VecC). Doc2VecC represents each document as a simple average of word embeddings. It ensures a representation generated as such captures the semantic meanings of the document during learning. A corruption model is included, which introduces a data-dependent regularization that favors informative or rare words while forcing the embeddings of common and non-discriminative ones to be close to zero. Doc2VecC produces significantly better word embeddings than Word2Vec. We compare Doc2VecC with several state-of-the-art document representation learning algorithms. The simple model architecture introduced by Doc2VecC matches or out-performs the state-of-the-art in generating high-quality document representations for sentiment analysis, document classification as well as semantic relatedness tasks. The simplicity of the model enables training on billions of words per hour on a single machine. At the same time, the model is very efficient in generating representations of unseen documents at test time.\n", "pdf": "/pdf/5e58d0d43f34525f995cc00fe57341537cb1b88e.pdf", "TL;DR": "a simple document representation learning framework that is very efficient to train and test", "paperhash": "chen|efficient_vector_representation_for_documents_through_corruption", "conflicts": ["wustl.edu"], "authors": ["Minmin Chen"], "authorids": ["m.chen@criteo.com"], "keywords": ["Natural language processing", "Deep learning", "Semi-Supervised Learning"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287514318, "id": "ICLR.cc/2017/conference/-/paper578/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1Igu2ogg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper578/reviewers", "ICLR.cc/2017/conference/paper578/areachairs"], "cdate": 1485287514318}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1495149586201, "tcdate": 1478375405993, "number": 578, "id": "B1Igu2ogg", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "B1Igu2ogg", "signatures": ["~Minmin_Chen1"], "readers": ["everyone"], "content": {"title": "Efficient Vector Representation for Documents through Corruption", "abstract": "We present an efficient document representation learning framework, Document Vector through Corruption (Doc2VecC). Doc2VecC represents each document as a simple average of word embeddings. It ensures a representation generated as such captures the semantic meanings of the document during learning. A corruption model is included, which introduces a data-dependent regularization that favors informative or rare words while forcing the embeddings of common and non-discriminative ones to be close to zero. Doc2VecC produces significantly better word embeddings than Word2Vec. We compare Doc2VecC with several state-of-the-art document representation learning algorithms. The simple model architecture introduced by Doc2VecC matches or out-performs the state-of-the-art in generating high-quality document representations for sentiment analysis, document classification as well as semantic relatedness tasks. The simplicity of the model enables training on billions of words per hour on a single machine. At the same time, the model is very efficient in generating representations of unseen documents at test time.\n", "pdf": "/pdf/5e58d0d43f34525f995cc00fe57341537cb1b88e.pdf", "TL;DR": "a simple document representation learning framework that is very efficient to train and test", "paperhash": "chen|efficient_vector_representation_for_documents_through_corruption", "conflicts": ["wustl.edu"], "authors": ["Minmin Chen"], "authorids": ["m.chen@criteo.com"], "keywords": ["Natural language processing", "Deep learning", "Semi-Supervised Learning"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 20, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "nonreaders": null, "tmdate": 1492671302952, "tcdate": 1492671120829, "number": 13, "id": "SkFcq0B0x", "invitation": "ICLR.cc/2017/conference/-/paper578/public/comment", "forum": "B1Igu2ogg", "replyto": "BJz3FYxnl", "signatures": ["~Minmin_Chen1"], "readers": ["everyone"], "writers": ["~Minmin_Chen1"], "content": {"title": "On data-dependent regularization", "comment": "Thank you for your interest! For this experiment, I set a cut-off of 100 to remove words that appear less than 100 times throughout the document. These words will have very small norm as they are rare. It was explained in page 7 of the paper. You can also see that in table 3 the words of smallest norms learned by the other methods are the ones appearing around 100 times in the corpus. "}, "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Vector Representation for Documents through Corruption", "abstract": "We present an efficient document representation learning framework, Document Vector through Corruption (Doc2VecC). Doc2VecC represents each document as a simple average of word embeddings. It ensures a representation generated as such captures the semantic meanings of the document during learning. A corruption model is included, which introduces a data-dependent regularization that favors informative or rare words while forcing the embeddings of common and non-discriminative ones to be close to zero. Doc2VecC produces significantly better word embeddings than Word2Vec. We compare Doc2VecC with several state-of-the-art document representation learning algorithms. The simple model architecture introduced by Doc2VecC matches or out-performs the state-of-the-art in generating high-quality document representations for sentiment analysis, document classification as well as semantic relatedness tasks. The simplicity of the model enables training on billions of words per hour on a single machine. At the same time, the model is very efficient in generating representations of unseen documents at test time.\n", "pdf": "/pdf/5e58d0d43f34525f995cc00fe57341537cb1b88e.pdf", "TL;DR": "a simple document representation learning framework that is very efficient to train and test", "paperhash": "chen|efficient_vector_representation_for_documents_through_corruption", "conflicts": ["wustl.edu"], "authors": ["Minmin Chen"], "authorids": ["m.chen@criteo.com"], "keywords": ["Natural language processing", "Deep learning", "Semi-Supervised Learning"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287514318, "id": "ICLR.cc/2017/conference/-/paper578/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1Igu2ogg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper578/reviewers", "ICLR.cc/2017/conference/paper578/areachairs"], "cdate": 1485287514318}}}, {"tddate": null, "tmdate": 1490225577790, "tcdate": 1490225577790, "number": 12, "id": "BJz3FYxnl", "invitation": "ICLR.cc/2017/conference/-/paper578/public/comment", "forum": "B1Igu2ogg", "replyto": "B1Igu2ogg", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "About Table 3: Words with embeddings closest to 0 learned by different algorithms.", "comment": "I'm using https://github.com/mchen24/iclr2017 to learn embeddings and then compute the l2 norm of embeddings. I find the least 10 words are\n '</s>',\n '--shelly',\n 'willett',\n '-ap3-',\n 'gruel',\n '-celluloid',\n '10*',\n 'massie',\n '****/****',\n 'hush-hush',\nTheir l2 norms are 7.79849624e-04,   2.45744940e-01,   2.53457799e-01, 2.56453203e-01,   3.34406160e-01,   4.00243759e-01, 4.22770766e-01,   5.10860541e-01,   5.40468018e-01, 5.50735000e-01. \nThey are close to 0, but except the </s>, other words are pretty unpopular.  Can you please check whether the inconsistency? "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Vector Representation for Documents through Corruption", "abstract": "We present an efficient document representation learning framework, Document Vector through Corruption (Doc2VecC). Doc2VecC represents each document as a simple average of word embeddings. It ensures a representation generated as such captures the semantic meanings of the document during learning. A corruption model is included, which introduces a data-dependent regularization that favors informative or rare words while forcing the embeddings of common and non-discriminative ones to be close to zero. Doc2VecC produces significantly better word embeddings than Word2Vec. We compare Doc2VecC with several state-of-the-art document representation learning algorithms. The simple model architecture introduced by Doc2VecC matches or out-performs the state-of-the-art in generating high-quality document representations for sentiment analysis, document classification as well as semantic relatedness tasks. The simplicity of the model enables training on billions of words per hour on a single machine. At the same time, the model is very efficient in generating representations of unseen documents at test time.\n", "pdf": "/pdf/5e58d0d43f34525f995cc00fe57341537cb1b88e.pdf", "TL;DR": "a simple document representation learning framework that is very efficient to train and test", "paperhash": "chen|efficient_vector_representation_for_documents_through_corruption", "conflicts": ["wustl.edu"], "authors": ["Minmin Chen"], "authorids": ["m.chen@criteo.com"], "keywords": ["Natural language processing", "Deep learning", "Semi-Supervised Learning"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287514318, "id": "ICLR.cc/2017/conference/-/paper578/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1Igu2ogg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper578/reviewers", "ICLR.cc/2017/conference/paper578/areachairs"], "cdate": 1485287514318}}}, {"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396686762, "tcdate": 1486396686762, "number": 1, "id": "S1wz6zLOl", "invitation": "ICLR.cc/2017/conference/-/paper578/acceptance", "forum": "B1Igu2ogg", "replyto": "B1Igu2ogg", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "The introduced method for producing document representations is simple, efficient and potentially quite useful. Though we could quibble a bit that the idea is just a combination of known techniques, the reviews generally agree that the idea is interesting.\n \n Pros:\n + interesting and simple algorithm\n + strong performance\n + efficient\n \n Cons:\n + individual ideas are not so novel\n \n This is a paper that will be well received at a poster presentation.", "decision": "Accept (Poster)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Vector Representation for Documents through Corruption", "abstract": "We present an efficient document representation learning framework, Document Vector through Corruption (Doc2VecC). Doc2VecC represents each document as a simple average of word embeddings. It ensures a representation generated as such captures the semantic meanings of the document during learning. A corruption model is included, which introduces a data-dependent regularization that favors informative or rare words while forcing the embeddings of common and non-discriminative ones to be close to zero. Doc2VecC produces significantly better word embeddings than Word2Vec. We compare Doc2VecC with several state-of-the-art document representation learning algorithms. The simple model architecture introduced by Doc2VecC matches or out-performs the state-of-the-art in generating high-quality document representations for sentiment analysis, document classification as well as semantic relatedness tasks. The simplicity of the model enables training on billions of words per hour on a single machine. At the same time, the model is very efficient in generating representations of unseen documents at test time.\n", "pdf": "/pdf/5e58d0d43f34525f995cc00fe57341537cb1b88e.pdf", "TL;DR": "a simple document representation learning framework that is very efficient to train and test", "paperhash": "chen|efficient_vector_representation_for_documents_through_corruption", "conflicts": ["wustl.edu"], "authors": ["Minmin Chen"], "authorids": ["m.chen@criteo.com"], "keywords": ["Natural language processing", "Deep learning", "Semi-Supervised Learning"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396687282, "id": "ICLR.cc/2017/conference/-/paper578/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "B1Igu2ogg", "replyto": "B1Igu2ogg", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396687282}}}, {"tddate": null, "tmdate": 1484938406191, "tcdate": 1484938406191, "number": 11, "id": "r1As2C1De", "invitation": "ICLR.cc/2017/conference/-/paper578/public/comment", "forum": "B1Igu2ogg", "replyto": "BJUqiLcBl", "signatures": ["~Minmin_Chen1"], "readers": ["everyone"], "writers": ["~Minmin_Chen1"], "content": {"title": "Updated review", "comment": "Dear reviewer, \n\nThank you for taking time to read the paper again and update the review. \n\nMinmin"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Vector Representation for Documents through Corruption", "abstract": "We present an efficient document representation learning framework, Document Vector through Corruption (Doc2VecC). Doc2VecC represents each document as a simple average of word embeddings. It ensures a representation generated as such captures the semantic meanings of the document during learning. A corruption model is included, which introduces a data-dependent regularization that favors informative or rare words while forcing the embeddings of common and non-discriminative ones to be close to zero. Doc2VecC produces significantly better word embeddings than Word2Vec. We compare Doc2VecC with several state-of-the-art document representation learning algorithms. The simple model architecture introduced by Doc2VecC matches or out-performs the state-of-the-art in generating high-quality document representations for sentiment analysis, document classification as well as semantic relatedness tasks. The simplicity of the model enables training on billions of words per hour on a single machine. At the same time, the model is very efficient in generating representations of unseen documents at test time.\n", "pdf": "/pdf/5e58d0d43f34525f995cc00fe57341537cb1b88e.pdf", "TL;DR": "a simple document representation learning framework that is very efficient to train and test", "paperhash": "chen|efficient_vector_representation_for_documents_through_corruption", "conflicts": ["wustl.edu"], "authors": ["Minmin Chen"], "authorids": ["m.chen@criteo.com"], "keywords": ["Natural language processing", "Deep learning", "Semi-Supervised Learning"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287514318, "id": "ICLR.cc/2017/conference/-/paper578/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1Igu2ogg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper578/reviewers", "ICLR.cc/2017/conference/paper578/areachairs"], "cdate": 1485287514318}}}, {"tddate": null, "tmdate": 1483529101660, "tcdate": 1483529101660, "number": 2, "id": "BJUqiLcBl", "invitation": "ICLR.cc/2017/conference/-/paper578/official/comment", "forum": "B1Igu2ogg", "replyto": "BkxfBmS4l", "signatures": ["ICLR.cc/2017/conference/paper578/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper578/AnonReviewer1"], "content": {"title": "thanks for running an additional set of experiments", "comment": "The new results addressed some of my concerns so I have updated my review score accordingly."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Vector Representation for Documents through Corruption", "abstract": "We present an efficient document representation learning framework, Document Vector through Corruption (Doc2VecC). Doc2VecC represents each document as a simple average of word embeddings. It ensures a representation generated as such captures the semantic meanings of the document during learning. A corruption model is included, which introduces a data-dependent regularization that favors informative or rare words while forcing the embeddings of common and non-discriminative ones to be close to zero. Doc2VecC produces significantly better word embeddings than Word2Vec. We compare Doc2VecC with several state-of-the-art document representation learning algorithms. The simple model architecture introduced by Doc2VecC matches or out-performs the state-of-the-art in generating high-quality document representations for sentiment analysis, document classification as well as semantic relatedness tasks. The simplicity of the model enables training on billions of words per hour on a single machine. At the same time, the model is very efficient in generating representations of unseen documents at test time.\n", "pdf": "/pdf/5e58d0d43f34525f995cc00fe57341537cb1b88e.pdf", "TL;DR": "a simple document representation learning framework that is very efficient to train and test", "paperhash": "chen|efficient_vector_representation_for_documents_through_corruption", "conflicts": ["wustl.edu"], "authors": ["Minmin Chen"], "authorids": ["m.chen@criteo.com"], "keywords": ["Natural language processing", "Deep learning", "Semi-Supervised Learning"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287513641, "id": "ICLR.cc/2017/conference/-/paper578/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "B1Igu2ogg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper578/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper578/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper578/reviewers", "ICLR.cc/2017/conference/paper578/areachairs"], "cdate": 1485287513641}}}, {"tddate": null, "tmdate": 1483528976643, "tcdate": 1482129906945, "number": 2, "id": "rkslf-rVl", "invitation": "ICLR.cc/2017/conference/-/paper578/official/review", "forum": "B1Igu2ogg", "replyto": "B1Igu2ogg", "signatures": ["ICLR.cc/2017/conference/paper578/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper578/AnonReviewer1"], "content": {"title": "review", "rating": "6: Marginally above acceptance threshold", "review": "This paper presents a framework for creating document representations. \nThe main idea is to represent a document as an average of its word embeddings with a data-dependent regularization that favors informative or rare words while forcing common words to be close to 0. \nExperiments on sentiment analysis and document classification show that the proposed method has the lowest error rates compared to baseline document embedding methods. \n\nWhile I like the motivation of finding the best way to encode a document into a vector, the paper does not offer significant technical contributions.\nMost of the techniques are not new, and the main selling point is the simplicity and speed of the proposed method. \nFor this reason, I would like to see good results for more than two tasks to be convinced that this is the best way to learn document representations.  \nFor RNN-LM, is the LM trained to minimize classification error, or is it trained  as a language model? Did you use the final hidden state as the representation, or the average of all hidden states?\nOne of the most widely used method to represent documents now is to have a bidirectional LSTM and concatenate the final hidden states as the document representation. \nI think it would be useful to know how the proposed method compares to this approach for tasks such as document classification or sentiment analysis.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Vector Representation for Documents through Corruption", "abstract": "We present an efficient document representation learning framework, Document Vector through Corruption (Doc2VecC). Doc2VecC represents each document as a simple average of word embeddings. It ensures a representation generated as such captures the semantic meanings of the document during learning. A corruption model is included, which introduces a data-dependent regularization that favors informative or rare words while forcing the embeddings of common and non-discriminative ones to be close to zero. Doc2VecC produces significantly better word embeddings than Word2Vec. We compare Doc2VecC with several state-of-the-art document representation learning algorithms. The simple model architecture introduced by Doc2VecC matches or out-performs the state-of-the-art in generating high-quality document representations for sentiment analysis, document classification as well as semantic relatedness tasks. The simplicity of the model enables training on billions of words per hour on a single machine. At the same time, the model is very efficient in generating representations of unseen documents at test time.\n", "pdf": "/pdf/5e58d0d43f34525f995cc00fe57341537cb1b88e.pdf", "TL;DR": "a simple document representation learning framework that is very efficient to train and test", "paperhash": "chen|efficient_vector_representation_for_documents_through_corruption", "conflicts": ["wustl.edu"], "authors": ["Minmin Chen"], "authorids": ["m.chen@criteo.com"], "keywords": ["Natural language processing", "Deep learning", "Semi-Supervised Learning"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512535873, "id": "ICLR.cc/2017/conference/-/paper578/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper578/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper578/AnonReviewer2", "ICLR.cc/2017/conference/paper578/AnonReviewer1", "ICLR.cc/2017/conference/paper578/AnonReviewer3"], "reply": {"forum": "B1Igu2ogg", "replyto": "B1Igu2ogg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper578/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper578/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512535873}}}, {"tddate": null, "tmdate": 1483469331265, "tcdate": 1483469331265, "number": 10, "id": "SysfGOtHe", "invitation": "ICLR.cc/2017/conference/-/paper578/public/comment", "forum": "B1Igu2ogg", "replyto": "rkslf-rVl", "signatures": ["~Minmin_Chen1"], "readers": ["everyone"], "writers": ["~Minmin_Chen1"], "content": {"title": "Additional information added", "comment": "Dear reviewer, \n\nI have added more experiments (a new dataset and comparison to LSTM-based methods) and explanations to the manuscript regarding some of the concerns you pointed out. I believe that the additional information provided could worth a second view and would be very much interested in an open discussion to find out if there are any remaining unfavorable factors.\n\nThank you again for your time. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Vector Representation for Documents through Corruption", "abstract": "We present an efficient document representation learning framework, Document Vector through Corruption (Doc2VecC). Doc2VecC represents each document as a simple average of word embeddings. It ensures a representation generated as such captures the semantic meanings of the document during learning. A corruption model is included, which introduces a data-dependent regularization that favors informative or rare words while forcing the embeddings of common and non-discriminative ones to be close to zero. Doc2VecC produces significantly better word embeddings than Word2Vec. We compare Doc2VecC with several state-of-the-art document representation learning algorithms. The simple model architecture introduced by Doc2VecC matches or out-performs the state-of-the-art in generating high-quality document representations for sentiment analysis, document classification as well as semantic relatedness tasks. The simplicity of the model enables training on billions of words per hour on a single machine. At the same time, the model is very efficient in generating representations of unseen documents at test time.\n", "pdf": "/pdf/5e58d0d43f34525f995cc00fe57341537cb1b88e.pdf", "TL;DR": "a simple document representation learning framework that is very efficient to train and test", "paperhash": "chen|efficient_vector_representation_for_documents_through_corruption", "conflicts": ["wustl.edu"], "authors": ["Minmin Chen"], "authorids": ["m.chen@criteo.com"], "keywords": ["Natural language processing", "Deep learning", "Semi-Supervised Learning"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287514318, "id": "ICLR.cc/2017/conference/-/paper578/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1Igu2ogg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper578/reviewers", "ICLR.cc/2017/conference/paper578/areachairs"], "cdate": 1485287514318}}}, {"tddate": null, "tmdate": 1482638269112, "tcdate": 1482638269112, "number": 9, "id": "r1Ba7anNx", "invitation": "ICLR.cc/2017/conference/-/paper578/public/comment", "forum": "B1Igu2ogg", "replyto": "rkslf-rVl", "signatures": ["~Minmin_Chen1"], "readers": ["everyone"], "writers": ["~Minmin_Chen1"], "content": {"title": "New dataset and baselines added", "comment": "Dear reviewer, \n\nI added another dataset to the draft (appendix): SemEval 2014 Task 1 semantic relatedness SICK dataset, as several of the recent works the reviewers pointed out have been reported on this dataset. Despite its simplicity, Doc2VecC significantly out-performs the winning solutions of the competition and several baseline methods, noticeably the dependency-tree RNNs introduced in [1], which relies on additional dependency parsers to compose sentence vectors from word embeddings. The performance of Doc2VecC is comparable (slightly worse) than the LSTM based methods or skip-thought vectors on this dataset. On the other hand, it is much faster to train and test. As reported in the original paper, training of the skip-thought vector models on the book corpus dataset takes around 2 weeks on GPU. In contrast, it takes less than 2 hours to learn the embeddings for Doc2VecC on a desktop with Intel i7 2.2Ghz cpu.\n\nI also provided more insight on why Skip-thought vectors did not perform well on the movie review dataset in Section 4.2 (Accuracy). \n\nI would greatly appreciate it if you could take another look at revisions and provide me with some feedbacks. \n\n[1] Socher, Richard, et al. \"Grounded compositional semantics for finding and describing images with sentences.\" Transactions of the Association for Computational Linguistics 2 (2014): 207-218."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Vector Representation for Documents through Corruption", "abstract": "We present an efficient document representation learning framework, Document Vector through Corruption (Doc2VecC). Doc2VecC represents each document as a simple average of word embeddings. It ensures a representation generated as such captures the semantic meanings of the document during learning. A corruption model is included, which introduces a data-dependent regularization that favors informative or rare words while forcing the embeddings of common and non-discriminative ones to be close to zero. Doc2VecC produces significantly better word embeddings than Word2Vec. We compare Doc2VecC with several state-of-the-art document representation learning algorithms. The simple model architecture introduced by Doc2VecC matches or out-performs the state-of-the-art in generating high-quality document representations for sentiment analysis, document classification as well as semantic relatedness tasks. The simplicity of the model enables training on billions of words per hour on a single machine. At the same time, the model is very efficient in generating representations of unseen documents at test time.\n", "pdf": "/pdf/5e58d0d43f34525f995cc00fe57341537cb1b88e.pdf", "TL;DR": "a simple document representation learning framework that is very efficient to train and test", "paperhash": "chen|efficient_vector_representation_for_documents_through_corruption", "conflicts": ["wustl.edu"], "authors": ["Minmin Chen"], "authorids": ["m.chen@criteo.com"], "keywords": ["Natural language processing", "Deep learning", "Semi-Supervised Learning"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287514318, "id": "ICLR.cc/2017/conference/-/paper578/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1Igu2ogg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper578/reviewers", "ICLR.cc/2017/conference/paper578/areachairs"], "cdate": 1485287514318}}}, {"tddate": null, "tmdate": 1482543856469, "tcdate": 1482543856469, "number": 8, "id": "Hy_gmUiVl", "invitation": "ICLR.cc/2017/conference/-/paper578/public/comment", "forum": "B1Igu2ogg", "replyto": "B1Igu2ogg", "signatures": ["~Minmin_Chen1"], "readers": ["everyone"], "writers": ["~Minmin_Chen1"], "content": {"title": "new dataset and baselines added", "comment": "Dear reviewers, \n\nI added another dataset to the draft (appendix): SemEval 2014 Task 1 semantic relatedness SICK dataset, as several of the recent works the reviewers pointed out have been reported on this dataset. Despite its simplicity, Doc2VecC significantly out-performs the winning solutions of the competition and several baseline methods, noticeably the dependency-tree RNNs introduced in [1], which relies on additional dependency parsers to compose sentence vectors from word embeddings. The performance of Doc2VecC is comparable (slightly worse) than the LSTM based methods or skip-thought vectors on this dataset. On the other hand, it is much faster to train and test. As reported in the original paper, training of the skip-thought vector models on the book corpus dataset takes around 2 weeks on GPU. In contrast, it takes less than 2 hours to learn the embeddings for Doc2VecC on a desktop with Intel i7 2.2Ghz cpu.\n\nI also provided more insight on why Skip-thought vectors did not perform well on the movie review dataset in Section 4.2 (Accuracy). \n\nI would greatly appreciate it if you could take another look at revisions and provide me with some feedbacks. \n\n[1] Socher, Richard, et al. \"Grounded compositional semantics for finding and describing images with sentences.\" Transactions of the Association for Computational Linguistics 2 (2014): 207-218."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Vector Representation for Documents through Corruption", "abstract": "We present an efficient document representation learning framework, Document Vector through Corruption (Doc2VecC). Doc2VecC represents each document as a simple average of word embeddings. It ensures a representation generated as such captures the semantic meanings of the document during learning. A corruption model is included, which introduces a data-dependent regularization that favors informative or rare words while forcing the embeddings of common and non-discriminative ones to be close to zero. Doc2VecC produces significantly better word embeddings than Word2Vec. We compare Doc2VecC with several state-of-the-art document representation learning algorithms. The simple model architecture introduced by Doc2VecC matches or out-performs the state-of-the-art in generating high-quality document representations for sentiment analysis, document classification as well as semantic relatedness tasks. The simplicity of the model enables training on billions of words per hour on a single machine. At the same time, the model is very efficient in generating representations of unseen documents at test time.\n", "pdf": "/pdf/5e58d0d43f34525f995cc00fe57341537cb1b88e.pdf", "TL;DR": "a simple document representation learning framework that is very efficient to train and test", "paperhash": "chen|efficient_vector_representation_for_documents_through_corruption", "conflicts": ["wustl.edu"], "authors": ["Minmin Chen"], "authorids": ["m.chen@criteo.com"], "keywords": ["Natural language processing", "Deep learning", "Semi-Supervised Learning"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287514318, "id": "ICLR.cc/2017/conference/-/paper578/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1Igu2ogg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper578/reviewers", "ICLR.cc/2017/conference/paper578/areachairs"], "cdate": 1485287514318}}}, {"tddate": null, "tmdate": 1482292226847, "tcdate": 1482292226847, "number": 6, "id": "B1sb3uD4x", "invitation": "ICLR.cc/2017/conference/-/paper578/public/comment", "forum": "B1Igu2ogg", "replyto": "B1Igu2ogg", "signatures": ["~Minmin_Chen1"], "readers": ["everyone"], "writers": ["~Minmin_Chen1"], "content": {"title": "revisions", "comment": "Dear reviewers, \n\nThank you for your feedback. The updated manuscript included skip-thought as another baseline method. We will test this idea on more datasets, in particular the ones experimented in Skip-thought vectors in the submission. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Vector Representation for Documents through Corruption", "abstract": "We present an efficient document representation learning framework, Document Vector through Corruption (Doc2VecC). Doc2VecC represents each document as a simple average of word embeddings. It ensures a representation generated as such captures the semantic meanings of the document during learning. A corruption model is included, which introduces a data-dependent regularization that favors informative or rare words while forcing the embeddings of common and non-discriminative ones to be close to zero. Doc2VecC produces significantly better word embeddings than Word2Vec. We compare Doc2VecC with several state-of-the-art document representation learning algorithms. The simple model architecture introduced by Doc2VecC matches or out-performs the state-of-the-art in generating high-quality document representations for sentiment analysis, document classification as well as semantic relatedness tasks. The simplicity of the model enables training on billions of words per hour on a single machine. At the same time, the model is very efficient in generating representations of unseen documents at test time.\n", "pdf": "/pdf/5e58d0d43f34525f995cc00fe57341537cb1b88e.pdf", "TL;DR": "a simple document representation learning framework that is very efficient to train and test", "paperhash": "chen|efficient_vector_representation_for_documents_through_corruption", "conflicts": ["wustl.edu"], "authors": ["Minmin Chen"], "authorids": ["m.chen@criteo.com"], "keywords": ["Natural language processing", "Deep learning", "Semi-Supervised Learning"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287514318, "id": "ICLR.cc/2017/conference/-/paper578/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1Igu2ogg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper578/reviewers", "ICLR.cc/2017/conference/paper578/areachairs"], "cdate": 1485287514318}}}, {"tddate": null, "tmdate": 1482190351249, "tcdate": 1482190351249, "number": 3, "id": "B1vz0k8Ne", "invitation": "ICLR.cc/2017/conference/-/paper578/official/review", "forum": "B1Igu2ogg", "replyto": "B1Igu2ogg", "signatures": ["ICLR.cc/2017/conference/paper578/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper578/AnonReviewer3"], "content": {"title": "Simple idea, nicely composed", "rating": "7: Good paper, accept", "review": "This paper discusses a method for computing vector representations for documents by using a skip-gram style learning mechanism with an added regularizer in the form of a global context vector with various bits of drop out. While none of the individual components proposed in this paper are new, I believe that the combination in this fashion is. Further, I appreciated the detailed analysis of model behaviour in section 3.\n\nThe main downside to this submission is in its relative weakness on the empirical front. Arguably there are more interesting tasks than sentiment analysis and k-way classification! Likewise, why waste 2/3 of a page on t-sne projections rather than use that space for further analysis?\n\nWhile I am a bit disappointed by this reduced evaluation and agree with the other reviewers concerning soft baselines, I think this paper should be accepted: it's an interesting algorithm, nicely composed and very efficient, so it's reasonable to assume that other readers might have use for some of the ideas presented here.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Vector Representation for Documents through Corruption", "abstract": "We present an efficient document representation learning framework, Document Vector through Corruption (Doc2VecC). Doc2VecC represents each document as a simple average of word embeddings. It ensures a representation generated as such captures the semantic meanings of the document during learning. A corruption model is included, which introduces a data-dependent regularization that favors informative or rare words while forcing the embeddings of common and non-discriminative ones to be close to zero. Doc2VecC produces significantly better word embeddings than Word2Vec. We compare Doc2VecC with several state-of-the-art document representation learning algorithms. The simple model architecture introduced by Doc2VecC matches or out-performs the state-of-the-art in generating high-quality document representations for sentiment analysis, document classification as well as semantic relatedness tasks. The simplicity of the model enables training on billions of words per hour on a single machine. At the same time, the model is very efficient in generating representations of unseen documents at test time.\n", "pdf": "/pdf/5e58d0d43f34525f995cc00fe57341537cb1b88e.pdf", "TL;DR": "a simple document representation learning framework that is very efficient to train and test", "paperhash": "chen|efficient_vector_representation_for_documents_through_corruption", "conflicts": ["wustl.edu"], "authors": ["Minmin Chen"], "authorids": ["m.chen@criteo.com"], "keywords": ["Natural language processing", "Deep learning", "Semi-Supervised Learning"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512535873, "id": "ICLR.cc/2017/conference/-/paper578/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper578/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper578/AnonReviewer2", "ICLR.cc/2017/conference/paper578/AnonReviewer1", "ICLR.cc/2017/conference/paper578/AnonReviewer3"], "reply": {"forum": "B1Igu2ogg", "replyto": "B1Igu2ogg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper578/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper578/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512535873}}}, {"tddate": null, "tmdate": 1482170438448, "tcdate": 1482138887575, "number": 5, "id": "BkxfBmS4l", "invitation": "ICLR.cc/2017/conference/-/paper578/public/comment", "forum": "B1Igu2ogg", "replyto": "rkslf-rVl", "signatures": ["~Minmin_Chen1"], "readers": ["everyone"], "writers": ["~Minmin_Chen1"], "content": {"title": "Thanks for your feedback", "comment": "Dear reviewer, thank you for your constructive feedback. Indeed our main goal is to come up with a simple and efficient framework for generating document representations. I would like to argue that simplicity does not deny originality. The reason we can simply average word embeddings at test time to form document representation is because of the new model architecture proposed, which represents documents with corrupted average of word embeddings at learning time, and learns the document embedding with word embeddings together. The corruption at learning time enables fast learning (comparing to [2][3]), as well as a data-dependent regularization. As far as I know, it is the first do so. \n\nI believe the paper contains quite thorough analyses of the proposed work on the sentiment analysis and document classification tasks. I would like to see the community start exploring and benefiting from this simple idea, while we work on testing it on more tasks. \n\nFor RNN-LM, we used the implementation provided by the author, which was tested on the same dataset in their 2015 ICLR submission [1]. It builds two language models, one for the positive class and one for negative. It then computes the probability of each LM generating the document and assigns the one with higher score as the prediction. \n\nI included skip-thought vectors as another baseline in the manuscript thanks to the feedback from another reviewer. The encoder and decoder in the method are constructed from gated RNN. The method produces two models, uni-skip and bi-skip. Among them, the bi-skip is a bi-directional model that generates one forward and one backward encoding of the document.  Its performance is not satisfactory on this dataset, and it takes long time to test due to the high-dimensional encoders used. \n\nAgain thanks for your feedback and please let me know if you have other questions. \n\n[1] Mesnil, Gr\u00e9goire, et al. \"Ensemble of generative and discriminative techniques for sentiment analysis of movie reviews.\" arXiv preprint arXiv:1412.5335 (2014).\n[2] Eric H Huang, Richard Socher, Christopher D Manning, and Andrew Y Ng. Improving word representations via global context and multiple word prototypes. In ACL, pp. 873\u2013882, 2012.\n[3] Lebret, R\u00e9mi, and Ronan Collobert. \"\" The Sum of Its Parts\": Joint Learning of Word and Phrase Representations with Autoencoders.\" arXiv preprint arXiv:1506.05703 (2015)."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Vector Representation for Documents through Corruption", "abstract": "We present an efficient document representation learning framework, Document Vector through Corruption (Doc2VecC). Doc2VecC represents each document as a simple average of word embeddings. It ensures a representation generated as such captures the semantic meanings of the document during learning. A corruption model is included, which introduces a data-dependent regularization that favors informative or rare words while forcing the embeddings of common and non-discriminative ones to be close to zero. Doc2VecC produces significantly better word embeddings than Word2Vec. We compare Doc2VecC with several state-of-the-art document representation learning algorithms. The simple model architecture introduced by Doc2VecC matches or out-performs the state-of-the-art in generating high-quality document representations for sentiment analysis, document classification as well as semantic relatedness tasks. The simplicity of the model enables training on billions of words per hour on a single machine. At the same time, the model is very efficient in generating representations of unseen documents at test time.\n", "pdf": "/pdf/5e58d0d43f34525f995cc00fe57341537cb1b88e.pdf", "TL;DR": "a simple document representation learning framework that is very efficient to train and test", "paperhash": "chen|efficient_vector_representation_for_documents_through_corruption", "conflicts": ["wustl.edu"], "authors": ["Minmin Chen"], "authorids": ["m.chen@criteo.com"], "keywords": ["Natural language processing", "Deep learning", "Semi-Supervised Learning"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287514318, "id": "ICLR.cc/2017/conference/-/paper578/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1Igu2ogg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper578/reviewers", "ICLR.cc/2017/conference/paper578/areachairs"], "cdate": 1485287514318}}}, {"tddate": null, "tmdate": 1482045512977, "tcdate": 1482033547984, "number": 4, "id": "r1NcFF74e", "invitation": "ICLR.cc/2017/conference/-/paper578/public/comment", "forum": "B1Igu2ogg", "replyto": "rk1DcoWVx", "signatures": ["~Minmin_Chen1"], "readers": ["everyone"], "writers": ["~Minmin_Chen1"], "content": {"title": "Skip-thought Vectors", "comment": "Dear reader, thank you for your suggestion. I updated the manuscript to include skip-thought vector as a baseline in the IMDB movie review dataset. It performs surprisingly poor on this dataset comparing to other methods. We hypothesize that it is due to the difference between the dataset used to train the model and the one we are testing it on. We notice that the paragraphs in the movie review dataset are much longer than the sentences in the book corpus dataset, as well as the other datasets experimented in the original paper.  As pointed out in the original paper, learning the representations, even on small datasets, are likely to out-perform a generic unsupervised representation learned on much bigger datasets. \n\nDue to the high-dimensional encoders employed in the method to generate generic representations, Skip-thought is not very efficient in testing, which is an important factor we would like to address with our method. We will try to benchmark against more baselines in follow up works.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Vector Representation for Documents through Corruption", "abstract": "We present an efficient document representation learning framework, Document Vector through Corruption (Doc2VecC). Doc2VecC represents each document as a simple average of word embeddings. It ensures a representation generated as such captures the semantic meanings of the document during learning. A corruption model is included, which introduces a data-dependent regularization that favors informative or rare words while forcing the embeddings of common and non-discriminative ones to be close to zero. Doc2VecC produces significantly better word embeddings than Word2Vec. We compare Doc2VecC with several state-of-the-art document representation learning algorithms. The simple model architecture introduced by Doc2VecC matches or out-performs the state-of-the-art in generating high-quality document representations for sentiment analysis, document classification as well as semantic relatedness tasks. The simplicity of the model enables training on billions of words per hour on a single machine. At the same time, the model is very efficient in generating representations of unseen documents at test time.\n", "pdf": "/pdf/5e58d0d43f34525f995cc00fe57341537cb1b88e.pdf", "TL;DR": "a simple document representation learning framework that is very efficient to train and test", "paperhash": "chen|efficient_vector_representation_for_documents_through_corruption", "conflicts": ["wustl.edu"], "authors": ["Minmin Chen"], "authorids": ["m.chen@criteo.com"], "keywords": ["Natural language processing", "Deep learning", "Semi-Supervised Learning"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287514318, "id": "ICLR.cc/2017/conference/-/paper578/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1Igu2ogg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper578/reviewers", "ICLR.cc/2017/conference/paper578/areachairs"], "cdate": 1485287514318}}}, {"tddate": null, "tmdate": 1481910870844, "tcdate": 1481910870844, "number": 3, "id": "rk1DcoWVx", "invitation": "ICLR.cc/2017/conference/-/paper578/public/comment", "forum": "B1Igu2ogg", "replyto": "B1Igu2ogg", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "Need more recent baselines", "comment": "Unsupervised document representations is an active area of research, so it would be useful to benchmark against something more recent than doc2vec, which was in ICML 2014. Skip-thought vectors, in particular, should really be included."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Vector Representation for Documents through Corruption", "abstract": "We present an efficient document representation learning framework, Document Vector through Corruption (Doc2VecC). Doc2VecC represents each document as a simple average of word embeddings. It ensures a representation generated as such captures the semantic meanings of the document during learning. A corruption model is included, which introduces a data-dependent regularization that favors informative or rare words while forcing the embeddings of common and non-discriminative ones to be close to zero. Doc2VecC produces significantly better word embeddings than Word2Vec. We compare Doc2VecC with several state-of-the-art document representation learning algorithms. The simple model architecture introduced by Doc2VecC matches or out-performs the state-of-the-art in generating high-quality document representations for sentiment analysis, document classification as well as semantic relatedness tasks. The simplicity of the model enables training on billions of words per hour on a single machine. At the same time, the model is very efficient in generating representations of unseen documents at test time.\n", "pdf": "/pdf/5e58d0d43f34525f995cc00fe57341537cb1b88e.pdf", "TL;DR": "a simple document representation learning framework that is very efficient to train and test", "paperhash": "chen|efficient_vector_representation_for_documents_through_corruption", "conflicts": ["wustl.edu"], "authors": ["Minmin Chen"], "authorids": ["m.chen@criteo.com"], "keywords": ["Natural language processing", "Deep learning", "Semi-Supervised Learning"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287514318, "id": "ICLR.cc/2017/conference/-/paper578/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1Igu2ogg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper578/reviewers", "ICLR.cc/2017/conference/paper578/areachairs"], "cdate": 1485287514318}}}, {"tddate": null, "tmdate": 1481902605188, "tcdate": 1481902605188, "number": 1, "id": "rJBM9YbVg", "invitation": "ICLR.cc/2017/conference/-/paper578/official/review", "forum": "B1Igu2ogg", "replyto": "B1Igu2ogg", "signatures": ["ICLR.cc/2017/conference/paper578/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper578/AnonReviewer2"], "content": {"title": "Interesting corruption mechanism for document representation", "rating": "7: Good paper, accept", "review": "This paper proposes learning document embeddings as a sum of the constituent word embeddings, which are jointly learned and randomly dropped out ('corrupted') during training. While none of the pieces of this model are particularly novel, the result is an efficient learning algorithm for document representation with good empirical performance.\n\nJoint training of word and document embeddings is not a new idea, nor is the idea of enforcing the document to be represented by the sum of its word embeddings (see, e.g. '\u201cThe Sum of Its Parts\u201d: Joint Learning of Word and Phrase Representations with Autoencoders' by Lebret and Collobert). Furthermore, the corruption mechanism is nothing other than traditional dropout on the input layer. Coupled with the word2vec-style loss and training methods, this paper offers little on the novelty front.\n\nOn the other hand, it is very efficient at generation time, requiring only an average of the word embeddings rather than a complicated inference step as in Doc2Vec. Moreover, by construction, the embedding captures salient global information about the document -- it captures specifically that information that aids in local-context prediction. For such a simple model, the performance on sentiment analysis and document classification is quite encouraging.\n\nOverall, despite the lack of novelty, the simplicity, efficiency, and performance of this model make it worthy of wider readership and study, and I recommend acceptance.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Vector Representation for Documents through Corruption", "abstract": "We present an efficient document representation learning framework, Document Vector through Corruption (Doc2VecC). Doc2VecC represents each document as a simple average of word embeddings. It ensures a representation generated as such captures the semantic meanings of the document during learning. A corruption model is included, which introduces a data-dependent regularization that favors informative or rare words while forcing the embeddings of common and non-discriminative ones to be close to zero. Doc2VecC produces significantly better word embeddings than Word2Vec. We compare Doc2VecC with several state-of-the-art document representation learning algorithms. The simple model architecture introduced by Doc2VecC matches or out-performs the state-of-the-art in generating high-quality document representations for sentiment analysis, document classification as well as semantic relatedness tasks. The simplicity of the model enables training on billions of words per hour on a single machine. At the same time, the model is very efficient in generating representations of unseen documents at test time.\n", "pdf": "/pdf/5e58d0d43f34525f995cc00fe57341537cb1b88e.pdf", "TL;DR": "a simple document representation learning framework that is very efficient to train and test", "paperhash": "chen|efficient_vector_representation_for_documents_through_corruption", "conflicts": ["wustl.edu"], "authors": ["Minmin Chen"], "authorids": ["m.chen@criteo.com"], "keywords": ["Natural language processing", "Deep learning", "Semi-Supervised Learning"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512535873, "id": "ICLR.cc/2017/conference/-/paper578/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper578/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper578/AnonReviewer2", "ICLR.cc/2017/conference/paper578/AnonReviewer1", "ICLR.cc/2017/conference/paper578/AnonReviewer3"], "reply": {"forum": "B1Igu2ogg", "replyto": "B1Igu2ogg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper578/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper578/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512535873}}}, {"tddate": null, "tmdate": 1480844076285, "tcdate": 1480844076281, "number": 2, "id": "B14NQP-Xx", "invitation": "ICLR.cc/2017/conference/-/paper578/public/comment", "forum": "B1Igu2ogg", "replyto": "H1yJ5ByQx", "signatures": ["~Minmin_Chen1"], "readers": ["everyone"], "writers": ["~Minmin_Chen1"], "content": {"title": "Effect of document length on resulting embeddings", "comment": "Thank you for your questions. Indeed, the norm of the learned document embedding decreases with document length. I added a figure in the appendix to demonstrate this effect on the embeddings learned on the Imdb dataset. I did not find the performance in particular depends on the length of the document. The classification error on the different document length buckets are comparable. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Vector Representation for Documents through Corruption", "abstract": "We present an efficient document representation learning framework, Document Vector through Corruption (Doc2VecC). Doc2VecC represents each document as a simple average of word embeddings. It ensures a representation generated as such captures the semantic meanings of the document during learning. A corruption model is included, which introduces a data-dependent regularization that favors informative or rare words while forcing the embeddings of common and non-discriminative ones to be close to zero. Doc2VecC produces significantly better word embeddings than Word2Vec. We compare Doc2VecC with several state-of-the-art document representation learning algorithms. The simple model architecture introduced by Doc2VecC matches or out-performs the state-of-the-art in generating high-quality document representations for sentiment analysis, document classification as well as semantic relatedness tasks. The simplicity of the model enables training on billions of words per hour on a single machine. At the same time, the model is very efficient in generating representations of unseen documents at test time.\n", "pdf": "/pdf/5e58d0d43f34525f995cc00fe57341537cb1b88e.pdf", "TL;DR": "a simple document representation learning framework that is very efficient to train and test", "paperhash": "chen|efficient_vector_representation_for_documents_through_corruption", "conflicts": ["wustl.edu"], "authors": ["Minmin Chen"], "authorids": ["m.chen@criteo.com"], "keywords": ["Natural language processing", "Deep learning", "Semi-Supervised Learning"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287514318, "id": "ICLR.cc/2017/conference/-/paper578/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1Igu2ogg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper578/reviewers", "ICLR.cc/2017/conference/paper578/areachairs"], "cdate": 1485287514318}}}, {"tddate": null, "tmdate": 1480706519057, "tcdate": 1480706519053, "number": 2, "id": "H1yJ5ByQx", "invitation": "ICLR.cc/2017/conference/-/paper578/pre-review/question", "forum": "B1Igu2ogg", "replyto": "B1Igu2ogg", "signatures": ["ICLR.cc/2017/conference/paper578/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper578/AnonReviewer2"], "content": {"title": "Effect of document length", "question": "Have you analyzed the effect of document length on the resulting embeddings, e.g. does the norm decrease with length? Does performance depend strongly on document length?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Vector Representation for Documents through Corruption", "abstract": "We present an efficient document representation learning framework, Document Vector through Corruption (Doc2VecC). Doc2VecC represents each document as a simple average of word embeddings. It ensures a representation generated as such captures the semantic meanings of the document during learning. A corruption model is included, which introduces a data-dependent regularization that favors informative or rare words while forcing the embeddings of common and non-discriminative ones to be close to zero. Doc2VecC produces significantly better word embeddings than Word2Vec. We compare Doc2VecC with several state-of-the-art document representation learning algorithms. The simple model architecture introduced by Doc2VecC matches or out-performs the state-of-the-art in generating high-quality document representations for sentiment analysis, document classification as well as semantic relatedness tasks. The simplicity of the model enables training on billions of words per hour on a single machine. At the same time, the model is very efficient in generating representations of unseen documents at test time.\n", "pdf": "/pdf/5e58d0d43f34525f995cc00fe57341537cb1b88e.pdf", "TL;DR": "a simple document representation learning framework that is very efficient to train and test", "paperhash": "chen|efficient_vector_representation_for_documents_through_corruption", "conflicts": ["wustl.edu"], "authors": ["Minmin Chen"], "authorids": ["m.chen@criteo.com"], "keywords": ["Natural language processing", "Deep learning", "Semi-Supervised Learning"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959205197, "id": "ICLR.cc/2017/conference/-/paper578/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper578/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper578/AnonReviewer1", "ICLR.cc/2017/conference/paper578/AnonReviewer2"], "reply": {"forum": "B1Igu2ogg", "replyto": "B1Igu2ogg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper578/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper578/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959205197}}}, {"tddate": null, "tmdate": 1480655539164, "tcdate": 1480655539159, "number": 1, "id": "HyohGtRfg", "invitation": "ICLR.cc/2017/conference/-/paper578/public/comment", "forum": "B1Igu2ogg", "replyto": "BySwY1AGl", "signatures": ["~Minmin_Chen1"], "readers": ["everyone"], "writers": ["~Minmin_Chen1"], "content": {"title": "About experiment setup", "comment": "Thank you for your questions. For both tasks, a bigger unlabeled set is used to learn representation for all the representation learning methods.  A classifier is then trained using the learned representation on the training data to perform either sentiment analysis or document classification. \n\nRNN-LM builds one language model  per class, each one taking long time to train. Its performance is not as competent as the others in the sentiment analysis task. Due to time limit, I omitted this method for the multi-class document classification task. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Vector Representation for Documents through Corruption", "abstract": "We present an efficient document representation learning framework, Document Vector through Corruption (Doc2VecC). Doc2VecC represents each document as a simple average of word embeddings. It ensures a representation generated as such captures the semantic meanings of the document during learning. A corruption model is included, which introduces a data-dependent regularization that favors informative or rare words while forcing the embeddings of common and non-discriminative ones to be close to zero. Doc2VecC produces significantly better word embeddings than Word2Vec. We compare Doc2VecC with several state-of-the-art document representation learning algorithms. The simple model architecture introduced by Doc2VecC matches or out-performs the state-of-the-art in generating high-quality document representations for sentiment analysis, document classification as well as semantic relatedness tasks. The simplicity of the model enables training on billions of words per hour on a single machine. At the same time, the model is very efficient in generating representations of unseen documents at test time.\n", "pdf": "/pdf/5e58d0d43f34525f995cc00fe57341537cb1b88e.pdf", "TL;DR": "a simple document representation learning framework that is very efficient to train and test", "paperhash": "chen|efficient_vector_representation_for_documents_through_corruption", "conflicts": ["wustl.edu"], "authors": ["Minmin Chen"], "authorids": ["m.chen@criteo.com"], "keywords": ["Natural language processing", "Deep learning", "Semi-Supervised Learning"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287514318, "id": "ICLR.cc/2017/conference/-/paper578/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1Igu2ogg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper578/reviewers", "ICLR.cc/2017/conference/paper578/areachairs"], "cdate": 1485287514318}}}, {"tddate": null, "tmdate": 1480616285327, "tcdate": 1480616285323, "number": 1, "id": "BySwY1AGl", "invitation": "ICLR.cc/2017/conference/-/paper578/pre-review/question", "forum": "B1Igu2ogg", "replyto": "B1Igu2ogg", "signatures": ["ICLR.cc/2017/conference/paper578/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper578/AnonReviewer1"], "content": {"title": "pre review questions", "question": "- Were the word embeddings for the Word2Vec+ baselines only trained on the training data? Did you try training them on a much bigger (unlabeled) corpus?\n- Why did you exclude the RNN-LM baseline for the document classification task?\n\nThanks!"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Vector Representation for Documents through Corruption", "abstract": "We present an efficient document representation learning framework, Document Vector through Corruption (Doc2VecC). Doc2VecC represents each document as a simple average of word embeddings. It ensures a representation generated as such captures the semantic meanings of the document during learning. A corruption model is included, which introduces a data-dependent regularization that favors informative or rare words while forcing the embeddings of common and non-discriminative ones to be close to zero. Doc2VecC produces significantly better word embeddings than Word2Vec. We compare Doc2VecC with several state-of-the-art document representation learning algorithms. The simple model architecture introduced by Doc2VecC matches or out-performs the state-of-the-art in generating high-quality document representations for sentiment analysis, document classification as well as semantic relatedness tasks. The simplicity of the model enables training on billions of words per hour on a single machine. At the same time, the model is very efficient in generating representations of unseen documents at test time.\n", "pdf": "/pdf/5e58d0d43f34525f995cc00fe57341537cb1b88e.pdf", "TL;DR": "a simple document representation learning framework that is very efficient to train and test", "paperhash": "chen|efficient_vector_representation_for_documents_through_corruption", "conflicts": ["wustl.edu"], "authors": ["Minmin Chen"], "authorids": ["m.chen@criteo.com"], "keywords": ["Natural language processing", "Deep learning", "Semi-Supervised Learning"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959205197, "id": "ICLR.cc/2017/conference/-/paper578/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper578/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper578/AnonReviewer1", "ICLR.cc/2017/conference/paper578/AnonReviewer2"], "reply": {"forum": "B1Igu2ogg", "replyto": "B1Igu2ogg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper578/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper578/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959205197}}}], "count": 21}