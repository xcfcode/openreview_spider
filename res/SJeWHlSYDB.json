{"notes": [{"id": "SJeWHlSYDB", "original": "rJlZ-ugKPS", "number": 2274, "cdate": 1569439800897, "ddate": null, "tcdate": 1569439800897, "tmdate": 1577168252524, "tddate": null, "forum": "SJeWHlSYDB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["mingtian.zhang.17@ucl.ac.uk", "david.barber@ucl.ac.uk", "thomas.bird.17@ucl.ac.uk", "peter.hayes.15@ucl.ac.uk", "r.habib@cs.ucl.ac.uk"], "title": "SPREAD  DIVERGENCE", "authors": ["Mingtian Zhang", "David Barber", "Thomas Bird", "Peter Hayes", "Raza Habib"], "pdf": "/pdf/5dc6531d109da0247305d2e70cce0c493988ed7e.pdf", "TL;DR": "A new divergence family dealing with distributions with different supports for training implicit generative models.", "abstract": "For distributions $p$ and $q$ with different supports, the divergence $\\div{p}{q}$ may not exist. We define a spread divergence $\\sdiv{p}{q}$ on modified $p$ and $q$ and describe sufficient conditions for the existence of such a divergence. We demonstrate how to maximize the discriminatory power of a given divergence by parameterizing and learning the spread. We also give examples of using a spread divergence to train and improve implicit generative models, including linear models (Independent Components Analysis) and non-linear models (Deep Generative Networks).", "code": "https://drive.google.com/file/d/1p6l7J1HpcNTV1RrF12wwCza-98m1J8di/view?usp=sharing", "keywords": ["divergence minimization", "generative model", "variational inference"], "paperhash": "zhang|spread_divergence", "original_pdf": "/attachment/ccb736cf3932d708f10cba9149fe80b8069a9f14.pdf", "_bibtex": "@misc{\nzhang2020spread,\ntitle={{\\{}SPREAD{\\}}  {\\{}DIVERGENCE{\\}}},\nauthor={Mingtian Zhang and David Barber and Thomas Bird and Peter Hayes and Raza Habib},\nyear={2020},\nurl={https://openreview.net/forum?id=SJeWHlSYDB}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "s51GZ-g4ee", "original": null, "number": 1, "cdate": 1576798744969, "ddate": null, "tcdate": 1576798744969, "tmdate": 1576800891180, "tddate": null, "forum": "SJeWHlSYDB", "replyto": "SJeWHlSYDB", "invitation": "ICLR.cc/2020/Conference/Paper2274/-/Decision", "content": {"decision": "Reject", "comment": "This paper studies spread divergence between distributions, which may exist in settings where the divergence between said distributions does not. The reviewers feel this work does not have sufficient technical novelty to merit acceptance at this time.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["mingtian.zhang.17@ucl.ac.uk", "david.barber@ucl.ac.uk", "thomas.bird.17@ucl.ac.uk", "peter.hayes.15@ucl.ac.uk", "r.habib@cs.ucl.ac.uk"], "title": "SPREAD  DIVERGENCE", "authors": ["Mingtian Zhang", "David Barber", "Thomas Bird", "Peter Hayes", "Raza Habib"], "pdf": "/pdf/5dc6531d109da0247305d2e70cce0c493988ed7e.pdf", "TL;DR": "A new divergence family dealing with distributions with different supports for training implicit generative models.", "abstract": "For distributions $p$ and $q$ with different supports, the divergence $\\div{p}{q}$ may not exist. We define a spread divergence $\\sdiv{p}{q}$ on modified $p$ and $q$ and describe sufficient conditions for the existence of such a divergence. We demonstrate how to maximize the discriminatory power of a given divergence by parameterizing and learning the spread. We also give examples of using a spread divergence to train and improve implicit generative models, including linear models (Independent Components Analysis) and non-linear models (Deep Generative Networks).", "code": "https://drive.google.com/file/d/1p6l7J1HpcNTV1RrF12wwCza-98m1J8di/view?usp=sharing", "keywords": ["divergence minimization", "generative model", "variational inference"], "paperhash": "zhang|spread_divergence", "original_pdf": "/attachment/ccb736cf3932d708f10cba9149fe80b8069a9f14.pdf", "_bibtex": "@misc{\nzhang2020spread,\ntitle={{\\{}SPREAD{\\}}  {\\{}DIVERGENCE{\\}}},\nauthor={Mingtian Zhang and David Barber and Thomas Bird and Peter Hayes and Raza Habib},\nyear={2020},\nurl={https://openreview.net/forum?id=SJeWHlSYDB}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "SJeWHlSYDB", "replyto": "SJeWHlSYDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795715544, "tmdate": 1576800265481, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2274/-/Decision"}}}, {"id": "rkeamfp9iB", "original": null, "number": 3, "cdate": 1573732900842, "ddate": null, "tcdate": 1573732900842, "tmdate": 1573827622595, "tddate": null, "forum": "SJeWHlSYDB", "replyto": "SJlHMmjiKH", "invitation": "ICLR.cc/2020/Conference/Paper2274/-/Official_Comment", "content": {"title": "Response to Reviewer 2", "comment": "We thank the reviewer for valuable reviews. We believe that we could fully address the concerns with the following arguments.\n\n1. \"The issues about JS:\nIn the case of two distributions that have disjoint support, the JS divergence is always a finite constant. We think this is ill-defined since it is not a valid measure of divergence between two distributions and is not useful for statistical inference.  It is a common belief for vanilla GAN (with JS divergence)  that the source of instability during training is due to the disjoint support of the two distributions. Other distances are proposed to mitigate this effect [1].\n\n2. \"compare with a mixture of noise\"\nIn the discrete noise case, it is known as the anti-freeze method [2]. It is a special case of spread divergence. As noted in section 2.1; because the linear operator is equivalent to the convolution. \nHowever, in the continuous case mixture is different than spread; we can see this in the MLE setting, where if the data distribution is a delta function on the data points, the density of the resulting mixture at that point is still infinite (infinity times a constant is still infinity) so the MLE is still ill-defined, whereas the density of the spreaded distribution (with convolution noise) at that point is finite, (delta distribution will become a gaussian distribution). Therefore, we argue that the spread divergence is superior. \n\n3. \"Does spreading introduce spurious modes? \"\nIn the discrete case, spread noise will add density to the area that has 0 probability, so it will create local modes and it is necessary to define a valid spread divergence (but not spurious).  Therefore, below we answer to \u201cdoes the spread noise introduce spurious local modes in the continuous case?\u201d\n\nNo. For stationary noise (proposed in our paper), it will never introduce additional modes.\nWe assume a density function f  is differentiable in neighbourhood of the local mode. Since the mode is the local stationary point of the density function, $f\u2019=0$ in the mode position. (This can be generalized to the case that  $f$ is locally continuous in the local mode, so there exits a sub derivative which equals to 0.)\nTo define a spread divergence, we convolve f by a stationary noise $g$ ($g>0$ everywhere by the requirement of spread divergence).  The derivative of the convoluted distribution is given by $(g*f)\u2019=g*f\u2019$  (differentiation property of convolution, $*$ means convolution here).  Since g>0 everywhere, so $g*f\u2019=0$ if and only if $f\u2019=0$ , therefore spread noise will never introduce modes. However, other noise such as \u201cmixture noise\u201d $(1-\\epsilon)p+\\epsilon noise$ may potentially introduce additional modes.\n\n4. \"does it change distribution sufficiency?\"\nNo. The spread noise family introduced in the paper is a bijective operator (one to one mapping). Therefore, according to Fisher-Neymann theorem, it will not change the sufficiency of data statistics.\n\n5. \"statistical properties of using spread KL and potential applications\"\nThe statistical properties for inference for spread MLE are discussed in section 5.1 and appendix E.1. Comparing to KL, spread KL maintains the asymptotic efficiency and consistency properties, but only needs weaker conditions. \nFor potential ML applications;  We have added a discussion in the end of section 5.2.1 of the revised paper.\n\n6. \"non-convolutional spreading\"\nWe agree that non-convolutional spreading noise is interesting.  However, this cannot be implemented easily for continuous systems in cases where we cannot evaluate explicitly the likelihood of the model. This means that one cannot directly use that method to train continuous implicit models using a modified EM approach. We will leave the analysis of non-convolutional spreading to be future work. \n\n\n7. \" Any principles that can guide this optimization rather than black-box optimization?\"\nOptimization of the spread noise hyperparameters is not necessary for simple problems - see section 5.2.1, we achieve significant improvement using spread divergence using a fixed spread distribution.\nWe agree in higher dimensional, more difficult problems learning the spread noise can improve performance significantly. \nWe provide a principled method in section 4 to learn the spread distribution in an online fashion that maximises the discriminatory power, which we do not consider a black-box optimisation technique. Similar techniques are widely used in the kernel domain (MMD). \n\n8. \"missing denominator of $\\sigma^2$\"\nThanks for pointing out the small error. We have added the assumption $\\sigma^2=0.5$ within the revised paper.\n\n9. \"definition of TV distance\"\nOur definition of TV was up to a constant; we have clarified within the revised paper.\n\n10. We thank the reviewer for pointing out the typos, which we have fixed.\n\n[1] M. Arjovsky et al.  https://arxiv.org/abs/1701.07875\n[2]  T. Furmston, D. Barber, https://pdfs.semanticscholar.org/2ab5/475f67f5bdb6d4e411b8d7f3c56185b51847.pdf "}, "signatures": ["ICLR.cc/2020/Conference/Paper2274/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2274/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["mingtian.zhang.17@ucl.ac.uk", "david.barber@ucl.ac.uk", "thomas.bird.17@ucl.ac.uk", "peter.hayes.15@ucl.ac.uk", "r.habib@cs.ucl.ac.uk"], "title": "SPREAD  DIVERGENCE", "authors": ["Mingtian Zhang", "David Barber", "Thomas Bird", "Peter Hayes", "Raza Habib"], "pdf": "/pdf/5dc6531d109da0247305d2e70cce0c493988ed7e.pdf", "TL;DR": "A new divergence family dealing with distributions with different supports for training implicit generative models.", "abstract": "For distributions $p$ and $q$ with different supports, the divergence $\\div{p}{q}$ may not exist. We define a spread divergence $\\sdiv{p}{q}$ on modified $p$ and $q$ and describe sufficient conditions for the existence of such a divergence. We demonstrate how to maximize the discriminatory power of a given divergence by parameterizing and learning the spread. We also give examples of using a spread divergence to train and improve implicit generative models, including linear models (Independent Components Analysis) and non-linear models (Deep Generative Networks).", "code": "https://drive.google.com/file/d/1p6l7J1HpcNTV1RrF12wwCza-98m1J8di/view?usp=sharing", "keywords": ["divergence minimization", "generative model", "variational inference"], "paperhash": "zhang|spread_divergence", "original_pdf": "/attachment/ccb736cf3932d708f10cba9149fe80b8069a9f14.pdf", "_bibtex": "@misc{\nzhang2020spread,\ntitle={{\\{}SPREAD{\\}}  {\\{}DIVERGENCE{\\}}},\nauthor={Mingtian Zhang and David Barber and Thomas Bird and Peter Hayes and Raza Habib},\nyear={2020},\nurl={https://openreview.net/forum?id=SJeWHlSYDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJeWHlSYDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2274/Authors", "ICLR.cc/2020/Conference/Paper2274/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2274/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2274/Reviewers", "ICLR.cc/2020/Conference/Paper2274/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2274/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2274/Authors|ICLR.cc/2020/Conference/Paper2274/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504143783, "tmdate": 1576860546350, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2274/Authors", "ICLR.cc/2020/Conference/Paper2274/Reviewers", "ICLR.cc/2020/Conference/Paper2274/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2274/-/Official_Comment"}}}, {"id": "H1xiH26qiB", "original": null, "number": 5, "cdate": 1573735490821, "ddate": null, "tcdate": 1573735490821, "tmdate": 1573735742242, "tddate": null, "forum": "SJeWHlSYDB", "replyto": "BJe9q6g09r", "invitation": "ICLR.cc/2020/Conference/Paper2274/-/Official_Comment", "content": {"title": "Response to Reviewer 3", "comment": "We thank the reviewer for valuable reviews. We believe that we could fully address the concerns with the following arguments.\n\n1. In the case where the KL divergence is infinity between two probability models, it is not well defined in the sense it cannot be used for statistical inference. We illustrate this point in section 2 with the delta distribution example; the KL divergence will be infinity regardless of the location of the two distributions.\n\n2.  Re the lack of theoretical comparison - the focus of the paper is on introducing a new divergence objective for applications of interest where distributions p and q have different support. From a theory perspective, we analyse how it relates to the widely used KL divergence as follows: In section 2, 3, we demonstrate how to augment the KL-divergence to produce the spread KL and under what sufficient conditions it is a valid divergence and will recover the true solution. \nIn the MLE case, we compare and contrast KL and spread KL (see section 5.1 and appendix E). Our initial conclusion is that spread-KL preserves the favourable properties of MLE, under weaker conditions. Additional supporting theory can be future work and we believe we have provided a solid foundation, worthy of publication.\n\nRe the focus of the applications; situations where the likelihood is not defined, i.e., data are deterministic w/o observation noise - we argue that section 5.2.1, related to ICA, demonstrates exactly the comparison between spread divergence and the traditional MLE based EM algorithm, and how the performance is related to the amount of observation noise present. In figure 1. a, when observation noise is small, relative error explodes on the traditional methods (due to slow down/freezing), whereas our spread divergence method is unaffected.\n\nThe set of problems of interest where the likelihood is not defined is large. In applications like deterministic ICA, maximum likelihood slow feature analysis, deterministic POMDP - EM cannot work effectively and heuristics exist with no guarantees. We believe the spread divergence provides a general framework which can be used to solve these problems in an elegant way.\n\n3. The optimal choice of spread noise (kernel) will depend on the task. Hence, no, a Gaussian noise is not necessarily an optimal choice in general case.\n\nThis choice is a hyper-parameter in our method - and setting parameters for a given choice of spread noise during training. We are restricted on which family of distributions we can use given our analysis in section 3 depends on the stationary characteristic for proving spread is a valid divergence. A more exhaustive ablation of noise choice is future work, for which we provide a solid foundation:\n\nWe propose Gaussian and Laplace noise as convenient choices, which satisfy the stationary requirement and in practice were well behaved within our experiments. Empirically we compare the effectiveness of the Gaussian and Laplace choices on performance within section 5.2.2, where, qualitatively, we conclude that Laplace is a better choice (evidence to support that the Gaussian noise is a sub-optimal choice there).  See paper for intuition provided.\n\nFor how to set the parameters for a given choice of spread noise, we provide a general strategy, which enables learning towards an optimal spread noise. The strategy is to maximize the power of a statistical test (or measure). For example, in the case of the delta-VAE in section 5.2.2, we maximise the discriminatory power of the spread divergence online wrt to the noise parameters, improving performance. Two complementary strategies are the Mean Transform and the Covariance Structure learning presented for the Gaussian case. These can be extended to other distributions, such as the Laplace, which have similar scale and location parameters.\n\n4.  The same conclusion from the ICA experiment should hold for other problems where the slow down/freezing behaviour is present when an EM style likelihood approach is taken (even with the trick of adding small observation noise). This is caused when the posterior is not updated within the E-step. Other examples that we are aware of where this problematic phenomenon is present are policy learning in deterministic MDP/POMDP [2], maximum likelihood learning of SFA [slow feature analysis [3] (and other probabilistic matrix decomposition techniques). All of which are good candidates for a spread divergence application. Furthermore, the observation noise tricks of existing methods do not guarantee to recover the true data generating process, whereas our proposed spread divergence method can. We have added a point of clarification to section 5 of the paper.\n\n[1] M. Arjovsky et al., https://arxiv.org/abs/1701.07875\n[2] T. Furmston, D. Barber, https://pdfs.semanticscholar.org/2ab5/475f67f5bdb6d4e411b8d7f3c56185b51847.pdf\n[3] R. Turner et al.,http://www.gatsby.ucl.ac.uk/~turner/SFA/TSNCOMP2006v8.pdf"}, "signatures": ["ICLR.cc/2020/Conference/Paper2274/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2274/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["mingtian.zhang.17@ucl.ac.uk", "david.barber@ucl.ac.uk", "thomas.bird.17@ucl.ac.uk", "peter.hayes.15@ucl.ac.uk", "r.habib@cs.ucl.ac.uk"], "title": "SPREAD  DIVERGENCE", "authors": ["Mingtian Zhang", "David Barber", "Thomas Bird", "Peter Hayes", "Raza Habib"], "pdf": "/pdf/5dc6531d109da0247305d2e70cce0c493988ed7e.pdf", "TL;DR": "A new divergence family dealing with distributions with different supports for training implicit generative models.", "abstract": "For distributions $p$ and $q$ with different supports, the divergence $\\div{p}{q}$ may not exist. We define a spread divergence $\\sdiv{p}{q}$ on modified $p$ and $q$ and describe sufficient conditions for the existence of such a divergence. We demonstrate how to maximize the discriminatory power of a given divergence by parameterizing and learning the spread. We also give examples of using a spread divergence to train and improve implicit generative models, including linear models (Independent Components Analysis) and non-linear models (Deep Generative Networks).", "code": "https://drive.google.com/file/d/1p6l7J1HpcNTV1RrF12wwCza-98m1J8di/view?usp=sharing", "keywords": ["divergence minimization", "generative model", "variational inference"], "paperhash": "zhang|spread_divergence", "original_pdf": "/attachment/ccb736cf3932d708f10cba9149fe80b8069a9f14.pdf", "_bibtex": "@misc{\nzhang2020spread,\ntitle={{\\{}SPREAD{\\}}  {\\{}DIVERGENCE{\\}}},\nauthor={Mingtian Zhang and David Barber and Thomas Bird and Peter Hayes and Raza Habib},\nyear={2020},\nurl={https://openreview.net/forum?id=SJeWHlSYDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJeWHlSYDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2274/Authors", "ICLR.cc/2020/Conference/Paper2274/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2274/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2274/Reviewers", "ICLR.cc/2020/Conference/Paper2274/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2274/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2274/Authors|ICLR.cc/2020/Conference/Paper2274/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504143783, "tmdate": 1576860546350, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2274/Authors", "ICLR.cc/2020/Conference/Paper2274/Reviewers", "ICLR.cc/2020/Conference/Paper2274/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2274/-/Official_Comment"}}}, {"id": "BJlMtz3csB", "original": null, "number": 2, "cdate": 1573728889731, "ddate": null, "tcdate": 1573728889731, "tmdate": 1573729422832, "tddate": null, "forum": "SJeWHlSYDB", "replyto": "B1ex_oAT9H", "invitation": "ICLR.cc/2020/Conference/Paper2274/-/Official_Comment", "content": {"title": "Thank you for the positive review. The desk reject suggestion is unfortunate.", "comment": "Thank you for the positive review. The desk reject suggestion is unfortunate - in attempting to release the code in a timely fashion to support our submission, the GitHub repository link https://github.com/zmtomorrow/spread_divergence_public used was associated to a personal Github account. We argue that the Github user is relatively inactive and utilises an anonymous alias, therefore, it would be difficult to identify the researcher behind the account. We claim it is as difficult to identify the researcher/their affiliations, as trying to find the paper in the public domain (given some other papers are openly endorsed on twitter and arxiv during the review process). Given this, we hope it does not exclude our submission and we have provided a new anonymous link.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2274/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2274/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["mingtian.zhang.17@ucl.ac.uk", "david.barber@ucl.ac.uk", "thomas.bird.17@ucl.ac.uk", "peter.hayes.15@ucl.ac.uk", "r.habib@cs.ucl.ac.uk"], "title": "SPREAD  DIVERGENCE", "authors": ["Mingtian Zhang", "David Barber", "Thomas Bird", "Peter Hayes", "Raza Habib"], "pdf": "/pdf/5dc6531d109da0247305d2e70cce0c493988ed7e.pdf", "TL;DR": "A new divergence family dealing with distributions with different supports for training implicit generative models.", "abstract": "For distributions $p$ and $q$ with different supports, the divergence $\\div{p}{q}$ may not exist. We define a spread divergence $\\sdiv{p}{q}$ on modified $p$ and $q$ and describe sufficient conditions for the existence of such a divergence. We demonstrate how to maximize the discriminatory power of a given divergence by parameterizing and learning the spread. We also give examples of using a spread divergence to train and improve implicit generative models, including linear models (Independent Components Analysis) and non-linear models (Deep Generative Networks).", "code": "https://drive.google.com/file/d/1p6l7J1HpcNTV1RrF12wwCza-98m1J8di/view?usp=sharing", "keywords": ["divergence minimization", "generative model", "variational inference"], "paperhash": "zhang|spread_divergence", "original_pdf": "/attachment/ccb736cf3932d708f10cba9149fe80b8069a9f14.pdf", "_bibtex": "@misc{\nzhang2020spread,\ntitle={{\\{}SPREAD{\\}}  {\\{}DIVERGENCE{\\}}},\nauthor={Mingtian Zhang and David Barber and Thomas Bird and Peter Hayes and Raza Habib},\nyear={2020},\nurl={https://openreview.net/forum?id=SJeWHlSYDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJeWHlSYDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2274/Authors", "ICLR.cc/2020/Conference/Paper2274/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2274/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2274/Reviewers", "ICLR.cc/2020/Conference/Paper2274/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2274/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2274/Authors|ICLR.cc/2020/Conference/Paper2274/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504143783, "tmdate": 1576860546350, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2274/Authors", "ICLR.cc/2020/Conference/Paper2274/Reviewers", "ICLR.cc/2020/Conference/Paper2274/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2274/-/Official_Comment"}}}, {"id": "SJlHMmjiKH", "original": null, "number": 1, "cdate": 1571693325360, "ddate": null, "tcdate": 1571693325360, "tmdate": 1572972360268, "tddate": null, "forum": "SJeWHlSYDB", "replyto": "SJeWHlSYDB", "invitation": "ICLR.cc/2020/Conference/Paper2274/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The paper introduced a way to modify densities such that their support agrees and that the Kullback-Leibler divergence can be computed without diverging. Proof of concept of using the spread KL divergence to ICA and Deep Generative Models ($\\delta$-VAE) are reported based on the study of spread MLE.\n\nComments:\n\nIn Sec 1, mention that f should be strictly convex at 1. Also mention \nJensen-Shannon divergence, a KL symmetrization, which is always finite \nand used in GAN analysis.\n\nIn Sec 2, you can also choose to dilute the densities with a mixture: \n(1-\\epsilon)p+\\epsilon noise.\nExplain why spread is better than that? Does spreading introduce \nspurious modes?, does it change distribution sufficiency? \n(Fisher-Neymann thm)\n\nIn Formula 4, there is an error: missing denominator of \\sigma^2. See \nAppendix D too.\n\nIn footnote 4, page 8, missing a 1/2 factor in from of TV (that is upper \nbounded by 1 and not 2)\n\nKL is relative entropy= cross-entropy minus entropy. What about spread KL?\nIn general, what statistical properties are kept by using the spread? \n(or its convolution subcase?)\n\nIs spreading a trick that introduces a hyperparameter that can then be \noptimized for retaining discriminatory power, or is there\nsome deeper statistical theory to motivate it. I think spread MLE should \nbe further explored and detailed to other scenarii.\n\nSpreading can be done with convolution and in general by Eq.3:\n\nThen what is the theoretical interpretation of doing non-convolutional \nspreading?\n\n\nA drawback is that optimization on the spread noise hyperparameter is \nnecessary (Fig 3b is indeed much better than Fig 3a).\nIs there any first principles that can guide this optimization rather \nthan black-box optimization?\n\nOverall, it is a nice work but further statistical guiding principles \nor/and new ML applications of spread divergences/MLE will strengthen the \nwork.\nThe connection, if any, with Jensen-Shannon divergence shall be stated \nand explored.\n\nMinor comments:\n\nIn the abstract, state KL divergence instead of divergence because \nJensen-Shannon divergence exists always.\n\n\nTypos:\np. 6 boumd->bound\nBibliography : Cramir->Cramer, and various upper cases missing (eg. \nwasserstein ->Wasserstein)\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2274/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2274/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["mingtian.zhang.17@ucl.ac.uk", "david.barber@ucl.ac.uk", "thomas.bird.17@ucl.ac.uk", "peter.hayes.15@ucl.ac.uk", "r.habib@cs.ucl.ac.uk"], "title": "SPREAD  DIVERGENCE", "authors": ["Mingtian Zhang", "David Barber", "Thomas Bird", "Peter Hayes", "Raza Habib"], "pdf": "/pdf/5dc6531d109da0247305d2e70cce0c493988ed7e.pdf", "TL;DR": "A new divergence family dealing with distributions with different supports for training implicit generative models.", "abstract": "For distributions $p$ and $q$ with different supports, the divergence $\\div{p}{q}$ may not exist. We define a spread divergence $\\sdiv{p}{q}$ on modified $p$ and $q$ and describe sufficient conditions for the existence of such a divergence. We demonstrate how to maximize the discriminatory power of a given divergence by parameterizing and learning the spread. We also give examples of using a spread divergence to train and improve implicit generative models, including linear models (Independent Components Analysis) and non-linear models (Deep Generative Networks).", "code": "https://drive.google.com/file/d/1p6l7J1HpcNTV1RrF12wwCza-98m1J8di/view?usp=sharing", "keywords": ["divergence minimization", "generative model", "variational inference"], "paperhash": "zhang|spread_divergence", "original_pdf": "/attachment/ccb736cf3932d708f10cba9149fe80b8069a9f14.pdf", "_bibtex": "@misc{\nzhang2020spread,\ntitle={{\\{}SPREAD{\\}}  {\\{}DIVERGENCE{\\}}},\nauthor={Mingtian Zhang and David Barber and Thomas Bird and Peter Hayes and Raza Habib},\nyear={2020},\nurl={https://openreview.net/forum?id=SJeWHlSYDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SJeWHlSYDB", "replyto": "SJeWHlSYDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2274/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2274/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575500403770, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2274/Reviewers"], "noninvitees": [], "tcdate": 1570237725179, "tmdate": 1575500403785, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2274/-/Official_Review"}}}, {"id": "B1ex_oAT9H", "original": null, "number": 2, "cdate": 1572887400379, "ddate": null, "tcdate": 1572887400379, "tmdate": 1572972360231, "tddate": null, "forum": "SJeWHlSYDB", "replyto": "SJeWHlSYDB", "invitation": "ICLR.cc/2020/Conference/Paper2274/-/Official_Review", "content": {"rating": "1: Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "I think the paper must be Desk-rejected as the identity of the authors was revealed. \nThis thing aside, the paper is an interesting contribution. The concept of spread divergence can be valuable in many context. The presentation is thorough and the theoretical part is correct. On the other hand, the examples are quite diverse and include a standard model (ICA) as well as modern deep generative models. Thus, it represents a valuable contribution worth of publication, if we ignore the identity revelation aspect. "}, "signatures": ["ICLR.cc/2020/Conference/Paper2274/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2274/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["mingtian.zhang.17@ucl.ac.uk", "david.barber@ucl.ac.uk", "thomas.bird.17@ucl.ac.uk", "peter.hayes.15@ucl.ac.uk", "r.habib@cs.ucl.ac.uk"], "title": "SPREAD  DIVERGENCE", "authors": ["Mingtian Zhang", "David Barber", "Thomas Bird", "Peter Hayes", "Raza Habib"], "pdf": "/pdf/5dc6531d109da0247305d2e70cce0c493988ed7e.pdf", "TL;DR": "A new divergence family dealing with distributions with different supports for training implicit generative models.", "abstract": "For distributions $p$ and $q$ with different supports, the divergence $\\div{p}{q}$ may not exist. We define a spread divergence $\\sdiv{p}{q}$ on modified $p$ and $q$ and describe sufficient conditions for the existence of such a divergence. We demonstrate how to maximize the discriminatory power of a given divergence by parameterizing and learning the spread. We also give examples of using a spread divergence to train and improve implicit generative models, including linear models (Independent Components Analysis) and non-linear models (Deep Generative Networks).", "code": "https://drive.google.com/file/d/1p6l7J1HpcNTV1RrF12wwCza-98m1J8di/view?usp=sharing", "keywords": ["divergence minimization", "generative model", "variational inference"], "paperhash": "zhang|spread_divergence", "original_pdf": "/attachment/ccb736cf3932d708f10cba9149fe80b8069a9f14.pdf", "_bibtex": "@misc{\nzhang2020spread,\ntitle={{\\{}SPREAD{\\}}  {\\{}DIVERGENCE{\\}}},\nauthor={Mingtian Zhang and David Barber and Thomas Bird and Peter Hayes and Raza Habib},\nyear={2020},\nurl={https://openreview.net/forum?id=SJeWHlSYDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SJeWHlSYDB", "replyto": "SJeWHlSYDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2274/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2274/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575500403770, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2274/Reviewers"], "noninvitees": [], "tcdate": 1570237725179, "tmdate": 1575500403785, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2274/-/Official_Review"}}}, {"id": "BJe9q6g09r", "original": null, "number": 3, "cdate": 1572896146288, "ddate": null, "tcdate": 1572896146288, "tmdate": 1572972360185, "tddate": null, "forum": "SJeWHlSYDB", "replyto": "SJeWHlSYDB", "invitation": "ICLR.cc/2020/Conference/Paper2274/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The paper proposes a new divergence, called spread divergence, to distinguish probability models. The approach is motivated from the concern that traditional divergence such as f-divergence or KL divergence may not always exist, in which the spread divergence may be a substitute. Some empirical supports are provided for the proposed method. Below I will summarize my concerns.\n\n1. The spread divergence is proven no larger than the traditional divergence, so the paper claims this as an advantage of using spread divergence. My question is that if KL or f-divergence of two probability models is infinity, which means they distinguish the models very well, whether a new method is necessary (though it may provide a finite value). \n\n2. As a new method, it would be useful to thoroughly compare it with the traditional ones. There is a lack theoretical comparison with the KL or f-divergence. Some numerical examples are provided but seem not enough. For instance, the applications focus on the situations where likelihood is not defined, i.e., data are deterministic w/o observation noise. It is interesting to see other examples where likelihood is defined and how traditional methods perform.\n\n3. Kernel based spread divergence has been a major focus of this paper. It is interesting to see which kernel maximizes the spread divergence. Section 3.2 considers Gaussian kernel. Is this an optimal option?\n\n4. Section 5 compares EM and spread EM based on one experiment and claims the latter has smaller error.   Does the same conclusion holds true in other examples?\n\nI believe the motivation of this paper is interesting. This would be a stronger paper if more theoretical and empirical analysis can be added."}, "signatures": ["ICLR.cc/2020/Conference/Paper2274/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2274/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["mingtian.zhang.17@ucl.ac.uk", "david.barber@ucl.ac.uk", "thomas.bird.17@ucl.ac.uk", "peter.hayes.15@ucl.ac.uk", "r.habib@cs.ucl.ac.uk"], "title": "SPREAD  DIVERGENCE", "authors": ["Mingtian Zhang", "David Barber", "Thomas Bird", "Peter Hayes", "Raza Habib"], "pdf": "/pdf/5dc6531d109da0247305d2e70cce0c493988ed7e.pdf", "TL;DR": "A new divergence family dealing with distributions with different supports for training implicit generative models.", "abstract": "For distributions $p$ and $q$ with different supports, the divergence $\\div{p}{q}$ may not exist. We define a spread divergence $\\sdiv{p}{q}$ on modified $p$ and $q$ and describe sufficient conditions for the existence of such a divergence. We demonstrate how to maximize the discriminatory power of a given divergence by parameterizing and learning the spread. We also give examples of using a spread divergence to train and improve implicit generative models, including linear models (Independent Components Analysis) and non-linear models (Deep Generative Networks).", "code": "https://drive.google.com/file/d/1p6l7J1HpcNTV1RrF12wwCza-98m1J8di/view?usp=sharing", "keywords": ["divergence minimization", "generative model", "variational inference"], "paperhash": "zhang|spread_divergence", "original_pdf": "/attachment/ccb736cf3932d708f10cba9149fe80b8069a9f14.pdf", "_bibtex": "@misc{\nzhang2020spread,\ntitle={{\\{}SPREAD{\\}}  {\\{}DIVERGENCE{\\}}},\nauthor={Mingtian Zhang and David Barber and Thomas Bird and Peter Hayes and Raza Habib},\nyear={2020},\nurl={https://openreview.net/forum?id=SJeWHlSYDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SJeWHlSYDB", "replyto": "SJeWHlSYDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2274/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2274/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575500403770, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2274/Reviewers"], "noninvitees": [], "tcdate": 1570237725179, "tmdate": 1575500403785, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2274/-/Official_Review"}}}], "count": 8}