{"notes": [{"id": "r1ltgp4FwS", "original": "HyePii7LwH", "number": 347, "cdate": 1569438961364, "ddate": null, "tcdate": 1569438961364, "tmdate": 1577168288108, "tddate": null, "forum": "r1ltgp4FwS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Learning Temporal Coherence via Self-Supervision for GAN-based Video Generation", "authors": ["Mengyu Chu", "You Xie", "Jonas Mayer", "Laura Leal-Taix\u00e9", "Nils Th\u00fcrey"], "authorids": ["mengyu.chu@tum.de", "you.xie@tum.de", "jonas.a.mayer@tum.de", "leal.taixe@tum.de", "nils.thuerey@tum.de"], "keywords": ["adversarial training", "generative models", "unpaired video translation", "video super-resolution", "temporal coherence", "self-supervision", "cycle-consistency"], "TL;DR": "We propose temporal self-supervisions for learning stable temporal functions with GANs.", "abstract": "We focus on temporal self-supervision for GAN-based video generation tasks. While adversarial training successfully yields generative models for a variety of areas, temporal relationship in the generated data is much less explored. This is crucial for sequential generation tasks, e.g. video super-resolution and unpaired video translation. For the former, state-of-the-art methods often favor simpler norm losses such as L2 over adversarial training. However, their averaging nature easily leads to temporally smooth results with an undesirable lack of spatial detail. For unpaired video translation, existing approaches modify the generator networks to form spatio-temporal cycle consistencies. In contrast, we focus on improving the learning objectives and propose a temporally self-supervised algorithm. For both tasks, we show that temporal adversarial learning is key to achieving temporally coherent solutions without sacrificing spatial detail. We also propose a novel Ping-Pong loss to improve the long-term temporal consistency. It effectively prevents recurrent networks from accumulating artifacts temporally without depressing detailed features. We also propose a first set of metrics to quantitatively evaluate the accuracy as well as the perceptual quality of the temporal evolution. A series of user studies confirms the rankings computed with these metrics.", "pdf": "/pdf/e72d16b4fa5723b555457d2083c28add42c39d7e.pdf", "paperhash": "chu|learning_temporal_coherence_via_selfsupervision_for_ganbased_video_generation", "original_pdf": "/attachment/6764636fc255eba29624d2aad7cd8621c750c552.pdf", "_bibtex": "@misc{\nchu2020learning,\ntitle={Learning Temporal Coherence via Self-Supervision for {\\{}GAN{\\}}-based Video Generation},\nauthor={Mengyu Chu and You Xie and Jonas Mayer and Laura Leal-Taix{\\'e} and Nils Th{\\\"u}rey},\nyear={2020},\nurl={https://openreview.net/forum?id=r1ltgp4FwS}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 14, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "04taF0PLu", "original": null, "number": 1, "cdate": 1576798693844, "ddate": null, "tcdate": 1576798693844, "tmdate": 1576800941649, "tddate": null, "forum": "r1ltgp4FwS", "replyto": "r1ltgp4FwS", "invitation": "ICLR.cc/2020/Conference/Paper347/-/Decision", "content": {"decision": "Reject", "comment": "The paper presents an architecture for conditional video generation tasks with temporal self-supervision and temporal adversarial learning. The proposed architecture is reasonable but looks somewhat complicated. In terms of technical novelty, the so-called \"ping-pong\" loss looks interesting and novel, but other parts are more-or-less some combinations of existing techniques. Experimental results show promise of the proposed method against selected baselines for video super-resolution (VSR) and unpaired video-to-video translation tasks (UVT). In terms of weakness, (1) the technical novelty is not very high; (2) the final loss is a combination of many losses with many hyperparameters; (3) experimentally the proposed method is not compared against recent SOTA methods on VSR and UVT. \n\nThe proposed method should be compared against more recent SOTA baselines for VSR tasks (see examples of references below):\n\nEDVR: Video Restoration with Enhanced Deformable Convolutional Networks\nhttps://arxiv.org/abs/1905.02716\n\nProgressive Fusion Video Super-Resolution Network via Exploiting Non-Local Spatio-Temporal Correlations\nICCV 2019\n\nRecurrent Back-Projection Network for Video Super-Resolution\nCVPR 2019\n\nThe same comment would apply for baselines for UVT tasks:\n\nMocycle-GAN: Unpaired Video-to-Video Translation\nhttps://arxiv.org/abs/1908.09514\n\nPreserving Semantic and Temporal Consistency for Unpaired Video-to-Video Translation\nhttps://arxiv.org/abs/1908.07683\n\nParticularly for UVT, the evaluated dataset seems limited in terms of scope as well (i.e., evaluations on more popular benchmarks, such as Viper would be needed for further validation). Overall, given that the contribution of this work is an empirical performance with a rather complex architecture/loss, more comprehensive empirical evaluations on SOTA baselines are warranted.\n", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Temporal Coherence via Self-Supervision for GAN-based Video Generation", "authors": ["Mengyu Chu", "You Xie", "Jonas Mayer", "Laura Leal-Taix\u00e9", "Nils Th\u00fcrey"], "authorids": ["mengyu.chu@tum.de", "you.xie@tum.de", "jonas.a.mayer@tum.de", "leal.taixe@tum.de", "nils.thuerey@tum.de"], "keywords": ["adversarial training", "generative models", "unpaired video translation", "video super-resolution", "temporal coherence", "self-supervision", "cycle-consistency"], "TL;DR": "We propose temporal self-supervisions for learning stable temporal functions with GANs.", "abstract": "We focus on temporal self-supervision for GAN-based video generation tasks. While adversarial training successfully yields generative models for a variety of areas, temporal relationship in the generated data is much less explored. This is crucial for sequential generation tasks, e.g. video super-resolution and unpaired video translation. For the former, state-of-the-art methods often favor simpler norm losses such as L2 over adversarial training. However, their averaging nature easily leads to temporally smooth results with an undesirable lack of spatial detail. For unpaired video translation, existing approaches modify the generator networks to form spatio-temporal cycle consistencies. In contrast, we focus on improving the learning objectives and propose a temporally self-supervised algorithm. For both tasks, we show that temporal adversarial learning is key to achieving temporally coherent solutions without sacrificing spatial detail. We also propose a novel Ping-Pong loss to improve the long-term temporal consistency. It effectively prevents recurrent networks from accumulating artifacts temporally without depressing detailed features. We also propose a first set of metrics to quantitatively evaluate the accuracy as well as the perceptual quality of the temporal evolution. A series of user studies confirms the rankings computed with these metrics.", "pdf": "/pdf/e72d16b4fa5723b555457d2083c28add42c39d7e.pdf", "paperhash": "chu|learning_temporal_coherence_via_selfsupervision_for_ganbased_video_generation", "original_pdf": "/attachment/6764636fc255eba29624d2aad7cd8621c750c552.pdf", "_bibtex": "@misc{\nchu2020learning,\ntitle={Learning Temporal Coherence via Self-Supervision for {\\{}GAN{\\}}-based Video Generation},\nauthor={Mengyu Chu and You Xie and Jonas Mayer and Laura Leal-Taix{\\'e} and Nils Th{\\\"u}rey},\nyear={2020},\nurl={https://openreview.net/forum?id=r1ltgp4FwS}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "r1ltgp4FwS", "replyto": "r1ltgp4FwS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795715341, "tmdate": 1576800265233, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper347/-/Decision"}}}, {"id": "BJgk6DpcsS", "original": null, "number": 9, "cdate": 1573734327467, "ddate": null, "tcdate": 1573734327467, "tmdate": 1573734327467, "tddate": null, "forum": "r1ltgp4FwS", "replyto": "Hylpj8LOjS", "invitation": "ICLR.cc/2020/Conference/Paper347/-/Official_Comment", "content": {"title": "an updated draft with modifications highlighted ", "comment": "We updated our draft according to the suggestions. \nNew parts are highlighted in the document. \nWe thank the reviewers for their help to improve the document."}, "signatures": ["ICLR.cc/2020/Conference/Paper347/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper347/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Temporal Coherence via Self-Supervision for GAN-based Video Generation", "authors": ["Mengyu Chu", "You Xie", "Jonas Mayer", "Laura Leal-Taix\u00e9", "Nils Th\u00fcrey"], "authorids": ["mengyu.chu@tum.de", "you.xie@tum.de", "jonas.a.mayer@tum.de", "leal.taixe@tum.de", "nils.thuerey@tum.de"], "keywords": ["adversarial training", "generative models", "unpaired video translation", "video super-resolution", "temporal coherence", "self-supervision", "cycle-consistency"], "TL;DR": "We propose temporal self-supervisions for learning stable temporal functions with GANs.", "abstract": "We focus on temporal self-supervision for GAN-based video generation tasks. While adversarial training successfully yields generative models for a variety of areas, temporal relationship in the generated data is much less explored. This is crucial for sequential generation tasks, e.g. video super-resolution and unpaired video translation. For the former, state-of-the-art methods often favor simpler norm losses such as L2 over adversarial training. However, their averaging nature easily leads to temporally smooth results with an undesirable lack of spatial detail. For unpaired video translation, existing approaches modify the generator networks to form spatio-temporal cycle consistencies. In contrast, we focus on improving the learning objectives and propose a temporally self-supervised algorithm. For both tasks, we show that temporal adversarial learning is key to achieving temporally coherent solutions without sacrificing spatial detail. We also propose a novel Ping-Pong loss to improve the long-term temporal consistency. It effectively prevents recurrent networks from accumulating artifacts temporally without depressing detailed features. We also propose a first set of metrics to quantitatively evaluate the accuracy as well as the perceptual quality of the temporal evolution. A series of user studies confirms the rankings computed with these metrics.", "pdf": "/pdf/e72d16b4fa5723b555457d2083c28add42c39d7e.pdf", "paperhash": "chu|learning_temporal_coherence_via_selfsupervision_for_ganbased_video_generation", "original_pdf": "/attachment/6764636fc255eba29624d2aad7cd8621c750c552.pdf", "_bibtex": "@misc{\nchu2020learning,\ntitle={Learning Temporal Coherence via Self-Supervision for {\\{}GAN{\\}}-based Video Generation},\nauthor={Mengyu Chu and You Xie and Jonas Mayer and Laura Leal-Taix{\\'e} and Nils Th{\\\"u}rey},\nyear={2020},\nurl={https://openreview.net/forum?id=r1ltgp4FwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1ltgp4FwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper347/Authors", "ICLR.cc/2020/Conference/Paper347/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper347/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper347/Reviewers", "ICLR.cc/2020/Conference/Paper347/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper347/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper347/Authors|ICLR.cc/2020/Conference/Paper347/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504172778, "tmdate": 1576860532123, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper347/Authors", "ICLR.cc/2020/Conference/Paper347/Reviewers", "ICLR.cc/2020/Conference/Paper347/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper347/-/Official_Comment"}}}, {"id": "HJxSNHUuir", "original": null, "number": 5, "cdate": 1573573933096, "ddate": null, "tcdate": 1573573933096, "tmdate": 1573575799674, "tddate": null, "forum": "r1ltgp4FwS", "replyto": "rkxVYkeJ9S", "invitation": "ICLR.cc/2020/Conference/Paper347/-/Official_Comment", "content": {"title": "Reply to reviewer #2", "comment": "\nDear reviewer, thank you very much for the review and comments. We are glad to hear the positive assessment of our results.\n\nQ1: \u201cRather complex overall objective\u201d\n\nA1: We agree that our learning objectives look complex because of the two distinct application domains. We found it important to show as much as possible that our approach is of general interest for difficult spatio-temporal data. We will try to further clarify the differences and similarities between VSR and UVT formulations in future revisions of our text. \n\n\nQ2: \u201cSeems to need a lot of tweaking\u201d\n\nA2: GANs are generally non-trivial to train, and our models share this behavior. In practice, we found working with the proposed architectures not to be overly difficult. We recommend starting with a stable version for spatial single-image inference, and then extending it with our main contributions for temporal self-supervision. For the VSR and UVT tasks, we found that once a balanced spatial GAN (single-image SR or single image translation) is realized, the temporal extension, i.e., adding a frame-recurrent input to the generator and changing the Ds to a Dst architecture, did not require further hyper-parameter fine-tuning.\n\nIn addition, two hyper-parameters, one to train the flow estimator F and one for the PP loss, need to be adjusted. These hyper-parameters can be chosen based on shorter test runs. In practice, we write results for a fixed set of validation samples to disk. Based on the evolution of these samples over the course of training, especially with respect to spatial detail and temporal stability, suitable parameters can typically be chosen after a small number of test runs. \n\n\nQ3: \u201cIsn't the PP loss just an incarnation of cycle consistency loss?\u201d\n\nA3: The \u201ccycle-consistency\u201d loss refers to different constraints in different settings. While some of them share similar ideas with our PP loss at a high level, we consider the proposed PP loss to be novel and especially useful for generative tasks in the area of videos. Our formulation is tailored to the adversarial learning of detailed content, and it is key for preventing artifacts from being accumulated over time for recurrent models.\n\nConsidering specific prior work, CycleGAN proposes the spatial cycle-consistency loss while RecycleGAN proposes a cycle loss across data domains and time. The PP loss instead aims for a strict temporal coherence constraint and is inherently different from both. \n\nBeyond the field of video generation, the papers on time cycle loss used for optical flow estimation and tracking is closer to the PP loss at a high level. In these areas, the goal is to infer valid motions or tracking over frames, and a similar L2 loss is used to constrain a fixed video warped with the inferred motions. This time cycle loss is used to optimize indirectly via the estimated motion/tracked positions. It can then improve the accuracy of the tracked positions or estimated motions. We instead use the PP loss to directly constrain the generated video content. In this setting, the PP loss successfully improves the long-term temporal consistency of the results. Due to the different goals of our PP loss and the concurrent nature of the other work in the area, we kept the different name of our formulation. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper347/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper347/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Temporal Coherence via Self-Supervision for GAN-based Video Generation", "authors": ["Mengyu Chu", "You Xie", "Jonas Mayer", "Laura Leal-Taix\u00e9", "Nils Th\u00fcrey"], "authorids": ["mengyu.chu@tum.de", "you.xie@tum.de", "jonas.a.mayer@tum.de", "leal.taixe@tum.de", "nils.thuerey@tum.de"], "keywords": ["adversarial training", "generative models", "unpaired video translation", "video super-resolution", "temporal coherence", "self-supervision", "cycle-consistency"], "TL;DR": "We propose temporal self-supervisions for learning stable temporal functions with GANs.", "abstract": "We focus on temporal self-supervision for GAN-based video generation tasks. While adversarial training successfully yields generative models for a variety of areas, temporal relationship in the generated data is much less explored. This is crucial for sequential generation tasks, e.g. video super-resolution and unpaired video translation. For the former, state-of-the-art methods often favor simpler norm losses such as L2 over adversarial training. However, their averaging nature easily leads to temporally smooth results with an undesirable lack of spatial detail. For unpaired video translation, existing approaches modify the generator networks to form spatio-temporal cycle consistencies. In contrast, we focus on improving the learning objectives and propose a temporally self-supervised algorithm. For both tasks, we show that temporal adversarial learning is key to achieving temporally coherent solutions without sacrificing spatial detail. We also propose a novel Ping-Pong loss to improve the long-term temporal consistency. It effectively prevents recurrent networks from accumulating artifacts temporally without depressing detailed features. We also propose a first set of metrics to quantitatively evaluate the accuracy as well as the perceptual quality of the temporal evolution. A series of user studies confirms the rankings computed with these metrics.", "pdf": "/pdf/e72d16b4fa5723b555457d2083c28add42c39d7e.pdf", "paperhash": "chu|learning_temporal_coherence_via_selfsupervision_for_ganbased_video_generation", "original_pdf": "/attachment/6764636fc255eba29624d2aad7cd8621c750c552.pdf", "_bibtex": "@misc{\nchu2020learning,\ntitle={Learning Temporal Coherence via Self-Supervision for {\\{}GAN{\\}}-based Video Generation},\nauthor={Mengyu Chu and You Xie and Jonas Mayer and Laura Leal-Taix{\\'e} and Nils Th{\\\"u}rey},\nyear={2020},\nurl={https://openreview.net/forum?id=r1ltgp4FwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1ltgp4FwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper347/Authors", "ICLR.cc/2020/Conference/Paper347/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper347/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper347/Reviewers", "ICLR.cc/2020/Conference/Paper347/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper347/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper347/Authors|ICLR.cc/2020/Conference/Paper347/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504172778, "tmdate": 1576860532123, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper347/Authors", "ICLR.cc/2020/Conference/Paper347/Reviewers", "ICLR.cc/2020/Conference/Paper347/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper347/-/Official_Comment"}}}, {"id": "SJlUT78OiB", "original": null, "number": 3, "cdate": 1573573565772, "ddate": null, "tcdate": 1573573565772, "tmdate": 1573575689799, "tddate": null, "forum": "r1ltgp4FwS", "replyto": "Bkezu8ZT5B", "invitation": "ICLR.cc/2020/Conference/Paper347/-/Official_Comment", "content": {"title": "Reply to reviewer #4 (part 3)", "comment": "Q7: \u201cFor the UVT task the DsDtPP model was omitted\u201d\n\nA7: We have included an additional DsDtPP model for comparison. As we mentioned in the main text, it is necessary to find a good balance between the two generators and the four discriminator networks. By weighting the temporal adversarial losses from Dt with 0.6 and the spatial ones from Ds with 1.0, the DsDtPP model yields similar performance to the Dst model on the smoke dataset. A result generated with this model can be found on the Rebuttal_R4.html page. We can include this model in our submission for completeness, but we would like to point out that the proposed Dst architecture is the better choice in practice, as it learns a natural balance of temporal and spatial components by itself and requires fewer resources.\n\n\nQ8: Figure 5 has no point of reference with which to compare the frames.\nA8: We will include a ground truth image in our revision for comparison.\n\nThanks again."}, "signatures": ["ICLR.cc/2020/Conference/Paper347/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper347/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Temporal Coherence via Self-Supervision for GAN-based Video Generation", "authors": ["Mengyu Chu", "You Xie", "Jonas Mayer", "Laura Leal-Taix\u00e9", "Nils Th\u00fcrey"], "authorids": ["mengyu.chu@tum.de", "you.xie@tum.de", "jonas.a.mayer@tum.de", "leal.taixe@tum.de", "nils.thuerey@tum.de"], "keywords": ["adversarial training", "generative models", "unpaired video translation", "video super-resolution", "temporal coherence", "self-supervision", "cycle-consistency"], "TL;DR": "We propose temporal self-supervisions for learning stable temporal functions with GANs.", "abstract": "We focus on temporal self-supervision for GAN-based video generation tasks. While adversarial training successfully yields generative models for a variety of areas, temporal relationship in the generated data is much less explored. This is crucial for sequential generation tasks, e.g. video super-resolution and unpaired video translation. For the former, state-of-the-art methods often favor simpler norm losses such as L2 over adversarial training. However, their averaging nature easily leads to temporally smooth results with an undesirable lack of spatial detail. For unpaired video translation, existing approaches modify the generator networks to form spatio-temporal cycle consistencies. In contrast, we focus on improving the learning objectives and propose a temporally self-supervised algorithm. For both tasks, we show that temporal adversarial learning is key to achieving temporally coherent solutions without sacrificing spatial detail. We also propose a novel Ping-Pong loss to improve the long-term temporal consistency. It effectively prevents recurrent networks from accumulating artifacts temporally without depressing detailed features. We also propose a first set of metrics to quantitatively evaluate the accuracy as well as the perceptual quality of the temporal evolution. A series of user studies confirms the rankings computed with these metrics.", "pdf": "/pdf/e72d16b4fa5723b555457d2083c28add42c39d7e.pdf", "paperhash": "chu|learning_temporal_coherence_via_selfsupervision_for_ganbased_video_generation", "original_pdf": "/attachment/6764636fc255eba29624d2aad7cd8621c750c552.pdf", "_bibtex": "@misc{\nchu2020learning,\ntitle={Learning Temporal Coherence via Self-Supervision for {\\{}GAN{\\}}-based Video Generation},\nauthor={Mengyu Chu and You Xie and Jonas Mayer and Laura Leal-Taix{\\'e} and Nils Th{\\\"u}rey},\nyear={2020},\nurl={https://openreview.net/forum?id=r1ltgp4FwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1ltgp4FwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper347/Authors", "ICLR.cc/2020/Conference/Paper347/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper347/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper347/Reviewers", "ICLR.cc/2020/Conference/Paper347/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper347/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper347/Authors|ICLR.cc/2020/Conference/Paper347/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504172778, "tmdate": 1576860532123, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper347/Authors", "ICLR.cc/2020/Conference/Paper347/Reviewers", "ICLR.cc/2020/Conference/Paper347/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper347/-/Official_Comment"}}}, {"id": "HJxviQI_jr", "original": null, "number": 2, "cdate": 1573573535285, "ddate": null, "tcdate": 1573573535285, "tmdate": 1573575571855, "tddate": null, "forum": "r1ltgp4FwS", "replyto": "Bkezu8ZT5B", "invitation": "ICLR.cc/2020/Conference/Paper347/-/Official_Comment", "content": {"title": "Reply to reviewer #4 (part 2) ", "comment": "Q3: \u201cTecoGAN vs baselines generator parameters (rather than TecoGAN^{-}).\u201d\n\nA3: Model Sizes: the following table summarizes the sizes of the different models compared in our evaluation section:\n\n    Model                  Weight Count                          \n+-------------------+-----------------------------------------------------------+\n| DUF                | 6.2 million \u2014\u2014 105% more than TecoGAN |\n+-------------------+-----------------------------------------------------------+\n| FRVSR             | 0.84 million for super-resolution,                 |\n|                         | 1.74 million for flow estimation.                   |\n+-------------------+-----------------------------------------------------------+\n| ENet                | 0.84 million \u2014\u2014 single-image SR                 |\n+--------------------+-----------------------------------------------------------+\n| TecoGAN^{-}  | 0.84 million for the generator,                       |\n|                          | 1.74 million for flow estimation                     |\n|                          | \u2014\u2014 same as FRVSR                                         |\n+--------------------+-----------------------------------------------------------+\n| TecoGAN         | 1.29 million for the generator,                       |\n|                          | 1.74 million for flow estimation                      |\n|                          | \u2014\u2014 17% more than FRVSR or TecoGAN^{-} |\n+--------------------+------------------------------------------------------------+\n\n- TecoGAN vs. FRVSR, DUF, ENet:\nTo summarize, TecoGAN has 17% more weights than FRVSR and DUF has 105% more weights than TecoGAN. Although the weight count is different for each model, the fundamental difference between them are the learning objectives: Even if we increase the size of FRVSR, it will still generate results that lack detail because of the averaging nature of its L2 loss.  \n\n- TecoGAN{-} vs. FRVSR:\nFor the Vid4 dataset, we show all the metrics in Table 2. Comparing TecoGAN^{-} and FRVSR, with the same size, TecoGAN^{-} achieves a much better LPIPS score, slightly better scores in terms of tOF, tLP, and a slightly worse PSNR score. Fig. 6 highlights that TecoGAN^{-} generates substantially more detail than FRVSR. We also provide the outputs of TecoGAN^{-} now for Vid4 (see Q6).\n\n\nQ4: \u201cEvaluation metric contributions in the supplementary material?\u201d & \u201cUVT task evaluation in the supplementary material?\u201d\nA4: Thank you for the suggestion. We\u2019re glad to hear the metrics and UVT evaluation are considered to be important, we will include them in the main document of our submission.\n\nQ5: \u201cUVT task only evaluated with the proposed evaluations? ...a human-based study could be done\u2026\u201d\n\nA5: It is a good suggestion to evaluate the UVT results with user studies. As no ground-truth data is available, we carried out two sets of user studies:  One uses an arbitrary sample from the target domain as the reference and the other uses the actual input from the source domain as the reference. Comparing CycleGAN, RecycleGAN and TecoGAN on the Obama-and-Trump data-set, we found that our TecoGAN results are clearly preferred in both cases. Details of these results can be found in the Rebuttal_R4.html page, and we will include the results of these user studies in future revisions of the paper.\n\n\nQ6: The same qualitative comparisons in the paper are not in the provided website.\nA6: We did not upload these videos as part of our submission due to the relatively large amount of material in the appendix and supplemental webpage. We are happy to provide additional comparison videos for the different methods. \n\nThe following sequences can now be found in the VSR_videos.html page of the \u201cVid4-ToS-Videos.zip\u201d archive at https://www.dropbox.com/sh/n07l8n51slh1e9c/AAAVngT9xsSzs1pJQqe5xV1Oa?dl=0:\n  - the ground-truth and results for 8 methods on the Vid4 dataset \n  - the GT and results for 4 methods on the ToS dataset and lizard, armor, spider scenes.\n\nThese sequences are the originals that were used to extract the single frames shown in our submission. As visible in the static figures in the PDF, adversarial learning yields more details across the different scenes. The videos highlight that our TecoGAN model additionally yields excellent coherence over time."}, "signatures": ["ICLR.cc/2020/Conference/Paper347/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper347/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Temporal Coherence via Self-Supervision for GAN-based Video Generation", "authors": ["Mengyu Chu", "You Xie", "Jonas Mayer", "Laura Leal-Taix\u00e9", "Nils Th\u00fcrey"], "authorids": ["mengyu.chu@tum.de", "you.xie@tum.de", "jonas.a.mayer@tum.de", "leal.taixe@tum.de", "nils.thuerey@tum.de"], "keywords": ["adversarial training", "generative models", "unpaired video translation", "video super-resolution", "temporal coherence", "self-supervision", "cycle-consistency"], "TL;DR": "We propose temporal self-supervisions for learning stable temporal functions with GANs.", "abstract": "We focus on temporal self-supervision for GAN-based video generation tasks. While adversarial training successfully yields generative models for a variety of areas, temporal relationship in the generated data is much less explored. This is crucial for sequential generation tasks, e.g. video super-resolution and unpaired video translation. For the former, state-of-the-art methods often favor simpler norm losses such as L2 over adversarial training. However, their averaging nature easily leads to temporally smooth results with an undesirable lack of spatial detail. For unpaired video translation, existing approaches modify the generator networks to form spatio-temporal cycle consistencies. In contrast, we focus on improving the learning objectives and propose a temporally self-supervised algorithm. For both tasks, we show that temporal adversarial learning is key to achieving temporally coherent solutions without sacrificing spatial detail. We also propose a novel Ping-Pong loss to improve the long-term temporal consistency. It effectively prevents recurrent networks from accumulating artifacts temporally without depressing detailed features. We also propose a first set of metrics to quantitatively evaluate the accuracy as well as the perceptual quality of the temporal evolution. A series of user studies confirms the rankings computed with these metrics.", "pdf": "/pdf/e72d16b4fa5723b555457d2083c28add42c39d7e.pdf", "paperhash": "chu|learning_temporal_coherence_via_selfsupervision_for_ganbased_video_generation", "original_pdf": "/attachment/6764636fc255eba29624d2aad7cd8621c750c552.pdf", "_bibtex": "@misc{\nchu2020learning,\ntitle={Learning Temporal Coherence via Self-Supervision for {\\{}GAN{\\}}-based Video Generation},\nauthor={Mengyu Chu and You Xie and Jonas Mayer and Laura Leal-Taix{\\'e} and Nils Th{\\\"u}rey},\nyear={2020},\nurl={https://openreview.net/forum?id=r1ltgp4FwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1ltgp4FwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper347/Authors", "ICLR.cc/2020/Conference/Paper347/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper347/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper347/Reviewers", "ICLR.cc/2020/Conference/Paper347/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper347/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper347/Authors|ICLR.cc/2020/Conference/Paper347/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504172778, "tmdate": 1576860532123, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper347/Authors", "ICLR.cc/2020/Conference/Paper347/Reviewers", "ICLR.cc/2020/Conference/Paper347/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper347/-/Official_Comment"}}}, {"id": "Hylpj8LOjS", "original": null, "number": 8, "cdate": 1573574309075, "ddate": null, "tcdate": 1573574309075, "tmdate": 1573574628402, "tddate": null, "forum": "r1ltgp4FwS", "replyto": "r1ltgp4FwS", "invitation": "ICLR.cc/2020/Conference/Paper347/-/Official_Comment", "content": {"title": "Additional materials required/suggested by reviewers", "comment": "\nWe would like to thank all the reviewers for their insightful comments and suggestions. In order to address the questions raised in the reviews, we have uploaded additional materials to shed light on the behavior and performance of our approach. \n\nThe corresponding experiments can be seen via the dropbox link: https://www.dropbox.com/sh/n07l8n51slh1e9c/AAAVngT9xsSzs1pJQqe5xV1Oa?dl=0\nPlease download the \u201cRebuttal.zip\u201d file, and open the Rebuttal_R*.html page contained in it.\n\nMost importantly, we show:\n- A series of new user studies to evaluate the unpaired video translation task. (Rebuttal_R4.html)\n- Example triplets to illustrate the importance of warped and unwarped content (Rebuttal_R4.html)\n- Outputs of a UVT DsDtPP model for comparison (Rebuttal_R4.html)\n- An ablation study for the data-augmentation versus temporal constraint properties of the proposed PP loss. (Rebuttal_R1.html)\n\nAdditionally, all VSR videos shown in our paper can be found on the \"VSR_videos.html\" webpage inside the \"Vid4-ToS-Videos.zip\" archive. A different file is created due to the large file size (196MB).\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper347/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper347/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Temporal Coherence via Self-Supervision for GAN-based Video Generation", "authors": ["Mengyu Chu", "You Xie", "Jonas Mayer", "Laura Leal-Taix\u00e9", "Nils Th\u00fcrey"], "authorids": ["mengyu.chu@tum.de", "you.xie@tum.de", "jonas.a.mayer@tum.de", "leal.taixe@tum.de", "nils.thuerey@tum.de"], "keywords": ["adversarial training", "generative models", "unpaired video translation", "video super-resolution", "temporal coherence", "self-supervision", "cycle-consistency"], "TL;DR": "We propose temporal self-supervisions for learning stable temporal functions with GANs.", "abstract": "We focus on temporal self-supervision for GAN-based video generation tasks. While adversarial training successfully yields generative models for a variety of areas, temporal relationship in the generated data is much less explored. This is crucial for sequential generation tasks, e.g. video super-resolution and unpaired video translation. For the former, state-of-the-art methods often favor simpler norm losses such as L2 over adversarial training. However, their averaging nature easily leads to temporally smooth results with an undesirable lack of spatial detail. For unpaired video translation, existing approaches modify the generator networks to form spatio-temporal cycle consistencies. In contrast, we focus on improving the learning objectives and propose a temporally self-supervised algorithm. For both tasks, we show that temporal adversarial learning is key to achieving temporally coherent solutions without sacrificing spatial detail. We also propose a novel Ping-Pong loss to improve the long-term temporal consistency. It effectively prevents recurrent networks from accumulating artifacts temporally without depressing detailed features. We also propose a first set of metrics to quantitatively evaluate the accuracy as well as the perceptual quality of the temporal evolution. A series of user studies confirms the rankings computed with these metrics.", "pdf": "/pdf/e72d16b4fa5723b555457d2083c28add42c39d7e.pdf", "paperhash": "chu|learning_temporal_coherence_via_selfsupervision_for_ganbased_video_generation", "original_pdf": "/attachment/6764636fc255eba29624d2aad7cd8621c750c552.pdf", "_bibtex": "@misc{\nchu2020learning,\ntitle={Learning Temporal Coherence via Self-Supervision for {\\{}GAN{\\}}-based Video Generation},\nauthor={Mengyu Chu and You Xie and Jonas Mayer and Laura Leal-Taix{\\'e} and Nils Th{\\\"u}rey},\nyear={2020},\nurl={https://openreview.net/forum?id=r1ltgp4FwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1ltgp4FwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper347/Authors", "ICLR.cc/2020/Conference/Paper347/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper347/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper347/Reviewers", "ICLR.cc/2020/Conference/Paper347/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper347/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper347/Authors|ICLR.cc/2020/Conference/Paper347/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504172778, "tmdate": 1576860532123, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper347/Authors", "ICLR.cc/2020/Conference/Paper347/Reviewers", "ICLR.cc/2020/Conference/Paper347/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper347/-/Official_Comment"}}}, {"id": "rJlzV8LOiB", "original": null, "number": 7, "cdate": 1573574186161, "ddate": null, "tcdate": 1573574186161, "tmdate": 1573574186161, "tddate": null, "forum": "r1ltgp4FwS", "replyto": "HkxvTKqTKS", "invitation": "ICLR.cc/2020/Conference/Paper347/-/Official_Comment", "content": {"title": "Reply to reviewer #1 (part 2) ", "comment": "Q6: I found the PP loss to be unjustified. \n\nA6: We found the PP loss to be crucial to tackle the issue of \u201cstreaking artifacts\u201d being accumulated over time. This problem is a general one for recurrent models, as training sequences of arbitrary length is not possible with current hardware. In Fig 5 of our paper and the supp.-mat. webpage section 4.3, we show that the PP loss is very effective for avoiding this problem. The foliage scene has 40 frames. Our models are trained with sequences of length 10. The PP loss extends the length to 19. From videos, we see that without PP loss, strong artifacts become visible from frame 15 to 40. In contrast, the model trained with a PP loss is artifact free for all 40 frames.\n\nIn the PP loss, there are two parts that contribute to solving the problem. One is the PP data augmentation and the other one is its temporal consistency constraint. \n1) the PP data augmentation extends an N-frame sequence to the symmetric sequence of length (2N-1). This is helpful because many high-quality video data-sets only have few frames, e.g., the 3-frame and 7-frame Vimeo 90k datasets. With the PP data augmentation, we can train recurrent models to effectively see longer sequences.\n2) the temporal constraint for the forward-backward pass. In order to show how much this aspect contributes separately, we carried out the following test.\n\nIn addition to a regular model with both the PP augmentation (1) and the PP constraint (2), we trained another TecoGAN variant that only employs the PP data augmentation of (1) without the forward-backward pass constraint (i.e., \u03bb_p=0 in the 4th row of Table 1). This model is denoted as \u201cPP-Augment\u201d. Video results can be seen in \u201cRebuttal_R1.html\u201d contained in the \u201cRebuttal.zip\u201d archive at: https://www.dropbox.com/sh/n07l8n51slh1e9c/AAAVngT9xsSzs1pJQqe5xV1Oa?dl=0\n\nSo during training, the generator network of DsDt sees 10 frames, while the generators of PP-Augment and TecoGAN{-} see 19 frames. While DsDt shows strong recurrent accumulation artifacts early on (from frame 15), the PP-Augment version slightly reduces the artifacts. It works good for frame 15 but shows artifacts from frame 32 on. Only our regular model (TecoGAN{-}) successfully avoids temporal accumulation for all 40 frames. With the PP constraint (2), the model can avoid the recurrent accumulation of artifacts and work well for sequences that are substantially longer than the training length. Among others, we have tested our model with ToS sequences of lengths 150, 166 and 233. For all of these sequences, TecoGAN model did not show temporal accumulation or streaking artifacts. \n\nThus, the PP loss not only provides augmentation, but the temporal constraint is crucial for avoiding visual artifacts. We will include this new comparison in the revised version of our document."}, "signatures": ["ICLR.cc/2020/Conference/Paper347/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper347/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Temporal Coherence via Self-Supervision for GAN-based Video Generation", "authors": ["Mengyu Chu", "You Xie", "Jonas Mayer", "Laura Leal-Taix\u00e9", "Nils Th\u00fcrey"], "authorids": ["mengyu.chu@tum.de", "you.xie@tum.de", "jonas.a.mayer@tum.de", "leal.taixe@tum.de", "nils.thuerey@tum.de"], "keywords": ["adversarial training", "generative models", "unpaired video translation", "video super-resolution", "temporal coherence", "self-supervision", "cycle-consistency"], "TL;DR": "We propose temporal self-supervisions for learning stable temporal functions with GANs.", "abstract": "We focus on temporal self-supervision for GAN-based video generation tasks. While adversarial training successfully yields generative models for a variety of areas, temporal relationship in the generated data is much less explored. This is crucial for sequential generation tasks, e.g. video super-resolution and unpaired video translation. For the former, state-of-the-art methods often favor simpler norm losses such as L2 over adversarial training. However, their averaging nature easily leads to temporally smooth results with an undesirable lack of spatial detail. For unpaired video translation, existing approaches modify the generator networks to form spatio-temporal cycle consistencies. In contrast, we focus on improving the learning objectives and propose a temporally self-supervised algorithm. For both tasks, we show that temporal adversarial learning is key to achieving temporally coherent solutions without sacrificing spatial detail. We also propose a novel Ping-Pong loss to improve the long-term temporal consistency. It effectively prevents recurrent networks from accumulating artifacts temporally without depressing detailed features. We also propose a first set of metrics to quantitatively evaluate the accuracy as well as the perceptual quality of the temporal evolution. A series of user studies confirms the rankings computed with these metrics.", "pdf": "/pdf/e72d16b4fa5723b555457d2083c28add42c39d7e.pdf", "paperhash": "chu|learning_temporal_coherence_via_selfsupervision_for_ganbased_video_generation", "original_pdf": "/attachment/6764636fc255eba29624d2aad7cd8621c750c552.pdf", "_bibtex": "@misc{\nchu2020learning,\ntitle={Learning Temporal Coherence via Self-Supervision for {\\{}GAN{\\}}-based Video Generation},\nauthor={Mengyu Chu and You Xie and Jonas Mayer and Laura Leal-Taix{\\'e} and Nils Th{\\\"u}rey},\nyear={2020},\nurl={https://openreview.net/forum?id=r1ltgp4FwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1ltgp4FwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper347/Authors", "ICLR.cc/2020/Conference/Paper347/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper347/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper347/Reviewers", "ICLR.cc/2020/Conference/Paper347/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper347/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper347/Authors|ICLR.cc/2020/Conference/Paper347/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504172778, "tmdate": 1576860532123, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper347/Authors", "ICLR.cc/2020/Conference/Paper347/Reviewers", "ICLR.cc/2020/Conference/Paper347/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper347/-/Official_Comment"}}}, {"id": "Hkec1U8djr", "original": null, "number": 6, "cdate": 1573574114317, "ddate": null, "tcdate": 1573574114317, "tmdate": 1573574114317, "tddate": null, "forum": "r1ltgp4FwS", "replyto": "HkxvTKqTKS", "invitation": "ICLR.cc/2020/Conference/Paper347/-/Official_Comment", "content": {"title": "Reply to reviewer #1 (part 1)", "comment": "Dear reviewer, thank you very much for the detailed review. Among others, we were glad to hear that the video results were considered to be impressive. \n\nQ1: \u201c\u2026the contribution seems to be getting the spatio-temporal adversarial loss to work at all ...\u201d\n\nA1: For image generation, GANs are extremely popular in order to synthesize realistic results. For videos, which add an additional dimension to the data domain, GANs are scarcely used due to a lack of techniques to control temporal coherence. We believe that showing how spatio-temporal adversarial learning can yield realistic and coherent results is an important step in this field. Many highly successful papers, e.g., the growing GANs of Karras et al., have introduced simple concepts that have led to huge advances. \n\nDue to the inherent challenges of GAN-training we also found it important to demonstrate how spatio-temporal adversarial learning can be realized in practice, among others via the Dst architecture, the PP loss, and the temporal metrics. In both tasks, i.e. VSR and UVT, our results contain realistic spatial details that previous methods cannot produce, together with temporal coherence which is on par with state-of-the-art non-adversarial methods. The combination is crucial here, as non-adversarial methods can trivially achieve temporal coherency by generation outputs that lack detail.\n \nQ2: \u201cI found the video generations impressive\u2026although TecoGAN does not beat its competitors on most proposed metrics\u2026\u201d\n\nA2: Due to the complicated nature of the data, unfortunately, no approach currently exists that can reliably quantify the perceptual quality of natural images over time. Hence, we found it very important to consider spatial metrics (LPIPS, and PSNR as a very rough indicator) as a baseline for visual quality, and in addition, consider temporal metrics (tLP and tOF) to assess temporal changes. We have refrained from introducing a single criterion using a weighted combination of the different metrics, as this would raise fundamental questions about how to weight the individual terms.\n\nTo give a specific example: compared with DUF, TecoGAN performs significantly better in terms of LPIPS, equally well on tLP&tOF, and only slightly worse in terms of PSNR. As reported by other works, the trade-off between PSNR and perceptual quality is unavoidable, therefore, when improving LPIPS from 2.607 (DUF) to 1.623 (TecoGAN), a PSNR decrease of less than 2dB is very reasonable. The TecoGAN model is on-par in terms of tLP&tOF despite the detail added by our method.\n\nTo conclude, the metric data in our submission consistently shows that no other method achieves the same performance in terms of spatial detail and temporal coherence at the same time. This is confirmed by our user studies.\n\n\nQ3: \u201c\u2026 I have concerns about the generalization of the approach\u2026\u201d\n\nA3: We agree that generalization is a crucial aspect for methods in this area. This is one of the motivations for evaluating our method thoroughly for two distinct application domains (most existing works target only one of the two). Within each application, we offer ablation studies, several video results, and metric evaluations. We performed user studies for VSR and we have additionally included new user studies for UVT (for details, cf. Q5 of Reviewer #4 above and \u201cRebuttal_R4.html\u201d). For VSR, we train one TecoGAN model which performs very well for a wide range of video content. In UVT, we train one model for each dataset and always use the same code and parameters. In all ablation studies, the proposed temporal self-supervision offers clear and consistent improvements for a variety of VSR and UVT test cases (i.e., foliage and calendar scenes for VSR, Obama-and-Trump and smoke scenes for UVT). We believe that taken together, these results provide a detailed and varied assessment of the generality of our approach.\n\n\nQ4: \u201cIs RecycleGAN unable to be applied to video super-resolution? Why is it not compared to in Table 2?\u201d\n\nA4: The RecycleGAN algorithm is designed for UVT tasks. Besides a generator for single-image translation, they propose to use a predictor network to learn the temporal evolution within one video domain. While the predictor networks could be used instead of commonly used flow estimators for VSR, it would be very hard to get high-quality predictions because of the multi-modality of general video data. The generator network of RecycleGAN by itself does not stand out. Hence, an adoption of RecycleGAN for a task it was not designed for (VSR) is unlikely to behave better than state-of-the-art VSR methods like DUF and FRVSR, which is why we only compare RecycleGAN for the evaluation of UVT tasks.\n\n\nQ5: Is L_Phi the perceptual loss? I don't think it was mentioned in the body of the text.\nA5: Thanks, this is correct - we will add the corresponding explanation.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper347/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper347/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Temporal Coherence via Self-Supervision for GAN-based Video Generation", "authors": ["Mengyu Chu", "You Xie", "Jonas Mayer", "Laura Leal-Taix\u00e9", "Nils Th\u00fcrey"], "authorids": ["mengyu.chu@tum.de", "you.xie@tum.de", "jonas.a.mayer@tum.de", "leal.taixe@tum.de", "nils.thuerey@tum.de"], "keywords": ["adversarial training", "generative models", "unpaired video translation", "video super-resolution", "temporal coherence", "self-supervision", "cycle-consistency"], "TL;DR": "We propose temporal self-supervisions for learning stable temporal functions with GANs.", "abstract": "We focus on temporal self-supervision for GAN-based video generation tasks. While adversarial training successfully yields generative models for a variety of areas, temporal relationship in the generated data is much less explored. This is crucial for sequential generation tasks, e.g. video super-resolution and unpaired video translation. For the former, state-of-the-art methods often favor simpler norm losses such as L2 over adversarial training. However, their averaging nature easily leads to temporally smooth results with an undesirable lack of spatial detail. For unpaired video translation, existing approaches modify the generator networks to form spatio-temporal cycle consistencies. In contrast, we focus on improving the learning objectives and propose a temporally self-supervised algorithm. For both tasks, we show that temporal adversarial learning is key to achieving temporally coherent solutions without sacrificing spatial detail. We also propose a novel Ping-Pong loss to improve the long-term temporal consistency. It effectively prevents recurrent networks from accumulating artifacts temporally without depressing detailed features. We also propose a first set of metrics to quantitatively evaluate the accuracy as well as the perceptual quality of the temporal evolution. A series of user studies confirms the rankings computed with these metrics.", "pdf": "/pdf/e72d16b4fa5723b555457d2083c28add42c39d7e.pdf", "paperhash": "chu|learning_temporal_coherence_via_selfsupervision_for_ganbased_video_generation", "original_pdf": "/attachment/6764636fc255eba29624d2aad7cd8621c750c552.pdf", "_bibtex": "@misc{\nchu2020learning,\ntitle={Learning Temporal Coherence via Self-Supervision for {\\{}GAN{\\}}-based Video Generation},\nauthor={Mengyu Chu and You Xie and Jonas Mayer and Laura Leal-Taix{\\'e} and Nils Th{\\\"u}rey},\nyear={2020},\nurl={https://openreview.net/forum?id=r1ltgp4FwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1ltgp4FwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper347/Authors", "ICLR.cc/2020/Conference/Paper347/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper347/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper347/Reviewers", "ICLR.cc/2020/Conference/Paper347/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper347/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper347/Authors|ICLR.cc/2020/Conference/Paper347/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504172778, "tmdate": 1576860532123, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper347/Authors", "ICLR.cc/2020/Conference/Paper347/Reviewers", "ICLR.cc/2020/Conference/Paper347/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper347/-/Official_Comment"}}}, {"id": "BJxBmVU_sr", "original": null, "number": 4, "cdate": 1573573660647, "ddate": null, "tcdate": 1573573660647, "tmdate": 1573573660647, "tddate": null, "forum": "r1ltgp4FwS", "replyto": "Syx9r90j5H", "invitation": "ICLR.cc/2020/Conference/Paper347/-/Official_Comment", "content": {"title": "Reply to reviewer #3", "comment": "Dear reviewer, thank you very much for the review and suggestions.\n\nQ1: \u201coriginality of the concatenation of several frames is somewhat limited\u201d\nA1: It is correct that such a concatenation is used in a variety of other settings. The core aim of our work is to highlight its importance in a very challenging setting, i.e., for training spatio-temporal GANs for natural image sequences. In this field, L2-based losses dominate, and with our work, we demonstrate how much additional detail can be generated with the right approach for adversarial training. We will clarify this in the text of our submission.\n\nQ2: \u201cthe metrics definitions were not included in the main body of the paper\u201d\nA2: We will move the metrics definitions into the main part in a future version of the paper. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper347/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper347/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Temporal Coherence via Self-Supervision for GAN-based Video Generation", "authors": ["Mengyu Chu", "You Xie", "Jonas Mayer", "Laura Leal-Taix\u00e9", "Nils Th\u00fcrey"], "authorids": ["mengyu.chu@tum.de", "you.xie@tum.de", "jonas.a.mayer@tum.de", "leal.taixe@tum.de", "nils.thuerey@tum.de"], "keywords": ["adversarial training", "generative models", "unpaired video translation", "video super-resolution", "temporal coherence", "self-supervision", "cycle-consistency"], "TL;DR": "We propose temporal self-supervisions for learning stable temporal functions with GANs.", "abstract": "We focus on temporal self-supervision for GAN-based video generation tasks. While adversarial training successfully yields generative models for a variety of areas, temporal relationship in the generated data is much less explored. This is crucial for sequential generation tasks, e.g. video super-resolution and unpaired video translation. For the former, state-of-the-art methods often favor simpler norm losses such as L2 over adversarial training. However, their averaging nature easily leads to temporally smooth results with an undesirable lack of spatial detail. For unpaired video translation, existing approaches modify the generator networks to form spatio-temporal cycle consistencies. In contrast, we focus on improving the learning objectives and propose a temporally self-supervised algorithm. For both tasks, we show that temporal adversarial learning is key to achieving temporally coherent solutions without sacrificing spatial detail. We also propose a novel Ping-Pong loss to improve the long-term temporal consistency. It effectively prevents recurrent networks from accumulating artifacts temporally without depressing detailed features. We also propose a first set of metrics to quantitatively evaluate the accuracy as well as the perceptual quality of the temporal evolution. A series of user studies confirms the rankings computed with these metrics.", "pdf": "/pdf/e72d16b4fa5723b555457d2083c28add42c39d7e.pdf", "paperhash": "chu|learning_temporal_coherence_via_selfsupervision_for_ganbased_video_generation", "original_pdf": "/attachment/6764636fc255eba29624d2aad7cd8621c750c552.pdf", "_bibtex": "@misc{\nchu2020learning,\ntitle={Learning Temporal Coherence via Self-Supervision for {\\{}GAN{\\}}-based Video Generation},\nauthor={Mengyu Chu and You Xie and Jonas Mayer and Laura Leal-Taix{\\'e} and Nils Th{\\\"u}rey},\nyear={2020},\nurl={https://openreview.net/forum?id=r1ltgp4FwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1ltgp4FwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper347/Authors", "ICLR.cc/2020/Conference/Paper347/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper347/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper347/Reviewers", "ICLR.cc/2020/Conference/Paper347/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper347/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper347/Authors|ICLR.cc/2020/Conference/Paper347/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504172778, "tmdate": 1576860532123, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper347/Authors", "ICLR.cc/2020/Conference/Paper347/Reviewers", "ICLR.cc/2020/Conference/Paper347/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper347/-/Official_Comment"}}}, {"id": "rkgdL7LOor", "original": null, "number": 1, "cdate": 1573573455958, "ddate": null, "tcdate": 1573573455958, "tmdate": 1573573455958, "tddate": null, "forum": "r1ltgp4FwS", "replyto": "Bkezu8ZT5B", "invitation": "ICLR.cc/2020/Conference/Paper347/-/Official_Comment", "content": {"title": "Reply to reviewer #4 (part 1)", "comment": "Dear reviewer, thank you very much for the detailed review and comments. We are glad to hear that you consider the paper to be well written and the method to be novel. The comments about unclear parts and potential weaknesses are very important for us to refine and improve our paper. Below, we will address your questions in more detail.\n\nPlease note that for questions Q1, Q5 and Q8 additional examples/results were required. We provide these via the file \u201cRebuttal_R4.html\u201d contained in the \u201cRebuttal.zip\u201d archive at: https://www.dropbox.com/sh/n07l8n51slh1e9c/AAAVngT9xsSzs1pJQqe5xV1Oa?dl=0\n\n\nQ1: \u201cConfused about inputs to discriminator (I_{w,g} and I_{w,b})\u201c\n\nA1: When the generated results I_{g} are artifact-free (i.e. do not jitter), it is correct that the triplet I_{w,g} warped with the forward and reverse motions (v_t and v_t') would simply contain three copies of the middle frame. The same holds for the ground-truth triplet I_{w,b}. However, in practice, the motions (v_t and v_t\u2019) cannot capture evolutions such as changing illumination, or occlusions. In addition, the motion fields contain approximation errors. Our tests show that these residual changes in the inputs are what allows the discriminator to learn about natural motions. (Specific examples are shown in the Rebuttal_R4.html page.)\n\nIntuitively, the warped frames make the job easier for the discriminator by supplying information \u201cin-place\u201d wherever possible, such that it can assess how natural the changes are. At the same time - as the motions are potentially imperfect - we found that the discriminator benefits from being able to fall back to the original (unwarped) triplets.\n\nOur ablation studies (Supp.-mat. webpage section 4.2, and the third column of section 5), show that adding the warped frames consistently and reliably improves the temporal behavior. D_{st} can learn complex temporal functions by supervising via warped triplets and original triplets, and thus provides good learning objectives for the generator. In practice, we have not encountered situations where the generator learns to simply map pixels or structures backward and forward. It is more natural for the generator to rely on the provided input data, i.e. the previous generated frame and the current low-resolution frame, to infer outputs.\n\n\nQ2: \u201cHow do you prevent the zero output in the PP loss?\u201d\n\nA2: Being one of the possible minima, a zero output trivially satisfies the PP loss formulation, but is easily identified by the discriminator D_st. Hence, such an output typically yields a large adversarial loss. Considering one pixel (i,j) in a forward sequence g_t(i,j), the PP loss also considers a pixel from the backward pass g_t\u2019(i,j). Assuming that g_t(i,j) contains a smaller intensity than g_t\u2019(i,j), the gradient of the PP loss will lead to an increase of g_t, while g_t\u2019 will be decreased. At the same time, the gradient of D_{st} will require g_t and g_t\u2019 to match the reference distribution, i.e., to contain natural and sharp details. So the PP loss only requires g_t and g_t\u2019 to be the same. An L2 distance is a right choice here as g_t and g_t\u2019 are perfectly aligned, and the L2 distance can be correctly minimized even for detailed content. There is no warping or occlusion, and the data by construction is not multi-modal. \n\nTaking the VSR task as an example, the hyper-parameter for the PP loss is 0.5. At the end of the training, the total generator loss of the TecoGAN model has 76.4% adversarial loss, 6.8% PP loss and 16.8% other losses (averaged across 4000 frames). Hence, the adversarial component is dominating, but the PP loss still yields a substantial contribution. When we instead evaluate the PP loss for the DsDt model in the same way (it was not used for training the DsDt model), the loss is 49.1% higher than for the TecoGAN model. So the PP loss is successfully and substantially reduced without converging to a trivial zero solution.\n\n\n\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper347/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper347/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Temporal Coherence via Self-Supervision for GAN-based Video Generation", "authors": ["Mengyu Chu", "You Xie", "Jonas Mayer", "Laura Leal-Taix\u00e9", "Nils Th\u00fcrey"], "authorids": ["mengyu.chu@tum.de", "you.xie@tum.de", "jonas.a.mayer@tum.de", "leal.taixe@tum.de", "nils.thuerey@tum.de"], "keywords": ["adversarial training", "generative models", "unpaired video translation", "video super-resolution", "temporal coherence", "self-supervision", "cycle-consistency"], "TL;DR": "We propose temporal self-supervisions for learning stable temporal functions with GANs.", "abstract": "We focus on temporal self-supervision for GAN-based video generation tasks. While adversarial training successfully yields generative models for a variety of areas, temporal relationship in the generated data is much less explored. This is crucial for sequential generation tasks, e.g. video super-resolution and unpaired video translation. For the former, state-of-the-art methods often favor simpler norm losses such as L2 over adversarial training. However, their averaging nature easily leads to temporally smooth results with an undesirable lack of spatial detail. For unpaired video translation, existing approaches modify the generator networks to form spatio-temporal cycle consistencies. In contrast, we focus on improving the learning objectives and propose a temporally self-supervised algorithm. For both tasks, we show that temporal adversarial learning is key to achieving temporally coherent solutions without sacrificing spatial detail. We also propose a novel Ping-Pong loss to improve the long-term temporal consistency. It effectively prevents recurrent networks from accumulating artifacts temporally without depressing detailed features. We also propose a first set of metrics to quantitatively evaluate the accuracy as well as the perceptual quality of the temporal evolution. A series of user studies confirms the rankings computed with these metrics.", "pdf": "/pdf/e72d16b4fa5723b555457d2083c28add42c39d7e.pdf", "paperhash": "chu|learning_temporal_coherence_via_selfsupervision_for_ganbased_video_generation", "original_pdf": "/attachment/6764636fc255eba29624d2aad7cd8621c750c552.pdf", "_bibtex": "@misc{\nchu2020learning,\ntitle={Learning Temporal Coherence via Self-Supervision for {\\{}GAN{\\}}-based Video Generation},\nauthor={Mengyu Chu and You Xie and Jonas Mayer and Laura Leal-Taix{\\'e} and Nils Th{\\\"u}rey},\nyear={2020},\nurl={https://openreview.net/forum?id=r1ltgp4FwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1ltgp4FwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper347/Authors", "ICLR.cc/2020/Conference/Paper347/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper347/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper347/Reviewers", "ICLR.cc/2020/Conference/Paper347/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper347/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper347/Authors|ICLR.cc/2020/Conference/Paper347/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504172778, "tmdate": 1576860532123, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper347/Authors", "ICLR.cc/2020/Conference/Paper347/Reviewers", "ICLR.cc/2020/Conference/Paper347/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper347/-/Official_Comment"}}}, {"id": "HkxvTKqTKS", "original": null, "number": 1, "cdate": 1571822015273, "ddate": null, "tcdate": 1571822015273, "tmdate": 1572972606781, "tddate": null, "forum": "r1ltgp4FwS", "replyto": "r1ltgp4FwS", "invitation": "ICLR.cc/2020/Conference/Paper347/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "Contribution\nAugments the loss of video generation systems with a discriminator that considers multiple frames (as opposed to single frames independently) and a new objective termed ping-pong loss which is introduced in order to deal with \u201cartifacts\u201d that appear in video generation. The paper also proposes a few automatic metrics with which to compare systems. Although the performance does not convincingly exceed its competitors, the contribution seems to be getting the spatio-temporal adversarial loss to work at all.\n \nOverall\nI found the video generations impressive, and the addition of the discriminator seems to improve sharpness of the individual frames over methods trained with non-adversarial losses, in particular DUF. Although TecoGAN does not beat its competitors (for example DUF) on most proposed automatic metrics, it appears to be ranked better in expectation in user studies (Table 2). However, I am concerned about generalizability of the method.\n \nDecision\nAs someone not in this field, I find it hard to judge the performance of the system from so few samples. I also found the differences in the losses between tasks to be worrisome, as it indicates this method does not generalize without heavy tweaking of the loss. My current decision is weak reject, since the generations look quite good but I have concerns about generalization of the approach to other video generation tasks as well as the justification of the ping-pong loss.\n\nQuestions\n- Is RecycleGAN unable to be applied to video super-resolution? Why is it not compared to in Table 2?\n- Is L_Phi the perceptual loss? I don't think it was mentioned in the body of the text.\n- I found the ping-pong (PP) loss to be unjustified. The loss is motivated by the issue of \"artifacts\", which I assume are poor generations due to lacking a good model of the world. The paper says this issue could be alleviated by training with longer video sequences, but that would prevent the generator from working with sequences of arbitrary length. I do not believe the ping-pong loss allows the generator to work with arbitrary sequences, as training only on short sequences and their reverse should not allow the model to generalize to longer sequences. Additionally, this approach would likely not scale to long sequences as well, since it requires doubling the sequence length. Would you be able to train on longer sequences?"}, "signatures": ["ICLR.cc/2020/Conference/Paper347/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper347/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Temporal Coherence via Self-Supervision for GAN-based Video Generation", "authors": ["Mengyu Chu", "You Xie", "Jonas Mayer", "Laura Leal-Taix\u00e9", "Nils Th\u00fcrey"], "authorids": ["mengyu.chu@tum.de", "you.xie@tum.de", "jonas.a.mayer@tum.de", "leal.taixe@tum.de", "nils.thuerey@tum.de"], "keywords": ["adversarial training", "generative models", "unpaired video translation", "video super-resolution", "temporal coherence", "self-supervision", "cycle-consistency"], "TL;DR": "We propose temporal self-supervisions for learning stable temporal functions with GANs.", "abstract": "We focus on temporal self-supervision for GAN-based video generation tasks. While adversarial training successfully yields generative models for a variety of areas, temporal relationship in the generated data is much less explored. This is crucial for sequential generation tasks, e.g. video super-resolution and unpaired video translation. For the former, state-of-the-art methods often favor simpler norm losses such as L2 over adversarial training. However, their averaging nature easily leads to temporally smooth results with an undesirable lack of spatial detail. For unpaired video translation, existing approaches modify the generator networks to form spatio-temporal cycle consistencies. In contrast, we focus on improving the learning objectives and propose a temporally self-supervised algorithm. For both tasks, we show that temporal adversarial learning is key to achieving temporally coherent solutions without sacrificing spatial detail. We also propose a novel Ping-Pong loss to improve the long-term temporal consistency. It effectively prevents recurrent networks from accumulating artifacts temporally without depressing detailed features. We also propose a first set of metrics to quantitatively evaluate the accuracy as well as the perceptual quality of the temporal evolution. A series of user studies confirms the rankings computed with these metrics.", "pdf": "/pdf/e72d16b4fa5723b555457d2083c28add42c39d7e.pdf", "paperhash": "chu|learning_temporal_coherence_via_selfsupervision_for_ganbased_video_generation", "original_pdf": "/attachment/6764636fc255eba29624d2aad7cd8621c750c552.pdf", "_bibtex": "@misc{\nchu2020learning,\ntitle={Learning Temporal Coherence via Self-Supervision for {\\{}GAN{\\}}-based Video Generation},\nauthor={Mengyu Chu and You Xie and Jonas Mayer and Laura Leal-Taix{\\'e} and Nils Th{\\\"u}rey},\nyear={2020},\nurl={https://openreview.net/forum?id=r1ltgp4FwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "r1ltgp4FwS", "replyto": "r1ltgp4FwS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper347/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper347/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576081374196, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper347/Reviewers"], "noninvitees": [], "tcdate": 1570237753458, "tmdate": 1576081374208, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper347/-/Official_Review"}}}, {"id": "rkxVYkeJ9S", "original": null, "number": 2, "cdate": 1571909500377, "ddate": null, "tcdate": 1571909500377, "tmdate": 1572972606737, "tddate": null, "forum": "r1ltgp4FwS", "replyto": "r1ltgp4FwS", "invitation": "ICLR.cc/2020/Conference/Paper347/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "\nThe paper presents a novel method for training video-to-video translation (vid2vid) models. The authors introduce a spatio-temporal adversarial discriminator for GAN training, that shows significant benefits over prior methods, in particular, parallel (as opposed to joint) spatial and temporal discriminators. In addition the authors introduce a self-supervised objective based on cycle dependency that is crucial for producing temporally consistent videos. A new set of metrics is introduced to validate the claims of the authors.\n\nI really like this paper. Although the method is a rather complex mix of multiple losses, these are justified in detail, both intuitively and empirically. The appendix is filled with much more detail about implementation, architecture and more results. Finally the results show that the proposed method is superior across the board compared to previous approaches from the literature.\n\n\nStrenghts:\n- The approach works on two distinct applications of vid2vid.\n- Detailed ablations justifying the introduction of every single part of the overall objective.\n- Strong results.\n- Clear writing and presentation.\n- A lot of additional details and results in the appendix for the more interested reader.\n\nWeaknesses:\n- Rather complex overall objective\n- Seems to need a lot of tweaking\n\nQuestions:\n- Isn't the PP loss just an incarnation of cycle consistency loss? If so, maybe there is no need for the introduction of a new name for it.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper347/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper347/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Temporal Coherence via Self-Supervision for GAN-based Video Generation", "authors": ["Mengyu Chu", "You Xie", "Jonas Mayer", "Laura Leal-Taix\u00e9", "Nils Th\u00fcrey"], "authorids": ["mengyu.chu@tum.de", "you.xie@tum.de", "jonas.a.mayer@tum.de", "leal.taixe@tum.de", "nils.thuerey@tum.de"], "keywords": ["adversarial training", "generative models", "unpaired video translation", "video super-resolution", "temporal coherence", "self-supervision", "cycle-consistency"], "TL;DR": "We propose temporal self-supervisions for learning stable temporal functions with GANs.", "abstract": "We focus on temporal self-supervision for GAN-based video generation tasks. While adversarial training successfully yields generative models for a variety of areas, temporal relationship in the generated data is much less explored. This is crucial for sequential generation tasks, e.g. video super-resolution and unpaired video translation. For the former, state-of-the-art methods often favor simpler norm losses such as L2 over adversarial training. However, their averaging nature easily leads to temporally smooth results with an undesirable lack of spatial detail. For unpaired video translation, existing approaches modify the generator networks to form spatio-temporal cycle consistencies. In contrast, we focus on improving the learning objectives and propose a temporally self-supervised algorithm. For both tasks, we show that temporal adversarial learning is key to achieving temporally coherent solutions without sacrificing spatial detail. We also propose a novel Ping-Pong loss to improve the long-term temporal consistency. It effectively prevents recurrent networks from accumulating artifacts temporally without depressing detailed features. We also propose a first set of metrics to quantitatively evaluate the accuracy as well as the perceptual quality of the temporal evolution. A series of user studies confirms the rankings computed with these metrics.", "pdf": "/pdf/e72d16b4fa5723b555457d2083c28add42c39d7e.pdf", "paperhash": "chu|learning_temporal_coherence_via_selfsupervision_for_ganbased_video_generation", "original_pdf": "/attachment/6764636fc255eba29624d2aad7cd8621c750c552.pdf", "_bibtex": "@misc{\nchu2020learning,\ntitle={Learning Temporal Coherence via Self-Supervision for {\\{}GAN{\\}}-based Video Generation},\nauthor={Mengyu Chu and You Xie and Jonas Mayer and Laura Leal-Taix{\\'e} and Nils Th{\\\"u}rey},\nyear={2020},\nurl={https://openreview.net/forum?id=r1ltgp4FwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "r1ltgp4FwS", "replyto": "r1ltgp4FwS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper347/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper347/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576081374196, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper347/Reviewers"], "noninvitees": [], "tcdate": 1570237753458, "tmdate": 1576081374208, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper347/-/Official_Review"}}}, {"id": "Syx9r90j5H", "original": null, "number": 3, "cdate": 1572756033835, "ddate": null, "tcdate": 1572756033835, "tmdate": 1572972606693, "tddate": null, "forum": "r1ltgp4FwS", "replyto": "r1ltgp4FwS", "invitation": "ICLR.cc/2020/Conference/Paper347/-/Official_Review", "content": {"rating": "6: Weak Accept", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The paper presents video generation method with spacio-temporally consistent features. This is done through: a) temporal adversarial learning, b) Ping Pong loss, and c) metrics that quantify the quality. The methods are evaluated on two datasets and user studies.\n\nThe idea is interesting and the paper is well written. The results are convincing.\n\nThe originality of the concatenation of several frames is somewhat limited, since it is a standard procedure in other domains such as robotics. Nevertheless the results are positive.\n\nSeems like the metrics definitions were not included in the main body of the paper - the authors should either include them to remove from the contributions. "}, "signatures": ["ICLR.cc/2020/Conference/Paper347/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper347/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Temporal Coherence via Self-Supervision for GAN-based Video Generation", "authors": ["Mengyu Chu", "You Xie", "Jonas Mayer", "Laura Leal-Taix\u00e9", "Nils Th\u00fcrey"], "authorids": ["mengyu.chu@tum.de", "you.xie@tum.de", "jonas.a.mayer@tum.de", "leal.taixe@tum.de", "nils.thuerey@tum.de"], "keywords": ["adversarial training", "generative models", "unpaired video translation", "video super-resolution", "temporal coherence", "self-supervision", "cycle-consistency"], "TL;DR": "We propose temporal self-supervisions for learning stable temporal functions with GANs.", "abstract": "We focus on temporal self-supervision for GAN-based video generation tasks. While adversarial training successfully yields generative models for a variety of areas, temporal relationship in the generated data is much less explored. This is crucial for sequential generation tasks, e.g. video super-resolution and unpaired video translation. For the former, state-of-the-art methods often favor simpler norm losses such as L2 over adversarial training. However, their averaging nature easily leads to temporally smooth results with an undesirable lack of spatial detail. For unpaired video translation, existing approaches modify the generator networks to form spatio-temporal cycle consistencies. In contrast, we focus on improving the learning objectives and propose a temporally self-supervised algorithm. For both tasks, we show that temporal adversarial learning is key to achieving temporally coherent solutions without sacrificing spatial detail. We also propose a novel Ping-Pong loss to improve the long-term temporal consistency. It effectively prevents recurrent networks from accumulating artifacts temporally without depressing detailed features. We also propose a first set of metrics to quantitatively evaluate the accuracy as well as the perceptual quality of the temporal evolution. A series of user studies confirms the rankings computed with these metrics.", "pdf": "/pdf/e72d16b4fa5723b555457d2083c28add42c39d7e.pdf", "paperhash": "chu|learning_temporal_coherence_via_selfsupervision_for_ganbased_video_generation", "original_pdf": "/attachment/6764636fc255eba29624d2aad7cd8621c750c552.pdf", "_bibtex": "@misc{\nchu2020learning,\ntitle={Learning Temporal Coherence via Self-Supervision for {\\{}GAN{\\}}-based Video Generation},\nauthor={Mengyu Chu and You Xie and Jonas Mayer and Laura Leal-Taix{\\'e} and Nils Th{\\\"u}rey},\nyear={2020},\nurl={https://openreview.net/forum?id=r1ltgp4FwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "r1ltgp4FwS", "replyto": "r1ltgp4FwS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper347/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper347/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576081374196, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper347/Reviewers"], "noninvitees": [], "tcdate": 1570237753458, "tmdate": 1576081374208, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper347/-/Official_Review"}}}, {"id": "Bkezu8ZT5B", "original": null, "number": 4, "cdate": 1572832873590, "ddate": null, "tcdate": 1572832873590, "tmdate": 1572972606648, "tddate": null, "forum": "r1ltgp4FwS", "replyto": "r1ltgp4FwS", "invitation": "ICLR.cc/2020/Conference/Paper347/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "\nSummary:\nThis paper proposes a training objective for higher quality video generation for the tasks of Video Super Resolution (VSR) and Unpaired Video Translation (UVT) and also two evaluation metrics tOF and tLP. They provide a comprehensive ablative study of the proposed method and also show comparisons against baselines for the tasks of VSR and UVT.\n\n\nPros:\n+ Novel video generation method for VSR and UVT.\n+ Novel metrics\n+ Well written paper\n\nWeaknesses / comments:\n\n- Confused about inputs to discriminator (I_{w,g} and I_{w,b})\nI am not sure I understand the inputs I_{w,g} and I_{w,b} to the discriminator network. Based on the description, I_{w,g} = {W(g_{t-1}, v_t), g_t, W(g_{t+1}, v_t^\u2019)} = {g_t, g_t, g_t}, assuming v_t^\u201d means reverse flow. A similar process seems to be happening with I_{w,b}. If this is the case, will the discriminator optimally get the same frame concatenated together (i.e. {g_t, g_t, g_t})? Now, if that\u2019s the case, how is the discriminator modeling temporal consistency other than making the generator learn to put pixels forward and back in place since the \u201coriginal triplets\u201d (real data) are {g_{t-1}, g_t, g_{t+1}} and {b_{t-1}, g_t, g_{t+1}}, respectively. This is very confusing to me. It would be good if the authors can clarify this in the rebuttal.\n\n\n- How do you prevent the zero output in PP loss?\nThe PP-loss compares forward and backward prediction by || g_t - g_t^\u2019 ||. If not careful, the neural network can just learn to output zeros or the same frame for the full video. Did the authors see any behavior like this? Or did the proposed formulations prevent this? Or was there a very small weight applied to this loss?\n\n\n\n- TecoGAN vs baselines generator parameters (rather than TecoGAN^{-}).\nBased on the experimental section, it looks like TecoGAN is the main network being compared to the baseline methods. TecoGAN has more parameters in the generator compared to TecoGAN^{-}. Did the authors make sure that the generator had the same number of parameters as the other methods? The authors mention a performance difference between TecoGAN and DUF due to DUF having more parameters, but what about the other baselines?\n\n- Evaluation metric contributions in the supplementary material?\nThe description of the two proposed metrics tOF and tLP have been placed in Appendix B. These are mentioned as contributions of the paper so their description should be in the main text. Please make sure that Appendix B is in the main text in future versions of this paper.\n\n\n- UVT task evaluation in the supplementary material?\nThe UVT task is mentioned in the abstract and as a target task in this work, however, the evaluations for this are in the Appendix. Please move them to the main text in future versions of this paper. Secondary experiments should be in the Appendix but I feel this is primary.\n\n\n- UVT task only evaluated with the proposed evaluations?\nThe UVT evaluation in the Appendix only has the proposed metrics as evaluation. I understand that, since it\u2019s an unpaired video translation task, there is no ground truth to compare against. However, a human based study could be done where human raters would judge for the more realistic of N videos.\n\n\n- The same qualitative comparisons in the paper are not in the provided website.\nThe provided website does not have the same qualitative evaluations as the paper does. For example: There are 7 methods being compared for the same video in Figure 8 in the supplementary material, but I don\u2019t see anything like that in the website.\n\n\n- For the UVT task, we omit the DsDtPP model because \u2026\u2026.\nIn the paper, the authors mention that they don\u2019t provide the DsDtPP baselines because it requires a lot of computation. However, I feel these missing baselines make the experiments incomplete since it\u2019s a different task.\n\n\n- Figure 5 has no point of reference with which to compare the frames.\nFigure 5 points out that the video b) is better than video a). While it is true that one seems more noisy than the other, there should be a point of reference for readers to make sure of it.\n\n\nConclusion:\nIn conclusion, the paper seems to present a novel method and evaluation metrics but has many issues as stated above. It would make the submission better if the authors can address them in the rebuttal."}, "signatures": ["ICLR.cc/2020/Conference/Paper347/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper347/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Temporal Coherence via Self-Supervision for GAN-based Video Generation", "authors": ["Mengyu Chu", "You Xie", "Jonas Mayer", "Laura Leal-Taix\u00e9", "Nils Th\u00fcrey"], "authorids": ["mengyu.chu@tum.de", "you.xie@tum.de", "jonas.a.mayer@tum.de", "leal.taixe@tum.de", "nils.thuerey@tum.de"], "keywords": ["adversarial training", "generative models", "unpaired video translation", "video super-resolution", "temporal coherence", "self-supervision", "cycle-consistency"], "TL;DR": "We propose temporal self-supervisions for learning stable temporal functions with GANs.", "abstract": "We focus on temporal self-supervision for GAN-based video generation tasks. While adversarial training successfully yields generative models for a variety of areas, temporal relationship in the generated data is much less explored. This is crucial for sequential generation tasks, e.g. video super-resolution and unpaired video translation. For the former, state-of-the-art methods often favor simpler norm losses such as L2 over adversarial training. However, their averaging nature easily leads to temporally smooth results with an undesirable lack of spatial detail. For unpaired video translation, existing approaches modify the generator networks to form spatio-temporal cycle consistencies. In contrast, we focus on improving the learning objectives and propose a temporally self-supervised algorithm. For both tasks, we show that temporal adversarial learning is key to achieving temporally coherent solutions without sacrificing spatial detail. We also propose a novel Ping-Pong loss to improve the long-term temporal consistency. It effectively prevents recurrent networks from accumulating artifacts temporally without depressing detailed features. We also propose a first set of metrics to quantitatively evaluate the accuracy as well as the perceptual quality of the temporal evolution. A series of user studies confirms the rankings computed with these metrics.", "pdf": "/pdf/e72d16b4fa5723b555457d2083c28add42c39d7e.pdf", "paperhash": "chu|learning_temporal_coherence_via_selfsupervision_for_ganbased_video_generation", "original_pdf": "/attachment/6764636fc255eba29624d2aad7cd8621c750c552.pdf", "_bibtex": "@misc{\nchu2020learning,\ntitle={Learning Temporal Coherence via Self-Supervision for {\\{}GAN{\\}}-based Video Generation},\nauthor={Mengyu Chu and You Xie and Jonas Mayer and Laura Leal-Taix{\\'e} and Nils Th{\\\"u}rey},\nyear={2020},\nurl={https://openreview.net/forum?id=r1ltgp4FwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "r1ltgp4FwS", "replyto": "r1ltgp4FwS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper347/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper347/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576081374196, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper347/Reviewers"], "noninvitees": [], "tcdate": 1570237753458, "tmdate": 1576081374208, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper347/-/Official_Review"}}}], "count": 15}