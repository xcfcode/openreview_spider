{"notes": [{"id": "HyeEIyBtvr", "original": "B1l8rIa_DB", "number": 1724, "cdate": 1569439563527, "ddate": null, "tcdate": 1569439563527, "tmdate": 1577168223441, "tddate": null, "forum": "HyeEIyBtvr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["fangmuyuan@huawei.com", "wangqiang168@huawei.com", "zhangjian157@huawei.com", "zorro.zhongzhao@huawei.com"], "title": "BETANAS: Balanced Training and selective drop for Neural Architecture Search", "authors": ["Muyuan Fang", "Qiang Wang", "Jian Zhang", "Zhao Zhong"], "pdf": "/pdf/8e2d0f358dbd2f0b23e6aa7317ceb893757f5284.pdf", "TL;DR": "A novel method to search for neural architectures via weight sharing.", "abstract": "Automatic neural architecture search techniques are becoming increasingly important in machine learning area recently.  Especially, weight sharing methods have shown remarkable potentials on searching good network architectures with few computational resources. However, existing weight sharing methods mainly suffer limitations on searching strategies: these methods either uniformly train all network paths to convergence which introduces conflicts between branches and wastes a large amount of computation on unpromising candidates, or selectively train branches with different frequency which leads to unfair evaluation and comparison among paths. To address these issues, we propose a novel neural architecture search method  with balanced training strategy to ensure fair comparisons and a selective drop mechanism to reduces conflicts among candidate paths. The experimental results show that our proposed method can achieve a leading performance of 79.0% on ImageNet under mobile settings, which outperforms other state-of-the-art methods in both accuracy and efficiency.", "keywords": ["neural architecture search", "weight sharing", "auto machine learning", "deep learning", "CNN"], "paperhash": "fang|betanas_balanced_training_and_selective_drop_for_neural_architecture_search", "original_pdf": "/attachment/dc0dae3358b5e3c7bc463b9581efd4020a221132.pdf", "_bibtex": "@misc{\nfang2020betanas,\ntitle={{\\{}BETANAS{\\}}: Balanced Training and selective drop for Neural Architecture Search},\nauthor={Muyuan Fang and Qiang Wang and Jian Zhang and Zhao Zhong},\nyear={2020},\nurl={https://openreview.net/forum?id=HyeEIyBtvr}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "6n5hzSh9ma", "original": null, "number": 1, "cdate": 1576798730816, "ddate": null, "tcdate": 1576798730816, "tmdate": 1576800905673, "tddate": null, "forum": "HyeEIyBtvr", "replyto": "HyeEIyBtvr", "invitation": "ICLR.cc/2020/Conference/Paper1724/-/Decision", "content": {"decision": "Reject", "comment": "This paper proposes a neural architecture search method that uses balanced sampling of architectures from the one-shot model and drops operators whose importance drops below a certain weight.\n\nThe reviewers agreed that the paper's approach is intuitive, but main points of criticism were:\n- Lack of good baselines\n- Potentially unfair comparison, not using the same training pipeline\n- Lack of available code and thus of reproducibility. (The authors promised code in response, which is much appreciated. If the open-sourcing process has completed in time for the next version of the paper, I encourage the authors to include an anonymized version of the code in the submission to avoid this criticism.)\n\nThe reviewers appreciated the authors' rebuttal, but it did not suffice for them to change their ratings.\nI agree with the reviewers that this work may be a solid contribution, but that additional evaluation is needed to demonstrate this. I therefore recommend rejection and encourage resubmission to a different venue after addressing the issues pointed out by the reviewers.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["fangmuyuan@huawei.com", "wangqiang168@huawei.com", "zhangjian157@huawei.com", "zorro.zhongzhao@huawei.com"], "title": "BETANAS: Balanced Training and selective drop for Neural Architecture Search", "authors": ["Muyuan Fang", "Qiang Wang", "Jian Zhang", "Zhao Zhong"], "pdf": "/pdf/8e2d0f358dbd2f0b23e6aa7317ceb893757f5284.pdf", "TL;DR": "A novel method to search for neural architectures via weight sharing.", "abstract": "Automatic neural architecture search techniques are becoming increasingly important in machine learning area recently.  Especially, weight sharing methods have shown remarkable potentials on searching good network architectures with few computational resources. However, existing weight sharing methods mainly suffer limitations on searching strategies: these methods either uniformly train all network paths to convergence which introduces conflicts between branches and wastes a large amount of computation on unpromising candidates, or selectively train branches with different frequency which leads to unfair evaluation and comparison among paths. To address these issues, we propose a novel neural architecture search method  with balanced training strategy to ensure fair comparisons and a selective drop mechanism to reduces conflicts among candidate paths. The experimental results show that our proposed method can achieve a leading performance of 79.0% on ImageNet under mobile settings, which outperforms other state-of-the-art methods in both accuracy and efficiency.", "keywords": ["neural architecture search", "weight sharing", "auto machine learning", "deep learning", "CNN"], "paperhash": "fang|betanas_balanced_training_and_selective_drop_for_neural_architecture_search", "original_pdf": "/attachment/dc0dae3358b5e3c7bc463b9581efd4020a221132.pdf", "_bibtex": "@misc{\nfang2020betanas,\ntitle={{\\{}BETANAS{\\}}: Balanced Training and selective drop for Neural Architecture Search},\nauthor={Muyuan Fang and Qiang Wang and Jian Zhang and Zhao Zhong},\nyear={2020},\nurl={https://openreview.net/forum?id=HyeEIyBtvr}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "HyeEIyBtvr", "replyto": "HyeEIyBtvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795721974, "tmdate": 1576800273167, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1724/-/Decision"}}}, {"id": "rkxrlpTisS", "original": null, "number": 4, "cdate": 1573801197156, "ddate": null, "tcdate": 1573801197156, "tmdate": 1573801269300, "tddate": null, "forum": "HyeEIyBtvr", "replyto": "rygU3SP8jH", "invitation": "ICLR.cc/2020/Conference/Paper1724/-/Official_Comment", "content": {"title": "Additional Response to Reviewer #2", "comment": "We're sorry to say that the code and models could not be open sourced until the approaval from our organization. We have filed the application for releasing code and models with the legal affairs department. This procedure might cost several weeks. Our training curves and experiments settings for all reported models have been uploaded on Github(  https://github.com/BetanasICLR2020/BetaNAS   ) in details. Other materials will be updated once they are permitted. We believe that our method is easy to follow and re-implement. If anyone has some questions on the method and experiments, we will try our best to help other researchers to re-implement the method. "}, "signatures": ["ICLR.cc/2020/Conference/Paper1724/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1724/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["fangmuyuan@huawei.com", "wangqiang168@huawei.com", "zhangjian157@huawei.com", "zorro.zhongzhao@huawei.com"], "title": "BETANAS: Balanced Training and selective drop for Neural Architecture Search", "authors": ["Muyuan Fang", "Qiang Wang", "Jian Zhang", "Zhao Zhong"], "pdf": "/pdf/8e2d0f358dbd2f0b23e6aa7317ceb893757f5284.pdf", "TL;DR": "A novel method to search for neural architectures via weight sharing.", "abstract": "Automatic neural architecture search techniques are becoming increasingly important in machine learning area recently.  Especially, weight sharing methods have shown remarkable potentials on searching good network architectures with few computational resources. However, existing weight sharing methods mainly suffer limitations on searching strategies: these methods either uniformly train all network paths to convergence which introduces conflicts between branches and wastes a large amount of computation on unpromising candidates, or selectively train branches with different frequency which leads to unfair evaluation and comparison among paths. To address these issues, we propose a novel neural architecture search method  with balanced training strategy to ensure fair comparisons and a selective drop mechanism to reduces conflicts among candidate paths. The experimental results show that our proposed method can achieve a leading performance of 79.0% on ImageNet under mobile settings, which outperforms other state-of-the-art methods in both accuracy and efficiency.", "keywords": ["neural architecture search", "weight sharing", "auto machine learning", "deep learning", "CNN"], "paperhash": "fang|betanas_balanced_training_and_selective_drop_for_neural_architecture_search", "original_pdf": "/attachment/dc0dae3358b5e3c7bc463b9581efd4020a221132.pdf", "_bibtex": "@misc{\nfang2020betanas,\ntitle={{\\{}BETANAS{\\}}: Balanced Training and selective drop for Neural Architecture Search},\nauthor={Muyuan Fang and Qiang Wang and Jian Zhang and Zhao Zhong},\nyear={2020},\nurl={https://openreview.net/forum?id=HyeEIyBtvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HyeEIyBtvr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1724/Authors", "ICLR.cc/2020/Conference/Paper1724/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1724/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1724/Reviewers", "ICLR.cc/2020/Conference/Paper1724/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1724/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1724/Authors|ICLR.cc/2020/Conference/Paper1724/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504151803, "tmdate": 1576860557837, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1724/Authors", "ICLR.cc/2020/Conference/Paper1724/Reviewers", "ICLR.cc/2020/Conference/Paper1724/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1724/-/Official_Comment"}}}, {"id": "BkxkWIvUiS", "original": null, "number": 3, "cdate": 1573447158950, "ddate": null, "tcdate": 1573447158950, "tmdate": 1573528159554, "tddate": null, "forum": "HyeEIyBtvr", "replyto": "rkg_5UjRYr", "invitation": "ICLR.cc/2020/Conference/Paper1724/-/Official_Comment", "content": {"title": "Response to Reviewer #1", "comment": "Thanks for your instructive comments and suggestion. \n1)\nWe have updated our paper with evaluation experiments according to \"Evaluating the Search Phase of Neural Architecture Search\". More experiments with different random seeds are conducted to compare our methods with randomly sampled architectures from the same search space. Our method outperforms the randomly sampled ones and other competing methods, as shown in Section 4.3.2 and Fig.6. \n\n2) The suggestion is meaningful and instructive. We have made a discussion on recently weight sharing methods in Tab(3) and Section 4.3.4, to compare their training approaches, searching strategies, efficiency, etc. These papers and \"Evaluating ...\" are introduced to related works and analyzed in Experiments and Discussion sections.\n\nTable 3. Comparison with the other weight sharing methods on their strategies.\n+----------------+--------------------------------+--------------------+-----------------------+--------------------------------+\n| Method      | supernet optimization | search policy | sample sub-net | mutual interference     |\n+----------------+--------------------------------+--------------------+-----------------------+--------------------------------+\n| ENAS          | imbalanced                    | RL                     | \u221a                           | from all candidates      |\n+----------------+--------------------------------+--------------------+-----------------------+--------------------------------+\n| Darts          | imbalanced                    | gradient          | x                           | from all candidates      |\n+----------------+--------------------------------+--------------------+-----------------------+--------------------------------+\n| SinglePath | balanced                        | EA                     | \u221a                           | from all candidates      |\n+----------------+--------------------------------+--------------------+-----------------------+--------------------------------+\n| ProxyLess  | imbalanced                    | gradient/RL    | \u221a                           | from all candidates      |\n+----------------+--------------------------------+--------------------+-----------------------+--------------------------------+\n| Progressive| imbalanced                   | gradient         | x                            | from fewer candidates|\n+----------------+--------------------------------+--------------------+-----------------------+--------------------------------+\n| Darts+        | imbalanced                    | gradient          | x                           | from all candidates      |\n+----------------+--------------------------------+--------------------+-----------------------+--------------------------------+\n| HM-NAS     | imbalanced                   | gradient+mask| x                          | from all candidates      |\n+----------------+--------------------------------+--------------------+-----------------------+--------------------------------+\n| Ours           | balanced                        | gradient           | \u221a                          | from fewer candidates |\n+----------------+--------------------------------+--------------------+-----------------------+--------------------------------+\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1724/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1724/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["fangmuyuan@huawei.com", "wangqiang168@huawei.com", "zhangjian157@huawei.com", "zorro.zhongzhao@huawei.com"], "title": "BETANAS: Balanced Training and selective drop for Neural Architecture Search", "authors": ["Muyuan Fang", "Qiang Wang", "Jian Zhang", "Zhao Zhong"], "pdf": "/pdf/8e2d0f358dbd2f0b23e6aa7317ceb893757f5284.pdf", "TL;DR": "A novel method to search for neural architectures via weight sharing.", "abstract": "Automatic neural architecture search techniques are becoming increasingly important in machine learning area recently.  Especially, weight sharing methods have shown remarkable potentials on searching good network architectures with few computational resources. However, existing weight sharing methods mainly suffer limitations on searching strategies: these methods either uniformly train all network paths to convergence which introduces conflicts between branches and wastes a large amount of computation on unpromising candidates, or selectively train branches with different frequency which leads to unfair evaluation and comparison among paths. To address these issues, we propose a novel neural architecture search method  with balanced training strategy to ensure fair comparisons and a selective drop mechanism to reduces conflicts among candidate paths. The experimental results show that our proposed method can achieve a leading performance of 79.0% on ImageNet under mobile settings, which outperforms other state-of-the-art methods in both accuracy and efficiency.", "keywords": ["neural architecture search", "weight sharing", "auto machine learning", "deep learning", "CNN"], "paperhash": "fang|betanas_balanced_training_and_selective_drop_for_neural_architecture_search", "original_pdf": "/attachment/dc0dae3358b5e3c7bc463b9581efd4020a221132.pdf", "_bibtex": "@misc{\nfang2020betanas,\ntitle={{\\{}BETANAS{\\}}: Balanced Training and selective drop for Neural Architecture Search},\nauthor={Muyuan Fang and Qiang Wang and Jian Zhang and Zhao Zhong},\nyear={2020},\nurl={https://openreview.net/forum?id=HyeEIyBtvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HyeEIyBtvr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1724/Authors", "ICLR.cc/2020/Conference/Paper1724/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1724/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1724/Reviewers", "ICLR.cc/2020/Conference/Paper1724/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1724/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1724/Authors|ICLR.cc/2020/Conference/Paper1724/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504151803, "tmdate": 1576860557837, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1724/Authors", "ICLR.cc/2020/Conference/Paper1724/Reviewers", "ICLR.cc/2020/Conference/Paper1724/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1724/-/Official_Comment"}}}, {"id": "rygU3SP8jH", "original": null, "number": 2, "cdate": 1573447086178, "ddate": null, "tcdate": 1573447086178, "tmdate": 1573527776427, "tddate": null, "forum": "HyeEIyBtvr", "replyto": "BkxCrcjmcH", "invitation": "ICLR.cc/2020/Conference/Paper1724/-/Official_Comment", "content": {"title": "Response to Reviewer #2", "comment": "Thanks for your meaningful comments and suggestion. The model architectures, weights and training hyper-parameters of the reported BetaNet-A and BetaNet-B will be open sourced in Google Drive in 2 ~3 days. We believe that our method is easy to follow and re-implement."}, "signatures": ["ICLR.cc/2020/Conference/Paper1724/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1724/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["fangmuyuan@huawei.com", "wangqiang168@huawei.com", "zhangjian157@huawei.com", "zorro.zhongzhao@huawei.com"], "title": "BETANAS: Balanced Training and selective drop for Neural Architecture Search", "authors": ["Muyuan Fang", "Qiang Wang", "Jian Zhang", "Zhao Zhong"], "pdf": "/pdf/8e2d0f358dbd2f0b23e6aa7317ceb893757f5284.pdf", "TL;DR": "A novel method to search for neural architectures via weight sharing.", "abstract": "Automatic neural architecture search techniques are becoming increasingly important in machine learning area recently.  Especially, weight sharing methods have shown remarkable potentials on searching good network architectures with few computational resources. However, existing weight sharing methods mainly suffer limitations on searching strategies: these methods either uniformly train all network paths to convergence which introduces conflicts between branches and wastes a large amount of computation on unpromising candidates, or selectively train branches with different frequency which leads to unfair evaluation and comparison among paths. To address these issues, we propose a novel neural architecture search method  with balanced training strategy to ensure fair comparisons and a selective drop mechanism to reduces conflicts among candidate paths. The experimental results show that our proposed method can achieve a leading performance of 79.0% on ImageNet under mobile settings, which outperforms other state-of-the-art methods in both accuracy and efficiency.", "keywords": ["neural architecture search", "weight sharing", "auto machine learning", "deep learning", "CNN"], "paperhash": "fang|betanas_balanced_training_and_selective_drop_for_neural_architecture_search", "original_pdf": "/attachment/dc0dae3358b5e3c7bc463b9581efd4020a221132.pdf", "_bibtex": "@misc{\nfang2020betanas,\ntitle={{\\{}BETANAS{\\}}: Balanced Training and selective drop for Neural Architecture Search},\nauthor={Muyuan Fang and Qiang Wang and Jian Zhang and Zhao Zhong},\nyear={2020},\nurl={https://openreview.net/forum?id=HyeEIyBtvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HyeEIyBtvr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1724/Authors", "ICLR.cc/2020/Conference/Paper1724/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1724/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1724/Reviewers", "ICLR.cc/2020/Conference/Paper1724/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1724/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1724/Authors|ICLR.cc/2020/Conference/Paper1724/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504151803, "tmdate": 1576860557837, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1724/Authors", "ICLR.cc/2020/Conference/Paper1724/Reviewers", "ICLR.cc/2020/Conference/Paper1724/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1724/-/Official_Comment"}}}, {"id": "BkeEGBDIiH", "original": null, "number": 1, "cdate": 1573446923631, "ddate": null, "tcdate": 1573446923631, "tmdate": 1573447007514, "tddate": null, "forum": "HyeEIyBtvr", "replyto": "SJxVImW65r", "invitation": "ICLR.cc/2020/Conference/Paper1724/-/Official_Comment", "content": {"title": "Response to Reviewer #3", "comment": "Thanks for your helpful comments. \n1)\"Matthew Effect\" means that the rich get richer and the poor get poorer (wiki), originally. We use this description as an analogy to describe the unfairly training strategy in some weight sharing methods. In those methods, paths with better performance at early searching phases can obtain more training opportunities than other candidates, and further enhance their performance. \n2) alpha is calculated according to Eq (4). We first normalize alpha to its softmax form p, after that a path will be dropped once its corresponding p is lower than th_\\alpha. In our experiments, th_\\alpha is set to a constant 0.5. We found that overall performance is largely robust (insensitive) to that parameter (th_\\alpha), with less than 0.3% top1 variation when varying 0.4\u2264 th_\\alpha \u2264 0.6. On one hand, when th_\\alpha is set too high, some promising paths might be dropped at early stages, leading to degraded performance. On the other hand, when th_\\alpha is set too low, paths will be dropped slow and we need more searching steps to keep the amount of finally remaining paths small enough. th_alpha is set to 0.5 to ensure that when trained to convergence, there are 2 or 3 paths remaining in each choice block. \n3) Thanks very much for your constructive and detailed comments. We will fix the typos."}, "signatures": ["ICLR.cc/2020/Conference/Paper1724/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1724/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["fangmuyuan@huawei.com", "wangqiang168@huawei.com", "zhangjian157@huawei.com", "zorro.zhongzhao@huawei.com"], "title": "BETANAS: Balanced Training and selective drop for Neural Architecture Search", "authors": ["Muyuan Fang", "Qiang Wang", "Jian Zhang", "Zhao Zhong"], "pdf": "/pdf/8e2d0f358dbd2f0b23e6aa7317ceb893757f5284.pdf", "TL;DR": "A novel method to search for neural architectures via weight sharing.", "abstract": "Automatic neural architecture search techniques are becoming increasingly important in machine learning area recently.  Especially, weight sharing methods have shown remarkable potentials on searching good network architectures with few computational resources. However, existing weight sharing methods mainly suffer limitations on searching strategies: these methods either uniformly train all network paths to convergence which introduces conflicts between branches and wastes a large amount of computation on unpromising candidates, or selectively train branches with different frequency which leads to unfair evaluation and comparison among paths. To address these issues, we propose a novel neural architecture search method  with balanced training strategy to ensure fair comparisons and a selective drop mechanism to reduces conflicts among candidate paths. The experimental results show that our proposed method can achieve a leading performance of 79.0% on ImageNet under mobile settings, which outperforms other state-of-the-art methods in both accuracy and efficiency.", "keywords": ["neural architecture search", "weight sharing", "auto machine learning", "deep learning", "CNN"], "paperhash": "fang|betanas_balanced_training_and_selective_drop_for_neural_architecture_search", "original_pdf": "/attachment/dc0dae3358b5e3c7bc463b9581efd4020a221132.pdf", "_bibtex": "@misc{\nfang2020betanas,\ntitle={{\\{}BETANAS{\\}}: Balanced Training and selective drop for Neural Architecture Search},\nauthor={Muyuan Fang and Qiang Wang and Jian Zhang and Zhao Zhong},\nyear={2020},\nurl={https://openreview.net/forum?id=HyeEIyBtvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HyeEIyBtvr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1724/Authors", "ICLR.cc/2020/Conference/Paper1724/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1724/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1724/Reviewers", "ICLR.cc/2020/Conference/Paper1724/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1724/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1724/Authors|ICLR.cc/2020/Conference/Paper1724/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504151803, "tmdate": 1576860557837, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1724/Authors", "ICLR.cc/2020/Conference/Paper1724/Reviewers", "ICLR.cc/2020/Conference/Paper1724/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1724/-/Official_Comment"}}}, {"id": "rkg_5UjRYr", "original": null, "number": 1, "cdate": 1571890831624, "ddate": null, "tcdate": 1571890831624, "tmdate": 1572972431678, "tddate": null, "forum": "HyeEIyBtvr", "replyto": "HyeEIyBtvr", "invitation": "ICLR.cc/2020/Conference/Paper1724/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper introduces a better searching strategy in the context of automatic neural architecture search (NAS). Especially, they focus on improving the search strategy for previously proposed computationally effective weight sharing methods for NAS. Current search strategies for the weight sharing NAS methods either focus on uniformly training all the network paths or selectively train different network paths with different frequency, where both have their own issues like wasting resources for unpromising candidates and unfair comparison among network paths. To this end, this paper proposes a balanced training strategy with \u201cselective drop mechanism\u201d. Further, they validate their approach by showing leading performance on ImageNet under mobile settings.\n\nOverall, I appreciate the effort on exploring new strategies for better search algorithm for NAS in the context of weight sharing methods. However, the analysis of the current approach is limited and some relevant papers are missing. More details below. \n\nArguments:\n1) This paper\u2019s main focus is on developing a better search strategy for NAS based weight sharing methods. The validation of their approach is supported via showing improvement in the accuracy on ImageNet task. However, my main argument is that you have to validate the proposed approach with better analysis on the search space along with improvement on the tasks. For example, \u201cEvaluating the Search Phase of Neural Architecture Search\u201d paper which studies the effectiveness of current search strategies is not referenced in this paper. I would suggest to do analysis of you search strategy by following the analysis experiments conducted in the above paper. \n\n2) The related work doesn\u2019t discuss the latest papers which suggest why and in what ways the current search strategies are based for weight-sharing based NAS approaches. As this paper is trying to address this problem, I would naturally assume that this is well discussed, but it\u2019s missing!\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1724/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1724/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["fangmuyuan@huawei.com", "wangqiang168@huawei.com", "zhangjian157@huawei.com", "zorro.zhongzhao@huawei.com"], "title": "BETANAS: Balanced Training and selective drop for Neural Architecture Search", "authors": ["Muyuan Fang", "Qiang Wang", "Jian Zhang", "Zhao Zhong"], "pdf": "/pdf/8e2d0f358dbd2f0b23e6aa7317ceb893757f5284.pdf", "TL;DR": "A novel method to search for neural architectures via weight sharing.", "abstract": "Automatic neural architecture search techniques are becoming increasingly important in machine learning area recently.  Especially, weight sharing methods have shown remarkable potentials on searching good network architectures with few computational resources. However, existing weight sharing methods mainly suffer limitations on searching strategies: these methods either uniformly train all network paths to convergence which introduces conflicts between branches and wastes a large amount of computation on unpromising candidates, or selectively train branches with different frequency which leads to unfair evaluation and comparison among paths. To address these issues, we propose a novel neural architecture search method  with balanced training strategy to ensure fair comparisons and a selective drop mechanism to reduces conflicts among candidate paths. The experimental results show that our proposed method can achieve a leading performance of 79.0% on ImageNet under mobile settings, which outperforms other state-of-the-art methods in both accuracy and efficiency.", "keywords": ["neural architecture search", "weight sharing", "auto machine learning", "deep learning", "CNN"], "paperhash": "fang|betanas_balanced_training_and_selective_drop_for_neural_architecture_search", "original_pdf": "/attachment/dc0dae3358b5e3c7bc463b9581efd4020a221132.pdf", "_bibtex": "@misc{\nfang2020betanas,\ntitle={{\\{}BETANAS{\\}}: Balanced Training and selective drop for Neural Architecture Search},\nauthor={Muyuan Fang and Qiang Wang and Jian Zhang and Zhao Zhong},\nyear={2020},\nurl={https://openreview.net/forum?id=HyeEIyBtvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HyeEIyBtvr", "replyto": "HyeEIyBtvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1724/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1724/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575955608314, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1724/Reviewers"], "noninvitees": [], "tcdate": 1570237733220, "tmdate": 1575955608328, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1724/-/Official_Review"}}}, {"id": "BkxCrcjmcH", "original": null, "number": 2, "cdate": 1572219462451, "ddate": null, "tcdate": 1572219462451, "tmdate": 1572972431635, "tddate": null, "forum": "HyeEIyBtvr", "replyto": "HyeEIyBtvr", "invitation": "ICLR.cc/2020/Conference/Paper1724/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "In this paper, the authors proposed a new training strategy in achieving better balance between training efficiency and evaluation accuracy with weight sharing-based NAS algorithms. It is consisted of two phrases: in phrase 1, all path are uniformly trained to avoid bias, in phrase 2, less competitive options are pruned to save cost. The proposed method achieved the SOTA on IN mobile setting. \n\n\nOverall I found the idea proposed in the paper intuitive and convincing. Especially I appreciate the ablation study that identified one of the option is encouraged too much in the early stage will can lead to worse final performance. From the methodology perspective, I think this is a solid incremental contribution. However, the highlight of this paper, in my opinion, is the SOTA results on IN. My main concern is that the authors did not indicate that the code/model will be open sourced, which will help verification as well as reproducibility. "}, "signatures": ["ICLR.cc/2020/Conference/Paper1724/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1724/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["fangmuyuan@huawei.com", "wangqiang168@huawei.com", "zhangjian157@huawei.com", "zorro.zhongzhao@huawei.com"], "title": "BETANAS: Balanced Training and selective drop for Neural Architecture Search", "authors": ["Muyuan Fang", "Qiang Wang", "Jian Zhang", "Zhao Zhong"], "pdf": "/pdf/8e2d0f358dbd2f0b23e6aa7317ceb893757f5284.pdf", "TL;DR": "A novel method to search for neural architectures via weight sharing.", "abstract": "Automatic neural architecture search techniques are becoming increasingly important in machine learning area recently.  Especially, weight sharing methods have shown remarkable potentials on searching good network architectures with few computational resources. However, existing weight sharing methods mainly suffer limitations on searching strategies: these methods either uniformly train all network paths to convergence which introduces conflicts between branches and wastes a large amount of computation on unpromising candidates, or selectively train branches with different frequency which leads to unfair evaluation and comparison among paths. To address these issues, we propose a novel neural architecture search method  with balanced training strategy to ensure fair comparisons and a selective drop mechanism to reduces conflicts among candidate paths. The experimental results show that our proposed method can achieve a leading performance of 79.0% on ImageNet under mobile settings, which outperforms other state-of-the-art methods in both accuracy and efficiency.", "keywords": ["neural architecture search", "weight sharing", "auto machine learning", "deep learning", "CNN"], "paperhash": "fang|betanas_balanced_training_and_selective_drop_for_neural_architecture_search", "original_pdf": "/attachment/dc0dae3358b5e3c7bc463b9581efd4020a221132.pdf", "_bibtex": "@misc{\nfang2020betanas,\ntitle={{\\{}BETANAS{\\}}: Balanced Training and selective drop for Neural Architecture Search},\nauthor={Muyuan Fang and Qiang Wang and Jian Zhang and Zhao Zhong},\nyear={2020},\nurl={https://openreview.net/forum?id=HyeEIyBtvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HyeEIyBtvr", "replyto": "HyeEIyBtvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1724/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1724/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575955608314, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1724/Reviewers"], "noninvitees": [], "tcdate": 1570237733220, "tmdate": 1575955608328, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1724/-/Official_Review"}}}, {"id": "SJxVImW65r", "original": null, "number": 3, "cdate": 1572832075617, "ddate": null, "tcdate": 1572832075617, "tmdate": 1572972431593, "tddate": null, "forum": "HyeEIyBtvr", "replyto": "HyeEIyBtvr", "invitation": "ICLR.cc/2020/Conference/Paper1724/-/Official_Review", "content": {"rating": "6: Weak Accept", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper relates to automatic neural architecture search techniques. Current methods have certain drawbacks: Some train all network paths to convergence, which wastes computational efforts in unpromising paths, whereas some don't train all the branches uniformly, which can lead to unfair comparisons.\n\nThe authors propose a model to balance the two issues mentioned above. Their aim is to produced balanced training while trying to reduce conflicts between the different potential network paths.  So their algorithm Has two phases, one where it randomly builds a block in the network and another one where it discards vertices form the layer that are below a certain threshold. \n\nThe experimental results seem sound to me, and I think this is a reasonable approach. \n\nHere some general comments that would help with clarity:\n\n1) I think the authors should explain more clearly what \"Matthew Effect\" in the introduction\n\n2) It's not very clear to me how th_\\alpha is computed. Could this please me made more specific. Section 4.4 says that \"only the operators with performance much lower than the average of others will be dropped.\" Is this approach conservative? Did they try different thresholds?\n\nThere are several minor typos that the authors might want to correct.\n\n1) There is a couple of spaces missing like between \"2018)have shown\" in page 1\n    - The word \"probability\" and p_1, p_2 in (1)\n    - Eq6 on page 5\n\n2) It is customary to use commas before and after \\ldots if one is listing a sequence. The authors don't do this in any of their lists, and this is very strange. \n\n3) In the description of Algorithm 1, I'd change \"S_max is denoted as\" for \"S_max denotes\"\n\n4) In (7) I think there is a \"{\" and a\"}\" missing before and after the o_{l,m_i}. It's set notation. \n\n5) In the discussion, they write the word \"differently.\" Would it be better to write \"by contrast\"?"}, "signatures": ["ICLR.cc/2020/Conference/Paper1724/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1724/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["fangmuyuan@huawei.com", "wangqiang168@huawei.com", "zhangjian157@huawei.com", "zorro.zhongzhao@huawei.com"], "title": "BETANAS: Balanced Training and selective drop for Neural Architecture Search", "authors": ["Muyuan Fang", "Qiang Wang", "Jian Zhang", "Zhao Zhong"], "pdf": "/pdf/8e2d0f358dbd2f0b23e6aa7317ceb893757f5284.pdf", "TL;DR": "A novel method to search for neural architectures via weight sharing.", "abstract": "Automatic neural architecture search techniques are becoming increasingly important in machine learning area recently.  Especially, weight sharing methods have shown remarkable potentials on searching good network architectures with few computational resources. However, existing weight sharing methods mainly suffer limitations on searching strategies: these methods either uniformly train all network paths to convergence which introduces conflicts between branches and wastes a large amount of computation on unpromising candidates, or selectively train branches with different frequency which leads to unfair evaluation and comparison among paths. To address these issues, we propose a novel neural architecture search method  with balanced training strategy to ensure fair comparisons and a selective drop mechanism to reduces conflicts among candidate paths. The experimental results show that our proposed method can achieve a leading performance of 79.0% on ImageNet under mobile settings, which outperforms other state-of-the-art methods in both accuracy and efficiency.", "keywords": ["neural architecture search", "weight sharing", "auto machine learning", "deep learning", "CNN"], "paperhash": "fang|betanas_balanced_training_and_selective_drop_for_neural_architecture_search", "original_pdf": "/attachment/dc0dae3358b5e3c7bc463b9581efd4020a221132.pdf", "_bibtex": "@misc{\nfang2020betanas,\ntitle={{\\{}BETANAS{\\}}: Balanced Training and selective drop for Neural Architecture Search},\nauthor={Muyuan Fang and Qiang Wang and Jian Zhang and Zhao Zhong},\nyear={2020},\nurl={https://openreview.net/forum?id=HyeEIyBtvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HyeEIyBtvr", "replyto": "HyeEIyBtvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1724/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1724/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575955608314, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1724/Reviewers"], "noninvitees": [], "tcdate": 1570237733220, "tmdate": 1575955608328, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1724/-/Official_Review"}}}], "count": 9}