{"notes": [{"id": "BJlqYlrtPB", "original": "S1l-VAgKPH", "number": 2445, "cdate": 1569439874145, "ddate": null, "tcdate": 1569439874145, "tmdate": 1577168258874, "tddate": null, "forum": "BJlqYlrtPB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["csadrian@renyi.hu", "bbeatrix1010@gmail.com", "daniel@renyi.hu"], "title": "Negative Sampling in Variational Autoencoders", "authors": ["Adri\u00e1n Csisz\u00e1rik", "Beatrix Benk\u0151", "D\u00e1niel Varga"], "pdf": "/pdf/1923aaa1393f2fec05882c3cf221cf1950bf36fe.pdf", "TL;DR": "Pulling near-manifold examples (utilizing an auxiliary dataset or generated samples) to a secondary prior improves the discriminative power of VAE models regarding out-of-distribution samples.", "abstract": "We propose negative sampling as an approach to improve the notoriously bad out-of-distribution likelihood estimates of Variational Autoencoder models. Our model pushes latent images of negative samples away from the prior. When the source of negative samples is an auxiliary dataset, such a model can vastly improve on baselines when evaluated on OOD detection tasks. Perhaps more surprisingly, we present a fully unsupervised variant that can also significantly improve detection performance: using the output of the generator as a source of negative samples results in a fully unsupervised model that can be interpreted as adversarially trained.\n", "keywords": ["Variational Autoencoder", "generative modelling", "out-of-distribution detection"], "paperhash": "csisz\u00e1rik|negative_sampling_in_variational_autoencoders", "original_pdf": "/attachment/01efe373073cdf6154c409d9fe028a6c4d482fa1.pdf", "_bibtex": "@misc{\ncsisz{\\'a}rik2020negative,\ntitle={Negative Sampling in Variational Autoencoders},\nauthor={Adri{\\'a}n Csisz{\\'a}rik and Beatrix Benk{\\H{o}} and D{\\'a}niel Varga},\nyear={2020},\nurl={https://openreview.net/forum?id=BJlqYlrtPB}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "zHl_Nylh-T", "original": null, "number": 1, "cdate": 1576798749327, "ddate": null, "tcdate": 1576798749327, "tmdate": 1576800886586, "tddate": null, "forum": "BJlqYlrtPB", "replyto": "BJlqYlrtPB", "invitation": "ICLR.cc/2020/Conference/Paper2445/-/Decision", "content": {"decision": "Reject", "comment": "This paper proposes to improve VAEs' modeling of out-of-distribution examples, by pushing the latent representations of negative examples away from the prior.  The general idea seems interesting, at least to some of the reviewers and to me.  However, the paper seems premature, even after revision, as it leaves unclear some of the justification and analysis of the approach, especially in the fully unsupervised case.  I think that with some more work it could be a very compelling contribution to a future conference.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["csadrian@renyi.hu", "bbeatrix1010@gmail.com", "daniel@renyi.hu"], "title": "Negative Sampling in Variational Autoencoders", "authors": ["Adri\u00e1n Csisz\u00e1rik", "Beatrix Benk\u0151", "D\u00e1niel Varga"], "pdf": "/pdf/1923aaa1393f2fec05882c3cf221cf1950bf36fe.pdf", "TL;DR": "Pulling near-manifold examples (utilizing an auxiliary dataset or generated samples) to a secondary prior improves the discriminative power of VAE models regarding out-of-distribution samples.", "abstract": "We propose negative sampling as an approach to improve the notoriously bad out-of-distribution likelihood estimates of Variational Autoencoder models. Our model pushes latent images of negative samples away from the prior. When the source of negative samples is an auxiliary dataset, such a model can vastly improve on baselines when evaluated on OOD detection tasks. Perhaps more surprisingly, we present a fully unsupervised variant that can also significantly improve detection performance: using the output of the generator as a source of negative samples results in a fully unsupervised model that can be interpreted as adversarially trained.\n", "keywords": ["Variational Autoencoder", "generative modelling", "out-of-distribution detection"], "paperhash": "csisz\u00e1rik|negative_sampling_in_variational_autoencoders", "original_pdf": "/attachment/01efe373073cdf6154c409d9fe028a6c4d482fa1.pdf", "_bibtex": "@misc{\ncsisz{\\'a}rik2020negative,\ntitle={Negative Sampling in Variational Autoencoders},\nauthor={Adri{\\'a}n Csisz{\\'a}rik and Beatrix Benk{\\H{o}} and D{\\'a}niel Varga},\nyear={2020},\nurl={https://openreview.net/forum?id=BJlqYlrtPB}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "BJlqYlrtPB", "replyto": "BJlqYlrtPB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795709218, "tmdate": 1576800257901, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2445/-/Decision"}}}, {"id": "S1leviI0YB", "original": null, "number": 3, "cdate": 1571871576494, "ddate": null, "tcdate": 1571871576494, "tmdate": 1574456699107, "tddate": null, "forum": "BJlqYlrtPB", "replyto": "BJlqYlrtPB", "invitation": "ICLR.cc/2020/Conference/Paper2445/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #2", "review": "Summary:\nThe authors propose augmenting VAEs with an additional latent variable to allow them to detect out-of-distribution (OOD) data. They propose several measures based on this model to distinguish between inliers and outliers, and evaluate the model empirically, finding it successful.\n\nUnfortunately, the method in this paper is developed unclearly and incorrectly. Although their experiments are somewhat successful, the problems with the text and method are severe enough to justify rejection.\n\nSpecifically, the authors' method proposes adding a term to the loss of the VAE that encourages the variational posterior (q) to distribute latent codes (z) for inliers and outliers differently. The equation which defines their new objective is unclear -- specifically, it is not clear whether the added KL term is computed for inliers and outliers both, or whether it is only computed for outliers. If it is the former, then the method does not make sense. If it is the latter, then the equation is incorrect or at the very least not clear in the extreme.\n\nFurthermore, the term is added without consideration of whether or not the method is still optimizing a sensible variational lower bound. The authors attempt to justify the objective by writing out a variational lower bound for a VAE with a mixture prior where inliers and outliers are generated from different mixture components. However, their equations are incorrect -- the equation that is called the log likelihood is not the log likelihood, and the ELBO is similarly wrong.\n\nTheir empirical evaluation is reasonable, although the measures they propose to distinguish between inliers and outliers (i.e. the kl from the approximate posterior to the prior) is not thoroughly justified.\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper2445/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2445/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["csadrian@renyi.hu", "bbeatrix1010@gmail.com", "daniel@renyi.hu"], "title": "Negative Sampling in Variational Autoencoders", "authors": ["Adri\u00e1n Csisz\u00e1rik", "Beatrix Benk\u0151", "D\u00e1niel Varga"], "pdf": "/pdf/1923aaa1393f2fec05882c3cf221cf1950bf36fe.pdf", "TL;DR": "Pulling near-manifold examples (utilizing an auxiliary dataset or generated samples) to a secondary prior improves the discriminative power of VAE models regarding out-of-distribution samples.", "abstract": "We propose negative sampling as an approach to improve the notoriously bad out-of-distribution likelihood estimates of Variational Autoencoder models. Our model pushes latent images of negative samples away from the prior. When the source of negative samples is an auxiliary dataset, such a model can vastly improve on baselines when evaluated on OOD detection tasks. Perhaps more surprisingly, we present a fully unsupervised variant that can also significantly improve detection performance: using the output of the generator as a source of negative samples results in a fully unsupervised model that can be interpreted as adversarially trained.\n", "keywords": ["Variational Autoencoder", "generative modelling", "out-of-distribution detection"], "paperhash": "csisz\u00e1rik|negative_sampling_in_variational_autoencoders", "original_pdf": "/attachment/01efe373073cdf6154c409d9fe028a6c4d482fa1.pdf", "_bibtex": "@misc{\ncsisz{\\'a}rik2020negative,\ntitle={Negative Sampling in Variational Autoencoders},\nauthor={Adri{\\'a}n Csisz{\\'a}rik and Beatrix Benk{\\H{o}} and D{\\'a}niel Varga},\nyear={2020},\nurl={https://openreview.net/forum?id=BJlqYlrtPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BJlqYlrtPB", "replyto": "BJlqYlrtPB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2445/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2445/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575652510757, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2445/Reviewers"], "noninvitees": [], "tcdate": 1570237722709, "tmdate": 1575652510774, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2445/-/Official_Review"}}}, {"id": "BkgalbL3FH", "original": null, "number": 2, "cdate": 1571737845233, "ddate": null, "tcdate": 1571737845233, "tmdate": 1574250364621, "tddate": null, "forum": "BJlqYlrtPB", "replyto": "BJlqYlrtPB", "invitation": "ICLR.cc/2020/Conference/Paper2445/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #1", "review": "The paper proposes to counteract OOD problem in VAE by adding a regularization term to the ELBO. The regularizer is defined as the Kullback-Leibler divergence between a variational posterior for a negative sample and a marginal distribution over latents for negative data. The authors present experiments on MNIST and MNIST-like datasets, and CIFAR10 with SVHN. Unfortunately, I do not find the paper especially interesting. The motivation for adding the regularization term is not convincing. The experiments are insufficiently discussed.\n\nRemarks:\n- The paper proposes to ad a regularization to ELBO, namely, the Kullback-Leibler divergence between a variational distribution for a negative sample and a marginal distribution over latent variables for negative samples. I do not fully understand the motivation given on page 3. The authors show that including the negative data yields a new objective that is a sum of two log-lihelihood functions for \"real\" and negative data. However, later they propose to skip a (negative) reconstruction error term for the negative data. As a result, the authors obtain the objective they proposed. This explanation is very vague and I do not see what it adds to the story. Contrary, it causes new questions about their model and whether it is properly formulated.\nI suggest to look into the following paper to see whether the model could be re-formulated:\nHu, Z., Yang, Z., Salakhutdinov, R., & Xing, E. P. (2017). On unifying deep generative models. arXiv preprint arXiv:1706.00550.\n\n- I do not understand why the authors used Bernoulli distribution to model color and gray-scale images. The Bernoulli distribution could be used only for binary random variables. This is obviously flawed.\n\n- In general, the results seem to partially confirm claims of the paper, however, they are quite vague. First, utilizing a wrong distribution is demotivating (see my previous remark). Second, I miss a better description of models and, in general, experiments' setup. Third, all results are explained in a laconic manner (e.g., \"The other results in the table (...) confirm the assymetric behaviour of the phenomenon (...)\"). There is neither deeper understanding nor discussion provided.\n\n- Why there are no samples for CIFAR or SVHN provided?\n\n======== AFTER REBUTTAL ========\nI would like to thank the authors for their rebuttal. I really appreciate that the paper is updated and some concerns are solved. After reading the updated paper again, I tend to agree that the proposed idea is interesting for the problem of OOD detection using generative models. Therefore, I decide to update my score.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper2445/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2445/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["csadrian@renyi.hu", "bbeatrix1010@gmail.com", "daniel@renyi.hu"], "title": "Negative Sampling in Variational Autoencoders", "authors": ["Adri\u00e1n Csisz\u00e1rik", "Beatrix Benk\u0151", "D\u00e1niel Varga"], "pdf": "/pdf/1923aaa1393f2fec05882c3cf221cf1950bf36fe.pdf", "TL;DR": "Pulling near-manifold examples (utilizing an auxiliary dataset or generated samples) to a secondary prior improves the discriminative power of VAE models regarding out-of-distribution samples.", "abstract": "We propose negative sampling as an approach to improve the notoriously bad out-of-distribution likelihood estimates of Variational Autoencoder models. Our model pushes latent images of negative samples away from the prior. When the source of negative samples is an auxiliary dataset, such a model can vastly improve on baselines when evaluated on OOD detection tasks. Perhaps more surprisingly, we present a fully unsupervised variant that can also significantly improve detection performance: using the output of the generator as a source of negative samples results in a fully unsupervised model that can be interpreted as adversarially trained.\n", "keywords": ["Variational Autoencoder", "generative modelling", "out-of-distribution detection"], "paperhash": "csisz\u00e1rik|negative_sampling_in_variational_autoencoders", "original_pdf": "/attachment/01efe373073cdf6154c409d9fe028a6c4d482fa1.pdf", "_bibtex": "@misc{\ncsisz{\\'a}rik2020negative,\ntitle={Negative Sampling in Variational Autoencoders},\nauthor={Adri{\\'a}n Csisz{\\'a}rik and Beatrix Benk{\\H{o}} and D{\\'a}niel Varga},\nyear={2020},\nurl={https://openreview.net/forum?id=BJlqYlrtPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BJlqYlrtPB", "replyto": "BJlqYlrtPB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2445/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2445/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575652510757, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2445/Reviewers"], "noninvitees": [], "tcdate": 1570237722709, "tmdate": 1575652510774, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2445/-/Official_Review"}}}, {"id": "Sylxlkrhsr", "original": null, "number": 5, "cdate": 1573830376122, "ddate": null, "tcdate": 1573830376122, "tmdate": 1573830376122, "tddate": null, "forum": "BJlqYlrtPB", "replyto": "BJlqYlrtPB", "invitation": "ICLR.cc/2020/Conference/Paper2445/-/Official_Comment", "content": {"title": "Reviewers, any comments on the author response?", "comment": "Dear Reviewers, thanks for your thoughtful input on this submission!  The authors have now responded to your comments.  Please be sure to go through their replies and revisions.  If you have additional feedback or questions, it would be great to know.  The authors still have one more day to respond/revise further.  Thanks!\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2445/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2445/Area_Chair1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["csadrian@renyi.hu", "bbeatrix1010@gmail.com", "daniel@renyi.hu"], "title": "Negative Sampling in Variational Autoencoders", "authors": ["Adri\u00e1n Csisz\u00e1rik", "Beatrix Benk\u0151", "D\u00e1niel Varga"], "pdf": "/pdf/1923aaa1393f2fec05882c3cf221cf1950bf36fe.pdf", "TL;DR": "Pulling near-manifold examples (utilizing an auxiliary dataset or generated samples) to a secondary prior improves the discriminative power of VAE models regarding out-of-distribution samples.", "abstract": "We propose negative sampling as an approach to improve the notoriously bad out-of-distribution likelihood estimates of Variational Autoencoder models. Our model pushes latent images of negative samples away from the prior. When the source of negative samples is an auxiliary dataset, such a model can vastly improve on baselines when evaluated on OOD detection tasks. Perhaps more surprisingly, we present a fully unsupervised variant that can also significantly improve detection performance: using the output of the generator as a source of negative samples results in a fully unsupervised model that can be interpreted as adversarially trained.\n", "keywords": ["Variational Autoencoder", "generative modelling", "out-of-distribution detection"], "paperhash": "csisz\u00e1rik|negative_sampling_in_variational_autoencoders", "original_pdf": "/attachment/01efe373073cdf6154c409d9fe028a6c4d482fa1.pdf", "_bibtex": "@misc{\ncsisz{\\'a}rik2020negative,\ntitle={Negative Sampling in Variational Autoencoders},\nauthor={Adri{\\'a}n Csisz{\\'a}rik and Beatrix Benk{\\H{o}} and D{\\'a}niel Varga},\nyear={2020},\nurl={https://openreview.net/forum?id=BJlqYlrtPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJlqYlrtPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2445/Authors", "ICLR.cc/2020/Conference/Paper2445/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2445/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2445/Reviewers", "ICLR.cc/2020/Conference/Paper2445/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2445/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2445/Authors|ICLR.cc/2020/Conference/Paper2445/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504141250, "tmdate": 1576860543789, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2445/Authors", "ICLR.cc/2020/Conference/Paper2445/Reviewers", "ICLR.cc/2020/Conference/Paper2445/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2445/-/Official_Comment"}}}, {"id": "SJghYWEhsH", "original": null, "number": 4, "cdate": 1573826947593, "ddate": null, "tcdate": 1573826947593, "tmdate": 1573826998706, "tddate": null, "forum": "BJlqYlrtPB", "replyto": "BJlqYlrtPB", "invitation": "ICLR.cc/2020/Conference/Paper2445/-/Official_Comment", "content": {"title": "Paper update", "comment": "We thank the reviewers for their many helpful comments. Incorporating them improved the paper tremendously, and we apologize in advance for pushing the limits of how much a paper can change during the rebuttal period.\n\nWe have uploaded a new version of the paper with significant improvements:\n- we further strengthen our experimental results: our new measurements show that our models improve on the baselines in a very consistent manner,\n- we have restructured the text for a clearer exposition and presentation,\n- we have removed the erroneous claim from Section 3. We thank AnonReviewer2 for pointing it out.\n\nBased on our detailed answers and the results of the new version, we kindly ask the reviewers to reassess their evaluation."}, "signatures": ["ICLR.cc/2020/Conference/Paper2445/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2445/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["csadrian@renyi.hu", "bbeatrix1010@gmail.com", "daniel@renyi.hu"], "title": "Negative Sampling in Variational Autoencoders", "authors": ["Adri\u00e1n Csisz\u00e1rik", "Beatrix Benk\u0151", "D\u00e1niel Varga"], "pdf": "/pdf/1923aaa1393f2fec05882c3cf221cf1950bf36fe.pdf", "TL;DR": "Pulling near-manifold examples (utilizing an auxiliary dataset or generated samples) to a secondary prior improves the discriminative power of VAE models regarding out-of-distribution samples.", "abstract": "We propose negative sampling as an approach to improve the notoriously bad out-of-distribution likelihood estimates of Variational Autoencoder models. Our model pushes latent images of negative samples away from the prior. When the source of negative samples is an auxiliary dataset, such a model can vastly improve on baselines when evaluated on OOD detection tasks. Perhaps more surprisingly, we present a fully unsupervised variant that can also significantly improve detection performance: using the output of the generator as a source of negative samples results in a fully unsupervised model that can be interpreted as adversarially trained.\n", "keywords": ["Variational Autoencoder", "generative modelling", "out-of-distribution detection"], "paperhash": "csisz\u00e1rik|negative_sampling_in_variational_autoencoders", "original_pdf": "/attachment/01efe373073cdf6154c409d9fe028a6c4d482fa1.pdf", "_bibtex": "@misc{\ncsisz{\\'a}rik2020negative,\ntitle={Negative Sampling in Variational Autoencoders},\nauthor={Adri{\\'a}n Csisz{\\'a}rik and Beatrix Benk{\\H{o}} and D{\\'a}niel Varga},\nyear={2020},\nurl={https://openreview.net/forum?id=BJlqYlrtPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJlqYlrtPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2445/Authors", "ICLR.cc/2020/Conference/Paper2445/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2445/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2445/Reviewers", "ICLR.cc/2020/Conference/Paper2445/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2445/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2445/Authors|ICLR.cc/2020/Conference/Paper2445/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504141250, "tmdate": 1576860543789, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2445/Authors", "ICLR.cc/2020/Conference/Paper2445/Reviewers", "ICLR.cc/2020/Conference/Paper2445/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2445/-/Official_Comment"}}}, {"id": "Hke5Kl43oS", "original": null, "number": 3, "cdate": 1573826690327, "ddate": null, "tcdate": 1573826690327, "tmdate": 1573826789352, "tddate": null, "forum": "BJlqYlrtPB", "replyto": "H1gQtiyvtH", "invitation": "ICLR.cc/2020/Conference/Paper2445/-/Official_Comment", "content": {"title": "Response to Review #3", "comment": "We thank the reviewer for the important feedback. Based on it, we have made significant improvements on the paper. We have made clarifications and restructured the text for a better exposition and clearer message. Also, now we present even stronger experimental results and a more detailed investigation in several aspects.\n\n1) Regarding novelty: it was not our aspiration to design an intricate model. Rather, we would like to give a simple and general approach to alleviate the bad OOD likelihood phenomenon in VAEs. We believe that our work is a valuable contribution to an ongoing discussion in the research community about out-of-distribution detection in likelihood-based models (see e.g. the works in the related work section or e.g. concurrent work submitted to this conference: https://openreview.net/forum?id=Skg7VAEKDS ).\n\n- To the best of our knowledge, we are the first to construct a training method that alleviates the bad OOD likelihoods phenomenon for VAE models. We do not just investigate and conduct detailed experiments with the Outlier Exposure technique in the VAE setting, but also present a completely new fully unsupervised approach.\n\n- We have added a short section in the paper that discusses the choice of $\\bar{p}$.\n\n- Regarding more sophisticated models: please note that Nalisnick et al 2019 (https://openreview.net/pdf?id=H1xwNhCcYm ) report identical problems by other maximum likelihood models with very strong modeling capacity, such as flow-based models and PixelCNNs. Our expectation is that basically any generative maximum likelihood model is affected by these issues. It is a question of future research how best to adapt our approach to other likelihood-based models.\n\n2) We have added a section in the paper that discusses this question, titled \"why using generated data as negative samples could help?\". To summarize:\n\n- Regarding the argument regarding a fully trained model, in practice, true data samples and generated samples can be distinguished even for fully trained models. But even assuming a perfect generator at convergence, during the training process the generated samples might still help to guide the model toward an equilibrium that promotes a lower likelihood for OOD samples.\n\n- Regarding data augmentation as a source of negative samples: If the augmentation actually keeps the samples within the true data manifold, then distinguishing between true and augmented data is something that we might not want to promote. The encoder would probably learn specific minor visual clues (e.g. bilinear filtering artifacts for rotations) that do not usually help assigning lower likelihood to OOD samples.\n\n- Regarding the performance of our models on color images: in the updated version of the paper we have tuned our color image models by using spectral normalization layers in the encoder. This change significantly improved the AUC values (e.g., from 0.53 to 0.85) for the models using generated negative samples, but did not improve the AUC values of the baseline models.\n\n3-1) We employed the first option, sampling from the positive prior. We have made clarifications in the text.\n\n3-2) Unfortunately, increasing the latent dimension does not help alleviating the bad OOD likelihood phenomenon. We have included an experiment in Appendix C that demonstrates this. We thank the reviewer for suggesting this investigation.\n\n3-3) We have experimented with both a fixed value and a learned global value. Both cases resulted in a similar behaviour.\n\n4) We tried to keep the experimental setup clean for the purposes of analysis, but indeed, in an engineering context we would definitely use a set of negative samples as diverse as possible.\n\nWe are grateful for the valuable feedback, it greatly helped us to improve our paper."}, "signatures": ["ICLR.cc/2020/Conference/Paper2445/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2445/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["csadrian@renyi.hu", "bbeatrix1010@gmail.com", "daniel@renyi.hu"], "title": "Negative Sampling in Variational Autoencoders", "authors": ["Adri\u00e1n Csisz\u00e1rik", "Beatrix Benk\u0151", "D\u00e1niel Varga"], "pdf": "/pdf/1923aaa1393f2fec05882c3cf221cf1950bf36fe.pdf", "TL;DR": "Pulling near-manifold examples (utilizing an auxiliary dataset or generated samples) to a secondary prior improves the discriminative power of VAE models regarding out-of-distribution samples.", "abstract": "We propose negative sampling as an approach to improve the notoriously bad out-of-distribution likelihood estimates of Variational Autoencoder models. Our model pushes latent images of negative samples away from the prior. When the source of negative samples is an auxiliary dataset, such a model can vastly improve on baselines when evaluated on OOD detection tasks. Perhaps more surprisingly, we present a fully unsupervised variant that can also significantly improve detection performance: using the output of the generator as a source of negative samples results in a fully unsupervised model that can be interpreted as adversarially trained.\n", "keywords": ["Variational Autoencoder", "generative modelling", "out-of-distribution detection"], "paperhash": "csisz\u00e1rik|negative_sampling_in_variational_autoencoders", "original_pdf": "/attachment/01efe373073cdf6154c409d9fe028a6c4d482fa1.pdf", "_bibtex": "@misc{\ncsisz{\\'a}rik2020negative,\ntitle={Negative Sampling in Variational Autoencoders},\nauthor={Adri{\\'a}n Csisz{\\'a}rik and Beatrix Benk{\\H{o}} and D{\\'a}niel Varga},\nyear={2020},\nurl={https://openreview.net/forum?id=BJlqYlrtPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJlqYlrtPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2445/Authors", "ICLR.cc/2020/Conference/Paper2445/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2445/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2445/Reviewers", "ICLR.cc/2020/Conference/Paper2445/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2445/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2445/Authors|ICLR.cc/2020/Conference/Paper2445/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504141250, "tmdate": 1576860543789, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2445/Authors", "ICLR.cc/2020/Conference/Paper2445/Reviewers", "ICLR.cc/2020/Conference/Paper2445/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2445/-/Official_Comment"}}}, {"id": "HJg7vJE2ir", "original": null, "number": 2, "cdate": 1573826395338, "ddate": null, "tcdate": 1573826395338, "tmdate": 1573826417694, "tddate": null, "forum": "BJlqYlrtPB", "replyto": "BkgalbL3FH", "invitation": "ICLR.cc/2020/Conference/Paper2445/-/Official_Comment", "content": {"title": "Response to Review #1", "comment": "We thank the reviewer for the valuable feedback, it helped a lot to improve our paper. We have made clarifications in many places and restructured the text for a better exposition and a clearer message.\n\nIn our humble opinion, our results are easier to appreciate in the fuller context of the growing amount of work related to out-of-distribution detection in likelihood-based models (the works in the related work section or e.g. concurrent work submitted to this conference: https://openreview.net/forum?id=Skg7VAEKDS ). Our contributions reflect on recent work, and provide several novelties:\n- To best of our knowledge, we are the first to give a training method for VAEs that helps alleviating the bad OOD likelihood performance.\n- We present an unsupervised approach that is completely novel, and report detailed experiments that confirm the robustness and usefulness of the method (see Table 1 and Table 2 in the updated paper).\n- Our work highlights a potential problem with utilizing Outlier Exposure (the very general framework laid down by Hendrycks at al. 2018). The results with auxiliary datasets in Table 1 show that while auxiliary samples help greatly in most cases, OOD detection performance can be very sensitive to the choice of the auxiliary dataset, see for example the last block of Table 1, Letters-Fashion-Numbers, where Outlier Exposure fails to improve while our adversarial method still achieves good performance.\n\n- We have expanded the description and the discussion of the experiments which now also continues in the appendix.\n\n- We have updated the explanations and motivations in several places where the review identified issues.\n\n- We have added an experiment in the appendix which explores the utilization of the reconstruction term for the negatives during training. Our experiments show that this does not improve the model in terms of OOD performance.\n\n- Regarding Bernoulli: we agree with the reviewer that the Bernoulli is a theoretically less sound modeling choice than, for example, the Gaussian. (We added a footnote to the paper with this remark.) That's one of the reasons we publish numbers with a Gaussian noise model as well. However, much of the literature directly relevant to us made the exact same modeling choice: working with the Bernoulli, and interpreting the grayscale values as probabilities of binary events. Loaiza-Ganem and Cunningham 2019 https://arxiv.org/abs/1907.06845 lists several papers following this practice. We side with the reviewer in this disagreement. However, as we said above, the Bernoulli is an unavoidable option if we wish to compare our results to the rest of this sub-field.\n\n- We thank the reviewer for the pointer for the Hu et al. paper. As we see, the main contribution of our paper is not in proposing a hybrid VAE-GAN model, but in tackling the issue of bad OOD likelihoods both in a supervised and unsupervised context. We clarified in the text how the adversarial model is trained.\n\n- We have added sample images for all datasets to Appendix D.\n\nWe are very thankful for the review, it helped us a lot to improve the paper."}, "signatures": ["ICLR.cc/2020/Conference/Paper2445/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2445/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["csadrian@renyi.hu", "bbeatrix1010@gmail.com", "daniel@renyi.hu"], "title": "Negative Sampling in Variational Autoencoders", "authors": ["Adri\u00e1n Csisz\u00e1rik", "Beatrix Benk\u0151", "D\u00e1niel Varga"], "pdf": "/pdf/1923aaa1393f2fec05882c3cf221cf1950bf36fe.pdf", "TL;DR": "Pulling near-manifold examples (utilizing an auxiliary dataset or generated samples) to a secondary prior improves the discriminative power of VAE models regarding out-of-distribution samples.", "abstract": "We propose negative sampling as an approach to improve the notoriously bad out-of-distribution likelihood estimates of Variational Autoencoder models. Our model pushes latent images of negative samples away from the prior. When the source of negative samples is an auxiliary dataset, such a model can vastly improve on baselines when evaluated on OOD detection tasks. Perhaps more surprisingly, we present a fully unsupervised variant that can also significantly improve detection performance: using the output of the generator as a source of negative samples results in a fully unsupervised model that can be interpreted as adversarially trained.\n", "keywords": ["Variational Autoencoder", "generative modelling", "out-of-distribution detection"], "paperhash": "csisz\u00e1rik|negative_sampling_in_variational_autoencoders", "original_pdf": "/attachment/01efe373073cdf6154c409d9fe028a6c4d482fa1.pdf", "_bibtex": "@misc{\ncsisz{\\'a}rik2020negative,\ntitle={Negative Sampling in Variational Autoencoders},\nauthor={Adri{\\'a}n Csisz{\\'a}rik and Beatrix Benk{\\H{o}} and D{\\'a}niel Varga},\nyear={2020},\nurl={https://openreview.net/forum?id=BJlqYlrtPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJlqYlrtPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2445/Authors", "ICLR.cc/2020/Conference/Paper2445/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2445/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2445/Reviewers", "ICLR.cc/2020/Conference/Paper2445/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2445/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2445/Authors|ICLR.cc/2020/Conference/Paper2445/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504141250, "tmdate": 1576860543789, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2445/Authors", "ICLR.cc/2020/Conference/Paper2445/Reviewers", "ICLR.cc/2020/Conference/Paper2445/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2445/-/Official_Comment"}}}, {"id": "rkg5xRX2jH", "original": null, "number": 1, "cdate": 1573826033764, "ddate": null, "tcdate": 1573826033764, "tmdate": 1573826033764, "tddate": null, "forum": "BJlqYlrtPB", "replyto": "S1leviI0YB", "invitation": "ICLR.cc/2020/Conference/Paper2445/-/Official_Comment", "content": {"title": "Response to Review #2", "comment": "We are grateful to the reviewer for the insightful comments. We have rewritten large parts of the text with the goal of making the descriptions of our models and our core claims more clear. Just as importantly, we have an updated experiments section with even stronger results. (See e.g. Table 1 or Table 2 in the updated paper, which highlight the effectiveness of our proposed method.)\n\n- Regarding the KL term: indeed, the latter interpretation is the intended one. $\\bar{x}^{(i)}$ is specified to be a negative sample, and the extra term only references $\\bar{x}^{(i)}$, not $x^{(i)}$. We have made clarifications in the text.\n\n- We are deeply thankful to the reviewer for pointing out that we made a mistake in writing up a variational model justifying our loss function. To our defense, this toy model was not central to our argument. We have removed it from the paper.\n\n- Regarding the positive KL term as a measure: examining the VAE likelihood estimates raises the question of how the two components of the ELBO (the reconstruction part and the KL part) contribute to the likelihood estimate and the discriminative power of the model. As the magnitude and the behavior of the reconstruction term are highly determined by the choice of the noise model, it is natural to investigate to what extent the separation between inliers and outliers is carried out in latent space. Note that we publish likelihood-based evaluations everywhere, one can consider the KL-based evaluations as extra information."}, "signatures": ["ICLR.cc/2020/Conference/Paper2445/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2445/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["csadrian@renyi.hu", "bbeatrix1010@gmail.com", "daniel@renyi.hu"], "title": "Negative Sampling in Variational Autoencoders", "authors": ["Adri\u00e1n Csisz\u00e1rik", "Beatrix Benk\u0151", "D\u00e1niel Varga"], "pdf": "/pdf/1923aaa1393f2fec05882c3cf221cf1950bf36fe.pdf", "TL;DR": "Pulling near-manifold examples (utilizing an auxiliary dataset or generated samples) to a secondary prior improves the discriminative power of VAE models regarding out-of-distribution samples.", "abstract": "We propose negative sampling as an approach to improve the notoriously bad out-of-distribution likelihood estimates of Variational Autoencoder models. Our model pushes latent images of negative samples away from the prior. When the source of negative samples is an auxiliary dataset, such a model can vastly improve on baselines when evaluated on OOD detection tasks. Perhaps more surprisingly, we present a fully unsupervised variant that can also significantly improve detection performance: using the output of the generator as a source of negative samples results in a fully unsupervised model that can be interpreted as adversarially trained.\n", "keywords": ["Variational Autoencoder", "generative modelling", "out-of-distribution detection"], "paperhash": "csisz\u00e1rik|negative_sampling_in_variational_autoencoders", "original_pdf": "/attachment/01efe373073cdf6154c409d9fe028a6c4d482fa1.pdf", "_bibtex": "@misc{\ncsisz{\\'a}rik2020negative,\ntitle={Negative Sampling in Variational Autoencoders},\nauthor={Adri{\\'a}n Csisz{\\'a}rik and Beatrix Benk{\\H{o}} and D{\\'a}niel Varga},\nyear={2020},\nurl={https://openreview.net/forum?id=BJlqYlrtPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJlqYlrtPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2445/Authors", "ICLR.cc/2020/Conference/Paper2445/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2445/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2445/Reviewers", "ICLR.cc/2020/Conference/Paper2445/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2445/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2445/Authors|ICLR.cc/2020/Conference/Paper2445/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504141250, "tmdate": 1576860543789, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2445/Authors", "ICLR.cc/2020/Conference/Paper2445/Reviewers", "ICLR.cc/2020/Conference/Paper2445/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2445/-/Official_Comment"}}}, {"id": "H1gQtiyvtH", "original": null, "number": 1, "cdate": 1571384186653, "ddate": null, "tcdate": 1571384186653, "tmdate": 1572972337274, "tddate": null, "forum": "BJlqYlrtPB", "replyto": "BJlqYlrtPB", "invitation": "ICLR.cc/2020/Conference/Paper2445/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper discusses the detection of out-of-distribution (OOD) samples for variational autoencoders (VAE).\nThe idea is to train the encoder such that its output variational distribution q(z|\\bar{x}) is pushed away from the prior of latent z. \nI think the paper needs more clarification and investigation for being published in the conference. \nMy major concern is that more empirical investigation is necessary since the formulation provides a minor novelty. \nSpecific points are given below. \n\n1) Weak novelty in terms of model design. \nThe objective function consists of the standard (negative) ELBO term and additional KL term to modify the variational posterior of negative samples. \nThis modification can be regarded as a form of outlier exposure (Hendrycks et al. 2018) specialized for VAE. \nThe choice of \\bar{p} is not much investigated. \nAny discussion if we use a more sophisticated model such as VampPrior* for stronger modeling capacity. \n* J. Tomczak and M. Welling, VAE with a VampPrior, AISTATS 2018.\n\n2) The use generated samples as negative samples is interesting but mysterious. \nThe authors conjecture that this works because the generated samples come from near the data manifold, but in-distribution samples and negative samples can be indistinguishable when the generative model is very well trained. \nWhat happens if, for example, the negative samples are generated by data augmentation techniques (such as cropping, rotation, mirroring, though mirroring and much rotation may be unsuitable for text images)? \nThis can also produce near-manifold points. \nA deeper analysis why generated samples can improve the OOD detection performance is necessary. \nFurthermore, why does not this approach impact much for color images in Table 4?\n\n3) More details of experimental procedures. \n3-1) How was data points are generated from VAE as negative samples? \nPossible ways are:\n* sample z ~ p(z), then draw from the decoder x ~ p(x|z).\n* use negative prior z ~ \\bar{p}(z), then draw from the decoder x ~ p(x|z).\n* this seems weird: use variational posterior z ~ q(z|x), then x ~ p(x|z).\n\n3-2) Latent dimension of 10 for grayscale images seems small. \nDoes the size affect the OOD detection performance when the size is 50 or 100 to make the model richer. \n\n3-3) How was the variance obtained when the decoder uses the Gaussian likelihood?\n* fixed value?\n* learned for each pixel?\n* output from the decoder?\n\n4) If we have access to diverse negative datasets, can the ODD detection perform better? \nMixing multiple datasets or using both available dataset and generated samples can improve the performance while the test OOD samples are kept unseen. \nFor example, train VAE on MNIST while using KMNIST and EMNIST as the negative sets to detect Fashion-MNIST as ODD. "}, "signatures": ["ICLR.cc/2020/Conference/Paper2445/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2445/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["csadrian@renyi.hu", "bbeatrix1010@gmail.com", "daniel@renyi.hu"], "title": "Negative Sampling in Variational Autoencoders", "authors": ["Adri\u00e1n Csisz\u00e1rik", "Beatrix Benk\u0151", "D\u00e1niel Varga"], "pdf": "/pdf/1923aaa1393f2fec05882c3cf221cf1950bf36fe.pdf", "TL;DR": "Pulling near-manifold examples (utilizing an auxiliary dataset or generated samples) to a secondary prior improves the discriminative power of VAE models regarding out-of-distribution samples.", "abstract": "We propose negative sampling as an approach to improve the notoriously bad out-of-distribution likelihood estimates of Variational Autoencoder models. Our model pushes latent images of negative samples away from the prior. When the source of negative samples is an auxiliary dataset, such a model can vastly improve on baselines when evaluated on OOD detection tasks. Perhaps more surprisingly, we present a fully unsupervised variant that can also significantly improve detection performance: using the output of the generator as a source of negative samples results in a fully unsupervised model that can be interpreted as adversarially trained.\n", "keywords": ["Variational Autoencoder", "generative modelling", "out-of-distribution detection"], "paperhash": "csisz\u00e1rik|negative_sampling_in_variational_autoencoders", "original_pdf": "/attachment/01efe373073cdf6154c409d9fe028a6c4d482fa1.pdf", "_bibtex": "@misc{\ncsisz{\\'a}rik2020negative,\ntitle={Negative Sampling in Variational Autoencoders},\nauthor={Adri{\\'a}n Csisz{\\'a}rik and Beatrix Benk{\\H{o}} and D{\\'a}niel Varga},\nyear={2020},\nurl={https://openreview.net/forum?id=BJlqYlrtPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BJlqYlrtPB", "replyto": "BJlqYlrtPB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2445/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2445/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575652510757, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2445/Reviewers"], "noninvitees": [], "tcdate": 1570237722709, "tmdate": 1575652510774, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2445/-/Official_Review"}}}], "count": 10}