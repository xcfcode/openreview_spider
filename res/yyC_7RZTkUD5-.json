{"notes": [{"ddate": null, "legacy_migration": true, "tmdate": 1363393320000, "tcdate": 1363393320000, "number": 1, "id": "XTZrXGh8rENYB", "invitation": "ICLR.cc/2013/-/submission/reply", "forum": "yyC_7RZTkUD5-", "replyto": "3vEUvBbCrO8cu", "signatures": ["Rakesh Chalasani"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "This is in reply to reviewer 1829, mistakenly pasted here. Please ignore."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Predictive Coding Networks", "decision": "conferencePoster-iclr2013-workshop", "abstract": "The quality of data representation in deep learning methods is directly related to the prior model imposed on the representations; however, generally used fixed priors are not capable of adjusting to the context in the data. To address this issue, we propose deep predictive coding networks, a hierarchical generative model that empirically alters priors on the latent representations in a dynamic and context-sensitive manner. This model captures the temporal dependencies in time-varying signals and uses top-down information to modulate the representation in lower layers. The centerpiece of our model is a novel procedure to infer sparse states of a dynamic model; which is used for feature extraction. We also extend this feature extraction block to introduce a pooling function that captures locally invariant representations. When applied on a natural video data, we show that our method is able to learn high-level visual features. We also demonstrate the role of the top-down connections by showing the robustness of the proposed model to structured noise.", "pdf": "https://arxiv.org/abs/1301.3541", "paperhash": "chalasani|deep_predictive_coding_networks", "authors": ["Rakesh Chalasani", "Jose C. Principe"], "authorids": ["vnit.rakesh@gmail.com", "principe@cnel.ufl.edu"], "keywords": [], "conflicts": []}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1363393200000, "tcdate": 1363393200000, "number": 2, "id": "Xu4KaWxqIDurf", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "yyC_7RZTkUD5-", "replyto": "yyC_7RZTkUD5-", "signatures": ["Rakesh Chalasani, Jose C. Principe"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "The revised paper is uploaded onto arXiv. It will be announced on 18th March.\r\n\r\nIn the mean time, the paper is also made available at \r\nhttps://www.dropbox.com/s/klmpu482q6nt1ws/DPCN.pdf"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Predictive Coding Networks", "decision": "conferencePoster-iclr2013-workshop", "abstract": "The quality of data representation in deep learning methods is directly related to the prior model imposed on the representations; however, generally used fixed priors are not capable of adjusting to the context in the data. To address this issue, we propose deep predictive coding networks, a hierarchical generative model that empirically alters priors on the latent representations in a dynamic and context-sensitive manner. This model captures the temporal dependencies in time-varying signals and uses top-down information to modulate the representation in lower layers. The centerpiece of our model is a novel procedure to infer sparse states of a dynamic model; which is used for feature extraction. We also extend this feature extraction block to introduce a pooling function that captures locally invariant representations. When applied on a natural video data, we show that our method is able to learn high-level visual features. We also demonstrate the role of the top-down connections by showing the robustness of the proposed model to structured noise.", "pdf": "https://arxiv.org/abs/1301.3541", "paperhash": "chalasani|deep_predictive_coding_networks", "authors": ["Rakesh Chalasani", "Jose C. Principe"], "authorids": ["vnit.rakesh@gmail.com", "principe@cnel.ufl.edu"], "keywords": [], "conflicts": []}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1363393020000, "tcdate": 1363393020000, "number": 1, "id": "o1YP1AMjPx1jv", "invitation": "ICLR.cc/2013/-/submission/reply", "forum": "yyC_7RZTkUD5-", "replyto": "Za8LX-xwgqXw5", "signatures": ["Rakesh Chalasani, Jose C. Principe"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "Thank you for review and comments. We revised the paper to address most of your concerns. Following is our response to some specific point you have raised. \r\n\r\n>>> ' The clarity of the paper needs to be improved. For example, it will be helpful to motivate more clearly about the specific formulation of the model' \r\n\r\nWe made some major changes to improve the presentation of the model, with more emphasis on explaining the formulation. Hopefully the revised version will improve the clarity of the paper. \r\n\r\n>>> ' The empirical evaluation of the model could be strengthened by directly comparing the DPCN to related works on non-synthetic datasets.'\r\n\r\nWe agree that the empirical evaluation could be strengthened by comparing DPCN with other models in tasks like denoising, classification etc., on large image and video datasets. However, to scale this model to larger inputs we require convolutional network like models, similar to many other methods. This is an on going work and we are presently working on a convolutional model for DPCN. \r\n\r\n>>>'In the beginning of the section 2.1, please define P, D, K to improve clarity. \r\n>>> In section 2.2, little explanation about the pooling matrix B is given. Also, more explanations about equation 4 would be desirable. \r\n>>> What is z_{t} in Equation 11?' \r\n\r\nCorrected. These are explained more clearly in the revised paper. z_{t} is the Gaussian transition noise over the parameters. \r\n\r\n>>> 'In Section 2.2, its not clear how u_{hat} is computed. ' \r\n\r\nThis is moved into section. 2.4 in the revised paper, where more explanation is provided about u_{hat}."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Predictive Coding Networks", "decision": "conferencePoster-iclr2013-workshop", "abstract": "The quality of data representation in deep learning methods is directly related to the prior model imposed on the representations; however, generally used fixed priors are not capable of adjusting to the context in the data. To address this issue, we propose deep predictive coding networks, a hierarchical generative model that empirically alters priors on the latent representations in a dynamic and context-sensitive manner. This model captures the temporal dependencies in time-varying signals and uses top-down information to modulate the representation in lower layers. The centerpiece of our model is a novel procedure to infer sparse states of a dynamic model; which is used for feature extraction. We also extend this feature extraction block to introduce a pooling function that captures locally invariant representations. When applied on a natural video data, we show that our method is able to learn high-level visual features. We also demonstrate the role of the top-down connections by showing the robustness of the proposed model to structured noise.", "pdf": "https://arxiv.org/abs/1301.3541", "paperhash": "chalasani|deep_predictive_coding_networks", "authors": ["Rakesh Chalasani", "Jose C. Principe"], "authorids": ["vnit.rakesh@gmail.com", "principe@cnel.ufl.edu"], "keywords": [], "conflicts": []}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1363392960000, "tcdate": 1363392960000, "number": 1, "id": "3vEUvBbCrO8cu", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "yyC_7RZTkUD5-", "replyto": "yyC_7RZTkUD5-", "signatures": ["Rakesh Chalasani, Jose C. Principe"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "Thank you for review and comments. We revised the paper to address most of your concerns. Following is our response to some specific point you have raised.\r\n\r\n>>> ' The clarity of the paper needs to be improved. For example, it will be helpful to motivate more clearly about the specific formulation of the model'\r\n\r\nWe made some major changes to improve the presentation of the model, with more emphasis on explaining the formulation. Hopefully the revised version will improve the clarity of the paper.\r\n\r\n>>> ' The empirical evaluation of the model could be strengthened by directly comparing the DPCN to related works on non-synthetic datasets.'\r\n\r\nWe agree that the empirical evaluation could be strengthened by comparing DPCN with other models in tasks like denoising, classification etc., on large image and video datasets. However, to scale this model to larger inputs we require convolutional network like models, similar to many other methods. This is an on going work and we are presently working on a convolutional model for DPCN. \r\n\r\n>>>'In the beginning of the section 2.1, please define P, D, K to improve clarity. \r\n>>> In section 2.2, little explanation about the pooling matrix B is given. Also, more explanations about equation 4 would be desirable.\r\n>>> What is z_{t} in Equation 11?'\r\n\r\nCorrected. These are explained more clearly in the revised paper. z_{t} is the Gaussian transition noise over the parameters. \r\n\r\n>>> 'In Section 2.2, its not clear how u_{hat} is computed. '\r\n\r\nThis is moved into section. 2.4 in the revised paper, where more explanation is provided about u_{hat}."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Predictive Coding Networks", "decision": "conferencePoster-iclr2013-workshop", "abstract": "The quality of data representation in deep learning methods is directly related to the prior model imposed on the representations; however, generally used fixed priors are not capable of adjusting to the context in the data. To address this issue, we propose deep predictive coding networks, a hierarchical generative model that empirically alters priors on the latent representations in a dynamic and context-sensitive manner. This model captures the temporal dependencies in time-varying signals and uses top-down information to modulate the representation in lower layers. The centerpiece of our model is a novel procedure to infer sparse states of a dynamic model; which is used for feature extraction. We also extend this feature extraction block to introduce a pooling function that captures locally invariant representations. When applied on a natural video data, we show that our method is able to learn high-level visual features. We also demonstrate the role of the top-down connections by showing the robustness of the proposed model to structured noise.", "pdf": "https://arxiv.org/abs/1301.3541", "paperhash": "chalasani|deep_predictive_coding_networks", "authors": ["Rakesh Chalasani", "Jose C. Principe"], "authorids": ["vnit.rakesh@gmail.com", "principe@cnel.ufl.edu"], "keywords": [], "conflicts": []}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1363392660000, "tcdate": 1363392660000, "number": 1, "id": "00ZvUXp_e10_E", "invitation": "ICLR.cc/2013/-/submission/reply", "forum": "yyC_7RZTkUD5-", "replyto": "EEhwkCLtAuko7", "signatures": ["Rakesh Chalasani, Jose C. Principe"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "Thank you for you review and comments, particularly for pointing out some mistakes in the paper. Following is our response to some concerns you have raised. \r\n\r\n>>> 'You should state the functional form for F and G!! Working backwards from the energy function, it looks as if these are just linear functions?'\r\n\r\nWe use the generalized state-space equations in Eq.1 and Eq.2 to motivate the relation between the proposed model and dynamic networks. However, please note that it is difficult to state the explicit form of F and G, since sparsity constraint even on a linear dynamical system leads to a non-linear mapping between the observations and the states.\r\n\r\n>>> 'In Eq. 1 should F( x_t, u_t ) instead just be F( x_t )? Eqs. 3 and 4 suggest it should just be F( x_t ), and this would resolve points which I found confusing later in the paper.'\r\n\r\nAgreed. We made appropriate changes in the revised paper.\r\n\r\n>>> The relationship between the energy functions in eqs. 3 and 4 is confusing to me. (this may have to do with the (non?)-dependence of F on u_t)\r\n\r\nWe made this explicit in the revised paper. Eq.3 represents the energy function for inferring the x_t with fixed u_t and Eq.4 represents the energy function for inferring the u_t with fixed x_t. In order to be more clear, we now wrote a unified energy function (Eq. 5) from which we jointly infer both x_t and u_t. \r\n\r\n>>> 'Section 2.3.1, 'It is easy to show that this is equivalent to finding the mode of the distribution...': You probably mean MAP not mode. Additionally this is non-obvious. It seems like this would especially not be true after marginalizing out u_t. You've never written the joint distributions over p(x_t, y_t, x_t-1), and the role of the different energy functions was unclear.'\r\n\r\nAgreed, this statement is incorrect and is removed.\r\n\r\n>>> 'Section 3.1: In a linear mapping, how are 4 overlapping patches different from a single larger patch?'\r\n\r\nPlease note that the states from the 4 overlapping patches are pooled using a non-linear function (sum of the absolute value of the state vectors). Hence, the output is no longer a linear mapping.\r\n\r\n>>> 'Section 3.2: Do you do anything about the discontinuities which would occur between the 100-frame sequences?'\r\n\r\nNo, we simply consider the concatenated sequence as a single video. This is made more clear in the paper."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Predictive Coding Networks", "decision": "conferencePoster-iclr2013-workshop", "abstract": "The quality of data representation in deep learning methods is directly related to the prior model imposed on the representations; however, generally used fixed priors are not capable of adjusting to the context in the data. To address this issue, we propose deep predictive coding networks, a hierarchical generative model that empirically alters priors on the latent representations in a dynamic and context-sensitive manner. This model captures the temporal dependencies in time-varying signals and uses top-down information to modulate the representation in lower layers. The centerpiece of our model is a novel procedure to infer sparse states of a dynamic model; which is used for feature extraction. We also extend this feature extraction block to introduce a pooling function that captures locally invariant representations. When applied on a natural video data, we show that our method is able to learn high-level visual features. We also demonstrate the role of the top-down connections by showing the robustness of the proposed model to structured noise.", "pdf": "https://arxiv.org/abs/1301.3541", "paperhash": "chalasani|deep_predictive_coding_networks", "authors": ["Rakesh Chalasani", "Jose C. Principe"], "authorids": ["vnit.rakesh@gmail.com", "principe@cnel.ufl.edu"], "keywords": [], "conflicts": []}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1363392180000, "tcdate": 1363392180000, "number": 1, "id": "iiUe8HAsepist", "invitation": "ICLR.cc/2013/-/submission/reply", "forum": "yyC_7RZTkUD5-", "replyto": "d6u7vbCNJV6Q8", "signatures": ["Rakesh Chalasani, Jose C. Principe"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "Thank you for your review and comments. We revised the paper to address most of your concerns. Following is our response to some specific point you have raised.\r\n\r\n>>> 'The explanation of the model was overly complicated. After reading the the entire explanation it appears the model is simply doing sparse coding with ISTA alternating on the states and causes. The gradient for ISTA simply has the gradients for the overall cost function, just as in sparse coding but this cost function has some extra temporal terms.'\r\n\r\nWe have made major changes to the paper to improve the presentation of the model. Hopefully the newer version will make the explanation more clear.\r\nWe would also like to emphasis that  the paper makes two important contributions: (1) as you have pointed out, introduces sparse coding in dynamical models and solves it using a novel inference procedure similar to ISTA. (2) considers top-down information while performing inference in the hierarchical model.\r\n\r\n>>> 'The noise reduction is only on toy images and it is not obvious if this is what you would also get with sparse coding using larger patch sizes and high amounts of sparsity.'\r\n\r\nWe agree with you that it would strengthen our arguments by showing denoising on large images or videos. However, to scale this model to large images require convolutional network like model. This is an on going work and we are presently developing a convolutional model for DPCN.\r\n\r\n>>> 'The explanation of points between clusters coming from change in sequences should also appear in the clean video as well because as the text mentions the video changes as well. This is likely due to multiple objects overlapping instead and confusing the model.'\r\n\r\nCorrected. The points between the clusters appear because we enforce temporal coherence on the causes belonging two consecutive frames  at the top layer (see Section 2.4). It is not due to gradual change in the sequences, as said previously.\r\n\r\n>>> 'Figure 1 should include the variable names because reading the text and consulting the figure is not very helpful currently.'\r\n\r\nCorrected. Also, a new figure is added to bring more clarity.\r\n\r\n>>> 'It is hard to reason what each of the A,B, and C is doing without a picture of what they learn on typical data. The layer 1 features seem fairly complex and noisy for the first layer of an image model which typically learns gabor-like features.'\r\n\r\nPlease see the supplementary material, section A.4 for visualization of the first layer parameters A, B and C. Also, please note that the Figure. 2 shows the visualization of the invariant matrices, B, in a two-layered network. These are obtained by taking the linear combination of Gabor like filters in C^(1) (see Figure .6) and hence, represent more complex structures. This is made more clear in the paper.\r\n\r\n>>> 'Where did z come from in equation 11?'\r\n\r\nCorrected. It is the Gaussian transition noise over the parameters.\r\n\r\n>>> 'It is not at all obvious why the states should be temporally consistent and not the causes. The causes are pooled versions of the states and this should be more invariant to changes at the input between frames.'\r\n\r\nWe say the states are more temporally 'consistent' to indicate that they are more stable than sparse coding, particularly in high sparsity conditions, because they have to maintain the temporal dependencies. On the other hand, we agree with you that the causes are more invariant to changes in the input and hence, are temporally 'coherent'."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Predictive Coding Networks", "decision": "conferencePoster-iclr2013-workshop", "abstract": "The quality of data representation in deep learning methods is directly related to the prior model imposed on the representations; however, generally used fixed priors are not capable of adjusting to the context in the data. To address this issue, we propose deep predictive coding networks, a hierarchical generative model that empirically alters priors on the latent representations in a dynamic and context-sensitive manner. This model captures the temporal dependencies in time-varying signals and uses top-down information to modulate the representation in lower layers. The centerpiece of our model is a novel procedure to infer sparse states of a dynamic model; which is used for feature extraction. We also extend this feature extraction block to introduce a pooling function that captures locally invariant representations. When applied on a natural video data, we show that our method is able to learn high-level visual features. We also demonstrate the role of the top-down connections by showing the robustness of the proposed model to structured noise.", "pdf": "https://arxiv.org/abs/1301.3541", "paperhash": "chalasani|deep_predictive_coding_networks", "authors": ["Rakesh Chalasani", "Jose C. Principe"], "authorids": ["vnit.rakesh@gmail.com", "principe@cnel.ufl.edu"], "keywords": [], "conflicts": []}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362498780000, "tcdate": 1362498780000, "number": 4, "id": "Za8LX-xwgqXw5", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "yyC_7RZTkUD5-", "replyto": "yyC_7RZTkUD5-", "signatures": ["anonymous reviewer 1829"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Deep Predictive Coding Networks", "review": "A brief summary of the paper's contributions, in the context of prior work.\r\nThe paper proposes a hierarchical sparse generative model in the context of a dynamical system. The model can capture temporal dependencies in time-varying data, and top-down information (from high-level contextual/causal units) can modulate the states and observations in lower layers. \r\n\r\nExperiments were conducted on a natural video dataset, and on a synthetic video dataset with moving geometric shapes. On the natural video dataset, the learned receptive fields represent edge detectors in the first layer, and higher-level concepts such as corners and junctions in the second layer. In the synthetic sequence dataset, hierarchical top-down inference is used to robustly infer about \u201ccausal\u201d units associated with object shapes.\r\n\r\n\r\nAn assessment of novelty and quality.\r\nThis work can be viewed as a novel extension of hierarchical sparse coding to temporal data. Specifically, it is interesting to see how to incorporate dynamical systems into sparse hierarchical models (that alternate between state units and causal units), and how the model can perform bottom-up/top-down inference. The use of Nestrov\u2019s method to approximate the non-smooth state transition terms in equation 5 is interesting.\r\n\r\nThe clarity of the paper needs to be improved. For example, it will be helpful to motivate more clearly about the specific formulation of the model (also, see comments below). \r\n\r\nThe experimental results (identifying high-level causes from corrupted temporal data) seem quite reasonable on the synthetic dataset. However, the results are all too qualitative. The empirical evaluation of the model could be strengthened by directly comparing the DPCN to related works on non-synthetic datasets.\r\n\r\n\r\nOther questions and comments:\r\n- In the beginning of the section 2.1, please define P, D, K to improve clarity.\r\n- In section 2.2, little explanation about the pooling matrix B is given. Also, more explanations about equation 4 would be desirable.\r\n- What is z_{t} in Equation 11?\r\n- In Section 2.2, it\u2019s not clear how u_hat is computed. \r\n\r\n\r\nA list of pros and cons (reasons to accept/reject).\r\nPros:\r\n- The formulation and the proposed solution are technically interesting. \r\n- Experimental results on a synthetic video data set provide a proof-of-concept demonstration.\r\n\r\nCons:\r\n- The significance of the experiments is quite limited. There is no empirical comparison to other models on real tasks.\r\n- Inference seems to be complicated and computationally expensive. \r\n- Unclear presentation"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Predictive Coding Networks", "decision": "conferencePoster-iclr2013-workshop", "abstract": "The quality of data representation in deep learning methods is directly related to the prior model imposed on the representations; however, generally used fixed priors are not capable of adjusting to the context in the data. To address this issue, we propose deep predictive coding networks, a hierarchical generative model that empirically alters priors on the latent representations in a dynamic and context-sensitive manner. This model captures the temporal dependencies in time-varying signals and uses top-down information to modulate the representation in lower layers. The centerpiece of our model is a novel procedure to infer sparse states of a dynamic model; which is used for feature extraction. We also extend this feature extraction block to introduce a pooling function that captures locally invariant representations. When applied on a natural video data, we show that our method is able to learn high-level visual features. We also demonstrate the role of the top-down connections by showing the robustness of the proposed model to structured noise.", "pdf": "https://arxiv.org/abs/1301.3541", "paperhash": "chalasani|deep_predictive_coding_networks", "authors": ["Rakesh Chalasani", "Jose C. Principe"], "authorids": ["vnit.rakesh@gmail.com", "principe@cnel.ufl.edu"], "keywords": [], "conflicts": []}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362405300000, "tcdate": 1362405300000, "number": 5, "id": "EEhwkCLtAuko7", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "yyC_7RZTkUD5-", "replyto": "yyC_7RZTkUD5-", "signatures": ["anonymous reviewer 62ac"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Deep Predictive Coding Networks", "review": "This paper attempts to capture both the temporal dynamics of signals and the contribution of top down connections for inference using a deep model.  The experimental results are qualitatively encouraging, and the model structure seems like a sensible direction to pursue.  I like the connection to dynamical systems.  The mathematical presentation is disorganized though, and it would have been nice to see some sort of benchmark or externally meaningful quantitative comparison in the experimental results.\r\n\r\nMore specific comments:\r\n\r\nYou should state the functional form for F and G!!  Working backwards from the energy function, it looks as if these are just linear functions?\r\n\r\nIn Eq. 1 should F( x_t, u_t ) instead just be F( x_t )?  Eqs. 3 and 4 suggest it should just be F( x_t ), and this would resolve points which I found confusing later in the paper.\r\n\r\nThe relationship between the energy functions in eqs. 3 and 4 is confusing to me.  (this may have to do with the (non?)-dependence of F on u_t)\r\n\r\nSection 2.3.1, 'It is easy to show that this is equivalent to finding the mode of the distribution...': You probably mean MAP not mode.  Additionally this is non-obvious.  It seems like this would especially not be true after marginalizing out u_t.  You've never written the joint distributions over p(x_t, y_t, x_t-1), and the role of the different energy functions was unclear.\r\n\r\nSection 3.1: In a linear mapping, how are 4 overlapping patches different from a single larger patch?\r\n\r\nSection 3.2: Do you do anything about the discontinuities which would occur between the 100-frame sequences?"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Predictive Coding Networks", "decision": "conferencePoster-iclr2013-workshop", "abstract": "The quality of data representation in deep learning methods is directly related to the prior model imposed on the representations; however, generally used fixed priors are not capable of adjusting to the context in the data. To address this issue, we propose deep predictive coding networks, a hierarchical generative model that empirically alters priors on the latent representations in a dynamic and context-sensitive manner. This model captures the temporal dependencies in time-varying signals and uses top-down information to modulate the representation in lower layers. The centerpiece of our model is a novel procedure to infer sparse states of a dynamic model; which is used for feature extraction. We also extend this feature extraction block to introduce a pooling function that captures locally invariant representations. When applied on a natural video data, we show that our method is able to learn high-level visual features. We also demonstrate the role of the top-down connections by showing the robustness of the proposed model to structured noise.", "pdf": "https://arxiv.org/abs/1301.3541", "paperhash": "chalasani|deep_predictive_coding_networks", "authors": ["Rakesh Chalasani", "Jose C. Principe"], "authorids": ["vnit.rakesh@gmail.com", "principe@cnel.ufl.edu"], "keywords": [], "conflicts": []}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1361968020000, "tcdate": 1361968020000, "number": 3, "id": "d6u7vbCNJV6Q8", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "yyC_7RZTkUD5-", "replyto": "yyC_7RZTkUD5-", "signatures": ["anonymous reviewer ac47"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Deep Predictive Coding Networks", "review": "Deep predictive coding networks\r\n\r\nThis paper introduces a new model which combines bottom-up, top-down, and temporal information to learning a generative model in an unsupervised fashion on videos. The model is formulated in terms of states, which carry temporal consistency information between time steps, and causes which are the latent variables inferred from the input image that attempt to explain what is in the image.\r\n\r\nPros:\r\nSomewhat interesting filters are learned in the second layer of the model, though these have been shown in prior work.\r\n\r\nNoise reduction on the toy images seems reasonable.\r\n\r\nCons:\r\nThe explanation of the model was overly complicated. After reading the the entire explanation it appears the model is simply doing sparse coding with ISTA alternating on the states and causes. The gradient for ISTA simply has the gradients for the overall cost function, just as in sparse coding but this cost function has some extra temporal terms.\r\n\r\nThe noise reduction is only on toy images and it is not obvious if this is what you would also get with sparse coding using larger patch sizes and high amounts of sparsity. The explanation of points between clusters coming from change in sequences should also appear in the clean video as well because as the text mentions the video changes as well. This is likely due to multiple objects overlapping instead and confusing the model.\r\n\r\nFigure 1 should include the variable names because reading the text and consulting the figure is not very helpful currently.\r\n\r\nIt is hard to reason what each of the A,B, and C is doing without a picture of what they learn on typical data. The layer 1 features seem fairly complex and noisy for the first layer of an image model which typically learns gabor-like features.\r\n\r\nWhere did z come from in equation 11?\r\n\r\nIt is not at all obvious why the states should be temporally consistent and not the causes. The causes are pooled versions of the states and this should be more invariant to changes at the input between frames.\r\n\r\nNovelty and Quality:\r\nThe paper introduces a novel extension to hierarchical sparse coding method by incorporating temporal information at each layer of the model. The poor explanation of this relatively simple idea holds the paper back slightly."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Predictive Coding Networks", "decision": "conferencePoster-iclr2013-workshop", "abstract": "The quality of data representation in deep learning methods is directly related to the prior model imposed on the representations; however, generally used fixed priors are not capable of adjusting to the context in the data. To address this issue, we propose deep predictive coding networks, a hierarchical generative model that empirically alters priors on the latent representations in a dynamic and context-sensitive manner. This model captures the temporal dependencies in time-varying signals and uses top-down information to modulate the representation in lower layers. The centerpiece of our model is a novel procedure to infer sparse states of a dynamic model; which is used for feature extraction. We also extend this feature extraction block to introduce a pooling function that captures locally invariant representations. When applied on a natural video data, we show that our method is able to learn high-level visual features. We also demonstrate the role of the top-down connections by showing the robustness of the proposed model to structured noise.", "pdf": "https://arxiv.org/abs/1301.3541", "paperhash": "chalasani|deep_predictive_coding_networks", "authors": ["Rakesh Chalasani", "Jose C. Principe"], "authorids": ["vnit.rakesh@gmail.com", "principe@cnel.ufl.edu"], "keywords": [], "conflicts": []}, "tags": [], "invitation": {}}}, {"replyto": null, "ddate": null, "legacy_migration": true, "tmdate": 1358405100000, "tcdate": 1358405100000, "number": 27, "id": "yyC_7RZTkUD5-", "invitation": "ICLR.cc/2013/conference/-/submission", "forum": "yyC_7RZTkUD5-", "signatures": ["vnit.rakesh@gmail.com"], "readers": ["everyone"], "content": {"title": "Deep Predictive Coding Networks", "decision": "conferencePoster-iclr2013-workshop", "abstract": "The quality of data representation in deep learning methods is directly related to the prior model imposed on the representations; however, generally used fixed priors are not capable of adjusting to the context in the data. To address this issue, we propose deep predictive coding networks, a hierarchical generative model that empirically alters priors on the latent representations in a dynamic and context-sensitive manner. This model captures the temporal dependencies in time-varying signals and uses top-down information to modulate the representation in lower layers. The centerpiece of our model is a novel procedure to infer sparse states of a dynamic model; which is used for feature extraction. We also extend this feature extraction block to introduce a pooling function that captures locally invariant representations. When applied on a natural video data, we show that our method is able to learn high-level visual features. We also demonstrate the role of the top-down connections by showing the robustness of the proposed model to structured noise.", "pdf": "https://arxiv.org/abs/1301.3541", "paperhash": "chalasani|deep_predictive_coding_networks", "authors": ["Rakesh Chalasani", "Jose C. Principe"], "authorids": ["vnit.rakesh@gmail.com", "principe@cnel.ufl.edu"], "keywords": [], "conflicts": []}, "writers": [], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1369422751717, "tmdate": 1496673673639, "cdate": 1496673673639, "tcdate": 1496673673639, "id": "ICLR.cc/2013/conference/-/submission", "writers": ["ICLR.cc/2013"], "signatures": ["OpenReview.net"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": []}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1377198751717}}}], "count": 10}