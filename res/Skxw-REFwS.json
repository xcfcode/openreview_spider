{"notes": [{"id": "Skxw-REFwS", "original": "rkevCBVOwS", "number": 969, "cdate": 1569439231205, "ddate": null, "tcdate": 1569439231205, "tmdate": 1577168223289, "tddate": null, "forum": "Skxw-REFwS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Unsupervised Progressive Learning and the STAM Architecture", "authors": ["James Smith", "Constantine Dovrolis"], "authorids": ["jamessealesmith@gatech.edu", "constantine@gatech.edu"], "keywords": ["continual learning", "unsupervised learning", "online learning"], "TL;DR": "We introduce Unsupervised  Progressive  Learning  (UPL) and evaluate a neuro-inspired architecture: Self-Taught Associative Memory (STAM).", "abstract": "We  first pose  the  Unsupervised  Progressive  Learning  (UPL)  problem:   learning salient representations from a non-stationary stream of unlabeled data in which the number of object classes increases with time.  If some limited labeled data is also available, those representations can be associated with specific classes, thus enabling classification tasks.  To solve the UPL problem, we propose an architecture that involves an online clustering module, called Self-Taught Associative Memory (STAM). Layered hierarchies of STAM modules learn based on a combination of online clustering, novelty detection, forgetting outliers, and storing only prototypical representations rather than specific examples. The goal of this paper is to introduce the UPL problem, describe the STAM architecture, and evaluate the latter in the UPL context. ", "pdf": "/pdf/1fb579c390e9e16814037ac30da12039ffe6457c.pdf", "paperhash": "smith|unsupervised_progressive_learning_and_the_stam_architecture", "original_pdf": "/attachment/e7d67bd903696a382218d88dae3d4396b5cea95d.pdf", "_bibtex": "@misc{\nsmith2020unsupervised,\ntitle={Unsupervised Progressive Learning and the {\\{}STAM{\\}} Architecture},\nauthor={James Smith and Constantine Dovrolis},\nyear={2020},\nurl={https://openreview.net/forum?id=Skxw-REFwS}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "ZvZkiVSDB", "original": null, "number": 11, "cdate": 1577114852660, "ddate": null, "tcdate": 1577114852660, "tmdate": 1577114852660, "tddate": null, "forum": "Skxw-REFwS", "replyto": "zYNwKnyuOC", "invitation": "ICLR.cc/2020/Conference/Paper969/-/Official_Comment", "content": {"title": "Answering Authors' response", "comment": "Dear Authors, \n\nyou are right: the word \"semi-supervised\" does neither appear in the title nor in the abstract of the paper. \n\nYet:\n* Introduction, first paragraph, 5-th sentence: \"Occasionally, the agent may be given a small number of labeled examples of one or more classes\".\n* Introduction, second paragraph, 1st sentence: \"We denote as $L_t$ the set of class labels the agent has seen up to time $t$\". \n\nRegarding the algorithm:\n* The proposed method builds centroids, in an unsupervised manner (steps 1-4, section 2). \n* In step 5, centroids are associated with classes. \n* In step 6, a centroid is selected iff its max association with a class is greater than a user-dependent threshold. \n* In step 7, the evaluation of the proposed representation is based on a vote, that involves the sum of the values of the *selected* centroids the image patches belong to.\n\nThe area chair thus thinks that the vote is semi-supervised, in the sense that the information conveyed by labels is used. The area chair thinks that the STAM method does not work either if there are no labels. \n\nThe area chair did not do an extensive bibliographic study of semi-supervised data streaming, and it might very well be that the proposed method is better than the state of the art.\n\nThe area chair objects (and based its decision upon) the fact that the authors did \"not compare the performance of STAMs with other methods, because, to the extent of our knowledge, none of the existing approaches in the literature are directly applicable to the problem\". \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper969/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper969/Area_Chair1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Progressive Learning and the STAM Architecture", "authors": ["James Smith", "Constantine Dovrolis"], "authorids": ["jamessealesmith@gatech.edu", "constantine@gatech.edu"], "keywords": ["continual learning", "unsupervised learning", "online learning"], "TL;DR": "We introduce Unsupervised  Progressive  Learning  (UPL) and evaluate a neuro-inspired architecture: Self-Taught Associative Memory (STAM).", "abstract": "We  first pose  the  Unsupervised  Progressive  Learning  (UPL)  problem:   learning salient representations from a non-stationary stream of unlabeled data in which the number of object classes increases with time.  If some limited labeled data is also available, those representations can be associated with specific classes, thus enabling classification tasks.  To solve the UPL problem, we propose an architecture that involves an online clustering module, called Self-Taught Associative Memory (STAM). Layered hierarchies of STAM modules learn based on a combination of online clustering, novelty detection, forgetting outliers, and storing only prototypical representations rather than specific examples. The goal of this paper is to introduce the UPL problem, describe the STAM architecture, and evaluate the latter in the UPL context. ", "pdf": "/pdf/1fb579c390e9e16814037ac30da12039ffe6457c.pdf", "paperhash": "smith|unsupervised_progressive_learning_and_the_stam_architecture", "original_pdf": "/attachment/e7d67bd903696a382218d88dae3d4396b5cea95d.pdf", "_bibtex": "@misc{\nsmith2020unsupervised,\ntitle={Unsupervised Progressive Learning and the {\\{}STAM{\\}} Architecture},\nauthor={James Smith and Constantine Dovrolis},\nyear={2020},\nurl={https://openreview.net/forum?id=Skxw-REFwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Skxw-REFwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper969/Authors", "ICLR.cc/2020/Conference/Paper969/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper969/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper969/Reviewers", "ICLR.cc/2020/Conference/Paper969/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper969/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper969/Authors|ICLR.cc/2020/Conference/Paper969/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504163308, "tmdate": 1576860557895, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper969/Authors", "ICLR.cc/2020/Conference/Paper969/Reviewers", "ICLR.cc/2020/Conference/Paper969/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper969/-/Official_Comment"}}}, {"id": "zYNwKnyuOC", "original": null, "number": 10, "cdate": 1577036766831, "ddate": null, "tcdate": 1577036766831, "tmdate": 1577036766831, "tddate": null, "forum": "Skxw-REFwS", "replyto": "mygBatm-N5", "invitation": "ICLR.cc/2020/Conference/Paper969/-/Official_Comment", "content": {"title": "Authors\u2019 response to paper decision", "comment": "We believe that the AC has misunderstood our paper, and we hope that his/her decision should be revisited. Our work focuses on \u201cunsupervised learning\u201d from an unlabeled datastream, articulated in the title, abstract, and paper content. Yet, the AC refers to our work as a \u201csemi-supervised learning\u201d method, and then articulates the fatal flaw of our work being that we do not compare to other semi-supervised learning methods (a criticism not present in the reviewer discussion).\n\nIn 1.1.3 of our paper, we articulate that our method is not semi-supervised learning. This fact was not debated by any of the three reviewers. Feature learning in the proposed UPL problem is done entirely with unlabeled data. Labeled data is only used for the \u201cdownstream task\u201d of classification to evaluate the features learned using the unlabeled datastream.\n\nThe meta-review identifies three self-described \u201csemi-supervised\u201d methods and claims that a major weakness of our paper is not evaluating these methods. These three papers however are clearly unrelated to the UPL problem that we focus on for the following reasons:\n\n[1] combines a reconstruction cost and semi-supervised cost. Labeled data is a core part of the learning algorithm, and would clearly not work in the absence of labeled data.\n\n[2] aims to \u201cinfer class labels for unlabeled data points using the labeled ones\u201d. The algorithm does not work in the absence of labeled data.\n\n[3] requires offline, pretraining using labeled data to initialize clusters. In the UPL problem, labeled data is only provided for the classification task - the model must learn clusters entirely from unlabeled data.\n\nGiven our high scores (our average score is 6.33) and the obvious misunderstanding in the meta-review, we hope the AC will re-evaluate the decision. \n\nRespectfully, \n\nThe authors.\n\n[1] Incremental Semi-supervised Learning on Streaming Data, Pattern Recognition 88, Li et al., 2018; \n[2] Incremental Semi-Supervised Learning from Streams for Object Classification, Chiotellis et al., 2018; \n[3] Online data stream classification with incremental semi-supervised learning, Loo et al., 2015.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper969/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper969/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Progressive Learning and the STAM Architecture", "authors": ["James Smith", "Constantine Dovrolis"], "authorids": ["jamessealesmith@gatech.edu", "constantine@gatech.edu"], "keywords": ["continual learning", "unsupervised learning", "online learning"], "TL;DR": "We introduce Unsupervised  Progressive  Learning  (UPL) and evaluate a neuro-inspired architecture: Self-Taught Associative Memory (STAM).", "abstract": "We  first pose  the  Unsupervised  Progressive  Learning  (UPL)  problem:   learning salient representations from a non-stationary stream of unlabeled data in which the number of object classes increases with time.  If some limited labeled data is also available, those representations can be associated with specific classes, thus enabling classification tasks.  To solve the UPL problem, we propose an architecture that involves an online clustering module, called Self-Taught Associative Memory (STAM). Layered hierarchies of STAM modules learn based on a combination of online clustering, novelty detection, forgetting outliers, and storing only prototypical representations rather than specific examples. The goal of this paper is to introduce the UPL problem, describe the STAM architecture, and evaluate the latter in the UPL context. ", "pdf": "/pdf/1fb579c390e9e16814037ac30da12039ffe6457c.pdf", "paperhash": "smith|unsupervised_progressive_learning_and_the_stam_architecture", "original_pdf": "/attachment/e7d67bd903696a382218d88dae3d4396b5cea95d.pdf", "_bibtex": "@misc{\nsmith2020unsupervised,\ntitle={Unsupervised Progressive Learning and the {\\{}STAM{\\}} Architecture},\nauthor={James Smith and Constantine Dovrolis},\nyear={2020},\nurl={https://openreview.net/forum?id=Skxw-REFwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Skxw-REFwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper969/Authors", "ICLR.cc/2020/Conference/Paper969/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper969/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper969/Reviewers", "ICLR.cc/2020/Conference/Paper969/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper969/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper969/Authors|ICLR.cc/2020/Conference/Paper969/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504163308, "tmdate": 1576860557895, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper969/Authors", "ICLR.cc/2020/Conference/Paper969/Reviewers", "ICLR.cc/2020/Conference/Paper969/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper969/-/Official_Comment"}}}, {"id": "mygBatm-N5", "original": null, "number": 1, "cdate": 1576798711064, "ddate": null, "tcdate": 1576798711064, "tmdate": 1576800925278, "tddate": null, "forum": "Skxw-REFwS", "replyto": "Skxw-REFwS", "invitation": "ICLR.cc/2020/Conference/Paper969/-/Decision", "content": {"decision": "Reject", "comment": "\n\n\nThe paper presents a semi-supervised data streaming approach. The proposed architecture is made of a layer-wise k-means structure (more specifically a epsilon-means approach, where the epsilon is adaptively defined from the distortion percentile). Each layer is associated a scope (patch dimensions); each patch of the image is associated its nearest cluster center (or a new cluster is created if needed); new cluster centers are adjusted to fit the examples (Short Term Memory); clusters that have been visited sufficiently many time are frozen (Long Term Memory). Each cluster is associated a label distribution from the labelled examples. The label for each new image is obtained by a vote of the clusters and layers.\n\nSome reviews raise some issues about the robustness of the approach, and its sensitivity w.r.t. hyper-parameters. Some claims (\"the distribution associated to a class may change with time\") are not experimentally confirmed; it seems that in such a case, the LTM size might grow along time; a forgetting mechanism would then be needed to enforce the tractability of classification. \n\nSome claims (the mechanism is related to how animal learn) are debatable, as noted by Rev#1; see hippocampal replay.\n\nThe area chair thinks that a main issue with the paper is that the Unsupervised Progressive Learning is considered to be a new setting (\"none of the existing approaches in the literature are directly applicable to the UPL problem\"), preventing the authors from comparing their results with baselines.\n\nHowever, after a short bibliographic search, some related approaches exist under another name:\n* Incremental Semi-supervised Learning on Streaming Data, Pattern Recognition 88, Li et al., 2018;\n* Incremental Semi-Supervised Learning from Streams for Object Classification, Chiotellis et al., 2018;\n* Online data stream classification with incremental semi-supervised learning, Loo et al., 2015.\n\nThe above approaches seem able to at least accommodate the Uniform UPL scenario. I therefore encourage the authors to consider some of the above as baselines and provide a comparative validation of STAM.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Progressive Learning and the STAM Architecture", "authors": ["James Smith", "Constantine Dovrolis"], "authorids": ["jamessealesmith@gatech.edu", "constantine@gatech.edu"], "keywords": ["continual learning", "unsupervised learning", "online learning"], "TL;DR": "We introduce Unsupervised  Progressive  Learning  (UPL) and evaluate a neuro-inspired architecture: Self-Taught Associative Memory (STAM).", "abstract": "We  first pose  the  Unsupervised  Progressive  Learning  (UPL)  problem:   learning salient representations from a non-stationary stream of unlabeled data in which the number of object classes increases with time.  If some limited labeled data is also available, those representations can be associated with specific classes, thus enabling classification tasks.  To solve the UPL problem, we propose an architecture that involves an online clustering module, called Self-Taught Associative Memory (STAM). Layered hierarchies of STAM modules learn based on a combination of online clustering, novelty detection, forgetting outliers, and storing only prototypical representations rather than specific examples. The goal of this paper is to introduce the UPL problem, describe the STAM architecture, and evaluate the latter in the UPL context. ", "pdf": "/pdf/1fb579c390e9e16814037ac30da12039ffe6457c.pdf", "paperhash": "smith|unsupervised_progressive_learning_and_the_stam_architecture", "original_pdf": "/attachment/e7d67bd903696a382218d88dae3d4396b5cea95d.pdf", "_bibtex": "@misc{\nsmith2020unsupervised,\ntitle={Unsupervised Progressive Learning and the {\\{}STAM{\\}} Architecture},\nauthor={James Smith and Constantine Dovrolis},\nyear={2020},\nurl={https://openreview.net/forum?id=Skxw-REFwS}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "Skxw-REFwS", "replyto": "Skxw-REFwS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795712375, "tmdate": 1576800261745, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper969/-/Decision"}}}, {"id": "rJeFX8M7cS", "original": null, "number": 3, "cdate": 1572181536937, "ddate": null, "tcdate": 1572181536937, "tmdate": 1574439426787, "tddate": null, "forum": "Skxw-REFwS", "replyto": "Skxw-REFwS", "invitation": "ICLR.cc/2020/Conference/Paper969/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #2", "review": "The paper presents a new problem formulation in the broader area of lifelong learning.\nSpecifically, the Unsupervised Progressive Learning (UPL) problem, requires a learner to consume a stream of data, where each data point is associated with a class, but only very few labels are provided. Similar to continuous learning the learner can only access current and not access previous data in the stream. The paper clearly describes how this setup is different to commonly other learning setups studied, including continual learning. \n\nTo solve this problem the paper proposes a deep-learning-free approach based on clustering and novelty detection.\n\nStrength:\n-\tI think the problem formulation is very realistic and interesting and I am not aware of it being explored previously\n-\tThe newly proposed architecture STAM is also interesting.\n-\tThe paper evaluates STAM on UPL on three datasets, MNIST, EMNIST, SVHN\n-\tThe paper ablates several aspects of the model evaluates the effect of hyper parameters.\n\nWeaknesses:\n1.\tThe paper misses to include baselines, both, w.r.t. the proposed method, as well as for the learning problem. \n1.1.\tThe paper argues that deep learning cannot work, I am wondering, why don\u2019t include standard baselines, e.g. auto-encoder based with a prototype stored whenever a labeled example is seen. (For single pass continual learning, see e.g. [B], although that work is supervised)\n1.2.\tHebbian learning inspired algorithms might similarly form a valuable baseline\n2.\tThe UPL problem makes sense to me, however, the experimental setup could be improved. Specifically, the paper uses the entire dataset to find hyperparameters. This is highly concerning as this basically means the entire dataset has been seen multiple times before the final pass over the dataset (the one with the best hyperparameters) is performed. This is basically a contradiction to the setup of UPL, which assumes all data is seen only once.\n3.\tRelated Work:\n3.1.\t[A] seems to study a similar setup and proposes an approach which should also be applicable in this work.\n\n\n\n\n\n\n\nConclusion:\nThe paper\u2019s contribution is a new learning setup (UPL) and an approach (STAM). While it remains unclear if STAM can generalize to realistic images, the idea of UPL is very interesting and speaks for accepting the paper, although there are several weaknesses the authors should address.\nIt would also be great if the authors plan to release the exact experimental setup (i.e. which images are sampled in which order and which ones are labeled) so future work can compare to this work.\n\n\n\nReference:\n[A] Continuous Online Sequence Learning with an Unsupervised Neural Network Model\nYuwei Cui, Subutai Ahmad and Jeff Hawkins; Neural Computation, 2016\n[B] Gradient episodic memory for continual learning, D Lopez-Paz, MA Ranzato ; NeurIPS, 2017\n\n\n==== Post rebuttal comment:\nThanks for the clarifications and additional experiments;\nI do think the the paper should be accepted and increased my score.\n\nRegarding 2) I was thinking more of using a small part of the dataset to do hyper parameter selection, see e.g. \nEfficient Lifelong Learning with A-GEM\nA Chaudhry, MA Ranzato, M Rohrbach, M Elhoseiny\nInternational Conference on Learning Representations (ICLR)\n\n\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"}, "signatures": ["ICLR.cc/2020/Conference/Paper969/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper969/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Progressive Learning and the STAM Architecture", "authors": ["James Smith", "Constantine Dovrolis"], "authorids": ["jamessealesmith@gatech.edu", "constantine@gatech.edu"], "keywords": ["continual learning", "unsupervised learning", "online learning"], "TL;DR": "We introduce Unsupervised  Progressive  Learning  (UPL) and evaluate a neuro-inspired architecture: Self-Taught Associative Memory (STAM).", "abstract": "We  first pose  the  Unsupervised  Progressive  Learning  (UPL)  problem:   learning salient representations from a non-stationary stream of unlabeled data in which the number of object classes increases with time.  If some limited labeled data is also available, those representations can be associated with specific classes, thus enabling classification tasks.  To solve the UPL problem, we propose an architecture that involves an online clustering module, called Self-Taught Associative Memory (STAM). Layered hierarchies of STAM modules learn based on a combination of online clustering, novelty detection, forgetting outliers, and storing only prototypical representations rather than specific examples. The goal of this paper is to introduce the UPL problem, describe the STAM architecture, and evaluate the latter in the UPL context. ", "pdf": "/pdf/1fb579c390e9e16814037ac30da12039ffe6457c.pdf", "paperhash": "smith|unsupervised_progressive_learning_and_the_stam_architecture", "original_pdf": "/attachment/e7d67bd903696a382218d88dae3d4396b5cea95d.pdf", "_bibtex": "@misc{\nsmith2020unsupervised,\ntitle={Unsupervised Progressive Learning and the {\\{}STAM{\\}} Architecture},\nauthor={James Smith and Constantine Dovrolis},\nyear={2020},\nurl={https://openreview.net/forum?id=Skxw-REFwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Skxw-REFwS", "replyto": "Skxw-REFwS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper969/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper969/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575730488970, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper969/Reviewers"], "noninvitees": [], "tcdate": 1570237744309, "tmdate": 1575730488984, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper969/-/Official_Review"}}}, {"id": "r1xKKf42oS", "original": null, "number": 5, "cdate": 1573827201506, "ddate": null, "tcdate": 1573827201506, "tmdate": 1573827201506, "tddate": null, "forum": "Skxw-REFwS", "replyto": "H1eYI2W3oB", "invitation": "ICLR.cc/2020/Conference/Paper969/-/Official_Comment", "content": {"title": "Response to reviewer #3", "comment": "Thank you for this additional follow up comment. We commit to moving the memory requirement results/discussion from the appendix to the main paper in the final revision of the paper. \n\nWe would like to clarify an important point. The memory requirement of STAM includes all parameters of this model (i.e. the centroids stored in both STM and LTM). STAM does not require stored images for replay. A deep neural net that uses replay to avoid catastrophic forgetting would need to store both its parameters and a number of images. For example, the deep learning baseline in Appendix F has a memory requirement of about 4,500 grayscale SVHN images just to store its parameters."}, "signatures": ["ICLR.cc/2020/Conference/Paper969/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper969/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Progressive Learning and the STAM Architecture", "authors": ["James Smith", "Constantine Dovrolis"], "authorids": ["jamessealesmith@gatech.edu", "constantine@gatech.edu"], "keywords": ["continual learning", "unsupervised learning", "online learning"], "TL;DR": "We introduce Unsupervised  Progressive  Learning  (UPL) and evaluate a neuro-inspired architecture: Self-Taught Associative Memory (STAM).", "abstract": "We  first pose  the  Unsupervised  Progressive  Learning  (UPL)  problem:   learning salient representations from a non-stationary stream of unlabeled data in which the number of object classes increases with time.  If some limited labeled data is also available, those representations can be associated with specific classes, thus enabling classification tasks.  To solve the UPL problem, we propose an architecture that involves an online clustering module, called Self-Taught Associative Memory (STAM). Layered hierarchies of STAM modules learn based on a combination of online clustering, novelty detection, forgetting outliers, and storing only prototypical representations rather than specific examples. The goal of this paper is to introduce the UPL problem, describe the STAM architecture, and evaluate the latter in the UPL context. ", "pdf": "/pdf/1fb579c390e9e16814037ac30da12039ffe6457c.pdf", "paperhash": "smith|unsupervised_progressive_learning_and_the_stam_architecture", "original_pdf": "/attachment/e7d67bd903696a382218d88dae3d4396b5cea95d.pdf", "_bibtex": "@misc{\nsmith2020unsupervised,\ntitle={Unsupervised Progressive Learning and the {\\{}STAM{\\}} Architecture},\nauthor={James Smith and Constantine Dovrolis},\nyear={2020},\nurl={https://openreview.net/forum?id=Skxw-REFwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Skxw-REFwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper969/Authors", "ICLR.cc/2020/Conference/Paper969/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper969/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper969/Reviewers", "ICLR.cc/2020/Conference/Paper969/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper969/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper969/Authors|ICLR.cc/2020/Conference/Paper969/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504163308, "tmdate": 1576860557895, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper969/Authors", "ICLR.cc/2020/Conference/Paper969/Reviewers", "ICLR.cc/2020/Conference/Paper969/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper969/-/Official_Comment"}}}, {"id": "H1eYI2W3oB", "original": null, "number": 4, "cdate": 1573817425263, "ddate": null, "tcdate": 1573817425263, "tmdate": 1573817425263, "tddate": null, "forum": "Skxw-REFwS", "replyto": "B1eQ7GoosS", "invitation": "ICLR.cc/2020/Conference/Paper969/-/Official_Comment", "content": {"title": "Response to authors", "comment": "Many thanks for your additional experiments and comments. It is good to hear you will make this publicly available. Please do use any of my suggestions/ideas as you see fit, I will be interested to see future work.\n\nThank you for the intuition of Delta=500 corresponding to about 930 grayscale images in SVHN. I would actually like to see this point made in the main paper: I think it provides much-needed intuition to the memory cost of STAM. 930 grayscale images is quite a lot (and Delta=500), and this would be useful for readers to know. I do not view this to be a downside for this paper's acceptance, rather a clarification of potential negatives of STAM, which could of course be improved in the future. UPL is a hard problem and the STAM approach requires what seems like a lot of memory. This number (of 930) would be useful for other approaches, as and when they are developed, to compare to."}, "signatures": ["ICLR.cc/2020/Conference/Paper969/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper969/AnonReviewer3", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Progressive Learning and the STAM Architecture", "authors": ["James Smith", "Constantine Dovrolis"], "authorids": ["jamessealesmith@gatech.edu", "constantine@gatech.edu"], "keywords": ["continual learning", "unsupervised learning", "online learning"], "TL;DR": "We introduce Unsupervised  Progressive  Learning  (UPL) and evaluate a neuro-inspired architecture: Self-Taught Associative Memory (STAM).", "abstract": "We  first pose  the  Unsupervised  Progressive  Learning  (UPL)  problem:   learning salient representations from a non-stationary stream of unlabeled data in which the number of object classes increases with time.  If some limited labeled data is also available, those representations can be associated with specific classes, thus enabling classification tasks.  To solve the UPL problem, we propose an architecture that involves an online clustering module, called Self-Taught Associative Memory (STAM). Layered hierarchies of STAM modules learn based on a combination of online clustering, novelty detection, forgetting outliers, and storing only prototypical representations rather than specific examples. The goal of this paper is to introduce the UPL problem, describe the STAM architecture, and evaluate the latter in the UPL context. ", "pdf": "/pdf/1fb579c390e9e16814037ac30da12039ffe6457c.pdf", "paperhash": "smith|unsupervised_progressive_learning_and_the_stam_architecture", "original_pdf": "/attachment/e7d67bd903696a382218d88dae3d4396b5cea95d.pdf", "_bibtex": "@misc{\nsmith2020unsupervised,\ntitle={Unsupervised Progressive Learning and the {\\{}STAM{\\}} Architecture},\nauthor={James Smith and Constantine Dovrolis},\nyear={2020},\nurl={https://openreview.net/forum?id=Skxw-REFwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Skxw-REFwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper969/Authors", "ICLR.cc/2020/Conference/Paper969/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper969/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper969/Reviewers", "ICLR.cc/2020/Conference/Paper969/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper969/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper969/Authors|ICLR.cc/2020/Conference/Paper969/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504163308, "tmdate": 1576860557895, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper969/Authors", "ICLR.cc/2020/Conference/Paper969/Reviewers", "ICLR.cc/2020/Conference/Paper969/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper969/-/Official_Comment"}}}, {"id": "B1xhkMossH", "original": null, "number": 1, "cdate": 1573790179634, "ddate": null, "tcdate": 1573790179634, "tmdate": 1573791340904, "tddate": null, "forum": "Skxw-REFwS", "replyto": "rJeFX8M7cS", "invitation": "ICLR.cc/2020/Conference/Paper969/-/Official_Comment", "content": {"title": "Authors\u2019 response to reviewer #2 (including new baseline comparison)", "comment": "Thank you for the constructive comments.\n\n1) Following your suggestion, we have also generated results using a convolutional autoencoder for feature learning, followed by a K-nearest-neighbor classifier. In the UPL context, where the unlabeled data should be seen only once, we train the autoencoder by setting the number of epochs and the batch size to one. So each unlabeled input in the data stream is used in the training process only once. The use of a KNN classifier means that we also use the labeled examples only once. The related results for SVHN appear in Appendix F (see Figures 9 and 10). Please note that STAM performs significantly better than this deep-learning baseline. We understand of course that this is only a simple baseline and we plan to develop a number of more sophisticated baselines in the near future. \n\n1.1) Thank you for sharing the work of [B]. We agree this is a similarly motivated work and we included that paper in the references. However, we think that their method is not applicable to UPL because they still need to know the tasks, even if the task schedule is not known. \n\n1.2) Regarding Hebbian learning: please note that the online clustering method that STAM is based on is effectively a Hebbian learning method: please see the reference by Pehlevan et al. 2017 in our paper. That work has shown how to do online k-means clustering based strictly on Hebbian learning. \n\n2) The reviewer is right that we have used the entire dataset to choose hyperparameters -- this was a mistake that we regret. However, based on our experience working with MNIST, EMNIST and SVHN, it is very unlikely that there would be any significant change if we had only used a portion of the dataset for hyperparameter tuning. Additionally, please note that with the exception of the parameter Delta, the rest of the hyperparameters do not cause a major effect on classification accuracy. In future work, we plan to choose hyperparameter values based on a different, but similar, dataset -- for example, to use examples from ImageNet to choose hyperparameters, and then applying that STAM architecture on Google\u2019s Open Images.  \n\n3) Thank you for sharing the work of [A]. It is relevant in that they do continuous online learning from streaming data -- we included that paper in the references. However, we do not think that their method is relevant enough to STAM that would warrant a direct comparison.\n\nAdditional comment: We have inserted an improved description of the experiment setup (see Section 3, Paragraph 3). \n\nWe commit to release the STAM code on GitHub after publication.\n\nNote: the changes discussed above are available in a recently uploaded revision. The red vertical lines on the right margin represent modifications from the original submission."}, "signatures": ["ICLR.cc/2020/Conference/Paper969/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper969/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Progressive Learning and the STAM Architecture", "authors": ["James Smith", "Constantine Dovrolis"], "authorids": ["jamessealesmith@gatech.edu", "constantine@gatech.edu"], "keywords": ["continual learning", "unsupervised learning", "online learning"], "TL;DR": "We introduce Unsupervised  Progressive  Learning  (UPL) and evaluate a neuro-inspired architecture: Self-Taught Associative Memory (STAM).", "abstract": "We  first pose  the  Unsupervised  Progressive  Learning  (UPL)  problem:   learning salient representations from a non-stationary stream of unlabeled data in which the number of object classes increases with time.  If some limited labeled data is also available, those representations can be associated with specific classes, thus enabling classification tasks.  To solve the UPL problem, we propose an architecture that involves an online clustering module, called Self-Taught Associative Memory (STAM). Layered hierarchies of STAM modules learn based on a combination of online clustering, novelty detection, forgetting outliers, and storing only prototypical representations rather than specific examples. The goal of this paper is to introduce the UPL problem, describe the STAM architecture, and evaluate the latter in the UPL context. ", "pdf": "/pdf/1fb579c390e9e16814037ac30da12039ffe6457c.pdf", "paperhash": "smith|unsupervised_progressive_learning_and_the_stam_architecture", "original_pdf": "/attachment/e7d67bd903696a382218d88dae3d4396b5cea95d.pdf", "_bibtex": "@misc{\nsmith2020unsupervised,\ntitle={Unsupervised Progressive Learning and the {\\{}STAM{\\}} Architecture},\nauthor={James Smith and Constantine Dovrolis},\nyear={2020},\nurl={https://openreview.net/forum?id=Skxw-REFwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Skxw-REFwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper969/Authors", "ICLR.cc/2020/Conference/Paper969/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper969/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper969/Reviewers", "ICLR.cc/2020/Conference/Paper969/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper969/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper969/Authors|ICLR.cc/2020/Conference/Paper969/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504163308, "tmdate": 1576860557895, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper969/Authors", "ICLR.cc/2020/Conference/Paper969/Reviewers", "ICLR.cc/2020/Conference/Paper969/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper969/-/Official_Comment"}}}, {"id": "B1eQ7GoosS", "original": null, "number": 2, "cdate": 1573790234609, "ddate": null, "tcdate": 1573790234609, "tmdate": 1573791330025, "tddate": null, "forum": "Skxw-REFwS", "replyto": "BJlTWgj6tS", "invitation": "ICLR.cc/2020/Conference/Paper969/-/Official_Comment", "content": {"title": "Authors\u2019 response to reviewer #3 (including memory footprint calculations)", "comment": "Thank you for this constructive review and suggestions. You gave us a lot to think about and, with your permission, we would like to investigate all these questions in follow-up work. Also. we commit to make the STAM code and evaluation framework publicly available on GitHub after publication.\n\nWe agree that the memory footprint of STAM is important and have now included some related calculations in Appendix G. We calculate (for the case of SVHN), how many pixels would be required -- and what is the equivalent number of SVHN images -- based on the number of learned centroids in both Short-Term Memory and Long-Term Memory.\n\nPlease note that in the submitted paper we did not optimize STAM in terms of memory efficiency. Following your suggestion, we performed a number of experiments in which we asked: how small can the memory footprint (controlled by the parameter Delta, which is the Short-Term Memory size) be without introducing a significant loss in accuracy. These results appear in Figures 8-d,e,f in Appendix E. Interestingly, we can reduce the memory footprint significantly (from Delta=2000 to 500 centroids) without a noticeable loss in accuracy. \n\nFor Delta=500 centroids, the memory footprint of STAM for SVHN is equivalent to about 930 grayscale images. This is only 5% of the memory that would be required to store the parameters of the convolutional autoencoder that we use in Appendix-F. \n\nRegarding the idea of increasing both Delta and theta, unfortunately we did not have enough time to generate those results yet but we plan to do so next. \n\nWe appreciate the suggestions of adopting progressive neural networks and including uncertainty estimates and robustness to adversarial examples. These are great directions to take this research forward!\n\nNote: the changes discussed above are available in a recently uploaded revision. The red vertical lines on the right margin represent modifications from the original submission."}, "signatures": ["ICLR.cc/2020/Conference/Paper969/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper969/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Progressive Learning and the STAM Architecture", "authors": ["James Smith", "Constantine Dovrolis"], "authorids": ["jamessealesmith@gatech.edu", "constantine@gatech.edu"], "keywords": ["continual learning", "unsupervised learning", "online learning"], "TL;DR": "We introduce Unsupervised  Progressive  Learning  (UPL) and evaluate a neuro-inspired architecture: Self-Taught Associative Memory (STAM).", "abstract": "We  first pose  the  Unsupervised  Progressive  Learning  (UPL)  problem:   learning salient representations from a non-stationary stream of unlabeled data in which the number of object classes increases with time.  If some limited labeled data is also available, those representations can be associated with specific classes, thus enabling classification tasks.  To solve the UPL problem, we propose an architecture that involves an online clustering module, called Self-Taught Associative Memory (STAM). Layered hierarchies of STAM modules learn based on a combination of online clustering, novelty detection, forgetting outliers, and storing only prototypical representations rather than specific examples. The goal of this paper is to introduce the UPL problem, describe the STAM architecture, and evaluate the latter in the UPL context. ", "pdf": "/pdf/1fb579c390e9e16814037ac30da12039ffe6457c.pdf", "paperhash": "smith|unsupervised_progressive_learning_and_the_stam_architecture", "original_pdf": "/attachment/e7d67bd903696a382218d88dae3d4396b5cea95d.pdf", "_bibtex": "@misc{\nsmith2020unsupervised,\ntitle={Unsupervised Progressive Learning and the {\\{}STAM{\\}} Architecture},\nauthor={James Smith and Constantine Dovrolis},\nyear={2020},\nurl={https://openreview.net/forum?id=Skxw-REFwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Skxw-REFwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper969/Authors", "ICLR.cc/2020/Conference/Paper969/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper969/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper969/Reviewers", "ICLR.cc/2020/Conference/Paper969/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper969/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper969/Authors|ICLR.cc/2020/Conference/Paper969/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504163308, "tmdate": 1576860557895, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper969/Authors", "ICLR.cc/2020/Conference/Paper969/Reviewers", "ICLR.cc/2020/Conference/Paper969/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper969/-/Official_Comment"}}}, {"id": "ByguSGojoB", "original": null, "number": 3, "cdate": 1573790271782, "ddate": null, "tcdate": 1573790271782, "tmdate": 1573791318039, "tddate": null, "forum": "Skxw-REFwS", "replyto": "BygbC5qqtr", "invitation": "ICLR.cc/2020/Conference/Paper969/-/Official_Comment", "content": {"title": "Authors\u2019 response to reviewer #1", "comment": "Thank you for the constructive comments. \n\nWe agree that experiments with additional datasets are required and we are already in the process of doing them. We want to note however that it is common, at least in the unsupervised learning literature, that any new approaches are first studied using simpler datasets (e.g., see [1]) before experimenting with more complex datasets that include multiple objects in the same image or limited examples per class. \n\nPer the suggestions of the two other reviewers, we have included a comparison with a deep-learning baseline (see Appendix F) and a memory footprint comparison (see Appendix G). \n\nFurthermore, we have generated new results that show how STAM performs with only a limited number of LTM centroids (updated Appendix E). The results show that STAM performs equally well, in the case of SVHN, even when we reduce the number of LTM centroids from 2000 to 500. \n\nWe would like to argue that the number of unlabeled training examples we use in STAM does indeed mimic animal learning. Animals receive a stream of sensory input (think of video instead of static images). Every object is seen by many angles, depths, perspectives, etc and, typically, for at least several seconds. \n\nWe agree that the number of labeled examples should be limited, and this is why we consider only 10-100 such examples per class. In the future, we plan to reduce this range to 1-10 but include some degree of data augmentation for the labeled examples (e.g., looking at the same object from different angles and distances). \n\nRegarding your comment about the L2 distance metric and the nearest-neighbor problem in high-dimensional data: we are deeply aware of the theoretical challenges of the nearest-neighbor problem in high-dimensional data (and we cite, for instance, the 1999 paper by Beyer et al). But please note that we apply this metric in small patches of the images -- not at the level of entire images. We should mention that we have also experimented with the L1 metric, without seeing significant changes in the results.\n\nNote: the changes discussed above are available in a recently uploaded revision. The red vertical lines on the right margin represent modifications from the original submission.\n\n[1] A. R. Kosiorek, S. Sabour, Y. W. Teh, and G. E. Hinton. \u201cStacked Capsule Autoencoders\u201d. In: CoRR. arXiv:1906.06818, 2019"}, "signatures": ["ICLR.cc/2020/Conference/Paper969/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper969/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Progressive Learning and the STAM Architecture", "authors": ["James Smith", "Constantine Dovrolis"], "authorids": ["jamessealesmith@gatech.edu", "constantine@gatech.edu"], "keywords": ["continual learning", "unsupervised learning", "online learning"], "TL;DR": "We introduce Unsupervised  Progressive  Learning  (UPL) and evaluate a neuro-inspired architecture: Self-Taught Associative Memory (STAM).", "abstract": "We  first pose  the  Unsupervised  Progressive  Learning  (UPL)  problem:   learning salient representations from a non-stationary stream of unlabeled data in which the number of object classes increases with time.  If some limited labeled data is also available, those representations can be associated with specific classes, thus enabling classification tasks.  To solve the UPL problem, we propose an architecture that involves an online clustering module, called Self-Taught Associative Memory (STAM). Layered hierarchies of STAM modules learn based on a combination of online clustering, novelty detection, forgetting outliers, and storing only prototypical representations rather than specific examples. The goal of this paper is to introduce the UPL problem, describe the STAM architecture, and evaluate the latter in the UPL context. ", "pdf": "/pdf/1fb579c390e9e16814037ac30da12039ffe6457c.pdf", "paperhash": "smith|unsupervised_progressive_learning_and_the_stam_architecture", "original_pdf": "/attachment/e7d67bd903696a382218d88dae3d4396b5cea95d.pdf", "_bibtex": "@misc{\nsmith2020unsupervised,\ntitle={Unsupervised Progressive Learning and the {\\{}STAM{\\}} Architecture},\nauthor={James Smith and Constantine Dovrolis},\nyear={2020},\nurl={https://openreview.net/forum?id=Skxw-REFwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Skxw-REFwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper969/Authors", "ICLR.cc/2020/Conference/Paper969/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper969/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper969/Reviewers", "ICLR.cc/2020/Conference/Paper969/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper969/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper969/Authors|ICLR.cc/2020/Conference/Paper969/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504163308, "tmdate": 1576860557895, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper969/Authors", "ICLR.cc/2020/Conference/Paper969/Reviewers", "ICLR.cc/2020/Conference/Paper969/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper969/-/Official_Comment"}}}, {"id": "BygbC5qqtr", "original": null, "number": 1, "cdate": 1571625673157, "ddate": null, "tcdate": 1571625673157, "tmdate": 1572972529116, "tddate": null, "forum": "Skxw-REFwS", "replyto": "Skxw-REFwS", "invitation": "ICLR.cc/2020/Conference/Paper969/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper proposes a method called Self-Taught Associative Memory (STAM) for Unsupervised Progressive Learning (UPL) , i.e., learning salient representation from streams of mostly unlabeled data with occasional class labels, where the number of class increases over time. The motivation of this paper is quite interesting in that the authors try to mimic how animals learn. The surrounding environments of animals are considered to be unlabeled, and animals gradually learn to distinguish between objects without explicit information. The model shed light on the problem of catastrophic forgetting by introducing dual-memory organization. To be specific, Short-Term Memory contains a set of centroids associated with the unlabeled data, whereas Long-Term Memory stores the prototypical centroids, which are frequently seen patterns. In addition, the model utilizes novelty detection technique to introduce new centroids to each layer of the model, and it prepares the newly created centroids to be associated with new classes. \nOverall, the paper reads well and it is self-explanatory with clear notations and hyperparameters. Each step of the architecture is well-formulated mathematically and the necessity of the step in the model is explained clearly. The problem proposed, Unsupervised Progressive Learning (UPL) problem, is novel.\nOn the other hand, there are shortcomings of the paper, one being the model evaluation on dataset such as MNIST, SVHN, and EMNIST. The dimensional size of the dataset and the number of classes are small which is not convincing to evaluate the performance of the model. In addition, each layer of the model is independent from other layers (no connections between units at different layers), keeping the updated representation of patches to itself. This fact would hinder the model from understanding complex representation. Furthermore, there is a concern on the use of L2 distance metric for the similarity between the patch and patterns (centroids). L2 is not meaningful in high dimensional spaces.\nThis paper is a good start in tackling the Unsupervised Progressive Learning problem, but some weaknesses are present in the nature of the model architecture as mentioned above. However, the approach taken appears to be ad hoc. It is difficult to imagine the method can work for more complex situations. Even for the small datasets used in the paper, the learning require 10\u2019s of thousands of training example. This does not mimic animal learning at all. Also, the accuracy of the model on EMNIST with 47 classes is around 65% which makes me doubt the application of the model on real world.\n-\tShed some light on the problem of catastrophic forgetting and continual learning without supervision\n-\tThe model architecture is well defined, but has some weaknesses in the methodology\n-\tInteresting motivation of trying to mimic how animals learn in an environment\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper969/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper969/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Progressive Learning and the STAM Architecture", "authors": ["James Smith", "Constantine Dovrolis"], "authorids": ["jamessealesmith@gatech.edu", "constantine@gatech.edu"], "keywords": ["continual learning", "unsupervised learning", "online learning"], "TL;DR": "We introduce Unsupervised  Progressive  Learning  (UPL) and evaluate a neuro-inspired architecture: Self-Taught Associative Memory (STAM).", "abstract": "We  first pose  the  Unsupervised  Progressive  Learning  (UPL)  problem:   learning salient representations from a non-stationary stream of unlabeled data in which the number of object classes increases with time.  If some limited labeled data is also available, those representations can be associated with specific classes, thus enabling classification tasks.  To solve the UPL problem, we propose an architecture that involves an online clustering module, called Self-Taught Associative Memory (STAM). Layered hierarchies of STAM modules learn based on a combination of online clustering, novelty detection, forgetting outliers, and storing only prototypical representations rather than specific examples. The goal of this paper is to introduce the UPL problem, describe the STAM architecture, and evaluate the latter in the UPL context. ", "pdf": "/pdf/1fb579c390e9e16814037ac30da12039ffe6457c.pdf", "paperhash": "smith|unsupervised_progressive_learning_and_the_stam_architecture", "original_pdf": "/attachment/e7d67bd903696a382218d88dae3d4396b5cea95d.pdf", "_bibtex": "@misc{\nsmith2020unsupervised,\ntitle={Unsupervised Progressive Learning and the {\\{}STAM{\\}} Architecture},\nauthor={James Smith and Constantine Dovrolis},\nyear={2020},\nurl={https://openreview.net/forum?id=Skxw-REFwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Skxw-REFwS", "replyto": "Skxw-REFwS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper969/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper969/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575730488970, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper969/Reviewers"], "noninvitees": [], "tcdate": 1570237744309, "tmdate": 1575730488984, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper969/-/Official_Review"}}}, {"id": "BJlTWgj6tS", "original": null, "number": 2, "cdate": 1571823621254, "ddate": null, "tcdate": 1571823621254, "tmdate": 1572972529082, "tddate": null, "forum": "Skxw-REFwS", "replyto": "Skxw-REFwS", "invitation": "ICLR.cc/2020/Conference/Paper969/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper sets up a new problem based on a continuous stream of potentially partially labelled data, which the authors call the Unsupervised Progressive Learning problem. The paper also introduces a new model designed to approach this problem, called the STAM architecture, with many concepts applied in a novel way. The STAM architecture is tested on example problems from the UPL problem.\n\nI find this paper to approach a problem that is not well studied in the literature, and it is well-written and easy to read. I am not aware of previous works that tackle this specific problem setup. Many previous works seem to slowly be converging to tackling this kind of problem, but this is the first work that directly tackles this realistic problem. It would be good if the authors made the benchmark (/how they are generated) public.\n\nThe STAM architecture is an interesting way of tackling image classification, and it is refreshing to see a technique not dependent on neural networks. The experiments in the paper are extremely detailed, with good figures and ablation studies.\n\nI am recommending this paper be accepted. But I have one main question about this work: what is the memory cost of the STAM architecture? Many 1000s of LTM centroids are stored, what is the memory cost of this? Is this memory cost greater than the cost of just storing all the labelled inputs that have been seen? I would imagine that just replaying these stored labelled inputs (or just training a NN on these stored inputs) would provide extremely high accuracies for the datasets considered in this paper.\n\nI understand that STAM has potential beyond just replaying memory for potentially larger datasets; the authors also make the point that they do not believe that the brain works by replaying memory. However, I would then like to see one (or both) of the following tests: a run with much fewer LTM centroids (with the corresponding memory cost detailed explicitly), and/or a dataset where it is clear that the memory cost incurred by the STAM architecture is a better use of memory than just storing previous data.\n\nI would also argue that the brain *does* replay memory in some manner in order to learn. But that is a debate for another time!\n\nBy fixing the LTM centroids that are learnt on previous data, the method is essentially freezing previous knowledge and then learning new knowledge by increasing model capacity. It would be interesting (as future work / other papers) to see an adopted version of eg Progressive Neural Networks and how that compares on the same benchmarks.\n\nI would also be interested to see, as future work, the uncertainty estimates of this new architecture and its robustness to adversarial examples.\n\nFinally, a couple of minor points:\n- Figure 4 is small and hard to read, particularly the left column.\n- The authors find (in Appendix E) that increasing the \\Delta hyperparameter leads to a growing number of LTM centroids. I wonder if that is still the case if \\theta is also increased along with \\Delta?\n- Typos: last line of Section 3.2: 'lowst'; a few places where \\citet{} and \\citep{} are incorrect."}, "signatures": ["ICLR.cc/2020/Conference/Paper969/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper969/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Progressive Learning and the STAM Architecture", "authors": ["James Smith", "Constantine Dovrolis"], "authorids": ["jamessealesmith@gatech.edu", "constantine@gatech.edu"], "keywords": ["continual learning", "unsupervised learning", "online learning"], "TL;DR": "We introduce Unsupervised  Progressive  Learning  (UPL) and evaluate a neuro-inspired architecture: Self-Taught Associative Memory (STAM).", "abstract": "We  first pose  the  Unsupervised  Progressive  Learning  (UPL)  problem:   learning salient representations from a non-stationary stream of unlabeled data in which the number of object classes increases with time.  If some limited labeled data is also available, those representations can be associated with specific classes, thus enabling classification tasks.  To solve the UPL problem, we propose an architecture that involves an online clustering module, called Self-Taught Associative Memory (STAM). Layered hierarchies of STAM modules learn based on a combination of online clustering, novelty detection, forgetting outliers, and storing only prototypical representations rather than specific examples. The goal of this paper is to introduce the UPL problem, describe the STAM architecture, and evaluate the latter in the UPL context. ", "pdf": "/pdf/1fb579c390e9e16814037ac30da12039ffe6457c.pdf", "paperhash": "smith|unsupervised_progressive_learning_and_the_stam_architecture", "original_pdf": "/attachment/e7d67bd903696a382218d88dae3d4396b5cea95d.pdf", "_bibtex": "@misc{\nsmith2020unsupervised,\ntitle={Unsupervised Progressive Learning and the {\\{}STAM{\\}} Architecture},\nauthor={James Smith and Constantine Dovrolis},\nyear={2020},\nurl={https://openreview.net/forum?id=Skxw-REFwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Skxw-REFwS", "replyto": "Skxw-REFwS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper969/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper969/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575730488970, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper969/Reviewers"], "noninvitees": [], "tcdate": 1570237744309, "tmdate": 1575730488984, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper969/-/Official_Review"}}}], "count": 12}