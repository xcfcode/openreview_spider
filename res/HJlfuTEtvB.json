{"notes": [{"id": "p7AhTChUEO", "original": null, "number": 17, "cdate": 1590078252039, "ddate": null, "tcdate": 1590078252039, "tmdate": 1590078317005, "tddate": null, "forum": "HJlfuTEtvB", "replyto": "U4dhEnOuClb", "invitation": "ICLR.cc/2020/Conference/Paper623/-/Official_Comment", "content": {"title": "Addressing Tautologies in Practice", "comment": "This is an excellent point. It is possible for the model to learn tautologies such as $(0*x_1 + 0*x_2 + \\cdots + 0*1 = 0)$ depending on the structure of the template. However, we found in practice that given random nonzero weight initializations, the model never converged to these degenerate solutions on loops in the evaluation.\n\nIn our more recent work in PLDI 2020, Learning Nonlinear Loop Invariants with Gated Continuous Logic Networks (https://arxiv.org/pdf/2003.07959.pdf), we provide a more principled solution to learning complex invariants while avoiding degenerate cases through weight normalization and gating."}, "signatures": ["ICLR.cc/2020/Conference/Paper623/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper623/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["gabe@cs.columbia.edu", "justin.wong@columbia.edu", "jy3022@columbia.edu", "ronghui.gu@columbia.edu", "suman@cs.columbia.edu"], "title": "CLN2INV: Learning Loop Invariants with Continuous Logic Networks", "authors": ["Gabriel Ryan", "Justin Wong", "Jianan Yao", "Ronghui Gu", "Suman Jana"], "pdf": "/pdf/21d9bcdb4bdb58e7efa60497187b7b1bb3227a9e.pdf", "TL;DR": "We introduce the Continuous Logic Network (CLN), a novel neural architecture for automatically learning loop invariants and general SMT formulas.", "abstract": "Program verification offers a framework for ensuring program correctness and therefore systematically eliminating different classes of bugs. Inferring loop invariants is one of the main challenges behind automated verification of real-world programs which often contain many loops. In this paper, we present the Continuous Logic Network (CLN), a novel neural architecture for automatically learning loop invariants directly from program execution traces. Unlike existing neural networks, CLNs can learn precise and explicit representations of formulas in Satisfiability Modulo Theories (SMT)  for loop invariants from program execution traces. We develop a new sound and complete semantic mapping for assigning SMT formulas to continuous truth values that allows CLNs to be trained efficiently. We use CLNs to implement a new inference system for loop invariants, CLN2INV, that significantly outperforms existing approaches on the popular Code2Inv dataset. CLN2INV is the first tool to solve all 124 theoretically solvable problems in the Code2Inv dataset. Moreover, CLN2INV takes only 1.1 second on average for each problem, which is 40 times faster than existing approaches. We further demonstrate that CLN2INV can even learn 12 significantly more complex loop invariants than the ones required for the Code2Inv dataset.", "keywords": ["loop invariants", "deep learning", "logic learning"], "paperhash": "ryan|cln2inv_learning_loop_invariants_with_continuous_logic_networks", "_bibtex": "@inproceedings{\nRyan2020CLN2INV:,\ntitle={CLN2INV: Learning Loop Invariants with Continuous Logic Networks},\nauthor={Gabriel Ryan and Justin Wong and Jianan Yao and Ronghui Gu and Suman Jana},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJlfuTEtvB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/f8f2be2baf18a5f5e1901730a39a5ab9558edaf0.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJlfuTEtvB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper623/Authors", "ICLR.cc/2020/Conference/Paper623/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper623/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper623/Reviewers", "ICLR.cc/2020/Conference/Paper623/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper623/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper623/Authors|ICLR.cc/2020/Conference/Paper623/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504168706, "tmdate": 1576860533236, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper623/Authors", "ICLR.cc/2020/Conference/Paper623/Reviewers", "ICLR.cc/2020/Conference/Paper623/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper623/-/Official_Comment"}}}, {"id": "U4dhEnOuClb", "original": null, "number": 1, "cdate": 1590009395287, "ddate": null, "tcdate": 1590009395287, "tmdate": 1590009395287, "tddate": null, "forum": "HJlfuTEtvB", "replyto": "HJlfuTEtvB", "invitation": "ICLR.cc/2020/Conference/Paper623/-/Public_Comment", "content": {"title": "Tautologies and negative training examples?", "comment": "Thanks for this interesting paper. I have a question about the learning objective in Section 4.\n\nIn the training data, all traces will satisfy the invariant. Therefore, it seems that the learning method will like to generate tautologies; more formally if there exists weights W s.t. for *all* x, M(x; W, B, epsilon) = 1, this will be globally optimal for the learning objective. Is this correct?\n\nIf so, have the authors observed such solutions being produced in practice? It is possible that many of the SMT templates generated by Code2Inv do not have W that would make them tautologies, but the learning method could still \"overgenerate\" as it were, causing the end-to-end method to miss potential invariants."}, "signatures": ["~Charles_Sutton1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Charles_Sutton1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["gabe@cs.columbia.edu", "justin.wong@columbia.edu", "jy3022@columbia.edu", "ronghui.gu@columbia.edu", "suman@cs.columbia.edu"], "title": "CLN2INV: Learning Loop Invariants with Continuous Logic Networks", "authors": ["Gabriel Ryan", "Justin Wong", "Jianan Yao", "Ronghui Gu", "Suman Jana"], "pdf": "/pdf/21d9bcdb4bdb58e7efa60497187b7b1bb3227a9e.pdf", "TL;DR": "We introduce the Continuous Logic Network (CLN), a novel neural architecture for automatically learning loop invariants and general SMT formulas.", "abstract": "Program verification offers a framework for ensuring program correctness and therefore systematically eliminating different classes of bugs. Inferring loop invariants is one of the main challenges behind automated verification of real-world programs which often contain many loops. In this paper, we present the Continuous Logic Network (CLN), a novel neural architecture for automatically learning loop invariants directly from program execution traces. Unlike existing neural networks, CLNs can learn precise and explicit representations of formulas in Satisfiability Modulo Theories (SMT)  for loop invariants from program execution traces. We develop a new sound and complete semantic mapping for assigning SMT formulas to continuous truth values that allows CLNs to be trained efficiently. We use CLNs to implement a new inference system for loop invariants, CLN2INV, that significantly outperforms existing approaches on the popular Code2Inv dataset. CLN2INV is the first tool to solve all 124 theoretically solvable problems in the Code2Inv dataset. Moreover, CLN2INV takes only 1.1 second on average for each problem, which is 40 times faster than existing approaches. We further demonstrate that CLN2INV can even learn 12 significantly more complex loop invariants than the ones required for the Code2Inv dataset.", "keywords": ["loop invariants", "deep learning", "logic learning"], "paperhash": "ryan|cln2inv_learning_loop_invariants_with_continuous_logic_networks", "_bibtex": "@inproceedings{\nRyan2020CLN2INV:,\ntitle={CLN2INV: Learning Loop Invariants with Continuous Logic Networks},\nauthor={Gabriel Ryan and Justin Wong and Jianan Yao and Ronghui Gu and Suman Jana},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJlfuTEtvB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/f8f2be2baf18a5f5e1901730a39a5ab9558edaf0.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJlfuTEtvB", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504206496, "tmdate": 1576860566879, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper623/Authors", "ICLR.cc/2020/Conference/Paper623/Reviewers", "ICLR.cc/2020/Conference/Paper623/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper623/-/Public_Comment"}}}, {"id": "HJlfuTEtvB", "original": "Syxr4njvDH", "number": 623, "cdate": 1569439081601, "ddate": null, "tcdate": 1569439081601, "tmdate": 1583912019962, "tddate": null, "forum": "HJlfuTEtvB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["gabe@cs.columbia.edu", "justin.wong@columbia.edu", "jy3022@columbia.edu", "ronghui.gu@columbia.edu", "suman@cs.columbia.edu"], "title": "CLN2INV: Learning Loop Invariants with Continuous Logic Networks", "authors": ["Gabriel Ryan", "Justin Wong", "Jianan Yao", "Ronghui Gu", "Suman Jana"], "pdf": "/pdf/21d9bcdb4bdb58e7efa60497187b7b1bb3227a9e.pdf", "TL;DR": "We introduce the Continuous Logic Network (CLN), a novel neural architecture for automatically learning loop invariants and general SMT formulas.", "abstract": "Program verification offers a framework for ensuring program correctness and therefore systematically eliminating different classes of bugs. Inferring loop invariants is one of the main challenges behind automated verification of real-world programs which often contain many loops. In this paper, we present the Continuous Logic Network (CLN), a novel neural architecture for automatically learning loop invariants directly from program execution traces. Unlike existing neural networks, CLNs can learn precise and explicit representations of formulas in Satisfiability Modulo Theories (SMT)  for loop invariants from program execution traces. We develop a new sound and complete semantic mapping for assigning SMT formulas to continuous truth values that allows CLNs to be trained efficiently. We use CLNs to implement a new inference system for loop invariants, CLN2INV, that significantly outperforms existing approaches on the popular Code2Inv dataset. CLN2INV is the first tool to solve all 124 theoretically solvable problems in the Code2Inv dataset. Moreover, CLN2INV takes only 1.1 second on average for each problem, which is 40 times faster than existing approaches. We further demonstrate that CLN2INV can even learn 12 significantly more complex loop invariants than the ones required for the Code2Inv dataset.", "keywords": ["loop invariants", "deep learning", "logic learning"], "paperhash": "ryan|cln2inv_learning_loop_invariants_with_continuous_logic_networks", "_bibtex": "@inproceedings{\nRyan2020CLN2INV:,\ntitle={CLN2INV: Learning Loop Invariants with Continuous Logic Networks},\nauthor={Gabriel Ryan and Justin Wong and Jianan Yao and Ronghui Gu and Suman Jana},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJlfuTEtvB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/f8f2be2baf18a5f5e1901730a39a5ab9558edaf0.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "Gaqy29z7ls", "original": null, "number": 1, "cdate": 1576798701643, "ddate": null, "tcdate": 1576798701643, "tmdate": 1576800934360, "tddate": null, "forum": "HJlfuTEtvB", "replyto": "HJlfuTEtvB", "invitation": "ICLR.cc/2020/Conference/Paper623/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "This paper implements a novel architecture for inferring loop invariants in verification (though the paper bridges to compilers).  The idea is novel and the paper is well executed.  It is not the usual topic for ICLR, but not presents an important application of deep learning done well, and it has interesting implications for program synthesis.  Therefore, I recommend acceptance.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["gabe@cs.columbia.edu", "justin.wong@columbia.edu", "jy3022@columbia.edu", "ronghui.gu@columbia.edu", "suman@cs.columbia.edu"], "title": "CLN2INV: Learning Loop Invariants with Continuous Logic Networks", "authors": ["Gabriel Ryan", "Justin Wong", "Jianan Yao", "Ronghui Gu", "Suman Jana"], "pdf": "/pdf/21d9bcdb4bdb58e7efa60497187b7b1bb3227a9e.pdf", "TL;DR": "We introduce the Continuous Logic Network (CLN), a novel neural architecture for automatically learning loop invariants and general SMT formulas.", "abstract": "Program verification offers a framework for ensuring program correctness and therefore systematically eliminating different classes of bugs. Inferring loop invariants is one of the main challenges behind automated verification of real-world programs which often contain many loops. In this paper, we present the Continuous Logic Network (CLN), a novel neural architecture for automatically learning loop invariants directly from program execution traces. Unlike existing neural networks, CLNs can learn precise and explicit representations of formulas in Satisfiability Modulo Theories (SMT)  for loop invariants from program execution traces. We develop a new sound and complete semantic mapping for assigning SMT formulas to continuous truth values that allows CLNs to be trained efficiently. We use CLNs to implement a new inference system for loop invariants, CLN2INV, that significantly outperforms existing approaches on the popular Code2Inv dataset. CLN2INV is the first tool to solve all 124 theoretically solvable problems in the Code2Inv dataset. Moreover, CLN2INV takes only 1.1 second on average for each problem, which is 40 times faster than existing approaches. We further demonstrate that CLN2INV can even learn 12 significantly more complex loop invariants than the ones required for the Code2Inv dataset.", "keywords": ["loop invariants", "deep learning", "logic learning"], "paperhash": "ryan|cln2inv_learning_loop_invariants_with_continuous_logic_networks", "_bibtex": "@inproceedings{\nRyan2020CLN2INV:,\ntitle={CLN2INV: Learning Loop Invariants with Continuous Logic Networks},\nauthor={Gabriel Ryan and Justin Wong and Jianan Yao and Ronghui Gu and Suman Jana},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJlfuTEtvB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/f8f2be2baf18a5f5e1901730a39a5ab9558edaf0.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "HJlfuTEtvB", "replyto": "HJlfuTEtvB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795719113, "tmdate": 1576800269706, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper623/-/Decision"}}}, {"id": "HJgTNLJFsB", "original": null, "number": 15, "cdate": 1573611061308, "ddate": null, "tcdate": 1573611061308, "tmdate": 1573611061308, "tddate": null, "forum": "HJlfuTEtvB", "replyto": "SygB2FUk5r", "invitation": "ICLR.cc/2020/Conference/Paper623/-/Official_Comment", "content": {"title": "Updated Revision", "comment": "Thank you for taking the time to review our submission and providing thoughtful suggestions. We have made revisions to the submission based on your earlier feedback as follows: \n\n1.) As recommended, we have expanded out related works to specifically discuss relaxation efforts for satisfiability problems like Circuit-SAT. \n\n2.) Additionally, we have tabulated counterexamples to the invalid problems in Appendix H.\n\n3.) We have made our description of implementation more detailed in section 5, and added a detailed outline of our template generation algorithm (where the heuristics are incorporated) in Appendix F. \n\n4.) We have fixed the error in Figure 2.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper623/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper623/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["gabe@cs.columbia.edu", "justin.wong@columbia.edu", "jy3022@columbia.edu", "ronghui.gu@columbia.edu", "suman@cs.columbia.edu"], "title": "CLN2INV: Learning Loop Invariants with Continuous Logic Networks", "authors": ["Gabriel Ryan", "Justin Wong", "Jianan Yao", "Ronghui Gu", "Suman Jana"], "pdf": "/pdf/21d9bcdb4bdb58e7efa60497187b7b1bb3227a9e.pdf", "TL;DR": "We introduce the Continuous Logic Network (CLN), a novel neural architecture for automatically learning loop invariants and general SMT formulas.", "abstract": "Program verification offers a framework for ensuring program correctness and therefore systematically eliminating different classes of bugs. Inferring loop invariants is one of the main challenges behind automated verification of real-world programs which often contain many loops. In this paper, we present the Continuous Logic Network (CLN), a novel neural architecture for automatically learning loop invariants directly from program execution traces. Unlike existing neural networks, CLNs can learn precise and explicit representations of formulas in Satisfiability Modulo Theories (SMT)  for loop invariants from program execution traces. We develop a new sound and complete semantic mapping for assigning SMT formulas to continuous truth values that allows CLNs to be trained efficiently. We use CLNs to implement a new inference system for loop invariants, CLN2INV, that significantly outperforms existing approaches on the popular Code2Inv dataset. CLN2INV is the first tool to solve all 124 theoretically solvable problems in the Code2Inv dataset. Moreover, CLN2INV takes only 1.1 second on average for each problem, which is 40 times faster than existing approaches. We further demonstrate that CLN2INV can even learn 12 significantly more complex loop invariants than the ones required for the Code2Inv dataset.", "keywords": ["loop invariants", "deep learning", "logic learning"], "paperhash": "ryan|cln2inv_learning_loop_invariants_with_continuous_logic_networks", "_bibtex": "@inproceedings{\nRyan2020CLN2INV:,\ntitle={CLN2INV: Learning Loop Invariants with Continuous Logic Networks},\nauthor={Gabriel Ryan and Justin Wong and Jianan Yao and Ronghui Gu and Suman Jana},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJlfuTEtvB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/f8f2be2baf18a5f5e1901730a39a5ab9558edaf0.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJlfuTEtvB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper623/Authors", "ICLR.cc/2020/Conference/Paper623/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper623/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper623/Reviewers", "ICLR.cc/2020/Conference/Paper623/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper623/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper623/Authors|ICLR.cc/2020/Conference/Paper623/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504168706, "tmdate": 1576860533236, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper623/Authors", "ICLR.cc/2020/Conference/Paper623/Reviewers", "ICLR.cc/2020/Conference/Paper623/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper623/-/Official_Comment"}}}, {"id": "rklXASJtjB", "original": null, "number": 14, "cdate": 1573610955274, "ddate": null, "tcdate": 1573610955274, "tmdate": 1573610955274, "tddate": null, "forum": "HJlfuTEtvB", "replyto": "rklXNHgroB", "invitation": "ICLR.cc/2020/Conference/Paper623/-/Official_Comment", "content": {"title": "Updated Revision", "comment": "Thank you for taking the time to review our submission and providing thoughtful suggestions. We have uploaded a revised version based on your feedback with the following updates:\n\n`1.) To address the limitations of our approach, we have added a discussion in Section 6.2 Paragraph 3 on page 8, and Appendix J with examples that CLN2INV cannot currently solve.\n\n2.) As suggested, we simplified the t-norm section and moved more of our formal discussion on soundness and completeness from the body to the appendix in order to allocate more space for describing our implementation and experiments. \n\n3.) We have added details about the amount of time our system spends on each stage of its pipeline on the Code2Inv benchmark in Section 6.1 Paragraph 2 on page 7 with additional details in Appendix I, and also added details about the number of samples generated in Section 6.1 as a footnote on page 7.  We additionally added an example showing how training data generation is performed in Appendix F.\n\n4.) We have clarified why we consider CLNs to be a general purpose neural architecture in Section 4 Paragraph heading \u201cCLN Construction\u201d on page 5. \n\n5.) In Section 5 Paragraphs 1-4, pages 5-6, we added a detailed description of the preprocessing, training data generation, and the template generation process as requested to make the experiment reproducible. To ensure absolute clarity on our procedure, we go into further details on the template generation algorithm in Appendix F. \n\n6.) We have updated the related work on page 2 to address neural network relaxation techniques used previously for SAT/SMT, including fastSMT and the work on NeuroSAT for unsat-core detection by Selsam and Bj\u00f8rner.\n\n7.) We have corrected the LoopInvGen citation on page 2.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper623/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper623/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["gabe@cs.columbia.edu", "justin.wong@columbia.edu", "jy3022@columbia.edu", "ronghui.gu@columbia.edu", "suman@cs.columbia.edu"], "title": "CLN2INV: Learning Loop Invariants with Continuous Logic Networks", "authors": ["Gabriel Ryan", "Justin Wong", "Jianan Yao", "Ronghui Gu", "Suman Jana"], "pdf": "/pdf/21d9bcdb4bdb58e7efa60497187b7b1bb3227a9e.pdf", "TL;DR": "We introduce the Continuous Logic Network (CLN), a novel neural architecture for automatically learning loop invariants and general SMT formulas.", "abstract": "Program verification offers a framework for ensuring program correctness and therefore systematically eliminating different classes of bugs. Inferring loop invariants is one of the main challenges behind automated verification of real-world programs which often contain many loops. In this paper, we present the Continuous Logic Network (CLN), a novel neural architecture for automatically learning loop invariants directly from program execution traces. Unlike existing neural networks, CLNs can learn precise and explicit representations of formulas in Satisfiability Modulo Theories (SMT)  for loop invariants from program execution traces. We develop a new sound and complete semantic mapping for assigning SMT formulas to continuous truth values that allows CLNs to be trained efficiently. We use CLNs to implement a new inference system for loop invariants, CLN2INV, that significantly outperforms existing approaches on the popular Code2Inv dataset. CLN2INV is the first tool to solve all 124 theoretically solvable problems in the Code2Inv dataset. Moreover, CLN2INV takes only 1.1 second on average for each problem, which is 40 times faster than existing approaches. We further demonstrate that CLN2INV can even learn 12 significantly more complex loop invariants than the ones required for the Code2Inv dataset.", "keywords": ["loop invariants", "deep learning", "logic learning"], "paperhash": "ryan|cln2inv_learning_loop_invariants_with_continuous_logic_networks", "_bibtex": "@inproceedings{\nRyan2020CLN2INV:,\ntitle={CLN2INV: Learning Loop Invariants with Continuous Logic Networks},\nauthor={Gabriel Ryan and Justin Wong and Jianan Yao and Ronghui Gu and Suman Jana},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJlfuTEtvB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/f8f2be2baf18a5f5e1901730a39a5ab9558edaf0.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJlfuTEtvB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper623/Authors", "ICLR.cc/2020/Conference/Paper623/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper623/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper623/Reviewers", "ICLR.cc/2020/Conference/Paper623/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper623/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper623/Authors|ICLR.cc/2020/Conference/Paper623/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504168706, "tmdate": 1576860533236, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper623/Authors", "ICLR.cc/2020/Conference/Paper623/Reviewers", "ICLR.cc/2020/Conference/Paper623/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper623/-/Official_Comment"}}}, {"id": "rJeB9uhBir", "original": null, "number": 13, "cdate": 1573402764630, "ddate": null, "tcdate": 1573402764630, "tmdate": 1573402764630, "tddate": null, "forum": "HJlfuTEtvB", "replyto": "rklXNHgroB", "invitation": "ICLR.cc/2020/Conference/Paper623/-/Official_Comment", "content": {"title": "Response to Concern about Data Generation Time", "comment": "Thank you for your quick response! Just a quick clarification about your main concern.\nThe reported numbers *already include* the data generation time. The data generation (sampling) procedure for the C programs in the code2inv dataset is very fast\u2014it takes on average 1.9 milliseconds and generates 1041 samples. The longest sampling time for a benchmark program is 11.0 milliseconds to generate 6171 samples. Also, just to make sure that we are on the same page, note that our system requires no pre-training and infers the loop invariant for each program directly based on the corresponding sampled data. \n\nWe will provide full details of our sampling procedure and a breakdown of the time spent on each stage of the pipeline in the updated version.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper623/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper623/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["gabe@cs.columbia.edu", "justin.wong@columbia.edu", "jy3022@columbia.edu", "ronghui.gu@columbia.edu", "suman@cs.columbia.edu"], "title": "CLN2INV: Learning Loop Invariants with Continuous Logic Networks", "authors": ["Gabriel Ryan", "Justin Wong", "Jianan Yao", "Ronghui Gu", "Suman Jana"], "pdf": "/pdf/21d9bcdb4bdb58e7efa60497187b7b1bb3227a9e.pdf", "TL;DR": "We introduce the Continuous Logic Network (CLN), a novel neural architecture for automatically learning loop invariants and general SMT formulas.", "abstract": "Program verification offers a framework for ensuring program correctness and therefore systematically eliminating different classes of bugs. Inferring loop invariants is one of the main challenges behind automated verification of real-world programs which often contain many loops. In this paper, we present the Continuous Logic Network (CLN), a novel neural architecture for automatically learning loop invariants directly from program execution traces. Unlike existing neural networks, CLNs can learn precise and explicit representations of formulas in Satisfiability Modulo Theories (SMT)  for loop invariants from program execution traces. We develop a new sound and complete semantic mapping for assigning SMT formulas to continuous truth values that allows CLNs to be trained efficiently. We use CLNs to implement a new inference system for loop invariants, CLN2INV, that significantly outperforms existing approaches on the popular Code2Inv dataset. CLN2INV is the first tool to solve all 124 theoretically solvable problems in the Code2Inv dataset. Moreover, CLN2INV takes only 1.1 second on average for each problem, which is 40 times faster than existing approaches. We further demonstrate that CLN2INV can even learn 12 significantly more complex loop invariants than the ones required for the Code2Inv dataset.", "keywords": ["loop invariants", "deep learning", "logic learning"], "paperhash": "ryan|cln2inv_learning_loop_invariants_with_continuous_logic_networks", "_bibtex": "@inproceedings{\nRyan2020CLN2INV:,\ntitle={CLN2INV: Learning Loop Invariants with Continuous Logic Networks},\nauthor={Gabriel Ryan and Justin Wong and Jianan Yao and Ronghui Gu and Suman Jana},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJlfuTEtvB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/f8f2be2baf18a5f5e1901730a39a5ab9558edaf0.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJlfuTEtvB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper623/Authors", "ICLR.cc/2020/Conference/Paper623/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper623/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper623/Reviewers", "ICLR.cc/2020/Conference/Paper623/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper623/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper623/Authors|ICLR.cc/2020/Conference/Paper623/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504168706, "tmdate": 1576860533236, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper623/Authors", "ICLR.cc/2020/Conference/Paper623/Reviewers", "ICLR.cc/2020/Conference/Paper623/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper623/-/Official_Comment"}}}, {"id": "rklXNHgroB", "original": null, "number": 12, "cdate": 1573352746623, "ddate": null, "tcdate": 1573352746623, "tmdate": 1573352746623, "tddate": null, "forum": "HJlfuTEtvB", "replyto": "r1l631xroB", "invitation": "ICLR.cc/2020/Conference/Paper623/-/Official_Comment", "content": {"title": "Response to Clarification", "comment": "I'm bringing up the t-norms since I want more detail in Section 5 and since I want your paper to stay within the page limits I'm suggesting some of the content on t-norms can be moved into the Appendix or honestly simply removed.\n\nI would appreciate if as much of the details can be in the paper body but use your best judgement about what goes where.\n\nAlso to help save time, I am deeply concerned that if a new dataset needs to be created for each benchmark problem that runtime should really reflect that. Otherwise it's a bit deceptive to say solving the problem only takes a second. I may be misunderstanding this so I'm repeating this is a major concern of mine."}, "signatures": ["ICLR.cc/2020/Conference/Paper623/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper623/AnonReviewer4", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["gabe@cs.columbia.edu", "justin.wong@columbia.edu", "jy3022@columbia.edu", "ronghui.gu@columbia.edu", "suman@cs.columbia.edu"], "title": "CLN2INV: Learning Loop Invariants with Continuous Logic Networks", "authors": ["Gabriel Ryan", "Justin Wong", "Jianan Yao", "Ronghui Gu", "Suman Jana"], "pdf": "/pdf/21d9bcdb4bdb58e7efa60497187b7b1bb3227a9e.pdf", "TL;DR": "We introduce the Continuous Logic Network (CLN), a novel neural architecture for automatically learning loop invariants and general SMT formulas.", "abstract": "Program verification offers a framework for ensuring program correctness and therefore systematically eliminating different classes of bugs. Inferring loop invariants is one of the main challenges behind automated verification of real-world programs which often contain many loops. In this paper, we present the Continuous Logic Network (CLN), a novel neural architecture for automatically learning loop invariants directly from program execution traces. Unlike existing neural networks, CLNs can learn precise and explicit representations of formulas in Satisfiability Modulo Theories (SMT)  for loop invariants from program execution traces. We develop a new sound and complete semantic mapping for assigning SMT formulas to continuous truth values that allows CLNs to be trained efficiently. We use CLNs to implement a new inference system for loop invariants, CLN2INV, that significantly outperforms existing approaches on the popular Code2Inv dataset. CLN2INV is the first tool to solve all 124 theoretically solvable problems in the Code2Inv dataset. Moreover, CLN2INV takes only 1.1 second on average for each problem, which is 40 times faster than existing approaches. We further demonstrate that CLN2INV can even learn 12 significantly more complex loop invariants than the ones required for the Code2Inv dataset.", "keywords": ["loop invariants", "deep learning", "logic learning"], "paperhash": "ryan|cln2inv_learning_loop_invariants_with_continuous_logic_networks", "_bibtex": "@inproceedings{\nRyan2020CLN2INV:,\ntitle={CLN2INV: Learning Loop Invariants with Continuous Logic Networks},\nauthor={Gabriel Ryan and Justin Wong and Jianan Yao and Ronghui Gu and Suman Jana},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJlfuTEtvB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/f8f2be2baf18a5f5e1901730a39a5ab9558edaf0.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJlfuTEtvB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper623/Authors", "ICLR.cc/2020/Conference/Paper623/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper623/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper623/Reviewers", "ICLR.cc/2020/Conference/Paper623/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper623/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper623/Authors|ICLR.cc/2020/Conference/Paper623/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504168706, "tmdate": 1576860533236, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper623/Authors", "ICLR.cc/2020/Conference/Paper623/Reviewers", "ICLR.cc/2020/Conference/Paper623/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper623/-/Official_Comment"}}}, {"id": "r1l631xroB", "original": null, "number": 11, "cdate": 1573351349401, "ddate": null, "tcdate": 1573351349401, "tmdate": 1573351526059, "tddate": null, "forum": "HJlfuTEtvB", "replyto": "rklrlliQjS", "invitation": "ICLR.cc/2020/Conference/Paper623/-/Official_Comment", "content": {"title": "Clarifying Questions", "comment": "Thanks for taking the time to review our submission and providing thoughtful suggestions. We are working on a revision that incorporates all of your suggested edits, and will provide a detailed response with listing the changes when it is complete.\n\nWe have two clarifying questions with regard to the requested edits:\n\n1.) We want to double check your concern about the discussion of t-norms and soundness/completeness taking too much space is with regard to the paper body (and not the proofs in the appendix). If so, we can certainly consolidate those sections and move more details to the appendix.\n\n2.) In order to provide sufficient details about the implementation and experiments we are considering putting full descriptions in the appendix (though we will fit as much as possible into the paper body). Will this be ok?\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper623/Authors"], "readers": ["ICLR.cc/2020/Conference/Paper623/Authors", "ICLR.cc/2020/Conference/Paper623/Reviewers", "everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper623/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["gabe@cs.columbia.edu", "justin.wong@columbia.edu", "jy3022@columbia.edu", "ronghui.gu@columbia.edu", "suman@cs.columbia.edu"], "title": "CLN2INV: Learning Loop Invariants with Continuous Logic Networks", "authors": ["Gabriel Ryan", "Justin Wong", "Jianan Yao", "Ronghui Gu", "Suman Jana"], "pdf": "/pdf/21d9bcdb4bdb58e7efa60497187b7b1bb3227a9e.pdf", "TL;DR": "We introduce the Continuous Logic Network (CLN), a novel neural architecture for automatically learning loop invariants and general SMT formulas.", "abstract": "Program verification offers a framework for ensuring program correctness and therefore systematically eliminating different classes of bugs. Inferring loop invariants is one of the main challenges behind automated verification of real-world programs which often contain many loops. In this paper, we present the Continuous Logic Network (CLN), a novel neural architecture for automatically learning loop invariants directly from program execution traces. Unlike existing neural networks, CLNs can learn precise and explicit representations of formulas in Satisfiability Modulo Theories (SMT)  for loop invariants from program execution traces. We develop a new sound and complete semantic mapping for assigning SMT formulas to continuous truth values that allows CLNs to be trained efficiently. We use CLNs to implement a new inference system for loop invariants, CLN2INV, that significantly outperforms existing approaches on the popular Code2Inv dataset. CLN2INV is the first tool to solve all 124 theoretically solvable problems in the Code2Inv dataset. Moreover, CLN2INV takes only 1.1 second on average for each problem, which is 40 times faster than existing approaches. We further demonstrate that CLN2INV can even learn 12 significantly more complex loop invariants than the ones required for the Code2Inv dataset.", "keywords": ["loop invariants", "deep learning", "logic learning"], "paperhash": "ryan|cln2inv_learning_loop_invariants_with_continuous_logic_networks", "_bibtex": "@inproceedings{\nRyan2020CLN2INV:,\ntitle={CLN2INV: Learning Loop Invariants with Continuous Logic Networks},\nauthor={Gabriel Ryan and Justin Wong and Jianan Yao and Ronghui Gu and Suman Jana},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJlfuTEtvB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/f8f2be2baf18a5f5e1901730a39a5ab9558edaf0.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJlfuTEtvB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper623/Authors", "ICLR.cc/2020/Conference/Paper623/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper623/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper623/Reviewers", "ICLR.cc/2020/Conference/Paper623/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper623/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper623/Authors|ICLR.cc/2020/Conference/Paper623/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504168706, "tmdate": 1576860533236, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper623/Authors", "ICLR.cc/2020/Conference/Paper623/Reviewers", "ICLR.cc/2020/Conference/Paper623/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper623/-/Official_Comment"}}}, {"id": "rklrlliQjS", "original": null, "number": 3, "cdate": 1573265388551, "ddate": null, "tcdate": 1573265388551, "tmdate": 1573265388551, "tddate": null, "forum": "HJlfuTEtvB", "replyto": "HJlfuTEtvB", "invitation": "ICLR.cc/2020/Conference/Paper623/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #4", "review": "Summary:\n\nThis paper introduces a novel way to find loop invariants for a\nprogram. The loop invariants are expressed as SMT formulas. A\ncontinuous relaxation of an SMT solver is introduces mapping\nevery SMT formula onto a truth value \\in [0, 1]. This relaxation\ncalled a continuous semantic mapping is decided such that every\ntrue formula has a higher value than every false formula. This\nallows an invariant to be learned.\n\nNovelty and Significance:\n\nThis work is interesting and although authors do seem to be\noutside of the community, I firmly believe is appropriate for\nICLR. If the claims made by the paper are true they constitute\na significant contribution to the field of program synthesis\nand program analysis.\n\nTechnical Quality:\n\nThe evaluation was fairly thorough, but the paper can be\nstrengthened massively with a few small changes and additions.\nIt might be more helpful if there was a sense of how many problems\nnone of the systems can do and how complicated of a program can\nthis system extract a loop invariant from. Why were was it these\nparticular 12 that work? Are there examples that don't?\n\nI don't know why all this time is spent on t-norms when behavior\non them is fairly similar and the simplest norm works best.\n\nLots of details are missing in this paper. How much training data\nwas generated? How long did it take? Does training data need to\nbe generated for each example? If so is that included in the\nruntime for Figure 3?\n\nThe paper talks about neural architecture, but all I see is\neffectively a curve-fitting task for some template. This feels\ndifferent from the code2inv paper where a program can be fed\ninto the system and the pretrained model emits the invariant.\n\nClarity:\n\nNot enough of this paper concentrates on the novel aspects of the\napproach. Section 5 discusses template generation, but not in\nenough detail that I would be able to replicate this work. I\ncould also not find enough details in the Appendix.\n\nI don't know why vital page space is spent on defining completeness\nand soundness.\n\n\nPossibly related work as relaxations to SAT/SMT solvers do exist in the literature.\n\nGuiding High-Performance SAT Solvers with Unsat-Core Predictions\nhttps://arxiv.org/abs/1903.04671\n\nLearning to Solve SMT Formulas\nhttps://papers.nips.cc/paper/8233-learning-to-solve-smt-formulas.pdf\n\nNotes:\n\nYou cite Si et al. for LoopInvGen when you should be citing Pathi\nand Millstein in the third paragraph on page 2.\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper623/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper623/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["gabe@cs.columbia.edu", "justin.wong@columbia.edu", "jy3022@columbia.edu", "ronghui.gu@columbia.edu", "suman@cs.columbia.edu"], "title": "CLN2INV: Learning Loop Invariants with Continuous Logic Networks", "authors": ["Gabriel Ryan", "Justin Wong", "Jianan Yao", "Ronghui Gu", "Suman Jana"], "pdf": "/pdf/21d9bcdb4bdb58e7efa60497187b7b1bb3227a9e.pdf", "TL;DR": "We introduce the Continuous Logic Network (CLN), a novel neural architecture for automatically learning loop invariants and general SMT formulas.", "abstract": "Program verification offers a framework for ensuring program correctness and therefore systematically eliminating different classes of bugs. Inferring loop invariants is one of the main challenges behind automated verification of real-world programs which often contain many loops. In this paper, we present the Continuous Logic Network (CLN), a novel neural architecture for automatically learning loop invariants directly from program execution traces. Unlike existing neural networks, CLNs can learn precise and explicit representations of formulas in Satisfiability Modulo Theories (SMT)  for loop invariants from program execution traces. We develop a new sound and complete semantic mapping for assigning SMT formulas to continuous truth values that allows CLNs to be trained efficiently. We use CLNs to implement a new inference system for loop invariants, CLN2INV, that significantly outperforms existing approaches on the popular Code2Inv dataset. CLN2INV is the first tool to solve all 124 theoretically solvable problems in the Code2Inv dataset. Moreover, CLN2INV takes only 1.1 second on average for each problem, which is 40 times faster than existing approaches. We further demonstrate that CLN2INV can even learn 12 significantly more complex loop invariants than the ones required for the Code2Inv dataset.", "keywords": ["loop invariants", "deep learning", "logic learning"], "paperhash": "ryan|cln2inv_learning_loop_invariants_with_continuous_logic_networks", "_bibtex": "@inproceedings{\nRyan2020CLN2INV:,\ntitle={CLN2INV: Learning Loop Invariants with Continuous Logic Networks},\nauthor={Gabriel Ryan and Justin Wong and Jianan Yao and Ronghui Gu and Suman Jana},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJlfuTEtvB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/f8f2be2baf18a5f5e1901730a39a5ab9558edaf0.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJlfuTEtvB", "replyto": "HJlfuTEtvB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper623/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper623/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576380329366, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper623/Reviewers"], "noninvitees": [], "tcdate": 1570237749448, "tmdate": 1576380329382, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper623/-/Official_Review"}}}, {"id": "r1gfEdsysr", "original": null, "number": 6, "cdate": 1573005353549, "ddate": null, "tcdate": 1573005353549, "tmdate": 1573005353549, "tddate": null, "forum": "HJlfuTEtvB", "replyto": "SygB2FUk5r", "invitation": "ICLR.cc/2020/Conference/Paper623/-/Official_Comment", "content": {"title": "Author Response to Review 2", "comment": "We appreciate the thoughtful review and the detailed and constructive feedback!\n\nWe agree that our work is related to OR work in relaxed SAT solving methods, and will extend our related work to address these approaches. \n\nWe will specify the unsolvable problems and add counterexamples in the appendix as recommended.\n\nThe dataset we use in our evaluation is from a state-of-the-art approach to learning loop invariants that was a NeurIPS 2018 spotlight (Si et al., 2018). Our current system was designed to operate on C programs, but we are working to extend it to work with problems that are described exclusively in an SMT formulation in order to operate on the Sygus competition benchmark, which contains 829 problems. \n\nWe will also correct the error in Figure 2. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper623/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper623/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["gabe@cs.columbia.edu", "justin.wong@columbia.edu", "jy3022@columbia.edu", "ronghui.gu@columbia.edu", "suman@cs.columbia.edu"], "title": "CLN2INV: Learning Loop Invariants with Continuous Logic Networks", "authors": ["Gabriel Ryan", "Justin Wong", "Jianan Yao", "Ronghui Gu", "Suman Jana"], "pdf": "/pdf/21d9bcdb4bdb58e7efa60497187b7b1bb3227a9e.pdf", "TL;DR": "We introduce the Continuous Logic Network (CLN), a novel neural architecture for automatically learning loop invariants and general SMT formulas.", "abstract": "Program verification offers a framework for ensuring program correctness and therefore systematically eliminating different classes of bugs. Inferring loop invariants is one of the main challenges behind automated verification of real-world programs which often contain many loops. In this paper, we present the Continuous Logic Network (CLN), a novel neural architecture for automatically learning loop invariants directly from program execution traces. Unlike existing neural networks, CLNs can learn precise and explicit representations of formulas in Satisfiability Modulo Theories (SMT)  for loop invariants from program execution traces. We develop a new sound and complete semantic mapping for assigning SMT formulas to continuous truth values that allows CLNs to be trained efficiently. We use CLNs to implement a new inference system for loop invariants, CLN2INV, that significantly outperforms existing approaches on the popular Code2Inv dataset. CLN2INV is the first tool to solve all 124 theoretically solvable problems in the Code2Inv dataset. Moreover, CLN2INV takes only 1.1 second on average for each problem, which is 40 times faster than existing approaches. We further demonstrate that CLN2INV can even learn 12 significantly more complex loop invariants than the ones required for the Code2Inv dataset.", "keywords": ["loop invariants", "deep learning", "logic learning"], "paperhash": "ryan|cln2inv_learning_loop_invariants_with_continuous_logic_networks", "_bibtex": "@inproceedings{\nRyan2020CLN2INV:,\ntitle={CLN2INV: Learning Loop Invariants with Continuous Logic Networks},\nauthor={Gabriel Ryan and Justin Wong and Jianan Yao and Ronghui Gu and Suman Jana},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJlfuTEtvB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/f8f2be2baf18a5f5e1901730a39a5ab9558edaf0.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJlfuTEtvB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper623/Authors", "ICLR.cc/2020/Conference/Paper623/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper623/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper623/Reviewers", "ICLR.cc/2020/Conference/Paper623/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper623/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper623/Authors|ICLR.cc/2020/Conference/Paper623/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504168706, "tmdate": 1576860533236, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper623/Authors", "ICLR.cc/2020/Conference/Paper623/Reviewers", "ICLR.cc/2020/Conference/Paper623/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper623/-/Official_Comment"}}}, {"id": "SygB2FUk5r", "original": null, "number": 1, "cdate": 1571936684661, "ddate": null, "tcdate": 1571936684661, "tmdate": 1572972572283, "tddate": null, "forum": "HJlfuTEtvB", "replyto": "HJlfuTEtvB", "invitation": "ICLR.cc/2020/Conference/Paper623/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The domain is loop invariant detection, in the static program analysis space. Loop invariants hold before, during, and after loop execution, and can be useful for compiler optimizations and/or correctness checking. \nThe paper explains Basic Fuzzy Logic and then uses it to introduce Continuous Satisfiability Modulo Theories, including proposed continuous mappings for inequalities (>, >=), negations, equalities, and requirements of t-norms to be useful for the relaxated optimization proposed. This Continuous Logic Network is then optimized to provide invariant proposals to Z3, an SMT solver. The whole system is used to solve the entire Code2Inv benchmark set, in substantially faster time and with fewer proposals to Z3 than comparable previous approaches. Ablations are provided which study the t-norm used (3 options considered), and another which uses heuristics only with no training/optimization to make static proposals to Z3. \n\nOn the whole, I like the presentation and the thinking here, and think it will be interesting to folks in the field, possibly spurring on further thinking in compilers, program synthesis, constrained optimization, etc, so recommend accepting. \n\nRelaxed representations of satisfiability problems seems like something people have thought about in OR for some time, so I wonder if there is a missing part of the literature survey. A cursory glance turns up https://openreview.net/forum?id=BJxgz2R9t7 \n\nInterestingly, the heuristics do quite well, which calls into question how hard the dataset is, and how competitive the preceding works really were. Since these heuristics seem to be an important contributor to this approach, I think they deserve further discussion in the appendix, and/or source code should be released.\n\n9 problems from the dataset are rejected as invalid. Please identify these in an appendix, and provide the counterexamples.\n\nThe dataset used here is quite small, and it seems like only ~30 of the problems are \"hard\" in requiring beyond-heuristic complexity. Couldn't the SyGuS tools be used to generate a much larger test set?\n\nIn fig 2, the model (x) doesn't match the template/invariant \\/."}, "signatures": ["ICLR.cc/2020/Conference/Paper623/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper623/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["gabe@cs.columbia.edu", "justin.wong@columbia.edu", "jy3022@columbia.edu", "ronghui.gu@columbia.edu", "suman@cs.columbia.edu"], "title": "CLN2INV: Learning Loop Invariants with Continuous Logic Networks", "authors": ["Gabriel Ryan", "Justin Wong", "Jianan Yao", "Ronghui Gu", "Suman Jana"], "pdf": "/pdf/21d9bcdb4bdb58e7efa60497187b7b1bb3227a9e.pdf", "TL;DR": "We introduce the Continuous Logic Network (CLN), a novel neural architecture for automatically learning loop invariants and general SMT formulas.", "abstract": "Program verification offers a framework for ensuring program correctness and therefore systematically eliminating different classes of bugs. Inferring loop invariants is one of the main challenges behind automated verification of real-world programs which often contain many loops. In this paper, we present the Continuous Logic Network (CLN), a novel neural architecture for automatically learning loop invariants directly from program execution traces. Unlike existing neural networks, CLNs can learn precise and explicit representations of formulas in Satisfiability Modulo Theories (SMT)  for loop invariants from program execution traces. We develop a new sound and complete semantic mapping for assigning SMT formulas to continuous truth values that allows CLNs to be trained efficiently. We use CLNs to implement a new inference system for loop invariants, CLN2INV, that significantly outperforms existing approaches on the popular Code2Inv dataset. CLN2INV is the first tool to solve all 124 theoretically solvable problems in the Code2Inv dataset. Moreover, CLN2INV takes only 1.1 second on average for each problem, which is 40 times faster than existing approaches. We further demonstrate that CLN2INV can even learn 12 significantly more complex loop invariants than the ones required for the Code2Inv dataset.", "keywords": ["loop invariants", "deep learning", "logic learning"], "paperhash": "ryan|cln2inv_learning_loop_invariants_with_continuous_logic_networks", "_bibtex": "@inproceedings{\nRyan2020CLN2INV:,\ntitle={CLN2INV: Learning Loop Invariants with Continuous Logic Networks},\nauthor={Gabriel Ryan and Justin Wong and Jianan Yao and Ronghui Gu and Suman Jana},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJlfuTEtvB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/f8f2be2baf18a5f5e1901730a39a5ab9558edaf0.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJlfuTEtvB", "replyto": "HJlfuTEtvB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper623/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper623/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576380329366, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper623/Reviewers"], "noninvitees": [], "tcdate": 1570237749448, "tmdate": 1576380329382, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper623/-/Official_Review"}}}], "count": 12}