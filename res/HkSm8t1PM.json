{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1528124448143, "tcdate": 1518470684802, "number": 286, "cdate": 1518470684802, "id": "HkSm8t1PM", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "HkSm8t1PM", "signatures": ["~Elliot_Creager1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "Gradient-based Optimization of Neural Network Architecture", "abstract": "Neural networks can learn relevant features from data, but their predictive accuracy and propensity to overfit are sensitive to the values of the discrete hyperparameters that specify the network architecture (number of hidden layers, number of units per layer, etc.). Previous work optimized these hyperparmeters via grid search, random search, and black box optimization techniques such as Bayesian optimization. Bolstered by recent advances in gradient-based optimization of discrete stochastic objectives, we instead propose to directly model a distribution over possible architectures and use variational optimization to jointly optimize the network architecture and weights in one training pass. We discuss an implementation of this approach that estimates gradients via the Concrete relaxation, and show that it finds compact and accurate architectures for convolutional neural networks applied to the CIFAR10 and CIFAR100 datasets.", "paperhash": "grathwohl|gradientbased_optimization_of_neural_network_architecture", "keywords": ["Architecture search", "Variational optimization", "Deep learning", "Model optimization"], "_bibtex": "@misc{\n  grathwohl2018gradient-based,\n  title={Gradient-based Optimization of Neural Network Architecture},\n  author={Will Grathwohl and Elliot Creager and Seyed Kamyar Seyed Ghasemipour and Richard Zemel},\n  year={2018},\n  url={https://openreview.net/forum?id=HkSm8t1PM}\n}", "authorids": ["wgrathwohl@cs.toronto.edu", "creager@cs.toronto.edu", "kamyar@cs.toronto.edu", "zemel@cs.toronto.edu"], "authors": ["Will Grathwohl", "Elliot Creager", "Seyed Kamyar Seyed Ghasemipour", "Richard Zemel"], "TL;DR": "In a variational optimization framework, we directly model depth and number of hidden units as discrete network parameters and jointly optimize them alongside the weights in a single training run.", "pdf": "/pdf/c9f80321d148d12fd5ff49863d5d399ca92689cb.pdf"}, "nonreaders": [], "details": {"replyCount": 3, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582993436, "tcdate": 1519417453789, "number": 1, "cdate": 1519417453789, "id": "ByI_OeAwM", "invitation": "ICLR.cc/2018/Workshop/-/Paper286/Official_Review", "forum": "HkSm8t1PM", "replyto": "HkSm8t1PM", "signatures": ["ICLR.cc/2018/Workshop/Paper286/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper286/AnonReviewer2"], "content": {"title": "Interesting research direction ", "rating": "9: Top 15% of accepted papers, strong accept", "review": "The proposed approach for inferring key configuration aspects of deep networks is novel and sound. Even though the provided results correspond to very early stage developments, they are worth of presentation in ICLR workshops. ", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Gradient-based Optimization of Neural Network Architecture", "abstract": "Neural networks can learn relevant features from data, but their predictive accuracy and propensity to overfit are sensitive to the values of the discrete hyperparameters that specify the network architecture (number of hidden layers, number of units per layer, etc.). Previous work optimized these hyperparmeters via grid search, random search, and black box optimization techniques such as Bayesian optimization. Bolstered by recent advances in gradient-based optimization of discrete stochastic objectives, we instead propose to directly model a distribution over possible architectures and use variational optimization to jointly optimize the network architecture and weights in one training pass. We discuss an implementation of this approach that estimates gradients via the Concrete relaxation, and show that it finds compact and accurate architectures for convolutional neural networks applied to the CIFAR10 and CIFAR100 datasets.", "paperhash": "grathwohl|gradientbased_optimization_of_neural_network_architecture", "keywords": ["Architecture search", "Variational optimization", "Deep learning", "Model optimization"], "_bibtex": "@misc{\n  grathwohl2018gradient-based,\n  title={Gradient-based Optimization of Neural Network Architecture},\n  author={Will Grathwohl and Elliot Creager and Seyed Kamyar Seyed Ghasemipour and Richard Zemel},\n  year={2018},\n  url={https://openreview.net/forum?id=HkSm8t1PM}\n}", "authorids": ["wgrathwohl@cs.toronto.edu", "creager@cs.toronto.edu", "kamyar@cs.toronto.edu", "zemel@cs.toronto.edu"], "authors": ["Will Grathwohl", "Elliot Creager", "Seyed Kamyar Seyed Ghasemipour", "Richard Zemel"], "TL;DR": "In a variational optimization framework, we directly model depth and number of hidden units as discrete network parameters and jointly optimize them alongside the weights in a single training run.", "pdf": "/pdf/c9f80321d148d12fd5ff49863d5d399ca92689cb.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582993186, "id": "ICLR.cc/2018/Workshop/-/Paper286/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper286/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper286/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper286/AnonReviewer1"], "reply": {"forum": "HkSm8t1PM", "replyto": "HkSm8t1PM", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper286/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper286/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582993186}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582628220, "tcdate": 1520822985015, "number": 2, "cdate": 1520822985015, "id": "HyWA5DmFM", "invitation": "ICLR.cc/2018/Workshop/-/Paper286/Official_Review", "forum": "HkSm8t1PM", "replyto": "HkSm8t1PM", "signatures": ["ICLR.cc/2018/Workshop/Paper286/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper286/AnonReviewer1"], "content": {"title": "Interesting small scale work on learning neural network architectures", "rating": "6: Marginally above acceptance threshold", "review": "This paper looks at formulating a learning to learn method for selecting the depth and width of neural networks. This problem is first formulated as a variational optimization problem: to pick an architecture A to minimize the loss function L(w, A), we instead look at minimizing this in expectation, over a distribution q over A. To pick depth, a linear classifier is trained for each layer (there is a missing cite to the 'linear classifier probes' paper) and element wise multiplied with the sample from the distribution q_phi. Width is done by having masks of form [1,...,1,0...,0] instead of the linear classifiers. \n\nThe setting of the problem is introduced clearly, but some of the details could use additional clarification. For example, in equation (5), if d is a one hot vector, then only one of the z_i's contributes to \\hat{z}, which is a little misleading with the summation term.\n\nThis work is a nice example of a simple way to study learning to learn, and would be well suited to the ICLR workshop. However, there are important directions to address before becoming a full fledged paper -- e.g. how expressive are the q_\\phi, is the regularization function really necessary? \n\nMost importantly, can this method find architectures that are nothing like the starting point? Currently, this just appears to apply small changes to the starting architecture. It's not bad for a workshop submission, but much more would have to be done for a conference paper.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Gradient-based Optimization of Neural Network Architecture", "abstract": "Neural networks can learn relevant features from data, but their predictive accuracy and propensity to overfit are sensitive to the values of the discrete hyperparameters that specify the network architecture (number of hidden layers, number of units per layer, etc.). Previous work optimized these hyperparmeters via grid search, random search, and black box optimization techniques such as Bayesian optimization. Bolstered by recent advances in gradient-based optimization of discrete stochastic objectives, we instead propose to directly model a distribution over possible architectures and use variational optimization to jointly optimize the network architecture and weights in one training pass. We discuss an implementation of this approach that estimates gradients via the Concrete relaxation, and show that it finds compact and accurate architectures for convolutional neural networks applied to the CIFAR10 and CIFAR100 datasets.", "paperhash": "grathwohl|gradientbased_optimization_of_neural_network_architecture", "keywords": ["Architecture search", "Variational optimization", "Deep learning", "Model optimization"], "_bibtex": "@misc{\n  grathwohl2018gradient-based,\n  title={Gradient-based Optimization of Neural Network Architecture},\n  author={Will Grathwohl and Elliot Creager and Seyed Kamyar Seyed Ghasemipour and Richard Zemel},\n  year={2018},\n  url={https://openreview.net/forum?id=HkSm8t1PM}\n}", "authorids": ["wgrathwohl@cs.toronto.edu", "creager@cs.toronto.edu", "kamyar@cs.toronto.edu", "zemel@cs.toronto.edu"], "authors": ["Will Grathwohl", "Elliot Creager", "Seyed Kamyar Seyed Ghasemipour", "Richard Zemel"], "TL;DR": "In a variational optimization framework, we directly model depth and number of hidden units as discrete network parameters and jointly optimize them alongside the weights in a single training run.", "pdf": "/pdf/c9f80321d148d12fd5ff49863d5d399ca92689cb.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582993186, "id": "ICLR.cc/2018/Workshop/-/Paper286/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper286/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper286/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper286/AnonReviewer1"], "reply": {"forum": "HkSm8t1PM", "replyto": "HkSm8t1PM", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper286/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper286/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582993186}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521573548653, "tcdate": 1521573548653, "number": 24, "cdate": 1521573548305, "id": "rJSnRARYz", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "HkSm8t1PM", "replyto": "HkSm8t1PM", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Accept", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "Congratulations, your paper was accepted to the ICLR workshop."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Gradient-based Optimization of Neural Network Architecture", "abstract": "Neural networks can learn relevant features from data, but their predictive accuracy and propensity to overfit are sensitive to the values of the discrete hyperparameters that specify the network architecture (number of hidden layers, number of units per layer, etc.). Previous work optimized these hyperparmeters via grid search, random search, and black box optimization techniques such as Bayesian optimization. Bolstered by recent advances in gradient-based optimization of discrete stochastic objectives, we instead propose to directly model a distribution over possible architectures and use variational optimization to jointly optimize the network architecture and weights in one training pass. We discuss an implementation of this approach that estimates gradients via the Concrete relaxation, and show that it finds compact and accurate architectures for convolutional neural networks applied to the CIFAR10 and CIFAR100 datasets.", "paperhash": "grathwohl|gradientbased_optimization_of_neural_network_architecture", "keywords": ["Architecture search", "Variational optimization", "Deep learning", "Model optimization"], "_bibtex": "@misc{\n  grathwohl2018gradient-based,\n  title={Gradient-based Optimization of Neural Network Architecture},\n  author={Will Grathwohl and Elliot Creager and Seyed Kamyar Seyed Ghasemipour and Richard Zemel},\n  year={2018},\n  url={https://openreview.net/forum?id=HkSm8t1PM}\n}", "authorids": ["wgrathwohl@cs.toronto.edu", "creager@cs.toronto.edu", "kamyar@cs.toronto.edu", "zemel@cs.toronto.edu"], "authors": ["Will Grathwohl", "Elliot Creager", "Seyed Kamyar Seyed Ghasemipour", "Richard Zemel"], "TL;DR": "In a variational optimization framework, we directly model depth and number of hidden units as discrete network parameters and jointly optimize them alongside the weights in a single training run.", "pdf": "/pdf/c9f80321d148d12fd5ff49863d5d399ca92689cb.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}], "count": 4}