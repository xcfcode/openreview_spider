{"notes": [{"id": "B1x33sC9KQ", "original": "HJgR0CYqtm", "number": 749, "cdate": 1538087860259, "ddate": null, "tcdate": 1538087860259, "tmdate": 1545355429607, "tddate": null, "forum": "B1x33sC9KQ", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "ACIQ: Analytical Clipping for Integer Quantization of neural networks", "abstract": "We analyze the trade-off between quantization noise and clipping distortion in low precision networks. We identify the statistics of various tensors, and derive exact expressions for the mean-square-error degradation due to clipping. By optimizing these expressions, we show marked improvements over standard quantization schemes that normally avoid clipping. For example, just by choosing the accurate clipping values, more than 40\\% accuracy improvement is obtained for the quantization of VGG-16 to 4-bits of precision. Our results have many applications for the quantization of neural networks at both training and inference time. \n", "keywords": ["quantization", "reduced precision", "training", "inference", "activation"], "authorids": ["ron.banner@intel.com", "yury.nahshan@intel.com", "daniel.soudry@gmail.com", "elad.hoffer@gmail.com"], "authors": ["Ron Banner", "Yury Nahshan", "Elad Hoffer", "Daniel Soudry"], "TL;DR": "We analyze the trade-off between quantization noise and clipping distortion in low precision networks, and show marked improvements over standard quantization schemes that normally avoid clipping", "pdf": "/pdf/0871d08996fbd469ba1b2bc9cf43c46de4cb18d3.pdf", "paperhash": "banner|aciq_analytical_clipping_for_integer_quantization_of_neural_networks", "_bibtex": "@misc{\nbanner2019aciq,\ntitle={{ACIQ}: Analytical Clipping for Integer Quantization of neural networks},\nauthor={Ron Banner and Yury Nahshan and Elad Hoffer and Daniel Soudry},\nyear={2019},\nurl={https://openreview.net/forum?id=B1x33sC9KQ},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "SkluXjwElV", "original": null, "number": 1, "cdate": 1545005855662, "ddate": null, "tcdate": 1545005855662, "tmdate": 1545354486693, "tddate": null, "forum": "B1x33sC9KQ", "replyto": "B1x33sC9KQ", "invitation": "ICLR.cc/2019/Conference/-/Paper749/Meta_Review", "content": {"metareview": "The paper describes a clipping method to improve the performance of quantization. The reviewers have a consensus on rejection due to the contribution is not significant. ", "confidence": "5: The area chair is absolutely certain", "recommendation": "Reject", "title": "incremental work"}, "signatures": ["ICLR.cc/2019/Conference/Paper749/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper749/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ACIQ: Analytical Clipping for Integer Quantization of neural networks", "abstract": "We analyze the trade-off between quantization noise and clipping distortion in low precision networks. We identify the statistics of various tensors, and derive exact expressions for the mean-square-error degradation due to clipping. By optimizing these expressions, we show marked improvements over standard quantization schemes that normally avoid clipping. For example, just by choosing the accurate clipping values, more than 40\\% accuracy improvement is obtained for the quantization of VGG-16 to 4-bits of precision. Our results have many applications for the quantization of neural networks at both training and inference time. \n", "keywords": ["quantization", "reduced precision", "training", "inference", "activation"], "authorids": ["ron.banner@intel.com", "yury.nahshan@intel.com", "daniel.soudry@gmail.com", "elad.hoffer@gmail.com"], "authors": ["Ron Banner", "Yury Nahshan", "Elad Hoffer", "Daniel Soudry"], "TL;DR": "We analyze the trade-off between quantization noise and clipping distortion in low precision networks, and show marked improvements over standard quantization schemes that normally avoid clipping", "pdf": "/pdf/0871d08996fbd469ba1b2bc9cf43c46de4cb18d3.pdf", "paperhash": "banner|aciq_analytical_clipping_for_integer_quantization_of_neural_networks", "_bibtex": "@misc{\nbanner2019aciq,\ntitle={{ACIQ}: Analytical Clipping for Integer Quantization of neural networks},\nauthor={Ron Banner and Yury Nahshan and Elad Hoffer and Daniel Soudry},\nyear={2019},\nurl={https://openreview.net/forum?id=B1x33sC9KQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper749/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353100769, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1x33sC9KQ", "replyto": "B1x33sC9KQ", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper749/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper749/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper749/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353100769}}}, {"id": "BygyCtul1E", "original": null, "number": 6, "cdate": 1543698886704, "ddate": null, "tcdate": 1543698886704, "tmdate": 1543728494868, "tddate": null, "forum": "B1x33sC9KQ", "replyto": "SklGmbfykN", "invitation": "ICLR.cc/2019/Conference/-/Paper749/Official_Comment", "content": {"title": "Reply to remaining issues", "comment": "The paper we mention (i.e.,  https://arxiv.org/pdf/1805.11046.pdf#page=9&zoom=100,0,96 ) assumes Gaussian distribution and construct a solution that couldn\u2019t work unless tensors have approximately a Gaussian distribution. Due to the central limit theorem, neural network distributions are not general. In practice, tensors have a bell shape distribution where small values are much more frequent compared to the large values. Recent efforts take this prior into account and design quantizers with improved accuracy (e.g., https://arxiv.org/pdf/1804.10969.pdf). Our clipping method uses this bell-shaped distribution (we focus on Gauss/Laplacian distributions) to give higher precision where we need it (i.e., small values) at the expense of truncating very few large values. At the bottom line, our simulations prove that much better accuracy can be obtained with these assumptions. In all six evaluated models, we gain at least 12% validation accuracy improvement (in VGG16-BN we get 38% improvement) compared to GEMLOWP, which doesn\u2019t assume anything about the data distribution and quantize according to max()-min().\n\nFigure 5 does not appear in the paper. From the context, we guess the reviewer refers to Figure 2. The figure shows that analysis is in a good agreement with simulations, and for each bit-width there exists a distinct minimum at a certain clipping value.  The reviewer makes the following statment: \u201cThe gaussian assumption is not true for lower bit networks (the paper you referred uses 8 bits)\u201d Here is a paper that takes the Gaussian assumption for binary networks to explain why binary networks work in terms of high dimensional geometry (see page 2 about angle preservation property of random tensors from Gaussian distributions): https://arxiv.org/pdf/1705.07199.pdf\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper749/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper749/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper749/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ACIQ: Analytical Clipping for Integer Quantization of neural networks", "abstract": "We analyze the trade-off between quantization noise and clipping distortion in low precision networks. We identify the statistics of various tensors, and derive exact expressions for the mean-square-error degradation due to clipping. By optimizing these expressions, we show marked improvements over standard quantization schemes that normally avoid clipping. For example, just by choosing the accurate clipping values, more than 40\\% accuracy improvement is obtained for the quantization of VGG-16 to 4-bits of precision. Our results have many applications for the quantization of neural networks at both training and inference time. \n", "keywords": ["quantization", "reduced precision", "training", "inference", "activation"], "authorids": ["ron.banner@intel.com", "yury.nahshan@intel.com", "daniel.soudry@gmail.com", "elad.hoffer@gmail.com"], "authors": ["Ron Banner", "Yury Nahshan", "Elad Hoffer", "Daniel Soudry"], "TL;DR": "We analyze the trade-off between quantization noise and clipping distortion in low precision networks, and show marked improvements over standard quantization schemes that normally avoid clipping", "pdf": "/pdf/0871d08996fbd469ba1b2bc9cf43c46de4cb18d3.pdf", "paperhash": "banner|aciq_analytical_clipping_for_integer_quantization_of_neural_networks", "_bibtex": "@misc{\nbanner2019aciq,\ntitle={{ACIQ}: Analytical Clipping for Integer Quantization of neural networks},\nauthor={Ron Banner and Yury Nahshan and Elad Hoffer and Daniel Soudry},\nyear={2019},\nurl={https://openreview.net/forum?id=B1x33sC9KQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper749/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621608999, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1x33sC9KQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper749/Authors", "ICLR.cc/2019/Conference/Paper749/Reviewers", "ICLR.cc/2019/Conference/Paper749/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper749/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper749/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper749/Authors|ICLR.cc/2019/Conference/Paper749/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper749/Reviewers", "ICLR.cc/2019/Conference/Paper749/Authors", "ICLR.cc/2019/Conference/Paper749/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621608999}}}, {"id": "SylOokVw6m", "original": null, "number": 3, "cdate": 1542041503944, "ddate": null, "tcdate": 1542041503944, "tmdate": 1543606582427, "tddate": null, "forum": "B1x33sC9KQ", "replyto": "B1x33sC9KQ", "invitation": "ICLR.cc/2019/Conference/-/Paper749/Official_Review", "content": {"title": "Errors and Contributions not significant ", "review": "The paper describes a clipping method to improve the performance of one particular type of quantization method that is naive clipping to closest \"bins\". The contribution of the paper is the (possibly incorrect) derivation of the clipping value that causes the least quantization error IF assumptions can be made about the distribution of the parameters (in a non-bayesian sense). Thus, the significance is low due to both reasons.\n\nOne conceptual issue is the assumed relationship between quantization error and classification accuracy. The literature has shown that high quantization error does not necessarily mean low classification accuracy when using non-uniform quantization. The proposed clipping does not account for classification accuracy (on training set), but I understand the motivation being that the training set is not available. \n\n1. There seems to be an error in derivation of Eq (3), the first term should be $(x-sgn(x).\\alpha) = x+\\alpha$ for $x$ negative. Please comment on this.\n\n2. When solving the integrals, the authors simply pull the solution \"out of the hat\" and show that the derivative is the integrand. This is a very opaque presentation that we cannot see how you solved the integral. What is C in $\\psi(x)$?\n\n3. The assumptions on the parameters are only valid for the particular model/dataset/precision. The assumption does not generalize arbitrarily. For example, models with quantized weights have bi-modal distributions. How would you clip the  activations after e.g. a ReLu? This is without going in to the weaknesses of the K-S test. \n\n4. Experiments do not show any comparison to the large body of prior work in this area. \n\n5. Page 4, para below (3), what is \"common additive orthogonal noise\"? You should explain or give intuition instead of simply referring to a different paper.\n\n6. In the uniform case, one would think f(x)=1/<range of the interval>=2\\alpha. Why is it 1/\\Delta?\n\n6. Section 4, range should be [-\\alpha, \\alpha] instead of [\\alpha, -\\alpha]? Since \\alpha is positive.", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper749/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "ACIQ: Analytical Clipping for Integer Quantization of neural networks", "abstract": "We analyze the trade-off between quantization noise and clipping distortion in low precision networks. We identify the statistics of various tensors, and derive exact expressions for the mean-square-error degradation due to clipping. By optimizing these expressions, we show marked improvements over standard quantization schemes that normally avoid clipping. For example, just by choosing the accurate clipping values, more than 40\\% accuracy improvement is obtained for the quantization of VGG-16 to 4-bits of precision. Our results have many applications for the quantization of neural networks at both training and inference time. \n", "keywords": ["quantization", "reduced precision", "training", "inference", "activation"], "authorids": ["ron.banner@intel.com", "yury.nahshan@intel.com", "daniel.soudry@gmail.com", "elad.hoffer@gmail.com"], "authors": ["Ron Banner", "Yury Nahshan", "Elad Hoffer", "Daniel Soudry"], "TL;DR": "We analyze the trade-off between quantization noise and clipping distortion in low precision networks, and show marked improvements over standard quantization schemes that normally avoid clipping", "pdf": "/pdf/0871d08996fbd469ba1b2bc9cf43c46de4cb18d3.pdf", "paperhash": "banner|aciq_analytical_clipping_for_integer_quantization_of_neural_networks", "_bibtex": "@misc{\nbanner2019aciq,\ntitle={{ACIQ}: Analytical Clipping for Integer Quantization of neural networks},\nauthor={Ron Banner and Yury Nahshan and Elad Hoffer and Daniel Soudry},\nyear={2019},\nurl={https://openreview.net/forum?id=B1x33sC9KQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper749/Official_Review", "cdate": 1542234385036, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "B1x33sC9KQ", "replyto": "B1x33sC9KQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper749/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335794641, "tmdate": 1552335794641, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper749/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SklGmbfykN", "original": null, "number": 5, "cdate": 1543606553920, "ddate": null, "tcdate": 1543606553920, "tmdate": 1543606553920, "tddate": null, "forum": "B1x33sC9KQ", "replyto": "HJeFnz1cCQ", "invitation": "ICLR.cc/2019/Conference/-/Paper749/Official_Comment", "content": {"title": "reply to response", "comment": "1. Re the distribution assumption, the response from the authors is not convincing. The paper you mentioned (https://arxiv.org/pdf/1805.11046.pdf#page=9&zoom=100,0,96) says that, when using BN, \"quantization preserves the direction (angle) of high-dimensional vectors when W follows a Gaussian distribution\", this has nothing to do with your assumption that W follows a gaussian distribution.\n\nThe original question was not that \"gaussian -> low quantization error -> good performance\" (I think this is clear in the past 3 years) but rather \"non-gaussian -> high quantization error -> bad performance?\". Recent work suggests this may not always lead to bad performance (e.g. there are binary models with good performance and high quantization error). \n\nWhat does Figure 5 show? That quantization error is similar for analysis and simulation. Is this level of error \"small\"? Clearly, it depends on the number of bits. The gaussian assumption is not true for lower bit networks (the paper you referred uses 8 bits). Overall, the distribution assumption is a weakness.\n\n3. The point was about more datasets like VOC, beyond image classification. \n\nThank you for improving the paper, I have increased my rating appropriately."}, "signatures": ["ICLR.cc/2019/Conference/Paper749/AnonReviewer3"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper749/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper749/AnonReviewer3", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ACIQ: Analytical Clipping for Integer Quantization of neural networks", "abstract": "We analyze the trade-off between quantization noise and clipping distortion in low precision networks. We identify the statistics of various tensors, and derive exact expressions for the mean-square-error degradation due to clipping. By optimizing these expressions, we show marked improvements over standard quantization schemes that normally avoid clipping. For example, just by choosing the accurate clipping values, more than 40\\% accuracy improvement is obtained for the quantization of VGG-16 to 4-bits of precision. Our results have many applications for the quantization of neural networks at both training and inference time. \n", "keywords": ["quantization", "reduced precision", "training", "inference", "activation"], "authorids": ["ron.banner@intel.com", "yury.nahshan@intel.com", "daniel.soudry@gmail.com", "elad.hoffer@gmail.com"], "authors": ["Ron Banner", "Yury Nahshan", "Elad Hoffer", "Daniel Soudry"], "TL;DR": "We analyze the trade-off between quantization noise and clipping distortion in low precision networks, and show marked improvements over standard quantization schemes that normally avoid clipping", "pdf": "/pdf/0871d08996fbd469ba1b2bc9cf43c46de4cb18d3.pdf", "paperhash": "banner|aciq_analytical_clipping_for_integer_quantization_of_neural_networks", "_bibtex": "@misc{\nbanner2019aciq,\ntitle={{ACIQ}: Analytical Clipping for Integer Quantization of neural networks},\nauthor={Ron Banner and Yury Nahshan and Elad Hoffer and Daniel Soudry},\nyear={2019},\nurl={https://openreview.net/forum?id=B1x33sC9KQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper749/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621608999, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1x33sC9KQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper749/Authors", "ICLR.cc/2019/Conference/Paper749/Reviewers", "ICLR.cc/2019/Conference/Paper749/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper749/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper749/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper749/Authors|ICLR.cc/2019/Conference/Paper749/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper749/Reviewers", "ICLR.cc/2019/Conference/Paper749/Authors", "ICLR.cc/2019/Conference/Paper749/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621608999}}}, {"id": "Bkeu2415AQ", "original": null, "number": 3, "cdate": 1543267504185, "ddate": null, "tcdate": 1543267504185, "tmdate": 1543297683662, "tddate": null, "forum": "B1x33sC9KQ", "replyto": "HygMWn2Dn7", "invitation": "ICLR.cc/2019/Conference/-/Paper749/Official_Comment", "content": {"title": "Response to reviewer 1", "comment": "The paper indeed provides a formula for optimal quantization when the distribution of tensor elements is either laplace or gauss. The paper also shows the relevance of these derivations to a very attractive use-case i.e., the conversion of full precision network to low precision network without time-consuming re-training or the availability of the full datasets. Our approach is shown to have significant advantages over previous approaches (as summarized in Table 1). \n \nResponse to more specific comments:\nLanguage problems: We have incorporated all typos and paraphrasing suggestions\n\n1.\u201cit's not clear a-priori that information loss is the property to minimize that maximizes performance of the quantized network.\u201d  The connection between quantization error and classification accuracy has been investigated through the preservation of the direction of the quantized tensor. See for example here:\n         a.\thttps://arxiv.org/pdf/1805.11046.pdf#page=9&zoom=100,0,96 (section 5.1)\n         b.\thttps://arxiv.org/pdf/1705.07199.pdf (section 3.1)\nWe have added a detailed explanation about the connection between power of the quantization error and accuracy drop (see paragraph #5 in the introduction).\n\n2.\u201cGive absolute accuracies too! Improvement relative to what baseline?\u201d We now provide the baselines we use in our experiments (see Table 1).  \n\n3. \u201cThe mean square error should never go to 0. This suggests something is wrong. If it's just a scaling issue, consider a semilogy plot.\u201d: This was indeed a scale issue only. It is not relevant anymore (the figure was removed and replaced by the synthetic experiments showing that analysis and simulations are in a good agreement). \n\n4.\t\u201cI'm unclear what baseline (no clipping) refers to in terms of clipping values. For uniform quantization there needs to be some min and max\u201d: we improved the explanation of this issue in the introduction (see beginning of paragraph 9), where we explain that the traditional method that avoids clipping uniformly quantize the values between the largest and smallest tensor elements.  \n"}, "signatures": ["ICLR.cc/2019/Conference/Paper749/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper749/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper749/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ACIQ: Analytical Clipping for Integer Quantization of neural networks", "abstract": "We analyze the trade-off between quantization noise and clipping distortion in low precision networks. We identify the statistics of various tensors, and derive exact expressions for the mean-square-error degradation due to clipping. By optimizing these expressions, we show marked improvements over standard quantization schemes that normally avoid clipping. For example, just by choosing the accurate clipping values, more than 40\\% accuracy improvement is obtained for the quantization of VGG-16 to 4-bits of precision. Our results have many applications for the quantization of neural networks at both training and inference time. \n", "keywords": ["quantization", "reduced precision", "training", "inference", "activation"], "authorids": ["ron.banner@intel.com", "yury.nahshan@intel.com", "daniel.soudry@gmail.com", "elad.hoffer@gmail.com"], "authors": ["Ron Banner", "Yury Nahshan", "Elad Hoffer", "Daniel Soudry"], "TL;DR": "We analyze the trade-off between quantization noise and clipping distortion in low precision networks, and show marked improvements over standard quantization schemes that normally avoid clipping", "pdf": "/pdf/0871d08996fbd469ba1b2bc9cf43c46de4cb18d3.pdf", "paperhash": "banner|aciq_analytical_clipping_for_integer_quantization_of_neural_networks", "_bibtex": "@misc{\nbanner2019aciq,\ntitle={{ACIQ}: Analytical Clipping for Integer Quantization of neural networks},\nauthor={Ron Banner and Yury Nahshan and Elad Hoffer and Daniel Soudry},\nyear={2019},\nurl={https://openreview.net/forum?id=B1x33sC9KQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper749/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621608999, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1x33sC9KQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper749/Authors", "ICLR.cc/2019/Conference/Paper749/Reviewers", "ICLR.cc/2019/Conference/Paper749/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper749/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper749/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper749/Authors|ICLR.cc/2019/Conference/Paper749/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper749/Reviewers", "ICLR.cc/2019/Conference/Paper749/Authors", "ICLR.cc/2019/Conference/Paper749/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621608999}}}, {"id": "HJeFnz1cCQ", "original": null, "number": 2, "cdate": 1543266993420, "ddate": null, "tcdate": 1543266993420, "tmdate": 1543268188990, "tddate": null, "forum": "B1x33sC9KQ", "replyto": "SylOokVw6m", "invitation": "ICLR.cc/2019/Conference/-/Paper749/Official_Comment", "content": {"title": "Response to reviewer 3", "comment": "We conduct synthetic experiments showing analysis is in a very good agreement with synthetic simulations when distributions of tensor elements are either Laplace or Normal (see figure 2 in our new submission). The code to replicate these sanity checks appears here: https://github.com/submission2019/AnalyticalScaleForIntegerQuantization. We also improved the presentation of section 4 and provide a new figure to make the analysis easier to understand.\n\nAs noted by many prior arts, neural network distributions, are near Gaussian in practice, sometimes further controlled by procedures such as batch normalization. See for example here: \n1. https://arxiv.org/pdf/1804.10969.pdf\n2. https://openreview.net/pdf?id=B1IDRdeCW\n3. https://papers.nips.cc/paper/5269-expectation-backpropagation-parameter-free-training-of-multilayer-neural-networks-with-continuous-or-discrete-weights. \nIn addition, we were able to see these bell-shaped distributions through both statistical tests (KS-test at section 3) and the visual appearance of the histograms (see appendix). \n\nThe connection between quantization error and classification accuracy has been investigated through the preservation of the direction of the quantized tensor. See for example here:\n1. https://arxiv.org/pdf/1805.11046.pdf#page=9&zoom=100,0,96 (section 5.1)\n2. https://arxiv.org/pdf/1705.07199.pdf (section 3.1)\nWe have added a detailed explanation about the connection between power of the quantization error and accuracy drop (see paragraph #5 in the introduction).\n\nDetailed comments:\n1. Typo corrected.\n2. For correctness, we believe it is enough to provide the primitive functions of these integrals (since this can be verified by differentiation). The direct derivations are long unnecessary tedious calculations. Also, C in $\\psi(x)$ is the standard constant of integration for indefinite integrals. To avoid unclarity we removed this constant now from the text. \n3. We disagree with this comment. We have validated our work on six different models and improvement was dramatic with respect to gemmlowp for the quantization of activation tensors.  Weight tensors are kept at 8 bit of precision so the bi-modal distribution of weights does not apply to our work. The activations are clipped before the ReLU as we clarify now in Section 5 (second paragraph).\n4. Since submission, we made a comparison against the only previous method we are aware of: the Kullback-Leibler Divergence (KLD) clipping method suggested by NVIDIA (see update for table 1). Our approach runs 4000 times faster compared to KLD and, excluding ResNet-101, outperforms KLD in terms of validation accuracy. \n5. We provide now a better intuition for the analysis in section 4. \n6. For the uniform case, f(x) = 1/(2*alpha). We explicitly mention that in the paper now (just before equation 5).\n7. Typo corrected.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper749/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper749/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper749/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ACIQ: Analytical Clipping for Integer Quantization of neural networks", "abstract": "We analyze the trade-off between quantization noise and clipping distortion in low precision networks. We identify the statistics of various tensors, and derive exact expressions for the mean-square-error degradation due to clipping. By optimizing these expressions, we show marked improvements over standard quantization schemes that normally avoid clipping. For example, just by choosing the accurate clipping values, more than 40\\% accuracy improvement is obtained for the quantization of VGG-16 to 4-bits of precision. Our results have many applications for the quantization of neural networks at both training and inference time. \n", "keywords": ["quantization", "reduced precision", "training", "inference", "activation"], "authorids": ["ron.banner@intel.com", "yury.nahshan@intel.com", "daniel.soudry@gmail.com", "elad.hoffer@gmail.com"], "authors": ["Ron Banner", "Yury Nahshan", "Elad Hoffer", "Daniel Soudry"], "TL;DR": "We analyze the trade-off between quantization noise and clipping distortion in low precision networks, and show marked improvements over standard quantization schemes that normally avoid clipping", "pdf": "/pdf/0871d08996fbd469ba1b2bc9cf43c46de4cb18d3.pdf", "paperhash": "banner|aciq_analytical_clipping_for_integer_quantization_of_neural_networks", "_bibtex": "@misc{\nbanner2019aciq,\ntitle={{ACIQ}: Analytical Clipping for Integer Quantization of neural networks},\nauthor={Ron Banner and Yury Nahshan and Elad Hoffer and Daniel Soudry},\nyear={2019},\nurl={https://openreview.net/forum?id=B1x33sC9KQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper749/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621608999, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1x33sC9KQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper749/Authors", "ICLR.cc/2019/Conference/Paper749/Reviewers", "ICLR.cc/2019/Conference/Paper749/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper749/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper749/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper749/Authors|ICLR.cc/2019/Conference/Paper749/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper749/Reviewers", "ICLR.cc/2019/Conference/Paper749/Authors", "ICLR.cc/2019/Conference/Paper749/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621608999}}}, {"id": "B1l-_UycR7", "original": null, "number": 4, "cdate": 1543267944703, "ddate": null, "tcdate": 1543267944703, "tmdate": 1543267944703, "tddate": null, "forum": "B1x33sC9KQ", "replyto": "HJegw7SEhQ", "invitation": "ICLR.cc/2019/Conference/-/Paper749/Official_Comment", "content": {"title": "Response to reviewer 2 ", "comment": "Reviewer  addressed three concerns:\n\n1. It has been observed by many prior arts that quantization error can be assumed to be uniformly distributed (e.g., http://daniel-marco.com/Academic%20Files/Additive%20Noise%20Model.pdf). Our expression estimates the MSE as a combination of the MSE that results from the quantization error in the \u201cmiddle part\u201d (i.e., uniformly distributed) and the MSE that results from the clipping error at the tails of the distribution (i.e., laplacian/gaussian). We verify this assumptions using synthetic simulations that clearly show that this type of approximation is accurate in practice (see figure 2). We also provide the code for this simple synthetic experiment here: https://github.com/submission2019/AnalyticalScaleForIntegerQuantization . Finally, we now have a more general expression that estimates the MSE of any density function at the middle part using a piece wise linear approximation, enabling to use not only uniform distributions at the middle part. \n\n2. Since submission, we made a comparison against the only previous method we are aware of: the Kullback-Leibler Divergence (KLD) clipping method suggested by NVIDIA (see update for table 1). Our approach runs 4000 times faster compared to KLD and, excluding ResNet-101, outperforms KLD in terms of validation accuracy. \n\n3. We agree that the mean and sigma are continuous numbers. But the correct clipping value can be calculated by scaling the optimal clipping value for the standard gaussian distribution N(0,1) by sigma. We use this trick in our simulations. We improved the explanation of this issue (see Section 5  - end of the second paragraph)"}, "signatures": ["ICLR.cc/2019/Conference/Paper749/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper749/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper749/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ACIQ: Analytical Clipping for Integer Quantization of neural networks", "abstract": "We analyze the trade-off between quantization noise and clipping distortion in low precision networks. We identify the statistics of various tensors, and derive exact expressions for the mean-square-error degradation due to clipping. By optimizing these expressions, we show marked improvements over standard quantization schemes that normally avoid clipping. For example, just by choosing the accurate clipping values, more than 40\\% accuracy improvement is obtained for the quantization of VGG-16 to 4-bits of precision. Our results have many applications for the quantization of neural networks at both training and inference time. \n", "keywords": ["quantization", "reduced precision", "training", "inference", "activation"], "authorids": ["ron.banner@intel.com", "yury.nahshan@intel.com", "daniel.soudry@gmail.com", "elad.hoffer@gmail.com"], "authors": ["Ron Banner", "Yury Nahshan", "Elad Hoffer", "Daniel Soudry"], "TL;DR": "We analyze the trade-off between quantization noise and clipping distortion in low precision networks, and show marked improvements over standard quantization schemes that normally avoid clipping", "pdf": "/pdf/0871d08996fbd469ba1b2bc9cf43c46de4cb18d3.pdf", "paperhash": "banner|aciq_analytical_clipping_for_integer_quantization_of_neural_networks", "_bibtex": "@misc{\nbanner2019aciq,\ntitle={{ACIQ}: Analytical Clipping for Integer Quantization of neural networks},\nauthor={Ron Banner and Yury Nahshan and Elad Hoffer and Daniel Soudry},\nyear={2019},\nurl={https://openreview.net/forum?id=B1x33sC9KQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper749/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621608999, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1x33sC9KQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper749/Authors", "ICLR.cc/2019/Conference/Paper749/Reviewers", "ICLR.cc/2019/Conference/Paper749/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper749/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper749/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper749/Authors|ICLR.cc/2019/Conference/Paper749/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper749/Reviewers", "ICLR.cc/2019/Conference/Paper749/Authors", "ICLR.cc/2019/Conference/Paper749/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621608999}}}, {"id": "HygMWn2Dn7", "original": null, "number": 2, "cdate": 1541028858076, "ddate": null, "tcdate": 1541028858076, "tmdate": 1541533720403, "tddate": null, "forum": "B1x33sC9KQ", "replyto": "B1x33sC9KQ", "invitation": "ICLR.cc/2019/Conference/-/Paper749/Official_Review", "content": {"title": "review", "review": "This paper derives a formula for finding the minimum and maximum clipping values for uniform quantization which minimize the square error resulting from quantization, for either a Laplace or Gaussian distribution over pre-quantized value. This seems like too small a contribution to warrant a paper. I wasn't convinced that appropriate baselines were used in experiments. There were a number of statements that I believed to be technically slightly incorrect. There were also some small language problems (though these didn't hinder understanding).\n\nmore specific comments:\n\nabstract:\n\"derive exact expressions\" -- these expressions aren't exact. they turn out to be based on a piecewise zeroth order Taylor approximation to the density.\n\nmain paper:\n\"allow fit bigger networks into\" -> \"allow bigger network to fit into\"\n\"that we are need\" -> \"that need\"\n\"introduces an additional\" -> \"introduces additional\"\nclippig -> clipping\n\nit's not clear a-priori that information loss is the property to minimize that maximizes performance of the quantized network.\n\n\"distributions of tensors\" -> \"distribution of tensor elements\"\nthis comment also applies in a number of other places, where the writing refers to the marginal distribution of values taken on by entries in a tensor as the distribution over the tensor. note that a distribution over tensors is a joint distribution over all entries in a tensor. e.g. it would capture things like eigenvalues, entry-entry covariance, rather than just marginal statistics.\n\n\"than they could have by working individually\" -> \"than could have been achieved by each individually\"\n\nWhy the focus on small activation bit depth? I would imagine weight bit-depth was more important than activation bit depth. Especially since you're using ?32-bit? precision in the weight/activations multiplications, so activations are computed at a high bit depth anyways.\n\nTable 1: Give absolute accuracies too! Improvement relative to what baseline?\n\nsec 2:\nsufficeint -> sufficient\n\\citep often used when it should instead be \\citet.\n\"As contrast\" -> \"In contrast\"\n\nsection 3:\nuniformity -> uniformly\n\nI don't believe the notion of p-value is being used correctly here w.r.t. the Kolmogorov-Smirnov test.\n\nFigure 1: The mean square error should never go to 0. This suggests something is wrong. If it's just a scaling issue, consider a semilogy plot.\n\nFigure 2: I'm unclear what baseline (no clipping) refers to in terms of clipping values. For uniform quantization there needs to be some min and max value.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper749/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ACIQ: Analytical Clipping for Integer Quantization of neural networks", "abstract": "We analyze the trade-off between quantization noise and clipping distortion in low precision networks. We identify the statistics of various tensors, and derive exact expressions for the mean-square-error degradation due to clipping. By optimizing these expressions, we show marked improvements over standard quantization schemes that normally avoid clipping. For example, just by choosing the accurate clipping values, more than 40\\% accuracy improvement is obtained for the quantization of VGG-16 to 4-bits of precision. Our results have many applications for the quantization of neural networks at both training and inference time. \n", "keywords": ["quantization", "reduced precision", "training", "inference", "activation"], "authorids": ["ron.banner@intel.com", "yury.nahshan@intel.com", "daniel.soudry@gmail.com", "elad.hoffer@gmail.com"], "authors": ["Ron Banner", "Yury Nahshan", "Elad Hoffer", "Daniel Soudry"], "TL;DR": "We analyze the trade-off between quantization noise and clipping distortion in low precision networks, and show marked improvements over standard quantization schemes that normally avoid clipping", "pdf": "/pdf/0871d08996fbd469ba1b2bc9cf43c46de4cb18d3.pdf", "paperhash": "banner|aciq_analytical_clipping_for_integer_quantization_of_neural_networks", "_bibtex": "@misc{\nbanner2019aciq,\ntitle={{ACIQ}: Analytical Clipping for Integer Quantization of neural networks},\nauthor={Ron Banner and Yury Nahshan and Elad Hoffer and Daniel Soudry},\nyear={2019},\nurl={https://openreview.net/forum?id=B1x33sC9KQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper749/Official_Review", "cdate": 1542234385036, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "B1x33sC9KQ", "replyto": "B1x33sC9KQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper749/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335794641, "tmdate": 1552335794641, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper749/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "HJegw7SEhQ", "original": null, "number": 1, "cdate": 1540801367941, "ddate": null, "tcdate": 1540801367941, "tmdate": 1541533720154, "tddate": null, "forum": "B1x33sC9KQ", "replyto": "B1x33sC9KQ", "invitation": "ICLR.cc/2019/Conference/-/Paper749/Official_Review", "content": {"title": "A simple but not very convincing clipping method for activation quantization in deep networks", "review": "This paper empirically finds that the distribution of activations in quantized networks follow  Gaussian or Laplacian distribution, and proposes to determine the optimal clipping factor by minimizing the quantization error based on the distribution assumption.\n\nThe pros of the work are its simplicity, the proposed clipping and quantization does not need additional re-training. However, while the key of this paper is to determine a good clipping factor, the authors use uniform density function to represent the middle part of both Gaussian and Laplacian distributions where the majority of data points lie in, but exact computation for the tails of the distributions at both ends. Thus the computation of quantization error is not quite convincing. Moreover, the authors do not compare with the other recent works that also clip the activations, thus it is hard to validate the efficacy of the proposed method.\n\nFor the experiments, the authors mention that a look-up table can be pre-computed for fast retrieval of clipping factors given the mean and sigma of a distribution.  However, the mean and sigma are continuous numbers, how is the look-up table made?  Moreover, how is the mean and std estimated for each weight tensor and what is  the complexity?\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper749/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ACIQ: Analytical Clipping for Integer Quantization of neural networks", "abstract": "We analyze the trade-off between quantization noise and clipping distortion in low precision networks. We identify the statistics of various tensors, and derive exact expressions for the mean-square-error degradation due to clipping. By optimizing these expressions, we show marked improvements over standard quantization schemes that normally avoid clipping. For example, just by choosing the accurate clipping values, more than 40\\% accuracy improvement is obtained for the quantization of VGG-16 to 4-bits of precision. Our results have many applications for the quantization of neural networks at both training and inference time. \n", "keywords": ["quantization", "reduced precision", "training", "inference", "activation"], "authorids": ["ron.banner@intel.com", "yury.nahshan@intel.com", "daniel.soudry@gmail.com", "elad.hoffer@gmail.com"], "authors": ["Ron Banner", "Yury Nahshan", "Elad Hoffer", "Daniel Soudry"], "TL;DR": "We analyze the trade-off between quantization noise and clipping distortion in low precision networks, and show marked improvements over standard quantization schemes that normally avoid clipping", "pdf": "/pdf/0871d08996fbd469ba1b2bc9cf43c46de4cb18d3.pdf", "paperhash": "banner|aciq_analytical_clipping_for_integer_quantization_of_neural_networks", "_bibtex": "@misc{\nbanner2019aciq,\ntitle={{ACIQ}: Analytical Clipping for Integer Quantization of neural networks},\nauthor={Ron Banner and Yury Nahshan and Elad Hoffer and Daniel Soudry},\nyear={2019},\nurl={https://openreview.net/forum?id=B1x33sC9KQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper749/Official_Review", "cdate": 1542234385036, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "B1x33sC9KQ", "replyto": "B1x33sC9KQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper749/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335794641, "tmdate": 1552335794641, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper749/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "rJeQxP5nF7", "original": null, "number": 2, "cdate": 1538201322553, "ddate": null, "tcdate": 1538201322553, "tmdate": 1538201322553, "tddate": null, "forum": "B1x33sC9KQ", "replyto": "BkgZu_Q3tQ", "invitation": "ICLR.cc/2019/Conference/-/Paper749/Public_Comment", "content": {"comment": "Thanks for clarification!", "title": "Thanks"}, "signatures": ["~Evgenii_Zheltonozhskii1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper749/Reviewers/Unsubmitted"], "writers": ["~Evgenii_Zheltonozhskii1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ACIQ: Analytical Clipping for Integer Quantization of neural networks", "abstract": "We analyze the trade-off between quantization noise and clipping distortion in low precision networks. We identify the statistics of various tensors, and derive exact expressions for the mean-square-error degradation due to clipping. By optimizing these expressions, we show marked improvements over standard quantization schemes that normally avoid clipping. For example, just by choosing the accurate clipping values, more than 40\\% accuracy improvement is obtained for the quantization of VGG-16 to 4-bits of precision. Our results have many applications for the quantization of neural networks at both training and inference time. \n", "keywords": ["quantization", "reduced precision", "training", "inference", "activation"], "authorids": ["ron.banner@intel.com", "yury.nahshan@intel.com", "daniel.soudry@gmail.com", "elad.hoffer@gmail.com"], "authors": ["Ron Banner", "Yury Nahshan", "Elad Hoffer", "Daniel Soudry"], "TL;DR": "We analyze the trade-off between quantization noise and clipping distortion in low precision networks, and show marked improvements over standard quantization schemes that normally avoid clipping", "pdf": "/pdf/0871d08996fbd469ba1b2bc9cf43c46de4cb18d3.pdf", "paperhash": "banner|aciq_analytical_clipping_for_integer_quantization_of_neural_networks", "_bibtex": "@misc{\nbanner2019aciq,\ntitle={{ACIQ}: Analytical Clipping for Integer Quantization of neural networks},\nauthor={Ron Banner and Yury Nahshan and Elad Hoffer and Daniel Soudry},\nyear={2019},\nurl={https://openreview.net/forum?id=B1x33sC9KQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper749/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311761508, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "B1x33sC9KQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper749/Authors", "ICLR.cc/2019/Conference/Paper749/Reviewers", "ICLR.cc/2019/Conference/Paper749/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper749/Authors", "ICLR.cc/2019/Conference/Paper749/Reviewers", "ICLR.cc/2019/Conference/Paper749/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311761508}}}, {"id": "BkgZu_Q3tQ", "original": null, "number": 1, "cdate": 1538173032799, "ddate": null, "tcdate": 1538173032799, "tmdate": 1538173032799, "tddate": null, "forum": "B1x33sC9KQ", "replyto": "Hkg248istm", "invitation": "ICLR.cc/2019/Conference/-/Paper749/Official_Comment", "content": {"title": "reply to explanation and comparison of results", "comment": "We are not suggesting a quantization approach at the network level. Rather, we try minimizing the quantization effect at the tensor level only. We claim that when tensor values exceed a certain threshold, they should be clipped. Our main result is an analytical formula for clipping these values that, depending on the statistical distribution of the tensor, finds the *optimal* threshold (with respect to mean-square-error). \n\nWe focus on clipping only (i.e., no re-training or fine-tuning). Hence, we compare against the standard integer quantization approach that avoids clipping (i.e., GEMMLOWP). This serves as the baseline for the comparison in Table 1.  You mention 65.75% top-1 accuracy for Res18 using our analytical clipping. Without clipping you would have 53.2% top-1 accuracy. We also have a similar result for the VGG-16 model, where we show that by just clipping correctly the activation tensors, you could gain more than 40% accuracy improvement compared to the case of no clipping. \n\nFinally, we are fully aware of the recent works you mention (in fact, PACT paper is cited and explained). Yet, these are completely not in the settings of our work. Both papers *learn* a good quantization through training.  Our work is orthogonal and can work in synergy with these techniques. You can minimize the effect of quantization at the tensor level and, at the same time, compensate for quantization at the network level using training/fine-tuning. There are many other applications. For example, a rapid deployment of neural networks trained in full precision to low precision accelerators without having the full datasets on which the networks are working on.\n\nThe missing caption of subfigure 2f refers to Inception_v3."}, "signatures": ["ICLR.cc/2019/Conference/Paper749/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper749/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper749/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ACIQ: Analytical Clipping for Integer Quantization of neural networks", "abstract": "We analyze the trade-off between quantization noise and clipping distortion in low precision networks. We identify the statistics of various tensors, and derive exact expressions for the mean-square-error degradation due to clipping. By optimizing these expressions, we show marked improvements over standard quantization schemes that normally avoid clipping. For example, just by choosing the accurate clipping values, more than 40\\% accuracy improvement is obtained for the quantization of VGG-16 to 4-bits of precision. Our results have many applications for the quantization of neural networks at both training and inference time. \n", "keywords": ["quantization", "reduced precision", "training", "inference", "activation"], "authorids": ["ron.banner@intel.com", "yury.nahshan@intel.com", "daniel.soudry@gmail.com", "elad.hoffer@gmail.com"], "authors": ["Ron Banner", "Yury Nahshan", "Elad Hoffer", "Daniel Soudry"], "TL;DR": "We analyze the trade-off between quantization noise and clipping distortion in low precision networks, and show marked improvements over standard quantization schemes that normally avoid clipping", "pdf": "/pdf/0871d08996fbd469ba1b2bc9cf43c46de4cb18d3.pdf", "paperhash": "banner|aciq_analytical_clipping_for_integer_quantization_of_neural_networks", "_bibtex": "@misc{\nbanner2019aciq,\ntitle={{ACIQ}: Analytical Clipping for Integer Quantization of neural networks},\nauthor={Ron Banner and Yury Nahshan and Elad Hoffer and Daniel Soudry},\nyear={2019},\nurl={https://openreview.net/forum?id=B1x33sC9KQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper749/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621608999, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1x33sC9KQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper749/Authors", "ICLR.cc/2019/Conference/Paper749/Reviewers", "ICLR.cc/2019/Conference/Paper749/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper749/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper749/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper749/Authors|ICLR.cc/2019/Conference/Paper749/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper749/Reviewers", "ICLR.cc/2019/Conference/Paper749/Authors", "ICLR.cc/2019/Conference/Paper749/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621608999}}}, {"id": "Hkg248istm", "original": null, "number": 1, "cdate": 1538139700429, "ddate": null, "tcdate": 1538139700429, "tmdate": 1538139700429, "tddate": null, "forum": "B1x33sC9KQ", "replyto": "B1x33sC9KQ", "invitation": "ICLR.cc/2019/Conference/-/Paper749/Public_Comment", "content": {"title": "Explanation and comparison of resutls", "comment": "Hi, can you explain the numbers you present in Table 1? What do you compare to? Also, I haven't manage to find comparison to any other quantization paper, neither numbers of accuracy you achieved for any network. I've run your code and acquired 65.75% top-1 and 86.70% top-5 for ResNet-18. However, recent work, such as PACT (https://arxiv.org/abs/1805.06085) and LQ-nets (https://arxiv.org/abs/1807.10029) achieve significantly higher results for much coarser quantization - 69+% top-1 for 4 bit for both activation and weights.\n\nP.S. The caption of subfigure 2f is missing."}, "signatures": ["~Evgenii_Zheltonozhskii1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper749/Reviewers/Unsubmitted"], "writers": ["~Evgenii_Zheltonozhskii1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ACIQ: Analytical Clipping for Integer Quantization of neural networks", "abstract": "We analyze the trade-off between quantization noise and clipping distortion in low precision networks. We identify the statistics of various tensors, and derive exact expressions for the mean-square-error degradation due to clipping. By optimizing these expressions, we show marked improvements over standard quantization schemes that normally avoid clipping. For example, just by choosing the accurate clipping values, more than 40\\% accuracy improvement is obtained for the quantization of VGG-16 to 4-bits of precision. Our results have many applications for the quantization of neural networks at both training and inference time. \n", "keywords": ["quantization", "reduced precision", "training", "inference", "activation"], "authorids": ["ron.banner@intel.com", "yury.nahshan@intel.com", "daniel.soudry@gmail.com", "elad.hoffer@gmail.com"], "authors": ["Ron Banner", "Yury Nahshan", "Elad Hoffer", "Daniel Soudry"], "TL;DR": "We analyze the trade-off between quantization noise and clipping distortion in low precision networks, and show marked improvements over standard quantization schemes that normally avoid clipping", "pdf": "/pdf/0871d08996fbd469ba1b2bc9cf43c46de4cb18d3.pdf", "paperhash": "banner|aciq_analytical_clipping_for_integer_quantization_of_neural_networks", "_bibtex": "@misc{\nbanner2019aciq,\ntitle={{ACIQ}: Analytical Clipping for Integer Quantization of neural networks},\nauthor={Ron Banner and Yury Nahshan and Elad Hoffer and Daniel Soudry},\nyear={2019},\nurl={https://openreview.net/forum?id=B1x33sC9KQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper749/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311761508, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "B1x33sC9KQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper749/Authors", "ICLR.cc/2019/Conference/Paper749/Reviewers", "ICLR.cc/2019/Conference/Paper749/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper749/Authors", "ICLR.cc/2019/Conference/Paper749/Reviewers", "ICLR.cc/2019/Conference/Paper749/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311761508}}}], "count": 13}