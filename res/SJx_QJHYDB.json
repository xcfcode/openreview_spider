{"notes": [{"id": "SJx_QJHYDB", "original": "H1xRDP2dPB", "number": 1623, "cdate": 1569439520351, "ddate": null, "tcdate": 1569439520351, "tmdate": 1577168249606, "tddate": null, "forum": "SJx_QJHYDB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["mathilde@fb.com", "arimorcos@gmail.com", "bojanowski@fb.com", "julien.mairal@inria.fr", "ajoulin@fb.com"], "title": "Finding Winning Tickets with Limited (or No) Supervision", "authors": ["Mathilde Caron", "Ari Morcos", "Piotr Bojanowski", "Julien Mairal", "Armand Joulin"], "pdf": "/pdf/6ee7308ab20319fbdb516d21984b533672c9c7b7.pdf", "TL;DR": "Finding winning tickets does not require much supervision or data.", "abstract": "The lottery ticket hypothesis argues that neural networks contain sparse subnetworks, which, if appropriately initialized (the winning tickets), are capable of matching the accuracy of the full network when trained in isolation. Empirically made in different contexts, such an observation opens interesting questions about the dynamics of neural network optimization and the importance of their initializations. However, the properties of winning tickets are not well understood, especially the importance of supervision in the generating process. In this paper, we aim to answer the following open questions: can we find winning tickets with few data samples or few labels? can we even obtain good tickets without supervision? Perhaps surprisingly, we provide a positive answer to both, by generating winning tickets with limited access to data, or with self-supervision---thus without using manual annotations---and then demonstrating the transferability of the tickets to challenging classification tasks such as ImageNet.\n", "keywords": ["Lottery Tickets Hypothesis", "Self-Supervised Learning", "Deep Learning", "Image Recognition"], "paperhash": "caron|finding_winning_tickets_with_limited_or_no_supervision", "original_pdf": "/attachment/abcaeba8b5121422f7daebbc1889adacc84ee54d.pdf", "_bibtex": "@misc{\ncaron2020finding,\ntitle={Finding Winning Tickets with Limited (or No) Supervision},\nauthor={Mathilde Caron and Ari Morcos and Piotr Bojanowski and Julien Mairal and Armand Joulin},\nyear={2020},\nurl={https://openreview.net/forum?id=SJx_QJHYDB}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "YWcJxsJx6i", "original": null, "number": 1, "cdate": 1576798728095, "ddate": null, "tcdate": 1576798728095, "tmdate": 1576800908449, "tddate": null, "forum": "SJx_QJHYDB", "replyto": "SJx_QJHYDB", "invitation": "ICLR.cc/2020/Conference/Paper1623/-/Decision", "content": {"decision": "Reject", "comment": "The paper studies finding winning tickets with limited supervision. The authors consider a variety of different settings. An interesting contribution is to show that findings on small datasets may be misleading. That said, all three reviewers agree that novelty is limited, and some found inconsistencies and passages that were hard to read: Based on this, it seems the paper doesn't quite meet the ICLR bar in its current form. ", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["mathilde@fb.com", "arimorcos@gmail.com", "bojanowski@fb.com", "julien.mairal@inria.fr", "ajoulin@fb.com"], "title": "Finding Winning Tickets with Limited (or No) Supervision", "authors": ["Mathilde Caron", "Ari Morcos", "Piotr Bojanowski", "Julien Mairal", "Armand Joulin"], "pdf": "/pdf/6ee7308ab20319fbdb516d21984b533672c9c7b7.pdf", "TL;DR": "Finding winning tickets does not require much supervision or data.", "abstract": "The lottery ticket hypothesis argues that neural networks contain sparse subnetworks, which, if appropriately initialized (the winning tickets), are capable of matching the accuracy of the full network when trained in isolation. Empirically made in different contexts, such an observation opens interesting questions about the dynamics of neural network optimization and the importance of their initializations. However, the properties of winning tickets are not well understood, especially the importance of supervision in the generating process. In this paper, we aim to answer the following open questions: can we find winning tickets with few data samples or few labels? can we even obtain good tickets without supervision? Perhaps surprisingly, we provide a positive answer to both, by generating winning tickets with limited access to data, or with self-supervision---thus without using manual annotations---and then demonstrating the transferability of the tickets to challenging classification tasks such as ImageNet.\n", "keywords": ["Lottery Tickets Hypothesis", "Self-Supervised Learning", "Deep Learning", "Image Recognition"], "paperhash": "caron|finding_winning_tickets_with_limited_or_no_supervision", "original_pdf": "/attachment/abcaeba8b5121422f7daebbc1889adacc84ee54d.pdf", "_bibtex": "@misc{\ncaron2020finding,\ntitle={Finding Winning Tickets with Limited (or No) Supervision},\nauthor={Mathilde Caron and Ari Morcos and Piotr Bojanowski and Julien Mairal and Armand Joulin},\nyear={2020},\nurl={https://openreview.net/forum?id=SJx_QJHYDB}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "SJx_QJHYDB", "replyto": "SJx_QJHYDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795709533, "tmdate": 1576800258317, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1623/-/Decision"}}}, {"id": "BkxyHdzhsr", "original": null, "number": 4, "cdate": 1573820471483, "ddate": null, "tcdate": 1573820471483, "tmdate": 1573820817890, "tddate": null, "forum": "SJx_QJHYDB", "replyto": "SJeuh5z19B", "invitation": "ICLR.cc/2020/Conference/Paper1623/-/Official_Comment", "content": {"title": "Reply to review #1", "comment": "1) Following the reviewer's comment, we report in Appendix E tables the exact accuracies in each setting. We report mean and standard errors for our experiments which we run with 3 (ImageNet and Places) or 6 (CIFAR) different seeds. We thank the reviewer for this recommendation and for helping us improving the clarity and robustness of the comparison.\n\n2) The \u201cRandom - adjusted\u201d baseline is not obtained by applying the pruning mask to randomly initialized weights. In the following lines, we motivate and clarify this baseline and have included this explanation in the paper updated version.\nWe find that deep architectures (VGG-19 or ResNet-18 for example) trained on CIFAR-10 are naturally sparse (~80% of the weights are zeroed at convergence). Pruning a network at rates below its level of natural sparsity without impacting the performance is trivial because the network is already sparse. Indeed, we found that in the random global pruning baseline (which can remove non zero weights), pruning at rates below the natural sparsity of the network degrades accuracy, while pruning of weights that are already zeroed has no effect. Experiments performed with pruning rates below the natural level of sparsity of the network (~80%) are uninformative. Inconveniently, this performance gap carries over to higher pruning rates (in which we are interested in) and can lead to misleading interpretations. The random adjusted baseline removes this effect by first pruning the weights that naturally converge to zero after training. Then, we randomly mask the remaining non-zeroed weights to get different final desired pruning rates. The remaining non-masked weights are randomly initialized. This baseline therefore corrects for the natural sparsity present in CIFAR-10 networks. \n\nRegarding the random initialization remark, Liu et al. indeed show in Figure 7.a (unstructured iterative pruning) that starting from randomly reinitialized weights works well on deep architectures (VGG-16 and ResNet-50) on CIFAR-10 when pruned at rates below ~90%. This is consistent with the observations of Frankle et al. (2019) in the Appendix A of their paper. Indeed, Frankle et al. (2019) also show that up to a certain level of sparsity, training the subnetwork from its original weights or from random re-initialization gives comparable performance. However, for more extreme pruning rates (>90%), resetting the subnetwork to its original weights gives better performance than random re-initialization. In this work, we follow up on the work of Frankle et al. (2019) and Morcos et al. (2019) that both provide empirical evidence that in the regime of large datasets or high pruning rates, starting from a particular set of weights instead of random initialization is critical to reach high accuracy.\nFor completeness though, we take into account the remark of the reviewer and have included in Appendix G results with random re-initialization for winning tickets found with labels or with RotNet self-supervised task on both ImageNet and CIFAR-10. \nOn ImageNet, consistently with the experiments of Frankle et al. (2019) we observe that resetting the weights accordingly is crucial to get high accuracy. Indeed, on both ResNet-50 and AlexNet, for labels, the subnetworks that are reset to their weights early in training (dark blue plain line) perform significantly better than subnetworks randomly re-initialized (dark blue dashed line). Interestingly, this is not the case for RotNet winning tickets: starting from original weights (pink plain line) gives only a very slight boost (or even no boost at all) of performance compared to randomly re-initialization (pink dashed line). Overall, labels or rotnet subnetworks perform in the same ball park when randomly re-initialized, but using the original weights gives a large boost of performance for labels but not for rotnet. Thus, it suggests that the information carried by the pruned mask itself is similar for labels and rotnet subnetworks but the weights of the rotnet winning tickets are not as good starting points as the weights from labels winning tickets. We thank the reviewer for suggesting this experiment; it gives interesting insights on the difference of performance between labels and rotnet winning tickets.\nOn CIFAR-10, up to a certain level of sparsity that roughly corresponds to the natural level of sparsity of the network, using random re-initialization or weights \u2018early in training\u2019 gives similar performance. However, for more extreme pruning rates, using a particular set of weights gives significantly better performance than random re-initialization.\n\n3) We chose not to vary the dataset size on CIFAR-10 because it is already small. However, following the reviewer recommendation we include results with CIFAR-10 in Appendix F.\n\nOverall, we hope that our updated version of the paper along with our comments provide clarifications about our experimental settings and reinforce the validity of our results.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1623/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1623/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["mathilde@fb.com", "arimorcos@gmail.com", "bojanowski@fb.com", "julien.mairal@inria.fr", "ajoulin@fb.com"], "title": "Finding Winning Tickets with Limited (or No) Supervision", "authors": ["Mathilde Caron", "Ari Morcos", "Piotr Bojanowski", "Julien Mairal", "Armand Joulin"], "pdf": "/pdf/6ee7308ab20319fbdb516d21984b533672c9c7b7.pdf", "TL;DR": "Finding winning tickets does not require much supervision or data.", "abstract": "The lottery ticket hypothesis argues that neural networks contain sparse subnetworks, which, if appropriately initialized (the winning tickets), are capable of matching the accuracy of the full network when trained in isolation. Empirically made in different contexts, such an observation opens interesting questions about the dynamics of neural network optimization and the importance of their initializations. However, the properties of winning tickets are not well understood, especially the importance of supervision in the generating process. In this paper, we aim to answer the following open questions: can we find winning tickets with few data samples or few labels? can we even obtain good tickets without supervision? Perhaps surprisingly, we provide a positive answer to both, by generating winning tickets with limited access to data, or with self-supervision---thus without using manual annotations---and then demonstrating the transferability of the tickets to challenging classification tasks such as ImageNet.\n", "keywords": ["Lottery Tickets Hypothesis", "Self-Supervised Learning", "Deep Learning", "Image Recognition"], "paperhash": "caron|finding_winning_tickets_with_limited_or_no_supervision", "original_pdf": "/attachment/abcaeba8b5121422f7daebbc1889adacc84ee54d.pdf", "_bibtex": "@misc{\ncaron2020finding,\ntitle={Finding Winning Tickets with Limited (or No) Supervision},\nauthor={Mathilde Caron and Ari Morcos and Piotr Bojanowski and Julien Mairal and Armand Joulin},\nyear={2020},\nurl={https://openreview.net/forum?id=SJx_QJHYDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJx_QJHYDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1623/Authors", "ICLR.cc/2020/Conference/Paper1623/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1623/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1623/Reviewers", "ICLR.cc/2020/Conference/Paper1623/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1623/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1623/Authors|ICLR.cc/2020/Conference/Paper1623/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504153284, "tmdate": 1576860547313, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1623/Authors", "ICLR.cc/2020/Conference/Paper1623/Reviewers", "ICLR.cc/2020/Conference/Paper1623/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1623/-/Official_Comment"}}}, {"id": "rJgexFfhir", "original": null, "number": 5, "cdate": 1573820648364, "ddate": null, "tcdate": 1573820648364, "tmdate": 1573820648364, "tddate": null, "forum": "SJx_QJHYDB", "replyto": "SkxRhOSTFB", "invitation": "ICLR.cc/2020/Conference/Paper1623/-/Official_Comment", "content": {"title": "Reply to review #3", "comment": "We agree with the reviewer that our contribution is not methodological but mainly experimental.\n\nAlso, we agree that some of our observations might be intuitively expected and reasonable, though we note that the fact that winning tickets transfer between similar datasets with labels (as shown in Morcos et al., 2019) does not necessarily imply that transfer from self-supervised tasks should be possible. Based on previous results, it is entirely plausible that winning tickets are dependent on labels (i.e., p(y|x) vs. p(x)). \nFurthermore, we argue that, even if these results are expected, confirming these intuitions with rigorous experimentations as we propose in our paper is still important, as noted by R2 (\u201ckind of expected, but it is still good that this paper provide solid experimental results to verify this\u201d). \n\n\u201cI also don't see a practical benefit beyond transfer learning setup.\u201c: As a byproduct, the label-agnostic winning tickets also allow to study the transferability of winning tickets between different tasks, which has a concrete practical benefit. Indeed, similarly to the motivation of Morcos et al., if winning tickets can transfer between tasks, then they can be reused across a variety of problems, thus dispensing the need for generating new winning tickets for each new task.\n\n\u201cgiven that lottery tickets are transferable (Morcos paper) it is really not that surprising\u201d: The datasets used in the paper of Morcos et al. were pretty similar to one another, and the fact that we can transfer between similar supervised tasks does not suggest that we should be able to transfer from self-supervised to supervised. Our work provides insights regarding the dependence of winning tickets on p(x) vs. p(y|x).\n\n\"I was surprised to not see pseudo-labeling or consistency training\": For our semi-supervised experiment, we choose to focus solely on the semi-supervised technique introduced in the paper \"S4L: Self-Supervised Semi-Supervised Learning\" of Zhai et al. (ICCV 2019) because it yields better performance compared to VAT or pseudo-labeling on ImageNet (see Table 1. from their paper).\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1623/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1623/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["mathilde@fb.com", "arimorcos@gmail.com", "bojanowski@fb.com", "julien.mairal@inria.fr", "ajoulin@fb.com"], "title": "Finding Winning Tickets with Limited (or No) Supervision", "authors": ["Mathilde Caron", "Ari Morcos", "Piotr Bojanowski", "Julien Mairal", "Armand Joulin"], "pdf": "/pdf/6ee7308ab20319fbdb516d21984b533672c9c7b7.pdf", "TL;DR": "Finding winning tickets does not require much supervision or data.", "abstract": "The lottery ticket hypothesis argues that neural networks contain sparse subnetworks, which, if appropriately initialized (the winning tickets), are capable of matching the accuracy of the full network when trained in isolation. Empirically made in different contexts, such an observation opens interesting questions about the dynamics of neural network optimization and the importance of their initializations. However, the properties of winning tickets are not well understood, especially the importance of supervision in the generating process. In this paper, we aim to answer the following open questions: can we find winning tickets with few data samples or few labels? can we even obtain good tickets without supervision? Perhaps surprisingly, we provide a positive answer to both, by generating winning tickets with limited access to data, or with self-supervision---thus without using manual annotations---and then demonstrating the transferability of the tickets to challenging classification tasks such as ImageNet.\n", "keywords": ["Lottery Tickets Hypothesis", "Self-Supervised Learning", "Deep Learning", "Image Recognition"], "paperhash": "caron|finding_winning_tickets_with_limited_or_no_supervision", "original_pdf": "/attachment/abcaeba8b5121422f7daebbc1889adacc84ee54d.pdf", "_bibtex": "@misc{\ncaron2020finding,\ntitle={Finding Winning Tickets with Limited (or No) Supervision},\nauthor={Mathilde Caron and Ari Morcos and Piotr Bojanowski and Julien Mairal and Armand Joulin},\nyear={2020},\nurl={https://openreview.net/forum?id=SJx_QJHYDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJx_QJHYDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1623/Authors", "ICLR.cc/2020/Conference/Paper1623/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1623/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1623/Reviewers", "ICLR.cc/2020/Conference/Paper1623/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1623/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1623/Authors|ICLR.cc/2020/Conference/Paper1623/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504153284, "tmdate": 1576860547313, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1623/Authors", "ICLR.cc/2020/Conference/Paper1623/Reviewers", "ICLR.cc/2020/Conference/Paper1623/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1623/-/Official_Comment"}}}, {"id": "BJxzhLG3jr", "original": null, "number": 3, "cdate": 1573820074332, "ddate": null, "tcdate": 1573820074332, "tmdate": 1573820074332, "tddate": null, "forum": "SJx_QJHYDB", "replyto": "B1gdk_E5cH", "invitation": "ICLR.cc/2020/Conference/Paper1623/-/Official_Comment", "content": {"title": "Reply to review #4", "comment": "We thank the reviewer for this positive feedback. We did not experiment on the particular scenario of multi-task learning with limited amount of data; we agree that this is an interesting problem for future work.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1623/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1623/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["mathilde@fb.com", "arimorcos@gmail.com", "bojanowski@fb.com", "julien.mairal@inria.fr", "ajoulin@fb.com"], "title": "Finding Winning Tickets with Limited (or No) Supervision", "authors": ["Mathilde Caron", "Ari Morcos", "Piotr Bojanowski", "Julien Mairal", "Armand Joulin"], "pdf": "/pdf/6ee7308ab20319fbdb516d21984b533672c9c7b7.pdf", "TL;DR": "Finding winning tickets does not require much supervision or data.", "abstract": "The lottery ticket hypothesis argues that neural networks contain sparse subnetworks, which, if appropriately initialized (the winning tickets), are capable of matching the accuracy of the full network when trained in isolation. Empirically made in different contexts, such an observation opens interesting questions about the dynamics of neural network optimization and the importance of their initializations. However, the properties of winning tickets are not well understood, especially the importance of supervision in the generating process. In this paper, we aim to answer the following open questions: can we find winning tickets with few data samples or few labels? can we even obtain good tickets without supervision? Perhaps surprisingly, we provide a positive answer to both, by generating winning tickets with limited access to data, or with self-supervision---thus without using manual annotations---and then demonstrating the transferability of the tickets to challenging classification tasks such as ImageNet.\n", "keywords": ["Lottery Tickets Hypothesis", "Self-Supervised Learning", "Deep Learning", "Image Recognition"], "paperhash": "caron|finding_winning_tickets_with_limited_or_no_supervision", "original_pdf": "/attachment/abcaeba8b5121422f7daebbc1889adacc84ee54d.pdf", "_bibtex": "@misc{\ncaron2020finding,\ntitle={Finding Winning Tickets with Limited (or No) Supervision},\nauthor={Mathilde Caron and Ari Morcos and Piotr Bojanowski and Julien Mairal and Armand Joulin},\nyear={2020},\nurl={https://openreview.net/forum?id=SJx_QJHYDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJx_QJHYDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1623/Authors", "ICLR.cc/2020/Conference/Paper1623/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1623/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1623/Reviewers", "ICLR.cc/2020/Conference/Paper1623/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1623/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1623/Authors|ICLR.cc/2020/Conference/Paper1623/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504153284, "tmdate": 1576860547313, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1623/Authors", "ICLR.cc/2020/Conference/Paper1623/Reviewers", "ICLR.cc/2020/Conference/Paper1623/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1623/-/Official_Comment"}}}, {"id": "SJl5O8GnjS", "original": null, "number": 2, "cdate": 1573820017912, "ddate": null, "tcdate": 1573820017912, "tmdate": 1573820017912, "tddate": null, "forum": "SJx_QJHYDB", "replyto": "ryxRdy869S", "invitation": "ICLR.cc/2020/Conference/Paper1623/-/Official_Comment", "content": {"title": "Reply to review #2", "comment": "We thank the reviewer for this constructive and thoughtful feedback.\n\u201cThis undermines the *bold* claim in the abstract\u201d: The remark about the bold claim is a fair point, and we have updated the paper with this caveat accordingly.\n\n\u201cThis paper raises the issue of ill-definedness of \u201cearly in training\u201d, but did not provide a solution.\u201c: We agree that the fact that we do not provide a solution to the problem of late resetting is slightly disappointing. Yet, this is not the main focus of our paper.\n\n\u201cthe ability to exactly perserve the accuracy while pruning the weights\u201c: We emphasize that our primary aim is to better understand lottery tickets rather than just get good performance. In particular, we are interested in whether the winning ticket initializations derived from data with little or no supervision outperform subnetworks initialized randomly. Our finding that these winning tickets do in fact outperform random tickets suggests that the properties of winning ticket initializations which lead to better optimization are largely independent of labels, and rather mostly rely on p(x) (though we do note, as the reviewer pointed out, that the inclusion of labels does lead to better winning tickets, though not by much). \n\n\u201cI feel that the novelty of this paper is limited, and do not provide much new insights.\u201c\u201d:  Please see our general comment for more detail on the novelty of our work and why the insights we generated are relevant to future work on the lottery ticket hypothesis. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1623/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1623/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["mathilde@fb.com", "arimorcos@gmail.com", "bojanowski@fb.com", "julien.mairal@inria.fr", "ajoulin@fb.com"], "title": "Finding Winning Tickets with Limited (or No) Supervision", "authors": ["Mathilde Caron", "Ari Morcos", "Piotr Bojanowski", "Julien Mairal", "Armand Joulin"], "pdf": "/pdf/6ee7308ab20319fbdb516d21984b533672c9c7b7.pdf", "TL;DR": "Finding winning tickets does not require much supervision or data.", "abstract": "The lottery ticket hypothesis argues that neural networks contain sparse subnetworks, which, if appropriately initialized (the winning tickets), are capable of matching the accuracy of the full network when trained in isolation. Empirically made in different contexts, such an observation opens interesting questions about the dynamics of neural network optimization and the importance of their initializations. However, the properties of winning tickets are not well understood, especially the importance of supervision in the generating process. In this paper, we aim to answer the following open questions: can we find winning tickets with few data samples or few labels? can we even obtain good tickets without supervision? Perhaps surprisingly, we provide a positive answer to both, by generating winning tickets with limited access to data, or with self-supervision---thus without using manual annotations---and then demonstrating the transferability of the tickets to challenging classification tasks such as ImageNet.\n", "keywords": ["Lottery Tickets Hypothesis", "Self-Supervised Learning", "Deep Learning", "Image Recognition"], "paperhash": "caron|finding_winning_tickets_with_limited_or_no_supervision", "original_pdf": "/attachment/abcaeba8b5121422f7daebbc1889adacc84ee54d.pdf", "_bibtex": "@misc{\ncaron2020finding,\ntitle={Finding Winning Tickets with Limited (or No) Supervision},\nauthor={Mathilde Caron and Ari Morcos and Piotr Bojanowski and Julien Mairal and Armand Joulin},\nyear={2020},\nurl={https://openreview.net/forum?id=SJx_QJHYDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJx_QJHYDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1623/Authors", "ICLR.cc/2020/Conference/Paper1623/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1623/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1623/Reviewers", "ICLR.cc/2020/Conference/Paper1623/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1623/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1623/Authors|ICLR.cc/2020/Conference/Paper1623/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504153284, "tmdate": 1576860547313, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1623/Authors", "ICLR.cc/2020/Conference/Paper1623/Reviewers", "ICLR.cc/2020/Conference/Paper1623/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1623/-/Official_Comment"}}}, {"id": "H1gzfIG2or", "original": null, "number": 1, "cdate": 1573819914304, "ddate": null, "tcdate": 1573819914304, "tmdate": 1573819914304, "tddate": null, "forum": "SJx_QJHYDB", "replyto": "SJx_QJHYDB", "invitation": "ICLR.cc/2020/Conference/Paper1623/-/Official_Comment", "content": {"title": "Global comment", "comment": "We thank the reviewers for taking the time to provide detailed and thoughtful comments. This constructive feedback has been helping us improving our submission.\n\nOur contribution is essentially experimental and we were pleased to see that overall, the reviewers found our experimental results to be \u201csolid and provide more understandings of the lottery ticket hypothesis\u201d (R2) and assessed that we have \u201cconducted extensive experiments on three open questions and results prove [ours] assumptions\u201d (R4).\nYet, reviewer 1 is concerned by the robustness of our experimental setup and we address his or her concerns in our reply and in the updated version of the paper.\n\nThe main caveat from the reviewers relates to the lack of novelty (R2: \u201cthe novelty of this paper is limited\u201d; R3: \u201cimmediate followup on Morcos et al.\u201d, \u201cfairly obvious\u201d). They also question the interest and practical value of our study (R2: \u201cdo not provide much new insights\u201d; R3: \u201c I also don't see a practical benefit\u201d).\n\n*Novelty.*\nTo the best of our knowledge, we propose the first study of the lottery ticket hypothesis in the context of limited access to samples and labels*.* Our experiments are fairly extensive: we generate winning tickets on ImageNet for several different settings (2 different self-supervision losses, 4 different sizes of dataset and 4 different number of classes, semi-supervision) at 14 different pruning rates ranging from 20% to 99.9%, thus covering both moderate and extreme sparsity.\nWe are the first paper addressing the lottery ticket hypothesis with a majority of our experiments conducted on ImageNet,\nwhile showing that conclusions on smaller datasets may be misleading. Our experiments show indeed that deep networks trained on CIFAR-10 are naturally sparse, making conclusions potentially incorrect.\n\nMoreover, our findings are different from Morcos et al., who show that winning tickets can transfer between different datasets with a common domain (natural images) trained on the same task (labels classification). The fact that we can transfer between similar supervised tasks does not suggest that we should be able to transfer from self-supervised to supervised tasks. Also, it does not guarantee that winning tickets found with only 10 classes (out of 1000) transfer well to full ImageNet for example. Besides, even if these results were expected somehow, it would still be essential to verify these with rigorous experiments, as we propose in our paper.\n\n\n*Motivation - why does it matter ?*\nIn our submission, we aim to better understand the properties of winning tickets. Indeed, a better understanding of winning ticket properties might enable faster winning ticket generation and thus allow for concrete applications in fields such as network compression or initialization. We propose an extensive series of experiments investigating winning ticket generation with limited access to labels and samples in order to isolate and assess the dependance in p(x) and p(y|x) of the winning tickets.\nAs a byproduct of this design, the label-agnostic winning tickets also allow to study the transferability of winning tickets between different tasks, which has a concrete practical benefit. Indeed, similar to the motivation of Morcos et al., if winning tickets can transfer between tasks, then they can be reused across a variety of problems, thus dispensing the need for generating new winning tickets for each new task.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1623/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1623/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["mathilde@fb.com", "arimorcos@gmail.com", "bojanowski@fb.com", "julien.mairal@inria.fr", "ajoulin@fb.com"], "title": "Finding Winning Tickets with Limited (or No) Supervision", "authors": ["Mathilde Caron", "Ari Morcos", "Piotr Bojanowski", "Julien Mairal", "Armand Joulin"], "pdf": "/pdf/6ee7308ab20319fbdb516d21984b533672c9c7b7.pdf", "TL;DR": "Finding winning tickets does not require much supervision or data.", "abstract": "The lottery ticket hypothesis argues that neural networks contain sparse subnetworks, which, if appropriately initialized (the winning tickets), are capable of matching the accuracy of the full network when trained in isolation. Empirically made in different contexts, such an observation opens interesting questions about the dynamics of neural network optimization and the importance of their initializations. However, the properties of winning tickets are not well understood, especially the importance of supervision in the generating process. In this paper, we aim to answer the following open questions: can we find winning tickets with few data samples or few labels? can we even obtain good tickets without supervision? Perhaps surprisingly, we provide a positive answer to both, by generating winning tickets with limited access to data, or with self-supervision---thus without using manual annotations---and then demonstrating the transferability of the tickets to challenging classification tasks such as ImageNet.\n", "keywords": ["Lottery Tickets Hypothesis", "Self-Supervised Learning", "Deep Learning", "Image Recognition"], "paperhash": "caron|finding_winning_tickets_with_limited_or_no_supervision", "original_pdf": "/attachment/abcaeba8b5121422f7daebbc1889adacc84ee54d.pdf", "_bibtex": "@misc{\ncaron2020finding,\ntitle={Finding Winning Tickets with Limited (or No) Supervision},\nauthor={Mathilde Caron and Ari Morcos and Piotr Bojanowski and Julien Mairal and Armand Joulin},\nyear={2020},\nurl={https://openreview.net/forum?id=SJx_QJHYDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJx_QJHYDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1623/Authors", "ICLR.cc/2020/Conference/Paper1623/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1623/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1623/Reviewers", "ICLR.cc/2020/Conference/Paper1623/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1623/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1623/Authors|ICLR.cc/2020/Conference/Paper1623/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504153284, "tmdate": 1576860547313, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1623/Authors", "ICLR.cc/2020/Conference/Paper1623/Reviewers", "ICLR.cc/2020/Conference/Paper1623/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1623/-/Official_Comment"}}}, {"id": "SkxRhOSTFB", "original": null, "number": 1, "cdate": 1571801270321, "ddate": null, "tcdate": 1571801270321, "tmdate": 1572972444753, "tddate": null, "forum": "SJx_QJHYDB", "replyto": "SJx_QJHYDB", "invitation": "ICLR.cc/2020/Conference/Paper1623/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper studies the problem of finding sparse networks in a limited supervision setup. The authors build on the lottery ticket work of Frankle & Carbin and investigate the validity of their idea when one has few or no labels. This work is an immediate followup on Morcos et al. who investigated the transferability of lottery tickets.\n\nThis work is more observational rather than algorithmic or theoretical. Authors study various small sample/label setups where network sparsification works well. \n\nMain contribution is Section 4.1 where self-supervision is investigated. However given that lottery tickets are transferable (Morcos paper) it is really not that surprising that semisupervised learning algorithms will do a decent job as well. I also don't see a practical benefit beyond transfer learning setup.\n\nSection 4.2 essentially sweeps through supervised problem parameters such as reducing sample size, adding noise etc and . The main application seems to be extracting lottery tickets faster by downsampling the data however this aspect is again fairly obvious. \n\nIn short, unfortunately, this paper doesn't cut it for ICLR. As improvements, I would recommend adding standard semi-supervised training techniques to their comparison. I was surprised to not see pseudo-labeling or consistency training (e.g. virtual adversarial training)."}, "signatures": ["ICLR.cc/2020/Conference/Paper1623/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1623/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["mathilde@fb.com", "arimorcos@gmail.com", "bojanowski@fb.com", "julien.mairal@inria.fr", "ajoulin@fb.com"], "title": "Finding Winning Tickets with Limited (or No) Supervision", "authors": ["Mathilde Caron", "Ari Morcos", "Piotr Bojanowski", "Julien Mairal", "Armand Joulin"], "pdf": "/pdf/6ee7308ab20319fbdb516d21984b533672c9c7b7.pdf", "TL;DR": "Finding winning tickets does not require much supervision or data.", "abstract": "The lottery ticket hypothesis argues that neural networks contain sparse subnetworks, which, if appropriately initialized (the winning tickets), are capable of matching the accuracy of the full network when trained in isolation. Empirically made in different contexts, such an observation opens interesting questions about the dynamics of neural network optimization and the importance of their initializations. However, the properties of winning tickets are not well understood, especially the importance of supervision in the generating process. In this paper, we aim to answer the following open questions: can we find winning tickets with few data samples or few labels? can we even obtain good tickets without supervision? Perhaps surprisingly, we provide a positive answer to both, by generating winning tickets with limited access to data, or with self-supervision---thus without using manual annotations---and then demonstrating the transferability of the tickets to challenging classification tasks such as ImageNet.\n", "keywords": ["Lottery Tickets Hypothesis", "Self-Supervised Learning", "Deep Learning", "Image Recognition"], "paperhash": "caron|finding_winning_tickets_with_limited_or_no_supervision", "original_pdf": "/attachment/abcaeba8b5121422f7daebbc1889adacc84ee54d.pdf", "_bibtex": "@misc{\ncaron2020finding,\ntitle={Finding Winning Tickets with Limited (or No) Supervision},\nauthor={Mathilde Caron and Ari Morcos and Piotr Bojanowski and Julien Mairal and Armand Joulin},\nyear={2020},\nurl={https://openreview.net/forum?id=SJx_QJHYDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SJx_QJHYDB", "replyto": "SJx_QJHYDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1623/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1623/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575522098786, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1623/Reviewers"], "noninvitees": [], "tcdate": 1570237734687, "tmdate": 1575522098802, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1623/-/Official_Review"}}}, {"id": "SJeuh5z19B", "original": null, "number": 2, "cdate": 1571920560406, "ddate": null, "tcdate": 1571920560406, "tmdate": 1572972444712, "tddate": null, "forum": "SJx_QJHYDB", "replyto": "SJx_QJHYDB", "invitation": "ICLR.cc/2020/Conference/Paper1623/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "In the original lottery ticket paper, it points out that training the pruned architecture from scratch with initial weight can achieve the same performance compared to fine-tuning it. This work further discuss this phenomenon when data or label is not enough. It is good to see the few-data/label can still provide a comparable results. But some experiment\u2019s results and its setting are confusing, while also makes me concerned about the conclusion solidness.\n\n1) Usually in classification task (especially in cifar10 dataset), 0.5% to 1% accuracy could be a huge gap between two models. For example, in the original \u201cLottery Ticket Hypothesis\u201d paper, using initial weight only has roughly 0.5% improvement compared to random initialization. But the figures in this paper do not contain a zoom-in details for each line, make me hard to distinguish the performance between each setting. If the author does not provide a detailed version, it will look like theses model have the same performance, which is actually wrong. The author should either plot a zoom-in figure especially when the pruning ratio larger than 50% or give a Table with accuracy of each setting. And it is better to complete the figure with several random seed and plot the error bar to avoid randomness.\n\n2) Does the \u201cRandom - adjusted\u201d item in Figure. 1 mean the correctly pruning architecture with random initialization? In \"rethinking the value of network pruning\", Liu et al. points that in the large learning rate setting (lr=0.1, which is also your setting), random initialization can achieve the same performance compared to the lottery ticket. In my perspective, I want to see whether few-data/label also works on random initialization instead of lottery tickets. I expect the author to explain the \u201cRandom -adjusted\u201d experiment setting clearly in the response and I suggest the author to discuss the\nrandom initialization part specifically.\n\n3) Figure.3 only shows the \u201cvarying dataset size\u201d experiments on ImageNet. The experiments on cifar10 is lacked. The author should complete this part in the response."}, "signatures": ["ICLR.cc/2020/Conference/Paper1623/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1623/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["mathilde@fb.com", "arimorcos@gmail.com", "bojanowski@fb.com", "julien.mairal@inria.fr", "ajoulin@fb.com"], "title": "Finding Winning Tickets with Limited (or No) Supervision", "authors": ["Mathilde Caron", "Ari Morcos", "Piotr Bojanowski", "Julien Mairal", "Armand Joulin"], "pdf": "/pdf/6ee7308ab20319fbdb516d21984b533672c9c7b7.pdf", "TL;DR": "Finding winning tickets does not require much supervision or data.", "abstract": "The lottery ticket hypothesis argues that neural networks contain sparse subnetworks, which, if appropriately initialized (the winning tickets), are capable of matching the accuracy of the full network when trained in isolation. Empirically made in different contexts, such an observation opens interesting questions about the dynamics of neural network optimization and the importance of their initializations. However, the properties of winning tickets are not well understood, especially the importance of supervision in the generating process. In this paper, we aim to answer the following open questions: can we find winning tickets with few data samples or few labels? can we even obtain good tickets without supervision? Perhaps surprisingly, we provide a positive answer to both, by generating winning tickets with limited access to data, or with self-supervision---thus without using manual annotations---and then demonstrating the transferability of the tickets to challenging classification tasks such as ImageNet.\n", "keywords": ["Lottery Tickets Hypothesis", "Self-Supervised Learning", "Deep Learning", "Image Recognition"], "paperhash": "caron|finding_winning_tickets_with_limited_or_no_supervision", "original_pdf": "/attachment/abcaeba8b5121422f7daebbc1889adacc84ee54d.pdf", "_bibtex": "@misc{\ncaron2020finding,\ntitle={Finding Winning Tickets with Limited (or No) Supervision},\nauthor={Mathilde Caron and Ari Morcos and Piotr Bojanowski and Julien Mairal and Armand Joulin},\nyear={2020},\nurl={https://openreview.net/forum?id=SJx_QJHYDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SJx_QJHYDB", "replyto": "SJx_QJHYDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1623/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1623/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575522098786, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1623/Reviewers"], "noninvitees": [], "tcdate": 1570237734687, "tmdate": 1575522098802, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1623/-/Official_Review"}}}, {"id": "B1gdk_E5cH", "original": null, "number": 3, "cdate": 1572648927694, "ddate": null, "tcdate": 1572648927694, "tmdate": 1572972444669, "tddate": null, "forum": "SJx_QJHYDB", "replyto": "SJx_QJHYDB", "invitation": "ICLR.cc/2020/Conference/Paper1623/-/Official_Review", "content": {"rating": "6: Weak Accept", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review": "In this paper, the authors try to provide empirical answers to several important open questions on winning tickets. They conduct most of experiments on ImageNet and results show that winning ticket is robust, and few data samples can also obtain good winning tickets.\n\nGenerally, the paper has conducted extensive experiments on three open questions and results prove their assumptions.\n\nAs describe in page 7, lottery tickets are sensitive to data distributions. I\u2019m wondering, whether there will be winning ticket for multi-task learning with limited data each task? Will this be helpful in distilling the model?"}, "signatures": ["ICLR.cc/2020/Conference/Paper1623/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1623/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["mathilde@fb.com", "arimorcos@gmail.com", "bojanowski@fb.com", "julien.mairal@inria.fr", "ajoulin@fb.com"], "title": "Finding Winning Tickets with Limited (or No) Supervision", "authors": ["Mathilde Caron", "Ari Morcos", "Piotr Bojanowski", "Julien Mairal", "Armand Joulin"], "pdf": "/pdf/6ee7308ab20319fbdb516d21984b533672c9c7b7.pdf", "TL;DR": "Finding winning tickets does not require much supervision or data.", "abstract": "The lottery ticket hypothesis argues that neural networks contain sparse subnetworks, which, if appropriately initialized (the winning tickets), are capable of matching the accuracy of the full network when trained in isolation. Empirically made in different contexts, such an observation opens interesting questions about the dynamics of neural network optimization and the importance of their initializations. However, the properties of winning tickets are not well understood, especially the importance of supervision in the generating process. In this paper, we aim to answer the following open questions: can we find winning tickets with few data samples or few labels? can we even obtain good tickets without supervision? Perhaps surprisingly, we provide a positive answer to both, by generating winning tickets with limited access to data, or with self-supervision---thus without using manual annotations---and then demonstrating the transferability of the tickets to challenging classification tasks such as ImageNet.\n", "keywords": ["Lottery Tickets Hypothesis", "Self-Supervised Learning", "Deep Learning", "Image Recognition"], "paperhash": "caron|finding_winning_tickets_with_limited_or_no_supervision", "original_pdf": "/attachment/abcaeba8b5121422f7daebbc1889adacc84ee54d.pdf", "_bibtex": "@misc{\ncaron2020finding,\ntitle={Finding Winning Tickets with Limited (or No) Supervision},\nauthor={Mathilde Caron and Ari Morcos and Piotr Bojanowski and Julien Mairal and Armand Joulin},\nyear={2020},\nurl={https://openreview.net/forum?id=SJx_QJHYDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SJx_QJHYDB", "replyto": "SJx_QJHYDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1623/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1623/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575522098786, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1623/Reviewers"], "noninvitees": [], "tcdate": 1570237734687, "tmdate": 1575522098802, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1623/-/Official_Review"}}}, {"id": "ryxRdy869S", "original": null, "number": 4, "cdate": 1572851574375, "ddate": null, "tcdate": 1572851574375, "tmdate": 1572972444628, "tddate": null, "forum": "SJx_QJHYDB", "replyto": "SJx_QJHYDB", "invitation": "ICLR.cc/2020/Conference/Paper1623/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper empirically studies the lottery ticket hypothesis with limited or no supervision. First, the authors use self-supervised learning to generate winning tickets, showing that \"good\" (reasonable) winning tickets can be found without labels. Second, the authors show that finding \"good\" (reasonable) winning tickets can be accelerated by a factor 5 on ImageNet by using only a subset of the data. The authors also argue that using large datasets is important to study lottery tickets, since deep networks trained on CIFAR-10 are natually sparse, making conclusions potentially misleading.\n\nThe experimental results are rich and provide more understanding of winning ticket generation with limited or no supervision. The results on self-supervised learning task (including the layer-wise pruning results) and a subset of training dataset are reasonable and kind of expected, but it is still good that this paper provide solid experimental results to verify this. As the paper observed, \"none of the tickets found with limited access to labels and or data matches the accuracy of tickets found with all the labeled data when considering moderate pruning rates (more than 10% of unpruned weights)\non ImageNet. Indeed, we consistently observe a decrease in performance compared to the full overparametrized network as soon as we prune the network.\" In this sense, winning tickets are certainly label and data dependant. This undermines the *bold* claim in the abstract that \"we provide a positive answer to both questions, by generating winning tickets with limited access to data, or with self-supervision\". From my perspective, the ability to exactly perserve the accuracy while pruning the weights (see the flat regions of \"Lables\" curves in Figure 1,2,3,4,5) is the interesting part of the lottery ticket hypothesis. We have several different ways to achieve a descreased accuracy with a smaller network, the dynamics there may be a mixture of the lottery ticket hypothesis and standard model pruning, which needs more careful experiment design to separate different dynamics.\n\n\"using large datasets is important to study lottery tickets, since deep networks trained on CIFAR-10 are natually sparse, making conclusions potentially misleading.\" \"The definition of \u201cearly in training\u201d is somehow ill-defined: network\nweights change much more for the first epochs than for the last ones.\" These two messages are important to future study of the lottery ticket hypothesis. This paper raises the issue of ill-definedness of \u201cearly in training\u201d, but did not provide a solution. \n\nOverall, I found that the experimental results in this paper are solid and provide more understandings of the lottery ticket hypothesis. However, I feel that the novelty of this paper is limited, and do not provide much new insights. Therefore, it does not reach the bar of being published at ICLR, from my perspective. Therefore, I say \"Weak Reject\"."}, "signatures": ["ICLR.cc/2020/Conference/Paper1623/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1623/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["mathilde@fb.com", "arimorcos@gmail.com", "bojanowski@fb.com", "julien.mairal@inria.fr", "ajoulin@fb.com"], "title": "Finding Winning Tickets with Limited (or No) Supervision", "authors": ["Mathilde Caron", "Ari Morcos", "Piotr Bojanowski", "Julien Mairal", "Armand Joulin"], "pdf": "/pdf/6ee7308ab20319fbdb516d21984b533672c9c7b7.pdf", "TL;DR": "Finding winning tickets does not require much supervision or data.", "abstract": "The lottery ticket hypothesis argues that neural networks contain sparse subnetworks, which, if appropriately initialized (the winning tickets), are capable of matching the accuracy of the full network when trained in isolation. Empirically made in different contexts, such an observation opens interesting questions about the dynamics of neural network optimization and the importance of their initializations. However, the properties of winning tickets are not well understood, especially the importance of supervision in the generating process. In this paper, we aim to answer the following open questions: can we find winning tickets with few data samples or few labels? can we even obtain good tickets without supervision? Perhaps surprisingly, we provide a positive answer to both, by generating winning tickets with limited access to data, or with self-supervision---thus without using manual annotations---and then demonstrating the transferability of the tickets to challenging classification tasks such as ImageNet.\n", "keywords": ["Lottery Tickets Hypothesis", "Self-Supervised Learning", "Deep Learning", "Image Recognition"], "paperhash": "caron|finding_winning_tickets_with_limited_or_no_supervision", "original_pdf": "/attachment/abcaeba8b5121422f7daebbc1889adacc84ee54d.pdf", "_bibtex": "@misc{\ncaron2020finding,\ntitle={Finding Winning Tickets with Limited (or No) Supervision},\nauthor={Mathilde Caron and Ari Morcos and Piotr Bojanowski and Julien Mairal and Armand Joulin},\nyear={2020},\nurl={https://openreview.net/forum?id=SJx_QJHYDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SJx_QJHYDB", "replyto": "SJx_QJHYDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1623/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1623/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575522098786, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1623/Reviewers"], "noninvitees": [], "tcdate": 1570237734687, "tmdate": 1575522098802, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1623/-/Official_Review"}}}], "count": 11}