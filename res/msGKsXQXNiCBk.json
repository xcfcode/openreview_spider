{"notes": [{"ddate": null, "legacy_migration": true, "tmdate": 1363419120000, "tcdate": 1363419120000, "number": 4, "id": "7jyp7wrwSzagb", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "msGKsXQXNiCBk", "replyto": "msGKsXQXNiCBk", "signatures": ["Danqi Chen, Richard Socher, Christopher D. Manning, Andrew Y. Ng"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "We thank the reviewers for their comments and agree with most of them.\r\n\r\n- We've updated our paper on arxiv, and added the important experimental comparison to the model in 'Joint Learning of Words and Meaning Representations for Open-Text Semantic Parsing' (AISTATS 2012). \r\n  Experimental results show that our model also outperforms this model in terms of ranking & classification.\r\n\r\n- We didn't report the results on the original data because of the issues of overlap between training and testing set. \r\n  80.23% of the examples in the testing set appear exactly in the training set.\r\n  99.23% of the examples have e1 and e2 'connected' via some relation in the training set. Some relationships such as 'is similar to' are symmetric. \r\n  Furthermore, we can reach 92.8% of top 10 accuracy (instead of 76.7% in the original paper) using their model.\r\n\r\n- The classification task can help us predict whether a relationship is correct or not, thus we report both the results of classification and ranking. \r\n\r\n- To use the pre-trained word vectors, we ignore the senses of the entities in Wordnet in this paper. \r\n\r\n- The experiments section is short because we tried to keep the paper's length close to the recommended length. From the ICLR website: 'Papers submitted to this track are ideally 2-3 pages long'."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning New Facts From Knowledge Bases With Neural Tensor Networks and\r\n      Semantic Word Vectors", "decision": "conferencePoster-iclr2013-workshop", "abstract": "Knowledge bases provide applications with the benefit of easily accessible, systematic relational knowledge but often suffer in practice from their incompleteness and lack of knowledge of new entities and relations. Much work has focused on building or extending them by finding patterns in large unannotated text corpora. In contrast, here we mainly aim to complete a knowledge base by predicting additional true relationships between entities, based on generalizations that can be discerned in the given knowledgebase. We introduce a neural tensor network (NTN) model which predicts new relationship entries that can be added to the database. This model can be improved by initializing entity representations with word vectors learned in an unsupervised fashion from text, and when doing this, existing relations can even be queried for entities that were not present in the database. Our model generalizes and outperforms existing models for this problem, and can classify unseen relationships in WordNet with an accuracy of 82.8%.", "pdf": "https://arxiv.org/abs/1301.3618", "paperhash": "chen|learning_new_facts_from_knowledge_bases_with_neural_tensor_networks_and_semantic_word_vectors", "keywords": [], "conflicts": [], "authors": ["Danqi Chen", "Richard Socher", "Christopher Manning", "Andrew Y. Ng"], "authorids": ["danqi@stanford.edu", "richard@socher.org", "manning@stanford.edu", "ang@stanford.edu"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1363419120000, "tcdate": 1363419120000, "number": 3, "id": "OgesTW8qZ5TWn", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "msGKsXQXNiCBk", "replyto": "msGKsXQXNiCBk", "signatures": ["Danqi Chen, Richard Socher, Christopher D. Manning, Andrew Y. Ng"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "We thank the reviewers for their comments and agree with most of them.\r\n\r\n- We've updated our paper on arxiv, and added the important experimental comparison to the model in 'Joint Learning of Words and Meaning Representations for Open-Text Semantic Parsing' (AISTATS 2012). \r\n  Experimental results show that our model also outperforms this model in terms of ranking & classification.\r\n\r\n- We didn't report the results on the original data because of the issues of overlap between training and testing set. \r\n  80.23% of the examples in the testing set appear exactly in the training set.\r\n  99.23% of the examples have e1 and e2 'connected' via some relation in the training set. Some relationships such as 'is similar to' are symmetric. \r\n  Furthermore, we can reach 92.8% of top 10 accuracy (instead of 76.7% in the original paper) using their model.\r\n\r\n- The classification task can help us predict whether a relationship is correct or not, thus we report both the results of classification and ranking. \r\n\r\n- To use the pre-trained word vectors, we ignore the senses of the entities in Wordnet in this paper. \r\n\r\n- The experiments section is short because we tried to keep the paper's length close to the recommended length. From the ICLR website: 'Papers submitted to this track are ideally 2-3 pages long'."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning New Facts From Knowledge Bases With Neural Tensor Networks and\r\n      Semantic Word Vectors", "decision": "conferencePoster-iclr2013-workshop", "abstract": "Knowledge bases provide applications with the benefit of easily accessible, systematic relational knowledge but often suffer in practice from their incompleteness and lack of knowledge of new entities and relations. Much work has focused on building or extending them by finding patterns in large unannotated text corpora. In contrast, here we mainly aim to complete a knowledge base by predicting additional true relationships between entities, based on generalizations that can be discerned in the given knowledgebase. We introduce a neural tensor network (NTN) model which predicts new relationship entries that can be added to the database. This model can be improved by initializing entity representations with word vectors learned in an unsupervised fashion from text, and when doing this, existing relations can even be queried for entities that were not present in the database. Our model generalizes and outperforms existing models for this problem, and can classify unseen relationships in WordNet with an accuracy of 82.8%.", "pdf": "https://arxiv.org/abs/1301.3618", "paperhash": "chen|learning_new_facts_from_knowledge_bases_with_neural_tensor_networks_and_semantic_word_vectors", "keywords": [], "conflicts": [], "authors": ["Danqi Chen", "Richard Socher", "Christopher Manning", "Andrew Y. Ng"], "authorids": ["danqi@stanford.edu", "richard@socher.org", "manning@stanford.edu", "ang@stanford.edu"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362246000000, "tcdate": 1362246000000, "number": 2, "id": "yA-tyFEFr2A5u", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "msGKsXQXNiCBk", "replyto": "msGKsXQXNiCBk", "signatures": ["anonymous reviewer 7e51"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Learning New Facts From Knowledge Bases With Neural Tensor Networks and\r\n      Semantic Word Vectors", "review": "This paper proposes a new model for modeling data of multi-relational knowledge bases such as Wordnet or YAGO. Inspired by the work of (Bordes et al., AAAI11), they propose a neural network-based scoring function, which is trained to assign high score to plausible relations. Evaluation is performed on Wordnet.\r\n\r\nThe main differences w.r.t. (Bordes et al., AAAI11) is the scoring function, which now involves a tensor product to encode for the relation type and the use of a non-linearity. It would be interesting if the authors could comment the motivations of their architecture. For instance, what does the tanh could model here?\r\n\r\nThe experiments raise some questions:\r\n- why do not also report the results on the original data set of (Bordes et al., AAAI11)? Even, is the data set contains duplicates, this stills makes a reference point.\r\n- the classification task is hard to motivate. Link prediction is a problem of detection: very few positive to find in huge set of negative examples. Transform that into a balanced classification problem is a non-sense to me.\r\n\r\nThere have been several follow-up works to (Bordes et al., AAAI11) such as (Bordes et al., AISTATS12) or (Jenatton et al., NIPS12), that should be cited and discussed (some of those involve tensor for coding the relation type as well). Besides, they would also make the experimental comparison stronger.\r\n\r\nIt should be explained how the pre-trained word vectors trained by the model of Collobert & Weston are use in the model. Wordnet entities are senses and not words and, of course, there is no direct mapping from words to senses. Which heuristic has been used?\r\n\r\nPros:\r\n- better experimental results\r\n\r\nCons:\r\n- skinny experimental section\r\n- lack of recent references"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning New Facts From Knowledge Bases With Neural Tensor Networks and\r\n      Semantic Word Vectors", "decision": "conferencePoster-iclr2013-workshop", "abstract": "Knowledge bases provide applications with the benefit of easily accessible, systematic relational knowledge but often suffer in practice from their incompleteness and lack of knowledge of new entities and relations. Much work has focused on building or extending them by finding patterns in large unannotated text corpora. In contrast, here we mainly aim to complete a knowledge base by predicting additional true relationships between entities, based on generalizations that can be discerned in the given knowledgebase. We introduce a neural tensor network (NTN) model which predicts new relationship entries that can be added to the database. This model can be improved by initializing entity representations with word vectors learned in an unsupervised fashion from text, and when doing this, existing relations can even be queried for entities that were not present in the database. Our model generalizes and outperforms existing models for this problem, and can classify unseen relationships in WordNet with an accuracy of 82.8%.", "pdf": "https://arxiv.org/abs/1301.3618", "paperhash": "chen|learning_new_facts_from_knowledge_bases_with_neural_tensor_networks_and_semantic_word_vectors", "keywords": [], "conflicts": [], "authors": ["Danqi Chen", "Richard Socher", "Christopher Manning", "Andrew Y. Ng"], "authorids": ["danqi@stanford.edu", "richard@socher.org", "manning@stanford.edu", "ang@stanford.edu"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362079260000, "tcdate": 1362079260000, "number": 1, "id": "PnfD3BSBKbnZh", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "msGKsXQXNiCBk", "replyto": "msGKsXQXNiCBk", "signatures": ["anonymous reviewer 75b8"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Learning New Facts From Knowledge Bases With Neural Tensor Networks and\r\n      Semantic Word Vectors", "review": "- A brief summary of the paper's contributions, in the context of prior work.\r\n\r\nThis paper proposes a new energy function (or scoring function) for ranking pairs of entities and their relationship type. The energy function is based on a so-called Neural Tensor Network, which essentially introduces a bilinear term in the computation of the hidden layer input activations of a single hidden layer neural network. A favorable comparison with the energy-function proposed in Bordes et al. 2011 is presented.\r\n\r\n- An assessment of novelty and quality.\r\n\r\nThis work follows fairly closely the work of Border et al. 2011, with the main difference being the choice of the energy/scoring function. This is an advantage in terms of the interpretability of the results: this paper clearly demonstrates that the proposed energy function is better, since everything else (the training objective, the evaluation procedure) is the same. This is however a disadvantage in terms of novelty as this makes this work somewhat incremental.\r\n\r\nBordes et al. 2011 also proposed an improved version of their model, using kernel density estimation, which is not used here. However, I suppose that the proposed model in this paper could also be similarly improved.\r\n\r\nMore importantly, Bordes and collaborators have more recently looked at another type of energy function, in 'Joint Learning of Words and Meaning Representations for Open-Text Semantic Parsing' (AISTATS 2012), which also involves bilinear terms and is thus similar (but not the same) as the proposed energy function here. In fact, the Bordes et al. 2012 energy function seems to outperform the 2011 one (without KDE), hence I would argue that the former would have been a better baseline for comparisons.\r\n\r\n- A list of pros and cons (reasons to accept/reject).\r\n\r\nPros: Clear demonstration of the superiority of the proposed energy function over that of Bordes et al. 2011.\r\n\r\nCons: No comparison with the more recent energy function of Bordes et al. 2012, which has some similarities to the proposed Neural Tensor Networks.\r\n\r\nSince this was submitted to the workshop track, I would be inclined to have this paper accepted still. This is clearly work in progress (the submitted paper is only 4 pages long), and I think this line of work should be encouraged. However, I would suggest the authors also perform a comparison with the scoring function of Bordes et al. 2012 in future work, using their current protocol (which is nicely setup so as to thoroughly compare energy functions)."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning New Facts From Knowledge Bases With Neural Tensor Networks and\r\n      Semantic Word Vectors", "decision": "conferencePoster-iclr2013-workshop", "abstract": "Knowledge bases provide applications with the benefit of easily accessible, systematic relational knowledge but often suffer in practice from their incompleteness and lack of knowledge of new entities and relations. Much work has focused on building or extending them by finding patterns in large unannotated text corpora. In contrast, here we mainly aim to complete a knowledge base by predicting additional true relationships between entities, based on generalizations that can be discerned in the given knowledgebase. We introduce a neural tensor network (NTN) model which predicts new relationship entries that can be added to the database. This model can be improved by initializing entity representations with word vectors learned in an unsupervised fashion from text, and when doing this, existing relations can even be queried for entities that were not present in the database. Our model generalizes and outperforms existing models for this problem, and can classify unseen relationships in WordNet with an accuracy of 82.8%.", "pdf": "https://arxiv.org/abs/1301.3618", "paperhash": "chen|learning_new_facts_from_knowledge_bases_with_neural_tensor_networks_and_semantic_word_vectors", "keywords": [], "conflicts": [], "authors": ["Danqi Chen", "Richard Socher", "Christopher Manning", "Andrew Y. Ng"], "authorids": ["danqi@stanford.edu", "richard@socher.org", "manning@stanford.edu", "ang@stanford.edu"]}, "tags": [], "invitation": {}}}, {"replyto": null, "ddate": null, "legacy_migration": true, "tmdate": 1358425800000, "tcdate": 1358425800000, "number": 67, "id": "msGKsXQXNiCBk", "invitation": "ICLR.cc/2013/conference/-/submission", "forum": "msGKsXQXNiCBk", "signatures": ["danqi@stanford.edu"], "readers": ["everyone"], "content": {"title": "Learning New Facts From Knowledge Bases With Neural Tensor Networks and\r\n      Semantic Word Vectors", "decision": "conferencePoster-iclr2013-workshop", "abstract": "Knowledge bases provide applications with the benefit of easily accessible, systematic relational knowledge but often suffer in practice from their incompleteness and lack of knowledge of new entities and relations. Much work has focused on building or extending them by finding patterns in large unannotated text corpora. In contrast, here we mainly aim to complete a knowledge base by predicting additional true relationships between entities, based on generalizations that can be discerned in the given knowledgebase. We introduce a neural tensor network (NTN) model which predicts new relationship entries that can be added to the database. This model can be improved by initializing entity representations with word vectors learned in an unsupervised fashion from text, and when doing this, existing relations can even be queried for entities that were not present in the database. Our model generalizes and outperforms existing models for this problem, and can classify unseen relationships in WordNet with an accuracy of 82.8%.", "pdf": "https://arxiv.org/abs/1301.3618", "paperhash": "chen|learning_new_facts_from_knowledge_bases_with_neural_tensor_networks_and_semantic_word_vectors", "keywords": [], "conflicts": [], "authors": ["Danqi Chen", "Richard Socher", "Christopher Manning", "Andrew Y. Ng"], "authorids": ["danqi@stanford.edu", "richard@socher.org", "manning@stanford.edu", "ang@stanford.edu"]}, "writers": [], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1369422751717, "tmdate": 1496673673639, "cdate": 1496673673639, "tcdate": 1496673673639, "id": "ICLR.cc/2013/conference/-/submission", "writers": ["ICLR.cc/2013"], "signatures": ["OpenReview.net"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": []}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1377198751717}}}], "count": 5}