{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1493476840339, "tcdate": 1487366083830, "number": 138, "id": "B1naD1rFx", "invitation": "ICLR.cc/2017/workshop/-/submission", "forum": "B1naD1rFx", "signatures": ["~Shikhar_Sharma2"], "readers": ["everyone"], "content": {"TL;DR": "", "title": "Natural Language Generation in Dialogue using Lexicalized and Delexicalized Data", "abstract": "Natural language generation plays a critical role in spoken dialogue systems. We present a new approach to natural language generation for task-oriented dialogue using recurrent neural networks in an encoder-decoder framework. In contrast to previous work, our model uses both lexicalized and delexicalized components i.e. slot-value pairs for dialogue acts, with slots and corresponding values aligned together. This allows our model to learn from all available data including the slot-value pairing, rather than being restricted to delexicalized slots. We show that this helps our model generate more natural sentences with better grammar. We further improve our model's performance by transferring weights learnt from a pretrained sentence auto-encoder. Human evaluation of our best-performing model indicates that it generates sentences which users find more appealing.", "pdf": "https://arxiv.org/pdf/1606.03632.pdf", "paperhash": "sharma|natural_language_generation_in_dialogue_using_lexicalized_and_delexicalized_data", "keywords": ["Natural language processing", "Deep learning", "Transfer Learning"], "conflicts": ["maluuba.com", "microsoft.com"], "authors": ["Shikhar Sharma", "Jing He", "Kaheer Suleman", "Hannes Schulz", "Philip Bachman"], "authorids": ["shikhar.sharma@microsoft.com", "peaceful.he@gmail.com", "kaheer.suleman@microsoft.com", "hannes.schulz@microsoft.com", "phbachma@microsoft.com"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 3, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1487690420000, "tmdate": 1484242559574, "id": "ICLR.cc/2017/workshop/-/submission", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1495466420000, "cdate": 1484242559574}}}, {"tddate": null, "ddate": null, "cdate": null, "original": null, "tmdate": 1490028624193, "tcdate": 1490028624193, "number": 1, "id": "SJO8_F6jx", "invitation": "ICLR.cc/2017/workshop/-/paper138/acceptance", "forum": "B1naD1rFx", "replyto": "B1naD1rFx", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Accept", "title": "ICLR committee final decision"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Natural Language Generation in Dialogue using Lexicalized and Delexicalized Data", "abstract": "Natural language generation plays a critical role in spoken dialogue systems. We present a new approach to natural language generation for task-oriented dialogue using recurrent neural networks in an encoder-decoder framework. In contrast to previous work, our model uses both lexicalized and delexicalized components i.e. slot-value pairs for dialogue acts, with slots and corresponding values aligned together. This allows our model to learn from all available data including the slot-value pairing, rather than being restricted to delexicalized slots. We show that this helps our model generate more natural sentences with better grammar. We further improve our model's performance by transferring weights learnt from a pretrained sentence auto-encoder. Human evaluation of our best-performing model indicates that it generates sentences which users find more appealing.", "pdf": "https://arxiv.org/pdf/1606.03632.pdf", "paperhash": "sharma|natural_language_generation_in_dialogue_using_lexicalized_and_delexicalized_data", "keywords": ["Natural language processing", "Deep learning", "Transfer Learning"], "conflicts": ["maluuba.com", "microsoft.com"], "authors": ["Shikhar Sharma", "Jing He", "Kaheer Suleman", "Hannes Schulz", "Philip Bachman"], "authorids": ["shikhar.sharma@microsoft.com", "peaceful.he@gmail.com", "kaheer.suleman@microsoft.com", "hannes.schulz@microsoft.com", "phbachma@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1490028624778, "id": "ICLR.cc/2017/workshop/-/paper138/acceptance", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "B1naD1rFx", "replyto": "B1naD1rFx", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept", "Reject"]}}}, "nonreaders": [], "cdate": 1490028624778}}}, {"tddate": null, "tmdate": 1489496474766, "tcdate": 1489496474766, "number": 2, "id": "BJ7itvBjg", "invitation": "ICLR.cc/2017/workshop/-/paper138/official/review", "forum": "B1naD1rFx", "replyto": "B1naD1rFx", "signatures": ["ICLR.cc/2017/workshop/paper138/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper138/AnonReviewer2"], "content": {"title": "an incremental improvement", "rating": "6: Marginally above acceptance threshold", "review": "In this paper, the authors extend Semantically Conditioned LSTMs [Wen et al, EMNLP`15] for dialogue generation from slot-value pairs. The main extensions are:\n(a) They allow lexical values in addition to categorical values for slots. They model the lexical values by the average pooling of their word embeddings and concatenate them with the one-hot representation for slot-ids to form the encoder's input.\n(b) They use pre-trained weights for the decoder LSTM based on an auto-encoder trained on sentences from the same domain.\n \nTheir experiments show that both lexical value modeling as well as transfer learning improve performance on all metrics on two different datasets although the improvements are not statistically significant most of the time. Example sentences produced by various models indicate that the proposed models are qualitatively better.\n\nThe paper lacks novelty -- it seems an incremental improvement to Semantically Conditioned LSTMs. \n \nMinor suggestion: For reasons of completeness, please show how the dialogue act vector (d_0) influences the decoder-LSTM. Although Wen et al is cited, it still makes sense to display it since it is an important component of the model.\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Natural Language Generation in Dialogue using Lexicalized and Delexicalized Data", "abstract": "Natural language generation plays a critical role in spoken dialogue systems. We present a new approach to natural language generation for task-oriented dialogue using recurrent neural networks in an encoder-decoder framework. In contrast to previous work, our model uses both lexicalized and delexicalized components i.e. slot-value pairs for dialogue acts, with slots and corresponding values aligned together. This allows our model to learn from all available data including the slot-value pairing, rather than being restricted to delexicalized slots. We show that this helps our model generate more natural sentences with better grammar. We further improve our model's performance by transferring weights learnt from a pretrained sentence auto-encoder. Human evaluation of our best-performing model indicates that it generates sentences which users find more appealing.", "pdf": "https://arxiv.org/pdf/1606.03632.pdf", "paperhash": "sharma|natural_language_generation_in_dialogue_using_lexicalized_and_delexicalized_data", "keywords": ["Natural language processing", "Deep learning", "Transfer Learning"], "conflicts": ["maluuba.com", "microsoft.com"], "authors": ["Shikhar Sharma", "Jing He", "Kaheer Suleman", "Hannes Schulz", "Philip Bachman"], "authorids": ["shikhar.sharma@microsoft.com", "peaceful.he@gmail.com", "kaheer.suleman@microsoft.com", "hannes.schulz@microsoft.com", "phbachma@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489496475408, "id": "ICLR.cc/2017/workshop/-/paper138/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper138/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper138/AnonReviewer1", "ICLR.cc/2017/workshop/paper138/AnonReviewer2"], "reply": {"forum": "B1naD1rFx", "replyto": "B1naD1rFx", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper138/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper138/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489496475408}}}, {"tddate": null, "tmdate": 1489194278319, "tcdate": 1489194278319, "number": 1, "id": "H1AmTalsx", "invitation": "ICLR.cc/2017/workshop/-/paper138/official/review", "forum": "B1naD1rFx", "replyto": "B1naD1rFx", "signatures": ["ICLR.cc/2017/workshop/paper138/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper138/AnonReviewer1"], "content": {"title": "Review", "rating": "7: Good paper, accept", "review": "\nThis paper presents a method for incorporating both lexicalized and delexicalized data for dialogue generation. This can be useful, for example if there is some overlapping information between a slot type and its value (for example, if a location of a pizza restaurant in a database is 'near X street', instead of 'X street', you want the dialogue system to learn to say 'the pizza is at X street' rather than 'the pizza is at near X street'). The proposed model seems to do this, and outperforms existing methods (Wen et al., 2015) by a decent margin on two datasets based on human evaluation. While the change is not ground-breaking, I think it's at an appropriate level for an ICLR workshop paper, and should be accepted.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Natural Language Generation in Dialogue using Lexicalized and Delexicalized Data", "abstract": "Natural language generation plays a critical role in spoken dialogue systems. We present a new approach to natural language generation for task-oriented dialogue using recurrent neural networks in an encoder-decoder framework. In contrast to previous work, our model uses both lexicalized and delexicalized components i.e. slot-value pairs for dialogue acts, with slots and corresponding values aligned together. This allows our model to learn from all available data including the slot-value pairing, rather than being restricted to delexicalized slots. We show that this helps our model generate more natural sentences with better grammar. We further improve our model's performance by transferring weights learnt from a pretrained sentence auto-encoder. Human evaluation of our best-performing model indicates that it generates sentences which users find more appealing.", "pdf": "https://arxiv.org/pdf/1606.03632.pdf", "paperhash": "sharma|natural_language_generation_in_dialogue_using_lexicalized_and_delexicalized_data", "keywords": ["Natural language processing", "Deep learning", "Transfer Learning"], "conflicts": ["maluuba.com", "microsoft.com"], "authors": ["Shikhar Sharma", "Jing He", "Kaheer Suleman", "Hannes Schulz", "Philip Bachman"], "authorids": ["shikhar.sharma@microsoft.com", "peaceful.he@gmail.com", "kaheer.suleman@microsoft.com", "hannes.schulz@microsoft.com", "phbachma@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489496475408, "id": "ICLR.cc/2017/workshop/-/paper138/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper138/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper138/AnonReviewer1", "ICLR.cc/2017/workshop/paper138/AnonReviewer2"], "reply": {"forum": "B1naD1rFx", "replyto": "B1naD1rFx", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper138/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper138/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489496475408}}}], "count": 4}