{"notes": [{"id": "B1GIB3A9YX", "original": "HJg4jC65KX", "number": 1546, "cdate": 1538087998293, "ddate": null, "tcdate": 1538087998293, "tmdate": 1545355404873, "tddate": null, "forum": "B1GIB3A9YX", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Explicit Recall for Efficient Exploration", "abstract": "In this paper, we advocate the use of explicit memory for efficient exploration in reinforcement learning.  This memory records structured trajectories that have led to interesting states in the past, and can be used by the agent to revisit those states more effectively.  In high-dimensional decision making problems, where deep reinforcement learning is considered crucial, our approach provides a simple, transparent and effective way that can be naturally combined with complex, deep learning models.  We show how such explicit memory may be used to enhance existing exploration algorithms such as intrinsically motivated ones and count-based ones, and demonstrate our method's advantages in various simulated environments.", "keywords": ["Exploration", "goal-directed", "deep reinforcement learning", "explicit memory"], "authorids": ["dhh19951@gmail.com", "maojiayuan@gmail.com", "rogar2233cxy@gmail.com", "lihongli.cs@gmail.com"], "authors": ["Honghua Dong", "Jiayuan Mao", "Xinyue Cui", "Lihong Li"], "TL;DR": "We advocate the use of explicit memory for efficient exploration in reinforcement learning", "pdf": "/pdf/72b577ce98ac76872d3b6104d0d19aae026f7772.pdf", "paperhash": "dong|explicit_recall_for_efficient_exploration", "_bibtex": "@misc{\ndong2019explicit,\ntitle={Explicit Recall for Efficient Exploration},\nauthor={Honghua Dong and Jiayuan Mao and Xinyue Cui and Lihong Li},\nyear={2019},\nurl={https://openreview.net/forum?id=B1GIB3A9YX},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "HkxufGbxgN", "original": null, "number": 1, "cdate": 1544716816435, "ddate": null, "tcdate": 1544716816435, "tmdate": 1545354508454, "tddate": null, "forum": "B1GIB3A9YX", "replyto": "B1GIB3A9YX", "invitation": "ICLR.cc/2019/Conference/-/Paper1546/Meta_Review", "content": {"metareview": "The paper presents an explicit memory that directly contributes to more efficient exploration. It stores trajectories to novel states, that serve as training data to learn to reach those states again (through iterative sub-goals). \n\nThe description of the method is quite clear, the method is not completely novel but has some merit. Most weaknesses of the paper come from the experimental section: too specific environments/solutions, lack of points of comparisons, lacking some details.\n\nWe strongly encourage the authors to add additional experimental evidence, and details. In its current form, the paper is not sufficient for publication at ICLR 2019.\n\nReviewers wanted to note that the blog post from Uber (\"Go-Explore\") did _not_ affect their evaluation of this paper.", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Reject", "title": "Interesting research direction but weak paper"}, "signatures": ["ICLR.cc/2019/Conference/Paper1546/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper1546/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Explicit Recall for Efficient Exploration", "abstract": "In this paper, we advocate the use of explicit memory for efficient exploration in reinforcement learning.  This memory records structured trajectories that have led to interesting states in the past, and can be used by the agent to revisit those states more effectively.  In high-dimensional decision making problems, where deep reinforcement learning is considered crucial, our approach provides a simple, transparent and effective way that can be naturally combined with complex, deep learning models.  We show how such explicit memory may be used to enhance existing exploration algorithms such as intrinsically motivated ones and count-based ones, and demonstrate our method's advantages in various simulated environments.", "keywords": ["Exploration", "goal-directed", "deep reinforcement learning", "explicit memory"], "authorids": ["dhh19951@gmail.com", "maojiayuan@gmail.com", "rogar2233cxy@gmail.com", "lihongli.cs@gmail.com"], "authors": ["Honghua Dong", "Jiayuan Mao", "Xinyue Cui", "Lihong Li"], "TL;DR": "We advocate the use of explicit memory for efficient exploration in reinforcement learning", "pdf": "/pdf/72b577ce98ac76872d3b6104d0d19aae026f7772.pdf", "paperhash": "dong|explicit_recall_for_efficient_exploration", "_bibtex": "@misc{\ndong2019explicit,\ntitle={Explicit Recall for Efficient Exploration},\nauthor={Honghua Dong and Jiayuan Mao and Xinyue Cui and Lihong Li},\nyear={2019},\nurl={https://openreview.net/forum?id=B1GIB3A9YX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1546/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545352799196, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1GIB3A9YX", "replyto": "B1GIB3A9YX", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1546/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper1546/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1546/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545352799196}}}, {"id": "rkx4PAj7JN", "original": null, "number": 6, "cdate": 1543908955578, "ddate": null, "tcdate": 1543908955578, "tmdate": 1543908955578, "tddate": null, "forum": "B1GIB3A9YX", "replyto": "B1GIB3A9YX", "invitation": "ICLR.cc/2019/Conference/-/Paper1546/Official_Comment", "content": {"title": "Comparison with Go-Explore", "comment": "Recently, a similar method is published on uber\u2019s website (https://eng.uber.com/go-explore/), which they called the go-explore method. Their results are very promising on both Montezuma\u2019s Revenge and Pitfall, two of the hardest exploration tasks in Atari games.\n\nWhile we share the similar 3-stage exploration structure, there are several differences.\n1. As they assume the environment is resettable/deterministic during training, they can utilize the ability of reset to quickly return to a state the agent want. Instead, we do not rely on the assumption, which brings significant hardness while reaching an intended state, and is the major reason why our performance is not as good as theirs.\n2. When the training environment is stochastic (in our montezuma\u2019s setting), they propose to use goal-conditioned policy, which is exactly what we are doing. Furthermore, we also propose to sample sub-goals from the trajectory."}, "signatures": ["ICLR.cc/2019/Conference/Paper1546/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1546/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1546/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Explicit Recall for Efficient Exploration", "abstract": "In this paper, we advocate the use of explicit memory for efficient exploration in reinforcement learning.  This memory records structured trajectories that have led to interesting states in the past, and can be used by the agent to revisit those states more effectively.  In high-dimensional decision making problems, where deep reinforcement learning is considered crucial, our approach provides a simple, transparent and effective way that can be naturally combined with complex, deep learning models.  We show how such explicit memory may be used to enhance existing exploration algorithms such as intrinsically motivated ones and count-based ones, and demonstrate our method's advantages in various simulated environments.", "keywords": ["Exploration", "goal-directed", "deep reinforcement learning", "explicit memory"], "authorids": ["dhh19951@gmail.com", "maojiayuan@gmail.com", "rogar2233cxy@gmail.com", "lihongli.cs@gmail.com"], "authors": ["Honghua Dong", "Jiayuan Mao", "Xinyue Cui", "Lihong Li"], "TL;DR": "We advocate the use of explicit memory for efficient exploration in reinforcement learning", "pdf": "/pdf/72b577ce98ac76872d3b6104d0d19aae026f7772.pdf", "paperhash": "dong|explicit_recall_for_efficient_exploration", "_bibtex": "@misc{\ndong2019explicit,\ntitle={Explicit Recall for Efficient Exploration},\nauthor={Honghua Dong and Jiayuan Mao and Xinyue Cui and Lihong Li},\nyear={2019},\nurl={https://openreview.net/forum?id=B1GIB3A9YX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1546/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621611114, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1GIB3A9YX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1546/Authors", "ICLR.cc/2019/Conference/Paper1546/Reviewers", "ICLR.cc/2019/Conference/Paper1546/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1546/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1546/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1546/Authors|ICLR.cc/2019/Conference/Paper1546/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1546/Reviewers", "ICLR.cc/2019/Conference/Paper1546/Authors", "ICLR.cc/2019/Conference/Paper1546/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621611114}}}, {"id": "rJlV2kj71E", "original": null, "number": 5, "cdate": 1543905195811, "ddate": null, "tcdate": 1543905195811, "tmdate": 1543905195811, "tddate": null, "forum": "B1GIB3A9YX", "replyto": "Skg08eT-s7", "invitation": "ICLR.cc/2019/Conference/-/Paper1546/Official_Comment", "content": {"title": "Our Response", "comment": "Thanks for your comments and suggestions.\n\n1. Comparison with previous methods.\nDifferent from \u201cSelf-Imitation Learning\u201d, our agent uses explicit memory to help exploration, whose advantages are described in the last paragraph of page 1. \nThe major difference between our method and \"Automatic Goal Generation for Reinforcement Learning Agents\" is the way to generate the goals and is stated in details in Section 4.2.\nThe notion of curiosity defined in \u201cCuriosity-driven exploration by self-supervised prediction\u201d is employed in our framework for the exploration. Comparison has been made with the ICM model proposed in this paper. See Figure 1-3 and 5.\n\n2. The definitions.\nThe details about Rooms environment can be found in Appendix A. The visit_times[x] means the number of times the cell x is being visited by the agent, accumulated throughout the training process. In the Rooms environment, each cell has a type of empty/wall/border, The stage avg reward is used as a metric for evaluating the trajectories, whose details can be found in Appendix B. We will try to integrate these definitions into the main text.\n\n3. Performance in stochastic setting\nBoth Montezuma\u2019s Revenge and PrivateEye environments are stochastic: each action leads to 2~4 frame-skips randomly. Our method outperforms the curiosity baseline in both environments. As for random starting states, please see the response to AnonReviewer3 (A. About the same-start assumption)\n\n4. Clarifications\nA. We would change the words to make it more clear. Here what we mean is that the inverse dynamics provides a feature space that ignores the noise which the agent cannot control (e.g. white noise in visual input) (as suggested by Pathak et al., 2017).\nB. When there are multiple actions leading to the same next state s\u2019, the inverse dynamics would have multiple answers. This is what we mean \u201cambiguous\u201d.\nC. In Fig2, the accuracy is the number of cases that the output \\hat{a} leads to the desired next state s\u2019, that is, env(s, hat(a)) = s\u2019.\nD. (cos+1)^3/8 is chosen empirically, used for modeling the similarity between states."}, "signatures": ["ICLR.cc/2019/Conference/Paper1546/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1546/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1546/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Explicit Recall for Efficient Exploration", "abstract": "In this paper, we advocate the use of explicit memory for efficient exploration in reinforcement learning.  This memory records structured trajectories that have led to interesting states in the past, and can be used by the agent to revisit those states more effectively.  In high-dimensional decision making problems, where deep reinforcement learning is considered crucial, our approach provides a simple, transparent and effective way that can be naturally combined with complex, deep learning models.  We show how such explicit memory may be used to enhance existing exploration algorithms such as intrinsically motivated ones and count-based ones, and demonstrate our method's advantages in various simulated environments.", "keywords": ["Exploration", "goal-directed", "deep reinforcement learning", "explicit memory"], "authorids": ["dhh19951@gmail.com", "maojiayuan@gmail.com", "rogar2233cxy@gmail.com", "lihongli.cs@gmail.com"], "authors": ["Honghua Dong", "Jiayuan Mao", "Xinyue Cui", "Lihong Li"], "TL;DR": "We advocate the use of explicit memory for efficient exploration in reinforcement learning", "pdf": "/pdf/72b577ce98ac76872d3b6104d0d19aae026f7772.pdf", "paperhash": "dong|explicit_recall_for_efficient_exploration", "_bibtex": "@misc{\ndong2019explicit,\ntitle={Explicit Recall for Efficient Exploration},\nauthor={Honghua Dong and Jiayuan Mao and Xinyue Cui and Lihong Li},\nyear={2019},\nurl={https://openreview.net/forum?id=B1GIB3A9YX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1546/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621611114, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1GIB3A9YX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1546/Authors", "ICLR.cc/2019/Conference/Paper1546/Reviewers", "ICLR.cc/2019/Conference/Paper1546/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1546/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1546/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1546/Authors|ICLR.cc/2019/Conference/Paper1546/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1546/Reviewers", "ICLR.cc/2019/Conference/Paper1546/Authors", "ICLR.cc/2019/Conference/Paper1546/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621611114}}}, {"id": "S1xeGRqQ14", "original": null, "number": 4, "cdate": 1543904775934, "ddate": null, "tcdate": 1543904775934, "tmdate": 1543904775934, "tddate": null, "forum": "B1GIB3A9YX", "replyto": "r1e86jUEo7", "invitation": "ICLR.cc/2019/Conference/-/Paper1546/Official_Comment", "content": {"title": "Our Response", "comment": "Thanks for your comments and suggestions, and we will revise the paper as you suggested.\n\n1: About the same-start assumption.\nWe discuss the starting states in four cases.\nA. The starting states are always the same, which is our the assumption.\nB. There is a small randomness (noise) for the starting state. Path function can handle this: after choosing a goal state from a trajectory, Path function will generate a trajectory from the current starting state to the goal state.\nC. There are multiple possible starting states. New episodes can start in the same states as *some* (not all) previous episodes: the agent can simply remember successful trajectories and apply our algorithm to distinct start states separately.\nD. If the starting states are too far away (or randomly given) and no assumption is made about their relation/similarity, little can be expected to take advantage of former trajectories, even for humans.\n\n2: Experiment details\nA. The number of seeds is 2 for experiments in Rooms environment, 3 for Atari Games. \nB. Re Fig2: better exploration in RL is expected to lead to a faster learning curve, not necessarily a better final model.  Fig 2 shows exactly this: our method learns faster than the baseline, without sacrificing the final model performance.\nC. Re Fig3: as the destination are very far away from the starting point (see Appendix A.2), agents\u2019 score would be almost 0 if the destination could not be found during the exploration. The Zigzag-shaped rooms environment requires the agents to explore the full map to reach the destination. The results are consistent with Fig1 showing the exploration efficiency.\n\n3: Clarification on technical details.\nWe apologize for any confusions in the paper and will improve the writing. Specific questions by the reviewer are addressed as the following:\nOn the first sentence of Section 3.2.2. While Path function (we regard it as skills) is being trained independently with the task, it can be applied on any other tasks. For example, in Zero-shot Visual Imitation [1], the goal-conditioned policy is used to follow a sequence of key-points demonstration.\nIn Fig 4, the second row shows the number of times each state being chosen as the target state (the last state of a selected trajectory). This number is illustrated as a heatmap with log-scale.\n\n[1] Deepak Pathak, Parsa Mahmoudieh, Guanghao Luo, Pulkit Agrawal, Dian Chen, Yide Shentu, Evan Shelhamer, Jitendra Malik, Alexei A. Efros, and Trevor Darrell. Zero-shot visual imitation. In ICLR, 2018.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1546/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1546/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1546/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Explicit Recall for Efficient Exploration", "abstract": "In this paper, we advocate the use of explicit memory for efficient exploration in reinforcement learning.  This memory records structured trajectories that have led to interesting states in the past, and can be used by the agent to revisit those states more effectively.  In high-dimensional decision making problems, where deep reinforcement learning is considered crucial, our approach provides a simple, transparent and effective way that can be naturally combined with complex, deep learning models.  We show how such explicit memory may be used to enhance existing exploration algorithms such as intrinsically motivated ones and count-based ones, and demonstrate our method's advantages in various simulated environments.", "keywords": ["Exploration", "goal-directed", "deep reinforcement learning", "explicit memory"], "authorids": ["dhh19951@gmail.com", "maojiayuan@gmail.com", "rogar2233cxy@gmail.com", "lihongli.cs@gmail.com"], "authors": ["Honghua Dong", "Jiayuan Mao", "Xinyue Cui", "Lihong Li"], "TL;DR": "We advocate the use of explicit memory for efficient exploration in reinforcement learning", "pdf": "/pdf/72b577ce98ac76872d3b6104d0d19aae026f7772.pdf", "paperhash": "dong|explicit_recall_for_efficient_exploration", "_bibtex": "@misc{\ndong2019explicit,\ntitle={Explicit Recall for Efficient Exploration},\nauthor={Honghua Dong and Jiayuan Mao and Xinyue Cui and Lihong Li},\nyear={2019},\nurl={https://openreview.net/forum?id=B1GIB3A9YX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1546/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621611114, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1GIB3A9YX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1546/Authors", "ICLR.cc/2019/Conference/Paper1546/Reviewers", "ICLR.cc/2019/Conference/Paper1546/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1546/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1546/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1546/Authors|ICLR.cc/2019/Conference/Paper1546/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1546/Reviewers", "ICLR.cc/2019/Conference/Paper1546/Authors", "ICLR.cc/2019/Conference/Paper1546/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621611114}}}, {"id": "Sym2j9myE", "original": null, "number": 3, "cdate": 1543904170515, "ddate": null, "tcdate": 1543904170515, "tmdate": 1543904170515, "tddate": null, "forum": "B1GIB3A9YX", "replyto": "HJeylmr62X", "invitation": "ICLR.cc/2019/Conference/-/Paper1546/Official_Comment", "content": {"title": "Our Response", "comment": "Thanks for your encouraging comments and nice suggestions. We plan to update the figures in the paper upon the decision. We will also integrate Appendix C and the cosine metrics into the main text."}, "signatures": ["ICLR.cc/2019/Conference/Paper1546/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1546/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1546/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Explicit Recall for Efficient Exploration", "abstract": "In this paper, we advocate the use of explicit memory for efficient exploration in reinforcement learning.  This memory records structured trajectories that have led to interesting states in the past, and can be used by the agent to revisit those states more effectively.  In high-dimensional decision making problems, where deep reinforcement learning is considered crucial, our approach provides a simple, transparent and effective way that can be naturally combined with complex, deep learning models.  We show how such explicit memory may be used to enhance existing exploration algorithms such as intrinsically motivated ones and count-based ones, and demonstrate our method's advantages in various simulated environments.", "keywords": ["Exploration", "goal-directed", "deep reinforcement learning", "explicit memory"], "authorids": ["dhh19951@gmail.com", "maojiayuan@gmail.com", "rogar2233cxy@gmail.com", "lihongli.cs@gmail.com"], "authors": ["Honghua Dong", "Jiayuan Mao", "Xinyue Cui", "Lihong Li"], "TL;DR": "We advocate the use of explicit memory for efficient exploration in reinforcement learning", "pdf": "/pdf/72b577ce98ac76872d3b6104d0d19aae026f7772.pdf", "paperhash": "dong|explicit_recall_for_efficient_exploration", "_bibtex": "@misc{\ndong2019explicit,\ntitle={Explicit Recall for Efficient Exploration},\nauthor={Honghua Dong and Jiayuan Mao and Xinyue Cui and Lihong Li},\nyear={2019},\nurl={https://openreview.net/forum?id=B1GIB3A9YX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1546/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621611114, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1GIB3A9YX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1546/Authors", "ICLR.cc/2019/Conference/Paper1546/Reviewers", "ICLR.cc/2019/Conference/Paper1546/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1546/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1546/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1546/Authors|ICLR.cc/2019/Conference/Paper1546/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1546/Reviewers", "ICLR.cc/2019/Conference/Paper1546/Authors", "ICLR.cc/2019/Conference/Paper1546/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621611114}}}, {"id": "rJx7li5XJE", "original": null, "number": 2, "cdate": 1543903978768, "ddate": null, "tcdate": 1543903978768, "tmdate": 1543903978768, "tddate": null, "forum": "B1GIB3A9YX", "replyto": "Hyg-gbgiA7", "invitation": "ICLR.cc/2019/Conference/-/Paper1546/Official_Comment", "content": {"title": "Thanks", "comment": "Thanks for your pointers to the related works. We will definitely add them to our references and compare with them upon the decision."}, "signatures": ["ICLR.cc/2019/Conference/Paper1546/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1546/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1546/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Explicit Recall for Efficient Exploration", "abstract": "In this paper, we advocate the use of explicit memory for efficient exploration in reinforcement learning.  This memory records structured trajectories that have led to interesting states in the past, and can be used by the agent to revisit those states more effectively.  In high-dimensional decision making problems, where deep reinforcement learning is considered crucial, our approach provides a simple, transparent and effective way that can be naturally combined with complex, deep learning models.  We show how such explicit memory may be used to enhance existing exploration algorithms such as intrinsically motivated ones and count-based ones, and demonstrate our method's advantages in various simulated environments.", "keywords": ["Exploration", "goal-directed", "deep reinforcement learning", "explicit memory"], "authorids": ["dhh19951@gmail.com", "maojiayuan@gmail.com", "rogar2233cxy@gmail.com", "lihongli.cs@gmail.com"], "authors": ["Honghua Dong", "Jiayuan Mao", "Xinyue Cui", "Lihong Li"], "TL;DR": "We advocate the use of explicit memory for efficient exploration in reinforcement learning", "pdf": "/pdf/72b577ce98ac76872d3b6104d0d19aae026f7772.pdf", "paperhash": "dong|explicit_recall_for_efficient_exploration", "_bibtex": "@misc{\ndong2019explicit,\ntitle={Explicit Recall for Efficient Exploration},\nauthor={Honghua Dong and Jiayuan Mao and Xinyue Cui and Lihong Li},\nyear={2019},\nurl={https://openreview.net/forum?id=B1GIB3A9YX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1546/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621611114, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1GIB3A9YX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1546/Authors", "ICLR.cc/2019/Conference/Paper1546/Reviewers", "ICLR.cc/2019/Conference/Paper1546/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1546/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1546/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1546/Authors|ICLR.cc/2019/Conference/Paper1546/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1546/Reviewers", "ICLR.cc/2019/Conference/Paper1546/Authors", "ICLR.cc/2019/Conference/Paper1546/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621611114}}}, {"id": "Hyg-gbgiA7", "original": null, "number": 1, "cdate": 1543336169046, "ddate": null, "tcdate": 1543336169046, "tmdate": 1543336184910, "tddate": null, "forum": "B1GIB3A9YX", "replyto": "B1GIB3A9YX", "invitation": "ICLR.cc/2019/Conference/-/Paper1546/Public_Comment", "content": {"comment": "Hello, \n\nI just came across your paper. I think few other papers should be cited, which also tries to use explicit memory in terms of high value states or goal states or high bellman error.\n\n[1] Recall Traces, https://arxiv.org/abs/1804.00379  (I'm the author of this paper)\n[2] Self Immitation learning, https://arxiv.org/abs/1806.05635\n[3] Neural episodic control https://arxiv.org/abs/1703.01988\n\nThanks for your time! :)", "title": "More references"}, "signatures": ["~Anirudh_Goyal1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Anirudh_Goyal1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Explicit Recall for Efficient Exploration", "abstract": "In this paper, we advocate the use of explicit memory for efficient exploration in reinforcement learning.  This memory records structured trajectories that have led to interesting states in the past, and can be used by the agent to revisit those states more effectively.  In high-dimensional decision making problems, where deep reinforcement learning is considered crucial, our approach provides a simple, transparent and effective way that can be naturally combined with complex, deep learning models.  We show how such explicit memory may be used to enhance existing exploration algorithms such as intrinsically motivated ones and count-based ones, and demonstrate our method's advantages in various simulated environments.", "keywords": ["Exploration", "goal-directed", "deep reinforcement learning", "explicit memory"], "authorids": ["dhh19951@gmail.com", "maojiayuan@gmail.com", "rogar2233cxy@gmail.com", "lihongli.cs@gmail.com"], "authors": ["Honghua Dong", "Jiayuan Mao", "Xinyue Cui", "Lihong Li"], "TL;DR": "We advocate the use of explicit memory for efficient exploration in reinforcement learning", "pdf": "/pdf/72b577ce98ac76872d3b6104d0d19aae026f7772.pdf", "paperhash": "dong|explicit_recall_for_efficient_exploration", "_bibtex": "@misc{\ndong2019explicit,\ntitle={Explicit Recall for Efficient Exploration},\nauthor={Honghua Dong and Jiayuan Mao and Xinyue Cui and Lihong Li},\nyear={2019},\nurl={https://openreview.net/forum?id=B1GIB3A9YX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1546/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311571793, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "B1GIB3A9YX", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1546/Authors", "ICLR.cc/2019/Conference/Paper1546/Reviewers", "ICLR.cc/2019/Conference/Paper1546/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1546/Authors", "ICLR.cc/2019/Conference/Paper1546/Reviewers", "ICLR.cc/2019/Conference/Paper1546/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311571793}}}, {"id": "HJeylmr62X", "original": null, "number": 3, "cdate": 1541391079153, "ddate": null, "tcdate": 1541391079153, "tmdate": 1541533045628, "tddate": null, "forum": "B1GIB3A9YX", "replyto": "B1GIB3A9YX", "invitation": "ICLR.cc/2019/Conference/-/Paper1546/Official_Review", "content": {"title": "Good idea, good demonstration, good score", "review": "This paper is the first showing that achieving self-generated tasks during spontaneous exploration and getting reinforced by self-supervised signals is a promising way for the agent to develop skills itself.\nThe scores are demonstrative on several tasks.\nIt opens interesting direction for further research.\n\nREM: \nfew typos like \"An state\"\nPlease plot in dash the count methods in the graphs (use oracle information)\n\nAnnexe C shall be integrated into the core of the paper. Could be simplified.\nThe cosine metrics shall be better integrated in it.", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1546/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Explicit Recall for Efficient Exploration", "abstract": "In this paper, we advocate the use of explicit memory for efficient exploration in reinforcement learning.  This memory records structured trajectories that have led to interesting states in the past, and can be used by the agent to revisit those states more effectively.  In high-dimensional decision making problems, where deep reinforcement learning is considered crucial, our approach provides a simple, transparent and effective way that can be naturally combined with complex, deep learning models.  We show how such explicit memory may be used to enhance existing exploration algorithms such as intrinsically motivated ones and count-based ones, and demonstrate our method's advantages in various simulated environments.", "keywords": ["Exploration", "goal-directed", "deep reinforcement learning", "explicit memory"], "authorids": ["dhh19951@gmail.com", "maojiayuan@gmail.com", "rogar2233cxy@gmail.com", "lihongli.cs@gmail.com"], "authors": ["Honghua Dong", "Jiayuan Mao", "Xinyue Cui", "Lihong Li"], "TL;DR": "We advocate the use of explicit memory for efficient exploration in reinforcement learning", "pdf": "/pdf/72b577ce98ac76872d3b6104d0d19aae026f7772.pdf", "paperhash": "dong|explicit_recall_for_efficient_exploration", "_bibtex": "@misc{\ndong2019explicit,\ntitle={Explicit Recall for Efficient Exploration},\nauthor={Honghua Dong and Jiayuan Mao and Xinyue Cui and Lihong Li},\nyear={2019},\nurl={https://openreview.net/forum?id=B1GIB3A9YX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1546/Official_Review", "cdate": 1542234206663, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "B1GIB3A9YX", "replyto": "B1GIB3A9YX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1546/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335969529, "tmdate": 1552335969529, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1546/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "r1e86jUEo7", "original": null, "number": 2, "cdate": 1539759037699, "ddate": null, "tcdate": 1539759037699, "tmdate": 1541533045423, "tddate": null, "forum": "B1GIB3A9YX", "replyto": "B1GIB3A9YX", "invitation": "ICLR.cc/2019/Conference/-/Paper1546/Official_Review", "content": {"title": "Interesting idea, but rather weak paper. Can be improved a lot with additional writing effort", "review": "In this paper, the authors propose an exploration strategy based on the explicit storage and recall of trajectories leading to novel states. A pool of such trajectories is managed over time, and a method is proposed so that the agent can learn how to follow a path corresponding to these trajectories so as to explore novel states. The idea is demonstrated in a set of room experiments, and quickly shown efficient in Montezuma's Revenge and PrivateEye Atari games.\n\nOverall, the idea has some merits, but the empirical study is weak and the paper suffers from unsufficient writing effort (or more probably time).\n\nWhat I like most in the paper is the split of exploration methods into 3 categories: adding some \"intrinsic reward\" bonuses to novel states (curiosity-driven exploration) , trying to reach various goals (goal-driven exploration) and using memory to reach again novel states (memory-driven exploration). Actually, this split may be debated. For instance, some frameworks based on goals have been labelled curiosity-driven, e.g. \"Curiosity-Driven Exploration of Learned Disentangled Goal Spaces\" (Laversanne-Finot, P\u00e9r\u00e9 and Oudeyer, CoRL 2018), but anyways I find it useful. That said, this aspect of the introduction is reiterated in the \"Related Work\" section in a quite redundant way, whereas both parts could have been better integrated. Furthermore, the related work section is hardly a draft, I'll come to that later.\n\nThe presentation of the method in Section 2 is rather clear and convincing. My only concern is about the assumption that the agent is always starting in the same state. This assumption may not hold in many settings, and the approach appears to be quite dependent on it. A discussion of how it could be extended to a less restricting assumption would be welcome.\n\nThe experimental section is weaker. A few concerns:\n- I could not find much about the number of seeds, trials, the way to depict some variance, the statistical significance of the differences between results presented in Figure 1. The same is true about Figs. 2, 3 and 5.\n- In Fig.2, the claim that the author's method learns better models is hardly supported by the left-hand-side plot, and significance is not assessed.\n- I'm puzzled about the very low performance of baselines in the plots of Fig. 3. Could the author explain why these performances are null.\n- The Atari games section helps figuring out that the framework is not too specific of the rooms environment, but the lack of analysis does not help making sure that this is just the explicit recall mechanism that is responsible for superior performance and why.\n\n\nAnother point about this section is that poor writing does not help understanding some points.\n- to me, the first sentence of Section 3.2.2 does not make sense at all.\n- in the caption of Fig. 4, \"The second row is the heatmaps for states that the number of times being selected as a target state.\": I don't get what it means, thus I don't understand what that row shows.\n- Fig.5 comes with no caption\n\nAbout the related work:\n- The comparison to other methods using memory needs to be expanded. In particular, I would put HER-like mechanisms here rather than in 4.1, as \"explicit recall\" shares some importan ideas with \"experience replay\"\n- Section 4.4 (HRL) is not useful as is.\n\nFinally, in the conclusion, the claim that the method can be combined with \"many sota exploration methods\" is not supported, as the authors have only tried two and did not analyse the results in much details.\n\n\ntypos:\n\n- p4:\nwe can easily counting\n(include borders) => including\nis provide => provided\n\nare less less-visited states: quite inelegant\n\n- p7:\nIn Montezuma's Revenge, Comparing => comparing\nWhere they encourage => remove \"Where\"\n\n- p8:\nrecallcan => recall can\nthe problem of reach goals => reaching\nit succesfully reproduce => reproduces\n\nThe last paragraph of Section 4.2 needs a careful rewriting, as long sentences with parenthese in the middle appear to be some draft version.\n\ncontrol(Pritzel => Missing space\nOur method use memory => uses\nAlthough ..., but => remove but\n\nThe path function can be seen as a form of skills => skill?\nBesides, the \"can be seen\" needs to be further explained...\n\nAppendix\n\nFinally, we provided => provide\n\nis around (math formula) => cannot you be more specific?", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1546/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Explicit Recall for Efficient Exploration", "abstract": "In this paper, we advocate the use of explicit memory for efficient exploration in reinforcement learning.  This memory records structured trajectories that have led to interesting states in the past, and can be used by the agent to revisit those states more effectively.  In high-dimensional decision making problems, where deep reinforcement learning is considered crucial, our approach provides a simple, transparent and effective way that can be naturally combined with complex, deep learning models.  We show how such explicit memory may be used to enhance existing exploration algorithms such as intrinsically motivated ones and count-based ones, and demonstrate our method's advantages in various simulated environments.", "keywords": ["Exploration", "goal-directed", "deep reinforcement learning", "explicit memory"], "authorids": ["dhh19951@gmail.com", "maojiayuan@gmail.com", "rogar2233cxy@gmail.com", "lihongli.cs@gmail.com"], "authors": ["Honghua Dong", "Jiayuan Mao", "Xinyue Cui", "Lihong Li"], "TL;DR": "We advocate the use of explicit memory for efficient exploration in reinforcement learning", "pdf": "/pdf/72b577ce98ac76872d3b6104d0d19aae026f7772.pdf", "paperhash": "dong|explicit_recall_for_efficient_exploration", "_bibtex": "@misc{\ndong2019explicit,\ntitle={Explicit Recall for Efficient Exploration},\nauthor={Honghua Dong and Jiayuan Mao and Xinyue Cui and Lihong Li},\nyear={2019},\nurl={https://openreview.net/forum?id=B1GIB3A9YX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1546/Official_Review", "cdate": 1542234206663, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "B1GIB3A9YX", "replyto": "B1GIB3A9YX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1546/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335969529, "tmdate": 1552335969529, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1546/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "Skg08eT-s7", "original": null, "number": 1, "cdate": 1539588182086, "ddate": null, "tcdate": 1539588182086, "tmdate": 1541533045183, "tddate": null, "forum": "B1GIB3A9YX", "replyto": "B1GIB3A9YX", "invitation": "ICLR.cc/2019/Conference/-/Paper1546/Official_Review", "content": {"title": "Many hacks and heuristics. ", "review": "In this paper, the authors propose a heuristic method to overcome the exploration in RL. They store trajectories which result in novel states. \nThe final state of the trajectory is called goal state, and the authors train a path function which given a state and a subgoal states (some states in the trajectory) the most probably action the agent needs to take to reach the subgoal. These way they navigate to the goal state. The goal state is claimed to be achieved if the feature representation stoping state is close to goal (or subgoal for subgoal navigation).\n\n\nThe authors mainly combine a few previous approaches \"Self-Imitation Learning,\" \"Automatic Goal Generation for Reinforcement Learning Agents,\" and \"Curiosity-driven exploration by self-supervised prediction\" to design this algorithm which makes this approach less novel.\n\nGeneral comment; there are variable and functions in the paper that are not defined, at least at the time, they have been used. The Rooms environment is not described. What is visit_times[x] and x is not a wall? What is stage avg reward? and many others\n\nThe main idea of the algorithm is clear, but the description of the pieces is missing.\n\nIt is not clear in stochastic setting how well this approach will perform. \n\nThe authors state that\n\"Among different choices of the modeling, we choose inverse dynamics (Pathak et al., 2017) as the environment model, which has been proved to be an effective way of representing states under noisy environments.\"\nI took a look at this paper and could not find neither proof or quantification of \"effective\"-ness. Please clarify what the meaning this statement is.\n\nWhy s=s' is ambiguous to the inverse dynamics?\n\nWhat is the definition of acc in fig2?\n\nwhy (consin+1)^3/8 is chosen?\n", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1546/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Explicit Recall for Efficient Exploration", "abstract": "In this paper, we advocate the use of explicit memory for efficient exploration in reinforcement learning.  This memory records structured trajectories that have led to interesting states in the past, and can be used by the agent to revisit those states more effectively.  In high-dimensional decision making problems, where deep reinforcement learning is considered crucial, our approach provides a simple, transparent and effective way that can be naturally combined with complex, deep learning models.  We show how such explicit memory may be used to enhance existing exploration algorithms such as intrinsically motivated ones and count-based ones, and demonstrate our method's advantages in various simulated environments.", "keywords": ["Exploration", "goal-directed", "deep reinforcement learning", "explicit memory"], "authorids": ["dhh19951@gmail.com", "maojiayuan@gmail.com", "rogar2233cxy@gmail.com", "lihongli.cs@gmail.com"], "authors": ["Honghua Dong", "Jiayuan Mao", "Xinyue Cui", "Lihong Li"], "TL;DR": "We advocate the use of explicit memory for efficient exploration in reinforcement learning", "pdf": "/pdf/72b577ce98ac76872d3b6104d0d19aae026f7772.pdf", "paperhash": "dong|explicit_recall_for_efficient_exploration", "_bibtex": "@misc{\ndong2019explicit,\ntitle={Explicit Recall for Efficient Exploration},\nauthor={Honghua Dong and Jiayuan Mao and Xinyue Cui and Lihong Li},\nyear={2019},\nurl={https://openreview.net/forum?id=B1GIB3A9YX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1546/Official_Review", "cdate": 1542234206663, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "B1GIB3A9YX", "replyto": "B1GIB3A9YX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1546/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335969529, "tmdate": 1552335969529, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1546/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 11}