{"notes": [{"id": "B1xw9n4Kwr", "original": "Ske6CFU1wS", "number": 118, "cdate": 1569438862864, "ddate": null, "tcdate": 1569438862864, "tmdate": 1577168283448, "tddate": null, "forum": "B1xw9n4Kwr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Model Architecture Controls Gradient Descent Dynamics: A Combinatorial Path-Based Formula", "authors": ["Xin Zhou", "Newsha Ardalani"], "authorids": ["chow459@gmail.com", "newsha@baidu.com"], "keywords": [], "abstract": "Recently, there has been a growing interest in automatically exploring neural network architecture design space with the goal of finding an architecture that improves performance (characterized as improved accuracy, speed of training, or resource requirements). However, our theoretical understanding of how model architecture affects performance or accuracy is limited. In this paper, we study the impact of model architecture on the speed of training in the context of gradient descent optimization. We model gradient descent as a first-order ODE and use ODE's coefficient matrix H to characterize the convergence rate. We introduce a simple analysis technique that enumerates H in terms of all possible ``paths'' in the network. \n We show that changes in model architecture parameters reflect as changes in the number of paths and the properties of each path, which jointly control the speed of convergence. We believe our analysis technique is useful in reasoning about more complex model architecture modifications.", "pdf": "/pdf/dd8f140ce2d4bafe4ab99a923a16a702443e9348.pdf", "paperhash": "zhou|model_architecture_controls_gradient_descent_dynamics_a_combinatorial_pathbased_formula", "original_pdf": "/attachment/8daed6816bd8fa5f3747e70f3d2a5e7a3fbd1054.pdf", "_bibtex": "@misc{\nzhou2020model,\ntitle={Model Architecture Controls Gradient Descent Dynamics: A Combinatorial Path-Based Formula},\nauthor={Xin Zhou and Newsha Ardalani},\nyear={2020},\nurl={https://openreview.net/forum?id=B1xw9n4Kwr}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "SZx6xDCUut", "original": null, "number": 1, "cdate": 1576798687911, "ddate": null, "tcdate": 1576798687911, "tmdate": 1576800947180, "tddate": null, "forum": "B1xw9n4Kwr", "replyto": "B1xw9n4Kwr", "invitation": "ICLR.cc/2020/Conference/Paper118/-/Decision", "content": {"decision": "Reject", "comment": "This paper focuses on understanding the role of model architecture on convergence behavior and in particular on the speed of training. The authors study the gradient flow of training via studying an ODE's coefficient matrix H. They study the effect of H in terms of possible paths in the network. The reviewers all agreed that characterizing the behavior in terms of path is nice. However, they had concerns about novelty with respect to existing work on NTK. Other comments by reviewers include (1) poor literature review (2) subpar exposition and (3) hand-wavy and rack of rigor in some results. While some of these concerns were alleviated during the discussion. Reviewers were not fully satisfied.  I general agree with the overall assessment of the reviewers. The paper has some interesting ideas but suffers from lack of clarity and rigor. Therefore, I can not recommend acceptance in the current form.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Model Architecture Controls Gradient Descent Dynamics: A Combinatorial Path-Based Formula", "authors": ["Xin Zhou", "Newsha Ardalani"], "authorids": ["chow459@gmail.com", "newsha@baidu.com"], "keywords": [], "abstract": "Recently, there has been a growing interest in automatically exploring neural network architecture design space with the goal of finding an architecture that improves performance (characterized as improved accuracy, speed of training, or resource requirements). However, our theoretical understanding of how model architecture affects performance or accuracy is limited. In this paper, we study the impact of model architecture on the speed of training in the context of gradient descent optimization. We model gradient descent as a first-order ODE and use ODE's coefficient matrix H to characterize the convergence rate. We introduce a simple analysis technique that enumerates H in terms of all possible ``paths'' in the network. \n We show that changes in model architecture parameters reflect as changes in the number of paths and the properties of each path, which jointly control the speed of convergence. We believe our analysis technique is useful in reasoning about more complex model architecture modifications.", "pdf": "/pdf/dd8f140ce2d4bafe4ab99a923a16a702443e9348.pdf", "paperhash": "zhou|model_architecture_controls_gradient_descent_dynamics_a_combinatorial_pathbased_formula", "original_pdf": "/attachment/8daed6816bd8fa5f3747e70f3d2a5e7a3fbd1054.pdf", "_bibtex": "@misc{\nzhou2020model,\ntitle={Model Architecture Controls Gradient Descent Dynamics: A Combinatorial Path-Based Formula},\nauthor={Xin Zhou and Newsha Ardalani},\nyear={2020},\nurl={https://openreview.net/forum?id=B1xw9n4Kwr}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "B1xw9n4Kwr", "replyto": "B1xw9n4Kwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795714033, "tmdate": 1576800263792, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper118/-/Decision"}}}, {"id": "HJlaIEjQqr", "original": null, "number": 3, "cdate": 1572217941496, "ddate": null, "tcdate": 1572217941496, "tmdate": 1574124844019, "tddate": null, "forum": "B1xw9n4Kwr", "replyto": "B1xw9n4Kwr", "invitation": "ICLR.cc/2020/Conference/Paper118/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #2", "review": "This paper considers the problem of understanding the impact of deep neural networks (DNN) model architecture on the convergence rate of gradient descent dynamics. To achieve this goal, the paper follows the recent trend of continuous-time perspective of optimization, and proposes to model gradient descent via the gradient flow, which is a first-order ODE. The induced loss dynamics is then also following a first-order ODE with a coefficient matrix H (that depends on the solution trajectory, and hence non-constant). The paper then claims that the convergence rate is characterized by the minimum eigenvalue of H, and analyzes this H through a straightforward path-based formula obtained by chain rules. In particular, the authors try to explain the effect of width, depth and number of paths on the convergence rate, and validated these through a few numerical experiments.\n\nAdmittedly, the idea of this paper is interesting. However, I think the novelty and rigorousness of this paper is not convincing, as explained in more details below.\n\n1. On the novelty side, characterizing the convergence via the H matrix is not new, and most of the discussions in Section 4 have appeared in exactly the same form in the previous works [13] (two-layer) and [25] (general), which are also cited in this paper. In addition, the path formula is also very closely related (if not completely the same as) with the expansion of G matrix in Section 4 of [25] (where G is the H in this paper), which decomposes the G (or H) matrix into summation over layers. These facts largely lower the contribution of this paper on a high level. \n\n2. On the rigorousness side, the paper is not very consistent in the notation. \n1) The notation is not very consistent. The authors use H in the notation section to denote the matrix, use X_i to denote the data input, but use \\bf{H} and x_i (lower cased) subsequently. \n2) In Section 4, the authors immediately start with the continuous-time perspective, without even mentioning the gradient dynamics or some related ansatz. The authors may want to mention that they use the ansatz w_k=W(kh), where W is a smooth curve, and take h to 0 to obtain the ODE models, as is done in [1].\n3) The notation list at the beginning of Section 4.3 is too long and not clear. In particular, l(p) is not even defined before appearing in (14), and \\sigma_s seems to be overridden by the notation activation(w_s, X_i) and does not appear later in the path gradient, which is weird. Shouldn't there be \\sigma_s inside the formula of (14)?\n\n3. Again on the rigorousness side, the paper is very non-rigorous when stating the claims and theorems.\n1) The authors claim that \"This result will hold for other l_p losses\", but indeed what holds is different for l_p losses (the rate is scaled by p/2).\n2) Section 4.2 does not make any sense. The authors should either directly cite the corresponding content in [25], which are much clearer, or directly invoke the standard linear ODE theory and use the matrix exponential and Taylor expansion to make the explanations.\n3) The paper seems to use a very informal argument that H stays close to its initial value to derive all the theory, which is only empirically checked in the appendix. But given the proportion of the theory part of this paper, I think the paper should either clearly state the assumption as H being constant, or follow the manner of [25] to prove that H stays close to some fixed matrix and use this to prove the other theorems rigorously. Otherwise, the theory part is both hard to understand and verify. \n4) Proposition 5.1 should clearly state whether the statement is in the expectation sense, or high probability sense, or something else. \n5) The explanation in Section 5.2 is rather unclear. In particular, I don't understand why \"eigenvalues of H(g) are pushed in the direction of g\" implies that \"the updates prefer the direction of u\", and why this then further implies something related to the momentum acceleration. The authors should provide a rigorous statement here.\n6) The authors claim at the bottom of page 1 that they show adding a new layer leads to H(t) being decomposed into an adaptive learning rate term and a momentum term. But the adaptive learning rate part is not showing anywhere later in the paper. \n\nMinor comments:\n1) In equation (5), there should be a minus sign on the right-hand side.\n2) In Section 5.2, there is no \"Equation 5.2\". It should be something else.\n3) The authors may want to add citations to [2] (which is a concurrent work with [25] on essentially the same topic) and [3] (which is a predecessor work of neural ODE).\n\n[1] Su, Weijie, Stephen Boyd, and Emmanuel Candes. \"A differential equation for modeling Nesterov\u2019s accelerated gradient method: Theory and insights.\" Advances in Neural Information Processing Systems. 2014.\n[2] Allen-Zhu, Zeyuan, Yuanzhi Li, and Zhao Song. \"A convergence theory for deep learning via over-parameterization.\" arXiv preprint arXiv:1811.03962 (2018).\n[3] Lu, Yiping, et al. \"Beyond finite layer neural networks: Bridging deep architectures and numerical differential equations.\" arXiv preprint arXiv:1710.10121 (2017).\n\n################## post rebuttal ##################\nAfter reading the authors' rebuttal, I decide to raise my rating to 3 (weak reject).", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper118/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper118/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Model Architecture Controls Gradient Descent Dynamics: A Combinatorial Path-Based Formula", "authors": ["Xin Zhou", "Newsha Ardalani"], "authorids": ["chow459@gmail.com", "newsha@baidu.com"], "keywords": [], "abstract": "Recently, there has been a growing interest in automatically exploring neural network architecture design space with the goal of finding an architecture that improves performance (characterized as improved accuracy, speed of training, or resource requirements). However, our theoretical understanding of how model architecture affects performance or accuracy is limited. In this paper, we study the impact of model architecture on the speed of training in the context of gradient descent optimization. We model gradient descent as a first-order ODE and use ODE's coefficient matrix H to characterize the convergence rate. We introduce a simple analysis technique that enumerates H in terms of all possible ``paths'' in the network. \n We show that changes in model architecture parameters reflect as changes in the number of paths and the properties of each path, which jointly control the speed of convergence. We believe our analysis technique is useful in reasoning about more complex model architecture modifications.", "pdf": "/pdf/dd8f140ce2d4bafe4ab99a923a16a702443e9348.pdf", "paperhash": "zhou|model_architecture_controls_gradient_descent_dynamics_a_combinatorial_pathbased_formula", "original_pdf": "/attachment/8daed6816bd8fa5f3747e70f3d2a5e7a3fbd1054.pdf", "_bibtex": "@misc{\nzhou2020model,\ntitle={Model Architecture Controls Gradient Descent Dynamics: A Combinatorial Path-Based Formula},\nauthor={Xin Zhou and Newsha Ardalani},\nyear={2020},\nurl={https://openreview.net/forum?id=B1xw9n4Kwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "B1xw9n4Kwr", "replyto": "B1xw9n4Kwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper118/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper118/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575670692722, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper118/Reviewers"], "noninvitees": [], "tcdate": 1570237756809, "tmdate": 1575670692739, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper118/-/Official_Review"}}}, {"id": "H1gfE7EjjB", "original": null, "number": 9, "cdate": 1573761834020, "ddate": null, "tcdate": 1573761834020, "tmdate": 1573762082107, "tddate": null, "forum": "B1xw9n4Kwr", "replyto": "BylpetaaFr", "invitation": "ICLR.cc/2020/Conference/Paper118/-/Official_Comment", "content": {"title": "Responses to Reviewer #1", "comment": "In particular, I found the argument that, du/dt prefers the direction of u itself which carries information about past directions therefore this is similar to momentum, to be not convincing. \n\nWe updated the exposition of this to be the new Section 6.2. We replaced the words with an inequality which hopefully makes the point more explicit.\n\n\nThe third implication argues that we need H to be full-rank for fast convergence, and in order to achieve this we want to increase the number of paths in the network, which is interesting.\n\nThank you for liking the point.\n\n\nThere are a few other things that could be clarified:\n- the terminology in section 4.3 is a bit confusing, w_s seems to be associated with an edge in the graph, but is also referred to as a node, \u201cwe denote the activation value at node w_s with activation(w_s, X_i) \u201d.\n\nThanks for pointing this out. We say the value at the right vertex of w_s in the next sentence but we meant the node after the edge w_s here. \n\n\n- on page 6 there is a reference to Equation 5.2 which does not exist\n\nThis is fixed with a better exposition. Please take a look.\n\n- the results in Figure 4a is counter-intuitive - is this showing wider networks actually converge slower?  Isn\u2019t this against the argument of this paper?\n\nWe unified our definition of convergence rate. We mean $1-\\frac{Loss_{t+1}}{Loss_{t}}$. \nThe figure is drawn in log scale. So convergence rate is the slope of the lines (after an initial \u201cshock\u201d that we can\u2019t quite explain). Wider networks have steeper lines hence bigger convergence rate.\n\n\n- it is unclear how the convergence rate lambda values are computed in Figure 4b, the curves in Figure 4a clearly doesn\u2019t follow an exponential decay pattern.\n\nThe convergence rates are taken as the slopes of the lines after the initial \u201cshock\u201d and plotted against model width. We should have stated $1-\\frac{Loss_{t+1}}{Loss_{t}}$ whereas the exponential decay only fits figure 3."}, "signatures": ["ICLR.cc/2020/Conference/Paper118/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper118/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Model Architecture Controls Gradient Descent Dynamics: A Combinatorial Path-Based Formula", "authors": ["Xin Zhou", "Newsha Ardalani"], "authorids": ["chow459@gmail.com", "newsha@baidu.com"], "keywords": [], "abstract": "Recently, there has been a growing interest in automatically exploring neural network architecture design space with the goal of finding an architecture that improves performance (characterized as improved accuracy, speed of training, or resource requirements). However, our theoretical understanding of how model architecture affects performance or accuracy is limited. In this paper, we study the impact of model architecture on the speed of training in the context of gradient descent optimization. We model gradient descent as a first-order ODE and use ODE's coefficient matrix H to characterize the convergence rate. We introduce a simple analysis technique that enumerates H in terms of all possible ``paths'' in the network. \n We show that changes in model architecture parameters reflect as changes in the number of paths and the properties of each path, which jointly control the speed of convergence. We believe our analysis technique is useful in reasoning about more complex model architecture modifications.", "pdf": "/pdf/dd8f140ce2d4bafe4ab99a923a16a702443e9348.pdf", "paperhash": "zhou|model_architecture_controls_gradient_descent_dynamics_a_combinatorial_pathbased_formula", "original_pdf": "/attachment/8daed6816bd8fa5f3747e70f3d2a5e7a3fbd1054.pdf", "_bibtex": "@misc{\nzhou2020model,\ntitle={Model Architecture Controls Gradient Descent Dynamics: A Combinatorial Path-Based Formula},\nauthor={Xin Zhou and Newsha Ardalani},\nyear={2020},\nurl={https://openreview.net/forum?id=B1xw9n4Kwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1xw9n4Kwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper118/Authors", "ICLR.cc/2020/Conference/Paper118/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper118/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper118/Reviewers", "ICLR.cc/2020/Conference/Paper118/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper118/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper118/Authors|ICLR.cc/2020/Conference/Paper118/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176139, "tmdate": 1576860534082, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper118/Authors", "ICLR.cc/2020/Conference/Paper118/Reviewers", "ICLR.cc/2020/Conference/Paper118/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper118/-/Official_Comment"}}}, {"id": "rylPzENssS", "original": null, "number": 10, "cdate": 1573762063046, "ddate": null, "tcdate": 1573762063046, "tmdate": 1573762063046, "tddate": null, "forum": "B1xw9n4Kwr", "replyto": "SkxOcKDTYS", "invitation": "ICLR.cc/2020/Conference/Paper118/-/Official_Comment", "content": {"title": "Responses to Reviewer #3", "comment": "One concern is about the description in section 4.1 and 4.2 for preliminaries results. I would suggest that emphasizing the fact 4.1 as an already discussed fact makes easy for readers to follow the results and focus more on the paper's contribution. The cited paper by Du et al [13] was published at the last year's ICLR, and the contents in 4.1 and 4.2 would be due to it including the formulation as gradient flow (gradient descent in continuous time). Given that it is already known that H is the determining factor for the convergence, then the contribution and usefulness of the proposed new representation of eq (15) should be discussed. \nThanks for the suggestion. The hope is that through symmetry, the path abstraction hides away the book keeping of summations and indices. The uses we came up with are now in a separate Section 6 which separately discusses the easy way to see expected convergence rate scale linearly with width in 6.1. More exact analysis than previous literature of depth and momentum in 6.2 and path count is important for convergence in 6.3. The remark F.1 may also be of interest as a network-wide perspective.\n\n\nAt the first glance, the use of gradient flow w.r.t prediction u instead of parameter w might be (misleadingly) seen as the idea of this paper. The sentences in the abstract can also mislead readers to this. Also check the paper by Du et al in ICML 2019 'Gradient descent finds global minimum of deep neural networks'. \n\nSorry for the confusion. We moved all of prior work into a separate section and give references to Gradient descent finds global minimum of deep neural networks as [14].\n\n\nTwo minor points: \n1) parentheses for citing references and eq numbers are quite confusing. Please check the style format for citations.\nFixed\n2) in Figure 2, pre(w_0) is written as \"presum(w_0)\".\nFixed.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper118/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper118/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Model Architecture Controls Gradient Descent Dynamics: A Combinatorial Path-Based Formula", "authors": ["Xin Zhou", "Newsha Ardalani"], "authorids": ["chow459@gmail.com", "newsha@baidu.com"], "keywords": [], "abstract": "Recently, there has been a growing interest in automatically exploring neural network architecture design space with the goal of finding an architecture that improves performance (characterized as improved accuracy, speed of training, or resource requirements). However, our theoretical understanding of how model architecture affects performance or accuracy is limited. In this paper, we study the impact of model architecture on the speed of training in the context of gradient descent optimization. We model gradient descent as a first-order ODE and use ODE's coefficient matrix H to characterize the convergence rate. We introduce a simple analysis technique that enumerates H in terms of all possible ``paths'' in the network. \n We show that changes in model architecture parameters reflect as changes in the number of paths and the properties of each path, which jointly control the speed of convergence. We believe our analysis technique is useful in reasoning about more complex model architecture modifications.", "pdf": "/pdf/dd8f140ce2d4bafe4ab99a923a16a702443e9348.pdf", "paperhash": "zhou|model_architecture_controls_gradient_descent_dynamics_a_combinatorial_pathbased_formula", "original_pdf": "/attachment/8daed6816bd8fa5f3747e70f3d2a5e7a3fbd1054.pdf", "_bibtex": "@misc{\nzhou2020model,\ntitle={Model Architecture Controls Gradient Descent Dynamics: A Combinatorial Path-Based Formula},\nauthor={Xin Zhou and Newsha Ardalani},\nyear={2020},\nurl={https://openreview.net/forum?id=B1xw9n4Kwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1xw9n4Kwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper118/Authors", "ICLR.cc/2020/Conference/Paper118/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper118/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper118/Reviewers", "ICLR.cc/2020/Conference/Paper118/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper118/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper118/Authors|ICLR.cc/2020/Conference/Paper118/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176139, "tmdate": 1576860534082, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper118/Authors", "ICLR.cc/2020/Conference/Paper118/Reviewers", "ICLR.cc/2020/Conference/Paper118/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper118/-/Official_Comment"}}}, {"id": "H1gKoGEosH", "original": null, "number": 8, "cdate": 1573761697235, "ddate": null, "tcdate": 1573761697235, "tmdate": 1573761697235, "tddate": null, "forum": "B1xw9n4Kwr", "replyto": "HJlaIEjQqr", "invitation": "ICLR.cc/2020/Conference/Paper118/-/Official_Comment", "content": {"title": "Response to Reviewer #2", "comment": "1. On the novelty side, characterizing the convergence via the H matrix is not new, and most of the discussions in Section 4 have appeared in exactly the same form in the previous works [13] (two-layer) and [25] (general), which are also cited in this paper. In addition, the path formula is also very closely related (if not completely the same as) with the expansion of G matrix in Section 4 of [25] (where G is the H in this paper), which decomposes the G (or H) matrix into summation over layers. These facts largely lower the contribution of this paper on a high level. \n\nThey are closely related and they calculate essentially the same object. Previous work used layer by layer recursive expressions. The path expansion is novel. The convergence rate and width argument is used to promote this exploitation of path symmetry more to lighten the burden of book keeping for further research. For the remaining two contributions, the path expansion is used to gain further insights into the relationship between depth and momentum in value space and path counts and convergence.\n\n3. Again on the rigorousness side, the paper is very non-rigorous when stating the claims and theorems.\n3) The paper seems to use a very informal argument that H stays close to its initial value to derive all the theory, which is only empirically checked in the appendix. But given the proportion of the theory part of this paper, I think the paper should either clearly state the assumption as H being constant, or follow the manner of [25] to prove that H stays close to some fixed matrix and use this to prove the other theorems rigorously. Otherwise, the theory part is both hard to understand and verify. \n\nOur work is built on ideas from prior work that shows (1) for sufficiently large neuraL network H stays close to its initial value. (2) In overparametrized models the weights stay close to their initial value, hence H remains close to its initial value. We have clarified this in our updated version.\n\n4) Proposition 5.1 should clearly state whether the statement is in the expectation sense, or high probability sense, or something else. \n\nThanks for pointing this out. It is in the expectation sense, so we modified it to say the expected convergence rate.\n\n5) The explanation in Section 5.2 is rather unclear. In particular, I don't understand why \"eigenvalues of H(g) are pushed in the direction of g\" implies that \"the updates prefer the direction of u\", and why this then further implies something related to the momentum acceleration. The authors should provide a rigorous statement here.\n\nWe apologize for the exposition of the old Section 5.2. It is now Section 6.2. We provided the rigorous statement as an inequality and discussed the similarities and differences between this momentum acceleration and momentum acceleration used in gradient descent. Please take a look.\n\n6) The authors claim at the bottom of page 1 that they show adding a new layer leads to H(t) being decomposed into an adaptive learning rate term and a momentum term. But the adaptive learning rate part is not showing anywhere later in the paper. \n\nAdaptive learning rate is captured by $\\mu^2$ of Equation (21) in original submission and in Equation (34) in the updated version. In the new revision, we focus more on the discussion of momentum in the forefront and adaptive learning rate only appears in the appendix F now.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper118/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper118/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Model Architecture Controls Gradient Descent Dynamics: A Combinatorial Path-Based Formula", "authors": ["Xin Zhou", "Newsha Ardalani"], "authorids": ["chow459@gmail.com", "newsha@baidu.com"], "keywords": [], "abstract": "Recently, there has been a growing interest in automatically exploring neural network architecture design space with the goal of finding an architecture that improves performance (characterized as improved accuracy, speed of training, or resource requirements). However, our theoretical understanding of how model architecture affects performance or accuracy is limited. In this paper, we study the impact of model architecture on the speed of training in the context of gradient descent optimization. We model gradient descent as a first-order ODE and use ODE's coefficient matrix H to characterize the convergence rate. We introduce a simple analysis technique that enumerates H in terms of all possible ``paths'' in the network. \n We show that changes in model architecture parameters reflect as changes in the number of paths and the properties of each path, which jointly control the speed of convergence. We believe our analysis technique is useful in reasoning about more complex model architecture modifications.", "pdf": "/pdf/dd8f140ce2d4bafe4ab99a923a16a702443e9348.pdf", "paperhash": "zhou|model_architecture_controls_gradient_descent_dynamics_a_combinatorial_pathbased_formula", "original_pdf": "/attachment/8daed6816bd8fa5f3747e70f3d2a5e7a3fbd1054.pdf", "_bibtex": "@misc{\nzhou2020model,\ntitle={Model Architecture Controls Gradient Descent Dynamics: A Combinatorial Path-Based Formula},\nauthor={Xin Zhou and Newsha Ardalani},\nyear={2020},\nurl={https://openreview.net/forum?id=B1xw9n4Kwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1xw9n4Kwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper118/Authors", "ICLR.cc/2020/Conference/Paper118/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper118/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper118/Reviewers", "ICLR.cc/2020/Conference/Paper118/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper118/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper118/Authors|ICLR.cc/2020/Conference/Paper118/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176139, "tmdate": 1576860534082, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper118/Authors", "ICLR.cc/2020/Conference/Paper118/Reviewers", "ICLR.cc/2020/Conference/Paper118/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper118/-/Official_Comment"}}}, {"id": "HygrYMVsiB", "original": null, "number": 7, "cdate": 1573761661150, "ddate": null, "tcdate": 1573761661150, "tmdate": 1573761661150, "tddate": null, "forum": "B1xw9n4Kwr", "replyto": "HJlaIEjQqr", "invitation": "ICLR.cc/2020/Conference/Paper118/-/Official_Comment", "content": {"title": "Response to Reviewer #2 editorial questions", "comment": "2. On the rigorousness side, the paper is not very consistent in the notation. \n1) The notation is not very consistent. The authors use H in the notation section to denote the matrix, use X_i to denote the data input, but use \\bf{H} and x_i (lower cased) subsequently. \nWe made H consistent and uses $X_i$ as a vector and $x_i$ a number consistently now.\n\n2) In Section 4, the authors immediately start with the continuous-time perspective, without even mentioning the gradient dynamics or some related ansatz. The authors may want to mention that they use the ansatz w_k=W(kh), where W is a smooth curve, and take h to 0 to obtain the ODE models, as is done in [1].\nWe fixed this by adding the reference for going from step-wise gradient descent to the continuous time gradient flow at the top of Section 4.  \n\n3) The notation list at the beginning of Section 4.3 is too long and not clear.  In particular, l(p) is not even defined before appearing in (14), \n\nl(p) refers to the length of the path p. We clarified this in the new version and made sure that the concepts are sequentially introduced.\n\nand \\sigma_s seems to be overridden by the notation activation(w_s, X_i) and does not appear later in the path gradient, which is weird. Shouldn't there be \\sigma_s inside the formula of (14)?\n\nWe removed the notation activation(w_s, X_i). By \\sigma_s, we are guessing the reviewer meant \\omega_s? So \\omega_s is not part of the product because in the end, the path gradient is part of the derivative with respect to \\omega_s, so \\omega_s won\u2019t be part of it. path_gradient(w, X_i) is more like w.r.t. w more than a function of w.\n\n3. Again on the rigorousness side, the paper is very non-rigorous when stating the claims and theorems.\n1) The authors claim that \"This result will hold for other l_p losses\", but indeed what holds is different for l_p losses (the rate is scaled by p/2).\nWe did point out the constant scale in the appendix. We also added \u201cup to a constant\u201d in the main text.\n\n2) Section 4.2 does not make any sense. The authors should either directly cite the corresponding content in [25], which are much clearer, or directly invoke the standard linear ODE theory and use the matrix exponential and Taylor expansion to make the explanations.\n\tAs part of moving the old Section 4.1, 4.2 into a separate background Section. We removed this subsection, only kept the conclusion and refer to [25] now at the beginning of the section.\n\nMinor comments:\n1) In equation (5), there should be a minus sign on the right-hand side.\nFixed.\n2) In Section 5.2, there is no \"Equation 5.2\". It should be something else.\nReorganized.\n3) The authors may want to add citations to [2] (which is a concurrent work with [25] on essentially the same topic) and [3] (which is a predecessor work of neural ODE).\nThanks for pointing these out. References added.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper118/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper118/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Model Architecture Controls Gradient Descent Dynamics: A Combinatorial Path-Based Formula", "authors": ["Xin Zhou", "Newsha Ardalani"], "authorids": ["chow459@gmail.com", "newsha@baidu.com"], "keywords": [], "abstract": "Recently, there has been a growing interest in automatically exploring neural network architecture design space with the goal of finding an architecture that improves performance (characterized as improved accuracy, speed of training, or resource requirements). However, our theoretical understanding of how model architecture affects performance or accuracy is limited. In this paper, we study the impact of model architecture on the speed of training in the context of gradient descent optimization. We model gradient descent as a first-order ODE and use ODE's coefficient matrix H to characterize the convergence rate. We introduce a simple analysis technique that enumerates H in terms of all possible ``paths'' in the network. \n We show that changes in model architecture parameters reflect as changes in the number of paths and the properties of each path, which jointly control the speed of convergence. We believe our analysis technique is useful in reasoning about more complex model architecture modifications.", "pdf": "/pdf/dd8f140ce2d4bafe4ab99a923a16a702443e9348.pdf", "paperhash": "zhou|model_architecture_controls_gradient_descent_dynamics_a_combinatorial_pathbased_formula", "original_pdf": "/attachment/8daed6816bd8fa5f3747e70f3d2a5e7a3fbd1054.pdf", "_bibtex": "@misc{\nzhou2020model,\ntitle={Model Architecture Controls Gradient Descent Dynamics: A Combinatorial Path-Based Formula},\nauthor={Xin Zhou and Newsha Ardalani},\nyear={2020},\nurl={https://openreview.net/forum?id=B1xw9n4Kwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1xw9n4Kwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper118/Authors", "ICLR.cc/2020/Conference/Paper118/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper118/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper118/Reviewers", "ICLR.cc/2020/Conference/Paper118/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper118/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper118/Authors|ICLR.cc/2020/Conference/Paper118/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176139, "tmdate": 1576860534082, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper118/Authors", "ICLR.cc/2020/Conference/Paper118/Reviewers", "ICLR.cc/2020/Conference/Paper118/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper118/-/Official_Comment"}}}, {"id": "SJgzi6QjjS", "original": null, "number": 5, "cdate": 1573760410373, "ddate": null, "tcdate": 1573760410373, "tmdate": 1573760410373, "tddate": null, "forum": "B1xw9n4Kwr", "replyto": "B1xw9n4Kwr", "invitation": "ICLR.cc/2020/Conference/Paper118/-/Official_Comment", "content": {"title": "Submission update and response to common questions", "comment": "We thank the reviewers for the helpful feedback and interests in various points made in the paper.  On a high level, we made the following changes. Each individual comment will be addressed below. We reorganized the old Section 4.1 and Section 4.2 into a separate Section called background to make clearer the separation between prior work and our contribution. We also thank reviewers for the feedback on the writing of the subsection on momentum. We re-formalized the conclusion with a Proposition then give remarks on similarities and differences. We think this would be a much clearer exposition. \n\nWe hope all these changes help distinguish our novelty and contribution better, which can be summarized as:\n\n(1) We show that H can be expressed in terms of the number of paths and the form of the paths. To the best of our knowledge, this has never been shown before. The closest to our work is [25] (now [14]) which studies H inductively over layers (not a global summation over paths) and model it recursively (not directly). After all H can be expanded in many different ways but how to expand it gives us the power to analyze the  impact of model architecture on convergence. \n(2) Showing that expected convergence rate scales linearly with network width. This demonstrates the path-based decomposition exploits the networks\u2019 path symmetry better compared to the recursive expressions. The hope is that this formalism liberates future researchers from working with many indices and help them reason about different activation functions universally. The conclusion also focuses on the expected convergence rate while varying network width instead of convergence bounds at the limit as prior works do.\n(3) We characterize the depth acceleration effect quantitatively, study the similarities and differences between depth-induced momentum and the commonly applied momentum in gradient descent. We observe some effects only observable through the value space analysis. All of this is done under less restrictive assumptions on the network. This study of momentum in value space is new.\n(4) We show that the number of paths compared to the number of nodes has a predominant impact on the convergence using our global path based expansion and that the Gram matrix H needs to be full-rank for fast convergence, and in order to achieve this, we want to increase the number of paths in the network. "}, "signatures": ["ICLR.cc/2020/Conference/Paper118/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper118/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Model Architecture Controls Gradient Descent Dynamics: A Combinatorial Path-Based Formula", "authors": ["Xin Zhou", "Newsha Ardalani"], "authorids": ["chow459@gmail.com", "newsha@baidu.com"], "keywords": [], "abstract": "Recently, there has been a growing interest in automatically exploring neural network architecture design space with the goal of finding an architecture that improves performance (characterized as improved accuracy, speed of training, or resource requirements). However, our theoretical understanding of how model architecture affects performance or accuracy is limited. In this paper, we study the impact of model architecture on the speed of training in the context of gradient descent optimization. We model gradient descent as a first-order ODE and use ODE's coefficient matrix H to characterize the convergence rate. We introduce a simple analysis technique that enumerates H in terms of all possible ``paths'' in the network. \n We show that changes in model architecture parameters reflect as changes in the number of paths and the properties of each path, which jointly control the speed of convergence. We believe our analysis technique is useful in reasoning about more complex model architecture modifications.", "pdf": "/pdf/dd8f140ce2d4bafe4ab99a923a16a702443e9348.pdf", "paperhash": "zhou|model_architecture_controls_gradient_descent_dynamics_a_combinatorial_pathbased_formula", "original_pdf": "/attachment/8daed6816bd8fa5f3747e70f3d2a5e7a3fbd1054.pdf", "_bibtex": "@misc{\nzhou2020model,\ntitle={Model Architecture Controls Gradient Descent Dynamics: A Combinatorial Path-Based Formula},\nauthor={Xin Zhou and Newsha Ardalani},\nyear={2020},\nurl={https://openreview.net/forum?id=B1xw9n4Kwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1xw9n4Kwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper118/Authors", "ICLR.cc/2020/Conference/Paper118/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper118/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper118/Reviewers", "ICLR.cc/2020/Conference/Paper118/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper118/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper118/Authors|ICLR.cc/2020/Conference/Paper118/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176139, "tmdate": 1576860534082, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper118/Authors", "ICLR.cc/2020/Conference/Paper118/Reviewers", "ICLR.cc/2020/Conference/Paper118/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper118/-/Official_Comment"}}}, {"id": "SkxOcKDTYS", "original": null, "number": 1, "cdate": 1571809679645, "ddate": null, "tcdate": 1571809679645, "tmdate": 1572972636529, "tddate": null, "forum": "B1xw9n4Kwr", "replyto": "B1xw9n4Kwr", "invitation": "ICLR.cc/2020/Conference/Paper118/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper presents a simple and intuitive interpretation of the dynamics of gradient descent between labels and predictions rewritten in terms of all possible paths from inputs to outputs in FC networks. The basic starting point is from the gradient flow (gradient descent in continuous time) by Du et al 2018 [13] on the difference y-u of labels y predictions u (eq (6) or (7) in the paper) instead of the standard gradient flow dW/dt = - \u2202L(W)/\u2202w. The coefficient matrix H of the system is known to determine the convergence properties, and this can be rewritten with respect to path-wise sums of gradients through the chain rule (Theorem 4.1).  This provides some intuitive interpretations for several facts discussed in the community: 1) the linear convergence rate is more intuitively obtained compared to the naive derivations (Remark 5.2 with comparison to [13]). 2) the 'depth' of FC network affects the convergence like momentum (section 5.2). 3) the number of paths compared to the number of nodes has a predominant impact on the convergence (section 5.3). The paper also demonstrates several experiments to understand the impact of depth or paths on the convergence.\n\nThough the paper's contribution is a quite simple path decomposition using chain rules for the coefficient matrix H of the gradient flow, it indeed provides several intuitive understandings on the impact of paths onto the gradient-descent convergence. All implications are basically confirming already known things in different (path-based) words, and the impact or novelty is rather small, but nevertheless, it would be informative. \n\nOne concern is about the description in section 4.1 and 4.2 for preliminaries results. I would suggest that emphasizing the fact 4.1 as an already discussed fact makes easy for readers to follow the results and focus more on the paper's contribution. The cited paper by Du et al [13] was published at the last year's ICLR, and the contents in 4.1 and 4.2 would be due to it including the formulation as gradient flow (gradient descent in continuous time). Given that it is already known that H is the determining factor for the convergence, then the contribution and usefulness of the proposed new representation of eq (15) should be discussed. At the first glance, the use of gradient flow w.r.t prediction u instead of parameter w might be (misleadingly) seen as the idea of this paper. The sentences in the abstract can also mislead readers to this. Also check the paper by Du et al in ICML 2019 'Gradient descent finds global minimum of deep neural networks'. \n\nTwo minor points: \n1) parentheses for citing references and eq numbers are quite confusing. Please check the style format for citations.\n2) in Figure 2, pre(w_0) is written as \"presum(w_0)\".\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper118/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper118/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Model Architecture Controls Gradient Descent Dynamics: A Combinatorial Path-Based Formula", "authors": ["Xin Zhou", "Newsha Ardalani"], "authorids": ["chow459@gmail.com", "newsha@baidu.com"], "keywords": [], "abstract": "Recently, there has been a growing interest in automatically exploring neural network architecture design space with the goal of finding an architecture that improves performance (characterized as improved accuracy, speed of training, or resource requirements). However, our theoretical understanding of how model architecture affects performance or accuracy is limited. In this paper, we study the impact of model architecture on the speed of training in the context of gradient descent optimization. We model gradient descent as a first-order ODE and use ODE's coefficient matrix H to characterize the convergence rate. We introduce a simple analysis technique that enumerates H in terms of all possible ``paths'' in the network. \n We show that changes in model architecture parameters reflect as changes in the number of paths and the properties of each path, which jointly control the speed of convergence. We believe our analysis technique is useful in reasoning about more complex model architecture modifications.", "pdf": "/pdf/dd8f140ce2d4bafe4ab99a923a16a702443e9348.pdf", "paperhash": "zhou|model_architecture_controls_gradient_descent_dynamics_a_combinatorial_pathbased_formula", "original_pdf": "/attachment/8daed6816bd8fa5f3747e70f3d2a5e7a3fbd1054.pdf", "_bibtex": "@misc{\nzhou2020model,\ntitle={Model Architecture Controls Gradient Descent Dynamics: A Combinatorial Path-Based Formula},\nauthor={Xin Zhou and Newsha Ardalani},\nyear={2020},\nurl={https://openreview.net/forum?id=B1xw9n4Kwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "B1xw9n4Kwr", "replyto": "B1xw9n4Kwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper118/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper118/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575670692722, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper118/Reviewers"], "noninvitees": [], "tcdate": 1570237756809, "tmdate": 1575670692739, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper118/-/Official_Review"}}}, {"id": "BylpetaaFr", "original": null, "number": 2, "cdate": 1571834100838, "ddate": null, "tcdate": 1571834100838, "tmdate": 1572972636483, "tddate": null, "forum": "B1xw9n4Kwr", "replyto": "B1xw9n4Kwr", "invitation": "ICLR.cc/2020/Conference/Paper118/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper studies the training dynamics of a neural network model as a dynamical system.  The authors proposed a path-based approach to compute the derivatives that would appear in the H matrix which governs the learning dynamics.  They further utilized this formulation to (1) simplify the analysis of convergence rate of 2-layer neural networks w.r.t. width; and (2) presented an argument for the similarity between added depth in the network and momentum-based optimization; and (3) also argued about the importance of the number of paths for fast convergence.\n\nThe dynamical systems view of the learning process is quite new to me, even though I\u2019m aware of a few recent papers exploring this.  I found the paper to be mostly clear and not too hard to follow, and the dynamical systems perspective interesting.  The path gradient is quite intuitive, but I\u2019m a bit surprised there\u2019s no prior work (at least not discussed in this paper) studying the relationship between gradients and the paths in the network.\n\nIt is a bit hard for me to judge the significance of this work because of my lack of context.  The main implications of the path gradients were presented in section 5.  The first part shows that using their theorem 4.1 can simplify the derivation of the linear relationship between convergence rate and the width of 2-layer nets.  This is a simplification but the original derivation is not complicated either.  The second part tried to draw a relationship between added depth in a network and momentum-based optimization, which I found to be a bit hand-wavy.  In particular, I found the argument that, du/dt prefers the direction of u itself which carries information about past directions therefore this is similar to momentum, to be not convincing.  The third implication argues that we need H to be full-rank for fast convergence, and in order to achieve this we want to increase the number of paths in the network, which is interesting.\n\nThere are a few other things that could be clarified:\n- the terminology in section 4.3 is a bit confusing, w_s seems to be associated with an edge in the graph, but is also referred to as a node, \u201cwe denote the activation value at node w_s with activation(w_s, X_i) \u201d.\n- on page 6 there is a reference to Equation 5.2 which does not exist\n- the results in Figure 4a is counter-intuitive - is this showing wider networks actually converge slower?  Isn\u2019t this against the argument of this paper?\n- it is unclear how the convergence rate lambda values are computed in Figure 4b, the curves in Figure 4a clearly doesn\u2019t follow an exponential decay pattern.\n\nOverall I found this paper presented some interesting ideas, but may need a bit more work to be ready to be published.  Happy to change my judgement however, if other more experienced reviewers can comment better on the significance of this work."}, "signatures": ["ICLR.cc/2020/Conference/Paper118/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper118/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Model Architecture Controls Gradient Descent Dynamics: A Combinatorial Path-Based Formula", "authors": ["Xin Zhou", "Newsha Ardalani"], "authorids": ["chow459@gmail.com", "newsha@baidu.com"], "keywords": [], "abstract": "Recently, there has been a growing interest in automatically exploring neural network architecture design space with the goal of finding an architecture that improves performance (characterized as improved accuracy, speed of training, or resource requirements). However, our theoretical understanding of how model architecture affects performance or accuracy is limited. In this paper, we study the impact of model architecture on the speed of training in the context of gradient descent optimization. We model gradient descent as a first-order ODE and use ODE's coefficient matrix H to characterize the convergence rate. We introduce a simple analysis technique that enumerates H in terms of all possible ``paths'' in the network. \n We show that changes in model architecture parameters reflect as changes in the number of paths and the properties of each path, which jointly control the speed of convergence. We believe our analysis technique is useful in reasoning about more complex model architecture modifications.", "pdf": "/pdf/dd8f140ce2d4bafe4ab99a923a16a702443e9348.pdf", "paperhash": "zhou|model_architecture_controls_gradient_descent_dynamics_a_combinatorial_pathbased_formula", "original_pdf": "/attachment/8daed6816bd8fa5f3747e70f3d2a5e7a3fbd1054.pdf", "_bibtex": "@misc{\nzhou2020model,\ntitle={Model Architecture Controls Gradient Descent Dynamics: A Combinatorial Path-Based Formula},\nauthor={Xin Zhou and Newsha Ardalani},\nyear={2020},\nurl={https://openreview.net/forum?id=B1xw9n4Kwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "B1xw9n4Kwr", "replyto": "B1xw9n4Kwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper118/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper118/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575670692722, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper118/Reviewers"], "noninvitees": [], "tcdate": 1570237756809, "tmdate": 1575670692739, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper118/-/Official_Review"}}}], "count": 10}