{"notes": [{"ddate": null, "legacy_migration": true, "tmdate": 1363717680000, "tcdate": 1363717680000, "number": 3, "id": "aK4z5qBF7bEod", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "YBi6KFA7PfKo5", "replyto": "YBi6KFA7PfKo5", "signatures": ["Hinrich Schuetze"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "Thanks for your comments! The suggestions seem all good and pertinent to us and (in case the paper should be accepted and assuming there is enough space) we will incorporate them when revising the paper. In particular: relate the new method to overview in Turney&Pantel, to kernel PCA and matrix factorization approaches; expand on discussion of focality, addressing concerns about broad applicability (if it's only used as a diagnostic, then it may not be a huge concern that it's somewhat unwieldy); discussion of Turian, Socher and Maas; more details and more thorough description of 1layer vs 2layer (we thought this was pretty directly analogous to single-layer learning vs two-layer deep learning, but will expand on this in a potentially revised version).\r\n\r\nWe also totally agree that ideally larger experiments on previous data sets should be done. We were hoping that more conceptual papers (introducing new methods and metrics) would be ok without immediate large experiments. We would try to conduct some larger experiments if the paper gets accepted, but cannot promise these would be ready for the conference."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Two SVDs produce more focal deep learning representations", "decision": "conferencePoster-iclr2013-workshop", "abstract": "A key characteristic of work on deep learning and neural networks in general is that it relies on representations of the input that support generalization, robust inference, domain adaptation and other desirable functionalities. Much recent progress in the field has focused on efficient and effective methods for computing representations. In this paper, we propose an alternative method that is more efficient than prior work and produces representations that have a property we call focality -- a property we hypothesize to be important for neural network representations. The method consists of a simple application of two consecutive SVDs and is inspired by Anandkumar (2012).", "pdf": "https://arxiv.org/abs/1301.3627", "paperhash": "schuetze|two_svds_produce_more_focal_deep_learning_representations", "keywords": [], "conflicts": [], "authors": ["Hinrich Schuetze", "Christian Scheible"], "authorids": ["hinrichwork@googlemail.com", "christian.scheible@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1363717200000, "tcdate": 1363717200000, "number": 1, "id": "vNpsUSMf3tNfx", "invitation": "ICLR.cc/2013/-/submission/reply", "forum": "YBi6KFA7PfKo5", "replyto": "VFwT2CLWfA2kU", "signatures": ["Hinrich Schuetze"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "Thanks for your comments! If the paper is accepted, we will expand the description of the discrimination task and explain in more detail how it is related to focality (the idea is that a single hidden unit does well on the discrimination -- which is what focality is supposed to capture).\r\n\r\nWe will also expand the discussion of related methods (LSA, PCA, denoising auto-encoder) if the paper accepted (assuming there is space -- which should be the case).\r\n\r\nWe will cite&discuss (Collobert and Weston) and (Socher). Pointers to other relevant literature would be appreciated."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Two SVDs produce more focal deep learning representations", "decision": "conferencePoster-iclr2013-workshop", "abstract": "A key characteristic of work on deep learning and neural networks in general is that it relies on representations of the input that support generalization, robust inference, domain adaptation and other desirable functionalities. Much recent progress in the field has focused on efficient and effective methods for computing representations. In this paper, we propose an alternative method that is more efficient than prior work and produces representations that have a property we call focality -- a property we hypothesize to be important for neural network representations. The method consists of a simple application of two consecutive SVDs and is inspired by Anandkumar (2012).", "pdf": "https://arxiv.org/abs/1301.3627", "paperhash": "schuetze|two_svds_produce_more_focal_deep_learning_representations", "keywords": [], "conflicts": [], "authors": ["Hinrich Schuetze", "Christian Scheible"], "authorids": ["hinrichwork@googlemail.com", "christian.scheible@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362188640000, "tcdate": 1362188640000, "number": 2, "id": "3wTuUWS9F_w4i", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "YBi6KFA7PfKo5", "replyto": "YBi6KFA7PfKo5", "signatures": ["anonymous reviewer 4c9d"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Two SVDs produce more focal deep learning representations", "review": "This paper introduces a novel method to induce word vector representations from a corpus of unlabeled text. The method relies upon 'stacking' singular value decomposition with an intermediate normalization nonlinearity. The authors propose 'focality' as a metric for quantifying the quality of a learned representation. Finally, control experiments on a small collection of sentence text demonstrates stacked SVD as producing more focal representations than a single SVD.\r\n\r\nThe method of stacked SVD is novel as far as I know, but could perhaps be generalized to use other nonlinearities between the two SVD layers than length normalization alone. As the authors acknowledge, SVD is a linear transform so the intermediate nonlinearity is important as to have the entire method not reduce to a single linear transform. There are a huge number of ways to use matrix factorization to induce word vectors, Turney & Pantel (JAIR 2010) give a nice review. I would like to better understand the proposed method in the context of the many alternatives to SVD factorization (e.g. kernel PCA etc.). \r\n\r\nThe introduced notion of focality might serve as a good metric for analysis of learned representation quality. It seems however that measuring focality is only possible with brute force experiments which could make it an unwieldy tool. Expanding on focality as a tool for representation evaluation, both in theory and practice, could strengthen this paper significantly. \r\n\r\nThe experiments use a small text corpus to demonstrate two SVDs as producing better representations than one. There is much room for improvement in the experiment section. In particular, there are several word representation benchmarks the authors could use to assess the quality of the proposed method relative to previous work:\r\n- Turian et al (ACL 2010) compare several word representations and release benchmark code. \r\n- Socher et al (EMNLP 2011) release a multi-dimensional sentiment analysis corpus and use neural nets to train word representations\r\n- Maas et al (ACL 2011) release a large semi-supervised sentiment analysis corpus and directly compare SVD-obtained word representations with other models\r\n\r\nThe experiments given are a reasonable sanity check for the model and demonstration of the introduced focality metric. However, the paper would be greatly improved by comparing to previous work on at least one of the tasks in papers listed above. \r\n\r\nThe 1LAYER vs 2LAYER experiment is not clearly explained. Please expand on the difference in 1 vs 2 layers and the experimental result.\r\n\r\nTo summarize:\r\n- Novel layer-wise SVD approach to inducing word vectors. Needs to be better explained in the context of matrix factorization alternatives\r\n- Novel 'focality' metric which could serve as a tool for measuring learned representation quality. Metric needs more explanation / analysis.\r\n- Experiments don't demonstrate the model relative to previous work. This is a major omission since many recent alternatives exist and comparison experiments should be straightforward with several public datasets exist\r\n- Overall paper is fairly clear but could use some work"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Two SVDs produce more focal deep learning representations", "decision": "conferencePoster-iclr2013-workshop", "abstract": "A key characteristic of work on deep learning and neural networks in general is that it relies on representations of the input that support generalization, robust inference, domain adaptation and other desirable functionalities. Much recent progress in the field has focused on efficient and effective methods for computing representations. In this paper, we propose an alternative method that is more efficient than prior work and produces representations that have a property we call focality -- a property we hypothesize to be important for neural network representations. The method consists of a simple application of two consecutive SVDs and is inspired by Anandkumar (2012).", "pdf": "https://arxiv.org/abs/1301.3627", "paperhash": "schuetze|two_svds_produce_more_focal_deep_learning_representations", "keywords": [], "conflicts": [], "authors": ["Hinrich Schuetze", "Christian Scheible"], "authorids": ["hinrichwork@googlemail.com", "christian.scheible@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1361986620000, "tcdate": 1361986620000, "number": 1, "id": "VFwT2CLWfA2kU", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "YBi6KFA7PfKo5", "replyto": "YBi6KFA7PfKo5", "signatures": ["anonymous reviewer 2448"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Two SVDs produce more focal deep learning representations", "review": "This paper proposes to use two consecutive SVDs to produce a\r\ncontinuous representation. This paper also introduces a property\r\ncalled focality. They claim that this property may be important for\r\nneural network: many classifiers cannot efficiently handle\r\nconjunctions of several features unless they are explicitly given as\r\nadditional features; therefore a more focal representation of the\r\ninputs can be a promising way to tackle this issue. This paper \r\nopens a very important discussion thread and provides some interesting\r\nstarting points. \r\n\r\nThere are two contributions in this paper. First, the authors define\r\nand motivate the property of focality for the representation of the\r\ninput. While the motivation is clear, its implementation is not\r\nobvious. For instance, the description provided in the subsection\r\n'Discriminative task' is hard to understand: what is really measured\r\nhow it is related to the focality property. This part of the paper\r\ncould be rephrased to be more explicit. The second contribution is the\r\nrepresentation derived by two consecutive SVDs. I would suggest to\r\nprovide a bit more of discussion about the related work like LSA, PCA,\r\nor the denoising auto-encoder.\r\n\r\nIn the third paragraph of section 'Discussion', the authors may cite\r\nthe work of (Collobert and Weston) and (Socher) for instance."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Two SVDs produce more focal deep learning representations", "decision": "conferencePoster-iclr2013-workshop", "abstract": "A key characteristic of work on deep learning and neural networks in general is that it relies on representations of the input that support generalization, robust inference, domain adaptation and other desirable functionalities. Much recent progress in the field has focused on efficient and effective methods for computing representations. In this paper, we propose an alternative method that is more efficient than prior work and produces representations that have a property we call focality -- a property we hypothesize to be important for neural network representations. The method consists of a simple application of two consecutive SVDs and is inspired by Anandkumar (2012).", "pdf": "https://arxiv.org/abs/1301.3627", "paperhash": "schuetze|two_svds_produce_more_focal_deep_learning_representations", "keywords": [], "conflicts": [], "authors": ["Hinrich Schuetze", "Christian Scheible"], "authorids": ["hinrichwork@googlemail.com", "christian.scheible@gmail.com"]}, "tags": [], "invitation": {}}}, {"replyto": null, "ddate": null, "legacy_migration": true, "tmdate": 1358457300000, "tcdate": 1358457300000, "number": 42, "id": "YBi6KFA7PfKo5", "invitation": "ICLR.cc/2013/conference/-/submission", "forum": "YBi6KFA7PfKo5", "signatures": ["hinrichwork@googlemail.com"], "readers": ["everyone"], "content": {"title": "Two SVDs produce more focal deep learning representations", "decision": "conferencePoster-iclr2013-workshop", "abstract": "A key characteristic of work on deep learning and neural networks in general is that it relies on representations of the input that support generalization, robust inference, domain adaptation and other desirable functionalities. Much recent progress in the field has focused on efficient and effective methods for computing representations. In this paper, we propose an alternative method that is more efficient than prior work and produces representations that have a property we call focality -- a property we hypothesize to be important for neural network representations. The method consists of a simple application of two consecutive SVDs and is inspired by Anandkumar (2012).", "pdf": "https://arxiv.org/abs/1301.3627", "paperhash": "schuetze|two_svds_produce_more_focal_deep_learning_representations", "keywords": [], "conflicts": [], "authors": ["Hinrich Schuetze", "Christian Scheible"], "authorids": ["hinrichwork@googlemail.com", "christian.scheible@gmail.com"]}, "writers": [], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1369422751717, "tmdate": 1496673673639, "cdate": 1496673673639, "tcdate": 1496673673639, "id": "ICLR.cc/2013/conference/-/submission", "writers": ["ICLR.cc/2013"], "signatures": ["OpenReview.net"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": []}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1377198751717}}}], "count": 5}