{"notes": [{"tddate": null, "ddate": null, "tmdate": 1518730187322, "tcdate": 1509040176646, "number": 153, "cdate": 1518730187312, "id": "B1tExikAW", "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "forum": "B1tExikAW", "original": "BJdEei1C-", "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference"], "content": {"title": "LatentPoison -- Adversarial Attacks On The Latent Space", "abstract": "Robustness and security of machine learning (ML) systems are intertwined, wherein a non-robust ML system (classifiers, regressors, etc.) can be subject to attacks using a wide variety of exploits. With the advent of scalable deep learning methodologies, a lot of emphasis has been put on the robustness of supervised, unsupervised and reinforcement learning algorithms. Here, we study the robustness of the latent space of a deep variational autoencoder (dVAE), an unsupervised generative framework, to show that it is indeed possible to perturb the latent space, flip the class predictions and keep the classification probability approximately equal before and after an attack. This means that an agent that looks at the outputs of a decoder would remain oblivious to an attack.", "pdf": "/pdf/78593296e77084bcc2bfd794c91ee48c76bd59e3.pdf", "TL;DR": "Adversarial attacks on the latent space of variational autoencoders to change the semantic meaning of inputs", "paperhash": "creswell|latentpoison_adversarial_attacks_on_the_latent_space", "_bibtex": "@misc{\ncreswell2018latentpoison,\ntitle={LatentPoison -- Adversarial Attacks On The Latent Space},\nauthor={Antonia Creswell and Biswa Sengupta and Anil A. Bharath},\nyear={2018},\nurl={https://openreview.net/forum?id=B1tExikAW},\n}", "keywords": ["adversarial attacks", "security", "auto-encoder"], "authors": ["Antonia Creswell", "Biswa Sengupta", "Anil A. Bharath"], "authorids": ["ac2211@ic.ac.uk", "b.sengupta@imperial.ac.uk", "a.bharath@imperial.ac.uk"]}, "nonreaders": [], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1506717071958, "id": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Conference"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference"]}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"authors": {"required": false, "order": 1, "values-regex": ".*", "description": "Comma separated list of author names, as they appear in the paper."}, "authorids": {"required": false, "order": 2, "values-regex": ".*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "cdate": 1506717071958}}, "tauthor": "ICLR.cc/2018/Conference"}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1517260078863, "tcdate": 1517250104364, "number": 769, "cdate": 1517250104347, "id": "H1xS8kpHM", "invitation": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "forum": "B1tExikAW", "replyto": "B1tExikAW", "signatures": ["ICLR.cc/2018/Conference/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Program_Chairs"], "content": {"decision": "Reject", "title": "ICLR 2018 Conference Acceptance Decision", "comment": "The paper proposes to launch adversarial attacks in the latent space of VAE such that the minimal change in the latent representation leads to the decoder producing an image with class predictions altered.  Given the pros/cons the paper in its current form falls short of acceptance.\n\nPros:\nReviewers agree that the paper is well written and easy to follow\n\nCons:\n- The paper lacks novelty and uses standard attacks and defense methodology.\n- Reviewers find the attack scenario presented is unrealistic and hence may not useful.\n- Experiments lack rigorous comparisons with baselines and it is not clear if the attack in the latent space will be stronger than the attack in the input space. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "LatentPoison -- Adversarial Attacks On The Latent Space", "abstract": "Robustness and security of machine learning (ML) systems are intertwined, wherein a non-robust ML system (classifiers, regressors, etc.) can be subject to attacks using a wide variety of exploits. With the advent of scalable deep learning methodologies, a lot of emphasis has been put on the robustness of supervised, unsupervised and reinforcement learning algorithms. Here, we study the robustness of the latent space of a deep variational autoencoder (dVAE), an unsupervised generative framework, to show that it is indeed possible to perturb the latent space, flip the class predictions and keep the classification probability approximately equal before and after an attack. This means that an agent that looks at the outputs of a decoder would remain oblivious to an attack.", "pdf": "/pdf/78593296e77084bcc2bfd794c91ee48c76bd59e3.pdf", "TL;DR": "Adversarial attacks on the latent space of variational autoencoders to change the semantic meaning of inputs", "paperhash": "creswell|latentpoison_adversarial_attacks_on_the_latent_space", "_bibtex": "@misc{\ncreswell2018latentpoison,\ntitle={LatentPoison -- Adversarial Attacks On The Latent Space},\nauthor={Antonia Creswell and Biswa Sengupta and Anil A. Bharath},\nyear={2018},\nurl={https://openreview.net/forum?id=B1tExikAW},\n}", "keywords": ["adversarial attacks", "security", "auto-encoder"], "authors": ["Antonia Creswell", "Biswa Sengupta", "Anil A. Bharath"], "authorids": ["ac2211@ic.ac.uk", "b.sengupta@imperial.ac.uk", "a.bharath@imperial.ac.uk"]}, "tags": [], "invitation": {"id": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "rdate": null, "ddate": null, "expdate": 1541175629000, "duedate": null, "tmdate": 1541177635767, "tddate": null, "super": null, "final": null, "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": {"values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Conference/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Conference Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": [], "noninvitees": [], "writers": ["ICLR.cc/2018/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1541177635767}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642399710, "tcdate": 1511771902361, "number": 1, "cdate": 1511771902361, "id": "H1I-1LYxf", "invitation": "ICLR.cc/2018/Conference/-/Paper153/Official_Review", "forum": "B1tExikAW", "replyto": "B1tExikAW", "signatures": ["ICLR.cc/2018/Conference/Paper153/AnonReviewer2"], "readers": ["everyone"], "content": {"title": "LatentPoison -- Adversarial Attacks On The Latent Space", "rating": "5: Marginally below acceptance threshold", "review": "The idea is clearly stated (but lacks some details) and I enjoyed reading the paper. \n\nI understand the difference between [Kos+17] and the proposed scheme but I could not understand in which situation the proposed scheme works better. From the adversary's standpoint, it would be easier to manipulate inputs than latent variables. On the other hand, I agree that sample-independent perturbation is much more practical than sample-dependent perturbation.\n\nIn Section 3.1, the attack methods #2 and #3 should be detailed more. I could not imagine how VAE and T are trained simultaneously.\n\nIn Section 3.2, the authors listed a couple of loss functions. How were these loss functions are combined? The final optimization problem that is used for training of the propose VAE should be formally defined. Also, the detailed specification of the VAE should be detailed.\n\nFrom figures in Figure 4 and Figure 5, I could see that the proposed scheme performs successfully in a qualitative manner, however, it is difficult to evaluate the proposed scheme qualitatively without comparisons with baselines. For example, can the proposed scheme can be compared with [Kos+17] or some other sample-dependent attacks? Also, can you experimentally show that attacks on latent variables are more powerful than attacks on inputs?\n\n\n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "LatentPoison -- Adversarial Attacks On The Latent Space", "abstract": "Robustness and security of machine learning (ML) systems are intertwined, wherein a non-robust ML system (classifiers, regressors, etc.) can be subject to attacks using a wide variety of exploits. With the advent of scalable deep learning methodologies, a lot of emphasis has been put on the robustness of supervised, unsupervised and reinforcement learning algorithms. Here, we study the robustness of the latent space of a deep variational autoencoder (dVAE), an unsupervised generative framework, to show that it is indeed possible to perturb the latent space, flip the class predictions and keep the classification probability approximately equal before and after an attack. This means that an agent that looks at the outputs of a decoder would remain oblivious to an attack.", "pdf": "/pdf/78593296e77084bcc2bfd794c91ee48c76bd59e3.pdf", "TL;DR": "Adversarial attacks on the latent space of variational autoencoders to change the semantic meaning of inputs", "paperhash": "creswell|latentpoison_adversarial_attacks_on_the_latent_space", "_bibtex": "@misc{\ncreswell2018latentpoison,\ntitle={LatentPoison -- Adversarial Attacks On The Latent Space},\nauthor={Antonia Creswell and Biswa Sengupta and Anil A. Bharath},\nyear={2018},\nurl={https://openreview.net/forum?id=B1tExikAW},\n}", "keywords": ["adversarial attacks", "security", "auto-encoder"], "authors": ["Antonia Creswell", "Biswa Sengupta", "Anil A. Bharath"], "authorids": ["ac2211@ic.ac.uk", "b.sengupta@imperial.ac.uk", "a.bharath@imperial.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642399624, "id": "ICLR.cc/2018/Conference/-/Paper153/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper153/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper153/AnonReviewer2", "ICLR.cc/2018/Conference/Paper153/AnonReviewer1", "ICLR.cc/2018/Conference/Paper153/AnonReviewer3"], "reply": {"forum": "B1tExikAW", "replyto": "B1tExikAW", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper153/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642399624}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642399675, "tcdate": 1511798902129, "number": 2, "cdate": 1511798902129, "id": "HkCuu2YxG", "invitation": "ICLR.cc/2018/Conference/-/Paper153/Official_Review", "forum": "B1tExikAW", "replyto": "B1tExikAW", "signatures": ["ICLR.cc/2018/Conference/Paper153/AnonReviewer1"], "readers": ["everyone"], "content": {"title": "VAE are not a compression scheme", "rating": "3: Clear rejection", "review": "This paper misses the point of what VAEs (or GANs, in general) are used for. The idea of using VAEs is not to encode and decode images (or in general any input), but to recover the generating process that created those images so we have an unlimited source of samples. The use of these techniques for compressing is still unclear and their quality today is too low. So the attack that the authors are proposing does not make sense and my take is that we should see significant changes before they can make sense. \n\nBut let\u2019s assume that at some point they can be used as the authors propose. In which one person encodes an image, send the latent variable to a friend, but a foe intercepts it on the way and tampers with it so the receiver recovers the wrong image without knowing. Now if the sender believes the sample can be tampered with, if the sender codes z with his private key would not make the attack useless? I think this will make the first attack useless. \n\nThe other two attacks require that the foe is inserted in the middle of the training of the VAE. This is even less doable, because the encoder and decoder are not train remotely. They are train of the same machine or cluster in a controlled manner by the person that would use the system. Once it is train it will give away the decoder and keep the encoder for sending information.\n\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "LatentPoison -- Adversarial Attacks On The Latent Space", "abstract": "Robustness and security of machine learning (ML) systems are intertwined, wherein a non-robust ML system (classifiers, regressors, etc.) can be subject to attacks using a wide variety of exploits. With the advent of scalable deep learning methodologies, a lot of emphasis has been put on the robustness of supervised, unsupervised and reinforcement learning algorithms. Here, we study the robustness of the latent space of a deep variational autoencoder (dVAE), an unsupervised generative framework, to show that it is indeed possible to perturb the latent space, flip the class predictions and keep the classification probability approximately equal before and after an attack. This means that an agent that looks at the outputs of a decoder would remain oblivious to an attack.", "pdf": "/pdf/78593296e77084bcc2bfd794c91ee48c76bd59e3.pdf", "TL;DR": "Adversarial attacks on the latent space of variational autoencoders to change the semantic meaning of inputs", "paperhash": "creswell|latentpoison_adversarial_attacks_on_the_latent_space", "_bibtex": "@misc{\ncreswell2018latentpoison,\ntitle={LatentPoison -- Adversarial Attacks On The Latent Space},\nauthor={Antonia Creswell and Biswa Sengupta and Anil A. Bharath},\nyear={2018},\nurl={https://openreview.net/forum?id=B1tExikAW},\n}", "keywords": ["adversarial attacks", "security", "auto-encoder"], "authors": ["Antonia Creswell", "Biswa Sengupta", "Anil A. Bharath"], "authorids": ["ac2211@ic.ac.uk", "b.sengupta@imperial.ac.uk", "a.bharath@imperial.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642399624, "id": "ICLR.cc/2018/Conference/-/Paper153/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper153/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper153/AnonReviewer2", "ICLR.cc/2018/Conference/Paper153/AnonReviewer1", "ICLR.cc/2018/Conference/Paper153/AnonReviewer3"], "reply": {"forum": "B1tExikAW", "replyto": "B1tExikAW", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper153/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642399624}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642399638, "tcdate": 1511813384183, "number": 3, "cdate": 1511813384183, "id": "B1xzWeqgG", "invitation": "ICLR.cc/2018/Conference/-/Paper153/Official_Review", "forum": "B1tExikAW", "replyto": "B1tExikAW", "signatures": ["ICLR.cc/2018/Conference/Paper153/AnonReviewer3"], "readers": ["everyone"], "content": {"title": "Too simplistic scenario", "rating": "4: Ok but not good enough - rejection", "review": "This paper is concerned with both security and machine learning. \nAssuming that data is encoded, transmited, and decoded using a VAE,\nthe paper proposes a man-in-middle attack that alters the VAE encoding of the input data so that the decoded output will be misclassified.\nThe objectives are to: 1) fool the autoencoder; the classification output of the autoencoder is different from the actual class of the input; 2) make minimal change in the middle so that the attack is not detectable. \n\nThis paper is concerned with both security and machine learning, but there is no clear contributions to either field. From the machine learning perspective, the proposed \"attacking\" method is standard without any technical novelty. From the security perspective, the scenarios are too simplistic. The encoding-decoding mechanism being attacked is too simple without any security enhancement. This is an unrealistic scenario. For applications with security concerns, there should have been methods to guard against man-in-the-middle attack, and the paper should have at least considered some of them. Without considering the state-of-the-art security defending mechanism, it is difficult to judge the contribution of the paper to the security community. \n\nI am not a security expert, but I doubt that the proposed method are formulated based on well founded security concepts and ideas. For example, what are the necessary and sufficient conditions for an attacking method to be undetectable? Are the criteria about the magnitude of epsilon given on Section 3.3. necessary and sufficient? Is there any reference for them? Why do we require the correspondence between the classification confidence of tranformed and original data? Would it be enough to match the DISTRIBUTION of the confidence? ", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "LatentPoison -- Adversarial Attacks On The Latent Space", "abstract": "Robustness and security of machine learning (ML) systems are intertwined, wherein a non-robust ML system (classifiers, regressors, etc.) can be subject to attacks using a wide variety of exploits. With the advent of scalable deep learning methodologies, a lot of emphasis has been put on the robustness of supervised, unsupervised and reinforcement learning algorithms. Here, we study the robustness of the latent space of a deep variational autoencoder (dVAE), an unsupervised generative framework, to show that it is indeed possible to perturb the latent space, flip the class predictions and keep the classification probability approximately equal before and after an attack. This means that an agent that looks at the outputs of a decoder would remain oblivious to an attack.", "pdf": "/pdf/78593296e77084bcc2bfd794c91ee48c76bd59e3.pdf", "TL;DR": "Adversarial attacks on the latent space of variational autoencoders to change the semantic meaning of inputs", "paperhash": "creswell|latentpoison_adversarial_attacks_on_the_latent_space", "_bibtex": "@misc{\ncreswell2018latentpoison,\ntitle={LatentPoison -- Adversarial Attacks On The Latent Space},\nauthor={Antonia Creswell and Biswa Sengupta and Anil A. Bharath},\nyear={2018},\nurl={https://openreview.net/forum?id=B1tExikAW},\n}", "keywords": ["adversarial attacks", "security", "auto-encoder"], "authors": ["Antonia Creswell", "Biswa Sengupta", "Anil A. Bharath"], "authorids": ["ac2211@ic.ac.uk", "b.sengupta@imperial.ac.uk", "a.bharath@imperial.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642399624, "id": "ICLR.cc/2018/Conference/-/Paper153/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper153/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper153/AnonReviewer2", "ICLR.cc/2018/Conference/Paper153/AnonReviewer1", "ICLR.cc/2018/Conference/Paper153/AnonReviewer3"], "reply": {"forum": "B1tExikAW", "replyto": "B1tExikAW", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper153/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642399624}}}, {"tddate": null, "ddate": null, "tmdate": 1515086713626, "tcdate": 1515086669618, "number": 3, "cdate": 1515086669618, "id": "rJUUQkn7f", "invitation": "ICLR.cc/2018/Conference/-/Paper153/Official_Comment", "forum": "B1tExikAW", "replyto": "H1I-1LYxf", "signatures": ["ICLR.cc/2018/Conference/Paper153/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper153/Authors"], "content": {"title": "Response to AnonReviewer2", "comment": "Thank you for your careful reading of our paper and for providing useful feedback. We have addressed your comments below:\n\n1.\u00a0I understand the difference between [Kos+17] and the proposed scheme but I could not understand in which situation the proposed scheme works better.\u00a0\n(a) From the adversary's standpoint, it would be easier to manipulate inputs than latent variables.\u00a0\n(b) On the other hand, I agree that sample-independent perturbation is much more practical than sample-dependent perturbation.\n\nResponse: (a) If it is possible to perturb the input to the encoder, it makes sense that it should be equally easy to perturb the input to the decoder. Possible confusion here may arise from the fact that the perturbation applied in [Kos2017] is applied to an image selected by the user therefore perturbation must still be applied in an online manner, unlike with traditional adversarial examples {CITE}, that may be synthesised offline and supplied to an algorithm.\n\n2. In Section 3.1, the attack methods #2 and #3 should be detailed more. I could not imagine how VAE and T are trained simultaneously.\n\nResponse: The way that we trained these was, in one epoch to update the parameters of the VAE and then the adversary, T. If you are suggesting that the adversary would not have access to the model during training, we point towards this quotation:\n\n\u201cAttackers may also act during the learning process, for example tampering with some of the training data, or reading intermediate states of the learning system.\u201d - Abadi et al. (On the Protection of Private Information in Machine Learning Systems, 2017).\n\n3. In Section 3.2, the authors listed a couple of loss functions. How were these loss functions are combined? The final optimization problem that is used for training of the propose VAE should be formally defined. Also, the detailed specification of the VAE should be detailed.\n\nResponse: In section 3.2 we define 3 losses: Jvae for updating the VAE, Jclass for updating the classifier for method #3 only and Jz for updating the adversarial transform model, T. The loss functions were not combined because they are each used to train a different model in the system.\u00a0\n(a) In method #1 the VAE and T are not updated at the same time therefore it does not make sense to combine Jvae and Jz in the same cost function. The VAE is first trained to minimise Jvae, then T is trained to minimise Jz. The classifier is learned separately too, and does not update the parameters of the VAE.\n(b) In method #2, for each epoch the VAE is updated using Jvae and then T is updated using Jz. The losses update different sets of parameters. The classifier is learned separately and does not update the parameters of the VAE.\n(c) In method #3, arguably we could have written a new cost function, J = Jvae + Jclass, however since this only applied to method #3 we did not include this.\n\n4. From figures in Figure 4 and Figure 5, I could see that the proposed scheme performs successfully in a qualitative manner, however, it is difficult to evaluate the proposed scheme qualitatively without comparisons with baselines. For example, can the proposed scheme can be compared with [Kos+17] or some other sample-dependent attacks? Also, can you experimentally show that attacks on latent variables are more powerful than attacks on inputs?\n\nThese experiments could certainly be carried out. However, there would not be space to add this comparison here."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "LatentPoison -- Adversarial Attacks On The Latent Space", "abstract": "Robustness and security of machine learning (ML) systems are intertwined, wherein a non-robust ML system (classifiers, regressors, etc.) can be subject to attacks using a wide variety of exploits. With the advent of scalable deep learning methodologies, a lot of emphasis has been put on the robustness of supervised, unsupervised and reinforcement learning algorithms. Here, we study the robustness of the latent space of a deep variational autoencoder (dVAE), an unsupervised generative framework, to show that it is indeed possible to perturb the latent space, flip the class predictions and keep the classification probability approximately equal before and after an attack. This means that an agent that looks at the outputs of a decoder would remain oblivious to an attack.", "pdf": "/pdf/78593296e77084bcc2bfd794c91ee48c76bd59e3.pdf", "TL;DR": "Adversarial attacks on the latent space of variational autoencoders to change the semantic meaning of inputs", "paperhash": "creswell|latentpoison_adversarial_attacks_on_the_latent_space", "_bibtex": "@misc{\ncreswell2018latentpoison,\ntitle={LatentPoison -- Adversarial Attacks On The Latent Space},\nauthor={Antonia Creswell and Biswa Sengupta and Anil A. Bharath},\nyear={2018},\nurl={https://openreview.net/forum?id=B1tExikAW},\n}", "keywords": ["adversarial attacks", "security", "auto-encoder"], "authors": ["Antonia Creswell", "Biswa Sengupta", "Anil A. Bharath"], "authorids": ["ac2211@ic.ac.uk", "b.sengupta@imperial.ac.uk", "a.bharath@imperial.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825738440, "id": "ICLR.cc/2018/Conference/-/Paper153/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "B1tExikAW", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper153/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper153/Authors|ICLR.cc/2018/Conference/Paper153/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper153/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper153/Authors|ICLR.cc/2018/Conference/Paper153/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper153/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper153/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper153/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper153/Reviewers", "ICLR.cc/2018/Conference/Paper153/Authors", "ICLR.cc/2018/Conference/Paper153/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825738440}}}, {"tddate": null, "ddate": null, "tmdate": 1515086530019, "tcdate": 1515086530019, "number": 2, "cdate": 1515086530019, "id": "Sk9azy3mf", "invitation": "ICLR.cc/2018/Conference/-/Paper153/Official_Comment", "forum": "B1tExikAW", "replyto": "HkCuu2YxG", "signatures": ["ICLR.cc/2018/Conference/Paper153/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper153/Authors"], "content": {"title": "Response to AnonReviewer1", "comment": "Thank you for your comments, we have addressed them below:\n\n1. This paper misses the point of what VAEs (or GANs, in general) are used for. The idea of using VAEs is not to encode and decode images (or in general any input), but to recover the generating process that created those images so we have an unlimited source of samples. The use of these techniques for compressing is still unclear and their quality today is too low. So the attack that the authors are proposing does not make sense and my take is that we should see significant changes before they can make sense.\u00a0\n\nResponse: While we appreciate that the specific VAE architecture is not directly used for image compression, autoencoders more generally have been used for image compression {Theis2017} and outperform industry standard compression techniques such as JPEG. We choose to look at the VAE as an example of *an* autoencoder as this appeared to be a good starting point for research in this area. Further, it was not possible (given the space and resource constraints) to apply this technique to all state of art autoencoding compression models. Instead, we chose a model that shares many properties with state-of-art autoencoding compression models.\u00a0\n\nDeep autoencoders are being developed for tasks such as compression and the purpose of our paper is to expose vulnerabilities in deep autoencoders, so that these vulnerabilities may be kept in mind when developing such algorithms.\n\n2. But let\u2019s assume that at some point they can be used as the authors propose. In which one person encodes an image, send the latent variable to a friend, but a foe intercepts it on the way and tampers with it so the receiver recovers the wrong image without knowing. Now if the sender believes the sample can be tampered with, if the sender codes z with his private key would not make the attack useless? I think this will make the first attack useless.\u00a0\n\nResponse: If the sender encodes z with his own private key, rather than with the encoder, the decoder will not be able to decode\u00a0the image at all.\u00a0\n\n3. The other two attacks require that the foe is inserted in the middle of the training of the VAE. This is even less doable, because the encoder and decoder are not train remotely. They are train of the same machine or cluster in a controlled manner by the person that would use the system. Once it is train it will give away the decoder and keep the encoder for sending information.\n\nResponse: The adversary itself does not affect the training. It simply learns along side the autoencoding model. The only assumption we make is that the adversary can access the encodings and see the outputs of the autoencoder model during training. Abadi et al. suggest that this is not an unreasonable assumption:\n\n\u201cAttackers may also act during the learning process, for example tampering with some of the training data, or reading intermediate states of the learning system.\u201d - Abadi et al. (On the Protection of Private Information in Machine Learning Systems, 2017).\n\nFurther, training on clusters often suggests training remotely and transfer of models, data and results between machines."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "LatentPoison -- Adversarial Attacks On The Latent Space", "abstract": "Robustness and security of machine learning (ML) systems are intertwined, wherein a non-robust ML system (classifiers, regressors, etc.) can be subject to attacks using a wide variety of exploits. With the advent of scalable deep learning methodologies, a lot of emphasis has been put on the robustness of supervised, unsupervised and reinforcement learning algorithms. Here, we study the robustness of the latent space of a deep variational autoencoder (dVAE), an unsupervised generative framework, to show that it is indeed possible to perturb the latent space, flip the class predictions and keep the classification probability approximately equal before and after an attack. This means that an agent that looks at the outputs of a decoder would remain oblivious to an attack.", "pdf": "/pdf/78593296e77084bcc2bfd794c91ee48c76bd59e3.pdf", "TL;DR": "Adversarial attacks on the latent space of variational autoencoders to change the semantic meaning of inputs", "paperhash": "creswell|latentpoison_adversarial_attacks_on_the_latent_space", "_bibtex": "@misc{\ncreswell2018latentpoison,\ntitle={LatentPoison -- Adversarial Attacks On The Latent Space},\nauthor={Antonia Creswell and Biswa Sengupta and Anil A. Bharath},\nyear={2018},\nurl={https://openreview.net/forum?id=B1tExikAW},\n}", "keywords": ["adversarial attacks", "security", "auto-encoder"], "authors": ["Antonia Creswell", "Biswa Sengupta", "Anil A. Bharath"], "authorids": ["ac2211@ic.ac.uk", "b.sengupta@imperial.ac.uk", "a.bharath@imperial.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825738440, "id": "ICLR.cc/2018/Conference/-/Paper153/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "B1tExikAW", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper153/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper153/Authors|ICLR.cc/2018/Conference/Paper153/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper153/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper153/Authors|ICLR.cc/2018/Conference/Paper153/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper153/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper153/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper153/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper153/Reviewers", "ICLR.cc/2018/Conference/Paper153/Authors", "ICLR.cc/2018/Conference/Paper153/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825738440}}}, {"tddate": null, "ddate": null, "tmdate": 1515086465574, "tcdate": 1515086465574, "number": 1, "cdate": 1515086465574, "id": "BJ5KMJhmf", "invitation": "ICLR.cc/2018/Conference/-/Paper153/Official_Comment", "forum": "B1tExikAW", "replyto": "B1xzWeqgG", "signatures": ["ICLR.cc/2018/Conference/Paper153/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper153/Authors"], "content": {"title": "Response to AnonReviewer3:", "comment": "Thank you for your comments and feedback. We have addressed your concerns below:\n\n1. This paper is concerned with both security and machine learning, but there is no clear contributions to either field. From the machine learning perspective, the proposed \"attacking\" method is standard without any technical novelty.\n\nAll\u00a0previous work, to our knowledge, has considered learning attacks on *image* space, not encoding space. Further, the perturbation in image space is such that a valid image can be constructed (but the class is flipped, according to a\u00a0classifier).\u00a0Please note that\u00a0the most similar relevant work of Kos (2017), the attack requires access to the encoder; ours does not. \u00a0We consider these to be worthy and significant technical contributions.\n\u00a0\n2. From the security perspective, the scenarios are too simplistic. The encoding-decoding mechanism being attacked is too simple without any security enhancement. This is an unrealistic scenario. For applications with security concerns, there should have been methods to guard against man-in-the-middle attack, and the paper should have at least considered some of them. Without considering the state-of-the-art security defending mechanism, it is difficult to judge the contribution of the paper to the security community.\u00a0\n\nResponse: The purpose of the paper is to expose vulnerabilities in deep autoencoders. A significant amount of research has been done to explore the application of autoencoders. Perhaps the most significant are the compressive autoencoders {Theis2017}, some of which out perform JPEG compression. If these systems were to be deployed, it would be appropriate to be aware of their vulnerabilities. It makes sense to simultaneously\u00a0 develop (1) attack, (2) defence and (3) autoencoder technologies so that when autoencoder technologies are deployed, we know that they can be deployed safely. Our experience with \u00a0teams using latent spaces to encode images for downstream tasks, is that\u00a0latent spaces tend to be thought of as being\u00a0incorruptible. \u00a0This is clearly not the case.\n\nNone of the other previous, widely cited papers {Goodfellow2014, Kos2017, Papernot2016} consider any other layers of security, such as encryption. We agree that, ultimately, these should be taken into account, but\u00a0best practice \u00a0in cybersecurity involves examining each layer of security *independently*.\u00a0\n\n3. I am not a security expert, but I doubt that the proposed method are formulated based on well founded security concepts and ideas. For example, what are the necessary and sufficient conditions for an attacking method to be undetectable?\u00a0\n\nResponse: Good question. \u00a0This is what we started to explore in the appendix (Section 6.4). We do not think the\u00a0answer can be given in anything other than a probabilistic sense. We implicitly think about the\u00a0scenario in which the only way to detect attack would be through examining the likelihood that\u00a0 a given unit, of an encoding, lies\u00a0outside certain confidence intervals of the encoding distribution. \u00a0For this paper, we considered a Gaussian prior on each element of latent encoding space; the logical extension to\u00a0this would be run experiments and develop the principle\u00a0behind detecting change in at least one unit.\n\n4. Are the criteria about the magnitude of epsilon given on Section 3.3. necessary and sufficient? Is there any reference for them?\n\nResponse: No, we would only be able to make decisions based on probabilistic measures.\n\n5. Why do we require the correspondence between the classification confidence of tranformed and original data?\u00a0\n\nResponse: For any particular classifier, we may know it\u2019s classification confidence under normal operating conditions. If an attack occurs, to produce a \u201ctransformed\u201d data sample, then it is possible that the confidence with which a sample is classified is different to that of \u201coriginal\u201d data samples that the model was validated on. If this difference is detectable, then we may say that the attack is detectable. If the difference is not detectable, we may not know if an attack has taken place.\n\nIndeed by simply sending the same sample multiple times and comparing classification confidence, you would be able to tell whether any samples had been contaminated.\n\nThese experiments are designed to show that the attack is not easily detectable in the image space.\n\n6. Would it be enough to match the DISTRIBUTION of the confidence?\u00a0\n\nIt is not entirely clear what you mean by \u201cmatch\u201d. I agree, that it would make sense to divide a distribution of confidence values for a system operating under normal conditions, and then it may be easier to identify if a corrupted samples is an outlier. However, the distribution is likely to be Gaussian, suggesting that to describe the distribution it is sufficient to know the mean and standard deviation. However, rather than doing this in image space, based on classification confidence, we proposed to do this in latent space. We explore these ideas in the Appendix section 6.4."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "LatentPoison -- Adversarial Attacks On The Latent Space", "abstract": "Robustness and security of machine learning (ML) systems are intertwined, wherein a non-robust ML system (classifiers, regressors, etc.) can be subject to attacks using a wide variety of exploits. With the advent of scalable deep learning methodologies, a lot of emphasis has been put on the robustness of supervised, unsupervised and reinforcement learning algorithms. Here, we study the robustness of the latent space of a deep variational autoencoder (dVAE), an unsupervised generative framework, to show that it is indeed possible to perturb the latent space, flip the class predictions and keep the classification probability approximately equal before and after an attack. This means that an agent that looks at the outputs of a decoder would remain oblivious to an attack.", "pdf": "/pdf/78593296e77084bcc2bfd794c91ee48c76bd59e3.pdf", "TL;DR": "Adversarial attacks on the latent space of variational autoencoders to change the semantic meaning of inputs", "paperhash": "creswell|latentpoison_adversarial_attacks_on_the_latent_space", "_bibtex": "@misc{\ncreswell2018latentpoison,\ntitle={LatentPoison -- Adversarial Attacks On The Latent Space},\nauthor={Antonia Creswell and Biswa Sengupta and Anil A. Bharath},\nyear={2018},\nurl={https://openreview.net/forum?id=B1tExikAW},\n}", "keywords": ["adversarial attacks", "security", "auto-encoder"], "authors": ["Antonia Creswell", "Biswa Sengupta", "Anil A. Bharath"], "authorids": ["ac2211@ic.ac.uk", "b.sengupta@imperial.ac.uk", "a.bharath@imperial.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825738440, "id": "ICLR.cc/2018/Conference/-/Paper153/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "B1tExikAW", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper153/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper153/Authors|ICLR.cc/2018/Conference/Paper153/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper153/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper153/Authors|ICLR.cc/2018/Conference/Paper153/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper153/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper153/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper153/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper153/Reviewers", "ICLR.cc/2018/Conference/Paper153/Authors", "ICLR.cc/2018/Conference/Paper153/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825738440}}}], "count": 8}