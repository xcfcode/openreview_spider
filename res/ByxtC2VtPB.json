{"notes": [{"id": "ByxtC2VtPB", "original": "Skej9ogHvS", "number": 272, "cdate": 1569438928933, "ddate": null, "tcdate": 1569438928933, "tmdate": 1583912030282, "tddate": null, "forum": "ByxtC2VtPB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Mixup Inference: Better Exploiting Mixup to Defend Adversarial Attacks", "authors": ["Tianyu Pang*", "Kun Xu*", "Jun Zhu"], "authorids": ["pty17@mails.tsinghua.edu.cn", "kunxu.thu@gmail.com", "dcszj@mail.tsinghua.edu.cn"], "keywords": ["Trustworthy Machine Learning", "Adversarial Robustness", "Inference Principle", "Mixup"], "TL;DR": "We exploit the global linearity of the mixup-trained models in inference to break the locality of the adversarial perturbations.", "abstract": "It has been widely recognized that adversarial examples can be easily crafted to fool deep networks, which mainly root from the locally non-linear behavior nearby input examples. Applying mixup in training provides an effective mechanism to improve generalization performance and model robustness against adversarial perturbations, which introduces the globally linear behavior in-between training examples. However, in previous work, the mixup-trained models only passively defend adversarial attacks in inference by directly classifying the inputs, where the induced global linearity is not well exploited. Namely, since the locality of the adversarial perturbations, it would be more efficient to actively break the locality via the globality of the model predictions. Inspired by simple geometric intuition, we develop an inference principle, named mixup inference (MI), for mixup-trained models. MI mixups the input with other random clean samples, which can shrink and transfer the equivalent perturbation if the input is adversarial. Our experiments on CIFAR-10 and CIFAR-100 demonstrate that MI can further improve the adversarial robustness for the models trained by mixup and its variants.", "pdf": "/pdf/2d15cb14ab9e0914711fc1187b082beb9a98f928.pdf", "code": "https://github.com/P2333/Mixup-Inference", "paperhash": "pang|mixup_inference_better_exploiting_mixup_to_defend_adversarial_attacks", "_bibtex": "@inproceedings{\nPang*2020Mixup,\ntitle={Mixup Inference: Better Exploiting Mixup to Defend Adversarial Attacks},\nauthor={Tianyu Pang* and Kun Xu* and Jun Zhu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=ByxtC2VtPB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/248f049c55dcf577f775b97d48d421c49e577e79.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 15, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "F9Xx1ImJqe", "original": null, "number": 1, "cdate": 1576798691989, "ddate": null, "tcdate": 1576798691989, "tmdate": 1576800943335, "tddate": null, "forum": "ByxtC2VtPB", "replyto": "ByxtC2VtPB", "invitation": "ICLR.cc/2020/Conference/Paper272/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "This paper proposed a mixup inference (MI) method, for  mixup-trained models, to better defend adversarial attacks.  The idea is novel and is proved to be effective on CIFAR-10 and CIFAR-100.  All reviewers and the AC agree to accept the paper.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Mixup Inference: Better Exploiting Mixup to Defend Adversarial Attacks", "authors": ["Tianyu Pang*", "Kun Xu*", "Jun Zhu"], "authorids": ["pty17@mails.tsinghua.edu.cn", "kunxu.thu@gmail.com", "dcszj@mail.tsinghua.edu.cn"], "keywords": ["Trustworthy Machine Learning", "Adversarial Robustness", "Inference Principle", "Mixup"], "TL;DR": "We exploit the global linearity of the mixup-trained models in inference to break the locality of the adversarial perturbations.", "abstract": "It has been widely recognized that adversarial examples can be easily crafted to fool deep networks, which mainly root from the locally non-linear behavior nearby input examples. Applying mixup in training provides an effective mechanism to improve generalization performance and model robustness against adversarial perturbations, which introduces the globally linear behavior in-between training examples. However, in previous work, the mixup-trained models only passively defend adversarial attacks in inference by directly classifying the inputs, where the induced global linearity is not well exploited. Namely, since the locality of the adversarial perturbations, it would be more efficient to actively break the locality via the globality of the model predictions. Inspired by simple geometric intuition, we develop an inference principle, named mixup inference (MI), for mixup-trained models. MI mixups the input with other random clean samples, which can shrink and transfer the equivalent perturbation if the input is adversarial. Our experiments on CIFAR-10 and CIFAR-100 demonstrate that MI can further improve the adversarial robustness for the models trained by mixup and its variants.", "pdf": "/pdf/2d15cb14ab9e0914711fc1187b082beb9a98f928.pdf", "code": "https://github.com/P2333/Mixup-Inference", "paperhash": "pang|mixup_inference_better_exploiting_mixup_to_defend_adversarial_attacks", "_bibtex": "@inproceedings{\nPang*2020Mixup,\ntitle={Mixup Inference: Better Exploiting Mixup to Defend Adversarial Attacks},\nauthor={Tianyu Pang* and Kun Xu* and Jun Zhu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=ByxtC2VtPB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/248f049c55dcf577f775b97d48d421c49e577e79.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "ByxtC2VtPB", "replyto": "ByxtC2VtPB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795724231, "tmdate": 1576800275842, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper272/-/Decision"}}}, {"id": "Hye3IBo9oH", "original": null, "number": 9, "cdate": 1573725524422, "ddate": null, "tcdate": 1573725524422, "tmdate": 1573725524422, "tddate": null, "forum": "ByxtC2VtPB", "replyto": "Bkloiyi5iS", "invitation": "ICLR.cc/2020/Conference/Paper272/-/Official_Comment", "content": {"title": "Thank you!", "comment": "Thank you again for your kind suggestions, which really help a lot to improve the original version of the paper. We deeply appreciate it!"}, "signatures": ["ICLR.cc/2020/Conference/Paper272/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper272/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Mixup Inference: Better Exploiting Mixup to Defend Adversarial Attacks", "authors": ["Tianyu Pang*", "Kun Xu*", "Jun Zhu"], "authorids": ["pty17@mails.tsinghua.edu.cn", "kunxu.thu@gmail.com", "dcszj@mail.tsinghua.edu.cn"], "keywords": ["Trustworthy Machine Learning", "Adversarial Robustness", "Inference Principle", "Mixup"], "TL;DR": "We exploit the global linearity of the mixup-trained models in inference to break the locality of the adversarial perturbations.", "abstract": "It has been widely recognized that adversarial examples can be easily crafted to fool deep networks, which mainly root from the locally non-linear behavior nearby input examples. Applying mixup in training provides an effective mechanism to improve generalization performance and model robustness against adversarial perturbations, which introduces the globally linear behavior in-between training examples. However, in previous work, the mixup-trained models only passively defend adversarial attacks in inference by directly classifying the inputs, where the induced global linearity is not well exploited. Namely, since the locality of the adversarial perturbations, it would be more efficient to actively break the locality via the globality of the model predictions. Inspired by simple geometric intuition, we develop an inference principle, named mixup inference (MI), for mixup-trained models. MI mixups the input with other random clean samples, which can shrink and transfer the equivalent perturbation if the input is adversarial. Our experiments on CIFAR-10 and CIFAR-100 demonstrate that MI can further improve the adversarial robustness for the models trained by mixup and its variants.", "pdf": "/pdf/2d15cb14ab9e0914711fc1187b082beb9a98f928.pdf", "code": "https://github.com/P2333/Mixup-Inference", "paperhash": "pang|mixup_inference_better_exploiting_mixup_to_defend_adversarial_attacks", "_bibtex": "@inproceedings{\nPang*2020Mixup,\ntitle={Mixup Inference: Better Exploiting Mixup to Defend Adversarial Attacks},\nauthor={Tianyu Pang* and Kun Xu* and Jun Zhu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=ByxtC2VtPB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/248f049c55dcf577f775b97d48d421c49e577e79.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "ByxtC2VtPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper272/Authors", "ICLR.cc/2020/Conference/Paper272/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper272/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper272/Reviewers", "ICLR.cc/2020/Conference/Paper272/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper272/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper272/Authors|ICLR.cc/2020/Conference/Paper272/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504173857, "tmdate": 1576860548559, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper272/Authors", "ICLR.cc/2020/Conference/Paper272/Reviewers", "ICLR.cc/2020/Conference/Paper272/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper272/-/Official_Comment"}}}, {"id": "Hylb_3L5uS", "original": null, "number": 1, "cdate": 1570561129243, "ddate": null, "tcdate": 1570561129243, "tmdate": 1573724093434, "tddate": null, "forum": "ByxtC2VtPB", "replyto": "ByxtC2VtPB", "invitation": "ICLR.cc/2020/Conference/Paper272/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #1", "review": "Notes: \n\n-Paper claims that adversarial examples stem from locally non-linear behavior.  However, wasn't this the exact opposite of the conclusion of \"Explaining and Harnessing Adversarial Examples\" (Goodfellow 2015)?  It is cited in this paper but I think the proper conclusion is the opposite of what is written here.  \n\n-Linearity however may still be an important component since it simplifies the problem of adversarial robustness.  \n\n-Many techniques try to introduce adversarial robustness through transformations during inference time.  \n\n-Paper claims that adversarial training induces locally linear behavior - but I'm rather skeptical of this claim.  Think about something like KNN with k=1 and euclidean distance.  This should be L2-robust on the training set, yet very non-linear.  \n\n-I understand the contents of Figure 1, but I'm not sure exactly what conclusion I should draw from it.  \n\n-The novel procedure presented is \"Mixup Inference\".  This involves classifying using interpolations of test inputs.  \n\n-Two mechanisms are proposed for why this could help.  One is that the magnitude of the perturbation will shrink after doing mixup (although the signal in the original image will also shrink, so I'm not sure if I like this argument).  The second argument is that the adversarial perturbation will have to appear with different random examples which will force the attack to be more \"universal\" to succeed.  This second argument I find much more compelling.  \n\n-The notation $y_s \\sim p_s(y_s)$ is rather abusive since the random variable and the same have the same name but this is common in machine learning.  \n\n-The technique if I understand correctly (Algorithm 1) amounts to using an average of the prediction at the original point along with an average of the mixes going to all other points in the dataset.  \n\n-The paper is a bit slow to explain what distribution lambda will come from - but it effects the algorithm a lot (especially if the distribution is symmetric or asymmetric).  \n\nComments: \n\n-There is another paper (Shimada 2019) that also uses interpolation at test time and should be cited here, although I admit that paper is written in a confusing way so the connection may not be immediately obvious: https://arxiv.org/pdf/1906.08412v1.pdf\n\n-I have a suggestion for the organization of the paper that I think would improve it.  I would suggest to first introduce the method in a clear fashion (after motivating it), along with the equations.  Then, *after that*, clearly and separately introduce the analysis of the \"optimal linear model\": \"a well mixup-trained model F can be denoted as a linear function H on the convex combinations of clean examples\".  I think that would make the paper much clearer.  \n\n-For example, how can we actually know what G_k() is unless we know the adversarial perturbation (which shouldn't generally be possible during inference)?  I found this discussion to be rather confusing (basically section 3.1) although admittedly it might be my own fault.  \n\n-Why does mixup-inference hurt the clean accuracy by 10% (table 2)?  This seems like quite a lot to me.  Still the degree of robustness does seem impressive.  And I also believe that the obtained robustness of this technique along with AT is state of the art.  \n\n-It might be nice to see examples of attacks on the resulting model, especially the one with the best robustness.  It's possible that Linf-bounded attacks against *this model* will be perceptible, which would support lowering the epsilon-attack-budget (which is actually a good thing for the research field as it's evidence that maybe this epsilon shouldn't be considered practically imperceptible).  \n\nReview: \n\nThis paper was interesting, because it has nice experimental results and seems like a good idea, but I feel like the paper needs to be improved.  The biggest issue is that the paper repeatedly claims that adversarial robustness can be improved by making networks more linear, yet I believe that this is the opposite of what prior work has found.  I also found the exposition of the idea to be confusing as it simultaneously introduces the idea and an analysis of the technique - I would much prefer if the technique were introduced first and the analysis under some optimal assumptions moved to a different section.  ", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper272/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper272/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Mixup Inference: Better Exploiting Mixup to Defend Adversarial Attacks", "authors": ["Tianyu Pang*", "Kun Xu*", "Jun Zhu"], "authorids": ["pty17@mails.tsinghua.edu.cn", "kunxu.thu@gmail.com", "dcszj@mail.tsinghua.edu.cn"], "keywords": ["Trustworthy Machine Learning", "Adversarial Robustness", "Inference Principle", "Mixup"], "TL;DR": "We exploit the global linearity of the mixup-trained models in inference to break the locality of the adversarial perturbations.", "abstract": "It has been widely recognized that adversarial examples can be easily crafted to fool deep networks, which mainly root from the locally non-linear behavior nearby input examples. Applying mixup in training provides an effective mechanism to improve generalization performance and model robustness against adversarial perturbations, which introduces the globally linear behavior in-between training examples. However, in previous work, the mixup-trained models only passively defend adversarial attacks in inference by directly classifying the inputs, where the induced global linearity is not well exploited. Namely, since the locality of the adversarial perturbations, it would be more efficient to actively break the locality via the globality of the model predictions. Inspired by simple geometric intuition, we develop an inference principle, named mixup inference (MI), for mixup-trained models. MI mixups the input with other random clean samples, which can shrink and transfer the equivalent perturbation if the input is adversarial. Our experiments on CIFAR-10 and CIFAR-100 demonstrate that MI can further improve the adversarial robustness for the models trained by mixup and its variants.", "pdf": "/pdf/2d15cb14ab9e0914711fc1187b082beb9a98f928.pdf", "code": "https://github.com/P2333/Mixup-Inference", "paperhash": "pang|mixup_inference_better_exploiting_mixup_to_defend_adversarial_attacks", "_bibtex": "@inproceedings{\nPang*2020Mixup,\ntitle={Mixup Inference: Better Exploiting Mixup to Defend Adversarial Attacks},\nauthor={Tianyu Pang* and Kun Xu* and Jun Zhu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=ByxtC2VtPB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/248f049c55dcf577f775b97d48d421c49e577e79.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "ByxtC2VtPB", "replyto": "ByxtC2VtPB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper272/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper272/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574722376000, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper272/Reviewers"], "noninvitees": [], "tcdate": 1570237754543, "tmdate": 1574723079015, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper272/-/Official_Review"}}}, {"id": "Bkloiyi5iS", "original": null, "number": 8, "cdate": 1573724066699, "ddate": null, "tcdate": 1573724066699, "tmdate": 1573724066699, "tddate": null, "forum": "ByxtC2VtPB", "replyto": "B1xw_i0lor", "invitation": "ICLR.cc/2020/Conference/Paper272/-/Official_Comment", "content": {"title": "Response to Rebuttal", "comment": "\"As to the relationship between linearity and robustness, actually there is not a final conclusion. Goodfellow et al. (2015) claimed that the vulnerability of neural networks comes from their linear nature mainly because they found that FGSM can successfully attack some networks, and FGSM is based on the linear assumption of classifiers. However, there are many cases where FGSM fails but iterative attacks like PGD can still evade the models, which means adversarial examples do not necessarily stem from the linear nature. Besides, recent work [*1] finds that linearity can improve robustness, which contradicts the conclusion in Goodfellow et al. (2015). So as you suggested, in the revision, we do not claim the relationship between model linearity and the existence of adversarial examples.\"\n\nSo I think that the older informal claim is that non-linearity is the *cause* of adversarial examples.  Goodfellow 2015 argued against this by showing that linear models also can have adversarial examples (but of course they can also be robust).  So I don't think that \"only linear models can have adversarial examples\" is the right conclusion to draw from Goodfellow 2015.  \n\nI'm still okay on this point as long as the paper doesn't claim anything misleading in the introduction, as I don't see it as essential to the paper.  As such I'm happy with the introduction in its current form.  \n\nPoint 2 makes sense to me and I really like the re-organization done for point 3.   "}, "signatures": ["ICLR.cc/2020/Conference/Paper272/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper272/AnonReviewer1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Mixup Inference: Better Exploiting Mixup to Defend Adversarial Attacks", "authors": ["Tianyu Pang*", "Kun Xu*", "Jun Zhu"], "authorids": ["pty17@mails.tsinghua.edu.cn", "kunxu.thu@gmail.com", "dcszj@mail.tsinghua.edu.cn"], "keywords": ["Trustworthy Machine Learning", "Adversarial Robustness", "Inference Principle", "Mixup"], "TL;DR": "We exploit the global linearity of the mixup-trained models in inference to break the locality of the adversarial perturbations.", "abstract": "It has been widely recognized that adversarial examples can be easily crafted to fool deep networks, which mainly root from the locally non-linear behavior nearby input examples. Applying mixup in training provides an effective mechanism to improve generalization performance and model robustness against adversarial perturbations, which introduces the globally linear behavior in-between training examples. However, in previous work, the mixup-trained models only passively defend adversarial attacks in inference by directly classifying the inputs, where the induced global linearity is not well exploited. Namely, since the locality of the adversarial perturbations, it would be more efficient to actively break the locality via the globality of the model predictions. Inspired by simple geometric intuition, we develop an inference principle, named mixup inference (MI), for mixup-trained models. MI mixups the input with other random clean samples, which can shrink and transfer the equivalent perturbation if the input is adversarial. Our experiments on CIFAR-10 and CIFAR-100 demonstrate that MI can further improve the adversarial robustness for the models trained by mixup and its variants.", "pdf": "/pdf/2d15cb14ab9e0914711fc1187b082beb9a98f928.pdf", "code": "https://github.com/P2333/Mixup-Inference", "paperhash": "pang|mixup_inference_better_exploiting_mixup_to_defend_adversarial_attacks", "_bibtex": "@inproceedings{\nPang*2020Mixup,\ntitle={Mixup Inference: Better Exploiting Mixup to Defend Adversarial Attacks},\nauthor={Tianyu Pang* and Kun Xu* and Jun Zhu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=ByxtC2VtPB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/248f049c55dcf577f775b97d48d421c49e577e79.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "ByxtC2VtPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper272/Authors", "ICLR.cc/2020/Conference/Paper272/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper272/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper272/Reviewers", "ICLR.cc/2020/Conference/Paper272/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper272/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper272/Authors|ICLR.cc/2020/Conference/Paper272/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504173857, "tmdate": 1576860548559, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper272/Authors", "ICLR.cc/2020/Conference/Paper272/Reviewers", "ICLR.cc/2020/Conference/Paper272/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper272/-/Official_Comment"}}}, {"id": "HklUuXwqor", "original": null, "number": 7, "cdate": 1573708654249, "ddate": null, "tcdate": 1573708654249, "tmdate": 1573708654249, "tddate": null, "forum": "ByxtC2VtPB", "replyto": "ByxtC2VtPB", "invitation": "ICLR.cc/2020/Conference/Paper272/-/Official_Comment", "content": {"title": "Looking forward to further feedbacks", "comment": "Dear Reviewers,\n\nThank you again for your valuable comments and suggestions, which are really helpful for us. We have uploaded new revisions and posted responses to the proposed concerns and questions.\n\nWe totally understand that this is a quite busy period of time, since the reviewers may be preparing the rebuttal for their own submissions or rushing for the deadline of the recent conferences.\n\nSo we deeply appreciate it if the reviewers can take some time to return further feedbacks on whether our responses and extra experiment results solve their concerns. If there is any other question, we will try our best to provide satisfactory answers. \n\nBest,\nThe authors"}, "signatures": ["ICLR.cc/2020/Conference/Paper272/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper272/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Mixup Inference: Better Exploiting Mixup to Defend Adversarial Attacks", "authors": ["Tianyu Pang*", "Kun Xu*", "Jun Zhu"], "authorids": ["pty17@mails.tsinghua.edu.cn", "kunxu.thu@gmail.com", "dcszj@mail.tsinghua.edu.cn"], "keywords": ["Trustworthy Machine Learning", "Adversarial Robustness", "Inference Principle", "Mixup"], "TL;DR": "We exploit the global linearity of the mixup-trained models in inference to break the locality of the adversarial perturbations.", "abstract": "It has been widely recognized that adversarial examples can be easily crafted to fool deep networks, which mainly root from the locally non-linear behavior nearby input examples. Applying mixup in training provides an effective mechanism to improve generalization performance and model robustness against adversarial perturbations, which introduces the globally linear behavior in-between training examples. However, in previous work, the mixup-trained models only passively defend adversarial attacks in inference by directly classifying the inputs, where the induced global linearity is not well exploited. Namely, since the locality of the adversarial perturbations, it would be more efficient to actively break the locality via the globality of the model predictions. Inspired by simple geometric intuition, we develop an inference principle, named mixup inference (MI), for mixup-trained models. MI mixups the input with other random clean samples, which can shrink and transfer the equivalent perturbation if the input is adversarial. Our experiments on CIFAR-10 and CIFAR-100 demonstrate that MI can further improve the adversarial robustness for the models trained by mixup and its variants.", "pdf": "/pdf/2d15cb14ab9e0914711fc1187b082beb9a98f928.pdf", "code": "https://github.com/P2333/Mixup-Inference", "paperhash": "pang|mixup_inference_better_exploiting_mixup_to_defend_adversarial_attacks", "_bibtex": "@inproceedings{\nPang*2020Mixup,\ntitle={Mixup Inference: Better Exploiting Mixup to Defend Adversarial Attacks},\nauthor={Tianyu Pang* and Kun Xu* and Jun Zhu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=ByxtC2VtPB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/248f049c55dcf577f775b97d48d421c49e577e79.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "ByxtC2VtPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper272/Authors", "ICLR.cc/2020/Conference/Paper272/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper272/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper272/Reviewers", "ICLR.cc/2020/Conference/Paper272/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper272/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper272/Authors|ICLR.cc/2020/Conference/Paper272/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504173857, "tmdate": 1576860548559, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper272/Authors", "ICLR.cc/2020/Conference/Paper272/Reviewers", "ICLR.cc/2020/Conference/Paper272/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper272/-/Official_Comment"}}}, {"id": "SkxDmETLoB", "original": null, "number": 6, "cdate": 1573471262914, "ddate": null, "tcdate": 1573471262914, "tmdate": 1573557140265, "tddate": null, "forum": "ByxtC2VtPB", "replyto": "SkgiTp66YB", "invitation": "ICLR.cc/2020/Conference/Paper272/-/Official_Comment", "content": {"title": "More results on ImageNet", "comment": "We train a Resnet-50 with the mixup on ImageNet, and the top-1 accuracy on 50,000 validation examples is 73.7%. In the adversarial setting, we test on 1,000 randomly selected examples in the validation set. The accuracy results (%) on the sampled validation subset are shown below:\n\n        Method     || Clean Acc| $\\text{PGD}_{10}^{\\textbf{tar}}$ |$\\text{PGD}_{10}^{\\textbf{un}}$ | $\\text{PGD}_{50}^{\\textbf{tar}}$|$\\text{PGD}_{50}^{\\textbf{un}}$ ||\n         Mixup       ||      88.3     |      1.3      |      3.6     |      0.5      |      3.4    ||\nMixup + MI-OL ||      82.2     |      67.2    |     35.0   |      64.1    |     24.6   ||\n\nHere $\\epsilon=4/255$, number of execution in MI is $N=30$, and $\\lambda_{\\text{OL}}=0.4$. "}, "signatures": ["ICLR.cc/2020/Conference/Paper272/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper272/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Mixup Inference: Better Exploiting Mixup to Defend Adversarial Attacks", "authors": ["Tianyu Pang*", "Kun Xu*", "Jun Zhu"], "authorids": ["pty17@mails.tsinghua.edu.cn", "kunxu.thu@gmail.com", "dcszj@mail.tsinghua.edu.cn"], "keywords": ["Trustworthy Machine Learning", "Adversarial Robustness", "Inference Principle", "Mixup"], "TL;DR": "We exploit the global linearity of the mixup-trained models in inference to break the locality of the adversarial perturbations.", "abstract": "It has been widely recognized that adversarial examples can be easily crafted to fool deep networks, which mainly root from the locally non-linear behavior nearby input examples. Applying mixup in training provides an effective mechanism to improve generalization performance and model robustness against adversarial perturbations, which introduces the globally linear behavior in-between training examples. However, in previous work, the mixup-trained models only passively defend adversarial attacks in inference by directly classifying the inputs, where the induced global linearity is not well exploited. Namely, since the locality of the adversarial perturbations, it would be more efficient to actively break the locality via the globality of the model predictions. Inspired by simple geometric intuition, we develop an inference principle, named mixup inference (MI), for mixup-trained models. MI mixups the input with other random clean samples, which can shrink and transfer the equivalent perturbation if the input is adversarial. Our experiments on CIFAR-10 and CIFAR-100 demonstrate that MI can further improve the adversarial robustness for the models trained by mixup and its variants.", "pdf": "/pdf/2d15cb14ab9e0914711fc1187b082beb9a98f928.pdf", "code": "https://github.com/P2333/Mixup-Inference", "paperhash": "pang|mixup_inference_better_exploiting_mixup_to_defend_adversarial_attacks", "_bibtex": "@inproceedings{\nPang*2020Mixup,\ntitle={Mixup Inference: Better Exploiting Mixup to Defend Adversarial Attacks},\nauthor={Tianyu Pang* and Kun Xu* and Jun Zhu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=ByxtC2VtPB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/248f049c55dcf577f775b97d48d421c49e577e79.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "ByxtC2VtPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper272/Authors", "ICLR.cc/2020/Conference/Paper272/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper272/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper272/Reviewers", "ICLR.cc/2020/Conference/Paper272/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper272/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper272/Authors|ICLR.cc/2020/Conference/Paper272/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504173857, "tmdate": 1576860548559, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper272/Authors", "ICLR.cc/2020/Conference/Paper272/Reviewers", "ICLR.cc/2020/Conference/Paper272/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper272/-/Official_Comment"}}}, {"id": "B1xw_i0lor", "original": null, "number": 5, "cdate": 1573084015432, "ddate": null, "tcdate": 1573084015432, "tmdate": 1573084366624, "tddate": null, "forum": "ByxtC2VtPB", "replyto": "Hylb_3L5uS", "invitation": "ICLR.cc/2020/Conference/Paper272/-/Official_Comment", "content": {"title": "Thank you for the valuable review", "comment": "Thank you for the valuable review.\n\nWe upload the revision according to your suggestions, which includes:\n1. To be more rigorous, we reclaim that the adversarial examples mainly root from locally unreasonable or unstable behavior, and the adversarial training method induces locally stable behavior.\n\n2. We check the paper of Schimada et al. (2019) and find that it is indeed a quite related paper. We cite it and mention the difference between our work in Introduction.\n\n3. We modify the organization as you suggested in Section 3, i.e., we first introduce the method and then provide the theoretical analysis. We also explain the distribution of lambda ahead.\n\n4. We modify the notations like $y_{s}\\sim p_{s}(y_{s})$ to $y_{s}\\sim p_{s}(y)$, and $x_{s}\\sim p_{s}(x_{s}|y_{s})$ to $x_{s}\\sim p_{s}(x|y_{s})$.\n\n5. We provide adversarial examples crafted by adaptive attacks against the IAT + MI (best robustness) in Figure 5 (in Appendix part).\n\n \n \nBelow we further clarify the remain questions.\n \nQuestion 1. About the linearity:\nAs to the relationship between linearity and robustness, actually there is not a final conclusion. Goodfellow et al. (2015) claimed that the vulnerability of neural networks comes from their linear nature mainly because they found that FGSM can successfully attack some networks, and FGSM is based on the linear assumption of classifiers. However, there are many cases where FGSM fails but iterative attacks like PGD can still evade the models, which means adversarial examples do not necessarily stem from the linear nature. Besides, recent work [*1] finds that linearity can improve robustness, which contradicts the conclusion in Goodfellow et al. (2015). So as you suggested, in the revision, we do not claim the relationship between model linearity and the existence of adversarial examples.\n \n[*1] Qin et al. Adversarial Robustness through Local Linearization. NeurIPS 2019\n \n \nQuestion 2. The shrinkage mechanism in MI:\nIt is true that the signal in the original image will also shrink after performing MI. Intuitively, we intend to improve the \u2018signal-to-noise ratio\u2019 fed into the classifier, so what we expect is that the effect of adversarial noise shrinks faster than it caused by the original signal. This is actually the property represented in Eq.(12) and Eq.(15), where the left parts of the inequalities indicate the change of the adversarial effect, and the right part indicates the change of the original clean effect.\n \n \nQuestion 3. About the function $G_{k}$:\nWe cannot have a closed-form representation on $G_{k}$, since the neural network is a black-box model. So what we can claim in the theoretical analyses is that MI can improve robustness on the attacks satisfying Eq.(12) and Eq.(15), i.e., the decay of the adversarial effect is enough to compensate the decay of the original clean effect. Then in Figure 2, we empirically show that strong attacks like PGD satisfy this property and such that MI can better defend them.\n \n \n\nAnswers on other detailed notes and comments:\n(1)  In Algorithm 1, the average is performed on the $N$ randomly sampled points in the dataset with label $y_{s}$. In the experiments, we follow the setting in previous work (Xie et al. ICLR 2018) to set $N=30$ for fair comparisions.\n\n(2)  In Table 2, we choose the hyperparameter setting with comparable clean accuracy for each method to perform a fair comparison. As shown in Figure 3(b), there is a trade-off between clean accuracy and adversarial accuracy depending on the hyperparameters in each method, and MI can lead to better trade-off.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper272/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper272/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Mixup Inference: Better Exploiting Mixup to Defend Adversarial Attacks", "authors": ["Tianyu Pang*", "Kun Xu*", "Jun Zhu"], "authorids": ["pty17@mails.tsinghua.edu.cn", "kunxu.thu@gmail.com", "dcszj@mail.tsinghua.edu.cn"], "keywords": ["Trustworthy Machine Learning", "Adversarial Robustness", "Inference Principle", "Mixup"], "TL;DR": "We exploit the global linearity of the mixup-trained models in inference to break the locality of the adversarial perturbations.", "abstract": "It has been widely recognized that adversarial examples can be easily crafted to fool deep networks, which mainly root from the locally non-linear behavior nearby input examples. Applying mixup in training provides an effective mechanism to improve generalization performance and model robustness against adversarial perturbations, which introduces the globally linear behavior in-between training examples. However, in previous work, the mixup-trained models only passively defend adversarial attacks in inference by directly classifying the inputs, where the induced global linearity is not well exploited. Namely, since the locality of the adversarial perturbations, it would be more efficient to actively break the locality via the globality of the model predictions. Inspired by simple geometric intuition, we develop an inference principle, named mixup inference (MI), for mixup-trained models. MI mixups the input with other random clean samples, which can shrink and transfer the equivalent perturbation if the input is adversarial. Our experiments on CIFAR-10 and CIFAR-100 demonstrate that MI can further improve the adversarial robustness for the models trained by mixup and its variants.", "pdf": "/pdf/2d15cb14ab9e0914711fc1187b082beb9a98f928.pdf", "code": "https://github.com/P2333/Mixup-Inference", "paperhash": "pang|mixup_inference_better_exploiting_mixup_to_defend_adversarial_attacks", "_bibtex": "@inproceedings{\nPang*2020Mixup,\ntitle={Mixup Inference: Better Exploiting Mixup to Defend Adversarial Attacks},\nauthor={Tianyu Pang* and Kun Xu* and Jun Zhu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=ByxtC2VtPB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/248f049c55dcf577f775b97d48d421c49e577e79.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "ByxtC2VtPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper272/Authors", "ICLR.cc/2020/Conference/Paper272/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper272/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper272/Reviewers", "ICLR.cc/2020/Conference/Paper272/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper272/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper272/Authors|ICLR.cc/2020/Conference/Paper272/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504173857, "tmdate": 1576860548559, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper272/Authors", "ICLR.cc/2020/Conference/Paper272/Reviewers", "ICLR.cc/2020/Conference/Paper272/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper272/-/Official_Comment"}}}, {"id": "SJljWoRgjS", "original": null, "number": 4, "cdate": 1573083906814, "ddate": null, "tcdate": 1573083906814, "tmdate": 1573083906814, "tddate": null, "forum": "ByxtC2VtPB", "replyto": "SkgiTp66YB", "invitation": "ICLR.cc/2020/Conference/Paper272/-/Official_Comment", "content": {"title": "Thank you for the supportive review", "comment": "Thank you for the supportive review.\n\nShowing the average and standard errors is a good suggestion, we are running the experiments and will add them in the final version.\n \nWe also have some initial results on ImageNet, where applying MI can improve the adversarial accuracy of mixup-trained models from 3.6% to 35%, under PGD-10 attacks with $\\epsilon=4/255$."}, "signatures": ["ICLR.cc/2020/Conference/Paper272/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper272/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Mixup Inference: Better Exploiting Mixup to Defend Adversarial Attacks", "authors": ["Tianyu Pang*", "Kun Xu*", "Jun Zhu"], "authorids": ["pty17@mails.tsinghua.edu.cn", "kunxu.thu@gmail.com", "dcszj@mail.tsinghua.edu.cn"], "keywords": ["Trustworthy Machine Learning", "Adversarial Robustness", "Inference Principle", "Mixup"], "TL;DR": "We exploit the global linearity of the mixup-trained models in inference to break the locality of the adversarial perturbations.", "abstract": "It has been widely recognized that adversarial examples can be easily crafted to fool deep networks, which mainly root from the locally non-linear behavior nearby input examples. Applying mixup in training provides an effective mechanism to improve generalization performance and model robustness against adversarial perturbations, which introduces the globally linear behavior in-between training examples. However, in previous work, the mixup-trained models only passively defend adversarial attacks in inference by directly classifying the inputs, where the induced global linearity is not well exploited. Namely, since the locality of the adversarial perturbations, it would be more efficient to actively break the locality via the globality of the model predictions. Inspired by simple geometric intuition, we develop an inference principle, named mixup inference (MI), for mixup-trained models. MI mixups the input with other random clean samples, which can shrink and transfer the equivalent perturbation if the input is adversarial. Our experiments on CIFAR-10 and CIFAR-100 demonstrate that MI can further improve the adversarial robustness for the models trained by mixup and its variants.", "pdf": "/pdf/2d15cb14ab9e0914711fc1187b082beb9a98f928.pdf", "code": "https://github.com/P2333/Mixup-Inference", "paperhash": "pang|mixup_inference_better_exploiting_mixup_to_defend_adversarial_attacks", "_bibtex": "@inproceedings{\nPang*2020Mixup,\ntitle={Mixup Inference: Better Exploiting Mixup to Defend Adversarial Attacks},\nauthor={Tianyu Pang* and Kun Xu* and Jun Zhu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=ByxtC2VtPB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/248f049c55dcf577f775b97d48d421c49e577e79.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "ByxtC2VtPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper272/Authors", "ICLR.cc/2020/Conference/Paper272/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper272/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper272/Reviewers", "ICLR.cc/2020/Conference/Paper272/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper272/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper272/Authors|ICLR.cc/2020/Conference/Paper272/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504173857, "tmdate": 1576860548559, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper272/Authors", "ICLR.cc/2020/Conference/Paper272/Reviewers", "ICLR.cc/2020/Conference/Paper272/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper272/-/Official_Comment"}}}, {"id": "ByxZpqCgjr", "original": null, "number": 3, "cdate": 1573083833037, "ddate": null, "tcdate": 1573083833037, "tmdate": 1573083833037, "tddate": null, "forum": "ByxtC2VtPB", "replyto": "B1g-lU5W9B", "invitation": "ICLR.cc/2020/Conference/Paper272/-/Official_Comment", "content": {"title": "Thank you for the supportive review", "comment": "Thank you for the supportive review.\n\nQuestion 1. The case of mislabeling:\nThe improvement on decreasing output mislabels is mainly finished in the training phase by mixup or other better training mechanisms. The main effect of MI is to improve adversarial robustness, where the original clean counterpart is correctly classified. It is possible that MI can correctly classify the adversarial examples crafted on mislabeled clean images, but this is not guaranteed by theoretical analyses.\n \n \nQuestion 2. More efficient inference:\nMI can still have good performance when $N=1$. A higher number of $N$ can stabilize the inference progress, but is not necessary. In our experiment, we choose $N=30$ to follow the setting in previous work (Xie et al. ICLR 2018).\n \n \nQuestion 3. About MI-Combined:\nThe MI-Combined method consists of MI-PL in the detection phase and MI-OL in the classification phase. These two phases are separately justified in Eq.(14) and Eq.(15).\n \n\nQuestion 4. Related work:\nThank you for pointing out, we have added the reference of Tokozume et al. (ICLR 2018) in the uploaded revision.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper272/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper272/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Mixup Inference: Better Exploiting Mixup to Defend Adversarial Attacks", "authors": ["Tianyu Pang*", "Kun Xu*", "Jun Zhu"], "authorids": ["pty17@mails.tsinghua.edu.cn", "kunxu.thu@gmail.com", "dcszj@mail.tsinghua.edu.cn"], "keywords": ["Trustworthy Machine Learning", "Adversarial Robustness", "Inference Principle", "Mixup"], "TL;DR": "We exploit the global linearity of the mixup-trained models in inference to break the locality of the adversarial perturbations.", "abstract": "It has been widely recognized that adversarial examples can be easily crafted to fool deep networks, which mainly root from the locally non-linear behavior nearby input examples. Applying mixup in training provides an effective mechanism to improve generalization performance and model robustness against adversarial perturbations, which introduces the globally linear behavior in-between training examples. However, in previous work, the mixup-trained models only passively defend adversarial attacks in inference by directly classifying the inputs, where the induced global linearity is not well exploited. Namely, since the locality of the adversarial perturbations, it would be more efficient to actively break the locality via the globality of the model predictions. Inspired by simple geometric intuition, we develop an inference principle, named mixup inference (MI), for mixup-trained models. MI mixups the input with other random clean samples, which can shrink and transfer the equivalent perturbation if the input is adversarial. Our experiments on CIFAR-10 and CIFAR-100 demonstrate that MI can further improve the adversarial robustness for the models trained by mixup and its variants.", "pdf": "/pdf/2d15cb14ab9e0914711fc1187b082beb9a98f928.pdf", "code": "https://github.com/P2333/Mixup-Inference", "paperhash": "pang|mixup_inference_better_exploiting_mixup_to_defend_adversarial_attacks", "_bibtex": "@inproceedings{\nPang*2020Mixup,\ntitle={Mixup Inference: Better Exploiting Mixup to Defend Adversarial Attacks},\nauthor={Tianyu Pang* and Kun Xu* and Jun Zhu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=ByxtC2VtPB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/248f049c55dcf577f775b97d48d421c49e577e79.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "ByxtC2VtPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper272/Authors", "ICLR.cc/2020/Conference/Paper272/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper272/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper272/Reviewers", "ICLR.cc/2020/Conference/Paper272/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper272/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper272/Authors|ICLR.cc/2020/Conference/Paper272/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504173857, "tmdate": 1576860548559, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper272/Authors", "ICLR.cc/2020/Conference/Paper272/Reviewers", "ICLR.cc/2020/Conference/Paper272/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper272/-/Official_Comment"}}}, {"id": "SkgiTp66YB", "original": null, "number": 2, "cdate": 1571835330806, "ddate": null, "tcdate": 1571835330806, "tmdate": 1572972616779, "tddate": null, "forum": "ByxtC2VtPB", "replyto": "ByxtC2VtPB", "invitation": "ICLR.cc/2020/Conference/Paper272/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a novel use of mixup, which is originally a data augmentation method incorporating two training samples and their corresponding labels. The authors utilize mixup not for training but for inference (MI; Mixup Inference). Experimental results on Cifar 10, and Cifar 100 show that MI can boost the classification performance in combination with interpolated AT (Adversarial  Training) and mixup.\n\nI lean to accept this paper. The proposed method is simple but effective, moreover well-motivated. The experimental results, including several ablation studies, show a high versatility with existing methods.\n\nMy minor concerns are, however, consisting of two points.\n- The authors should repeat the experiments several times and show the averages and standard errors to make the significance clear.\n- Both Cifar 10 and Cifar 100 are relatively small scale datasets. I would like the authors to investigate larger ones."}, "signatures": ["ICLR.cc/2020/Conference/Paper272/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper272/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Mixup Inference: Better Exploiting Mixup to Defend Adversarial Attacks", "authors": ["Tianyu Pang*", "Kun Xu*", "Jun Zhu"], "authorids": ["pty17@mails.tsinghua.edu.cn", "kunxu.thu@gmail.com", "dcszj@mail.tsinghua.edu.cn"], "keywords": ["Trustworthy Machine Learning", "Adversarial Robustness", "Inference Principle", "Mixup"], "TL;DR": "We exploit the global linearity of the mixup-trained models in inference to break the locality of the adversarial perturbations.", "abstract": "It has been widely recognized that adversarial examples can be easily crafted to fool deep networks, which mainly root from the locally non-linear behavior nearby input examples. Applying mixup in training provides an effective mechanism to improve generalization performance and model robustness against adversarial perturbations, which introduces the globally linear behavior in-between training examples. However, in previous work, the mixup-trained models only passively defend adversarial attacks in inference by directly classifying the inputs, where the induced global linearity is not well exploited. Namely, since the locality of the adversarial perturbations, it would be more efficient to actively break the locality via the globality of the model predictions. Inspired by simple geometric intuition, we develop an inference principle, named mixup inference (MI), for mixup-trained models. MI mixups the input with other random clean samples, which can shrink and transfer the equivalent perturbation if the input is adversarial. Our experiments on CIFAR-10 and CIFAR-100 demonstrate that MI can further improve the adversarial robustness for the models trained by mixup and its variants.", "pdf": "/pdf/2d15cb14ab9e0914711fc1187b082beb9a98f928.pdf", "code": "https://github.com/P2333/Mixup-Inference", "paperhash": "pang|mixup_inference_better_exploiting_mixup_to_defend_adversarial_attacks", "_bibtex": "@inproceedings{\nPang*2020Mixup,\ntitle={Mixup Inference: Better Exploiting Mixup to Defend Adversarial Attacks},\nauthor={Tianyu Pang* and Kun Xu* and Jun Zhu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=ByxtC2VtPB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/248f049c55dcf577f775b97d48d421c49e577e79.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "ByxtC2VtPB", "replyto": "ByxtC2VtPB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper272/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper272/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574722376000, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper272/Reviewers"], "noninvitees": [], "tcdate": 1570237754543, "tmdate": 1574723079015, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper272/-/Official_Review"}}}, {"id": "B1g-lU5W9B", "original": null, "number": 3, "cdate": 1572083177021, "ddate": null, "tcdate": 1572083177021, "tmdate": 1572972616736, "tddate": null, "forum": "ByxtC2VtPB", "replyto": "ByxtC2VtPB", "invitation": "ICLR.cc/2020/Conference/Paper272/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper introduces a novel method for an adversarial attack named mixup inference (MI).  Most of the work focuses on embedding mixup mechanism in the training phase, but MI uses the mixup in the inference phase. MI method has two main effects for the adversarial attack: one is perturbation shrinkage, and the other one is input transfer because MI can exploit\nthe induced global linearity. The experimental results show that MI can return more reliable predictions under different threat models.\n\nThis paper should be accepted because the proposed method is super simple but effective for defending from adversarial attacks under different threat conditions. This paper is well-written, including theoretical insights on why the MI method works.\n\nThe reviewer has some questions or comments to clarify the paper:\n1) In the explanation of the MI method, the authors assume only the cases where the input data is correctly classified if it is clean, or wrongly classified if it is adversarial. In a realistic situation, the classifier sometimes outputs mislabels. Thus is the discussion in Sec.3 valid if the clean input data misclassified?\n\n2) To predict the category of the input, MI methods must perform inference N times. It is not efficient. Are there any ideas to reduce the number of inferences?\n\n3) MI-Combined seems ad-hoc. It would be better to state its justification by theory.\n\n4) The same idea of mixup was proposed at the same conference (ICLR2018). It should be cited.\nTokozume et al., Learning from Between-class Examples for Deep Sound Recognition. ICLR, 2018."}, "signatures": ["ICLR.cc/2020/Conference/Paper272/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper272/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Mixup Inference: Better Exploiting Mixup to Defend Adversarial Attacks", "authors": ["Tianyu Pang*", "Kun Xu*", "Jun Zhu"], "authorids": ["pty17@mails.tsinghua.edu.cn", "kunxu.thu@gmail.com", "dcszj@mail.tsinghua.edu.cn"], "keywords": ["Trustworthy Machine Learning", "Adversarial Robustness", "Inference Principle", "Mixup"], "TL;DR": "We exploit the global linearity of the mixup-trained models in inference to break the locality of the adversarial perturbations.", "abstract": "It has been widely recognized that adversarial examples can be easily crafted to fool deep networks, which mainly root from the locally non-linear behavior nearby input examples. Applying mixup in training provides an effective mechanism to improve generalization performance and model robustness against adversarial perturbations, which introduces the globally linear behavior in-between training examples. However, in previous work, the mixup-trained models only passively defend adversarial attacks in inference by directly classifying the inputs, where the induced global linearity is not well exploited. Namely, since the locality of the adversarial perturbations, it would be more efficient to actively break the locality via the globality of the model predictions. Inspired by simple geometric intuition, we develop an inference principle, named mixup inference (MI), for mixup-trained models. MI mixups the input with other random clean samples, which can shrink and transfer the equivalent perturbation if the input is adversarial. Our experiments on CIFAR-10 and CIFAR-100 demonstrate that MI can further improve the adversarial robustness for the models trained by mixup and its variants.", "pdf": "/pdf/2d15cb14ab9e0914711fc1187b082beb9a98f928.pdf", "code": "https://github.com/P2333/Mixup-Inference", "paperhash": "pang|mixup_inference_better_exploiting_mixup_to_defend_adversarial_attacks", "_bibtex": "@inproceedings{\nPang*2020Mixup,\ntitle={Mixup Inference: Better Exploiting Mixup to Defend Adversarial Attacks},\nauthor={Tianyu Pang* and Kun Xu* and Jun Zhu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=ByxtC2VtPB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/248f049c55dcf577f775b97d48d421c49e577e79.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "ByxtC2VtPB", "replyto": "ByxtC2VtPB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper272/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper272/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574722376000, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper272/Reviewers"], "noninvitees": [], "tcdate": 1570237754543, "tmdate": 1574723079015, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper272/-/Official_Review"}}}, {"id": "HJgt1UVGcr", "original": null, "number": 2, "cdate": 1572124128851, "ddate": null, "tcdate": 1572124128851, "tmdate": 1572124872568, "tddate": null, "forum": "ByxtC2VtPB", "replyto": "ryxtdQAuYS", "invitation": "ICLR.cc/2020/Conference/Paper272/-/Public_Comment", "content": {"title": "Both inference and training", "comment": "Thanks! The previous papers used interpolation steps in both training and inference to improve robustness of the deep neural nets. Not only in the semi-supervised learning but also in the supervised learning. I believe the papers are very related, and these papers are among the first few papers that considered interpolation in deep nets. "}, "signatures": ["~Bao_Wang1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Bao_Wang1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Mixup Inference: Better Exploiting Mixup to Defend Adversarial Attacks", "authors": ["Tianyu Pang*", "Kun Xu*", "Jun Zhu"], "authorids": ["pty17@mails.tsinghua.edu.cn", "kunxu.thu@gmail.com", "dcszj@mail.tsinghua.edu.cn"], "keywords": ["Trustworthy Machine Learning", "Adversarial Robustness", "Inference Principle", "Mixup"], "TL;DR": "We exploit the global linearity of the mixup-trained models in inference to break the locality of the adversarial perturbations.", "abstract": "It has been widely recognized that adversarial examples can be easily crafted to fool deep networks, which mainly root from the locally non-linear behavior nearby input examples. Applying mixup in training provides an effective mechanism to improve generalization performance and model robustness against adversarial perturbations, which introduces the globally linear behavior in-between training examples. However, in previous work, the mixup-trained models only passively defend adversarial attacks in inference by directly classifying the inputs, where the induced global linearity is not well exploited. Namely, since the locality of the adversarial perturbations, it would be more efficient to actively break the locality via the globality of the model predictions. Inspired by simple geometric intuition, we develop an inference principle, named mixup inference (MI), for mixup-trained models. MI mixups the input with other random clean samples, which can shrink and transfer the equivalent perturbation if the input is adversarial. Our experiments on CIFAR-10 and CIFAR-100 demonstrate that MI can further improve the adversarial robustness for the models trained by mixup and its variants.", "pdf": "/pdf/2d15cb14ab9e0914711fc1187b082beb9a98f928.pdf", "code": "https://github.com/P2333/Mixup-Inference", "paperhash": "pang|mixup_inference_better_exploiting_mixup_to_defend_adversarial_attacks", "_bibtex": "@inproceedings{\nPang*2020Mixup,\ntitle={Mixup Inference: Better Exploiting Mixup to Defend Adversarial Attacks},\nauthor={Tianyu Pang* and Kun Xu* and Jun Zhu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=ByxtC2VtPB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/248f049c55dcf577f775b97d48d421c49e577e79.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "ByxtC2VtPB", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504211454, "tmdate": 1576860581866, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper272/Authors", "ICLR.cc/2020/Conference/Paper272/Reviewers", "ICLR.cc/2020/Conference/Paper272/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper272/-/Public_Comment"}}}, {"id": "rygsYPl9tB", "original": null, "number": 2, "cdate": 1571583874681, "ddate": null, "tcdate": 1571583874681, "tmdate": 1571583874681, "tddate": null, "forum": "ByxtC2VtPB", "replyto": "SkeFK-RuYS", "invitation": "ICLR.cc/2020/Conference/Paper272/-/Official_Comment", "content": {"comment": "Thank you for your recommendation on these interesting work, we read the linked papers to check if they are related to our work. As indicated by Reviewer1, our paper focuses on using mixup at inference time for better robustness.\n\nFirst of all, we find that none of the three linked papers is based on the mixup method (Zhang et al. ICLR 2018), or mentions the mixup method in their related work.\n\nThe three linked papers propose and analyze one similar method, named weighted nonlocal Laplacian (WNLL) layer as the output layer for DNNs. The claimed advantages of the WNLL layer is mainly on data-efficient learning and robustness in the semi-supervised setting, while we consider supervised learning. Besides, the WNLL layer benefits the learning in the training phase, while our mixup-inference (MI) method further improves the robustness of mixup-trained models in the inference phase.\n\nThanks again for your kind suggestions, but it seems that the linked papers are not quite related to our work. Please feel free to let us know if we misunderstand some parts of your linked papers.", "title": "The linked papers seem not quite related"}, "signatures": ["ICLR.cc/2020/Conference/Paper272/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper272/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Mixup Inference: Better Exploiting Mixup to Defend Adversarial Attacks", "authors": ["Tianyu Pang*", "Kun Xu*", "Jun Zhu"], "authorids": ["pty17@mails.tsinghua.edu.cn", "kunxu.thu@gmail.com", "dcszj@mail.tsinghua.edu.cn"], "keywords": ["Trustworthy Machine Learning", "Adversarial Robustness", "Inference Principle", "Mixup"], "TL;DR": "We exploit the global linearity of the mixup-trained models in inference to break the locality of the adversarial perturbations.", "abstract": "It has been widely recognized that adversarial examples can be easily crafted to fool deep networks, which mainly root from the locally non-linear behavior nearby input examples. Applying mixup in training provides an effective mechanism to improve generalization performance and model robustness against adversarial perturbations, which introduces the globally linear behavior in-between training examples. However, in previous work, the mixup-trained models only passively defend adversarial attacks in inference by directly classifying the inputs, where the induced global linearity is not well exploited. Namely, since the locality of the adversarial perturbations, it would be more efficient to actively break the locality via the globality of the model predictions. Inspired by simple geometric intuition, we develop an inference principle, named mixup inference (MI), for mixup-trained models. MI mixups the input with other random clean samples, which can shrink and transfer the equivalent perturbation if the input is adversarial. Our experiments on CIFAR-10 and CIFAR-100 demonstrate that MI can further improve the adversarial robustness for the models trained by mixup and its variants.", "pdf": "/pdf/2d15cb14ab9e0914711fc1187b082beb9a98f928.pdf", "code": "https://github.com/P2333/Mixup-Inference", "paperhash": "pang|mixup_inference_better_exploiting_mixup_to_defend_adversarial_attacks", "_bibtex": "@inproceedings{\nPang*2020Mixup,\ntitle={Mixup Inference: Better Exploiting Mixup to Defend Adversarial Attacks},\nauthor={Tianyu Pang* and Kun Xu* and Jun Zhu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=ByxtC2VtPB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/248f049c55dcf577f775b97d48d421c49e577e79.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "ByxtC2VtPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper272/Authors", "ICLR.cc/2020/Conference/Paper272/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper272/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper272/Reviewers", "ICLR.cc/2020/Conference/Paper272/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper272/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper272/Authors|ICLR.cc/2020/Conference/Paper272/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504173857, "tmdate": 1576860548559, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper272/Authors", "ICLR.cc/2020/Conference/Paper272/Reviewers", "ICLR.cc/2020/Conference/Paper272/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper272/-/Official_Comment"}}}, {"id": "ryxtdQAuYS", "original": null, "number": 1, "cdate": 1571509105276, "ddate": null, "tcdate": 1571509105276, "tmdate": 1571509105276, "tddate": null, "forum": "ByxtC2VtPB", "replyto": "SkeFK-RuYS", "invitation": "ICLR.cc/2020/Conference/Paper272/-/Official_Comment", "content": {"comment": "Were the papers you linked to using mixup during training or inference?  I believe that this paper's focus is on using mixup at inference-time for robustness.  \n\nIt may still be the case that those papers should be cited.  ", "title": "Inference or Training?  "}, "signatures": ["ICLR.cc/2020/Conference/Paper272/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper272/AnonReviewer1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Mixup Inference: Better Exploiting Mixup to Defend Adversarial Attacks", "authors": ["Tianyu Pang*", "Kun Xu*", "Jun Zhu"], "authorids": ["pty17@mails.tsinghua.edu.cn", "kunxu.thu@gmail.com", "dcszj@mail.tsinghua.edu.cn"], "keywords": ["Trustworthy Machine Learning", "Adversarial Robustness", "Inference Principle", "Mixup"], "TL;DR": "We exploit the global linearity of the mixup-trained models in inference to break the locality of the adversarial perturbations.", "abstract": "It has been widely recognized that adversarial examples can be easily crafted to fool deep networks, which mainly root from the locally non-linear behavior nearby input examples. Applying mixup in training provides an effective mechanism to improve generalization performance and model robustness against adversarial perturbations, which introduces the globally linear behavior in-between training examples. However, in previous work, the mixup-trained models only passively defend adversarial attacks in inference by directly classifying the inputs, where the induced global linearity is not well exploited. Namely, since the locality of the adversarial perturbations, it would be more efficient to actively break the locality via the globality of the model predictions. Inspired by simple geometric intuition, we develop an inference principle, named mixup inference (MI), for mixup-trained models. MI mixups the input with other random clean samples, which can shrink and transfer the equivalent perturbation if the input is adversarial. Our experiments on CIFAR-10 and CIFAR-100 demonstrate that MI can further improve the adversarial robustness for the models trained by mixup and its variants.", "pdf": "/pdf/2d15cb14ab9e0914711fc1187b082beb9a98f928.pdf", "code": "https://github.com/P2333/Mixup-Inference", "paperhash": "pang|mixup_inference_better_exploiting_mixup_to_defend_adversarial_attacks", "_bibtex": "@inproceedings{\nPang*2020Mixup,\ntitle={Mixup Inference: Better Exploiting Mixup to Defend Adversarial Attacks},\nauthor={Tianyu Pang* and Kun Xu* and Jun Zhu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=ByxtC2VtPB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/248f049c55dcf577f775b97d48d421c49e577e79.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "ByxtC2VtPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper272/Authors", "ICLR.cc/2020/Conference/Paper272/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper272/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper272/Reviewers", "ICLR.cc/2020/Conference/Paper272/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper272/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper272/Authors|ICLR.cc/2020/Conference/Paper272/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504173857, "tmdate": 1576860548559, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper272/Authors", "ICLR.cc/2020/Conference/Paper272/Reviewers", "ICLR.cc/2020/Conference/Paper272/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper272/-/Official_Comment"}}}, {"id": "SkeFK-RuYS", "original": null, "number": 1, "cdate": 1571508609191, "ddate": null, "tcdate": 1571508609191, "tmdate": 1571508634609, "tddate": null, "forum": "ByxtC2VtPB", "replyto": "ByxtC2VtPB", "invitation": "ICLR.cc/2020/Conference/Paper272/-/Public_Comment", "content": {"comment": "Hi, I read this paper which is quite interesting to me. I would like to point out three papers that among the first a few that considered interpolation for adversarial defense.\n\n1. B. Wang, et al. Deep Neural Nets with Interpolating Function as Output Activation, NeurIPS 2018.\n\n2. B. Wang, et al. Adversarial Defense via Data Dependent Activation Function and Total Variation Minimization, arXiv:1809.08516 2018\n\n3. B. Wang, et al. Graph Interpolating Activation Improves Both Natural and Robust Accuracies in Data-Efficient Deep Learning, arXiv:1907.06800 2019.\n\nThanks for your attention.\n", "title": "An interesting paper"}, "signatures": ["~Bao_Wang1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Bao_Wang1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Mixup Inference: Better Exploiting Mixup to Defend Adversarial Attacks", "authors": ["Tianyu Pang*", "Kun Xu*", "Jun Zhu"], "authorids": ["pty17@mails.tsinghua.edu.cn", "kunxu.thu@gmail.com", "dcszj@mail.tsinghua.edu.cn"], "keywords": ["Trustworthy Machine Learning", "Adversarial Robustness", "Inference Principle", "Mixup"], "TL;DR": "We exploit the global linearity of the mixup-trained models in inference to break the locality of the adversarial perturbations.", "abstract": "It has been widely recognized that adversarial examples can be easily crafted to fool deep networks, which mainly root from the locally non-linear behavior nearby input examples. Applying mixup in training provides an effective mechanism to improve generalization performance and model robustness against adversarial perturbations, which introduces the globally linear behavior in-between training examples. However, in previous work, the mixup-trained models only passively defend adversarial attacks in inference by directly classifying the inputs, where the induced global linearity is not well exploited. Namely, since the locality of the adversarial perturbations, it would be more efficient to actively break the locality via the globality of the model predictions. Inspired by simple geometric intuition, we develop an inference principle, named mixup inference (MI), for mixup-trained models. MI mixups the input with other random clean samples, which can shrink and transfer the equivalent perturbation if the input is adversarial. Our experiments on CIFAR-10 and CIFAR-100 demonstrate that MI can further improve the adversarial robustness for the models trained by mixup and its variants.", "pdf": "/pdf/2d15cb14ab9e0914711fc1187b082beb9a98f928.pdf", "code": "https://github.com/P2333/Mixup-Inference", "paperhash": "pang|mixup_inference_better_exploiting_mixup_to_defend_adversarial_attacks", "_bibtex": "@inproceedings{\nPang*2020Mixup,\ntitle={Mixup Inference: Better Exploiting Mixup to Defend Adversarial Attacks},\nauthor={Tianyu Pang* and Kun Xu* and Jun Zhu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=ByxtC2VtPB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/248f049c55dcf577f775b97d48d421c49e577e79.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "ByxtC2VtPB", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504211454, "tmdate": 1576860581866, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper272/Authors", "ICLR.cc/2020/Conference/Paper272/Reviewers", "ICLR.cc/2020/Conference/Paper272/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper272/-/Public_Comment"}}}], "count": 16}