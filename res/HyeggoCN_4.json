{"notes": [{"id": "HyeggoCN_4", "original": "r1eHMT4NuV", "number": 48, "cdate": 1553423079725, "ddate": null, "tcdate": 1553423079725, "tmdate": 1562082115107, "tddate": null, "forum": "HyeggoCN_4", "replyto": null, "invitation": "ICLR.cc/2019/Workshop/LLD/-/Blind_Submission", "content": {"title": "Learning To Avoid Negative Transfer in Few Shot Transfer Learning", "authors": ["James O' Neill"], "authorids": ["james.o-neill@liverpool.ac.uk"], "keywords": ["few shot learning", "negative transfer", "cubic spline", "ensemble learning"], "TL;DR": "A dynamic bagging methods approach to avoiding negatve transfer in neural network few-shot transfer learning", "abstract": "Many tasks in natural language understanding require learning relationships between two sequences for various tasks such as natural language inference, paraphrasing and entailment. These aforementioned tasks are similar in nature, yet they are often modeled individually. Knowledge transfer can be effective for closely related tasks, which is usually carried out using parameter transfer in neural networks. However, transferring all parameters, some of which irrelevant for a target task, can lead to sub-optimal results and can have a negative effect on performance, referred to as \\textit{negative} transfer. \n\nHence, this paper focuses on the transferability of both instances and parameters across natural language understanding tasks by proposing an ensemble-based transfer learning method in the context of few-shot learning.\n\nOur main contribution is a method for mitigating negative transfer across tasks when using neural networks, which involves dynamically bagging small recurrent neural networks trained on different subsets of the source task/s. We present a straightforward yet novel approach for incorporating these networks to a target task for few-shot learning by using a decaying parameter chosen according to the slope changes of a smoothed spline error curve at sub-intervals during training.\n\nOur proposed method show improvements over hard and soft parameter sharing transfer methods in the few-shot learning case and shows competitive performance against models that are trained given full supervision on the target task, from only few examples.", "pdf": "/pdf/4dfe81e059c263505ef5714bce087f7575f60456.pdf", "paperhash": "neill|learning_to_avoid_negative_transfer_in_few_shot_transfer_learning"}, "signatures": ["ICLR.cc/2019/Workshop/LLD"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/LLD"], "details": {"replyCount": 3, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/LLD/-/Blind_Submission", "cdate": 1548689671889, "reply": {"forum": null, "replyto": null, "readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2019/Workshop/LLD"]}, "signatures": {"values": ["ICLR.cc/2019/Workshop/LLD"]}, "content": {"authors": {"values-regex": ".*"}, "authorids": {"values-regex": ".*"}}}, "tcdate": 1548689671889, "tmdate": 1557933709646, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/LLD"], "invitees": ["~"], "signatures": ["ICLR.cc/2019/Workshop/LLD"], "details": {"writable": true}}}, "tauthor": "OpenReview.net"}, {"id": "Hy-fabSK4", "original": null, "number": 1, "cdate": 1554484488516, "ddate": null, "tcdate": 1554484488516, "tmdate": 1555512025916, "tddate": null, "forum": "HyeggoCN_4", "replyto": "HyeggoCN_4", "invitation": "ICLR.cc/2019/Workshop/LLD/-/Paper48/Official_Review", "content": {"title": "8 pages, desk reject", "review": "This paper is longer than the 4 page limit.", "rating": "1: Strong rejection", "confidence": "3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Workshop/LLD/Paper48/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/LLD/Paper48/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning To Avoid Negative Transfer in Few Shot Transfer Learning", "authors": ["James O' Neill"], "authorids": ["james.o-neill@liverpool.ac.uk"], "keywords": ["few shot learning", "negative transfer", "cubic spline", "ensemble learning"], "TL;DR": "A dynamic bagging methods approach to avoiding negatve transfer in neural network few-shot transfer learning", "abstract": "Many tasks in natural language understanding require learning relationships between two sequences for various tasks such as natural language inference, paraphrasing and entailment. These aforementioned tasks are similar in nature, yet they are often modeled individually. Knowledge transfer can be effective for closely related tasks, which is usually carried out using parameter transfer in neural networks. However, transferring all parameters, some of which irrelevant for a target task, can lead to sub-optimal results and can have a negative effect on performance, referred to as \\textit{negative} transfer. \n\nHence, this paper focuses on the transferability of both instances and parameters across natural language understanding tasks by proposing an ensemble-based transfer learning method in the context of few-shot learning.\n\nOur main contribution is a method for mitigating negative transfer across tasks when using neural networks, which involves dynamically bagging small recurrent neural networks trained on different subsets of the source task/s. We present a straightforward yet novel approach for incorporating these networks to a target task for few-shot learning by using a decaying parameter chosen according to the slope changes of a smoothed spline error curve at sub-intervals during training.\n\nOur proposed method show improvements over hard and soft parameter sharing transfer methods in the few-shot learning case and shows competitive performance against models that are trained given full supervision on the target task, from only few examples.", "pdf": "/pdf/4dfe81e059c263505ef5714bce087f7575f60456.pdf", "paperhash": "neill|learning_to_avoid_negative_transfer_in_few_shot_transfer_learning"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/LLD/-/Paper48/Official_Review", "cdate": 1553713413709, "expdate": 1555718400000, "duedate": 1554681600000, "reply": {"forum": "HyeggoCN_4", "replyto": "HyeggoCN_4", "writers": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2019/Workshop/LLD/Paper48/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2019/Workshop/LLD/Paper48/AnonReviewer[0-9]+"}, "readers": {"values": ["everyone"], "description": "The users who will be allowed to read the above content."}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["5: Top 15% of accepted papers, strong accept", "4: Top 50% of accepted papers, clear accept", "3: Marginally above acceptance threshold", "2: Marginally below acceptance threshold", "1: Strong rejection"], "required": true}, "confidence": {"order": 4, "value-radio": ["3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "2: The reviewer is fairly confident that the evaluation is correct", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "tcdate": 1553713413709, "tmdate": 1555511823879, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/LLD"], "invitees": ["ICLR.cc/2019/Workshop/LLD/Paper48/Reviewers"], "signatures": ["ICLR.cc/2019/Workshop/LLD"]}}}, {"id": "SyxsIA-uKV", "original": null, "number": 2, "cdate": 1554681427341, "ddate": null, "tcdate": 1554681427341, "tmdate": 1555511886978, "tddate": null, "forum": "HyeggoCN_4", "replyto": "HyeggoCN_4", "invitation": "ICLR.cc/2019/Workshop/LLD/-/Paper48/Official_Review", "content": {"title": "Could be interesting but in need of a rewrite", "review": "The paper investigates a method for transfer learning where the amount of transfer from one task to another is controlled by a dynamic hyperparameter.\nEvaluation is performed by combining the SNLI, MultiNLI and Question Matching datasets in various ways using transfer learning.\n\nThe work seems to have some interesting ideas, but the paper is lacking in clarity and therefore it is difficult to evaluate the validity and benefit of this approach.\n\nMuch of the system description is confusing and difficult to follow. Some examples:\n- In Section 2.2, acronym SN used without defining it.\n- In Section 3, matrix A and its size is defined, but it is not explained what the values in there represent or how they are then used.\n- Section 3.1 says calculations are made based on \"voting parameters\", which are not mentioned anywhere else in the paper.\n- Variables \\theta_s and \\theta_t are used but not defined.\n- GRU-1 and GRU-2 are different configurations in the results tables, but are never defined.\n- Tables 2 and 3 use S, M and Q, which are not defined anywhere. I'm guessing these are meant to reference SNLI, MNLI and QM datasets, but explaining this in the caption would be helpful.\n\nOverall, the results and findings are difficult to interpret. For clarity and comparable evaluation, the results tables should contain a non-transfer baseline, a simple fine-tuning baseline, and the SOTA results, in addition to the proposed models. Also, the tables are in need of informative captions about the training conditions.\n\nSection 2.2 describes 3 systems from previous work that are claimed to be SOTA for the tasks that are being investigated. The results section also claims to achieve results that are comparable to SOTA. However, these models are from 2016 and 2017, and there has been quite a bit of work on NLI since then.\nThe best reported accuracy on SNLI in the paper is 82.5%, whereas current SOTA is at 90+%\nhttps://arxiv.org/pdf/1805.11360v2.pdf\nhttps://arxiv.org/pdf/1901.11504v1.pdf\n\nThe best reported accuracy on MultiNLI in the paper is 70.7%, while there are papers reporting 72.2%, 73.9% and 86.7%.\nhttps://arxiv.org/pdf/1804.07461v3.pdf\nhttps://arxiv.org/pdf/1812.01840v2.pdf\nhttps://arxiv.org/pdf/1901.11504v1.pdf\n\nIt is not necessary to achieve SOTA in every paper, but comparing to old models and claiming top performance is misleading.\n\nSection 5 says learning was done on \"most of the available training data (between 70%-80%). Why not use the whole training data? And why not say exactly how much training data was used?\n\nSection 4.2 says tuning is done on \"50% of X \\in T_s of upper layers\", and it is not clear what is meant by this. That tuning the upper layers was done on 50% of source examples? Why was tuning done on source examples as opposed to target examples? Why only 50%? And why not tune all layers as opposed to only the upper layers?\n\nThe concept of \"negative transfer\" is the main focus of the paper but not really a frequently used term in the field of machine learning, so it should be defined and explained.\n\nThe paper is in serious need of proof-reading, as it contains half-finished sentences, many spelling and grammatical errors, and repeated words.", "rating": "2: Marginally below acceptance threshold", "confidence": "2: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Workshop/LLD/Paper48/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/LLD/Paper48/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning To Avoid Negative Transfer in Few Shot Transfer Learning", "authors": ["James O' Neill"], "authorids": ["james.o-neill@liverpool.ac.uk"], "keywords": ["few shot learning", "negative transfer", "cubic spline", "ensemble learning"], "TL;DR": "A dynamic bagging methods approach to avoiding negatve transfer in neural network few-shot transfer learning", "abstract": "Many tasks in natural language understanding require learning relationships between two sequences for various tasks such as natural language inference, paraphrasing and entailment. These aforementioned tasks are similar in nature, yet they are often modeled individually. Knowledge transfer can be effective for closely related tasks, which is usually carried out using parameter transfer in neural networks. However, transferring all parameters, some of which irrelevant for a target task, can lead to sub-optimal results and can have a negative effect on performance, referred to as \\textit{negative} transfer. \n\nHence, this paper focuses on the transferability of both instances and parameters across natural language understanding tasks by proposing an ensemble-based transfer learning method in the context of few-shot learning.\n\nOur main contribution is a method for mitigating negative transfer across tasks when using neural networks, which involves dynamically bagging small recurrent neural networks trained on different subsets of the source task/s. We present a straightforward yet novel approach for incorporating these networks to a target task for few-shot learning by using a decaying parameter chosen according to the slope changes of a smoothed spline error curve at sub-intervals during training.\n\nOur proposed method show improvements over hard and soft parameter sharing transfer methods in the few-shot learning case and shows competitive performance against models that are trained given full supervision on the target task, from only few examples.", "pdf": "/pdf/4dfe81e059c263505ef5714bce087f7575f60456.pdf", "paperhash": "neill|learning_to_avoid_negative_transfer_in_few_shot_transfer_learning"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/LLD/-/Paper48/Official_Review", "cdate": 1553713413709, "expdate": 1555718400000, "duedate": 1554681600000, "reply": {"forum": "HyeggoCN_4", "replyto": "HyeggoCN_4", "writers": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2019/Workshop/LLD/Paper48/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2019/Workshop/LLD/Paper48/AnonReviewer[0-9]+"}, "readers": {"values": ["everyone"], "description": "The users who will be allowed to read the above content."}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["5: Top 15% of accepted papers, strong accept", "4: Top 50% of accepted papers, clear accept", "3: Marginally above acceptance threshold", "2: Marginally below acceptance threshold", "1: Strong rejection"], "required": true}, "confidence": {"order": 4, "value-radio": ["3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "2: The reviewer is fairly confident that the evaluation is correct", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "tcdate": 1553713413709, "tmdate": 1555511823879, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/LLD"], "invitees": ["ICLR.cc/2019/Workshop/LLD/Paper48/Reviewers"], "signatures": ["ICLR.cc/2019/Workshop/LLD"]}}}, {"id": "Bye9VUkKtV", "original": null, "number": 1, "cdate": 1554736689959, "ddate": null, "tcdate": 1554736689959, "tmdate": 1555510987603, "tddate": null, "forum": "HyeggoCN_4", "replyto": "HyeggoCN_4", "invitation": "ICLR.cc/2019/Workshop/LLD/-/Paper48/Decision", "content": {"title": "Acceptance Decision", "decision": "Reject", "comment": "Desk Reject. The paper severely exceeds the 4 page limit."}, "signatures": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning To Avoid Negative Transfer in Few Shot Transfer Learning", "authors": ["James O' Neill"], "authorids": ["james.o-neill@liverpool.ac.uk"], "keywords": ["few shot learning", "negative transfer", "cubic spline", "ensemble learning"], "TL;DR": "A dynamic bagging methods approach to avoiding negatve transfer in neural network few-shot transfer learning", "abstract": "Many tasks in natural language understanding require learning relationships between two sequences for various tasks such as natural language inference, paraphrasing and entailment. These aforementioned tasks are similar in nature, yet they are often modeled individually. Knowledge transfer can be effective for closely related tasks, which is usually carried out using parameter transfer in neural networks. However, transferring all parameters, some of which irrelevant for a target task, can lead to sub-optimal results and can have a negative effect on performance, referred to as \\textit{negative} transfer. \n\nHence, this paper focuses on the transferability of both instances and parameters across natural language understanding tasks by proposing an ensemble-based transfer learning method in the context of few-shot learning.\n\nOur main contribution is a method for mitigating negative transfer across tasks when using neural networks, which involves dynamically bagging small recurrent neural networks trained on different subsets of the source task/s. We present a straightforward yet novel approach for incorporating these networks to a target task for few-shot learning by using a decaying parameter chosen according to the slope changes of a smoothed spline error curve at sub-intervals during training.\n\nOur proposed method show improvements over hard and soft parameter sharing transfer methods in the few-shot learning case and shows competitive performance against models that are trained given full supervision on the target task, from only few examples.", "pdf": "/pdf/4dfe81e059c263505ef5714bce087f7575f60456.pdf", "paperhash": "neill|learning_to_avoid_negative_transfer_in_few_shot_transfer_learning"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/LLD/-/Paper48/Decision", "cdate": 1554736075168, "reply": {"forum": "HyeggoCN_4", "replyto": "HyeggoCN_4", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-regex": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "description": "How your identity will be displayed."}, "signatures": {"values": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"title": {"order": 1, "required": true, "value": "Acceptance Decision"}, "decision": {"order": 2, "required": true, "value-radio": ["Accept", "Reject"], "description": "Acceptance decision"}, "comment": {"order": 3, "required": false, "value-regex": "[\\S\\s]{0,5000}", "description": ""}}}, "tcdate": 1554736075168, "tmdate": 1555510963686, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/LLD"], "invitees": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "signatures": ["ICLR.cc/2019/Workshop/LLD"]}}}], "count": 4}