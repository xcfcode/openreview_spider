{"notes": [{"id": "ghjxvfgv9ht", "original": "Eb68uod08u1", "number": 156, "cdate": 1601308026141, "ddate": null, "tcdate": 1601308026141, "tmdate": 1614985620327, "tddate": null, "forum": "ghjxvfgv9ht", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Self-Pretraining for Small Datasets by Exploiting Patch Information", "authorids": ["~Zhang_Chunyang1"], "authors": ["Zhang Chunyang"], "keywords": ["Learning with Small Datasets", "Self-Pretraining"], "abstract": "  Deep learning tasks with small datasets are often tackled by pretraining models with large datasets on relevent tasks. Although pretraining methods mitigate the problem of overfitting, it can be difficult to find appropriate pretrained models sometimes. In this paper, we proposed a self-pretraininng method by exploiting patch information in the dataset itself without pretraining on other datasets. Our experiments show that the self-pretraining method leads to better performance than training from scratch both in the condition of not using other data.", "one-sentence_summary": "Pretraining the model using patch information in the small dataset itself", "pdf": "/pdf/fdfb8c320bbdac93302bf63d636ad0a04736b3d4.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chunyang|selfpretraining_for_small_datasets_by_exploiting_patch_information", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=-UT2ecnQ6", "_bibtex": "@misc{\nchunyang2021selfpretraining,\ntitle={Self-Pretraining for Small Datasets by Exploiting Patch Information},\nauthor={Zhang Chunyang},\nyear={2021},\nurl={https://openreview.net/forum?id=ghjxvfgv9ht}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "BKDt9UP9Kqm", "original": null, "number": 1, "cdate": 1610040538968, "ddate": null, "tcdate": 1610040538968, "tmdate": 1610474149270, "tddate": null, "forum": "ghjxvfgv9ht", "replyto": "ghjxvfgv9ht", "invitation": "ICLR.cc/2021/Conference/Paper156/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "All reviewers agreed on the major shortcomings of this submission, the most important of which is that the contributions are insufficiently evaluated. There was no author response. "}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Self-Pretraining for Small Datasets by Exploiting Patch Information", "authorids": ["~Zhang_Chunyang1"], "authors": ["Zhang Chunyang"], "keywords": ["Learning with Small Datasets", "Self-Pretraining"], "abstract": "  Deep learning tasks with small datasets are often tackled by pretraining models with large datasets on relevent tasks. Although pretraining methods mitigate the problem of overfitting, it can be difficult to find appropriate pretrained models sometimes. In this paper, we proposed a self-pretraininng method by exploiting patch information in the dataset itself without pretraining on other datasets. Our experiments show that the self-pretraining method leads to better performance than training from scratch both in the condition of not using other data.", "one-sentence_summary": "Pretraining the model using patch information in the small dataset itself", "pdf": "/pdf/fdfb8c320bbdac93302bf63d636ad0a04736b3d4.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chunyang|selfpretraining_for_small_datasets_by_exploiting_patch_information", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=-UT2ecnQ6", "_bibtex": "@misc{\nchunyang2021selfpretraining,\ntitle={Self-Pretraining for Small Datasets by Exploiting Patch Information},\nauthor={Zhang Chunyang},\nyear={2021},\nurl={https://openreview.net/forum?id=ghjxvfgv9ht}\n}"}, "tags": [], "invitation": {"reply": {"forum": "ghjxvfgv9ht", "replyto": "ghjxvfgv9ht", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040538955, "tmdate": 1610474149254, "id": "ICLR.cc/2021/Conference/Paper156/-/Decision"}}}, {"id": "6sSNKzEw4PW", "original": null, "number": 1, "cdate": 1603600579321, "ddate": null, "tcdate": 1603600579321, "tmdate": 1606459238129, "tddate": null, "forum": "ghjxvfgv9ht", "replyto": "ghjxvfgv9ht", "invitation": "ICLR.cc/2021/Conference/Paper156/-/Official_Review", "content": {"title": "Good idea", "review": "This paper presents an interesting approach for training neural networks with a small dataset. The main idea is to train the model from the early layers to the deeper layers step-by-step, with different types of inputs (i.e., patches, cropped images, or full images) sampled from the given training set. Experimental results show great performance compared to training from scratch.\n\nPros:\n1. This paper presents a good idea for training with small dataset. The proposed method is technical valid, and it is clear that step-wise training can help improve the performance. From my point of view, it is more like applying intermediate supervision on each layer using different types of training inputs, By doing so, we ensure that the early layers and the deeper layers are forced to learn to find the specified low-level and high-level semantics, respectively. Thus, the resultant model could behaves like the large-scale pre-trained deep CNNs we analyzed in the literature.\n2. In experiment section, the authors present the key results on two datasets showing the advantage of the proposed method.\n3. The paper is easy to understand. Also, the writing is very concise. \n\nCons:\n1. Though this paper presents a really focused contribution on training with small dataset, one can see that the paper lacks of in-depth analysis on either the target task or the proposed algorithm. I would suggest that the authors could conduct more experiments to better validate the target task (i.e., training with small dataset). It would be great to add a transfer learning baseline (i.e., pre-trained on ImageNet, and then fine-tuned on the target dataset), and show that it does not work for your research problem. The readers could better understand the difficulty of your research problem.\n2. My another question is more related to the problem definition, or more specifically the importance of the addressed problem. (1) Why we need to deal with small datasets? If the target problem/application is important, it should be easy to enlarge the dataset at scale. For example, there is a scene classification dataset built by MIT in 2009. It is a small-scale dataset which is used in this paper. In 2015, due to the importance of the task, MIT people have scaled the dataset and the new one is called Places, which has 2.5 millions of images with scene-category labels. (2) Why it is small? There are two possible reasons I could think of: (i) it may be difficult and expensive to collect training labels due to extreme labor efforts or privacy concerns (i.e., pixel-wise labels, medical images). (ii) the applications are newly emerged so all data are sparse but surely it will be scaled up in the nearly future. But in the paper, the presented tasks, including scene classification and quality assessment, are well-established and they should not be so difficult to obtain training data. In general, I think the presented experiments are toy examples and the small-scale setting may not be convincing enough. I would suggest that the authors could include more examples and results of small-dataset scenarios, which could add values to the paper.\n\n##################################\nPost-rebuttal:\nThe idea is good, but the experiments and analysis are not enough to validate the proposed idea. The paper is not ready for a publication.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper156/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper156/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Self-Pretraining for Small Datasets by Exploiting Patch Information", "authorids": ["~Zhang_Chunyang1"], "authors": ["Zhang Chunyang"], "keywords": ["Learning with Small Datasets", "Self-Pretraining"], "abstract": "  Deep learning tasks with small datasets are often tackled by pretraining models with large datasets on relevent tasks. Although pretraining methods mitigate the problem of overfitting, it can be difficult to find appropriate pretrained models sometimes. In this paper, we proposed a self-pretraininng method by exploiting patch information in the dataset itself without pretraining on other datasets. Our experiments show that the self-pretraining method leads to better performance than training from scratch both in the condition of not using other data.", "one-sentence_summary": "Pretraining the model using patch information in the small dataset itself", "pdf": "/pdf/fdfb8c320bbdac93302bf63d636ad0a04736b3d4.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chunyang|selfpretraining_for_small_datasets_by_exploiting_patch_information", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=-UT2ecnQ6", "_bibtex": "@misc{\nchunyang2021selfpretraining,\ntitle={Self-Pretraining for Small Datasets by Exploiting Patch Information},\nauthor={Zhang Chunyang},\nyear={2021},\nurl={https://openreview.net/forum?id=ghjxvfgv9ht}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "ghjxvfgv9ht", "replyto": "ghjxvfgv9ht", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper156/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538149199, "tmdate": 1606915810861, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper156/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper156/-/Official_Review"}}}, {"id": "OuwYN4KLpbC", "original": null, "number": 2, "cdate": 1603891271365, "ddate": null, "tcdate": 1603891271365, "tmdate": 1606317557856, "tddate": null, "forum": "ghjxvfgv9ht", "replyto": "ghjxvfgv9ht", "invitation": "ICLR.cc/2021/Conference/Paper156/-/Official_Review", "content": {"title": "It seems to be a draft of the idea rather than a ready paper", "review": "The paper presents a method for improving the accuracy when training CNNs from scratch on a small dataset. \n\nThe method is as following: instead of training the whole network on the full images, one first creates a shallow model and trains it on the small crops from the dataset. The labels for the patches are set the same, as for the whole image. Then one adds more layers, freezes the previously trained and train on bigger patches and so on. \n\nThe method is evaluated on 2 datasets: image quality assesment on LIVE dataset and scene classification on MIT Indoor 67. \n\n\n******\n\nWhile the idea might be good, the paper itself is a low quality.\nProblems:\n\n\n1. Two things are mixed together: (a) progressive patch cropping, starting from small and ending up with big image and (b) progressive adding more and more layers, while freezeing earliest. \n\nThat is not clear and justified to use both together.\n\nThe cropping can be seen is very aggressize data augmentation and if fact is commonly used for training ImageNet CNNs, e.g. see [2]. \n\nThe freezing and training is a variant of Net2Net[1], which is not cited. \n\n\n2. Datasets useBlind image quality assessment is a bad task for testing the idea. Why? Because when image is of a bad quality, it can often easily be told from a small patch. That is why aggressize patch cropping and  keeping label the same is justified. That is not true for other tasks, like classification, object detection, metric learning and so on. Yet the method is claimed to be quite general. \n\n3. There is no baseline on any of the standard dataset, not strong supervised baseline on the datasets paper proposed. By strong I mean, where hyperparameters are reasonably tuned and standard data augmentation is used. \nE.g. for the MIT Indoor67 the paper reports 42.95% accuracy. In the same time, course report from 2015 [3] has 43.8% accuracy. I don't see improvement from the proposed method.\n\n4. The model is not even specified. Is it custom CNN? VGG-style? ResNet-style?\n\n\n***\n\nI would recommend to start from the strong baseline and proper evaluation. If the method can improve on them and then  well presented, then it can be published.\n\n\n[1]. Net2Net: Accelerating Learning via Knowledge Transfer. Chen et.al, ICLR 2016 https://arxiv.org/abs/1511.05641\n\n[2]. https://github.com/NVIDIA/DALI/blob/1e9196702d991d3342ad7a5a7d57c2893abad832/docs/examples/use_cases/pytorch/resnet50/main.py#L116\n\n[3]. http://cs231n.stanford.edu/reports/2015/pdfs/ondieki_final_paper.pdf\n\n***\n\n## After rebuttal update.\n\nGiven there is no rebuttal, there will be no update.", "rating": "2: Strong rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper156/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper156/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Self-Pretraining for Small Datasets by Exploiting Patch Information", "authorids": ["~Zhang_Chunyang1"], "authors": ["Zhang Chunyang"], "keywords": ["Learning with Small Datasets", "Self-Pretraining"], "abstract": "  Deep learning tasks with small datasets are often tackled by pretraining models with large datasets on relevent tasks. Although pretraining methods mitigate the problem of overfitting, it can be difficult to find appropriate pretrained models sometimes. In this paper, we proposed a self-pretraininng method by exploiting patch information in the dataset itself without pretraining on other datasets. Our experiments show that the self-pretraining method leads to better performance than training from scratch both in the condition of not using other data.", "one-sentence_summary": "Pretraining the model using patch information in the small dataset itself", "pdf": "/pdf/fdfb8c320bbdac93302bf63d636ad0a04736b3d4.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chunyang|selfpretraining_for_small_datasets_by_exploiting_patch_information", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=-UT2ecnQ6", "_bibtex": "@misc{\nchunyang2021selfpretraining,\ntitle={Self-Pretraining for Small Datasets by Exploiting Patch Information},\nauthor={Zhang Chunyang},\nyear={2021},\nurl={https://openreview.net/forum?id=ghjxvfgv9ht}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "ghjxvfgv9ht", "replyto": "ghjxvfgv9ht", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper156/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538149199, "tmdate": 1606915810861, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper156/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper156/-/Official_Review"}}}, {"id": "KlPGIKyYp-w", "original": null, "number": 3, "cdate": 1604038396623, "ddate": null, "tcdate": 1604038396623, "tmdate": 1605024751740, "tddate": null, "forum": "ghjxvfgv9ht", "replyto": "ghjxvfgv9ht", "invitation": "ICLR.cc/2021/Conference/Paper156/-/Official_Review", "content": {"title": "Interesting problem, simple idea, poor experimental evaluation", "review": "The authors propose an interesting problem where only a small sized labeled data set is available for supervised learning. This approach is different from the existing deep learning benchmarks which either assume the availability of pre-trained supervised imageNet classification model or an unsupervised model trained using large scale auxiliary data (e.g. MoCo, Swav). The proposed method divides training of different stages of the convNet architecture using different sized crops/patches of images. The lower layers are trained to classify the smallest size crops. These layers are then frozen to train higher level layers with larger patches as inputs. The network learns to classify input patches into the classes of images they belong to. Evaluation is performed on image quality assessment task on Live dataset and indoor scene classification on MIT Indoor Scenes dataset.\n\nThe problem is of practical interest since a large amount of labeled/unlabeled data is not always available for the domain of interest (e.g. depth data etc). However, I am not entirely sure that methods using small scale data alone can ever achieve performances close to the methods that leverage pre-trained supervised/unsupervised prior models. Even in cases where large scale data is unavailable, small scale paired data (e.g. image + depth ) can be used to regularize the training of one domain using a pre-trained model of the other domain [e.g. Gupta et. al. CVPR 2016].\nThe experimental evaluation in this paper is woefully inadequate. It is clear that the method is more successful compared to learning from scratch, but I would have liked to see how much worse it is compared to fine-tuning of pre-trained models. If the gap is too large, is it even a direction worth exploring? Is the proposed method, the first attempt at training neural nets without priors? If not, how does it compare to others? Evaluation on more serious datasets such as SUN 397 would have been more appealing. At present it seems like a simple experiment on a couple small datasets. ", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper156/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper156/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Self-Pretraining for Small Datasets by Exploiting Patch Information", "authorids": ["~Zhang_Chunyang1"], "authors": ["Zhang Chunyang"], "keywords": ["Learning with Small Datasets", "Self-Pretraining"], "abstract": "  Deep learning tasks with small datasets are often tackled by pretraining models with large datasets on relevent tasks. Although pretraining methods mitigate the problem of overfitting, it can be difficult to find appropriate pretrained models sometimes. In this paper, we proposed a self-pretraininng method by exploiting patch information in the dataset itself without pretraining on other datasets. Our experiments show that the self-pretraining method leads to better performance than training from scratch both in the condition of not using other data.", "one-sentence_summary": "Pretraining the model using patch information in the small dataset itself", "pdf": "/pdf/fdfb8c320bbdac93302bf63d636ad0a04736b3d4.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chunyang|selfpretraining_for_small_datasets_by_exploiting_patch_information", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=-UT2ecnQ6", "_bibtex": "@misc{\nchunyang2021selfpretraining,\ntitle={Self-Pretraining for Small Datasets by Exploiting Patch Information},\nauthor={Zhang Chunyang},\nyear={2021},\nurl={https://openreview.net/forum?id=ghjxvfgv9ht}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "ghjxvfgv9ht", "replyto": "ghjxvfgv9ht", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper156/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538149199, "tmdate": 1606915810861, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper156/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper156/-/Official_Review"}}}], "count": 5}