{"notes": [{"id": "Skeh-xBYDH", "original": "r1lc-ZetvH", "number": 2150, "cdate": 1569439747655, "ddate": null, "tcdate": 1569439747655, "tmdate": 1577168294879, "tddate": null, "forum": "Skeh-xBYDH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "On Symmetry and Initialization for Neural Networks", "authors": ["Ido Nachum", "Amir Yehudayoff"], "authorids": ["ido0808@gmail.com", "amir.yehudayoff@gmail.com"], "keywords": ["Neural Network Theory", "Symmetry"], "TL;DR": "When initialized properly, neural networks can learn the simple class of symmetric functions; when initialized randomly, they fail.  ", "abstract": "This work provides an additional step in the theoretical understanding of neural networks. We consider neural networks with one hidden layer and show that when learning symmetric functions, one can choose initial conditions so that standard SGD training efficiently produces generalization guarantees. We empirically verify this and show that this does not hold when the initial conditions are chosen at random. The proof of convergence investigates the interaction between the two layers of the network. Our results highlight the importance of using symmetry in the design of neural networks.", "pdf": "/pdf/aa42c4a7138b364449b57f898bc18731cab6cb3d.pdf", "paperhash": "nachum|on_symmetry_and_initialization_for_neural_networks", "original_pdf": "/attachment/c6c36b6aee3ae1db19c09b3af85a8b6e85eb002e.pdf", "_bibtex": "@misc{\nnachum2020on,\ntitle={On Symmetry and Initialization for Neural Networks},\nauthor={Ido Nachum and Amir Yehudayoff},\nyear={2020},\nurl={https://openreview.net/forum?id=Skeh-xBYDH}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "CjzLx08ckf", "original": null, "number": 1, "cdate": 1576798741828, "ddate": null, "tcdate": 1576798741828, "tmdate": 1576800894419, "tddate": null, "forum": "Skeh-xBYDH", "replyto": "Skeh-xBYDH", "invitation": "ICLR.cc/2020/Conference/Paper2150/-/Decision", "content": {"decision": "Reject", "comment": "The two main concerns raised by reviewers is that whether the results are significant, and a potential issue in the proof. While the rebuttal clarified some steps in the proof, the main concerns about the significance remain. The authors are encouraged to make this significance more clear.\n\nNote that one reviewer argued theoretical papers are not suitable for ICLR. This is false, as a theoretical understanding of neural networks remains a key research area that is of wide interest to the community. Consequently, this review was not considered in the final evaluation.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Symmetry and Initialization for Neural Networks", "authors": ["Ido Nachum", "Amir Yehudayoff"], "authorids": ["ido0808@gmail.com", "amir.yehudayoff@gmail.com"], "keywords": ["Neural Network Theory", "Symmetry"], "TL;DR": "When initialized properly, neural networks can learn the simple class of symmetric functions; when initialized randomly, they fail.  ", "abstract": "This work provides an additional step in the theoretical understanding of neural networks. We consider neural networks with one hidden layer and show that when learning symmetric functions, one can choose initial conditions so that standard SGD training efficiently produces generalization guarantees. We empirically verify this and show that this does not hold when the initial conditions are chosen at random. The proof of convergence investigates the interaction between the two layers of the network. Our results highlight the importance of using symmetry in the design of neural networks.", "pdf": "/pdf/aa42c4a7138b364449b57f898bc18731cab6cb3d.pdf", "paperhash": "nachum|on_symmetry_and_initialization_for_neural_networks", "original_pdf": "/attachment/c6c36b6aee3ae1db19c09b3af85a8b6e85eb002e.pdf", "_bibtex": "@misc{\nnachum2020on,\ntitle={On Symmetry and Initialization for Neural Networks},\nauthor={Ido Nachum and Amir Yehudayoff},\nyear={2020},\nurl={https://openreview.net/forum?id=Skeh-xBYDH}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "Skeh-xBYDH", "replyto": "Skeh-xBYDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795708815, "tmdate": 1576800257346, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2150/-/Decision"}}}, {"id": "rklvJlN3jB", "original": null, "number": 8, "cdate": 1573826526568, "ddate": null, "tcdate": 1573826526568, "tmdate": 1573826526568, "tddate": null, "forum": "Skeh-xBYDH", "replyto": "Skeh-xBYDH", "invitation": "ICLR.cc/2020/Conference/Paper2150/-/Official_Comment", "content": {"title": "A final comment to all reviewers", "comment": "We uploaded a revised version of the paper.\n1. We modified the representations of the symmetric functions to the simpler representations suggested by Reviewer#2.\n2. We modified some phrasing in the proof of Theorem 1, to make it clearer.\n\nRecently, we came to know this work: https://arxiv.org/abs/1910.06956 and some several works that followed it.\nIt studies the behavior of neural networks when dealing with an infinite width network while only considering the training with continuous GD and its empirical error. In essence, these works study training of neural networks where the embedding of the original space by the network does not change with time much. These works attracted considerable attention. So it seems we don't have enough understanding of the dynamics in these regimes yet.\n\nOur work is an example of the above phenomenon and studied in full detail: finite network, discrete SGD on all layers, polytime, and generalization guarantees.  Additionally, Lemma 4 gives the tools to study the training process of a finite width neural network performing discrete SGD (not only the special initialization we suggested). Also, Lemma 4 can enable to derive some generalization guarantees. \n\n\n\n \n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2150/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2150/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Symmetry and Initialization for Neural Networks", "authors": ["Ido Nachum", "Amir Yehudayoff"], "authorids": ["ido0808@gmail.com", "amir.yehudayoff@gmail.com"], "keywords": ["Neural Network Theory", "Symmetry"], "TL;DR": "When initialized properly, neural networks can learn the simple class of symmetric functions; when initialized randomly, they fail.  ", "abstract": "This work provides an additional step in the theoretical understanding of neural networks. We consider neural networks with one hidden layer and show that when learning symmetric functions, one can choose initial conditions so that standard SGD training efficiently produces generalization guarantees. We empirically verify this and show that this does not hold when the initial conditions are chosen at random. The proof of convergence investigates the interaction between the two layers of the network. Our results highlight the importance of using symmetry in the design of neural networks.", "pdf": "/pdf/aa42c4a7138b364449b57f898bc18731cab6cb3d.pdf", "paperhash": "nachum|on_symmetry_and_initialization_for_neural_networks", "original_pdf": "/attachment/c6c36b6aee3ae1db19c09b3af85a8b6e85eb002e.pdf", "_bibtex": "@misc{\nnachum2020on,\ntitle={On Symmetry and Initialization for Neural Networks},\nauthor={Ido Nachum and Amir Yehudayoff},\nyear={2020},\nurl={https://openreview.net/forum?id=Skeh-xBYDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Skeh-xBYDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2150/Authors", "ICLR.cc/2020/Conference/Paper2150/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2150/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2150/Reviewers", "ICLR.cc/2020/Conference/Paper2150/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2150/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2150/Authors|ICLR.cc/2020/Conference/Paper2150/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504145605, "tmdate": 1576860529168, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2150/Authors", "ICLR.cc/2020/Conference/Paper2150/Reviewers", "ICLR.cc/2020/Conference/Paper2150/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2150/-/Official_Comment"}}}, {"id": "H1eCE4LosB", "original": null, "number": 7, "cdate": 1573770294112, "ddate": null, "tcdate": 1573770294112, "tmdate": 1573770294112, "tddate": null, "forum": "Skeh-xBYDH", "replyto": "BkewxGfGsH", "invitation": "ICLR.cc/2020/Conference/Paper2150/-/Official_Comment", "content": {"title": "Response", "comment": "Thanks for addressing my comments. I did have the same issue as the other reviewer. Would be useful to make it explicit in the proof. As for the overall class of functions the paper is trying to learn, I'm still not convinced it gives us any new insight on how neural network training works since post initialization it is equivalent to learning a linear classifier. It would help the paper to extend the techniques developed to other more complex classes."}, "signatures": ["ICLR.cc/2020/Conference/Paper2150/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2150/AnonReviewer2", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Symmetry and Initialization for Neural Networks", "authors": ["Ido Nachum", "Amir Yehudayoff"], "authorids": ["ido0808@gmail.com", "amir.yehudayoff@gmail.com"], "keywords": ["Neural Network Theory", "Symmetry"], "TL;DR": "When initialized properly, neural networks can learn the simple class of symmetric functions; when initialized randomly, they fail.  ", "abstract": "This work provides an additional step in the theoretical understanding of neural networks. We consider neural networks with one hidden layer and show that when learning symmetric functions, one can choose initial conditions so that standard SGD training efficiently produces generalization guarantees. We empirically verify this and show that this does not hold when the initial conditions are chosen at random. The proof of convergence investigates the interaction between the two layers of the network. Our results highlight the importance of using symmetry in the design of neural networks.", "pdf": "/pdf/aa42c4a7138b364449b57f898bc18731cab6cb3d.pdf", "paperhash": "nachum|on_symmetry_and_initialization_for_neural_networks", "original_pdf": "/attachment/c6c36b6aee3ae1db19c09b3af85a8b6e85eb002e.pdf", "_bibtex": "@misc{\nnachum2020on,\ntitle={On Symmetry and Initialization for Neural Networks},\nauthor={Ido Nachum and Amir Yehudayoff},\nyear={2020},\nurl={https://openreview.net/forum?id=Skeh-xBYDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Skeh-xBYDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2150/Authors", "ICLR.cc/2020/Conference/Paper2150/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2150/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2150/Reviewers", "ICLR.cc/2020/Conference/Paper2150/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2150/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2150/Authors|ICLR.cc/2020/Conference/Paper2150/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504145605, "tmdate": 1576860529168, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2150/Authors", "ICLR.cc/2020/Conference/Paper2150/Reviewers", "ICLR.cc/2020/Conference/Paper2150/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2150/-/Official_Comment"}}}, {"id": "BJgezvzMsH", "original": null, "number": 6, "cdate": 1573164807805, "ddate": null, "tcdate": 1573164807805, "tmdate": 1573164807805, "tddate": null, "forum": "Skeh-xBYDH", "replyto": "rJxskC-Mor", "invitation": "ICLR.cc/2020/Conference/Paper2150/-/Official_Comment", "content": {"title": "Response to your response", "comment": "Sorry guys! After reading the comments from other reviewers, I realised that I totally missed your points. I will read your paper again and resubmit my review. "}, "signatures": ["ICLR.cc/2020/Conference/Paper2150/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2150/AnonReviewer1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Symmetry and Initialization for Neural Networks", "authors": ["Ido Nachum", "Amir Yehudayoff"], "authorids": ["ido0808@gmail.com", "amir.yehudayoff@gmail.com"], "keywords": ["Neural Network Theory", "Symmetry"], "TL;DR": "When initialized properly, neural networks can learn the simple class of symmetric functions; when initialized randomly, they fail.  ", "abstract": "This work provides an additional step in the theoretical understanding of neural networks. We consider neural networks with one hidden layer and show that when learning symmetric functions, one can choose initial conditions so that standard SGD training efficiently produces generalization guarantees. We empirically verify this and show that this does not hold when the initial conditions are chosen at random. The proof of convergence investigates the interaction between the two layers of the network. Our results highlight the importance of using symmetry in the design of neural networks.", "pdf": "/pdf/aa42c4a7138b364449b57f898bc18731cab6cb3d.pdf", "paperhash": "nachum|on_symmetry_and_initialization_for_neural_networks", "original_pdf": "/attachment/c6c36b6aee3ae1db19c09b3af85a8b6e85eb002e.pdf", "_bibtex": "@misc{\nnachum2020on,\ntitle={On Symmetry and Initialization for Neural Networks},\nauthor={Ido Nachum and Amir Yehudayoff},\nyear={2020},\nurl={https://openreview.net/forum?id=Skeh-xBYDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Skeh-xBYDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2150/Authors", "ICLR.cc/2020/Conference/Paper2150/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2150/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2150/Reviewers", "ICLR.cc/2020/Conference/Paper2150/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2150/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2150/Authors|ICLR.cc/2020/Conference/Paper2150/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504145605, "tmdate": 1576860529168, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2150/Authors", "ICLR.cc/2020/Conference/Paper2150/Reviewers", "ICLR.cc/2020/Conference/Paper2150/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2150/-/Official_Comment"}}}, {"id": "BkewxGfGsH", "original": null, "number": 3, "cdate": 1573163503308, "ddate": null, "tcdate": 1573163503308, "tmdate": 1573163557379, "tddate": null, "forum": "Skeh-xBYDH", "replyto": "H1gUil1TYS", "invitation": "ICLR.cc/2020/Conference/Paper2150/-/Official_Comment", "content": {"title": "Response to Review #2", "comment": "Thank you for your time reading our work and for your feedback. Below, we try to clarify your concerns and we\u2019ll be happy to hear your perspective again after you read our response.\n\nYour main issue is similar to Review #3, so please also refer to our comments there.\n\n\u201cIt is unclear why this setup warrants the use of a neural network for training.\u201d\n\nIt does not warrant it. The class is easily learnable with no special algorithm. Once you receive an input of a given weight you immediately know the label of all vectors with the same weight. It is just a matter of collecting enough vectors with different weights.\n\nWe study this problem to have a fine-grained analysis of neural networks for a specific class of functions. Such analysis does not appear in the literature, as far as we know. Notice that this theoretical result demonstrates that although the neural network is over parametrized, by using SGD it still generalizes well.\n\nFrom an empirical and theoretical perspective, it is surprising that such a simple class of functions is not learnable by standard neural networks.\n\n\u201cThe class of symmetric boolean functions can be modeled as a univariate function by mapping x -> |x| which is easy to solve in the no noise setting analyzed by the paper.\u201d \n\nPotentially, this mapping will not work in the noisy case. We show empirically that our initialization works in two different noisy cases.\n\n\u201cWriting - Proofs are mostly clear however it would help to add more details in the proof of the main theorem (especially to argue about the use of the Perceptron convergence theorem for the changing representations). \u201c\n\nAre you maybe referring to the same issue about Lemma 4 Review#3 had? Maybe our response can clarify things. Indeed, Lemma 4 has a bit of a tricky proof. \nIf not, can you specifically refer us to arguments/sentences that were not clear?\n\n \u201cRegarding the representation for indicators using ReLUs, one could use a simpler and more standard representation\u2026\u201d\n\nThis is indeed a simpler representation and we can use it. We didn\u2019t use it because we simply were not aware of it. We just used the representation that we naturally found ourselves. Thank you for introducing it to us.\n\n\u201cThe number of epochs seem to be varying in the experiments, please make that consistent.\u201d\n\nThe focus of this work is the number of SGD updates. So if the number of samples is different for every case (n, n^2, n^3, n^4), it is only natural to have fewer epochs for larger datasets.\n\n\u201cLastly, the important plots need to be moved to the main paper.\u201d\n\nWe agree. They were put there because of the page limitation.\n\n\u201cAre the experiments for multiple runs or just a single run?\u201d\n\nA single run, that represents all other runs we experimented with while working in this setting. We observed the same behavior on all of our runs. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2150/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2150/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Symmetry and Initialization for Neural Networks", "authors": ["Ido Nachum", "Amir Yehudayoff"], "authorids": ["ido0808@gmail.com", "amir.yehudayoff@gmail.com"], "keywords": ["Neural Network Theory", "Symmetry"], "TL;DR": "When initialized properly, neural networks can learn the simple class of symmetric functions; when initialized randomly, they fail.  ", "abstract": "This work provides an additional step in the theoretical understanding of neural networks. We consider neural networks with one hidden layer and show that when learning symmetric functions, one can choose initial conditions so that standard SGD training efficiently produces generalization guarantees. We empirically verify this and show that this does not hold when the initial conditions are chosen at random. The proof of convergence investigates the interaction between the two layers of the network. Our results highlight the importance of using symmetry in the design of neural networks.", "pdf": "/pdf/aa42c4a7138b364449b57f898bc18731cab6cb3d.pdf", "paperhash": "nachum|on_symmetry_and_initialization_for_neural_networks", "original_pdf": "/attachment/c6c36b6aee3ae1db19c09b3af85a8b6e85eb002e.pdf", "_bibtex": "@misc{\nnachum2020on,\ntitle={On Symmetry and Initialization for Neural Networks},\nauthor={Ido Nachum and Amir Yehudayoff},\nyear={2020},\nurl={https://openreview.net/forum?id=Skeh-xBYDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Skeh-xBYDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2150/Authors", "ICLR.cc/2020/Conference/Paper2150/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2150/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2150/Reviewers", "ICLR.cc/2020/Conference/Paper2150/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2150/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2150/Authors|ICLR.cc/2020/Conference/Paper2150/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504145605, "tmdate": 1576860529168, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2150/Authors", "ICLR.cc/2020/Conference/Paper2150/Reviewers", "ICLR.cc/2020/Conference/Paper2150/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2150/-/Official_Comment"}}}, {"id": "rJxskC-Mor", "original": null, "number": 2, "cdate": 1573162467166, "ddate": null, "tcdate": 1573162467166, "tmdate": 1573162486048, "tddate": null, "forum": "Skeh-xBYDH", "replyto": "Byld6xYy9H", "invitation": "ICLR.cc/2020/Conference/Paper2150/-/Official_Comment", "content": {"title": "Response to Review #1", "comment": " \u201cIn my opinion, this paper doesn't fit to the main interests of ICLR. Therefore, I didn't spend too much time reviewing this paper and suggest for rejection.\u201d\n\nWe would be happy if you can elaborate on why a theoretical result on neural networks is not relevant.\nIn the past, it seems that ICLR accepted various theoretical results on neural networks.\nCan you suggest another venue that fits this paper?\n\n\u201cIt could be potentially very interesting if the authors provide also results with other initialisation methods.\u201d\n\nWe\u2019ll be happy to hear about other initialization methods you are interested in. Care to elaborate? \nWe have used the standard initialization that is mostly used and studied theoretically.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2150/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2150/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Symmetry and Initialization for Neural Networks", "authors": ["Ido Nachum", "Amir Yehudayoff"], "authorids": ["ido0808@gmail.com", "amir.yehudayoff@gmail.com"], "keywords": ["Neural Network Theory", "Symmetry"], "TL;DR": "When initialized properly, neural networks can learn the simple class of symmetric functions; when initialized randomly, they fail.  ", "abstract": "This work provides an additional step in the theoretical understanding of neural networks. We consider neural networks with one hidden layer and show that when learning symmetric functions, one can choose initial conditions so that standard SGD training efficiently produces generalization guarantees. We empirically verify this and show that this does not hold when the initial conditions are chosen at random. The proof of convergence investigates the interaction between the two layers of the network. Our results highlight the importance of using symmetry in the design of neural networks.", "pdf": "/pdf/aa42c4a7138b364449b57f898bc18731cab6cb3d.pdf", "paperhash": "nachum|on_symmetry_and_initialization_for_neural_networks", "original_pdf": "/attachment/c6c36b6aee3ae1db19c09b3af85a8b6e85eb002e.pdf", "_bibtex": "@misc{\nnachum2020on,\ntitle={On Symmetry and Initialization for Neural Networks},\nauthor={Ido Nachum and Amir Yehudayoff},\nyear={2020},\nurl={https://openreview.net/forum?id=Skeh-xBYDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Skeh-xBYDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2150/Authors", "ICLR.cc/2020/Conference/Paper2150/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2150/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2150/Reviewers", "ICLR.cc/2020/Conference/Paper2150/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2150/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2150/Authors|ICLR.cc/2020/Conference/Paper2150/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504145605, "tmdate": 1576860529168, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2150/Authors", "ICLR.cc/2020/Conference/Paper2150/Reviewers", "ICLR.cc/2020/Conference/Paper2150/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2150/-/Official_Comment"}}}, {"id": "HklNwhbMsB", "original": null, "number": 1, "cdate": 1573162075696, "ddate": null, "tcdate": 1573162075696, "tmdate": 1573162075696, "tddate": null, "forum": "Skeh-xBYDH", "replyto": "Bkg_Pp2rcB", "invitation": "ICLR.cc/2020/Conference/Paper2150/-/Official_Comment", "content": {"title": "Response to Review #3", "comment": "Thank you for your time reading our work and for your feedback. Below, we try to clarify your concerns and we\u2019ll be happy to hear your perspective again after you read our response.\n\nRegarding the results\u2019 significance, we are not aware of any previous result that proves that a class of functions is PAC learnable using neural networks and SGD and considers all real-life elements of the training and generalization. For example, training all layers simultaneously, working with a fixed dataset, considering generalization and not just the training error, etc. We state the differences between our work and others\u2019 in the Related Work section. \n\nAs a fine-grained analysis of neural networks (such as appears in the paper) should start somewhere, working with the simple class of symmetric functions is maybe a good start. It wasn\u2019t the point to learn symmetric functions but to understand better the dynamics of training neural networks. Our analysis of SGD shows that it has an optimal sample complexity (in terms of the VC-dimension of the class of symmetric functions) and that SGD is also efficient in time and memory. \n\nAlso, Theorem 1 demonstrates that although the network has much more parameters \\Omega(n^2) than samples O(n), it is still able to generalize well (even when all weights are allowed to change). A phenomenon that happens in practice and not well explained.\n\nWe also find the empirical evidence surprising that a neural network cannot learn a random symmetric function from a random initialization, although these functions have a lot of structure and are easy to learn. This suggests another line of research; why these simple functions are hard to learn? \n\nFinally, Lemma 4 holds in general. It is not specific for symmetric functions and our initialization. It can be of theoretical interest independently of the paper.\n\nFrom a practical perspective:\n1.\tEmpirically, we show that our initialization is robust to different types of noise. This suggests that our initialization can maybe help in cases where the function we learn is \u201chighly symmetric\u201d but not entirely.\n2.\tLemma 4 suggests that if during training the neural network found a good embedding of the original space and the weights of the output neuron are not too large, then running SGD will converge to a network with a small empirical error that also generalizes well. This suggests a practical idea to regularize only the weights of the output neuron. Doing this will make sure the network will not \u201ctake a pass\u201d on a good embedding of the original space.\n\nWe didn\u2019t add these points, as it sidetracks the theoretical analysis and the purpose of the paper.\n\nRegarding the other comments:\n\n\u201cI could not follow one step in the proof of Lemma 4 (used to show that SGD does not move the weights too far from the initialization). Why does Theorem 2 imply that the number of updates is at most 20R^2/gamma^2? In Theorem 2, is fixed whereas in Lemma 4 it varies with. To me this seems important, since without a bound on the number of steps it is unclear how you can control how far the embeddings move. \u201c\n\nThis is indeed important and is taken into consideration in the paper. Let us clarify: We first prove that if at most $20R^2/gamma^2$ updates were made, then the norm of all $v$ is at most $2R$ while keeping a margin of 0.9\\gamma. Now, we can use Theorem 2, which bounds the number of possible updates only in terms of the margin (0.9\\gamma) and maximal norm (2R) (independent of the size of the dataset).\n\n\u201cIn the statement of Lemma 4, linear separability of V should be with respect to some fixed partition Y?\u201d\n\nYes. X is partitioned and this induces a partition of V. We assume that at the beginning, V is linearly separable and under the right conditions, it stays this way (although V changes).\n\n\u201cFirst, I think it would be helpful to the reader if the authors could make this intuition more explicit. In the submission the authors do not give much explanation for the choice of initialization. \u201c\n\nYes, the purpose of the initialization is to embed the original space of binary vectors in such a way that every symmetric function linearly partitions the embedded space.\n\n\u201cIn Figure 5, why is empirical error not decreasing over epochs? \u201c\n\nThe empirical error does not decrease because we have more samples (n^4) than parameters (O(n^2)). The optimization problem is now hard. The bigger the sample, the harder it is for the network to fit the data. You can observe this gradual increase in hardness the bigger the sample you take.\n\n\"I think the figures referenced in the text should be in the paper, not the appendix.\"\n\nWe agree. They were put there because of the page restriction.\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2150/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2150/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Symmetry and Initialization for Neural Networks", "authors": ["Ido Nachum", "Amir Yehudayoff"], "authorids": ["ido0808@gmail.com", "amir.yehudayoff@gmail.com"], "keywords": ["Neural Network Theory", "Symmetry"], "TL;DR": "When initialized properly, neural networks can learn the simple class of symmetric functions; when initialized randomly, they fail.  ", "abstract": "This work provides an additional step in the theoretical understanding of neural networks. We consider neural networks with one hidden layer and show that when learning symmetric functions, one can choose initial conditions so that standard SGD training efficiently produces generalization guarantees. We empirically verify this and show that this does not hold when the initial conditions are chosen at random. The proof of convergence investigates the interaction between the two layers of the network. Our results highlight the importance of using symmetry in the design of neural networks.", "pdf": "/pdf/aa42c4a7138b364449b57f898bc18731cab6cb3d.pdf", "paperhash": "nachum|on_symmetry_and_initialization_for_neural_networks", "original_pdf": "/attachment/c6c36b6aee3ae1db19c09b3af85a8b6e85eb002e.pdf", "_bibtex": "@misc{\nnachum2020on,\ntitle={On Symmetry and Initialization for Neural Networks},\nauthor={Ido Nachum and Amir Yehudayoff},\nyear={2020},\nurl={https://openreview.net/forum?id=Skeh-xBYDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Skeh-xBYDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2150/Authors", "ICLR.cc/2020/Conference/Paper2150/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2150/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2150/Reviewers", "ICLR.cc/2020/Conference/Paper2150/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2150/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2150/Authors|ICLR.cc/2020/Conference/Paper2150/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504145605, "tmdate": 1576860529168, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2150/Authors", "ICLR.cc/2020/Conference/Paper2150/Reviewers", "ICLR.cc/2020/Conference/Paper2150/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2150/-/Official_Comment"}}}, {"id": "H1gUil1TYS", "original": null, "number": 1, "cdate": 1571774622049, "ddate": null, "tcdate": 1571774622049, "tmdate": 1572972376631, "tddate": null, "forum": "Skeh-xBYDH", "replyto": "Skeh-xBYDH", "invitation": "ICLR.cc/2020/Conference/Paper2150/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper studies the problem of learning the class of symmetric boolean function, that is, functions that depend only on |x| = \\sum_i x_i. The paper shows that with proper initialization, one-hidden layer over-parametrized networks can learn this class of functions. The main observation that the authors make is that the last layer weights are updated as in the Perceptron algorithm and as long as the first layer has learned a large-margin representation, the first-layer weights do not change much. The authors experimentally validate their theory and additionally show that random initialization fails to converge to a low test-error solution while their special initialization works.\n\nOverall, the main complexity arises from handling training of both layers and this is cleverly analyzed. However I am leaning towards rejection as the underlying problem does not seem well-motivated. The class of symmetric boolean functions can be modeled as a univariate function by mapping x -> |x| which is easy to solve in the no noise setting analyzed by the paper. Also, in terms of learning with neural networks, as the authors point out, one can learn this class by training only the last layer. It is unclear why this setup warrants the use of a neural network for training. The problem would be more challenging and interesting for the class of symmetric (permutation invariant) functions on the real domain where using symmetry in the architecture/initialization can potentially give gains.\n\nWriting - Proofs are mostly clear however it would help to add more details in the proof of the main theorem (especially to argue about the use of the Perceptron convergence theorem for the changing representations). Also, the introduction needs to further motivate the setup and its relevance to neural networks.\n\nRepresentation - Regarding the representation for indicators using ReLUs, one could use a simpler and more standard representation. In prior work the indicator is represented using a difference of ReLUs, 1[|x| >= i] = ReLU(|x| - i + 1) - ReLU(|x| - i) and 1[|x| = i] = 1[|x| >= i] - 1[|x| >= i+1] = ReLU(|x| - i + 1) - 2 ReLU(|x| - i) + ReLU(|x| - i - 1). Now one can express \\sum_{i \\in A} 1[|x| = i] by summing up these indicators and adding a bias term of -0.5 will make the sign be the correct value. Note that this would overall require only n + 2 hidden units with the weights now being bounded by constants. This would still have a margin of \\Omega(1/n). Is there a particular reason for the choice of representation in the paper?\n\nExperiments - The plots are hard to parse and inconsistent. Firstly, it would be better to use line plots instead of scatter plots to highlight the trend. Secondly, the x-axis needs to be sampled more frequently. The number of epochs seem to be varying in the experiments, please make that consistent. Lastly, the important plots need to be moved to the main paper. Are the experiments for multiple runs or just a single run?"}, "signatures": ["ICLR.cc/2020/Conference/Paper2150/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2150/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Symmetry and Initialization for Neural Networks", "authors": ["Ido Nachum", "Amir Yehudayoff"], "authorids": ["ido0808@gmail.com", "amir.yehudayoff@gmail.com"], "keywords": ["Neural Network Theory", "Symmetry"], "TL;DR": "When initialized properly, neural networks can learn the simple class of symmetric functions; when initialized randomly, they fail.  ", "abstract": "This work provides an additional step in the theoretical understanding of neural networks. We consider neural networks with one hidden layer and show that when learning symmetric functions, one can choose initial conditions so that standard SGD training efficiently produces generalization guarantees. We empirically verify this and show that this does not hold when the initial conditions are chosen at random. The proof of convergence investigates the interaction between the two layers of the network. Our results highlight the importance of using symmetry in the design of neural networks.", "pdf": "/pdf/aa42c4a7138b364449b57f898bc18731cab6cb3d.pdf", "paperhash": "nachum|on_symmetry_and_initialization_for_neural_networks", "original_pdf": "/attachment/c6c36b6aee3ae1db19c09b3af85a8b6e85eb002e.pdf", "_bibtex": "@misc{\nnachum2020on,\ntitle={On Symmetry and Initialization for Neural Networks},\nauthor={Ido Nachum and Amir Yehudayoff},\nyear={2020},\nurl={https://openreview.net/forum?id=Skeh-xBYDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Skeh-xBYDH", "replyto": "Skeh-xBYDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2150/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2150/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575909666858, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2150/Reviewers"], "noninvitees": [], "tcdate": 1570237726991, "tmdate": 1575909666874, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2150/-/Official_Review"}}}, {"id": "Bkg_Pp2rcB", "original": null, "number": 3, "cdate": 1572355424287, "ddate": null, "tcdate": 1572355424287, "tmdate": 1572972376536, "tddate": null, "forum": "Skeh-xBYDH", "replyto": "Skeh-xBYDH", "invitation": "ICLR.cc/2020/Conference/Paper2150/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "PAPER SUMMARY: This paper studies the problem of training a single hidden layer neural network to represent an arbitrary symmetric function. These are functions $f : \\{0,1\\}^n \\to \\{-1, 1\\}$ which are invariant to permutations in the input coordinates. The authors' main result (Theorem 1) shows that if you take a single hidden layer network with $O(n)$ hidden units and initialize the weights in a particular way, then for any symmetric $f$, SGD training will converge to an empirical risk minimizer with guaranteed small generalization error. On the other hand, the authors' experiments suggest that arbitrary symmetric functions are not learnable from random initialization. Taken together, these results point to the importance of designing network architectures/ initializations that respect the structure in the function class you're trying to represent.\n\nREVIEW SUMMARY: I lean towards rejecting this paper however, because I am not convinced of the results' significance. We already know how to learn symmetric functions (see Exercise 3.26 in Mohri et al., 2018). The authors' results show that we can inject this knowledge into a neural network at initialization, and then run SGD without making things too much worse. I do not see how these ideas might apply to more substantial learning problems where our prior knowledge is less precise. Moreover, while the proofs are clearly presented overall, I have one concern with a key step in Lemma 4.\n\nMAJOR COMMENTS:\n\n1) The key property of symmetric functions is that their output depends only on $|x|$. Thus, one can first extract \"cardinality features\" $x \\mapsto |x|$, after which learnability follows by standard generalization theory results (as the authors note in the proof of Theorem 1).\n\nThe basic idea of Theorem 1 then seems to be to realize this feature map as the hidden layer of a single hidden layer ReLU network (this is essentially what the initialization does) and then show that running SGD will not move the weights too far from the initialization (Lemma 4).\n\n(a) First, I think it would be helpful to the reader if the authors could make this intuition more explicit. In the submission the authors do not give much explanation for the choice of initialization.\n\n(b) Second, because this is a learning problem we already know how to solve, the results seems a little contrived. I do not see how these ideas could extend to more challenging cases where our prior knowledge of symmetry (e.g. translation invariance) does not by itself lead to an algorithm with efficient learnability guarantees.\n\n2) I could not follow one step in the proof of Lemma 4 (used to show that SGD does not move the weights too far from the initialization). Why does Theorem 2 imply that the number of updates is at most $20 R^2 / \\gamma^2$? In Theorem 2, $R$ is fixed whereas in Lemma 4 it varies with $t$. To me this seems important, since without a bound on the number of steps it is unclear how you can control how far the embeddings move.\n\nMINOR COMMENTS\n\n3) In the statement of Lemma 4, linear separability of $V$ should be with respect to some fixed partition $Y$?.\n\n4) In Figure 5, why is empirical error not decreasing over epochs?\n\n5) I think the figures referenced in the text should be in the paper, not the appendix.\n\nMohri, M., Rostamizadeh, A., & Talwalkar, A. (2018). Foundations of machine learning. MIT press."}, "signatures": ["ICLR.cc/2020/Conference/Paper2150/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2150/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Symmetry and Initialization for Neural Networks", "authors": ["Ido Nachum", "Amir Yehudayoff"], "authorids": ["ido0808@gmail.com", "amir.yehudayoff@gmail.com"], "keywords": ["Neural Network Theory", "Symmetry"], "TL;DR": "When initialized properly, neural networks can learn the simple class of symmetric functions; when initialized randomly, they fail.  ", "abstract": "This work provides an additional step in the theoretical understanding of neural networks. We consider neural networks with one hidden layer and show that when learning symmetric functions, one can choose initial conditions so that standard SGD training efficiently produces generalization guarantees. We empirically verify this and show that this does not hold when the initial conditions are chosen at random. The proof of convergence investigates the interaction between the two layers of the network. Our results highlight the importance of using symmetry in the design of neural networks.", "pdf": "/pdf/aa42c4a7138b364449b57f898bc18731cab6cb3d.pdf", "paperhash": "nachum|on_symmetry_and_initialization_for_neural_networks", "original_pdf": "/attachment/c6c36b6aee3ae1db19c09b3af85a8b6e85eb002e.pdf", "_bibtex": "@misc{\nnachum2020on,\ntitle={On Symmetry and Initialization for Neural Networks},\nauthor={Ido Nachum and Amir Yehudayoff},\nyear={2020},\nurl={https://openreview.net/forum?id=Skeh-xBYDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Skeh-xBYDH", "replyto": "Skeh-xBYDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2150/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2150/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575909666858, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2150/Reviewers"], "noninvitees": [], "tcdate": 1570237726991, "tmdate": 1575909666874, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2150/-/Official_Review"}}}], "count": 10}