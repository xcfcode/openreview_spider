{"notes": [{"id": "2d34y5bRWxB", "original": "rYoDBfHsWK", "number": 3218, "cdate": 1601308357502, "ddate": null, "tcdate": 1601308357502, "tmdate": 1614985778269, "tddate": null, "forum": "2d34y5bRWxB", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Regularization Cocktails for Tabular Datasets", "authorids": ["~Arlind_Kadra1", "~Marius_Lindauer1", "~Frank_Hutter1", "~Josif_Grabocka1"], "authors": ["Arlind Kadra", "Marius Lindauer", "Frank Hutter", "Josif Grabocka"], "keywords": ["deep learning", "regularization", "hyperparameter optimization", "benchmarks."], "abstract": "The regularization of prediction models is arguably the most crucial ingredient that allows Machine Learning solutions to generalize well on unseen data. Several types of regularization are popular in the Deep Learning community (e.g., weight decay, drop-out, early stopping, etc.), but so far these are selected on an ad-hoc basis, and there is no systematic study as to how different regularizers should be combined into the best \u201ccocktail\u201d. In this paper, we fill this gap, by considering the cocktails of 13 different regularization methods and framing the question of how to best combine them as a standard hyperparameter optimization problem. We perform a large-scale empirical study on 40 tabular datasets, concluding that, firstly, regularization cocktails substantially outperform individual regularization methods, even if the hyperparameters of the latter are carefully tuned; secondly, the optimal regularization cocktail depends on the dataset; and thirdly, regularization cocktails yield the state-of-the-art in classifying tabular datasets by outperforming Gradient-Boosted Decision Trees.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kadra|regularization_cocktails_for_tabular_datasets", "one-sentence_summary": "An empirical study on the optimal combination of regularization methods.", "supplementary_material": "/attachment/0d8d077b5f848be4ed2569148097971c9400b54e.zip", "pdf": "/pdf/7eebe22c4ab2ef082d3c49fdfdf7fc40e3e67547.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=OZDyJjhcdc", "_bibtex": "@misc{\nkadra2021regularization,\ntitle={Regularization Cocktails for Tabular Datasets},\nauthor={Arlind Kadra and Marius Lindauer and Frank Hutter and Josif Grabocka},\nyear={2021},\nurl={https://openreview.net/forum?id=2d34y5bRWxB}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 16, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "MB4sea5sHr", "original": null, "number": 1, "cdate": 1610040352305, "ddate": null, "tcdate": 1610040352305, "tmdate": 1610473941490, "tddate": null, "forum": "2d34y5bRWxB", "replyto": "2d34y5bRWxB", "invitation": "ICLR.cc/2021/Conference/Paper3218/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "The paper in its most recent version claims that deep neural networks, when very carefully regularized, outperform methods such as Gradient Boosting Trees on tabular data. This is genuinely surprising to me (in a good way), and I suppose it is as well to the community.\n\nThe paper initially received negative reviews with two key remarks that \"The results are somewhat expected.\" (R4, R3, R2). Indeed, the original version mainly stated that very careful regularization helps on tabular data.  Naturally, the reviewers (including myself) seen then as the second key weakness that \"All experiments are run on tabular data.\" (R4, R3).\n\nBased on the reviews, the Authors have clarified and changed their message. I think it is well summarized by R2 \"The paper has been significantly refocused and now sells itself as a way of deep neural networks being competitive versus gradient boosting methods, which are dominating the tabular heterogeneous tasks.\"\n\nAs R2 said and was reflected in comments by other reviewers, \"[...] convinced by authors response on paper novelty, technical contribution and (after the re-focusing) potential usefulness to the community\".\n\nGiven the new message of the paper, a key new question surfaces. Is this indeed the first convincing demonstration that deep learning can outperform more standard methods on tabular data? R2 pointed out TabNet (see also Google Cloud offering) that already in 2019 claimed \"beating GB methods for the tabular data\". There is also NeurIPS work \"Regularization Learning Networks: Deep Learning\nfor Tabular Datasets\"; their abstract opens with \"Despite their impressive performance, Deep Neural Networks (DNNs) typically underperform Gradient Boosting Trees (GBTs) on many tabular-dataset learning tasks. We propose that applying a different regularization coefficient to each weight might boost the performance of DNNs by allowing them to make more use\nof the more relevant inputs\". The latter work did not claim to beat GBT. Regardless, the two works should be carefully discussed and compared empirically to in the new version of the work. \n\nI am also not yet fully convinced by the added comparison to GDBT. Arguably, AutoML from the sklearn package is not the most popular way to use GDBT in practice. How would regularization cocktails compare to GDBT from XGBoost, optimized using either random search or bayesian optimization?\n\nBased on the above, I have to recommend the rejection of the paper. The key reason is: *the new reframing of the paper is exciting but warrants a much more detailed and careful evaluation*.\n\nI really appreciate the work the Authors have put in clarifying and changing the message of the paper. I understand this is disappointing that we won't be able to include the work in ICLR. Nevertheless, I hope that the Authors found the feedback useful, and wanted to thank the Authors for submitting the work for consideration in ICLR."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Regularization Cocktails for Tabular Datasets", "authorids": ["~Arlind_Kadra1", "~Marius_Lindauer1", "~Frank_Hutter1", "~Josif_Grabocka1"], "authors": ["Arlind Kadra", "Marius Lindauer", "Frank Hutter", "Josif Grabocka"], "keywords": ["deep learning", "regularization", "hyperparameter optimization", "benchmarks."], "abstract": "The regularization of prediction models is arguably the most crucial ingredient that allows Machine Learning solutions to generalize well on unseen data. Several types of regularization are popular in the Deep Learning community (e.g., weight decay, drop-out, early stopping, etc.), but so far these are selected on an ad-hoc basis, and there is no systematic study as to how different regularizers should be combined into the best \u201ccocktail\u201d. In this paper, we fill this gap, by considering the cocktails of 13 different regularization methods and framing the question of how to best combine them as a standard hyperparameter optimization problem. We perform a large-scale empirical study on 40 tabular datasets, concluding that, firstly, regularization cocktails substantially outperform individual regularization methods, even if the hyperparameters of the latter are carefully tuned; secondly, the optimal regularization cocktail depends on the dataset; and thirdly, regularization cocktails yield the state-of-the-art in classifying tabular datasets by outperforming Gradient-Boosted Decision Trees.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kadra|regularization_cocktails_for_tabular_datasets", "one-sentence_summary": "An empirical study on the optimal combination of regularization methods.", "supplementary_material": "/attachment/0d8d077b5f848be4ed2569148097971c9400b54e.zip", "pdf": "/pdf/7eebe22c4ab2ef082d3c49fdfdf7fc40e3e67547.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=OZDyJjhcdc", "_bibtex": "@misc{\nkadra2021regularization,\ntitle={Regularization Cocktails for Tabular Datasets},\nauthor={Arlind Kadra and Marius Lindauer and Frank Hutter and Josif Grabocka},\nyear={2021},\nurl={https://openreview.net/forum?id=2d34y5bRWxB}\n}"}, "tags": [], "invitation": {"reply": {"forum": "2d34y5bRWxB", "replyto": "2d34y5bRWxB", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040352288, "tmdate": 1610473941472, "id": "ICLR.cc/2021/Conference/Paper3218/-/Decision"}}}, {"id": "PBQ0PlpdH-y", "original": null, "number": 3, "cdate": 1603900543082, "ddate": null, "tcdate": 1603900543082, "tmdate": 1606823306741, "tddate": null, "forum": "2d34y5bRWxB", "replyto": "2d34y5bRWxB", "invitation": "ICLR.cc/2021/Conference/Paper3218/-/Official_Review", "content": {"title": "Interesting study, requires further analysis", "review": "Summary: This work takes a step towards understanding the effect of automated selection of regularisation techniques and analyses the results across 42 structured datasets. It defines a search space over 13 regularisation techniques and employs one flavour of Bayesian Optimisation + Hyperband approach to find an optimal combination of regularisers. It concludes by substantiating three claims with corresponding experiments. \n\nThe work addresses an interesting problem that is relevant for full-scale automation of deep learning. The paper is easy to follow with a thorough literature survey (to my knowledge) and contains relevant experiments.\n\nI find the study can improve by addressing the following \n- Firstly, it is difficult to argue for generalisability of this work as currently this work relies on only one algorithm to find the optimal combination of regulaisers. One would need to check how much influence does BOHO have on the claims made within this work. \n- The three hypotheses are insightful but limited to small scale datasets. In particular, the third claims about gains on small datasets stands out as not-so-surprsing but challenges the effectiveness of the cocktail-system as DL models often find use in large scale problems.\n- While the study provides some descriptive insights, it falls short on prescriptive solutions for automated selection of regularisers.\n- It is interesting to note that in Exp 1 CutMix, CutOut and Mixup rank better as individual techniques and yet only CutMix crosses the threshold of 0.3 for Figure 3. Did the authors find any explanation for this? It would be interesting to see how Figure 4 changes if these are included along with BN, DO and WD. \n- Finally, I think the paper can improve with some additional information and more discussion. For instance, a brief description of BOHO, details on what the scatter plot correspond to in Figure 4, insights and discussion of results from all figures in the supplementary etc. It would also be important to establish statistical significance of the experiments with multiple re-runs. \n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3218/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3218/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Regularization Cocktails for Tabular Datasets", "authorids": ["~Arlind_Kadra1", "~Marius_Lindauer1", "~Frank_Hutter1", "~Josif_Grabocka1"], "authors": ["Arlind Kadra", "Marius Lindauer", "Frank Hutter", "Josif Grabocka"], "keywords": ["deep learning", "regularization", "hyperparameter optimization", "benchmarks."], "abstract": "The regularization of prediction models is arguably the most crucial ingredient that allows Machine Learning solutions to generalize well on unseen data. Several types of regularization are popular in the Deep Learning community (e.g., weight decay, drop-out, early stopping, etc.), but so far these are selected on an ad-hoc basis, and there is no systematic study as to how different regularizers should be combined into the best \u201ccocktail\u201d. In this paper, we fill this gap, by considering the cocktails of 13 different regularization methods and framing the question of how to best combine them as a standard hyperparameter optimization problem. We perform a large-scale empirical study on 40 tabular datasets, concluding that, firstly, regularization cocktails substantially outperform individual regularization methods, even if the hyperparameters of the latter are carefully tuned; secondly, the optimal regularization cocktail depends on the dataset; and thirdly, regularization cocktails yield the state-of-the-art in classifying tabular datasets by outperforming Gradient-Boosted Decision Trees.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kadra|regularization_cocktails_for_tabular_datasets", "one-sentence_summary": "An empirical study on the optimal combination of regularization methods.", "supplementary_material": "/attachment/0d8d077b5f848be4ed2569148097971c9400b54e.zip", "pdf": "/pdf/7eebe22c4ab2ef082d3c49fdfdf7fc40e3e67547.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=OZDyJjhcdc", "_bibtex": "@misc{\nkadra2021regularization,\ntitle={Regularization Cocktails for Tabular Datasets},\nauthor={Arlind Kadra and Marius Lindauer and Frank Hutter and Josif Grabocka},\nyear={2021},\nurl={https://openreview.net/forum?id=2d34y5bRWxB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "2d34y5bRWxB", "replyto": "2d34y5bRWxB", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3218/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538079881, "tmdate": 1606915759053, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3218/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3218/-/Official_Review"}}}, {"id": "3SUQZNxMehf", "original": null, "number": 4, "cdate": 1604810701454, "ddate": null, "tcdate": 1604810701454, "tmdate": 1606779850742, "tddate": null, "forum": "2d34y5bRWxB", "replyto": "2d34y5bRWxB", "invitation": "ICLR.cc/2021/Conference/Paper3218/-/Official_Review", "content": {"title": "A bag of experiments regarding parametric mixes of regularizers ", "review": "*Summary and contributions:*\nThis paper provides empirical evidence showing that optimizing a parametric mix of regularizers when training a model provides better generalization than using handcrafted ones.\nThe results show that the best regularizers mix is dataset dependent, and that regularization matters most when limited data is available.\n\n\n*Originality:*\nThis paper falls into the category \u201cmore parameters in the system lead to better results\u201d.\nI am not aware of a specific paper that has explored using a parametric mix of regularizer.\nI suspect some previous papers might have explored smaller variants of this idea, but none with the systematic approach and breath of this paper.\n\n\n*Significance:*\nThis paper is part of the AutoML trend. It provides support to the idea that the regularizers should also be thrown in the optimization target box. \n(Together with network architecture, data augmentation, and the optimizer themselves).\nAs such, it provides recommendations for best practices in machine learning.\n\n\n*Strengths:* \n* The paper carefully explicits the hypotheses being tested, and design experiments to probe them.\n* The observed effect is significant to the point of justifying the additional system complexity.\n* The experiments include a reasonable amount of regularization techniques in the mix (and for comparison).\n\n\n*Weaknesses:*\n* The results are somewhat expected. More parameters make systems better, everything is task dependent, and that regularization matters more when less data is available.\n* I did not find in the paper experiments adding the validation set in the training set (aka \u201cwhat if we do not split ?\u201d). Id est, a baseline accounting for the fact that in this setup the validation set is part of the overall training set.\n* Experiments only consider tabular datasets for classification. The intuition indicates that the results would generalize to 1d audio, 2d image, or 3d point-cloud data (as well as for other tasks beyond classification); but it would be better to have some data points on this aspect. \n\n\n*Clarity:*\nThe writing is clear, and the text is overall easy to follow.\n\n\n*Correctness:*\nI did not find anything particularly wrong.\n\n\n*Relation to prior work:*\nThe related work section is satisfactory. \nI am not aware of a specific related paper should be discussed (and a quick online search did not show either).\n\n\n*Reproducibility:*\nThe general idea seems easily reproducible. The text provides enough detail to be able to reproduce the overall system.\n\n\n*Specific feedback:*\n- Section 3.1, footnote: hold-out validation versus cross-validation might not be a detail, since it affects what is considered the training set of the baseline.\nIt would be good to include results of the baselines with \u201cdefault parameters\u201d trained over train+val.\n- Section 3.2: please mention the total number of additional parameters added to the system (or at least its order of magnitude).\n- Section 4.1: please mention that the task considered is always classification, and the total number of datasets considered.\n- Section 5, experiment 1: confirm our hypothesis that -> confirm our hypothesis 1 that\n- Figure 3: what is the logic of the top row sorting ? Consider sorting by frequency of usage. Please put the numbers as percentages, to be consistent with the paper text.\n- Section 5, experiment 3: just to confirm, the validation set size was kept fix ? \n- Section 5, experiment 3: \u201cindicate that strong regularization\u201d; maybe I missed a nuance, but the presented results show the consequences of _tuned_ regularization. This is different from _strong_ regularization. Is there a mention of the weights used for regularization in this case tend to be higher than for other setups ? That would be expected, but I did not see it presented. \n- Section 5, experiment 3: \u201cregularization can help...better...than generally perceived\u201d; please cite a paper (or blog post) taking that position. I learned in ML classes that regularization matters when data is small, thus to my knowledge these results simply fit the classical ML theory (Vapnik-era Statistical learning theory).\n- Figure 4: please mention what the points represent (a dataset each ?). Also use scientific notation for the number of examples.\n\n\n*Updates after reviews and authors feedback: *\nThe updates from the author are appreciated and make the arguments of the paper clearer.\nAfter reading the other reviews and discussions, I keep my original score of \"6: Marginally above acceptance threshold\".\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3218/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3218/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Regularization Cocktails for Tabular Datasets", "authorids": ["~Arlind_Kadra1", "~Marius_Lindauer1", "~Frank_Hutter1", "~Josif_Grabocka1"], "authors": ["Arlind Kadra", "Marius Lindauer", "Frank Hutter", "Josif Grabocka"], "keywords": ["deep learning", "regularization", "hyperparameter optimization", "benchmarks."], "abstract": "The regularization of prediction models is arguably the most crucial ingredient that allows Machine Learning solutions to generalize well on unseen data. Several types of regularization are popular in the Deep Learning community (e.g., weight decay, drop-out, early stopping, etc.), but so far these are selected on an ad-hoc basis, and there is no systematic study as to how different regularizers should be combined into the best \u201ccocktail\u201d. In this paper, we fill this gap, by considering the cocktails of 13 different regularization methods and framing the question of how to best combine them as a standard hyperparameter optimization problem. We perform a large-scale empirical study on 40 tabular datasets, concluding that, firstly, regularization cocktails substantially outperform individual regularization methods, even if the hyperparameters of the latter are carefully tuned; secondly, the optimal regularization cocktail depends on the dataset; and thirdly, regularization cocktails yield the state-of-the-art in classifying tabular datasets by outperforming Gradient-Boosted Decision Trees.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kadra|regularization_cocktails_for_tabular_datasets", "one-sentence_summary": "An empirical study on the optimal combination of regularization methods.", "supplementary_material": "/attachment/0d8d077b5f848be4ed2569148097971c9400b54e.zip", "pdf": "/pdf/7eebe22c4ab2ef082d3c49fdfdf7fc40e3e67547.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=OZDyJjhcdc", "_bibtex": "@misc{\nkadra2021regularization,\ntitle={Regularization Cocktails for Tabular Datasets},\nauthor={Arlind Kadra and Marius Lindauer and Frank Hutter and Josif Grabocka},\nyear={2021},\nurl={https://openreview.net/forum?id=2d34y5bRWxB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "2d34y5bRWxB", "replyto": "2d34y5bRWxB", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3218/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538079881, "tmdate": 1606915759053, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3218/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3218/-/Official_Review"}}}, {"id": "0a51bbW8kl-", "original": null, "number": 1, "cdate": 1603823313121, "ddate": null, "tcdate": 1603823313121, "tmdate": 1606319820740, "tddate": null, "forum": "2d34y5bRWxB", "replyto": "2d34y5bRWxB", "invitation": "ICLR.cc/2021/Conference/Paper3218/-/Official_Review", "content": {"title": "No contribution?", "review": "The paper presents a study on regularization methods for the feedforward fully connected neural networks.\nThe study is formulated as hyper-parameter optimization task, heavily using Auto-Pytorch library. The paper claims as contribution (sorry for copy-pasting):\n\n1. We demonstrate the empirical accuracy gains of regularization cocktails in a systematic\nmanner via a large-scale experimental study;\n2. We challenge the status-quo practices of designing universal dataset-agnostic regularizers,\nby showing that an optimal regularization cocktail is highly dataset-dependent;\n3. We demonstrate that regularization cocktails achieve a higher gain on smaller datasets;\n4. As an overarching contribution, this paper provides previously-lacking in-depth empiri-\ncal evidence to better understand the importance of combining different mechanisms for\nregularization, one of the most fundamental concepts in machine learning.\n\n***\n\nI am highly sceptical on the paper usefulness for the community. \nIn general terms, the benchmark/empirical study type of paper typically can have one (or more) of the following contributions:\n\na) New knowledge, which was obtained as a result of a study. E.g. surprising results, practical recommendations, so on. For example, A Metric Learning Reality Check by Musgrave et. al (ECCV 2020) revealed surprising knowledge about metric learning methods. \n\nb) The methodology of such study, which was not used before. E.g. Visual Object Tracking Challenge, which become the benchmark for the tracking methods since 2013. \nc) The software and/or dataset, which were developed for the study. E.g. OpenAI Gym.\n\n\n(a) is not the case IMO, because all the recommendations are known to the practicioners, e.g. check the any Kaggle winning solution\nhttps://www.kaggle.com/sudalairajkumar/winning-solutions-of-kaggle-competitions\n\n(b) I see no novelty in using hyper-parameter optimization for the study. The paper agrees with me on this  (see Related Work, \"Positioning in the realm of AutoML\"\n\n(c) Neither software, nor dataset is proposed -- the paper uses existing ones. \n\n****\n\nNow I will go over contributions.\n\n\n1. It is well known that the regularization/augmentation/... need to be tuned to archieve the best results. One can publish a CVPR paper about such good combination, e.g. He et.al. (CVPR2019), \nBag of Tricks for Image Classification with Convolutional Neural Networks\n\n2. I don't see the support for that claim in the paper. Yes, the specific combination of regularization techniques, which performs the best on the given dataset is, perhaps, unique. But the techniques are applicable broadly, which is supported by the paper (Fig. 1), e.g. DropOut, MixUp, BatchNorm, are pretty universal.\n\n3. It is also obvious, that the less data you have, more regularization and design bias are needed for better results, see OpenAI Image GPT, or more ViT paper vs. ConvNets. \n\n4. See (1)\n\n\nOverall, if the paper spend some space on particually interesting regularization combinations/interplay of components/analisys, it might be quite useful for researchers. For now it seems as a lot of experiments were done, but no analisys is really performed.\n\nE.g. abstract says: \"there is no systematic study as to how different regularizers should be combined into the best \"cocktail\"\". \n\nBut I don't see the answer of how should they be combined.\n\n********\n\nSmall things, not contributing to the score:\n\n- AutoPyTorch is cited twice, as arXiv and as CoRR\n\n- While skip-connections and BN can be seen as \"regularization\", I would rather call them \"architecture\". Anyway, that is just matter of naming. \n\n\n### After rebuttal update.\n\nThe paper has been significantly refocused and now sells itself as a way of deep neural networks being competitive versus gradient boosting methods, which are dominating the tabular heterogeneous tasks. \n\nI was also convinced by authors responce on paper novelty, technical contribution and (after the re-focusing) potential usefulness to the community.\nThus, I am raising my rating to weak accept.\n\n### Comments on authors response (as after Nov 24 I cannot post messages, visible to authors)\n\n> 1. However, we would be thankful if you might share any prior work (paper or published practice) where the authors automatically searched for the optimal combination of regularizers for deep learning models among a large set of regularizers, as presented in this study.\n\nI am surprised with the results of the googling, but have to admit that authors are technically right and I was wrong. While it seems obvious to me, that regularization (specifically, dropout and L2 weight decay) are the hyperparameters of the deep network training, somehow papers and guides online mostly consider mostly architectural things + learning rate + (sometimes) dropout rate as a hyperparameters to optimize. \n\nhttps://arxiv.org/pdf/2006.12703.pdf\nhttps://nanonets.com/blog/hyperparameter-optimization/\n\nAnyway, I lift my objections on novelty.\n\n> 3. \"Neither software, nor dataset is proposed, the paper uses existing ones.\" : We engineered a source code that selects the application of 13 regularizers to a neural network, which required extensive programming efforts and several additions to the AutoPytorch library, as mentioned in Section 4.1.\n\nOK, I agree.\n\n\n> 4. \"It is well known that the regularization/augmentation/ methods need to be tuned ... He et.al. (CVPR2019)\" : The suggested reference is a collection of refinements (many of which are actually not regularization techniques), which have been suggested by the deep learning community for maximizing the generalization on Imagenet. That paper only summarizes a collection of some practices, however, it does not present a method that searches for the best combination of a large set of practices\n\nNo, we are discussing different things. I gave the He at.al as an example, that community is well aware about the fact, the regularization and augmentation (and other things) have to be highly tuned. I agree that He at.al and the current paper use completely different methods for solving the problem (manual tinkering vs. auto-search). What I disagree is that the community was not aware about importance of regularization tuning before this paper.\n\n>5.  E.g. DropOut was present in only 35% of the dataset cocktails, hence was not selected in the cocktails of the 65% of the datasets.\n\nAnd experiments were using fixed-size network. Of course, is the network is not wide enough for the task, the dropout might not be needed. It is also quite strong statement that \"there is no universal regularization\", given that L2 weight decay and dropout are widely used in a such different domains as image, text, speech processings, RL and so on. \n****\n\nI would like to point out the TabNet paper https://arxiv.org/pdf/1908.07442.pdf, which claimed \"beating GB methods for the tabular data\". I appreciate the fact, that unlike the TabNet, RegCocktails were using a standard deep MLP and not the attention model, yet one needs to add that reference.\n\n\n\n\n ", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3218/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3218/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Regularization Cocktails for Tabular Datasets", "authorids": ["~Arlind_Kadra1", "~Marius_Lindauer1", "~Frank_Hutter1", "~Josif_Grabocka1"], "authors": ["Arlind Kadra", "Marius Lindauer", "Frank Hutter", "Josif Grabocka"], "keywords": ["deep learning", "regularization", "hyperparameter optimization", "benchmarks."], "abstract": "The regularization of prediction models is arguably the most crucial ingredient that allows Machine Learning solutions to generalize well on unseen data. Several types of regularization are popular in the Deep Learning community (e.g., weight decay, drop-out, early stopping, etc.), but so far these are selected on an ad-hoc basis, and there is no systematic study as to how different regularizers should be combined into the best \u201ccocktail\u201d. In this paper, we fill this gap, by considering the cocktails of 13 different regularization methods and framing the question of how to best combine them as a standard hyperparameter optimization problem. We perform a large-scale empirical study on 40 tabular datasets, concluding that, firstly, regularization cocktails substantially outperform individual regularization methods, even if the hyperparameters of the latter are carefully tuned; secondly, the optimal regularization cocktail depends on the dataset; and thirdly, regularization cocktails yield the state-of-the-art in classifying tabular datasets by outperforming Gradient-Boosted Decision Trees.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kadra|regularization_cocktails_for_tabular_datasets", "one-sentence_summary": "An empirical study on the optimal combination of regularization methods.", "supplementary_material": "/attachment/0d8d077b5f848be4ed2569148097971c9400b54e.zip", "pdf": "/pdf/7eebe22c4ab2ef082d3c49fdfdf7fc40e3e67547.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=OZDyJjhcdc", "_bibtex": "@misc{\nkadra2021regularization,\ntitle={Regularization Cocktails for Tabular Datasets},\nauthor={Arlind Kadra and Marius Lindauer and Frank Hutter and Josif Grabocka},\nyear={2021},\nurl={https://openreview.net/forum?id=2d34y5bRWxB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "2d34y5bRWxB", "replyto": "2d34y5bRWxB", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3218/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538079881, "tmdate": 1606915759053, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3218/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3218/-/Official_Review"}}}, {"id": "pZfcPFeZbLx", "original": null, "number": 6, "cdate": 1606058572669, "ddate": null, "tcdate": 1606058572669, "tmdate": 1606302803213, "tddate": null, "forum": "2d34y5bRWxB", "replyto": "3SUQZNxMehf", "invitation": "ICLR.cc/2021/Conference/Paper3218/-/Official_Comment", "content": {"title": "Response to AnonReviewer4", "comment": "1) **\"I did not find in the paper experiments adding the validation set in the training set ... baseline accounting for the fact that the validation set is part of the overall training set.\"** : Our apologies that this was unclear. For all the baselines and our method we combine the training and validation set before fitting the final model with the best hyper-parameter configuration for measuring the test error, exactly as you suggest. In fact that was stated in Section 4.3 of the paper, however, to improve readability we modified Section 4.1 to reinforce the point.\n2) **\" Experiments only consider tabular datasets for classification. The intuition indicates that the results would generalize to 1d audio, 2d image, or 3d point-cloud data (as well as for other tasks beyond classification); but it would be better to have some data points on this aspect.\"** : We intentionally chose to work with tabular datasets for multiple reasons, apologies that this was not clear. First of all, tabular datasets represent a major and important dataset modality that is currently in possession by the industries. Secondly, neural networks do not perform strongly in tabular datasets and are outperformed compared to state-of-the-art models such as Gradient-Boosted Decision Trees (GBDT). Our belief is that neural networks are high variance models for tabular datasets and if they can be regularized more thoroughly, then they can achieve competitive results on tabular datasets. In order to demonstrate this point, we added an extensive comparison to GBDT which is arguably the best classifier on tabular datasets. We tuned the parameters of GBDT by giving the same large HPO time budget as our regularization cocktail. The results, which we are presenting in Section 5 show that a data-set specific regularization outperforms the carefully-tuned GBDT with a statistically, significant margin. This is the first method we are aware of that actually shows that Neural Networks can outperform GBDT in a very-large scaled experiment with an experimental protocol that includes extensive hyper-parameter tuning. As a result, we believe the findings are not only a demonstration of the power of regularization cocktails, but also a game-changing message to the community that deep learning can be state-of-the-art on tabular data if regularized carefully.\n3) **\"It would be good to include results of the baselines with \u201cdefault parameters\u201d trained over train+val.\"** :  The reported results already show the test performance when testing all methods after training the best found hyper-parameter on train + val combined. We actually did more than running baselines with the default parameters since the reported results include tuned hyper-parameters of all baselines in a principled HPO protocol with the search space of Table 1.\n4) **\"mention the total number of additional parameters added to the system\"** : If the reviewer means the hyper-parameters of the regularization cocktail, it is the search space of Table 1 with a total of 18 hyper-parameters (we further clarified it in section 3.2) . If the reviewer means the added neural network params, it remains roughly the same (with the minor exception below) as the unregularized network, because the reg. cocktail does not add parameters to the neural network, it only regularizes the existing params differently: I.e. the cocktail adds a hyper-parametrization to the HPO, not to the prediction model itself.  (Exception: Shake-Shake doubles the params because it creates a parallel branch of layers).\n5) **\"mention that the task considered is always classification, and the total number of datasets.\"** : Thank you for the feedback, we added the clarification in Section 4.1.\n6) **\"Consider sorting by frequency. Please put the numbers as percentages ...\"** : Thank you for your feedback, we changed it accordingly at the new Figure 3.\n7) **\"Experiment 3, was the validation set size kept fix?\"** : The validation set was proportionally down-scaled like the training set in order to provide a correct setup of data sub-sampling (the folds were always stratified). However, for the results to be comparable, the test set was always fixed. \n8) **\"Tuned regularization is different from strong regularization\"** : We agree that the usage of the term \u201cstrong\u201d was misleading and it means exactly as the reviewer states: a dataset-specific subset of regularizers where the hyper-parameters are \u201ctuned\u201d jointly. So they don\u2019t have to have high values, it depends on the combination, which is always dataset-specific.\n9) **\"...regularization matters when data is small, thus to my knowledge these results simply fit the classical ML theory.\"** : We agree that this experiment shows trivial findings and does not add novelty. After carefully considering your comment and the other reviewers we decided that the paper would be much stronger if we simply remove this experiment from the paper and make room for the comparison to GBDT.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3218/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3218/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Regularization Cocktails for Tabular Datasets", "authorids": ["~Arlind_Kadra1", "~Marius_Lindauer1", "~Frank_Hutter1", "~Josif_Grabocka1"], "authors": ["Arlind Kadra", "Marius Lindauer", "Frank Hutter", "Josif Grabocka"], "keywords": ["deep learning", "regularization", "hyperparameter optimization", "benchmarks."], "abstract": "The regularization of prediction models is arguably the most crucial ingredient that allows Machine Learning solutions to generalize well on unseen data. Several types of regularization are popular in the Deep Learning community (e.g., weight decay, drop-out, early stopping, etc.), but so far these are selected on an ad-hoc basis, and there is no systematic study as to how different regularizers should be combined into the best \u201ccocktail\u201d. In this paper, we fill this gap, by considering the cocktails of 13 different regularization methods and framing the question of how to best combine them as a standard hyperparameter optimization problem. We perform a large-scale empirical study on 40 tabular datasets, concluding that, firstly, regularization cocktails substantially outperform individual regularization methods, even if the hyperparameters of the latter are carefully tuned; secondly, the optimal regularization cocktail depends on the dataset; and thirdly, regularization cocktails yield the state-of-the-art in classifying tabular datasets by outperforming Gradient-Boosted Decision Trees.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kadra|regularization_cocktails_for_tabular_datasets", "one-sentence_summary": "An empirical study on the optimal combination of regularization methods.", "supplementary_material": "/attachment/0d8d077b5f848be4ed2569148097971c9400b54e.zip", "pdf": "/pdf/7eebe22c4ab2ef082d3c49fdfdf7fc40e3e67547.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=OZDyJjhcdc", "_bibtex": "@misc{\nkadra2021regularization,\ntitle={Regularization Cocktails for Tabular Datasets},\nauthor={Arlind Kadra and Marius Lindauer and Frank Hutter and Josif Grabocka},\nyear={2021},\nurl={https://openreview.net/forum?id=2d34y5bRWxB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "2d34y5bRWxB", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3218/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3218/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3218/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3218/Authors|ICLR.cc/2021/Conference/Paper3218/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3218/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839847, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3218/-/Official_Comment"}}}, {"id": "IwoKYRfXof5", "original": null, "number": 14, "cdate": 1606294787729, "ddate": null, "tcdate": 1606294787729, "tmdate": 1606301637883, "tddate": null, "forum": "2d34y5bRWxB", "replyto": "M1oSEfxpdN", "invitation": "ICLR.cc/2021/Conference/Paper3218/-/Official_Comment", "content": {"title": "Strengthening emphasis on Tabular Datasets", "comment": "Dear AnonReviewer3,\n\nWe strengthened the emphasis on tabular datasets following your recommendation. Tabular datasets are mentioned multiple times now in the abstract, introduction (incl. a motivation why we focus on tabular data), and experimental protocol. We also changed the title accordingly."}, "signatures": ["ICLR.cc/2021/Conference/Paper3218/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3218/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Regularization Cocktails for Tabular Datasets", "authorids": ["~Arlind_Kadra1", "~Marius_Lindauer1", "~Frank_Hutter1", "~Josif_Grabocka1"], "authors": ["Arlind Kadra", "Marius Lindauer", "Frank Hutter", "Josif Grabocka"], "keywords": ["deep learning", "regularization", "hyperparameter optimization", "benchmarks."], "abstract": "The regularization of prediction models is arguably the most crucial ingredient that allows Machine Learning solutions to generalize well on unseen data. Several types of regularization are popular in the Deep Learning community (e.g., weight decay, drop-out, early stopping, etc.), but so far these are selected on an ad-hoc basis, and there is no systematic study as to how different regularizers should be combined into the best \u201ccocktail\u201d. In this paper, we fill this gap, by considering the cocktails of 13 different regularization methods and framing the question of how to best combine them as a standard hyperparameter optimization problem. We perform a large-scale empirical study on 40 tabular datasets, concluding that, firstly, regularization cocktails substantially outperform individual regularization methods, even if the hyperparameters of the latter are carefully tuned; secondly, the optimal regularization cocktail depends on the dataset; and thirdly, regularization cocktails yield the state-of-the-art in classifying tabular datasets by outperforming Gradient-Boosted Decision Trees.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kadra|regularization_cocktails_for_tabular_datasets", "one-sentence_summary": "An empirical study on the optimal combination of regularization methods.", "supplementary_material": "/attachment/0d8d077b5f848be4ed2569148097971c9400b54e.zip", "pdf": "/pdf/7eebe22c4ab2ef082d3c49fdfdf7fc40e3e67547.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=OZDyJjhcdc", "_bibtex": "@misc{\nkadra2021regularization,\ntitle={Regularization Cocktails for Tabular Datasets},\nauthor={Arlind Kadra and Marius Lindauer and Frank Hutter and Josif Grabocka},\nyear={2021},\nurl={https://openreview.net/forum?id=2d34y5bRWxB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "2d34y5bRWxB", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3218/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3218/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3218/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3218/Authors|ICLR.cc/2021/Conference/Paper3218/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3218/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839847, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3218/-/Official_Comment"}}}, {"id": "8chFtEnJi2y", "original": null, "number": 12, "cdate": 1606078495356, "ddate": null, "tcdate": 1606078495356, "tmdate": 1606300173379, "tddate": null, "forum": "2d34y5bRWxB", "replyto": "xBn_qmqCYJp", "invitation": "ICLR.cc/2021/Conference/Paper3218/-/Official_Comment", "content": {"title": "Response to AnonReviewer2", "comment": "Dear AnonReviewer2,\n\nYou are raising an important point, we fully agree that the AutoML-inspired HPO of our regularization cocktail is time-consuming in terms of the runtime budget (especially for very large datasets). However, the manual fine-tuning and calibration of methods to fit a new dataset is also time-demanding. As models do not \u201cplug-and-play\u201d (i.e. generalize directly) on new datasets, practitioners need to revise the inductive biases of their model choices continuously until a generalized version is achieved. In the end of the day, complex HPO methods demand a large computational time, however the manual design of efficient models takes both a long time and considerable manual labor.\n\nOn the other hand, the reviewer has a good point if we would be thinking about the typical deep learning of very large, raw data (e.g., training on ImageNet), where such a full HPO from scratch is indeed ludicrously expensive. But (1) on tabular data (focus of this paper) this tuning is perfectly reasonable, and (2) knowing that the regularization cocktails perform so well will almost certainly trigger a lot of follow-up work aiming to reduce the computational overhead of finding the best cocktail for a specific dataset. \n\nBest,\nThe Authors"}, "signatures": ["ICLR.cc/2021/Conference/Paper3218/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3218/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Regularization Cocktails for Tabular Datasets", "authorids": ["~Arlind_Kadra1", "~Marius_Lindauer1", "~Frank_Hutter1", "~Josif_Grabocka1"], "authors": ["Arlind Kadra", "Marius Lindauer", "Frank Hutter", "Josif Grabocka"], "keywords": ["deep learning", "regularization", "hyperparameter optimization", "benchmarks."], "abstract": "The regularization of prediction models is arguably the most crucial ingredient that allows Machine Learning solutions to generalize well on unseen data. Several types of regularization are popular in the Deep Learning community (e.g., weight decay, drop-out, early stopping, etc.), but so far these are selected on an ad-hoc basis, and there is no systematic study as to how different regularizers should be combined into the best \u201ccocktail\u201d. In this paper, we fill this gap, by considering the cocktails of 13 different regularization methods and framing the question of how to best combine them as a standard hyperparameter optimization problem. We perform a large-scale empirical study on 40 tabular datasets, concluding that, firstly, regularization cocktails substantially outperform individual regularization methods, even if the hyperparameters of the latter are carefully tuned; secondly, the optimal regularization cocktail depends on the dataset; and thirdly, regularization cocktails yield the state-of-the-art in classifying tabular datasets by outperforming Gradient-Boosted Decision Trees.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kadra|regularization_cocktails_for_tabular_datasets", "one-sentence_summary": "An empirical study on the optimal combination of regularization methods.", "supplementary_material": "/attachment/0d8d077b5f848be4ed2569148097971c9400b54e.zip", "pdf": "/pdf/7eebe22c4ab2ef082d3c49fdfdf7fc40e3e67547.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=OZDyJjhcdc", "_bibtex": "@misc{\nkadra2021regularization,\ntitle={Regularization Cocktails for Tabular Datasets},\nauthor={Arlind Kadra and Marius Lindauer and Frank Hutter and Josif Grabocka},\nyear={2021},\nurl={https://openreview.net/forum?id=2d34y5bRWxB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "2d34y5bRWxB", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3218/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3218/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3218/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3218/Authors|ICLR.cc/2021/Conference/Paper3218/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3218/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839847, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3218/-/Official_Comment"}}}, {"id": "p_2byJ0IY_0", "original": null, "number": 2, "cdate": 1603877898024, "ddate": null, "tcdate": 1603877898024, "tmdate": 1606260719173, "tddate": null, "forum": "2d34y5bRWxB", "replyto": "2d34y5bRWxB", "invitation": "ICLR.cc/2021/Conference/Paper3218/-/Official_Review", "content": {"title": "Review of \"Regularization Cocktails\"", "review": "Summary:\nThis paper provides an empirical study of combining different regularizers. Fourteen regularizers including batch norm, weight decay, etc. are considered. The authors use BOHB (Falkner et al. 2018) to optimize for whether each regularizer is active, and additional regularizer-specific hyperparameters. Using 40 tabular datasets, they show mixtures nearly always outperform tuning a single regularizer, and that the benefits of regularization improves for smaller datasets.\n\nStrengths:\n- To my knowledge, this is the first paper that does an empirical study of combining regularizers. \n- The list of regularizers considered are extensive.\n- The paper is very well written and easy to follow. The figures are nice and easy to understand.\n\nWeaknesses:\nI think the biggest weakness of this work is that it is not very useful practically. \n- All experiments are run on tabular data. I think it\u2019s fair to say that most practitioners who deal with regularization are interested in non tabular data, given the fact that the regularization methods were individually developed for training on non-tabular data. I find it hard to imagine extrapolating results on tabular data to images, or text.\n- The paper frames itself as a \u201cmethods\u201d paper more than an analysis paper, where the main claims revolve around the superiority of the regularization cocktail. In fact, the take-home messages given in the conclusion explicitly recommends the use of the regularization cocktail. I find this advice not very useful because: 1) the experiments were run on tabular data, 2) regularization cocktail is too expensive to justify the improvement (if any).\n- The conclusions are trivial. It\u2019s quite obvious that a more general method always does at least as good as the method it subsumes, as long as the tuning of the parameters can be done sufficiently. The fact that the regularization cocktail does better than individual methods, or a combination of a few, is very believable in the tabular setting, since the tuning can be done sufficiently. The fact that more regularization is needed for smaller datasets is also well-known. \n\nThe paper would be more useful with a similar experimental protocol applied to a non-tabular dataset (CIFAR-10, Fashion MNIST, SVHN, etc), but with a focus on analyzing trends (rather than highlighting the mixture method), or comparing with SOTA human designed cocktails. I recommend reject because of the lack of practicality of the paper.\n\nComments:\n- For the experiments corresponding to Figure 5, I wonder what the gap would be like for the best performing individual regularizer. I am curious because I think for a small dataset, the need for a state of the art regularizer diminishes, and just one good regularizer suffices.\n\n=== update ===\n\nI have read the revised paper, and decided to update the score (see response to authors' comment).\n", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper3218/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3218/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Regularization Cocktails for Tabular Datasets", "authorids": ["~Arlind_Kadra1", "~Marius_Lindauer1", "~Frank_Hutter1", "~Josif_Grabocka1"], "authors": ["Arlind Kadra", "Marius Lindauer", "Frank Hutter", "Josif Grabocka"], "keywords": ["deep learning", "regularization", "hyperparameter optimization", "benchmarks."], "abstract": "The regularization of prediction models is arguably the most crucial ingredient that allows Machine Learning solutions to generalize well on unseen data. Several types of regularization are popular in the Deep Learning community (e.g., weight decay, drop-out, early stopping, etc.), but so far these are selected on an ad-hoc basis, and there is no systematic study as to how different regularizers should be combined into the best \u201ccocktail\u201d. In this paper, we fill this gap, by considering the cocktails of 13 different regularization methods and framing the question of how to best combine them as a standard hyperparameter optimization problem. We perform a large-scale empirical study on 40 tabular datasets, concluding that, firstly, regularization cocktails substantially outperform individual regularization methods, even if the hyperparameters of the latter are carefully tuned; secondly, the optimal regularization cocktail depends on the dataset; and thirdly, regularization cocktails yield the state-of-the-art in classifying tabular datasets by outperforming Gradient-Boosted Decision Trees.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kadra|regularization_cocktails_for_tabular_datasets", "one-sentence_summary": "An empirical study on the optimal combination of regularization methods.", "supplementary_material": "/attachment/0d8d077b5f848be4ed2569148097971c9400b54e.zip", "pdf": "/pdf/7eebe22c4ab2ef082d3c49fdfdf7fc40e3e67547.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=OZDyJjhcdc", "_bibtex": "@misc{\nkadra2021regularization,\ntitle={Regularization Cocktails for Tabular Datasets},\nauthor={Arlind Kadra and Marius Lindauer and Frank Hutter and Josif Grabocka},\nyear={2021},\nurl={https://openreview.net/forum?id=2d34y5bRWxB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "2d34y5bRWxB", "replyto": "2d34y5bRWxB", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3218/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538079881, "tmdate": 1606915759053, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3218/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3218/-/Official_Review"}}}, {"id": "M1oSEfxpdN", "original": null, "number": 13, "cdate": 1606260661305, "ddate": null, "tcdate": 1606260661305, "tmdate": 1606260661305, "tddate": null, "forum": "2d34y5bRWxB", "replyto": "TDS8-zHKz9P", "invitation": "ICLR.cc/2021/Conference/Paper3218/-/Official_Comment", "content": {"title": "I have updated my score.", "comment": "I have read the other reviewers\u2019 comments, the authors\u2019 responses, and the updated paper. Given that the paper now seems to be focused on tabular datasets, Most of my concerns are addressed, and therefore, I\u2019m increasing the score to weak acceptance. I would like the abstract, introduction, and section 4.1 to be further changed to make the focus on tabular dataset more clear. "}, "signatures": ["ICLR.cc/2021/Conference/Paper3218/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3218/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Regularization Cocktails for Tabular Datasets", "authorids": ["~Arlind_Kadra1", "~Marius_Lindauer1", "~Frank_Hutter1", "~Josif_Grabocka1"], "authors": ["Arlind Kadra", "Marius Lindauer", "Frank Hutter", "Josif Grabocka"], "keywords": ["deep learning", "regularization", "hyperparameter optimization", "benchmarks."], "abstract": "The regularization of prediction models is arguably the most crucial ingredient that allows Machine Learning solutions to generalize well on unseen data. Several types of regularization are popular in the Deep Learning community (e.g., weight decay, drop-out, early stopping, etc.), but so far these are selected on an ad-hoc basis, and there is no systematic study as to how different regularizers should be combined into the best \u201ccocktail\u201d. In this paper, we fill this gap, by considering the cocktails of 13 different regularization methods and framing the question of how to best combine them as a standard hyperparameter optimization problem. We perform a large-scale empirical study on 40 tabular datasets, concluding that, firstly, regularization cocktails substantially outperform individual regularization methods, even if the hyperparameters of the latter are carefully tuned; secondly, the optimal regularization cocktail depends on the dataset; and thirdly, regularization cocktails yield the state-of-the-art in classifying tabular datasets by outperforming Gradient-Boosted Decision Trees.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kadra|regularization_cocktails_for_tabular_datasets", "one-sentence_summary": "An empirical study on the optimal combination of regularization methods.", "supplementary_material": "/attachment/0d8d077b5f848be4ed2569148097971c9400b54e.zip", "pdf": "/pdf/7eebe22c4ab2ef082d3c49fdfdf7fc40e3e67547.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=OZDyJjhcdc", "_bibtex": "@misc{\nkadra2021regularization,\ntitle={Regularization Cocktails for Tabular Datasets},\nauthor={Arlind Kadra and Marius Lindauer and Frank Hutter and Josif Grabocka},\nyear={2021},\nurl={https://openreview.net/forum?id=2d34y5bRWxB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "2d34y5bRWxB", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3218/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3218/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3218/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3218/Authors|ICLR.cc/2021/Conference/Paper3218/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3218/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839847, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3218/-/Official_Comment"}}}, {"id": "X_PNN7sOyRF", "original": null, "number": 4, "cdate": 1606056817220, "ddate": null, "tcdate": 1606056817220, "tmdate": 1606250824692, "tddate": null, "forum": "2d34y5bRWxB", "replyto": "2d34y5bRWxB", "invitation": "ICLR.cc/2021/Conference/Paper3218/-/Official_Comment", "content": {"title": "Reply to all reviewers about changes", "comment": "Thank you to all the reviewers for their important feedback and suggestions. We would like to apologize for our delay in replying that was caused by the extensive new experiments we had to run for the rebuttal. Although we will reply below to all individual reviews, we would additionally like to highlight the main changes for the rebuttal:\n\n1. In order to consider the positive criticism of the reviewers that the paper lacks a practical benefit to the practitioners, we ran additional experiments to show that neural networks trained with the regularization cocktails achieve state-of-the-art predictive accuracy. Therefore, practitioners will be interested in using our method because it is the new state-of-the-art in classifying tabular datasets. \n    * To demonstrate the point we compared against Gradient-Boosted Decision Trees (GBDT), which is the de-facto state-of-the- \n    art classifier for tabular datasets and the practitioners\u2019 main current tool in winning data science competitions involving tabular \n    datasets (e.g. Kaggle). It is widely believed in the community that GBDT outperforms Neural Networks in tabular datasets. \n    * We provide a fair comparison by tuning the hyper-parameters of GBDT on each dataset with the same train, val, test spits and \n    the same hyper-parameter search time. The experiments suggest that the regularization cocktails are more accurate than GBDT \n    with a statistically-significant margin. To the best of our knowledge this is the first paper that shows how to make neural \n    networks outperform GBDT on tabular datasets, by rethinking the way neural networks are regularized.\n    * The fact that regularization cocktails enable deep neural networks to outperform GBDT opens up the possibility for another \n     huge area of applications of deep learning: small, tabular data sets. \n\n2. Another great point of the reviewers was that we need to show a benefit not only against single regularization methods, but also against combinations of regularization techniques which are intuitive and universal for all datasets. To achieve the aim, we designed a new experiment 2, where we compare against:\n    * A cocktail of the top 5 strongest regularization methods based on their performance on our dataset collection.\n    * A cocktail of the top 5 most frequent regularization methods based on their occurrence frequencies on the dataset-specific \n    cocktails.\n    * The results show that these two cocktails of universal combinations of regularization methods, still underperform w.r.t. a \n    dataset-specific optimal cocktails. It means that just combining best top recipes on regularizers does not achieve optimal \n    results compared to the dedicated dataset-specific combinations.\n\n3) The last major positive criticism of the reviewers was that the findings are somehow obvious. To address the concern we added new analysis on frequent combinations of regularizers and removed the plot that was showing the performance of  regularization on smaller dataset sizes. \n\n4) We re-engineered our code and now stochastic weight averaging and snapshot ensembling can be both activated at the same time. We reran the experiments with this update and this combination shows a lift in several datasets. We modified Section 3.2 and we updated the figures and tables accordingly."}, "signatures": ["ICLR.cc/2021/Conference/Paper3218/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3218/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Regularization Cocktails for Tabular Datasets", "authorids": ["~Arlind_Kadra1", "~Marius_Lindauer1", "~Frank_Hutter1", "~Josif_Grabocka1"], "authors": ["Arlind Kadra", "Marius Lindauer", "Frank Hutter", "Josif Grabocka"], "keywords": ["deep learning", "regularization", "hyperparameter optimization", "benchmarks."], "abstract": "The regularization of prediction models is arguably the most crucial ingredient that allows Machine Learning solutions to generalize well on unseen data. Several types of regularization are popular in the Deep Learning community (e.g., weight decay, drop-out, early stopping, etc.), but so far these are selected on an ad-hoc basis, and there is no systematic study as to how different regularizers should be combined into the best \u201ccocktail\u201d. In this paper, we fill this gap, by considering the cocktails of 13 different regularization methods and framing the question of how to best combine them as a standard hyperparameter optimization problem. We perform a large-scale empirical study on 40 tabular datasets, concluding that, firstly, regularization cocktails substantially outperform individual regularization methods, even if the hyperparameters of the latter are carefully tuned; secondly, the optimal regularization cocktail depends on the dataset; and thirdly, regularization cocktails yield the state-of-the-art in classifying tabular datasets by outperforming Gradient-Boosted Decision Trees.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kadra|regularization_cocktails_for_tabular_datasets", "one-sentence_summary": "An empirical study on the optimal combination of regularization methods.", "supplementary_material": "/attachment/0d8d077b5f848be4ed2569148097971c9400b54e.zip", "pdf": "/pdf/7eebe22c4ab2ef082d3c49fdfdf7fc40e3e67547.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=OZDyJjhcdc", "_bibtex": "@misc{\nkadra2021regularization,\ntitle={Regularization Cocktails for Tabular Datasets},\nauthor={Arlind Kadra and Marius Lindauer and Frank Hutter and Josif Grabocka},\nyear={2021},\nurl={https://openreview.net/forum?id=2d34y5bRWxB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "2d34y5bRWxB", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3218/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3218/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3218/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3218/Authors|ICLR.cc/2021/Conference/Paper3218/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3218/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839847, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3218/-/Official_Comment"}}}, {"id": "_8Jd0UH3hTQ", "original": null, "number": 7, "cdate": 1606059479665, "ddate": null, "tcdate": 1606059479665, "tmdate": 1606071248436, "tddate": null, "forum": "2d34y5bRWxB", "replyto": "PBQ0PlpdH-y", "invitation": "ICLR.cc/2021/Conference/Paper3218/-/Official_Comment", "content": {"title": "Response to AnonReviewer1", "comment": "1) **\"Firstly, it is difficult to argue for generalisability of this work ... would need to check how much influence does BOHO have on the claims made within this work.\"** : BOHB is the state-of-the-art algorithm in AutoML for tuning hyper-parameters of gray-box models such as neural networks (in graybox optimization we make usage of intermediate performances, e.g. validation error on a small number of epochs). The method is a combination of Hyperband with surrogate models and is the natural choice for our setup. Furthermore, our setup is fair because all baselines were tuned by the same state-of-the-art HPO solver with ample hyper-parameter runtime on each dataset (up to 4 days per tabular dataset is quite sufficient) on multi-core CPUs (please see Section 4.1). However, we would like to emphasize that the quality of the discovered hyper-parameters is not actually very dependent on the choice of the HPO method if the runtime budget is large (as in our setup). Even random search will eventually perform as good as any other method given an abundant amount of trials. Considering that: 1) BOHB is the state-of-the-art and the natural choice for neural networks, 2) the same HPO algorithm was used for the baselines under the same conditions, and  3) that we gave each dataset a large budget for HPO, then, we do not see a bias in the selection of the HPO method.\n2) **\"... gains on small datasets stands out as not-so-surprsing but challenges the effectiveness of the cocktail-system as DL models often find use in large scale problems.\"** : After carefully reading the reviewers, which mostly stated that this experiment is not \u201cwrong\u201d, but neither it demonstrates a novel finding, we decided the paper would be much better if we delete it entirely and make room for our new experiment that compares the regularization cocktail against the state-of-the-art classifier Gradient-Boosted Decision Tree. \n3) **\"While the study provides some descriptive insights, it falls short on prescriptive solutions for automated selection of regularisers.\"\"** : We take responsibility for not making this point crystal clear. Our finding is that there is no magic recipe on combining regularizers. We are aware that the community would love to hear findings such as \u201ce.g. if you use method A, B then you should combine it with method C\u201d, but in our results we found no clear pattern as it always depends on a dataset (see the cocktail frequencies of Section 5, Figure 3). It does not mean, e.g. that dropout is not a good regularizer, but that it does not always help when combined with other regularizers. We stress that the combination of regularizers which achieve the best validation accuracy is always dataset-dependent (It actually is very intuitional because of the different effects that interactions of regularizers have on each dataset, which demand a dataset-specific surrogate model for capturing such dataset-dependent hyper-parameter interactions). So our novel prescriptive solution is: Treat regularizing your model as a Combined Algorithm Selection and Hyper-parameter (CASH) problem and solve it for each new dataset. Please notice that the many existing prescriptive solutions of the deep learning community are largely a naive instance of our problem. They are mostly trial-and-error combinations of regularizers on ImageNet, so the community can be perceived to act as a distributed hyper-parameter optimization algorithm discovering regularization cocktails on ImageNet by human-run trials. At the end of the day, our work shows an accurate and automatized solution for replacing this inefficient process with a parametric HPO method. \n4) **\"CutMix, CutOut and Mixup rank better as individual techniques and yet only CutMix crosses the threshold of 0.3 for Figure 3. It would be interesting to see if these are included.\"** : The low frequencies arise from the fact that only one data augmentation method was permitted in a cocktail (as was specified in Section 3.2 and on Table 1 of the submitted version). But we admit that the point raised by the reviewer is a very good point here! To answer it, we invested quite some time to program an upgrade to our source code that allows the joint combination of multiple data augmentation techniques (hence the delay of the rebuttal). We are presenting two experiments as a follow-up to your suggestion: 1) repeated all the cocktail HPO and evaluation runs with the upgrade on multiple active data augmentation methods, and added two new baselines on the top-5 most frequent and stronger baselines (as you suggest BN, DO, WD, plus data augmentation). The results are presented in Sections 5, Figure 4 of the new draft.\n5) **\"... Finally, I think the paper can improve with some additional information and more discussion.\"** : Thank you for your feedback, we described BOHB in the Appendix, added details on what the scatter plot in Figure 4 corresponds to and added better descriptions on the supplementary materials"}, "signatures": ["ICLR.cc/2021/Conference/Paper3218/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3218/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Regularization Cocktails for Tabular Datasets", "authorids": ["~Arlind_Kadra1", "~Marius_Lindauer1", "~Frank_Hutter1", "~Josif_Grabocka1"], "authors": ["Arlind Kadra", "Marius Lindauer", "Frank Hutter", "Josif Grabocka"], "keywords": ["deep learning", "regularization", "hyperparameter optimization", "benchmarks."], "abstract": "The regularization of prediction models is arguably the most crucial ingredient that allows Machine Learning solutions to generalize well on unseen data. Several types of regularization are popular in the Deep Learning community (e.g., weight decay, drop-out, early stopping, etc.), but so far these are selected on an ad-hoc basis, and there is no systematic study as to how different regularizers should be combined into the best \u201ccocktail\u201d. In this paper, we fill this gap, by considering the cocktails of 13 different regularization methods and framing the question of how to best combine them as a standard hyperparameter optimization problem. We perform a large-scale empirical study on 40 tabular datasets, concluding that, firstly, regularization cocktails substantially outperform individual regularization methods, even if the hyperparameters of the latter are carefully tuned; secondly, the optimal regularization cocktail depends on the dataset; and thirdly, regularization cocktails yield the state-of-the-art in classifying tabular datasets by outperforming Gradient-Boosted Decision Trees.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kadra|regularization_cocktails_for_tabular_datasets", "one-sentence_summary": "An empirical study on the optimal combination of regularization methods.", "supplementary_material": "/attachment/0d8d077b5f848be4ed2569148097971c9400b54e.zip", "pdf": "/pdf/7eebe22c4ab2ef082d3c49fdfdf7fc40e3e67547.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=OZDyJjhcdc", "_bibtex": "@misc{\nkadra2021regularization,\ntitle={Regularization Cocktails for Tabular Datasets},\nauthor={Arlind Kadra and Marius Lindauer and Frank Hutter and Josif Grabocka},\nyear={2021},\nurl={https://openreview.net/forum?id=2d34y5bRWxB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "2d34y5bRWxB", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3218/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3218/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3218/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3218/Authors|ICLR.cc/2021/Conference/Paper3218/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3218/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839847, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3218/-/Official_Comment"}}}, {"id": "wgDedykKZ8S", "original": null, "number": 11, "cdate": 1606063361199, "ddate": null, "tcdate": 1606063361199, "tmdate": 1606063361199, "tddate": null, "forum": "2d34y5bRWxB", "replyto": "6o7lyTw6-G1", "invitation": "ICLR.cc/2021/Conference/Paper3218/-/Official_Comment", "content": {"title": "Response to AnonReviewer2", "comment": "Dear AnonReviewer2,\n\nOur apologies for the misunderstanding, the paper was not updated until this moment. We have provided the answers and the updated version of the paper accordingly. We look forward to your feedback.\n\nBest,\nThe authors"}, "signatures": ["ICLR.cc/2021/Conference/Paper3218/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3218/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Regularization Cocktails for Tabular Datasets", "authorids": ["~Arlind_Kadra1", "~Marius_Lindauer1", "~Frank_Hutter1", "~Josif_Grabocka1"], "authors": ["Arlind Kadra", "Marius Lindauer", "Frank Hutter", "Josif Grabocka"], "keywords": ["deep learning", "regularization", "hyperparameter optimization", "benchmarks."], "abstract": "The regularization of prediction models is arguably the most crucial ingredient that allows Machine Learning solutions to generalize well on unseen data. Several types of regularization are popular in the Deep Learning community (e.g., weight decay, drop-out, early stopping, etc.), but so far these are selected on an ad-hoc basis, and there is no systematic study as to how different regularizers should be combined into the best \u201ccocktail\u201d. In this paper, we fill this gap, by considering the cocktails of 13 different regularization methods and framing the question of how to best combine them as a standard hyperparameter optimization problem. We perform a large-scale empirical study on 40 tabular datasets, concluding that, firstly, regularization cocktails substantially outperform individual regularization methods, even if the hyperparameters of the latter are carefully tuned; secondly, the optimal regularization cocktail depends on the dataset; and thirdly, regularization cocktails yield the state-of-the-art in classifying tabular datasets by outperforming Gradient-Boosted Decision Trees.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kadra|regularization_cocktails_for_tabular_datasets", "one-sentence_summary": "An empirical study on the optimal combination of regularization methods.", "supplementary_material": "/attachment/0d8d077b5f848be4ed2569148097971c9400b54e.zip", "pdf": "/pdf/7eebe22c4ab2ef082d3c49fdfdf7fc40e3e67547.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=OZDyJjhcdc", "_bibtex": "@misc{\nkadra2021regularization,\ntitle={Regularization Cocktails for Tabular Datasets},\nauthor={Arlind Kadra and Marius Lindauer and Frank Hutter and Josif Grabocka},\nyear={2021},\nurl={https://openreview.net/forum?id=2d34y5bRWxB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "2d34y5bRWxB", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3218/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3218/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3218/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3218/Authors|ICLR.cc/2021/Conference/Paper3218/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3218/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839847, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3218/-/Official_Comment"}}}, {"id": "xBn_qmqCYJp", "original": null, "number": 10, "cdate": 1606062965085, "ddate": null, "tcdate": 1606062965085, "tmdate": 1606062965085, "tddate": null, "forum": "2d34y5bRWxB", "replyto": "LerYO0J7jJ8", "invitation": "ICLR.cc/2021/Conference/Paper3218/-/Official_Comment", "content": {"title": "Quick response to the general idea of the comment", "comment": "I appreciate the detailed answer and the paper revision, which I need to evaluate in more detail and a bit more time for it.\n\nHowever, I would like to point out one misunderstanding between author's position and my position as a practitioner. \nI agree, that \"one should treat finding regularization as HyperOpt problem, and NOT solve it with Random/Grid search\" which paper proposes, can be seen as a novel idea.  It is also kind of in agreement with Sutton'`s \"bitter lesson\". \n\nHowever, I, as a practitioner, don't really like the idea \"in order to get good performance on the single dataset, you always should run an expensive HyperOpt search\". \nI mean, yes, that is probably true, but that is quite expensive, to say the least. \n\nIt also looks like that I am a wrong person to review AutoML papers because of the such mindset.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3218/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3218/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Regularization Cocktails for Tabular Datasets", "authorids": ["~Arlind_Kadra1", "~Marius_Lindauer1", "~Frank_Hutter1", "~Josif_Grabocka1"], "authors": ["Arlind Kadra", "Marius Lindauer", "Frank Hutter", "Josif Grabocka"], "keywords": ["deep learning", "regularization", "hyperparameter optimization", "benchmarks."], "abstract": "The regularization of prediction models is arguably the most crucial ingredient that allows Machine Learning solutions to generalize well on unseen data. Several types of regularization are popular in the Deep Learning community (e.g., weight decay, drop-out, early stopping, etc.), but so far these are selected on an ad-hoc basis, and there is no systematic study as to how different regularizers should be combined into the best \u201ccocktail\u201d. In this paper, we fill this gap, by considering the cocktails of 13 different regularization methods and framing the question of how to best combine them as a standard hyperparameter optimization problem. We perform a large-scale empirical study on 40 tabular datasets, concluding that, firstly, regularization cocktails substantially outperform individual regularization methods, even if the hyperparameters of the latter are carefully tuned; secondly, the optimal regularization cocktail depends on the dataset; and thirdly, regularization cocktails yield the state-of-the-art in classifying tabular datasets by outperforming Gradient-Boosted Decision Trees.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kadra|regularization_cocktails_for_tabular_datasets", "one-sentence_summary": "An empirical study on the optimal combination of regularization methods.", "supplementary_material": "/attachment/0d8d077b5f848be4ed2569148097971c9400b54e.zip", "pdf": "/pdf/7eebe22c4ab2ef082d3c49fdfdf7fc40e3e67547.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=OZDyJjhcdc", "_bibtex": "@misc{\nkadra2021regularization,\ntitle={Regularization Cocktails for Tabular Datasets},\nauthor={Arlind Kadra and Marius Lindauer and Frank Hutter and Josif Grabocka},\nyear={2021},\nurl={https://openreview.net/forum?id=2d34y5bRWxB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "2d34y5bRWxB", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3218/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3218/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3218/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3218/Authors|ICLR.cc/2021/Conference/Paper3218/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3218/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839847, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3218/-/Official_Comment"}}}, {"id": "LerYO0J7jJ8", "original": null, "number": 9, "cdate": 1606061218578, "ddate": null, "tcdate": 1606061218578, "tmdate": 1606061218578, "tddate": null, "forum": "2d34y5bRWxB", "replyto": "0a51bbW8kl-", "invitation": "ICLR.cc/2021/Conference/Paper3218/-/Official_Comment", "content": {"title": "Response to AnonReviewer2", "comment": "1) **\"... the recommendations are known to the practitioners, e.g. check any Kaggle winning solution https://www.kaggle.com/sudalairajkumar/winning-solutions-of-kaggle-competitions\"** : In our search we found no previous paper that tackles the same problem: discovering the best combination of regularizers in an automatic manner from a large pool of regularizers. However, we would be thankful if you might share any prior work (paper or published practice) where the authors automatically searched for the optimal combination of regularizers for deep learning models among a large set of regularizers, as presented in this study. We believe that apart from being the first paper that learns optimal regularization cocktails, we also provide ample evidence on the efficiency of our approach over a large-scale benchmark that features tabular datasets from different domains.\n2) **\"... no novelty in using hyperparameter optimization for the study\"** : The novelty is in rethinking the optimal fusion of different regularization methods as an instance of a HPO problem and demonstrating its empirical superiority against existing regularizers. In that sense, the actual HPO solving algorithm (BOHB in our case) is just a tool for solving our problem, in a similar way to e.g. using Adam+SGD as an optimization tool to train a new architecture, or a new loss function. However, we emphasize that the problem formulation is new, the overall solution is applied for the first time on the problem, and the empirical findings are strong and significant.\n3) **\"Neither software, nor dataset is proposed, the paper uses existing ones.\"** : We engineered a source code that selects the application of 13 regularizers to a neural network, which required extensive programming efforts and several additions to the AutoPytorch library, as mentioned in Section 4.1.\n4) **\"It is well known that the regularization/augmentation/ methods need to be tuned ... He et.al. (CVPR2019)\"** : The suggested reference is a collection of refinements (many of which are actually not regularization techniques), which have been suggested by the deep learning community for maximizing the generalization on Imagenet. That paper only summarizes a collection of some practices, however, it does not present a method that searches for the best combination of a large set of practices. In contrast, our method proposes an approach to automatically discover the optimal subset of regularization techniques for a specific dataset from a large set of available regularizers for neural networks. Therefore, we believe that our paper presents a novel approach in automatizing the combination of regularizers that to the best of awareness has never been tried and analyzed before. If the reviewer is aware of any previous paper that proposes a close idea, we would be glad to know. \n5) **\"... DropOut, MixUp, BatchNorm, are pretty universal.\"** : We found no \u201cuniversal\u201d regularization methods when used in combination as Experiment 2 in Section 5 shows. E.g. DropOut was present in only 35% of the dataset cocktails, hence was not selected in the cocktails of the 65% of the datasets. In fact, searching for optimal combinations of subsets of regularization techniques that are dataset-specific is a novel problem that this paper addresses for the first time.\n6) **\"...the less data you have, more regularization and bias is needed\"** : You are right on this point. Even though the experiment is correct and the outcome clear, still the result can be perceived as \u201cobvious\u201d. We agree with you (and other reviewers) and decided to drop this experiment to make room for the new results comparing the cocktails against Gradient Boosted Decision Trees (GBDT).\n7) **\"there is no systematic study as to how different regularizers should be combined\"** : Thanks for your comment and suggestions which we appreciate. Our answer to the question \u201cHow to combine regularizers?\u201d, is that the combination of regularizers should be treated as a Combined Algorithm Selection and Hyper-parameter (CASH) problem and solved specifically on each dataset. Unfortunately, because every optimal combination is dataset-dependent our experimental findings suggest there is no \u201cmagic recipe\u201d that is optimal on all datasets. We actually did provide an analysis of the most frequent regularizers within the best combinations (cocktail) of each dataset, as well as the frequent pairs of regularizers. Furthermore we compared against top-5 universal cocktails of strong, or frequent regularizers and found them to be sub-optimal w.r.t. our approach (pls notice the new experiments). The frequencies of cocktail ingredients show that there is no pair of methods whose interaction is optimal in more than 50% of the datasets (please see Figure 3 of the updated draft). So general recipes in the form of \u201calways use method A together with method B\u201d are unfortunately not evidenced by our large-scale experiment, as optimal combinations are highly  dataset-dependent.  "}, "signatures": ["ICLR.cc/2021/Conference/Paper3218/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3218/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Regularization Cocktails for Tabular Datasets", "authorids": ["~Arlind_Kadra1", "~Marius_Lindauer1", "~Frank_Hutter1", "~Josif_Grabocka1"], "authors": ["Arlind Kadra", "Marius Lindauer", "Frank Hutter", "Josif Grabocka"], "keywords": ["deep learning", "regularization", "hyperparameter optimization", "benchmarks."], "abstract": "The regularization of prediction models is arguably the most crucial ingredient that allows Machine Learning solutions to generalize well on unseen data. Several types of regularization are popular in the Deep Learning community (e.g., weight decay, drop-out, early stopping, etc.), but so far these are selected on an ad-hoc basis, and there is no systematic study as to how different regularizers should be combined into the best \u201ccocktail\u201d. In this paper, we fill this gap, by considering the cocktails of 13 different regularization methods and framing the question of how to best combine them as a standard hyperparameter optimization problem. We perform a large-scale empirical study on 40 tabular datasets, concluding that, firstly, regularization cocktails substantially outperform individual regularization methods, even if the hyperparameters of the latter are carefully tuned; secondly, the optimal regularization cocktail depends on the dataset; and thirdly, regularization cocktails yield the state-of-the-art in classifying tabular datasets by outperforming Gradient-Boosted Decision Trees.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kadra|regularization_cocktails_for_tabular_datasets", "one-sentence_summary": "An empirical study on the optimal combination of regularization methods.", "supplementary_material": "/attachment/0d8d077b5f848be4ed2569148097971c9400b54e.zip", "pdf": "/pdf/7eebe22c4ab2ef082d3c49fdfdf7fc40e3e67547.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=OZDyJjhcdc", "_bibtex": "@misc{\nkadra2021regularization,\ntitle={Regularization Cocktails for Tabular Datasets},\nauthor={Arlind Kadra and Marius Lindauer and Frank Hutter and Josif Grabocka},\nyear={2021},\nurl={https://openreview.net/forum?id=2d34y5bRWxB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "2d34y5bRWxB", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3218/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3218/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3218/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3218/Authors|ICLR.cc/2021/Conference/Paper3218/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3218/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839847, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3218/-/Official_Comment"}}}, {"id": "TDS8-zHKz9P", "original": null, "number": 8, "cdate": 1606059873387, "ddate": null, "tcdate": 1606059873387, "tmdate": 1606059873387, "tddate": null, "forum": "2d34y5bRWxB", "replyto": "p_2byJ0IY_0", "invitation": "ICLR.cc/2021/Conference/Paper3218/-/Official_Comment", "content": {"title": "Response to AnonReviewer3", "comment": "1) **\"All experiments are run on tabular data. I think it\u2019s fair to say that most practitioners who deal with regularization are interested in non tabular data ... I find it hard to imagine extrapolating results on tabular data to images, or text.\"** : It is true that a large section of the deep learning community focuses on computer vision, but there are plenty of application domains that have tabular data (medical domain, advertisement business, recommender systems, etc.). Furthermore, regularizing neural networks for tabular data is quite crucial because deep learning is not the state-of-the-art on tabular data and is outperformed by Gradient Boosted Decision Trees (GBDT). After carefully thinking about your point we decided that the best way to prove our point, is to show that by regularizing neural networks carefully we can achieve state-of-the-art and outperform GBDT. We ran extensive new results against GBDT with per-dataset tuned hyper-parameters and we are presenting them in Section 5 of the new paper draft. The findings show that the regularization cocktail outperforms Gradient Boosting with a significant margin. This is the very first time in our knowledge that neural networks outperform GBDT. In clear contrast to the belief of the community, the new experiments demonstrate that principally-regularized deep learning models can achieve state of the art in tabular data via a large-scale experimental protocol, which can have a game-changing impact on the perception that \u201cdeep learning doesn\u2019t work for tabular data\u201d.\n2) **\"The paper frames itself as a \u201cmethods\u201d paper more than an analysis paper, where the main claims revolve around the superiority of the regularization cocktail. In fact, the take-home messages given in the conclusion explicitly recommends the use of the regularization cocktail. I find this advice not very useful because: 1) the experiments were run on tabular data, 2) regularization cocktail is too expensive to justify the improvement (if any).\"** : Please find our response to the usage of tabular data above. Regarding the statement that the cocktail is too expensive to justify the improvement, we believe it does not correctly summarize the findings. On one hand the gain is statistically significant and the improvement against baselines is very considerable by any existing prior practice of comparing machine learning models. On the other hand, the HPO runs we presented were completed within a few days on CPU servers for large datasets with up to half a million instances. \n3) **\"The conclusions are trivial. It\u2019s quite obvious that a more general method always does at least as good as the method it subsumes, as long as the tuning of the parameters can be done sufficiently. The fact that the regularization cocktail does better than individual methods, or a combination of a few, is very believable in the tabular setting, since the tuning can be done sufficiently. The fact that more regularization is needed for smaller datasets is also well-known.\"** : To the best of our awareness there is no prior work which demonstrates that a per-dataset learned collection of regularizers outperforms common regularizers. Neither is the usage of per-dataset regularization cocktails a common practice by any prior paper we are aware of, nor it is a common practice by practitioners. We are actually happy that the reviewer found the outcome of our work \u201cbelievable\u201d and we would be glad to address any further technical criticism of our work.\n4) **\"The paper would be more useful with a similar experimental protocol applied to a non-tabular dataset ... but with a focus on analyzing trends (rather than highlighting the mixture method), or comparing with SOTA human designed cocktails.\"** : In addition to the points we responded above, we would like to further address the concern on the lack of practicality with the new evidence on the comparison against the state-of-the-art in tabular datasets. We demonstrate in Section 5 that the regularization cocktail makes neural networks outperform Gradient Boosting in tabular data, which to the best of our knowledge happens for the first time and changes the community perception that \u201cdeep learning does not work on tabular data\u201d. If the reviewer is happy with the rebuttal, but s/he conditions his change of opinion on making it clear that the paper is focused on tabular data, then we offer to change the title to \u201cRegularization Cocktails on Tabular Datasets\u201d. \n5) **\"For the experiments corresponding to Figure 5, I wonder what the gap would be like for the best performing individual regularizer. I am curious because I think for a small dataset, the need for a state of the art regularizer diminishes, and just one good regularizer suffices.\"** : We agree that although this experiment is technically correct, it does not show a novel finding (the other reviewers agree with you). So we decided to drop it from the paper and make room for presenting the comparison to GBDT."}, "signatures": ["ICLR.cc/2021/Conference/Paper3218/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3218/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Regularization Cocktails for Tabular Datasets", "authorids": ["~Arlind_Kadra1", "~Marius_Lindauer1", "~Frank_Hutter1", "~Josif_Grabocka1"], "authors": ["Arlind Kadra", "Marius Lindauer", "Frank Hutter", "Josif Grabocka"], "keywords": ["deep learning", "regularization", "hyperparameter optimization", "benchmarks."], "abstract": "The regularization of prediction models is arguably the most crucial ingredient that allows Machine Learning solutions to generalize well on unseen data. Several types of regularization are popular in the Deep Learning community (e.g., weight decay, drop-out, early stopping, etc.), but so far these are selected on an ad-hoc basis, and there is no systematic study as to how different regularizers should be combined into the best \u201ccocktail\u201d. In this paper, we fill this gap, by considering the cocktails of 13 different regularization methods and framing the question of how to best combine them as a standard hyperparameter optimization problem. We perform a large-scale empirical study on 40 tabular datasets, concluding that, firstly, regularization cocktails substantially outperform individual regularization methods, even if the hyperparameters of the latter are carefully tuned; secondly, the optimal regularization cocktail depends on the dataset; and thirdly, regularization cocktails yield the state-of-the-art in classifying tabular datasets by outperforming Gradient-Boosted Decision Trees.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kadra|regularization_cocktails_for_tabular_datasets", "one-sentence_summary": "An empirical study on the optimal combination of regularization methods.", "supplementary_material": "/attachment/0d8d077b5f848be4ed2569148097971c9400b54e.zip", "pdf": "/pdf/7eebe22c4ab2ef082d3c49fdfdf7fc40e3e67547.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=OZDyJjhcdc", "_bibtex": "@misc{\nkadra2021regularization,\ntitle={Regularization Cocktails for Tabular Datasets},\nauthor={Arlind Kadra and Marius Lindauer and Frank Hutter and Josif Grabocka},\nyear={2021},\nurl={https://openreview.net/forum?id=2d34y5bRWxB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "2d34y5bRWxB", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3218/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3218/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3218/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3218/Authors|ICLR.cc/2021/Conference/Paper3218/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3218/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839847, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3218/-/Official_Comment"}}}, {"id": "6o7lyTw6-G1", "original": null, "number": 2, "cdate": 1606040153516, "ddate": null, "tcdate": 1606040153516, "tmdate": 1606040153516, "tddate": null, "forum": "2d34y5bRWxB", "replyto": "2d34y5bRWxB", "invitation": "ICLR.cc/2021/Conference/Paper3218/-/Official_Comment", "content": {"title": "Answers/rebuttal?", "comment": "Dear authors, \n\nI see that you have updated the paper (\"28 Sep 2020 (modified: 11 Nov 2020)\"). \nCould you please summarize the changes and answer the questions in reviews?\n\nBest, R2"}, "signatures": ["ICLR.cc/2021/Conference/Paper3218/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3218/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Regularization Cocktails for Tabular Datasets", "authorids": ["~Arlind_Kadra1", "~Marius_Lindauer1", "~Frank_Hutter1", "~Josif_Grabocka1"], "authors": ["Arlind Kadra", "Marius Lindauer", "Frank Hutter", "Josif Grabocka"], "keywords": ["deep learning", "regularization", "hyperparameter optimization", "benchmarks."], "abstract": "The regularization of prediction models is arguably the most crucial ingredient that allows Machine Learning solutions to generalize well on unseen data. Several types of regularization are popular in the Deep Learning community (e.g., weight decay, drop-out, early stopping, etc.), but so far these are selected on an ad-hoc basis, and there is no systematic study as to how different regularizers should be combined into the best \u201ccocktail\u201d. In this paper, we fill this gap, by considering the cocktails of 13 different regularization methods and framing the question of how to best combine them as a standard hyperparameter optimization problem. We perform a large-scale empirical study on 40 tabular datasets, concluding that, firstly, regularization cocktails substantially outperform individual regularization methods, even if the hyperparameters of the latter are carefully tuned; secondly, the optimal regularization cocktail depends on the dataset; and thirdly, regularization cocktails yield the state-of-the-art in classifying tabular datasets by outperforming Gradient-Boosted Decision Trees.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kadra|regularization_cocktails_for_tabular_datasets", "one-sentence_summary": "An empirical study on the optimal combination of regularization methods.", "supplementary_material": "/attachment/0d8d077b5f848be4ed2569148097971c9400b54e.zip", "pdf": "/pdf/7eebe22c4ab2ef082d3c49fdfdf7fc40e3e67547.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=OZDyJjhcdc", "_bibtex": "@misc{\nkadra2021regularization,\ntitle={Regularization Cocktails for Tabular Datasets},\nauthor={Arlind Kadra and Marius Lindauer and Frank Hutter and Josif Grabocka},\nyear={2021},\nurl={https://openreview.net/forum?id=2d34y5bRWxB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "2d34y5bRWxB", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3218/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3218/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3218/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3218/Authors|ICLR.cc/2021/Conference/Paper3218/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3218/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839847, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3218/-/Official_Comment"}}}], "count": 17}