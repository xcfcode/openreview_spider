{"notes": [{"id": "Byg_vREtvB", "original": "rJeNO_PdDr", "number": 1180, "cdate": 1569439327888, "ddate": null, "tcdate": 1569439327888, "tmdate": 1577168263358, "tddate": null, "forum": "Byg_vREtvB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Generalized Bayesian Posterior Expectation Distillation for Deep Neural Networks", "authors": ["Meet P. Vadera", "Benjamin M. Marlin"], "authorids": ["mvadera@cs.umass.edu", "marlin@cs.umass.edu"], "keywords": ["Bayesian Neural Networks", "Distillation"], "TL;DR": "A general framework for distilling Bayesian posterior expectations for deep neural networks.", "abstract": "In this paper, we present a general framework for distilling expectations with respect to the Bayesian posterior distribution of a deep neural network, significantly extending prior work on a method known as ``Bayesian Dark Knowledge.\"  Our generalized framework applies to the case of classification models and takes as input the architecture of a ``teacher\" network, a general posterior expectation of interest, and the architecture of a ``student\" network. The distillation method performs an online compression of the selected posterior expectation using iteratively generated Monte Carlo samples from the parameter posterior of the teacher model. We further consider the problem of optimizing the student model architecture with respect to an accuracy-speed-storage trade-off. We present experimental results investigating multiple data sets, distillation targets,  teacher model architectures, and approaches to searching for student model architectures. We establish the key result that distilling into a student model with an architecture that matches the teacher, as is done in Bayesian Dark Knowledge, can lead to sub-optimal performance. Lastly, we show that student architecture search methods can identify student models with significantly improved performance. ", "pdf": "/pdf/5db49d69762684595f766d570924fc6c30047939.pdf", "paperhash": "vadera|generalized_bayesian_posterior_expectation_distillation_for_deep_neural_networks", "original_pdf": "/attachment/8568286652bc83162cb98ff0ad9e557219391d5c.pdf", "_bibtex": "@misc{\nvadera2020generalized,\ntitle={Generalized Bayesian Posterior Expectation Distillation for Deep Neural Networks},\nauthor={Meet P. Vadera and Benjamin M. Marlin},\nyear={2020},\nurl={https://openreview.net/forum?id=Byg_vREtvB}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "wJ06WN4M2", "original": null, "number": 1, "cdate": 1576798716658, "ddate": null, "tcdate": 1576798716658, "tmdate": 1576800919864, "tddate": null, "forum": "Byg_vREtvB", "replyto": "Byg_vREtvB", "invitation": "ICLR.cc/2020/Conference/Paper1180/-/Decision", "content": {"decision": "Reject", "comment": "The authors consider distilling posterior expectations for Bayesian neural networks. While reviewers found the material interesting, and the responses thoughtful, there were questions about the practical utility of the work. Evaluations of classification favour NLL (and typically do not show accuracy), and regression (which was considered in the original Bayesian Dark Knowledge paper) is not considered. In general, it is difficult to assess and interpret how the approach is working, and in what application regime it would be a gold standard, e.g., with respect to downstream tasks. The authors are encouraged to continue with this work, taking reviewer comments into account in a final version.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generalized Bayesian Posterior Expectation Distillation for Deep Neural Networks", "authors": ["Meet P. Vadera", "Benjamin M. Marlin"], "authorids": ["mvadera@cs.umass.edu", "marlin@cs.umass.edu"], "keywords": ["Bayesian Neural Networks", "Distillation"], "TL;DR": "A general framework for distilling Bayesian posterior expectations for deep neural networks.", "abstract": "In this paper, we present a general framework for distilling expectations with respect to the Bayesian posterior distribution of a deep neural network, significantly extending prior work on a method known as ``Bayesian Dark Knowledge.\"  Our generalized framework applies to the case of classification models and takes as input the architecture of a ``teacher\" network, a general posterior expectation of interest, and the architecture of a ``student\" network. The distillation method performs an online compression of the selected posterior expectation using iteratively generated Monte Carlo samples from the parameter posterior of the teacher model. We further consider the problem of optimizing the student model architecture with respect to an accuracy-speed-storage trade-off. We present experimental results investigating multiple data sets, distillation targets,  teacher model architectures, and approaches to searching for student model architectures. We establish the key result that distilling into a student model with an architecture that matches the teacher, as is done in Bayesian Dark Knowledge, can lead to sub-optimal performance. Lastly, we show that student architecture search methods can identify student models with significantly improved performance. ", "pdf": "/pdf/5db49d69762684595f766d570924fc6c30047939.pdf", "paperhash": "vadera|generalized_bayesian_posterior_expectation_distillation_for_deep_neural_networks", "original_pdf": "/attachment/8568286652bc83162cb98ff0ad9e557219391d5c.pdf", "_bibtex": "@misc{\nvadera2020generalized,\ntitle={Generalized Bayesian Posterior Expectation Distillation for Deep Neural Networks},\nauthor={Meet P. Vadera and Benjamin M. Marlin},\nyear={2020},\nurl={https://openreview.net/forum?id=Byg_vREtvB}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "Byg_vREtvB", "replyto": "Byg_vREtvB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795704489, "tmdate": 1576800252076, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1180/-/Decision"}}}, {"id": "HJeLOilLtr", "original": null, "number": 1, "cdate": 1571322733685, "ddate": null, "tcdate": 1571322733685, "tmdate": 1574187388805, "tddate": null, "forum": "Byg_vREtvB", "replyto": "Byg_vREtvB", "invitation": "ICLR.cc/2020/Conference/Paper1180/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #3", "review": "Contributions:\n\nThe paper considers the distillation of a Bayesian neural network as presented in [Balan et al. 2015]\n\nThe main contribution of the paper is the extension of [Balan et al. 2015] to apply to general posterior expectations instead of being restricted to predictions. \n\nA second contribution of the paper is the finding that restricting the architecture of the student network to coincide with the teacher can lead to suboptimal performance and this can be mitigated by expanding the student's architecture using architecture search.\n\nOriginality/Significance:\n\nI want to discuss the result regarding the generalization of the posterior expectation (section 3.1). To my knowledge this is novel, however, I am failing to see the importance of this result. The paper mentions two cases as motivations: to calculate the entropy and the variance of the marginal. The  problem with these two examples is that it is unclear why these are important and they are not used in the experiments anywhere. Their use should be motivated and the performance of the distillation should be properly evaluated in the experiments.\n\nThe result regarding architecture search is interesting, but it should be expanded and explained more to be a main contribution.\n\nClarity:\n\nThe paper generally understandable, and well written, but it could be better organized. It should expand on the motivation and the architecture search, since these are key components of the paper. The figures are not legible. Even fully zoomed in, they are difficult to read.\n\nOverall assessment:\n\nThe paper has some interesting ideas, but it lacks motivation and significant results.\n\n_______________________________________________________________________\n\nResponse to the rebuttal:\n\nThank you for the detailed reply.\n\n> A primary motivation for generalising posterior expectations is to help quantify model uncertainty. Indeed, the expectation of the predictive entropy is an important quantity that is distinct from the entropy of the posterior predictive distribution. For instance, the difference between these two quantities is exactly the BALD score used in active learning [1]. ...\n\nBALD would be problematic to use with this framework due to computational costs. The main benefit of distillation is the reduced computational cost at inference time. But training itself is still expensive. In BALD, the bulk of the computational cost is fitting the model after each new observation. Distillation does not provide a speedup here.\n\nIf the method indeed works well in an active learning setting, it would be interesting to see experiments showcasing this result.\n\n> - The result regarding architecture search is interesting, but it should be expanded and explained more to be a main contribution.\n\nI was hoping for more discussion/guidance on finding the right architecture or perhaps an algorithm that efficiently optimises the architecture. But I understand that this is more of a future work so I am not holding this against the paper.\n\nI realise that my initial assessment was rather short so I decided to increase my rating and lower my confidence score. I think the paper is borderline, but I am slightly leaning towards rejection due to the insufficient motivation.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1180/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1180/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generalized Bayesian Posterior Expectation Distillation for Deep Neural Networks", "authors": ["Meet P. Vadera", "Benjamin M. Marlin"], "authorids": ["mvadera@cs.umass.edu", "marlin@cs.umass.edu"], "keywords": ["Bayesian Neural Networks", "Distillation"], "TL;DR": "A general framework for distilling Bayesian posterior expectations for deep neural networks.", "abstract": "In this paper, we present a general framework for distilling expectations with respect to the Bayesian posterior distribution of a deep neural network, significantly extending prior work on a method known as ``Bayesian Dark Knowledge.\"  Our generalized framework applies to the case of classification models and takes as input the architecture of a ``teacher\" network, a general posterior expectation of interest, and the architecture of a ``student\" network. The distillation method performs an online compression of the selected posterior expectation using iteratively generated Monte Carlo samples from the parameter posterior of the teacher model. We further consider the problem of optimizing the student model architecture with respect to an accuracy-speed-storage trade-off. We present experimental results investigating multiple data sets, distillation targets,  teacher model architectures, and approaches to searching for student model architectures. We establish the key result that distilling into a student model with an architecture that matches the teacher, as is done in Bayesian Dark Knowledge, can lead to sub-optimal performance. Lastly, we show that student architecture search methods can identify student models with significantly improved performance. ", "pdf": "/pdf/5db49d69762684595f766d570924fc6c30047939.pdf", "paperhash": "vadera|generalized_bayesian_posterior_expectation_distillation_for_deep_neural_networks", "original_pdf": "/attachment/8568286652bc83162cb98ff0ad9e557219391d5c.pdf", "_bibtex": "@misc{\nvadera2020generalized,\ntitle={Generalized Bayesian Posterior Expectation Distillation for Deep Neural Networks},\nauthor={Meet P. Vadera and Benjamin M. Marlin},\nyear={2020},\nurl={https://openreview.net/forum?id=Byg_vREtvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Byg_vREtvB", "replyto": "Byg_vREtvB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1180/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1180/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575677737845, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1180/Reviewers"], "noninvitees": [], "tcdate": 1570237741176, "tmdate": 1575677737857, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1180/-/Official_Review"}}}, {"id": "SJgR0mt2sH", "original": null, "number": 8, "cdate": 1573848022099, "ddate": null, "tcdate": 1573848022099, "tmdate": 1573850501857, "tddate": null, "forum": "Byg_vREtvB", "replyto": "rkxHN4UXjr", "invitation": "ICLR.cc/2020/Conference/Paper1180/-/Official_Comment", "content": {"title": "Requesting clarification on the architecture search comment", "comment": "As the rebuttal period is ending, we would like to note that we would be happy to address your concerns about the architecture search section of the paper if you can provide more detail in your final review."}, "signatures": ["ICLR.cc/2020/Conference/Paper1180/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1180/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generalized Bayesian Posterior Expectation Distillation for Deep Neural Networks", "authors": ["Meet P. Vadera", "Benjamin M. Marlin"], "authorids": ["mvadera@cs.umass.edu", "marlin@cs.umass.edu"], "keywords": ["Bayesian Neural Networks", "Distillation"], "TL;DR": "A general framework for distilling Bayesian posterior expectations for deep neural networks.", "abstract": "In this paper, we present a general framework for distilling expectations with respect to the Bayesian posterior distribution of a deep neural network, significantly extending prior work on a method known as ``Bayesian Dark Knowledge.\"  Our generalized framework applies to the case of classification models and takes as input the architecture of a ``teacher\" network, a general posterior expectation of interest, and the architecture of a ``student\" network. The distillation method performs an online compression of the selected posterior expectation using iteratively generated Monte Carlo samples from the parameter posterior of the teacher model. We further consider the problem of optimizing the student model architecture with respect to an accuracy-speed-storage trade-off. We present experimental results investigating multiple data sets, distillation targets,  teacher model architectures, and approaches to searching for student model architectures. We establish the key result that distilling into a student model with an architecture that matches the teacher, as is done in Bayesian Dark Knowledge, can lead to sub-optimal performance. Lastly, we show that student architecture search methods can identify student models with significantly improved performance. ", "pdf": "/pdf/5db49d69762684595f766d570924fc6c30047939.pdf", "paperhash": "vadera|generalized_bayesian_posterior_expectation_distillation_for_deep_neural_networks", "original_pdf": "/attachment/8568286652bc83162cb98ff0ad9e557219391d5c.pdf", "_bibtex": "@misc{\nvadera2020generalized,\ntitle={Generalized Bayesian Posterior Expectation Distillation for Deep Neural Networks},\nauthor={Meet P. Vadera and Benjamin M. Marlin},\nyear={2020},\nurl={https://openreview.net/forum?id=Byg_vREtvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Byg_vREtvB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1180/Authors", "ICLR.cc/2020/Conference/Paper1180/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1180/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1180/Reviewers", "ICLR.cc/2020/Conference/Paper1180/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1180/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1180/Authors|ICLR.cc/2020/Conference/Paper1180/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504160004, "tmdate": 1576860541851, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1180/Authors", "ICLR.cc/2020/Conference/Paper1180/Reviewers", "ICLR.cc/2020/Conference/Paper1180/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1180/-/Official_Comment"}}}, {"id": "Hkx8oW7nsH", "original": null, "number": 6, "cdate": 1573822877561, "ddate": null, "tcdate": 1573822877561, "tmdate": 1573848345352, "tddate": null, "forum": "Byg_vREtvB", "replyto": "Byg_vREtvB", "invitation": "ICLR.cc/2020/Conference/Paper1180/-/Official_Comment", "content": {"title": "Revision Summary", "comment": "Based on the useful feedback given by the reviewers, we have made multiple revisions to the paper. Below we summarize the changes between the version that was reviewed and the final version that we have now.\n\n1) There were concerns about the readability of the figures. We have updated the paper to ensure that all figures are legible at print resolution. \n\n2) Reviewer #1 wanted us to report the performance comparison between Uo and Us for different targets and also highlight the additional storage and computation cost required for Uo. We ran additional experiments to compare Uo and Us for both the distillation targets on all model-data set combinations. The results from these experiments are present in the Appendix (Tables 2-4). We have also mentioned explicitly in our paper that the additional storage and computation requirement for Uo grows linearly in the number of training set instances available for the student model. \n\n3) Reviewer #3 felt that the motivation behind choosing our distillation targets was not explained clearly. We have responded to reviewer #3 and also updated our introduction section to expand on the motivation.\n\n4) Reviewer #1 also suggested mentioning explicitly how large the optimal student model was after architecture search w.r.t the base student model. We have updated all the figures which had the performance-computation-storage tradeoff plots to include this information.  \n\nIn addition to these revisions, we have also addressed the concerns of each of the reviewers in the individual responses posted."}, "signatures": ["ICLR.cc/2020/Conference/Paper1180/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1180/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generalized Bayesian Posterior Expectation Distillation for Deep Neural Networks", "authors": ["Meet P. Vadera", "Benjamin M. Marlin"], "authorids": ["mvadera@cs.umass.edu", "marlin@cs.umass.edu"], "keywords": ["Bayesian Neural Networks", "Distillation"], "TL;DR": "A general framework for distilling Bayesian posterior expectations for deep neural networks.", "abstract": "In this paper, we present a general framework for distilling expectations with respect to the Bayesian posterior distribution of a deep neural network, significantly extending prior work on a method known as ``Bayesian Dark Knowledge.\"  Our generalized framework applies to the case of classification models and takes as input the architecture of a ``teacher\" network, a general posterior expectation of interest, and the architecture of a ``student\" network. The distillation method performs an online compression of the selected posterior expectation using iteratively generated Monte Carlo samples from the parameter posterior of the teacher model. We further consider the problem of optimizing the student model architecture with respect to an accuracy-speed-storage trade-off. We present experimental results investigating multiple data sets, distillation targets,  teacher model architectures, and approaches to searching for student model architectures. We establish the key result that distilling into a student model with an architecture that matches the teacher, as is done in Bayesian Dark Knowledge, can lead to sub-optimal performance. Lastly, we show that student architecture search methods can identify student models with significantly improved performance. ", "pdf": "/pdf/5db49d69762684595f766d570924fc6c30047939.pdf", "paperhash": "vadera|generalized_bayesian_posterior_expectation_distillation_for_deep_neural_networks", "original_pdf": "/attachment/8568286652bc83162cb98ff0ad9e557219391d5c.pdf", "_bibtex": "@misc{\nvadera2020generalized,\ntitle={Generalized Bayesian Posterior Expectation Distillation for Deep Neural Networks},\nauthor={Meet P. Vadera and Benjamin M. Marlin},\nyear={2020},\nurl={https://openreview.net/forum?id=Byg_vREtvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Byg_vREtvB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1180/Authors", "ICLR.cc/2020/Conference/Paper1180/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1180/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1180/Reviewers", "ICLR.cc/2020/Conference/Paper1180/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1180/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1180/Authors|ICLR.cc/2020/Conference/Paper1180/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504160004, "tmdate": 1576860541851, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1180/Authors", "ICLR.cc/2020/Conference/Paper1180/Reviewers", "ICLR.cc/2020/Conference/Paper1180/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1180/-/Official_Comment"}}}, {"id": "r1xgFEvWqH", "original": null, "number": 3, "cdate": 1572070520233, "ddate": null, "tcdate": 1572070520233, "tmdate": 1573829817227, "tddate": null, "forum": "Byg_vREtvB", "replyto": "Byg_vREtvB", "invitation": "ICLR.cc/2020/Conference/Paper1180/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #2", "review": "Summary:\nThe paper introduces a general framework for distilling expectations of the Bayesian posterior distribution of a deep neural network, aiming to extend the original Bayesian Dark Knowledge approach [1]. More concretely, the generalized framework takes as input a teacher network, a general posterior expectation of interest, a student network, and thus performs an online compression of the selected posterior expectation using iteratively generated Monte Carlo samples from the parameter posterior of the teacher model. The proposed framework is applied to the case of classification models and empirical results demonstrate that distilling into a student model with an architecture that matches the teacher, as is done in Bayesian Dark Knowledge, can lead to sub-optimal performance. It is also shown that student architecture search methods can identify student models with significantly improved speed-storage-accuracy trade-offs.\n\nStrengths:\nOverall, the paper is well written and the relationship to previous works is well described. I personally like the Bayesian Dark Knowledge approach, which combines SGLD and knowledge distillation or dark knowledge, and very happy to see its generalization. Unlike the previous work, it is clearly shown that restricting the student architecture to match the teacher can sometimes lead to a significant performance drop, which provides a basis for guiding future developments.\n\nWeaknesses:\n- I think it is a valuable contribution, but my major concern is that the authors only conduct experiments for the classification task, whereas the original Bayesian Dark Knowledge approach also deals with the regression task and shows some interesting results (see Sect. 3.2 and 3.3 in Ref. [1]). I would recommend the authors to extend the experimental evaluation and provide some insight on how to extend the proposed framework to cover the regression task.\n- On page 5, the choice of loss function does not seem to be discussed. I would like the authors to clarify why cross entropy loss is replaced with l(h, h\u2019)=|h-h\u2019| in the classification case.\n- The size of some figures appears too small, for example Fig. 1 and Fig. 2, which may hinder readability.\n\nAt the moment, I recommend a weak reject as the main weakness is the experimental evaluation, but I could be open to increasing my score if my concerns are addressed.\n\nReferences:\n[1] Anoop Korattikara Balan, Vivek Rathod, Kevin P Murphy, and Max Welling. Bayesian dark knowledge. In Advances in Neural Information Processing Systems, pp. 3438\u20133446, 2015.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1180/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1180/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generalized Bayesian Posterior Expectation Distillation for Deep Neural Networks", "authors": ["Meet P. Vadera", "Benjamin M. Marlin"], "authorids": ["mvadera@cs.umass.edu", "marlin@cs.umass.edu"], "keywords": ["Bayesian Neural Networks", "Distillation"], "TL;DR": "A general framework for distilling Bayesian posterior expectations for deep neural networks.", "abstract": "In this paper, we present a general framework for distilling expectations with respect to the Bayesian posterior distribution of a deep neural network, significantly extending prior work on a method known as ``Bayesian Dark Knowledge.\"  Our generalized framework applies to the case of classification models and takes as input the architecture of a ``teacher\" network, a general posterior expectation of interest, and the architecture of a ``student\" network. The distillation method performs an online compression of the selected posterior expectation using iteratively generated Monte Carlo samples from the parameter posterior of the teacher model. We further consider the problem of optimizing the student model architecture with respect to an accuracy-speed-storage trade-off. We present experimental results investigating multiple data sets, distillation targets,  teacher model architectures, and approaches to searching for student model architectures. We establish the key result that distilling into a student model with an architecture that matches the teacher, as is done in Bayesian Dark Knowledge, can lead to sub-optimal performance. Lastly, we show that student architecture search methods can identify student models with significantly improved performance. ", "pdf": "/pdf/5db49d69762684595f766d570924fc6c30047939.pdf", "paperhash": "vadera|generalized_bayesian_posterior_expectation_distillation_for_deep_neural_networks", "original_pdf": "/attachment/8568286652bc83162cb98ff0ad9e557219391d5c.pdf", "_bibtex": "@misc{\nvadera2020generalized,\ntitle={Generalized Bayesian Posterior Expectation Distillation for Deep Neural Networks},\nauthor={Meet P. Vadera and Benjamin M. Marlin},\nyear={2020},\nurl={https://openreview.net/forum?id=Byg_vREtvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Byg_vREtvB", "replyto": "Byg_vREtvB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1180/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1180/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575677737845, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1180/Reviewers"], "noninvitees": [], "tcdate": 1570237741176, "tmdate": 1575677737857, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1180/-/Official_Review"}}}, {"id": "Byg5145dir", "original": null, "number": 4, "cdate": 1573589986061, "ddate": null, "tcdate": 1573589986061, "tmdate": 1573591184023, "tddate": null, "forum": "Byg_vREtvB", "replyto": "Sklkc14ZjH", "invitation": "ICLR.cc/2020/Conference/Paper1180/-/Official_Comment", "content": {"title": "[Contd.] Response to Review#1", "comment": "Thank you for your patience. Our response to your additional comment is given below.\n\n+ Us vs Uo estimators: It would be interesting to more clearly see what the additional storage (and computation) of Uo is buying us. How much worse are the posterior predictive entropies if Uo is switched with Us? And do the posterior predictive estimates improve if Uo is used in place of Us? \n\n> The additional storage and computation required by the Uo estimator will be O(N), where N is the total number of cases in our training set used for training the student. To illustrate the effect of using Uo vs Us, we ran additional experiments on CIFAR10 with the same experimental configurations used in Figure 1(f). The results are reported below:\n\nNumber of training samples for teacher | MAE (Entropy) - Uo | MAE (Entropy) - Us\n                   10000                                           |             0.144              |             0.192\n                   20000                                           |             0.210              |             0.231\n                   50000                                           |             0.245               |             0.290\n\nNote that the results for Uo in the table above are already present in Figure 1(f). As we can see, Uo achieves a lower mean absolute error (about 15% - 25% lower) while distilling expectation of entropy. We will also run the same set of experiments for both the models on MNIST and include them in the supplemental section of the paper. "}, "signatures": ["ICLR.cc/2020/Conference/Paper1180/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1180/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generalized Bayesian Posterior Expectation Distillation for Deep Neural Networks", "authors": ["Meet P. Vadera", "Benjamin M. Marlin"], "authorids": ["mvadera@cs.umass.edu", "marlin@cs.umass.edu"], "keywords": ["Bayesian Neural Networks", "Distillation"], "TL;DR": "A general framework for distilling Bayesian posterior expectations for deep neural networks.", "abstract": "In this paper, we present a general framework for distilling expectations with respect to the Bayesian posterior distribution of a deep neural network, significantly extending prior work on a method known as ``Bayesian Dark Knowledge.\"  Our generalized framework applies to the case of classification models and takes as input the architecture of a ``teacher\" network, a general posterior expectation of interest, and the architecture of a ``student\" network. The distillation method performs an online compression of the selected posterior expectation using iteratively generated Monte Carlo samples from the parameter posterior of the teacher model. We further consider the problem of optimizing the student model architecture with respect to an accuracy-speed-storage trade-off. We present experimental results investigating multiple data sets, distillation targets,  teacher model architectures, and approaches to searching for student model architectures. We establish the key result that distilling into a student model with an architecture that matches the teacher, as is done in Bayesian Dark Knowledge, can lead to sub-optimal performance. Lastly, we show that student architecture search methods can identify student models with significantly improved performance. ", "pdf": "/pdf/5db49d69762684595f766d570924fc6c30047939.pdf", "paperhash": "vadera|generalized_bayesian_posterior_expectation_distillation_for_deep_neural_networks", "original_pdf": "/attachment/8568286652bc83162cb98ff0ad9e557219391d5c.pdf", "_bibtex": "@misc{\nvadera2020generalized,\ntitle={Generalized Bayesian Posterior Expectation Distillation for Deep Neural Networks},\nauthor={Meet P. Vadera and Benjamin M. Marlin},\nyear={2020},\nurl={https://openreview.net/forum?id=Byg_vREtvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Byg_vREtvB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1180/Authors", "ICLR.cc/2020/Conference/Paper1180/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1180/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1180/Reviewers", "ICLR.cc/2020/Conference/Paper1180/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1180/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1180/Authors|ICLR.cc/2020/Conference/Paper1180/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504160004, "tmdate": 1576860541851, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1180/Authors", "ICLR.cc/2020/Conference/Paper1180/Reviewers", "ICLR.cc/2020/Conference/Paper1180/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1180/-/Official_Comment"}}}, {"id": "SygMPVcuiB", "original": null, "number": 5, "cdate": 1573590105799, "ddate": null, "tcdate": 1573590105799, "tmdate": 1573590948248, "tddate": null, "forum": "Byg_vREtvB", "replyto": "B1edOTmboB", "invitation": "ICLR.cc/2020/Conference/Paper1180/-/Official_Comment", "content": {"title": "[Contd.] Response to Review#2", "comment": "Thank you for your patience. Our response to your additional comment is given below. \n\n- I think it is a valuable contribution, but my major concern is that the authors only conduct experiments for the classification task, whereas the original Bayesian Dark Knowledge approach also deals with the regression task and shows some interesting results (see Sect. 3.2 and 3.3 in Ref. [1]). I would recommend the authors to extend the experimental evaluation and provide some insight on how to extend the proposed framework to cover the regression task.\n\n> We agree with the reviewer that the regression results in the original Bayesian Dark Knowledge paper [1] are interesting. However, the current paper is explicitly framed to focus only on the classification task to enable a much deeper exploration of the method than was performed in the original paper. Indeed, this exploration has revealed fundamental issues with the assumptions of the original method (e.g., that it is possible to compress the posterior for a network back into a student network of the same size), and we have provided and evaluated solutions to this problem via methods to configure the student architecture. We have also extended the original work in a different direction by studying the distillation of generalized posterior expectations specifically in the classification setting. Together, we feel that our existing results as presented in the current paper will provide significant value to the community. \n\nReferences:\n[1] Anoop Korattikara Balan, Vivek Rathod, Kevin P Murphy, and Max Welling. Bayesian dark knowledge. In Advances in Neural Information Processing Systems, pp. 3438\u20133446, 2015.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1180/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1180/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generalized Bayesian Posterior Expectation Distillation for Deep Neural Networks", "authors": ["Meet P. Vadera", "Benjamin M. Marlin"], "authorids": ["mvadera@cs.umass.edu", "marlin@cs.umass.edu"], "keywords": ["Bayesian Neural Networks", "Distillation"], "TL;DR": "A general framework for distilling Bayesian posterior expectations for deep neural networks.", "abstract": "In this paper, we present a general framework for distilling expectations with respect to the Bayesian posterior distribution of a deep neural network, significantly extending prior work on a method known as ``Bayesian Dark Knowledge.\"  Our generalized framework applies to the case of classification models and takes as input the architecture of a ``teacher\" network, a general posterior expectation of interest, and the architecture of a ``student\" network. The distillation method performs an online compression of the selected posterior expectation using iteratively generated Monte Carlo samples from the parameter posterior of the teacher model. We further consider the problem of optimizing the student model architecture with respect to an accuracy-speed-storage trade-off. We present experimental results investigating multiple data sets, distillation targets,  teacher model architectures, and approaches to searching for student model architectures. We establish the key result that distilling into a student model with an architecture that matches the teacher, as is done in Bayesian Dark Knowledge, can lead to sub-optimal performance. Lastly, we show that student architecture search methods can identify student models with significantly improved performance. ", "pdf": "/pdf/5db49d69762684595f766d570924fc6c30047939.pdf", "paperhash": "vadera|generalized_bayesian_posterior_expectation_distillation_for_deep_neural_networks", "original_pdf": "/attachment/8568286652bc83162cb98ff0ad9e557219391d5c.pdf", "_bibtex": "@misc{\nvadera2020generalized,\ntitle={Generalized Bayesian Posterior Expectation Distillation for Deep Neural Networks},\nauthor={Meet P. Vadera and Benjamin M. Marlin},\nyear={2020},\nurl={https://openreview.net/forum?id=Byg_vREtvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Byg_vREtvB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1180/Authors", "ICLR.cc/2020/Conference/Paper1180/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1180/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1180/Reviewers", "ICLR.cc/2020/Conference/Paper1180/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1180/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1180/Authors|ICLR.cc/2020/Conference/Paper1180/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504160004, "tmdate": 1576860541851, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1180/Authors", "ICLR.cc/2020/Conference/Paper1180/Reviewers", "ICLR.cc/2020/Conference/Paper1180/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1180/-/Official_Comment"}}}, {"id": "rkxHN4UXjr", "original": null, "number": 3, "cdate": 1573245997479, "ddate": null, "tcdate": 1573245997479, "tmdate": 1573246050239, "tddate": null, "forum": "Byg_vREtvB", "replyto": "HJeLOilLtr", "invitation": "ICLR.cc/2020/Conference/Paper1180/-/Official_Comment", "content": {"title": "Response to Review#3", "comment": "Thank you for taking the time out to provide feedback on our work. Below we address the concerns raised in the review:\n\n\n- I want to discuss the result regarding the generalization of the posterior expectation (section 3.1). To my knowledge this is novel, however, I am failing to see the importance of this result. The paper mentions two cases as motivations: to calculate the entropy and the variance of the marginal. The problem with these two examples is that it is unclear why these are important and they are not used in the experiments anywhere. Their use should be motivated and the performance of the distillation should be properly evaluated in the experiments.\n\n> First, we note that we have indeed used the expectation of the predictive entropy under the posterior as a distillation target in our experiment. These results are in Section 4. Table 1 (last column). Further,  Figure 1(c, f) and Figure 4(c) show the performance of the student model on distilling entropy as we apply our data augmentation techniques to vary posterior uncertainty. Finally, in the appendix, Figures 6, 9, 12 show the error-storage-computation tradeoff for entropy for the student model whose architecture is determined by either exhaustive search and group l1/l2 pruning method for different model-dataset combinations. \n\n> A primary motivation for generalizing posterior expectations is to help quantify model uncertainty. Indeed, the expectation of the predictive entropy is an important quantity that is distinct from the entropy of the posterior predictive distribution. For instance, the difference between these two quantities is exactly the BALD score used in active learning [1]. Note that the entropy of the posterior predictive distribution can be high while the expectation of the predictive entropy can be low, indicating that there are multiple competing hypotheses that each assign high confidence to an instance. This structure of the posterior can not be revealed without the expectation of the predictive entropy. Our proposed distillation approach then yields a computationally efficient method to compute both of these quantities.\n\n- The result regarding architecture search is interesting, but it should be expanded and explained more to be a main contribution.\n\n> We are a little unclear as to what the concern is in this comment. It\u2019ll help us address this concern better if you could elaborate a bit more on this.\n\n- The paper generally understandable, and well written, but it could be better organized. It should expand on the motivation and the architecture search, since these are key components of the paper. The figures are not legible. Even fully zoomed in, they are difficult to read.\n\n> As mentioned in our previous response above, it\u2019ll be very helpful to us if you could elaborate on the feedback regarding the explanation around architecture search. Lastly, we have updated the paper to ensure that all figures are legible at print resolution.\n\nReferences:\n[1] Neil Houlsby, Ferenc Husz\u00e1r, Zoubin Ghahramani, and M\u00e1t\u00e9 Lengyel. Bayesian active learning for classification and preference learning. arXiv preprint arXiv:1112.5745, 2011."}, "signatures": ["ICLR.cc/2020/Conference/Paper1180/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1180/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generalized Bayesian Posterior Expectation Distillation for Deep Neural Networks", "authors": ["Meet P. Vadera", "Benjamin M. Marlin"], "authorids": ["mvadera@cs.umass.edu", "marlin@cs.umass.edu"], "keywords": ["Bayesian Neural Networks", "Distillation"], "TL;DR": "A general framework for distilling Bayesian posterior expectations for deep neural networks.", "abstract": "In this paper, we present a general framework for distilling expectations with respect to the Bayesian posterior distribution of a deep neural network, significantly extending prior work on a method known as ``Bayesian Dark Knowledge.\"  Our generalized framework applies to the case of classification models and takes as input the architecture of a ``teacher\" network, a general posterior expectation of interest, and the architecture of a ``student\" network. The distillation method performs an online compression of the selected posterior expectation using iteratively generated Monte Carlo samples from the parameter posterior of the teacher model. We further consider the problem of optimizing the student model architecture with respect to an accuracy-speed-storage trade-off. We present experimental results investigating multiple data sets, distillation targets,  teacher model architectures, and approaches to searching for student model architectures. We establish the key result that distilling into a student model with an architecture that matches the teacher, as is done in Bayesian Dark Knowledge, can lead to sub-optimal performance. Lastly, we show that student architecture search methods can identify student models with significantly improved performance. ", "pdf": "/pdf/5db49d69762684595f766d570924fc6c30047939.pdf", "paperhash": "vadera|generalized_bayesian_posterior_expectation_distillation_for_deep_neural_networks", "original_pdf": "/attachment/8568286652bc83162cb98ff0ad9e557219391d5c.pdf", "_bibtex": "@misc{\nvadera2020generalized,\ntitle={Generalized Bayesian Posterior Expectation Distillation for Deep Neural Networks},\nauthor={Meet P. Vadera and Benjamin M. Marlin},\nyear={2020},\nurl={https://openreview.net/forum?id=Byg_vREtvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Byg_vREtvB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1180/Authors", "ICLR.cc/2020/Conference/Paper1180/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1180/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1180/Reviewers", "ICLR.cc/2020/Conference/Paper1180/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1180/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1180/Authors|ICLR.cc/2020/Conference/Paper1180/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504160004, "tmdate": 1576860541851, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1180/Authors", "ICLR.cc/2020/Conference/Paper1180/Reviewers", "ICLR.cc/2020/Conference/Paper1180/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1180/-/Official_Comment"}}}, {"id": "B1edOTmboB", "original": null, "number": 1, "cdate": 1573105007663, "ddate": null, "tcdate": 1573105007663, "tmdate": 1573105629432, "tddate": null, "forum": "Byg_vREtvB", "replyto": "r1xgFEvWqH", "invitation": "ICLR.cc/2020/Conference/Paper1180/-/Official_Comment", "content": {"title": "Response to Review#2", "comment": "Thank you for taking the time out to provide feedback on our work. Below, we address some of the concerns raised in your review:\n\n- On page 5, the choice of loss function does not seem to be discussed. I would like the authors to clarify why cross-entropy loss is replaced with l(h, h\u2019)=|h-h\u2019| in the classification case.\n\n> The cross-entropy loss is used when we are distilling the expectation of the predictive distribution E[p(y|x, \\theta)] under the parameter posterior p(\\theta|D). We switch to l(h, h\u2019)=|h-h\u2019| when we are distilling the expectation of the predictive entropy E[H[y|x,D, \\theta]] under the parameter posterior. Note that the distillation problem for posterior entropy is actually a regression problem and the use of the absolute loss to assess performance on this task is intuitive as it measures the absolute difference in entropy in units of nats.\n\n- The size of some figures appears too small, for example, Fig. 1 and Fig. 2, which may hinder readability.\n\n> Thank you for noting this issue. We have updated the paper to ensure that all figures are legible at print resolution. \n\nWe are also very grateful for the additional comments in the review. We will be responding to them subsequently.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1180/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1180/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generalized Bayesian Posterior Expectation Distillation for Deep Neural Networks", "authors": ["Meet P. Vadera", "Benjamin M. Marlin"], "authorids": ["mvadera@cs.umass.edu", "marlin@cs.umass.edu"], "keywords": ["Bayesian Neural Networks", "Distillation"], "TL;DR": "A general framework for distilling Bayesian posterior expectations for deep neural networks.", "abstract": "In this paper, we present a general framework for distilling expectations with respect to the Bayesian posterior distribution of a deep neural network, significantly extending prior work on a method known as ``Bayesian Dark Knowledge.\"  Our generalized framework applies to the case of classification models and takes as input the architecture of a ``teacher\" network, a general posterior expectation of interest, and the architecture of a ``student\" network. The distillation method performs an online compression of the selected posterior expectation using iteratively generated Monte Carlo samples from the parameter posterior of the teacher model. We further consider the problem of optimizing the student model architecture with respect to an accuracy-speed-storage trade-off. We present experimental results investigating multiple data sets, distillation targets,  teacher model architectures, and approaches to searching for student model architectures. We establish the key result that distilling into a student model with an architecture that matches the teacher, as is done in Bayesian Dark Knowledge, can lead to sub-optimal performance. Lastly, we show that student architecture search methods can identify student models with significantly improved performance. ", "pdf": "/pdf/5db49d69762684595f766d570924fc6c30047939.pdf", "paperhash": "vadera|generalized_bayesian_posterior_expectation_distillation_for_deep_neural_networks", "original_pdf": "/attachment/8568286652bc83162cb98ff0ad9e557219391d5c.pdf", "_bibtex": "@misc{\nvadera2020generalized,\ntitle={Generalized Bayesian Posterior Expectation Distillation for Deep Neural Networks},\nauthor={Meet P. Vadera and Benjamin M. Marlin},\nyear={2020},\nurl={https://openreview.net/forum?id=Byg_vREtvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Byg_vREtvB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1180/Authors", "ICLR.cc/2020/Conference/Paper1180/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1180/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1180/Reviewers", "ICLR.cc/2020/Conference/Paper1180/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1180/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1180/Authors|ICLR.cc/2020/Conference/Paper1180/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504160004, "tmdate": 1576860541851, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1180/Authors", "ICLR.cc/2020/Conference/Paper1180/Reviewers", "ICLR.cc/2020/Conference/Paper1180/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1180/-/Official_Comment"}}}, {"id": "Sklkc14ZjH", "original": null, "number": 2, "cdate": 1573105543124, "ddate": null, "tcdate": 1573105543124, "tmdate": 1573105616314, "tddate": null, "forum": "Byg_vREtvB", "replyto": "SyeK3VXTYS", "invitation": "ICLR.cc/2020/Conference/Paper1180/-/Official_Comment", "content": {"title": "Response to Review#1", "comment": "Thank you for spending time on our paper and for giving your review. Below, we address some of the concerns raised in the review:\n\n+ In the paragraph following equation 4, the posterior marginal variance expression implicitly assumes that p(y|x, \\theta) is a Categorical distribution. This should be clarified. The expression doesn\u2019t generally hold, for example if p(y | x, \\theta) is a Gaussian.\n\n> In this work, we focus on the classification setting explicitly (see, for example, the fourth line of the abstract that establishes this framing as well as the  start of Section 3). As such, we assume throughout that p(y|x, \\theta) is categorical.\n\n+ Figures 1 and 2 are too small and difficult to parse. I would recommend moving some of these to the supplement. \n\n> Thank you for noting this issue. We have updated the paper to ensure that all figures are legible at print resolution. \n\n+ It would be good to explicitly point out how much larger is the best (one with the smallest teacher student gap) l1/l2 regularized model compared to the base student model. I realize this is hiding in Figure 2 somewhere, but is not obvious. \n\n> Interpreting the size of a model in terms of the number of parameters, the CNN case for MNIST posterior predictive distribution distillation results in the best model under group l1/l2 pruning that has roughly 6.6 times the number of parameters when compared to the base student model. We have updated the figure captions to include this information. \n\nWe are also thankful for the additional comments in your review. We will respond to them subsequently. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1180/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1180/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generalized Bayesian Posterior Expectation Distillation for Deep Neural Networks", "authors": ["Meet P. Vadera", "Benjamin M. Marlin"], "authorids": ["mvadera@cs.umass.edu", "marlin@cs.umass.edu"], "keywords": ["Bayesian Neural Networks", "Distillation"], "TL;DR": "A general framework for distilling Bayesian posterior expectations for deep neural networks.", "abstract": "In this paper, we present a general framework for distilling expectations with respect to the Bayesian posterior distribution of a deep neural network, significantly extending prior work on a method known as ``Bayesian Dark Knowledge.\"  Our generalized framework applies to the case of classification models and takes as input the architecture of a ``teacher\" network, a general posterior expectation of interest, and the architecture of a ``student\" network. The distillation method performs an online compression of the selected posterior expectation using iteratively generated Monte Carlo samples from the parameter posterior of the teacher model. We further consider the problem of optimizing the student model architecture with respect to an accuracy-speed-storage trade-off. We present experimental results investigating multiple data sets, distillation targets,  teacher model architectures, and approaches to searching for student model architectures. We establish the key result that distilling into a student model with an architecture that matches the teacher, as is done in Bayesian Dark Knowledge, can lead to sub-optimal performance. Lastly, we show that student architecture search methods can identify student models with significantly improved performance. ", "pdf": "/pdf/5db49d69762684595f766d570924fc6c30047939.pdf", "paperhash": "vadera|generalized_bayesian_posterior_expectation_distillation_for_deep_neural_networks", "original_pdf": "/attachment/8568286652bc83162cb98ff0ad9e557219391d5c.pdf", "_bibtex": "@misc{\nvadera2020generalized,\ntitle={Generalized Bayesian Posterior Expectation Distillation for Deep Neural Networks},\nauthor={Meet P. Vadera and Benjamin M. Marlin},\nyear={2020},\nurl={https://openreview.net/forum?id=Byg_vREtvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Byg_vREtvB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1180/Authors", "ICLR.cc/2020/Conference/Paper1180/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1180/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1180/Reviewers", "ICLR.cc/2020/Conference/Paper1180/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1180/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1180/Authors|ICLR.cc/2020/Conference/Paper1180/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504160004, "tmdate": 1576860541851, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1180/Authors", "ICLR.cc/2020/Conference/Paper1180/Reviewers", "ICLR.cc/2020/Conference/Paper1180/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1180/-/Official_Comment"}}}, {"id": "SyeK3VXTYS", "original": null, "number": 2, "cdate": 1571792049017, "ddate": null, "tcdate": 1571792049017, "tmdate": 1572972502201, "tddate": null, "forum": "Byg_vREtvB", "replyto": "Byg_vREtvB", "invitation": "ICLR.cc/2020/Conference/Paper1180/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The authors consider the problem of distilling expectations with respect to Bayesian neural network (BNN) posteriors. These expectations rely on Monte Carlo integration and owing to the large number of BNN parameters can be computationally expensive and memory intensive to compute, motivating the need for distillation.  \n\nI recommend a weak accept for the paper. The authors generalize previous work on distilling posterior predictives by allowing for the computation of posterior expectations beyond posterior predictive distributions, proposing alternate low variance MC estimators, and using an amortization network whose architecture need not be identical to the original BNNs architecture. \n\nWhile the extensions individually are incremental and not particularly exciting, taken together, I believe, they do address a gap in the existing literature. The experiments successfully demonstrate a) when naive distillation fails and b) the proposed extensions help alleviate some of the observed issues. The paper would likely be an useful resource for practitioners in the area. \n\nMinor:\n+ Us vs Uo estimators: It would be interesting to more clearly see what the additional storage (and computation) of Uo is buying us. How much worse are the posterior predictive entropies if Uo is switched with Us? And do the posterior predictive estimates improve if Uo is used inlace of Us? \n\n+ In the paragraph following equation 4, the posterior marginal variance expression implicitly assumes that p(y|x, \\theta) is a Categorical distribution. This should be clarified. The expression doesn\u2019t generally hold, for example if p(y | x, \\theta) is a Gaussian.\n\n+ Figures 1 and 2 are too small and difficult to parse. I would recommend moving some of these to the supplement. \n\n+ It would be good to explicitly point out how much larger is the best (one with the smallest teacher student gap) l1/l2 regularized model compared to the base student model. I realize this is hiding in Figure 2 somewhere, but is not obvious.  "}, "signatures": ["ICLR.cc/2020/Conference/Paper1180/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1180/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generalized Bayesian Posterior Expectation Distillation for Deep Neural Networks", "authors": ["Meet P. Vadera", "Benjamin M. Marlin"], "authorids": ["mvadera@cs.umass.edu", "marlin@cs.umass.edu"], "keywords": ["Bayesian Neural Networks", "Distillation"], "TL;DR": "A general framework for distilling Bayesian posterior expectations for deep neural networks.", "abstract": "In this paper, we present a general framework for distilling expectations with respect to the Bayesian posterior distribution of a deep neural network, significantly extending prior work on a method known as ``Bayesian Dark Knowledge.\"  Our generalized framework applies to the case of classification models and takes as input the architecture of a ``teacher\" network, a general posterior expectation of interest, and the architecture of a ``student\" network. The distillation method performs an online compression of the selected posterior expectation using iteratively generated Monte Carlo samples from the parameter posterior of the teacher model. We further consider the problem of optimizing the student model architecture with respect to an accuracy-speed-storage trade-off. We present experimental results investigating multiple data sets, distillation targets,  teacher model architectures, and approaches to searching for student model architectures. We establish the key result that distilling into a student model with an architecture that matches the teacher, as is done in Bayesian Dark Knowledge, can lead to sub-optimal performance. Lastly, we show that student architecture search methods can identify student models with significantly improved performance. ", "pdf": "/pdf/5db49d69762684595f766d570924fc6c30047939.pdf", "paperhash": "vadera|generalized_bayesian_posterior_expectation_distillation_for_deep_neural_networks", "original_pdf": "/attachment/8568286652bc83162cb98ff0ad9e557219391d5c.pdf", "_bibtex": "@misc{\nvadera2020generalized,\ntitle={Generalized Bayesian Posterior Expectation Distillation for Deep Neural Networks},\nauthor={Meet P. Vadera and Benjamin M. Marlin},\nyear={2020},\nurl={https://openreview.net/forum?id=Byg_vREtvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Byg_vREtvB", "replyto": "Byg_vREtvB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1180/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1180/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575677737845, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1180/Reviewers"], "noninvitees": [], "tcdate": 1570237741176, "tmdate": 1575677737857, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1180/-/Official_Review"}}}], "count": 12}