{"notes": [{"id": "BJl6t64tvr", "original": "ryxRiBTwPH", "number": 686, "cdate": 1569439108753, "ddate": null, "tcdate": 1569439108753, "tmdate": 1577168284162, "tddate": null, "forum": "BJl6t64tvr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["namanagarwal@google.com", "rohananil@google.com", "ehazan@cs.princeton.edu", "tkoren@google.com", "cyril.zhang@princeton.edu"], "title": "Revisiting the Generalization of Adaptive Gradient Methods", "authors": ["Naman Agarwal", "Rohan Anil", "Elad Hazan", "Tomer Koren", "Cyril Zhang"], "pdf": "/pdf/1c52f348398812c26e02f69439d08357b459d282.pdf", "TL;DR": "Adaptive gradient methods, when done right, do not incur a generalization penalty. ", "abstract": "A commonplace belief in the machine learning community is that using adaptive gradient methods hurts generalization. We re-examine this belief both theoretically and experimentally, in light of insights and trends from recent years.\nWe revisit some previous oft-cited experiments and theoretical accounts in more depth, and provide a new set of experiments in larger-scale, state-of-the-art settings. We conclude that with proper tuning, the improved training performance of adaptive optimizers does not in general carry an overfitting penalty, especially in contemporary deep learning. Finally, we synthesize a ``user's guide'' to adaptive optimizers, including some proposed modifications to AdaGrad to mitigate some of its empirical shortcomings.", "keywords": ["Adaptive Methods", "AdaGrad", "Generalization"], "paperhash": "agarwal|revisiting_the_generalization_of_adaptive_gradient_methods", "original_pdf": "/attachment/1c52f348398812c26e02f69439d08357b459d282.pdf", "_bibtex": "@misc{\nagarwal2020revisiting,\ntitle={Revisiting the Generalization of Adaptive Gradient Methods},\nauthor={Naman Agarwal and Rohan Anil and Elad Hazan and Tomer Koren and Cyril Zhang},\nyear={2020},\nurl={https://openreview.net/forum?id=BJl6t64tvr}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "AeWBEF7_Kv", "original": null, "number": 1, "cdate": 1576798703273, "ddate": null, "tcdate": 1576798703273, "tmdate": 1576800932751, "tddate": null, "forum": "BJl6t64tvr", "replyto": "BJl6t64tvr", "invitation": "ICLR.cc/2020/Conference/Paper686/-/Decision", "content": {"decision": "Reject", "comment": "The paper combines several recent optimizer tricks to provide empirical evidence that goes against the common belief that adaptive methods result in larger generalization errors. The contribution of this paper is rather small: no new strategies are introduced and no new theory is presented. The paper makes a good workshop paper, but does not meet the bar for publication at ICLR.\n", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["namanagarwal@google.com", "rohananil@google.com", "ehazan@cs.princeton.edu", "tkoren@google.com", "cyril.zhang@princeton.edu"], "title": "Revisiting the Generalization of Adaptive Gradient Methods", "authors": ["Naman Agarwal", "Rohan Anil", "Elad Hazan", "Tomer Koren", "Cyril Zhang"], "pdf": "/pdf/1c52f348398812c26e02f69439d08357b459d282.pdf", "TL;DR": "Adaptive gradient methods, when done right, do not incur a generalization penalty. ", "abstract": "A commonplace belief in the machine learning community is that using adaptive gradient methods hurts generalization. We re-examine this belief both theoretically and experimentally, in light of insights and trends from recent years.\nWe revisit some previous oft-cited experiments and theoretical accounts in more depth, and provide a new set of experiments in larger-scale, state-of-the-art settings. We conclude that with proper tuning, the improved training performance of adaptive optimizers does not in general carry an overfitting penalty, especially in contemporary deep learning. Finally, we synthesize a ``user's guide'' to adaptive optimizers, including some proposed modifications to AdaGrad to mitigate some of its empirical shortcomings.", "keywords": ["Adaptive Methods", "AdaGrad", "Generalization"], "paperhash": "agarwal|revisiting_the_generalization_of_adaptive_gradient_methods", "original_pdf": "/attachment/1c52f348398812c26e02f69439d08357b459d282.pdf", "_bibtex": "@misc{\nagarwal2020revisiting,\ntitle={Revisiting the Generalization of Adaptive Gradient Methods},\nauthor={Naman Agarwal and Rohan Anil and Elad Hazan and Tomer Koren and Cyril Zhang},\nyear={2020},\nurl={https://openreview.net/forum?id=BJl6t64tvr}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "BJl6t64tvr", "replyto": "BJl6t64tvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795728824, "tmdate": 1576800281309, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper686/-/Decision"}}}, {"id": "H1lQstr2jr", "original": null, "number": 3, "cdate": 1573833115406, "ddate": null, "tcdate": 1573833115406, "tmdate": 1573833115406, "tddate": null, "forum": "BJl6t64tvr", "replyto": "HklEDFu3YB", "invitation": "ICLR.cc/2020/Conference/Paper686/-/Official_Comment", "content": {"title": "Response to R2", "comment": "Thanks for the thorough review and many good questions and suggestions.\n\n@Q1 (Theory): By \u201ctheoretically\u201d we were referring to the discussion of theoretical examples, intended to continue the discussion from Section 3 of [Wilson et al. \u201817]. We will package the discussions of Section 5 into formal theorems.\n\n@Q2 (Standard deviations): We will revise to include error bars for completeness. Of course, this will not change the conclusions of the experiment.\n\n@Q3 (\u201cUser\u2019s guide\u201d): We did not mean to misstate or overstate the contribution, and agree that this was a poor choice of wording. By \u201cuser\u2019s guide\u201d we meant to refer to the collection of observations and suggestions of aspects of adaptive optimizers which need not be tuned in a first cut. This is not a comprehensive toolbox, but rather a way to pare down the hyperparameter search space, towards a minimal manifestation of adaptivity. As mentioned to R3, we will overhaul the writing to reflect this.\n\n@Q4 (SGD comparison): The HB (Heavy Ball) curve is SGD with momentum but not adaptivity, providing the comparison between adaptive and non-adaptive methods here. As a side note, SGD without momentum on this architecture performed significantly worse.\n\n@Q5 (Learning rate schedule): There is no empirical consensus on learning rate schedule tuning, and we are not intending in this work to enter that debate. We are also not concluding here that LR schedule is more important than optimizer choice. Our discussion of LR schedules is exclusively to make the following point: the discrepancies between different adaptive optimizers may be explained by their different implicit LR schedules. This is, again, aimed towards paring down the hyperparameter space.\n\n@Q6 (Relation to optimizer search): Again, we are intending in this work to revisit controversial beliefs about the generalization of adaptive methods, not provide new optimizer tricks or state-of-the-art results."}, "signatures": ["ICLR.cc/2020/Conference/Paper686/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper686/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["namanagarwal@google.com", "rohananil@google.com", "ehazan@cs.princeton.edu", "tkoren@google.com", "cyril.zhang@princeton.edu"], "title": "Revisiting the Generalization of Adaptive Gradient Methods", "authors": ["Naman Agarwal", "Rohan Anil", "Elad Hazan", "Tomer Koren", "Cyril Zhang"], "pdf": "/pdf/1c52f348398812c26e02f69439d08357b459d282.pdf", "TL;DR": "Adaptive gradient methods, when done right, do not incur a generalization penalty. ", "abstract": "A commonplace belief in the machine learning community is that using adaptive gradient methods hurts generalization. We re-examine this belief both theoretically and experimentally, in light of insights and trends from recent years.\nWe revisit some previous oft-cited experiments and theoretical accounts in more depth, and provide a new set of experiments in larger-scale, state-of-the-art settings. We conclude that with proper tuning, the improved training performance of adaptive optimizers does not in general carry an overfitting penalty, especially in contemporary deep learning. Finally, we synthesize a ``user's guide'' to adaptive optimizers, including some proposed modifications to AdaGrad to mitigate some of its empirical shortcomings.", "keywords": ["Adaptive Methods", "AdaGrad", "Generalization"], "paperhash": "agarwal|revisiting_the_generalization_of_adaptive_gradient_methods", "original_pdf": "/attachment/1c52f348398812c26e02f69439d08357b459d282.pdf", "_bibtex": "@misc{\nagarwal2020revisiting,\ntitle={Revisiting the Generalization of Adaptive Gradient Methods},\nauthor={Naman Agarwal and Rohan Anil and Elad Hazan and Tomer Koren and Cyril Zhang},\nyear={2020},\nurl={https://openreview.net/forum?id=BJl6t64tvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJl6t64tvr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper686/Authors", "ICLR.cc/2020/Conference/Paper686/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper686/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper686/Reviewers", "ICLR.cc/2020/Conference/Paper686/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper686/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper686/Authors|ICLR.cc/2020/Conference/Paper686/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504167757, "tmdate": 1576860533790, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper686/Authors", "ICLR.cc/2020/Conference/Paper686/Reviewers", "ICLR.cc/2020/Conference/Paper686/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper686/-/Official_Comment"}}}, {"id": "ryx7adBniB", "original": null, "number": 2, "cdate": 1573832891076, "ddate": null, "tcdate": 1573832891076, "tmdate": 1573832891076, "tddate": null, "forum": "BJl6t64tvr", "replyto": "BkxNHyW6YB", "invitation": "ICLR.cc/2020/Conference/Paper686/-/Official_Comment", "content": {"title": "Response to R3", "comment": "Thanks for the review.\n\n@Insights: Our message is explicitly *not* to add new optimizer tricks. The main insight we wish to convey is that contrary to common belief, adaptive optimizers can generalize well. There are confounding factors (including bad choice of hyperparameters) that lead to the opposite misleading belief which we wish to disentangle in this paper (see next point). Thus, we have attempted to only cite the vast literature on recent advances in optimizer tricks when they include points important to our message.\n\n@Conclusions about \\eps=0: We do not claim that there is no mileage to be obtained out of using a tuned positive \\eps for different algorithms and models, especially since this has the interpretation of interpolating with SGD. The purpose of the \\eps=0 suggestion, as well as all of our other algorithmic points, is to pare down the hyperparameter search space as much as possible, and obtain the simplest useful manifestation of adaptivity for which a consistent discussion of generalization can be made. Indeed as we demonstrate removing this epsilon hyperparameter by itself leads to a very effective and useful algorithm. We agree that this was not as clear as it should have been, and will overhaul the writing to reflect this intent."}, "signatures": ["ICLR.cc/2020/Conference/Paper686/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper686/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["namanagarwal@google.com", "rohananil@google.com", "ehazan@cs.princeton.edu", "tkoren@google.com", "cyril.zhang@princeton.edu"], "title": "Revisiting the Generalization of Adaptive Gradient Methods", "authors": ["Naman Agarwal", "Rohan Anil", "Elad Hazan", "Tomer Koren", "Cyril Zhang"], "pdf": "/pdf/1c52f348398812c26e02f69439d08357b459d282.pdf", "TL;DR": "Adaptive gradient methods, when done right, do not incur a generalization penalty. ", "abstract": "A commonplace belief in the machine learning community is that using adaptive gradient methods hurts generalization. We re-examine this belief both theoretically and experimentally, in light of insights and trends from recent years.\nWe revisit some previous oft-cited experiments and theoretical accounts in more depth, and provide a new set of experiments in larger-scale, state-of-the-art settings. We conclude that with proper tuning, the improved training performance of adaptive optimizers does not in general carry an overfitting penalty, especially in contemporary deep learning. Finally, we synthesize a ``user's guide'' to adaptive optimizers, including some proposed modifications to AdaGrad to mitigate some of its empirical shortcomings.", "keywords": ["Adaptive Methods", "AdaGrad", "Generalization"], "paperhash": "agarwal|revisiting_the_generalization_of_adaptive_gradient_methods", "original_pdf": "/attachment/1c52f348398812c26e02f69439d08357b459d282.pdf", "_bibtex": "@misc{\nagarwal2020revisiting,\ntitle={Revisiting the Generalization of Adaptive Gradient Methods},\nauthor={Naman Agarwal and Rohan Anil and Elad Hazan and Tomer Koren and Cyril Zhang},\nyear={2020},\nurl={https://openreview.net/forum?id=BJl6t64tvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJl6t64tvr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper686/Authors", "ICLR.cc/2020/Conference/Paper686/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper686/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper686/Reviewers", "ICLR.cc/2020/Conference/Paper686/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper686/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper686/Authors|ICLR.cc/2020/Conference/Paper686/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504167757, "tmdate": 1576860533790, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper686/Authors", "ICLR.cc/2020/Conference/Paper686/Reviewers", "ICLR.cc/2020/Conference/Paper686/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper686/-/Official_Comment"}}}, {"id": "r1x-9dS3jS", "original": null, "number": 1, "cdate": 1573832840935, "ddate": null, "tcdate": 1573832840935, "tmdate": 1573832840935, "tddate": null, "forum": "BJl6t64tvr", "replyto": "BylHymY19r", "invitation": "ICLR.cc/2020/Conference/Paper686/-/Official_Comment", "content": {"title": "Response to R1", "comment": "Thanks for the review.\n\n@Figure 5: Yes, what we mean is that AdaGrad by default does not perform as well as the others in terms of the achieved test accuracy. We will make the correction. \n\n@Missing reference: We will correct this in the paper."}, "signatures": ["ICLR.cc/2020/Conference/Paper686/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper686/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["namanagarwal@google.com", "rohananil@google.com", "ehazan@cs.princeton.edu", "tkoren@google.com", "cyril.zhang@princeton.edu"], "title": "Revisiting the Generalization of Adaptive Gradient Methods", "authors": ["Naman Agarwal", "Rohan Anil", "Elad Hazan", "Tomer Koren", "Cyril Zhang"], "pdf": "/pdf/1c52f348398812c26e02f69439d08357b459d282.pdf", "TL;DR": "Adaptive gradient methods, when done right, do not incur a generalization penalty. ", "abstract": "A commonplace belief in the machine learning community is that using adaptive gradient methods hurts generalization. We re-examine this belief both theoretically and experimentally, in light of insights and trends from recent years.\nWe revisit some previous oft-cited experiments and theoretical accounts in more depth, and provide a new set of experiments in larger-scale, state-of-the-art settings. We conclude that with proper tuning, the improved training performance of adaptive optimizers does not in general carry an overfitting penalty, especially in contemporary deep learning. Finally, we synthesize a ``user's guide'' to adaptive optimizers, including some proposed modifications to AdaGrad to mitigate some of its empirical shortcomings.", "keywords": ["Adaptive Methods", "AdaGrad", "Generalization"], "paperhash": "agarwal|revisiting_the_generalization_of_adaptive_gradient_methods", "original_pdf": "/attachment/1c52f348398812c26e02f69439d08357b459d282.pdf", "_bibtex": "@misc{\nagarwal2020revisiting,\ntitle={Revisiting the Generalization of Adaptive Gradient Methods},\nauthor={Naman Agarwal and Rohan Anil and Elad Hazan and Tomer Koren and Cyril Zhang},\nyear={2020},\nurl={https://openreview.net/forum?id=BJl6t64tvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJl6t64tvr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper686/Authors", "ICLR.cc/2020/Conference/Paper686/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper686/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper686/Reviewers", "ICLR.cc/2020/Conference/Paper686/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper686/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper686/Authors|ICLR.cc/2020/Conference/Paper686/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504167757, "tmdate": 1576860533790, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper686/Authors", "ICLR.cc/2020/Conference/Paper686/Reviewers", "ICLR.cc/2020/Conference/Paper686/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper686/-/Official_Comment"}}}, {"id": "HklEDFu3YB", "original": null, "number": 1, "cdate": 1571748187615, "ddate": null, "tcdate": 1571748187615, "tmdate": 1572972564793, "tddate": null, "forum": "BJl6t64tvr", "replyto": "BJl6t64tvr", "invitation": "ICLR.cc/2020/Conference/Paper686/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper does an empirical study for the problem of \"whether acceleration harms the empirical learning performance\". Based on the study, the authors propose some empirical suggestions to fix the generalization gap for acceleration methods.\n\nOverall, this paper is a lack of insights and looks more like an experimental report. The significance of the paper is not enough. Please see questions below\n\nQ1. \"We re-examine this belief both theoretically and experimentally\". Where is the theoretical part? \n- I wish the authors can offer some useful Lemmas or Theorems, but Section 5 only offers some discussions. \n\nQ2. Since the paper wants to promote some empirical observations, please add STD to all curves, like,  Wilson et al. (2017). Some curves are too close (e.g., those Figure 6), it is hard to judge their statistic significance.\n\nQ3. \"we synthesize a user\u2019s guide to adaptive optimizers\". \n- After reading the paper several times, I am still kind of confused. What is the \"user\u2019s guide\"? Could the authors make some short summary?\n- Since the paper mainly does empirical study, this question is important.\n\nQ4. Why not SGD in Figure 3?\n\nQ5. \"As demonstrated, learning rate schedule is a highly important hyperparameter and requires tuning for each task\". \n- So, how to tune the learning rate?\n- Is the learning rate more important than the choice of the optimizer? If so, the problem with the acceleration this paper investigated has little meaning.\n\nQ6. Is it better to carry on hyperparameter optimization on factors that can contribute to the final performance of each algorithm? \n- In this way, we can see the extreme performance of each optimizer, the performance comparison condition on best possible hyperparameter can make more sense than what has been done in this paper.\n- It will be good if authors can check this paper \"Neural Optimizer Search with Reinforcement Learning\" and then re-design their methodology."}, "signatures": ["ICLR.cc/2020/Conference/Paper686/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper686/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["namanagarwal@google.com", "rohananil@google.com", "ehazan@cs.princeton.edu", "tkoren@google.com", "cyril.zhang@princeton.edu"], "title": "Revisiting the Generalization of Adaptive Gradient Methods", "authors": ["Naman Agarwal", "Rohan Anil", "Elad Hazan", "Tomer Koren", "Cyril Zhang"], "pdf": "/pdf/1c52f348398812c26e02f69439d08357b459d282.pdf", "TL;DR": "Adaptive gradient methods, when done right, do not incur a generalization penalty. ", "abstract": "A commonplace belief in the machine learning community is that using adaptive gradient methods hurts generalization. We re-examine this belief both theoretically and experimentally, in light of insights and trends from recent years.\nWe revisit some previous oft-cited experiments and theoretical accounts in more depth, and provide a new set of experiments in larger-scale, state-of-the-art settings. We conclude that with proper tuning, the improved training performance of adaptive optimizers does not in general carry an overfitting penalty, especially in contemporary deep learning. Finally, we synthesize a ``user's guide'' to adaptive optimizers, including some proposed modifications to AdaGrad to mitigate some of its empirical shortcomings.", "keywords": ["Adaptive Methods", "AdaGrad", "Generalization"], "paperhash": "agarwal|revisiting_the_generalization_of_adaptive_gradient_methods", "original_pdf": "/attachment/1c52f348398812c26e02f69439d08357b459d282.pdf", "_bibtex": "@misc{\nagarwal2020revisiting,\ntitle={Revisiting the Generalization of Adaptive Gradient Methods},\nauthor={Naman Agarwal and Rohan Anil and Elad Hazan and Tomer Koren and Cyril Zhang},\nyear={2020},\nurl={https://openreview.net/forum?id=BJl6t64tvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BJl6t64tvr", "replyto": "BJl6t64tvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper686/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper686/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575427731493, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper686/Reviewers"], "noninvitees": [], "tcdate": 1570237748556, "tmdate": 1575427731511, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper686/-/Official_Review"}}}, {"id": "BkxNHyW6YB", "original": null, "number": 2, "cdate": 1571782459723, "ddate": null, "tcdate": 1571782459723, "tmdate": 1572972564754, "tddate": null, "forum": "BJl6t64tvr", "replyto": "BJl6t64tvr", "invitation": "ICLR.cc/2020/Conference/Paper686/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper revisited the a common belief that adaptive gradient methods hurts generalization performances. The authors re-examine this in more depth and provide a new set of experiments in larger-scale, state-of-the-art settings. The authors claimed that with proper tuning, the performance of adaptive optimizers can mitigate the gap  with non-adaptive methods.\n\n- It is great to have someone revisit and challenge the conventional ideas in the community. However, I did not find much insightful information in this paper. As the author mentioned, there are many recent works focusing on further improving the empirical generalization performances of adaptive gradient methods. The main aspects mentioned in the paper, \\epsilon tuning, learning rate warmup and decaying schedule, are not something new and many of which are mentioned or used in the recent advances. This makes the contribution of this paper look like combining all the tricks together. The authors might want to carefully comment the differences with the recent advances\n\n[1] Liu, Liyuan, et al. \"On the variance of the adaptive learning rate and beyond.\" arXiv preprint arXiv:1908.03265 (2019).\n[2] Loshchilov, Ilya, and Frank Hutter. \"Decoupled weight decay regularization.\" ICLR 2019.\n[3] Zaheer, Manzil, et al. \"Adaptive methods for nonconvex optimization.\" Advances in Neural Information Processing Systems. 2018.\n[4] Chen, Jinghui, and Quanquan Gu. \"Closing the generalization gap of adaptive gradient methods in training deep neural networks.\" arXiv preprint arXiv:1806.06763 (2018).\n[5] Luo, Liangchen, et al. \"Adaptive gradient methods with dynamic bound of learning rate.\" arXiv preprint arXiv:1902.09843 (2019).\n\n- In terms of tuning \\epsilon, the authors mentioned that default setting in Tensorflow is 0.1 for Adagrad which is too high. However, most papers regarding adaptive gradient method usually set \\epsilon as 10^-8. Pytorch set default value as 10^-10. In fact, Yogi paper mentioned above gives some different conclusions. In their experiments, they found that setting \\epsilon to be a bit larger like 10^-3 give better results compared with 10^-8. I wonder if the authors examine the reasons for different conclusions here?\n \n- at the end of page 6, missing reference for histogram\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper686/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper686/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["namanagarwal@google.com", "rohananil@google.com", "ehazan@cs.princeton.edu", "tkoren@google.com", "cyril.zhang@princeton.edu"], "title": "Revisiting the Generalization of Adaptive Gradient Methods", "authors": ["Naman Agarwal", "Rohan Anil", "Elad Hazan", "Tomer Koren", "Cyril Zhang"], "pdf": "/pdf/1c52f348398812c26e02f69439d08357b459d282.pdf", "TL;DR": "Adaptive gradient methods, when done right, do not incur a generalization penalty. ", "abstract": "A commonplace belief in the machine learning community is that using adaptive gradient methods hurts generalization. We re-examine this belief both theoretically and experimentally, in light of insights and trends from recent years.\nWe revisit some previous oft-cited experiments and theoretical accounts in more depth, and provide a new set of experiments in larger-scale, state-of-the-art settings. We conclude that with proper tuning, the improved training performance of adaptive optimizers does not in general carry an overfitting penalty, especially in contemporary deep learning. Finally, we synthesize a ``user's guide'' to adaptive optimizers, including some proposed modifications to AdaGrad to mitigate some of its empirical shortcomings.", "keywords": ["Adaptive Methods", "AdaGrad", "Generalization"], "paperhash": "agarwal|revisiting_the_generalization_of_adaptive_gradient_methods", "original_pdf": "/attachment/1c52f348398812c26e02f69439d08357b459d282.pdf", "_bibtex": "@misc{\nagarwal2020revisiting,\ntitle={Revisiting the Generalization of Adaptive Gradient Methods},\nauthor={Naman Agarwal and Rohan Anil and Elad Hazan and Tomer Koren and Cyril Zhang},\nyear={2020},\nurl={https://openreview.net/forum?id=BJl6t64tvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BJl6t64tvr", "replyto": "BJl6t64tvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper686/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper686/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575427731493, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper686/Reviewers"], "noninvitees": [], "tcdate": 1570237748556, "tmdate": 1575427731511, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper686/-/Official_Review"}}}, {"id": "BylHymY19r", "original": null, "number": 3, "cdate": 1571947229302, "ddate": null, "tcdate": 1571947229302, "tmdate": 1572972564708, "tddate": null, "forum": "BJl6t64tvr", "replyto": "BJl6t64tvr", "invitation": "ICLR.cc/2020/Conference/Paper686/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper revisits the question of whether adaptive methods deliver solutions with larger generalization errors than those of solutions found using SGD. The conclusion is that \"which algorithm will perform better on a given problem, depends on various properties of the precise instance.\" They reach this conclusion by rerunning the same experiments with the same settings as in the 2017 Wilson et al. paper which showed empirical evidence that adaptive methods exhibit worse generalization error than SGD, and showing that the results have changed, due to extraneous factors like the fact that they run on different hardware. The only advantage of adaptive methods seems to be that the amount of tuning is less intensive (e.g., no need to handtune staircase decaying learning rates). They provide two mathematical examples to show that SGD (with a particular decay rate) and Agagrad can beat the other in terms of generalization error, depending on the setting.\n\nThe takeaway of this paper seems to be that adaptive methods and non-adaptive methods both need careful parameter tuning. They also suggest using Adagrad without adding a multiple of the identity to the approximate Hessian, and point out that externally imposed decay rates can help adaptive methods. I think the paper is well-written, and provides convincing evidence that neither SGD or adaptive methods dominates the other. I lean towards accept.\n\nComments:\nFigure 5 doesn't show a significant gap between the test and training error for the adaptive methods, as claimed in its caption.\nThere is a missing figure reference at bottom of page 6"}, "signatures": ["ICLR.cc/2020/Conference/Paper686/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper686/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["namanagarwal@google.com", "rohananil@google.com", "ehazan@cs.princeton.edu", "tkoren@google.com", "cyril.zhang@princeton.edu"], "title": "Revisiting the Generalization of Adaptive Gradient Methods", "authors": ["Naman Agarwal", "Rohan Anil", "Elad Hazan", "Tomer Koren", "Cyril Zhang"], "pdf": "/pdf/1c52f348398812c26e02f69439d08357b459d282.pdf", "TL;DR": "Adaptive gradient methods, when done right, do not incur a generalization penalty. ", "abstract": "A commonplace belief in the machine learning community is that using adaptive gradient methods hurts generalization. We re-examine this belief both theoretically and experimentally, in light of insights and trends from recent years.\nWe revisit some previous oft-cited experiments and theoretical accounts in more depth, and provide a new set of experiments in larger-scale, state-of-the-art settings. We conclude that with proper tuning, the improved training performance of adaptive optimizers does not in general carry an overfitting penalty, especially in contemporary deep learning. Finally, we synthesize a ``user's guide'' to adaptive optimizers, including some proposed modifications to AdaGrad to mitigate some of its empirical shortcomings.", "keywords": ["Adaptive Methods", "AdaGrad", "Generalization"], "paperhash": "agarwal|revisiting_the_generalization_of_adaptive_gradient_methods", "original_pdf": "/attachment/1c52f348398812c26e02f69439d08357b459d282.pdf", "_bibtex": "@misc{\nagarwal2020revisiting,\ntitle={Revisiting the Generalization of Adaptive Gradient Methods},\nauthor={Naman Agarwal and Rohan Anil and Elad Hazan and Tomer Koren and Cyril Zhang},\nyear={2020},\nurl={https://openreview.net/forum?id=BJl6t64tvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BJl6t64tvr", "replyto": "BJl6t64tvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper686/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper686/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575427731493, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper686/Reviewers"], "noninvitees": [], "tcdate": 1570237748556, "tmdate": 1575427731511, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper686/-/Official_Review"}}}], "count": 8}