{"notes": [{"ddate": null, "legacy_migration": true, "tmdate": 1363646880000, "tcdate": 1363646880000, "number": 1, "id": "X-2g4ZbGhE5Gf", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "9bFY3t2IJ19AC", "replyto": "9bFY3t2IJ19AC", "signatures": ["Jason Weston"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "- The results of G alone are basically the 'k-Nearest Neighbor (Wsabie space)' results that are in the tables.\r\n\r\n- We initialized the parameters of step 3 with the ones from step 1. Without this I think the results could be worse as you are losing a lot of the pairwise label comparisons from the training if G is sparse, so somehow because of the increased capacity, it is more possible to overfit. This may not be necessary if the dataset is big enough.\r\n\r\n- Running time depends on the cost of computing G. In the imagenet experiments we did the full nearest neighbor computation (computed in parallel) which is obviously very costly (proportional to the training set size). However approximate kNN could also be considered as we said, amongst other choices of G."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Affinity Weighted Embedding", "decision": "conferenceOral-iclr2013-workshop", "abstract": "Supervised (linear) embedding models like Wsabie and PSI have proven successful at ranking, recommendation and annotation tasks. However, despite being scalable to large datasets they do not take full advantage of the extra data due to their linear nature, and typically underfit. We propose a new class of models which aim to provide improved performance while retaining many of the benefits of the existing class of embedding models. Our new approach works by iteratively learning a linear embedding model where the next iteration's features and labels are reweighted as a function of the previous iteration. We describe several variants of the family, and give some initial results.", "pdf": "https://arxiv.org/abs/1301.4171", "paperhash": "weston|affinity_weighted_embedding", "keywords": [], "conflicts": [], "authors": ["Jason Weston", "Ron Weiss", "Hector Yee"], "authorids": ["jweston@google.com", "ronw@google.com", "hector.yee@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362229560000, "tcdate": 1362229560000, "number": 2, "id": "T5KWotfp6lot7", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "9bFY3t2IJ19AC", "replyto": "9bFY3t2IJ19AC", "signatures": ["anonymous reviewer 0248"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Affinity Weighted Embedding", "review": "This work proposes a new nonlinear embedding model and applies it to a music annotation and image annotation task. Motivated by the fact that linear embedding models typically underfit on large datasets, the authors propose a nonlinear embedding model with greater capacity. This model weights examples in the embedding by their affinity in an initial linear embedding. The model achieves modest performance improvements on a music annotation task, and large performance improvements on ImageNet annotation. The ImageNet result achieves comparable performance to a very large convolutional net.\r\n\r\nThe model presented in the paper is novel, addresses an apparent need for a higher capacity model class, and achieves good performance on a very challenging problem. \r\n\r\nThe paper is clear but has a rushed feel, with some explanations being extremely terse. Although the details of the algorithms and experiments are specified, the intuition behind particular algorithmic design choices is not spelled out and the paper would be stronger if these were.\r\n\r\nThe experimental results are labeled 'preliminary,' and although they demonstrate good performance on ImageNet, they do not carefully investigate how different design choices impact performance. The ImageNet performance comparisons to related algorithms are hard to interpret because of a different train/testing split, and because a recent highly performing convolutional net was not considered (though the authors discuss its likely superior performance). \r\n\r\nFinally, the presented experiments focus on performance on tasks of interest, but do not address the running time and storage cost of the algorithm. The authors mention the fact that their algorithm is more computationally and space-intensive than linear embedding; it would be useful to see running times (particularly in comparison to Dean et al. and Krizhevsky et al.) to give a more complete picture of the advantages of the algorithm."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Affinity Weighted Embedding", "decision": "conferenceOral-iclr2013-workshop", "abstract": "Supervised (linear) embedding models like Wsabie and PSI have proven successful at ranking, recommendation and annotation tasks. However, despite being scalable to large datasets they do not take full advantage of the extra data due to their linear nature, and typically underfit. We propose a new class of models which aim to provide improved performance while retaining many of the benefits of the existing class of embedding models. Our new approach works by iteratively learning a linear embedding model where the next iteration's features and labels are reweighted as a function of the previous iteration. We describe several variants of the family, and give some initial results.", "pdf": "https://arxiv.org/abs/1301.4171", "paperhash": "weston|affinity_weighted_embedding", "keywords": [], "conflicts": [], "authors": ["Jason Weston", "Ron Weiss", "Hector Yee"], "authorids": ["jweston@google.com", "ronw@google.com", "hector.yee@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362123720000, "tcdate": 1362123720000, "number": 3, "id": "9A_uTWCfuoTeF", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "9bFY3t2IJ19AC", "replyto": "9bFY3t2IJ19AC", "signatures": ["anonymous reviewer 3e4d"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Affinity Weighted Embedding", "review": "Affinity Weighted Embedding\r\n\r\nPaper summary\r\n\r\nThis paper extends supervised embedding models by combining them multiplicatively,\r\ni.e. f'(x,y) = G(x,y) f(x,y).\r\nIt considers two types of model, dot product in the *embedding* space and kernel density in the *embedding* space, where the kernel in the embedding space is restricted to\r\nk((x,y),(x','y)) = k(x-x')k(y-y').\r\nIt proposes an iterative algorithm which alternates f and G parameter updates.\r\n\r\nReview Summary\r\n\r\nThe paper is clear and reads well. The proposed solution is novel. Combining local kernels and linear kernel in different embedding space could leverage the best characteristic for each of them (locality for non-linear, easier training for linear). The experiments are convincing. I would suggest adding the results for G alone.\r\n\r\nReview Details\r\n\r\nStep (2), i.e. local kernel, is interesting on its own. Could you report its result? The optimization problem seems harder than step (1), could you quantify how much the pretraining with step (1) helps step (2)?  A last related question, how do you initialize the parameters for step (3)?"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Affinity Weighted Embedding", "decision": "conferenceOral-iclr2013-workshop", "abstract": "Supervised (linear) embedding models like Wsabie and PSI have proven successful at ranking, recommendation and annotation tasks. However, despite being scalable to large datasets they do not take full advantage of the extra data due to their linear nature, and typically underfit. We propose a new class of models which aim to provide improved performance while retaining many of the benefits of the existing class of embedding models. Our new approach works by iteratively learning a linear embedding model where the next iteration's features and labels are reweighted as a function of the previous iteration. We describe several variants of the family, and give some initial results.", "pdf": "https://arxiv.org/abs/1301.4171", "paperhash": "weston|affinity_weighted_embedding", "keywords": [], "conflicts": [], "authors": ["Jason Weston", "Ron Weiss", "Hector Yee"], "authorids": ["jweston@google.com", "ronw@google.com", "hector.yee@gmail.com"]}, "tags": [], "invitation": {}}}, {"replyto": null, "ddate": null, "legacy_migration": true, "tmdate": 1358588700000, "tcdate": 1358588700000, "number": 5, "id": "9bFY3t2IJ19AC", "invitation": "ICLR.cc/2013/conference/-/submission", "forum": "9bFY3t2IJ19AC", "signatures": ["jweston@google.com"], "readers": ["everyone"], "content": {"title": "Affinity Weighted Embedding", "decision": "conferenceOral-iclr2013-workshop", "abstract": "Supervised (linear) embedding models like Wsabie and PSI have proven successful at ranking, recommendation and annotation tasks. However, despite being scalable to large datasets they do not take full advantage of the extra data due to their linear nature, and typically underfit. We propose a new class of models which aim to provide improved performance while retaining many of the benefits of the existing class of embedding models. Our new approach works by iteratively learning a linear embedding model where the next iteration's features and labels are reweighted as a function of the previous iteration. We describe several variants of the family, and give some initial results.", "pdf": "https://arxiv.org/abs/1301.4171", "paperhash": "weston|affinity_weighted_embedding", "keywords": [], "conflicts": [], "authors": ["Jason Weston", "Ron Weiss", "Hector Yee"], "authorids": ["jweston@google.com", "ronw@google.com", "hector.yee@gmail.com"]}, "writers": [], "details": {"replyCount": 3, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1369422751717, "tmdate": 1496673673639, "cdate": 1496673673639, "tcdate": 1496673673639, "id": "ICLR.cc/2013/conference/-/submission", "writers": ["ICLR.cc/2013"], "signatures": ["OpenReview.net"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": []}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1377198751717}}}], "count": 4}