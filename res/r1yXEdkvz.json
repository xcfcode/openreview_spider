{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1528124293691, "tcdate": 1518466070555, "number": 243, "cdate": 1518466070555, "id": "r1yXEdkvz", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "r1yXEdkvz", "signatures": ["~Andrei_Atanov1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "Uncertainty Estimation via Stochastic Batch Normalization", "abstract": "In this work, we investigate Batch Normalization technique and propose its probabilistic interpretation. We propose a probabilistic model and show that Batch Normalization maximazes the lower bound of its marginalized log-likelihood. Then, according to the new probabilistic model, we design an algorithm which acts consistently during train and test. However, inference becomes computationally inefficient. To reduce memory and computational cost, we propose Stochastic Batch Normalization -- an efficient approximation of proper inference procedure. This method provides us with a scalable uncertainty estimation technique. We demonstrate the performance of Stochastic Batch Normalization on popular architectures (including deep convolutional architectures: VGG-like and ResNets) for MNIST and CIFAR-10 datasets.\n", "paperhash": "atanov|uncertainty_estimation_via_stochastic_batch_normalization", "keywords": ["Batch Normalization", "Uncertainty Estimation", "Deep Neural Networks"], "_bibtex": "@misc{\n  atanov2018uncertainty,\n  title={Uncertainty Estimation via Stochastic Batch Normalization},\n  author={Andrei Atanov and Arsenii Ashukha and Dmitry Molchanov and Kirill Neklyudov and Dmitry Vetrov},\n  year={2018},\n  url={https://openreview.net/forum?id=r1yXEdkvz}\n}", "authorids": ["andrewatanov@yandex.ru", "ars.ashuha@gmail.com", "dmolch111@gmail.com", "k.necludov@gmail.com", "vetrovd@yandex.ru"], "authors": ["Andrei Atanov", "Arsenii Ashukha", "Dmitry Molchanov", "Kirill Neklyudov", "Dmitry Vetrov"], "TL;DR": "We propose a probabilistic view on Batch Normalization and an efficient test-time averaging technique for uncertainty estimation in batch-normalized DNNs.", "pdf": "/pdf/38f2c011368f49d35147dae0ff71b112c681b84d.pdf"}, "nonreaders": [], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}}, {"tddate": null, "ddate": null, "tmdate": 1525736561160, "tcdate": 1525736561160, "number": 2, "cdate": 1525736561160, "id": "ByFdNDCaz", "invitation": "ICLR.cc/2018/Workshop/-/Paper243/Official_Comment", "forum": "r1yXEdkvz", "replyto": "ryU5yD5TM", "signatures": ["ICLR.cc/2018/Workshop/Paper243/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper243/Authors"], "content": {"title": "Response", "comment": "Dear Prof. Smith,\n\nThank you for the valuable comment. The works indeed are using similar ideas, but with some significant differences:\n\n1) We interpret Batch Normalization as a stochastic technique that is not necessarily Bayesian. We also present the real distribution over statistics induced by mini-batches.\n2) Similar to original Batch Normalization our interpretation is valid without an explicit prior distribution or a regularization term. As a consequence, the training procedure of a batch-normalized network is identical to the optimization of the lower bound on marginal log-likelihood in our model (Appendix A).\n\nWe should notice that the papers appeared independently. Nevertheless, your paper indeed appeared at openreview much earlier, and we will cite the paper in our next update.\n\nRegards,\nAndrei Atanov"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Uncertainty Estimation via Stochastic Batch Normalization", "abstract": "In this work, we investigate Batch Normalization technique and propose its probabilistic interpretation. We propose a probabilistic model and show that Batch Normalization maximazes the lower bound of its marginalized log-likelihood. Then, according to the new probabilistic model, we design an algorithm which acts consistently during train and test. However, inference becomes computationally inefficient. To reduce memory and computational cost, we propose Stochastic Batch Normalization -- an efficient approximation of proper inference procedure. This method provides us with a scalable uncertainty estimation technique. We demonstrate the performance of Stochastic Batch Normalization on popular architectures (including deep convolutional architectures: VGG-like and ResNets) for MNIST and CIFAR-10 datasets.\n", "paperhash": "atanov|uncertainty_estimation_via_stochastic_batch_normalization", "keywords": ["Batch Normalization", "Uncertainty Estimation", "Deep Neural Networks"], "_bibtex": "@misc{\n  atanov2018uncertainty,\n  title={Uncertainty Estimation via Stochastic Batch Normalization},\n  author={Andrei Atanov and Arsenii Ashukha and Dmitry Molchanov and Kirill Neklyudov and Dmitry Vetrov},\n  year={2018},\n  url={https://openreview.net/forum?id=r1yXEdkvz}\n}", "authorids": ["andrewatanov@yandex.ru", "ars.ashuha@gmail.com", "dmolch111@gmail.com", "k.necludov@gmail.com", "vetrovd@yandex.ru"], "authors": ["Andrei Atanov", "Arsenii Ashukha", "Dmitry Molchanov", "Kirill Neklyudov", "Dmitry Vetrov"], "TL;DR": "We propose a probabilistic view on Batch Normalization and an efficient test-time averaging technique for uncertainty estimation in batch-normalized DNNs.", "pdf": "/pdf/38f2c011368f49d35147dae0ff71b112c681b84d.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1519222446986, "id": "ICLR.cc/2018/Workshop/-/Paper243/Official_Comment", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "r1yXEdkvz", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper243/AnonReviewer[0-9]+|ICLR.cc/2018/Workshop/Paper243/Authors|ICLR.cc/2018/Workshop/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper243/AnonReviewer[0-9]+|ICLR.cc/2018/Workshop/Paper243/Authors|ICLR.cc/2018/Workshop/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Workshop/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Workshop/Paper243/Reviewers", "ICLR.cc/2018/Workshop/Paper243/Authors", "ICLR.cc/2018/Workshop/Program_Chairs"], "cdate": 1519222446986}}, "tauthor": "andrewatanov@yandex.ru"}, {"tddate": null, "ddate": null, "tmdate": 1525473165841, "tcdate": 1525473165841, "number": 1, "cdate": 1525473165841, "id": "ryU5yD5TM", "invitation": "ICLR.cc/2018/Workshop/-/Paper243/Public_Comment", "forum": "r1yXEdkvz", "replyto": "r1yXEdkvz", "signatures": ["~Kevin_Smith1"], "readers": ["everyone"], "writers": ["~Kevin_Smith1"], "content": {"title": "Request for reference to prior work with significant overlap", "comment": "Dear authors, program chairs, and reviewers,\n\nOn Oct 27, 2017 our work \"Bayesian Uncertainty Estimation for Batch Normalized Deep Networks\" was submitted to ICLR and posted on OpenReview.net five months prior to the submission of this work (https://openreview.net/forum?id=BJlrSmbAZ). It also appeared on arXiv on Feb 18, 2018 (https://arxiv.org/abs/1802.06455). The fundamental novelty of interpreting batch normalized networks as probabilistic models is identical in both works. In our article, we made the observation that stochasticity from batch normalization can be exploited to estimate predictive uncertainty. We demonstrated this empirically and argued that this process can be cast as approximate Bayesian inference.\n\nWe think the approach proposed in this work is interesting and complementary to our paper but we kindly ask the authors to include a reference to our work along with an appropriate discussion.\n\nRegards,\n\nKevin Smith (on behalf of Mattias Teye and Hossein Azizpour)\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Uncertainty Estimation via Stochastic Batch Normalization", "abstract": "In this work, we investigate Batch Normalization technique and propose its probabilistic interpretation. We propose a probabilistic model and show that Batch Normalization maximazes the lower bound of its marginalized log-likelihood. Then, according to the new probabilistic model, we design an algorithm which acts consistently during train and test. However, inference becomes computationally inefficient. To reduce memory and computational cost, we propose Stochastic Batch Normalization -- an efficient approximation of proper inference procedure. This method provides us with a scalable uncertainty estimation technique. We demonstrate the performance of Stochastic Batch Normalization on popular architectures (including deep convolutional architectures: VGG-like and ResNets) for MNIST and CIFAR-10 datasets.\n", "paperhash": "atanov|uncertainty_estimation_via_stochastic_batch_normalization", "keywords": ["Batch Normalization", "Uncertainty Estimation", "Deep Neural Networks"], "_bibtex": "@misc{\n  atanov2018uncertainty,\n  title={Uncertainty Estimation via Stochastic Batch Normalization},\n  author={Andrei Atanov and Arsenii Ashukha and Dmitry Molchanov and Kirill Neklyudov and Dmitry Vetrov},\n  year={2018},\n  url={https://openreview.net/forum?id=r1yXEdkvz}\n}", "authorids": ["andrewatanov@yandex.ru", "ars.ashuha@gmail.com", "dmolch111@gmail.com", "k.necludov@gmail.com", "vetrovd@yandex.ru"], "authors": ["Andrei Atanov", "Arsenii Ashukha", "Dmitry Molchanov", "Kirill Neklyudov", "Dmitry Vetrov"], "TL;DR": "We propose a probabilistic view on Batch Normalization and an efficient test-time averaging technique for uncertainty estimation in batch-normalized DNNs.", "pdf": "/pdf/38f2c011368f49d35147dae0ff71b112c681b84d.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518712624362, "id": "ICLR.cc/2018/Workshop/-/Paper243/Public_Comment", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper243/Reviewers"], "reply": {"replyto": null, "forum": "r1yXEdkvz", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1518712624362}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582965477, "tcdate": 1520087253561, "number": 1, "cdate": 1520087253561, "id": "S1TAxEu_f", "invitation": "ICLR.cc/2018/Workshop/-/Paper243/Official_Review", "forum": "r1yXEdkvz", "replyto": "r1yXEdkvz", "signatures": ["ICLR.cc/2018/Workshop/Paper243/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper243/AnonReviewer3"], "content": {"title": "Interesting interpretation of batch normalization, but more experiments and details needed", "rating": "6: Marginally above acceptance threshold", "review": "The authors describe an interpretation of batch normalization (BN) as a probabilistic model with latent stochastic variables. These latent variables are scale and location parameters determined by the randomness in the formation of the minibatch.  After this interpretation, the authors propose to replace the traditional batch normalization method by a probabilistic model which uses the same latent variables but learning a variational approximation for them instead of using the randomness in the minibatch sample. The resulting method is called stochastic Batch normalization (SBN). The authors present different experiments to analyze the performance of SBN. It seems that the main advantage of SBN over BN is that the randomness in the predictions can be computed without having to perform multiple passes of the data through the network.\n\nI think the paper is interesting and relevant (better understanding of batch normalization is necessary by the community), but also needs more work . In particular, I identified the following issues:\n\n1 - The authors do not compare with the original version of batch normalization. It is not clear then what are the advantages of SBN vs. BN.\n2 - The results shown do not provide enough evidence of SBN having an advantage with respect to the considered baselines (dropout and ensemble-based methods). More exhaustive experiments are needed.\n3 -  From the paper, it is unclear why SBN has a lower computational cost than BN.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Uncertainty Estimation via Stochastic Batch Normalization", "abstract": "In this work, we investigate Batch Normalization technique and propose its probabilistic interpretation. We propose a probabilistic model and show that Batch Normalization maximazes the lower bound of its marginalized log-likelihood. Then, according to the new probabilistic model, we design an algorithm which acts consistently during train and test. However, inference becomes computationally inefficient. To reduce memory and computational cost, we propose Stochastic Batch Normalization -- an efficient approximation of proper inference procedure. This method provides us with a scalable uncertainty estimation technique. We demonstrate the performance of Stochastic Batch Normalization on popular architectures (including deep convolutional architectures: VGG-like and ResNets) for MNIST and CIFAR-10 datasets.\n", "paperhash": "atanov|uncertainty_estimation_via_stochastic_batch_normalization", "keywords": ["Batch Normalization", "Uncertainty Estimation", "Deep Neural Networks"], "_bibtex": "@misc{\n  atanov2018uncertainty,\n  title={Uncertainty Estimation via Stochastic Batch Normalization},\n  author={Andrei Atanov and Arsenii Ashukha and Dmitry Molchanov and Kirill Neklyudov and Dmitry Vetrov},\n  year={2018},\n  url={https://openreview.net/forum?id=r1yXEdkvz}\n}", "authorids": ["andrewatanov@yandex.ru", "ars.ashuha@gmail.com", "dmolch111@gmail.com", "k.necludov@gmail.com", "vetrovd@yandex.ru"], "authors": ["Andrei Atanov", "Arsenii Ashukha", "Dmitry Molchanov", "Kirill Neklyudov", "Dmitry Vetrov"], "TL;DR": "We propose a probabilistic view on Batch Normalization and an efficient test-time averaging technique for uncertainty estimation in batch-normalized DNNs.", "pdf": "/pdf/38f2c011368f49d35147dae0ff71b112c681b84d.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582965247, "id": "ICLR.cc/2018/Workshop/-/Paper243/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper243/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper243/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper243/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper243/AnonReviewer1"], "reply": {"forum": "r1yXEdkvz", "replyto": "r1yXEdkvz", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper243/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper243/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582965247}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582831428, "tcdate": 1520604443485, "number": 2, "cdate": 1520604443485, "id": "rJQQrzgFG", "invitation": "ICLR.cc/2018/Workshop/-/Paper243/Official_Review", "forum": "r1yXEdkvz", "replyto": "r1yXEdkvz", "signatures": ["ICLR.cc/2018/Workshop/Paper243/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper243/AnonReviewer2"], "content": {"title": "Interesting idea but the results are not convincing", "rating": "6: Marginally above acceptance threshold", "review": "This work presents a probabilistic perspective on batch normalisation and shows that it maximises the lower bound of a marginalised, over mini batches, log-likelihood. The core idea is that for a given datapoint in the minibatch we can treat the mean and variance statistics induced by the remaining datapoints as a random variable that is integrated out. The authors then proceed to show that during training batch normalization performs an unbiased 1 sample estimate of the bound and then propose stochastic batch normalization (SBN), a technique that allows for efficient Monte Carlo estimation of the average. The overall approach is then evaluated in a variety of experiments that measure performance on in-domain data and uncertainty on out-of-domain data.\n\nThe paper presents a simple and interesting idea that can allow for the extraction of uncertainty out of batch normalised networks, which, given the prevalence of batch normalisation in modern networks, can be important. The overall technical presentation is clear although the language in the paper could use more work. Nevertheless, from the experiments it seems that the obtained uncertainty is not very useful as SBN needs to be combined with either dropout or ensembles in order to yield marginal improvements. Furthermore, I think that the experimental sections lacks discussion about the robustness of the results to the minibatch size; to me it seems that this is an important hyper parameter that affects the output uncertainty, since smaller mini batches lead to more noisy statistics and overall higher predictive entropy.\n\nPros:\n\t- Interesting idea that allows for the extraction of uncertainty out of batch normalised networks\n\t- Easy to implement \nCons:\n\t- SBN alone seems to not lead to particularly useful uncertainty\n\t- Marginal improvement over dropout and ensembles when SBN is combined with them \n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Uncertainty Estimation via Stochastic Batch Normalization", "abstract": "In this work, we investigate Batch Normalization technique and propose its probabilistic interpretation. We propose a probabilistic model and show that Batch Normalization maximazes the lower bound of its marginalized log-likelihood. Then, according to the new probabilistic model, we design an algorithm which acts consistently during train and test. However, inference becomes computationally inefficient. To reduce memory and computational cost, we propose Stochastic Batch Normalization -- an efficient approximation of proper inference procedure. This method provides us with a scalable uncertainty estimation technique. We demonstrate the performance of Stochastic Batch Normalization on popular architectures (including deep convolutional architectures: VGG-like and ResNets) for MNIST and CIFAR-10 datasets.\n", "paperhash": "atanov|uncertainty_estimation_via_stochastic_batch_normalization", "keywords": ["Batch Normalization", "Uncertainty Estimation", "Deep Neural Networks"], "_bibtex": "@misc{\n  atanov2018uncertainty,\n  title={Uncertainty Estimation via Stochastic Batch Normalization},\n  author={Andrei Atanov and Arsenii Ashukha and Dmitry Molchanov and Kirill Neklyudov and Dmitry Vetrov},\n  year={2018},\n  url={https://openreview.net/forum?id=r1yXEdkvz}\n}", "authorids": ["andrewatanov@yandex.ru", "ars.ashuha@gmail.com", "dmolch111@gmail.com", "k.necludov@gmail.com", "vetrovd@yandex.ru"], "authors": ["Andrei Atanov", "Arsenii Ashukha", "Dmitry Molchanov", "Kirill Neklyudov", "Dmitry Vetrov"], "TL;DR": "We propose a probabilistic view on Batch Normalization and an efficient test-time averaging technique for uncertainty estimation in batch-normalized DNNs.", "pdf": "/pdf/38f2c011368f49d35147dae0ff71b112c681b84d.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582965247, "id": "ICLR.cc/2018/Workshop/-/Paper243/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper243/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper243/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper243/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper243/AnonReviewer1"], "reply": {"forum": "r1yXEdkvz", "replyto": "r1yXEdkvz", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper243/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper243/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582965247}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582665116, "tcdate": 1520752233700, "number": 3, "cdate": 1520752233700, "id": "Bkf_IIztG", "invitation": "ICLR.cc/2018/Workshop/-/Paper243/Official_Review", "forum": "r1yXEdkvz", "replyto": "r1yXEdkvz", "signatures": ["ICLR.cc/2018/Workshop/Paper243/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper243/AnonReviewer1"], "content": {"title": "An okay interpretation, but needs valid empirical testing.", "rating": "6: Marginally above acceptance threshold", "review": "In this paper, the authors provide an interesting probabilistic interpretation to the Batch Normalization trick. Based on the interpretation, the authors provide stochastic batch normalization (SBN) to reduce the memory and computation cost. Specifically, the proposed stochastic batch normalization can be understood as exploiting variational inference technique to approximates the mean and variance. They conducts empirical comparison on both LeNet-5, VGG-11, and ResNet-18 on both MNIST and CIFAR-5 to illustrate the versatile of the SBN.\n\nThe only concern I have is about the validation of such Bayesian interpretation of Batch Normalization. \n\nAs the authors show in the appendix that the batch normalization is only one variational lower bound of the original MLE, which might not be the optimal bound. Following the variational principle, one can expected better performances if we optimize the distribution of mean and variational to achieve the maximium of the lower bound, rather than just approximating the arbitrary one constructed directlyy from samples. However, the authors directly approximate the distribution via minimizing the KL-divergence between the empirical distribution on mean and variance, rather than optimize the lower bound. \n\nIf the probabilistic interpretation is indeed the  reason of the success of the Batch Normalization, following the variational principle should provide better results. Otherwise, such explanation might be not the essential, which may lead to misleading. It will be great to add the empirical study with the approximation from directly minimizing the lower bound to demonstrate the validation of the probabilistic interpretation.\n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Uncertainty Estimation via Stochastic Batch Normalization", "abstract": "In this work, we investigate Batch Normalization technique and propose its probabilistic interpretation. We propose a probabilistic model and show that Batch Normalization maximazes the lower bound of its marginalized log-likelihood. Then, according to the new probabilistic model, we design an algorithm which acts consistently during train and test. However, inference becomes computationally inefficient. To reduce memory and computational cost, we propose Stochastic Batch Normalization -- an efficient approximation of proper inference procedure. This method provides us with a scalable uncertainty estimation technique. We demonstrate the performance of Stochastic Batch Normalization on popular architectures (including deep convolutional architectures: VGG-like and ResNets) for MNIST and CIFAR-10 datasets.\n", "paperhash": "atanov|uncertainty_estimation_via_stochastic_batch_normalization", "keywords": ["Batch Normalization", "Uncertainty Estimation", "Deep Neural Networks"], "_bibtex": "@misc{\n  atanov2018uncertainty,\n  title={Uncertainty Estimation via Stochastic Batch Normalization},\n  author={Andrei Atanov and Arsenii Ashukha and Dmitry Molchanov and Kirill Neklyudov and Dmitry Vetrov},\n  year={2018},\n  url={https://openreview.net/forum?id=r1yXEdkvz}\n}", "authorids": ["andrewatanov@yandex.ru", "ars.ashuha@gmail.com", "dmolch111@gmail.com", "k.necludov@gmail.com", "vetrovd@yandex.ru"], "authors": ["Andrei Atanov", "Arsenii Ashukha", "Dmitry Molchanov", "Kirill Neklyudov", "Dmitry Vetrov"], "TL;DR": "We propose a probabilistic view on Batch Normalization and an efficient test-time averaging technique for uncertainty estimation in batch-normalized DNNs.", "pdf": "/pdf/38f2c011368f49d35147dae0ff71b112c681b84d.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582965247, "id": "ICLR.cc/2018/Workshop/-/Paper243/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper243/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper243/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper243/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper243/AnonReviewer1"], "reply": {"forum": "r1yXEdkvz", "replyto": "r1yXEdkvz", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper243/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper243/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582965247}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521573571237, "tcdate": 1521573571237, "number": 124, "cdate": 1521573570895, "id": "SyipRAAtM", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "r1yXEdkvz", "replyto": "r1yXEdkvz", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Accept", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "Congratulations, your paper was accepted to the ICLR workshop."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Uncertainty Estimation via Stochastic Batch Normalization", "abstract": "In this work, we investigate Batch Normalization technique and propose its probabilistic interpretation. We propose a probabilistic model and show that Batch Normalization maximazes the lower bound of its marginalized log-likelihood. Then, according to the new probabilistic model, we design an algorithm which acts consistently during train and test. However, inference becomes computationally inefficient. To reduce memory and computational cost, we propose Stochastic Batch Normalization -- an efficient approximation of proper inference procedure. This method provides us with a scalable uncertainty estimation technique. We demonstrate the performance of Stochastic Batch Normalization on popular architectures (including deep convolutional architectures: VGG-like and ResNets) for MNIST and CIFAR-10 datasets.\n", "paperhash": "atanov|uncertainty_estimation_via_stochastic_batch_normalization", "keywords": ["Batch Normalization", "Uncertainty Estimation", "Deep Neural Networks"], "_bibtex": "@misc{\n  atanov2018uncertainty,\n  title={Uncertainty Estimation via Stochastic Batch Normalization},\n  author={Andrei Atanov and Arsenii Ashukha and Dmitry Molchanov and Kirill Neklyudov and Dmitry Vetrov},\n  year={2018},\n  url={https://openreview.net/forum?id=r1yXEdkvz}\n}", "authorids": ["andrewatanov@yandex.ru", "ars.ashuha@gmail.com", "dmolch111@gmail.com", "k.necludov@gmail.com", "vetrovd@yandex.ru"], "authors": ["Andrei Atanov", "Arsenii Ashukha", "Dmitry Molchanov", "Kirill Neklyudov", "Dmitry Vetrov"], "TL;DR": "We propose a probabilistic view on Batch Normalization and an efficient test-time averaging technique for uncertainty estimation in batch-normalized DNNs.", "pdf": "/pdf/38f2c011368f49d35147dae0ff71b112c681b84d.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}, {"tddate": null, "ddate": null, "tmdate": 1519902469664, "tcdate": 1519902469664, "number": 1, "cdate": 1519902469664, "id": "S1CbyvB_z", "invitation": "ICLR.cc/2018/Workshop/-/Paper243/Official_Comment", "forum": "r1yXEdkvz", "replyto": "r1yXEdkvz", "signatures": ["ICLR.cc/2018/Workshop/Paper243/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper243/Authors"], "content": {"title": "Code for experiments", "comment": "Code to reproduce the results of the experiments from this paper is available at https://github.com/AndrewAtanov/stochastic-batch-normalization ."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Uncertainty Estimation via Stochastic Batch Normalization", "abstract": "In this work, we investigate Batch Normalization technique and propose its probabilistic interpretation. We propose a probabilistic model and show that Batch Normalization maximazes the lower bound of its marginalized log-likelihood. Then, according to the new probabilistic model, we design an algorithm which acts consistently during train and test. However, inference becomes computationally inefficient. To reduce memory and computational cost, we propose Stochastic Batch Normalization -- an efficient approximation of proper inference procedure. This method provides us with a scalable uncertainty estimation technique. We demonstrate the performance of Stochastic Batch Normalization on popular architectures (including deep convolutional architectures: VGG-like and ResNets) for MNIST and CIFAR-10 datasets.\n", "paperhash": "atanov|uncertainty_estimation_via_stochastic_batch_normalization", "keywords": ["Batch Normalization", "Uncertainty Estimation", "Deep Neural Networks"], "_bibtex": "@misc{\n  atanov2018uncertainty,\n  title={Uncertainty Estimation via Stochastic Batch Normalization},\n  author={Andrei Atanov and Arsenii Ashukha and Dmitry Molchanov and Kirill Neklyudov and Dmitry Vetrov},\n  year={2018},\n  url={https://openreview.net/forum?id=r1yXEdkvz}\n}", "authorids": ["andrewatanov@yandex.ru", "ars.ashuha@gmail.com", "dmolch111@gmail.com", "k.necludov@gmail.com", "vetrovd@yandex.ru"], "authors": ["Andrei Atanov", "Arsenii Ashukha", "Dmitry Molchanov", "Kirill Neklyudov", "Dmitry Vetrov"], "TL;DR": "We propose a probabilistic view on Batch Normalization and an efficient test-time averaging technique for uncertainty estimation in batch-normalized DNNs.", "pdf": "/pdf/38f2c011368f49d35147dae0ff71b112c681b84d.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1519222446986, "id": "ICLR.cc/2018/Workshop/-/Paper243/Official_Comment", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "r1yXEdkvz", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper243/AnonReviewer[0-9]+|ICLR.cc/2018/Workshop/Paper243/Authors|ICLR.cc/2018/Workshop/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper243/AnonReviewer[0-9]+|ICLR.cc/2018/Workshop/Paper243/Authors|ICLR.cc/2018/Workshop/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Workshop/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Workshop/Paper243/Reviewers", "ICLR.cc/2018/Workshop/Paper243/Authors", "ICLR.cc/2018/Workshop/Program_Chairs"], "cdate": 1519222446986}}, "tauthor": "andrewatanov@yandex.ru"}], "count": 8}