{"notes": [{"id": "r1lQeoCVu4", "original": "B1x0aocEdN", "number": 54, "cdate": 1553423082911, "ddate": null, "tcdate": 1553423082911, "tmdate": 1562082113720, "tddate": null, "forum": "r1lQeoCVu4", "replyto": null, "invitation": "ICLR.cc/2019/Workshop/LLD/-/Blind_Submission", "content": {"title": "Data for free: Fewer-shot algorithm learning with parametricity data augmentation", "authors": ["Owen Lewis", "Katherine Hermann"], "authorids": ["olewis17@gmail.com", "hermannk@stanford.edu"], "keywords": ["data augmentation", "algorithm learning", "RNN"], "TL;DR": "Learned data augmentation instills algorithm-favoring inductive biases that let RNNs learn list-processing algorithms from fewer examples.", "abstract": "We address the problem of teaching an RNN to approximate list-processing algorithms given a small number of input-output training examples. Our approach is to generalize the idea of parametricity from programming language theory to formulate a semantic property that distinguishes common algorithms from arbitrary non-algorithmic functions. This characterization leads naturally to a learned data augmentation scheme that encourages RNNs to learn algorithmic behavior and enables small-sample learning in a variety of list-processing tasks.", "pdf": "/pdf/617de0a675ba14d44fd08eb6316413e092b90643.pdf", "paperhash": "lewis|data_for_free_fewershot_algorithm_learning_with_parametricity_data_augmentation"}, "signatures": ["ICLR.cc/2019/Workshop/LLD"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/LLD"], "details": {"replyCount": 3, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/LLD/-/Blind_Submission", "cdate": 1548689671889, "reply": {"forum": null, "replyto": null, "readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2019/Workshop/LLD"]}, "signatures": {"values": ["ICLR.cc/2019/Workshop/LLD"]}, "content": {"authors": {"values-regex": ".*"}, "authorids": {"values-regex": ".*"}}}, "tcdate": 1548689671889, "tmdate": 1557933709646, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/LLD"], "invitees": ["~"], "signatures": ["ICLR.cc/2019/Workshop/LLD"], "details": {"writable": true}}}, "tauthor": "OpenReview.net"}, {"id": "B1xU3LADtV", "original": null, "number": 1, "cdate": 1554667182332, "ddate": null, "tcdate": 1554667182332, "tmdate": 1555512017292, "tddate": null, "forum": "r1lQeoCVu4", "replyto": "r1lQeoCVu4", "invitation": "ICLR.cc/2019/Workshop/LLD/-/Paper54/Official_Review", "content": {"title": "Interesting Paper", "review": "\nNotes: \n  -Problem of teaching an RNN to approximate list processing algorithms from a small number of examples.  \n\n  -Learned data augmentation scheme based on parametricity from PL theory.  \n\n  -Helps with small-sample learning.  \n\n  -Algorithmic problems seem to require more training data than should be necessary.  \n\n  -List processing algorithms map from an input list to an output list.  \n\n  -Many algorithms are distinguished from arbitrary functions is that they commute over elementwise changes to the inputs.  \n\n  -Commutative means applying a function to each element of the list and running the list processor \n\n  -While I like the basic idea that there are logical properties that algorithms should obey, the commutative property for arbitrary functions seems too strong to me.  To give one example: multiplying by -1 does not commute with list sorting.  (note: the paper addresses this later).  \n\nSummary: This is a very good workshop paper which presents a simple idea\n\nSome ideas for future work and directions: \n\n  -It might be interesting to consider enforcing this type of commutative property in the hidden states.  It in some ways would require reversing your way of thinking - because you'd need to think about what properties the hidden states would need to have to allow the sequential part to be commutative, but it would have a big advantage that it would require a less data-dependent way of deciding if the function commutes or not.  \n\n  -There is a technique called Mixup (Zhang 2018) as well as Interpolation Consistency Training (Verma 2019) which tries to encourage linear combination of input examples to map to the corresponding interpolations of the outputs.  If you write the interpolation as a function x, you can rewrite this as: mix(f(x)) = f(mix(x)), where f is the neural network (this is literally what (Verma 2019) enforces and (Zhang 2018) does something slightly different).  Thus it is encouraging interpolations to commute with the neural network function.  \n\n", "rating": "5: Top 15% of accepted papers, strong accept", "confidence": "3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Workshop/LLD/Paper54/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/LLD/Paper54/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Data for free: Fewer-shot algorithm learning with parametricity data augmentation", "authors": ["Owen Lewis", "Katherine Hermann"], "authorids": ["olewis17@gmail.com", "hermannk@stanford.edu"], "keywords": ["data augmentation", "algorithm learning", "RNN"], "TL;DR": "Learned data augmentation instills algorithm-favoring inductive biases that let RNNs learn list-processing algorithms from fewer examples.", "abstract": "We address the problem of teaching an RNN to approximate list-processing algorithms given a small number of input-output training examples. Our approach is to generalize the idea of parametricity from programming language theory to formulate a semantic property that distinguishes common algorithms from arbitrary non-algorithmic functions. This characterization leads naturally to a learned data augmentation scheme that encourages RNNs to learn algorithmic behavior and enables small-sample learning in a variety of list-processing tasks.", "pdf": "/pdf/617de0a675ba14d44fd08eb6316413e092b90643.pdf", "paperhash": "lewis|data_for_free_fewershot_algorithm_learning_with_parametricity_data_augmentation"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/LLD/-/Paper54/Official_Review", "cdate": 1553713412591, "expdate": 1555718400000, "duedate": 1554681600000, "reply": {"forum": "r1lQeoCVu4", "replyto": "r1lQeoCVu4", "writers": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2019/Workshop/LLD/Paper54/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2019/Workshop/LLD/Paper54/AnonReviewer[0-9]+"}, "readers": {"values": ["everyone"], "description": "The users who will be allowed to read the above content."}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["5: Top 15% of accepted papers, strong accept", "4: Top 50% of accepted papers, clear accept", "3: Marginally above acceptance threshold", "2: Marginally below acceptance threshold", "1: Strong rejection"], "required": true}, "confidence": {"order": 4, "value-radio": ["3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "2: The reviewer is fairly confident that the evaluation is correct", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "tcdate": 1553713412591, "tmdate": 1555511822752, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/LLD"], "invitees": ["ICLR.cc/2019/Workshop/LLD/Paper54/Reviewers"], "signatures": ["ICLR.cc/2019/Workshop/LLD"]}}}, {"id": "SJlrvdkW5V", "original": null, "number": 2, "cdate": 1555261533204, "ddate": null, "tcdate": 1555261533204, "tmdate": 1555511873868, "tddate": null, "forum": "r1lQeoCVu4", "replyto": "r1lQeoCVu4", "invitation": "ICLR.cc/2019/Workshop/LLD/-/Paper54/Official_Review", "content": {"title": "Review", "review": "The paper tackles the problem of teaching an RNNs to approximate list-processing algorithms. The authors argue that what distinguishes algorithms from arbitrary functions is that they commute with a family of element-wise changes to their inputs and propose a method that learns RNN functions to approximate such family from data.\n\nTo learn commuting functions, the authors propose to synthetically generate labeled data by testing whether a function commutes with a collection of swaps. The corresponding classifier that approximates commutative swap functions is used for data augmentation. I find the observation about the parametricity property from type theory is interesting and the proposed data augmentation approach seems novel and interesting. \n\nThe only concern I have (perhaps, stemming from a mild misunderstanding of the method), if the proposed approach would work beyond the simple inputs of integer sequences (i.e., with more complex input-output data types, such as images, text, sounds, etc. as most of the modern machine learning has to deal with), or there are potential limitations that need to be resolved.", "rating": "4: Top 50% of accepted papers, clear accept", "confidence": "1: The reviewer's evaluation is an educated guess"}, "signatures": ["ICLR.cc/2019/Workshop/LLD/Paper54/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/LLD/Paper54/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Data for free: Fewer-shot algorithm learning with parametricity data augmentation", "authors": ["Owen Lewis", "Katherine Hermann"], "authorids": ["olewis17@gmail.com", "hermannk@stanford.edu"], "keywords": ["data augmentation", "algorithm learning", "RNN"], "TL;DR": "Learned data augmentation instills algorithm-favoring inductive biases that let RNNs learn list-processing algorithms from fewer examples.", "abstract": "We address the problem of teaching an RNN to approximate list-processing algorithms given a small number of input-output training examples. Our approach is to generalize the idea of parametricity from programming language theory to formulate a semantic property that distinguishes common algorithms from arbitrary non-algorithmic functions. This characterization leads naturally to a learned data augmentation scheme that encourages RNNs to learn algorithmic behavior and enables small-sample learning in a variety of list-processing tasks.", "pdf": "/pdf/617de0a675ba14d44fd08eb6316413e092b90643.pdf", "paperhash": "lewis|data_for_free_fewershot_algorithm_learning_with_parametricity_data_augmentation"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/LLD/-/Paper54/Official_Review", "cdate": 1553713412591, "expdate": 1555718400000, "duedate": 1554681600000, "reply": {"forum": "r1lQeoCVu4", "replyto": "r1lQeoCVu4", "writers": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2019/Workshop/LLD/Paper54/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2019/Workshop/LLD/Paper54/AnonReviewer[0-9]+"}, "readers": {"values": ["everyone"], "description": "The users who will be allowed to read the above content."}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["5: Top 15% of accepted papers, strong accept", "4: Top 50% of accepted papers, clear accept", "3: Marginally above acceptance threshold", "2: Marginally below acceptance threshold", "1: Strong rejection"], "required": true}, "confidence": {"order": 4, "value-radio": ["3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "2: The reviewer is fairly confident that the evaluation is correct", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "tcdate": 1553713412591, "tmdate": 1555511822752, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/LLD"], "invitees": ["ICLR.cc/2019/Workshop/LLD/Paper54/Reviewers"], "signatures": ["ICLR.cc/2019/Workshop/LLD"]}}}, {"id": "SyeTrWQM94", "original": null, "number": 1, "cdate": 1555341637208, "ddate": null, "tcdate": 1555341637208, "tmdate": 1555510981794, "tddate": null, "forum": "r1lQeoCVu4", "replyto": "r1lQeoCVu4", "invitation": "ICLR.cc/2019/Workshop/LLD/-/Paper54/Decision", "content": {"title": "Acceptance Decision", "decision": "Accept"}, "signatures": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Data for free: Fewer-shot algorithm learning with parametricity data augmentation", "authors": ["Owen Lewis", "Katherine Hermann"], "authorids": ["olewis17@gmail.com", "hermannk@stanford.edu"], "keywords": ["data augmentation", "algorithm learning", "RNN"], "TL;DR": "Learned data augmentation instills algorithm-favoring inductive biases that let RNNs learn list-processing algorithms from fewer examples.", "abstract": "We address the problem of teaching an RNN to approximate list-processing algorithms given a small number of input-output training examples. Our approach is to generalize the idea of parametricity from programming language theory to formulate a semantic property that distinguishes common algorithms from arbitrary non-algorithmic functions. This characterization leads naturally to a learned data augmentation scheme that encourages RNNs to learn algorithmic behavior and enables small-sample learning in a variety of list-processing tasks.", "pdf": "/pdf/617de0a675ba14d44fd08eb6316413e092b90643.pdf", "paperhash": "lewis|data_for_free_fewershot_algorithm_learning_with_parametricity_data_augmentation"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/LLD/-/Paper54/Decision", "cdate": 1554736074066, "reply": {"forum": "r1lQeoCVu4", "replyto": "r1lQeoCVu4", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-regex": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "description": "How your identity will be displayed."}, "signatures": {"values": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"title": {"order": 1, "required": true, "value": "Acceptance Decision"}, "decision": {"order": 2, "required": true, "value-radio": ["Accept", "Reject"], "description": "Acceptance decision"}, "comment": {"order": 3, "required": false, "value-regex": "[\\S\\s]{0,5000}", "description": ""}}}, "tcdate": 1554736074066, "tmdate": 1555510964797, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/LLD"], "invitees": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "signatures": ["ICLR.cc/2019/Workshop/LLD"]}}}], "count": 4}