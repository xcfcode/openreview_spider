{"notes": [{"id": "Oe2XI-Aft-k", "original": "NKny1AazhSm", "number": 778, "cdate": 1601308090630, "ddate": null, "tcdate": 1601308090630, "tmdate": 1614985707313, "tddate": null, "forum": "Oe2XI-Aft-k", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Perturbation Type Categorization for Multiple $\\ell_p$ Bounded Adversarial Robustness", "authorids": ["~Pratyush_Maini1", "~Xinyun_Chen1", "~Bo_Li19", "~Dawn_Song1"], "authors": ["Pratyush Maini", "Xinyun Chen", "Bo Li", "Dawn Song"], "keywords": ["adversarial examples", "robustness", "multiple perturbation types"], "abstract": "Despite the recent advances in $\\textit{adversarial training}$ based defenses, deep neural networks are still vulnerable to adversarial attacks outside the perturbation type they are trained to be robust against. Recent works have proposed defenses to improve the robustness of a single model against the union of multiple perturbation types. However, when evaluating the model against each individual attack, these methods still suffer significant trade-offs compared to the ones specifically trained to be robust against that perturbation type. In this work, we introduce the problem of categorizing adversarial examples based on their $\\ell_p$ perturbation types. Based on our analysis, we propose $\\textit{PROTECTOR}$, a two-stage pipeline to improve the robustness against multiple perturbation types. Instead of training a single predictor, $\\textit{PROTECTOR}$ first categorizes the perturbation type of the input, and then utilizes a predictor specifically trained against the predicted perturbation type to make the final prediction. We first theoretically show that adversarial examples created by different perturbation types constitute different distributions, which makes it possible to distinguish them. Further, we show that at test time the adversary faces a natural trade-off between fooling the perturbation type classifier and the succeeding predictor optimized with perturbation specific adversarial training. This makes it challenging for an adversary to plant strong attacks against the whole pipeline. In addition, we demonstrate the realization of this trade-off in deep networks by adding random noise to the model input at test time, enabling enhanced robustness  against strong adaptive attacks. Extensive experiments on MNIST and CIFAR-10 show that $\\textit{PROTECTOR}$ outperforms prior adversarial training based defenses by over $5\\%$, when tested against the union of $\\ell_1, \\ell_2, \\ell_\\infty$ attacks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "maini|perturbation_type_categorization_for_multiple_\\ell_p_bounded_adversarial_robustness", "one-sentence_summary": "We introduce a method that performs Perturbation Type Categorization for Robustness against multiple perturbation types", "pdf": "/pdf/03a9895c89930deb5d7fc789700e570957c0e56d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=xRbXv6cd6D", "_bibtex": "@misc{\nmaini2021perturbation,\ntitle={Perturbation Type Categorization for Multiple {\\$}{\\textbackslash}ell{\\_}p{\\$} Bounded Adversarial Robustness},\nauthor={Pratyush Maini and Xinyun Chen and Bo Li and Dawn Song},\nyear={2021},\nurl={https://openreview.net/forum?id=Oe2XI-Aft-k}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 13, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "OB9CIl6LsDi", "original": null, "number": 1, "cdate": 1610040436357, "ddate": null, "tcdate": 1610040436357, "tmdate": 1610474036996, "tddate": null, "forum": "Oe2XI-Aft-k", "replyto": "Oe2XI-Aft-k", "invitation": "ICLR.cc/2021/Conference/Paper778/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "The paper proposes a model to defend against multiple lp norm attacks by classifying those attacks. The reviewers raised several concerns about the methodologies. Furthermore, it's not clear how the proposed algorithm can deal with an unseen attack (e.g., only trained on l1, l_infty attacks but encounter l2 attack in the testing phase). The assumption that the attack types are known beforehand is restricted.  "}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Perturbation Type Categorization for Multiple $\\ell_p$ Bounded Adversarial Robustness", "authorids": ["~Pratyush_Maini1", "~Xinyun_Chen1", "~Bo_Li19", "~Dawn_Song1"], "authors": ["Pratyush Maini", "Xinyun Chen", "Bo Li", "Dawn Song"], "keywords": ["adversarial examples", "robustness", "multiple perturbation types"], "abstract": "Despite the recent advances in $\\textit{adversarial training}$ based defenses, deep neural networks are still vulnerable to adversarial attacks outside the perturbation type they are trained to be robust against. Recent works have proposed defenses to improve the robustness of a single model against the union of multiple perturbation types. However, when evaluating the model against each individual attack, these methods still suffer significant trade-offs compared to the ones specifically trained to be robust against that perturbation type. In this work, we introduce the problem of categorizing adversarial examples based on their $\\ell_p$ perturbation types. Based on our analysis, we propose $\\textit{PROTECTOR}$, a two-stage pipeline to improve the robustness against multiple perturbation types. Instead of training a single predictor, $\\textit{PROTECTOR}$ first categorizes the perturbation type of the input, and then utilizes a predictor specifically trained against the predicted perturbation type to make the final prediction. We first theoretically show that adversarial examples created by different perturbation types constitute different distributions, which makes it possible to distinguish them. Further, we show that at test time the adversary faces a natural trade-off between fooling the perturbation type classifier and the succeeding predictor optimized with perturbation specific adversarial training. This makes it challenging for an adversary to plant strong attacks against the whole pipeline. In addition, we demonstrate the realization of this trade-off in deep networks by adding random noise to the model input at test time, enabling enhanced robustness  against strong adaptive attacks. Extensive experiments on MNIST and CIFAR-10 show that $\\textit{PROTECTOR}$ outperforms prior adversarial training based defenses by over $5\\%$, when tested against the union of $\\ell_1, \\ell_2, \\ell_\\infty$ attacks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "maini|perturbation_type_categorization_for_multiple_\\ell_p_bounded_adversarial_robustness", "one-sentence_summary": "We introduce a method that performs Perturbation Type Categorization for Robustness against multiple perturbation types", "pdf": "/pdf/03a9895c89930deb5d7fc789700e570957c0e56d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=xRbXv6cd6D", "_bibtex": "@misc{\nmaini2021perturbation,\ntitle={Perturbation Type Categorization for Multiple {\\$}{\\textbackslash}ell{\\_}p{\\$} Bounded Adversarial Robustness},\nauthor={Pratyush Maini and Xinyun Chen and Bo Li and Dawn Song},\nyear={2021},\nurl={https://openreview.net/forum?id=Oe2XI-Aft-k}\n}"}, "tags": [], "invitation": {"reply": {"forum": "Oe2XI-Aft-k", "replyto": "Oe2XI-Aft-k", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040436344, "tmdate": 1610474036980, "id": "ICLR.cc/2021/Conference/Paper778/-/Decision"}}}, {"id": "0pMLwtEUBEy", "original": null, "number": 3, "cdate": 1603930171745, "ddate": null, "tcdate": 1603930171745, "tmdate": 1606797268890, "tddate": null, "forum": "Oe2XI-Aft-k", "replyto": "Oe2XI-Aft-k", "invitation": "ICLR.cc/2021/Conference/Paper778/-/Official_Review", "content": {"title": "The paper looks good but some points are unclear to me", "review": "This paper proposes an ensemble approach to deal with multiple perturbation types. The underlying idea is to train a robust classifier for each perturbation type (i.e., l1, l2, and l-inf) and choose a model to predict based on the decision of a perturbation classifier which is trained to distinguish perturbation types.\nThe idea is interesting, and the experiments seem solid; however, I am hesitating to give this paper an acceptance decision due to some unclear points as follows:\n1.\tI have not checked the proof of Theorem 1, but it seems a bit counter-intuitive to me. The reason is as follows. Let\u2019s assume that we are attacking a clean image x. We start from x0 inside both l1 and l-inf balls for example. We further assume that during the process of updating x{t}, we never do any projection onto any ball (i.e., x{t} always lie inside the balls). If so, there is not any difference between x_adv w.r.t l1 and l_inf, meaning that there are possibly a certain number of adversarial examples shared across l1 and l-inf attacks.\n2.\tIn your Theorem 1 (also your experiments), the radius of l1 is eps and that of l-inf is eps/sqrt(d), meaning that the ball of l-inf is a subset of the ball of l1. However, in Section 5.3, you claimed that \u201cThe dotted line shows the decision boundary for the perturbation classifier C_adv, which correctly classifies inputs subjected to large \u20181 perturbations \u03b400 as \u20181 attacks (green), but can misclassify samples with smaller perturbations\u201d. It seems that you reckon l-inf adversarial examples are having more perturbation than l1, do not you?\n3.\tTheorem 2 is not understandable to me. What is worst-case adversary (this needs to be explained and defined because we can understand this notion in several different ways)? What is delta?\n4.\tI recommend testing Projector against the attack over a uniform average of Mp in addition to what is doing.\n5.\tIt is encouraging to compare your proposed method against (Francesco Croce and Matthias Hein, 2020 a). You did mention this work, but not compare it in experiments.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper778/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper778/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Perturbation Type Categorization for Multiple $\\ell_p$ Bounded Adversarial Robustness", "authorids": ["~Pratyush_Maini1", "~Xinyun_Chen1", "~Bo_Li19", "~Dawn_Song1"], "authors": ["Pratyush Maini", "Xinyun Chen", "Bo Li", "Dawn Song"], "keywords": ["adversarial examples", "robustness", "multiple perturbation types"], "abstract": "Despite the recent advances in $\\textit{adversarial training}$ based defenses, deep neural networks are still vulnerable to adversarial attacks outside the perturbation type they are trained to be robust against. Recent works have proposed defenses to improve the robustness of a single model against the union of multiple perturbation types. However, when evaluating the model against each individual attack, these methods still suffer significant trade-offs compared to the ones specifically trained to be robust against that perturbation type. In this work, we introduce the problem of categorizing adversarial examples based on their $\\ell_p$ perturbation types. Based on our analysis, we propose $\\textit{PROTECTOR}$, a two-stage pipeline to improve the robustness against multiple perturbation types. Instead of training a single predictor, $\\textit{PROTECTOR}$ first categorizes the perturbation type of the input, and then utilizes a predictor specifically trained against the predicted perturbation type to make the final prediction. We first theoretically show that adversarial examples created by different perturbation types constitute different distributions, which makes it possible to distinguish them. Further, we show that at test time the adversary faces a natural trade-off between fooling the perturbation type classifier and the succeeding predictor optimized with perturbation specific adversarial training. This makes it challenging for an adversary to plant strong attacks against the whole pipeline. In addition, we demonstrate the realization of this trade-off in deep networks by adding random noise to the model input at test time, enabling enhanced robustness  against strong adaptive attacks. Extensive experiments on MNIST and CIFAR-10 show that $\\textit{PROTECTOR}$ outperforms prior adversarial training based defenses by over $5\\%$, when tested against the union of $\\ell_1, \\ell_2, \\ell_\\infty$ attacks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "maini|perturbation_type_categorization_for_multiple_\\ell_p_bounded_adversarial_robustness", "one-sentence_summary": "We introduce a method that performs Perturbation Type Categorization for Robustness against multiple perturbation types", "pdf": "/pdf/03a9895c89930deb5d7fc789700e570957c0e56d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=xRbXv6cd6D", "_bibtex": "@misc{\nmaini2021perturbation,\ntitle={Perturbation Type Categorization for Multiple {\\$}{\\textbackslash}ell{\\_}p{\\$} Bounded Adversarial Robustness},\nauthor={Pratyush Maini and Xinyun Chen and Bo Li and Dawn Song},\nyear={2021},\nurl={https://openreview.net/forum?id=Oe2XI-Aft-k}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Oe2XI-Aft-k", "replyto": "Oe2XI-Aft-k", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper778/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538135239, "tmdate": 1606915782747, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper778/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper778/-/Official_Review"}}}, {"id": "5pCLwowHr4O", "original": null, "number": 9, "cdate": 1606138926517, "ddate": null, "tcdate": 1606138926517, "tmdate": 1606138926517, "tddate": null, "forum": "Oe2XI-Aft-k", "replyto": "f9jxum0oodT", "invitation": "ICLR.cc/2021/Conference/Paper778/-/Official_Comment", "content": {"title": "Feedback incorporated and more experiments", "comment": "In our revision, we have incorporated all the changes that you recommended to enhance the clarity of the narrative. A summary of the changes can be found [here](https://openreview.net/forum?id=Oe2XI-Aft-k&noteId=UduYTjNuVJG).\n\n### Attack over a uniform average of $M_p$\n\nThank you for the suggestion. In the table below we present results of the *PROTECTOR* model when naively using a uniform average over the outputs from each individual model. We find that there is nearly a 25% accuracy drop for $\\ell_\\infty$ and $\\ell_2$ PGD attacks, and 4% for sparse $\\ell_1$ attack. We observe accuracy drops for both MNIST and CIFAR-10. \n\n                          Robust Accuracies (MNIST)\n\n| Attack ||| | Protector | Uniform Average | |\n|-------------------------- |-|-|-|----------------------- |-------------------------- |--- |\n| PGD $\\ell_\\infty$ ($\\epsilon_\\infty$ = 0.3) ||| | 86.7% | 62.8% | |\n| PGD $\\ell_2$ ($\\epsilon_2$ = 2.0) |||| 73.5% | 50.4% | |\n| PGD $\\ell_1$ ($\\epsilon_1$ = 10.0) |||| 69.3% | 65.3% | |\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper778/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper778/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Perturbation Type Categorization for Multiple $\\ell_p$ Bounded Adversarial Robustness", "authorids": ["~Pratyush_Maini1", "~Xinyun_Chen1", "~Bo_Li19", "~Dawn_Song1"], "authors": ["Pratyush Maini", "Xinyun Chen", "Bo Li", "Dawn Song"], "keywords": ["adversarial examples", "robustness", "multiple perturbation types"], "abstract": "Despite the recent advances in $\\textit{adversarial training}$ based defenses, deep neural networks are still vulnerable to adversarial attacks outside the perturbation type they are trained to be robust against. Recent works have proposed defenses to improve the robustness of a single model against the union of multiple perturbation types. However, when evaluating the model against each individual attack, these methods still suffer significant trade-offs compared to the ones specifically trained to be robust against that perturbation type. In this work, we introduce the problem of categorizing adversarial examples based on their $\\ell_p$ perturbation types. Based on our analysis, we propose $\\textit{PROTECTOR}$, a two-stage pipeline to improve the robustness against multiple perturbation types. Instead of training a single predictor, $\\textit{PROTECTOR}$ first categorizes the perturbation type of the input, and then utilizes a predictor specifically trained against the predicted perturbation type to make the final prediction. We first theoretically show that adversarial examples created by different perturbation types constitute different distributions, which makes it possible to distinguish them. Further, we show that at test time the adversary faces a natural trade-off between fooling the perturbation type classifier and the succeeding predictor optimized with perturbation specific adversarial training. This makes it challenging for an adversary to plant strong attacks against the whole pipeline. In addition, we demonstrate the realization of this trade-off in deep networks by adding random noise to the model input at test time, enabling enhanced robustness  against strong adaptive attacks. Extensive experiments on MNIST and CIFAR-10 show that $\\textit{PROTECTOR}$ outperforms prior adversarial training based defenses by over $5\\%$, when tested against the union of $\\ell_1, \\ell_2, \\ell_\\infty$ attacks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "maini|perturbation_type_categorization_for_multiple_\\ell_p_bounded_adversarial_robustness", "one-sentence_summary": "We introduce a method that performs Perturbation Type Categorization for Robustness against multiple perturbation types", "pdf": "/pdf/03a9895c89930deb5d7fc789700e570957c0e56d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=xRbXv6cd6D", "_bibtex": "@misc{\nmaini2021perturbation,\ntitle={Perturbation Type Categorization for Multiple {\\$}{\\textbackslash}ell{\\_}p{\\$} Bounded Adversarial Robustness},\nauthor={Pratyush Maini and Xinyun Chen and Bo Li and Dawn Song},\nyear={2021},\nurl={https://openreview.net/forum?id=Oe2XI-Aft-k}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Oe2XI-Aft-k", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper778/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper778/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper778/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper778/Authors|ICLR.cc/2021/Conference/Paper778/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper778/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923867289, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper778/-/Official_Comment"}}}, {"id": "UduYTjNuVJG", "original": null, "number": 8, "cdate": 1606138731253, "ddate": null, "tcdate": 1606138731253, "tmdate": 1606138731253, "tddate": null, "forum": "Oe2XI-Aft-k", "replyto": "Oe2XI-Aft-k", "invitation": "ICLR.cc/2021/Conference/Paper778/-/Official_Comment", "content": {"title": "Summary of Revision", "comment": "Based on the reviews, we have revised our submission to clarify reviewers\u2019 confusions and added additional experiments to provide further evidence.\n\n1. **On Perturbation Statistics**: Multiple reviewers have asked about the overlap among different perturbation types. We added a new section (**Section 6.2**) to discuss the empirical overlap of adversarial perturbations between different norms, and we observe that the overlap is 0% when attacking the vanilla model. We further added more discussion in **Appendix G**, where we show how this overlap varies when attacking *PROTECTOR*.\n2. **Theorem 1**: in Section 4.2, we have included a proof sketch to further explain the theorem.\n3. **Theorem 2**: in Section 4.3, we have added some discussion on overlapping perturbation regions, and we have also added a proof sketch to further explain the theorem.\n4. **Additional Comparison** with Croce and Hein 2020: We have made the distinction between our (empirical) work and their (certified) work clearer in the Related Works.\n5. **Theoretical Assumptions** about Dataset Distribution: In Section 3, we have added the discussion about the differences between our problem setting and that chosen by Ilyas et. al. 2018, and further motivated why our assumptions are more realistic and applicable to real-world datasets. \n6. **Section 5.2**: We have made it clear that the gradient steps are taken using the adaptive approach (hence not leading to gradient masking), and only the final forward-propagation is done using Equation 1. In Appendix H.4, we have also added comparative results on the effect of changing the method of aggregating predictions from different $M_p$ models.\n7. **Section 5.3**: The meaning of small and large perturbations has been clarified.\n\nWe once again thank all the reviewers for their feedback that helped enhance the clarity of the draft.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper778/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper778/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Perturbation Type Categorization for Multiple $\\ell_p$ Bounded Adversarial Robustness", "authorids": ["~Pratyush_Maini1", "~Xinyun_Chen1", "~Bo_Li19", "~Dawn_Song1"], "authors": ["Pratyush Maini", "Xinyun Chen", "Bo Li", "Dawn Song"], "keywords": ["adversarial examples", "robustness", "multiple perturbation types"], "abstract": "Despite the recent advances in $\\textit{adversarial training}$ based defenses, deep neural networks are still vulnerable to adversarial attacks outside the perturbation type they are trained to be robust against. Recent works have proposed defenses to improve the robustness of a single model against the union of multiple perturbation types. However, when evaluating the model against each individual attack, these methods still suffer significant trade-offs compared to the ones specifically trained to be robust against that perturbation type. In this work, we introduce the problem of categorizing adversarial examples based on their $\\ell_p$ perturbation types. Based on our analysis, we propose $\\textit{PROTECTOR}$, a two-stage pipeline to improve the robustness against multiple perturbation types. Instead of training a single predictor, $\\textit{PROTECTOR}$ first categorizes the perturbation type of the input, and then utilizes a predictor specifically trained against the predicted perturbation type to make the final prediction. We first theoretically show that adversarial examples created by different perturbation types constitute different distributions, which makes it possible to distinguish them. Further, we show that at test time the adversary faces a natural trade-off between fooling the perturbation type classifier and the succeeding predictor optimized with perturbation specific adversarial training. This makes it challenging for an adversary to plant strong attacks against the whole pipeline. In addition, we demonstrate the realization of this trade-off in deep networks by adding random noise to the model input at test time, enabling enhanced robustness  against strong adaptive attacks. Extensive experiments on MNIST and CIFAR-10 show that $\\textit{PROTECTOR}$ outperforms prior adversarial training based defenses by over $5\\%$, when tested against the union of $\\ell_1, \\ell_2, \\ell_\\infty$ attacks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "maini|perturbation_type_categorization_for_multiple_\\ell_p_bounded_adversarial_robustness", "one-sentence_summary": "We introduce a method that performs Perturbation Type Categorization for Robustness against multiple perturbation types", "pdf": "/pdf/03a9895c89930deb5d7fc789700e570957c0e56d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=xRbXv6cd6D", "_bibtex": "@misc{\nmaini2021perturbation,\ntitle={Perturbation Type Categorization for Multiple {\\$}{\\textbackslash}ell{\\_}p{\\$} Bounded Adversarial Robustness},\nauthor={Pratyush Maini and Xinyun Chen and Bo Li and Dawn Song},\nyear={2021},\nurl={https://openreview.net/forum?id=Oe2XI-Aft-k}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Oe2XI-Aft-k", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper778/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper778/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper778/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper778/Authors|ICLR.cc/2021/Conference/Paper778/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper778/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923867289, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper778/-/Official_Comment"}}}, {"id": "V1q5prt8RqE", "original": null, "number": 5, "cdate": 1605446774164, "ddate": null, "tcdate": 1605446774164, "tmdate": 1606137663983, "tddate": null, "forum": "Oe2XI-Aft-k", "replyto": "Oe2XI-Aft-k", "invitation": "ICLR.cc/2021/Conference/Paper778/-/Official_Comment", "content": {"title": "Non-Overlapping nature of different perturbation regions", "comment": "Multiple reviewers have requested for further clarification on the amount of overlap among different perturbation regions. At a high level, while the $\\ell_1,\\ell_2,\\ell_\\infty$ perturbation regions do have overlapping regions (such as $\\delta=0$), the strongest adversaries for each perturbation type lie on the boundary of the perturbation region, and are largely non-overlapping. In this response, we first provide visualization aid for these regions, then empirically quantify the overlap, and finally provide intuition for Theorem 1 on Separability of perturbation types. We will also revise our paper to add related discussion. \u200c  \n \u200c\n\n### Visualization of $\\ell_p$ regions\nConsider the setup for MNIST. While there are points in $\\ell_\\infty$ region with $\\epsilon_\\infty$=0.3 which are outside of $\\ell_2$ region with $\\epsilon_2$=2, the opposite is also true (an $\\ell_2$ perturbation at $\\epsilon_2$=2 can easily change a single pixel by more than 0.3). These radii are in line with prior work, and **one does not subsume the other**. Similar is the case for our theoretical arguments as well. For a visualization of how these perturbation regions are non-overlapping, you may refer to Figure 1 ([[ICLR2020] Provable robustness against all adversarial $l_p$-perturbations for $p\\geq 1$](https://arxiv.org/abs/1905.11213)) for a 3-dimensional perspective and Figure 1 ([[ICML2020] Adversarial Robustness Against the Union of Multiple Perturbation Models](https://arxiv.org/abs/1909.04068)) for a 2-dimensional perspective. As we move to high-dimensional spaces, these perturbation regions become increasingly non-overlapping. \u200c  \n \u200c\n\n### Perturbation statistics\nTo observe how often adversarial perturbations corresponding to one attack type lie within alternate perturbation regions, we empirically quantify the overlapping regions in case of PGD attacks on the MNIST dataset. In the following, we report the **percentage overlap**, and the **range of the norm** of perturbations in the alternate perturbation region for any given attack type -- when attacking a vanilla model. The observed overlap is 0% in all cases. A similar conclusion holds for CIFAR-10 as well. We will add the full analysis in our revision.\n\n\t\t\t\t\t\t\t\tAdversarial Overlap on MNIST (Empirical)\n\n\n| Attack                    ||| | $\\ell_\\infty < 0.3$ |  $\\ell_2 < 2.0$ |  $\\ell_1 < 10$ |   |\n|--------------------------   |-|-|-|-----------------------    |------------------   |--------   |---   |\n| PGD $\\ell_\\infty$ ($\\epsilon_\\infty$ = 0.3)    |||| 100%   |     0% (3.67 - 6.05)     | 0%  (54.8 - 140.9)    |   |\n| PGD    $\\ell_2$ ($\\epsilon_\\infty$ = 2.0)   ||| | 0% (0.40 - 0.86)       | 100%  | 0% (11.2 - 24.1)    |   |\n| Sparse-PGD $\\ell_1$ ($\\epsilon_1$ = 10)    |||| 0%    (0.70 - 1.0)       | 0% (2.08 - 2.92)      |   100%     |   | \u200c  \n \u200c\n\n### Intuition behind Theorem 1\n*R1: Let\u2019s assume that we are attacking a clean image x ..... there are possibly a certain number of adversarial examples shared across l1 and l-inf attacks.*\nIndeed, there exist regions that are common to both $\\ell_1$, $\\ell_\\infty$ norm bounded spaces. However, \n1. Adversaries in such regions can be correctly classified by both the second level models ($M_1$, $M_\\infty$). Therefore, these attacks are futile for our paradigm, since the *PROTECTOR* pipeline is robust to them. (In fact, it is an advantage of the pipeline.)\n2. The strongest adversaries generally stay at the perturbation boundary. This is because the adversarial objective is typically to maximize the loss of prediction of the victim model (during optimization steps). These regions are generally non-overlapping, and we present a detailed discussion under the previous sub-heading.  \n\nBased on the above observations, we show in Theorem 1 that the worst-case adversaries against a vanilla model $M$ are easily separable, with an assumption of perturbation sizes explained below.\u200c  \n \u200c\n\n\n\nAs elaborated in Appendix C.2, we assume that the perturbation size $\\epsilon_1$ is sufficient to stage effective attacks against $M_\\infty$ model; meanwhile, the perturbation size $\\epsilon_\\infty$ enables the  $\\ell_\\infty$ adversary to perform similarly effective attacks against $M_1$ model. The rationale is that if the $\\epsilon_p$ adversary already could not attack the alternative model $M_q$ (p \\neq q), while  $\\epsilon_q$ adversary is able to attack $M_p$, then it means that $M_q$ is already robust to the union of $\\epsilon_p$ and $\\epsilon_q$ attacks. In our work (and also in the literature of adversarial robustness against multiple attack types) we focus on the scenario where achieving the robustness against a single perturbation type does not trivially imply the robustness against other perturbation types. In fact, prior works have noted that it often hurts the robustness to other attacks. Therefore, we make the above-mentioned assumption to reflect such scenarios. We will make these points clearer in our revision."}, "signatures": ["ICLR.cc/2021/Conference/Paper778/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper778/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Perturbation Type Categorization for Multiple $\\ell_p$ Bounded Adversarial Robustness", "authorids": ["~Pratyush_Maini1", "~Xinyun_Chen1", "~Bo_Li19", "~Dawn_Song1"], "authors": ["Pratyush Maini", "Xinyun Chen", "Bo Li", "Dawn Song"], "keywords": ["adversarial examples", "robustness", "multiple perturbation types"], "abstract": "Despite the recent advances in $\\textit{adversarial training}$ based defenses, deep neural networks are still vulnerable to adversarial attacks outside the perturbation type they are trained to be robust against. Recent works have proposed defenses to improve the robustness of a single model against the union of multiple perturbation types. However, when evaluating the model against each individual attack, these methods still suffer significant trade-offs compared to the ones specifically trained to be robust against that perturbation type. In this work, we introduce the problem of categorizing adversarial examples based on their $\\ell_p$ perturbation types. Based on our analysis, we propose $\\textit{PROTECTOR}$, a two-stage pipeline to improve the robustness against multiple perturbation types. Instead of training a single predictor, $\\textit{PROTECTOR}$ first categorizes the perturbation type of the input, and then utilizes a predictor specifically trained against the predicted perturbation type to make the final prediction. We first theoretically show that adversarial examples created by different perturbation types constitute different distributions, which makes it possible to distinguish them. Further, we show that at test time the adversary faces a natural trade-off between fooling the perturbation type classifier and the succeeding predictor optimized with perturbation specific adversarial training. This makes it challenging for an adversary to plant strong attacks against the whole pipeline. In addition, we demonstrate the realization of this trade-off in deep networks by adding random noise to the model input at test time, enabling enhanced robustness  against strong adaptive attacks. Extensive experiments on MNIST and CIFAR-10 show that $\\textit{PROTECTOR}$ outperforms prior adversarial training based defenses by over $5\\%$, when tested against the union of $\\ell_1, \\ell_2, \\ell_\\infty$ attacks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "maini|perturbation_type_categorization_for_multiple_\\ell_p_bounded_adversarial_robustness", "one-sentence_summary": "We introduce a method that performs Perturbation Type Categorization for Robustness against multiple perturbation types", "pdf": "/pdf/03a9895c89930deb5d7fc789700e570957c0e56d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=xRbXv6cd6D", "_bibtex": "@misc{\nmaini2021perturbation,\ntitle={Perturbation Type Categorization for Multiple {\\$}{\\textbackslash}ell{\\_}p{\\$} Bounded Adversarial Robustness},\nauthor={Pratyush Maini and Xinyun Chen and Bo Li and Dawn Song},\nyear={2021},\nurl={https://openreview.net/forum?id=Oe2XI-Aft-k}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Oe2XI-Aft-k", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper778/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper778/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper778/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper778/Authors|ICLR.cc/2021/Conference/Paper778/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper778/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923867289, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper778/-/Official_Comment"}}}, {"id": "sIcexq1_Ibq", "original": null, "number": 2, "cdate": 1605170671305, "ddate": null, "tcdate": 1605170671305, "tmdate": 1606135805758, "tddate": null, "forum": "Oe2XI-Aft-k", "replyto": "eOZZwHJVEPX", "invitation": "ICLR.cc/2021/Conference/Paper778/-/Official_Comment", "content": {"title": "Important \u200cClarification\u200c \u200con\u200c \u200cEvaluation\u200c", "comment": "Thank you for your feedback and insightful comment on the possibility of gradient masking. We would like to clarify an important misunderstanding about the evaluation adequacy in this response, and will provide a detailed discussion on the adversarial example separability in a followup post.\n\n*\"In the last sentence of section 5.2, it says uses the soft relaxation only in generating the adversarial example, but not for inference. It clearly caused the gradient making problem in the adversarial attack later on to test the robustness.\"*\n\nWe would like to clarify that in our experiments, the adversary always constructs adversarial examples using the softmax mode in the pipeline, as described in Equation 4. Indeed, the possibility of gradient masking was our primary motivation to include adaptive attacks (using the softmax relaxation), as discussed in Section 5.2. With our adaptive attacks, the gradient is never blocked while generating the adversarial example.\n\nThe model applies the \u201cmax\u201d mode (described in Equation 1) only to provide the final prediction for generated adversarial examples. In our evaluation, we found a negligible impact of changing to the \u201csoftmax\u201d mode for producing the predictions. For example, in case of the APGD($\\ell_\\infty$, $\\ell_2$) attacks on CIFAR-10, our results are as follows:\n\n\t\t\t\t\t\t\t\tRobust Accuracies\n\n| Attack                    ||| | Max Approach (Eq (1)) | Softmax Approach (Eq(4)) |   |\n|--------------------------   |-|-|-|-----------------------   |--------------------------   |---   |\n| APGD-CE $\\ell_2$ ($\\epsilon_2$ = 0.5)   ||| | 75.7%                 | 75.6%                    |   |\n| APGD-DLR $\\ell_2$ ($\\epsilon_2$ = 0.5)    ||| | 76.5%                 | 76.7%                    |   |\n| APGD-CE    $\\ell_\\infty$ ($\\epsilon_\\infty$ = 0.03)   ||| | 86.9%                 | 86.9%                    |   |\n| APGD-DLR  $\\ell_\\infty$ ($\\epsilon_\\infty$ = 0.03)    |||| 91.8%                 | 91.2%                    |   |\n\n\n\nIn the spirit of consistency of the defense irrespective of whether the attack was adaptive or not, we decided to follow Eq (1) for the final accuracy calculation everywhere.\n\nFurther, as noted in the paper, we evaluate our models with the most comprehensive set of attacks in the adversarial ML literature, including both gradient-based and gradient-free attacks. We hope that you reconsider your review in light of this clarification. We will address other concerns in a succeeding response, and will revise our submission to make these points clearer.\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper778/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper778/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Perturbation Type Categorization for Multiple $\\ell_p$ Bounded Adversarial Robustness", "authorids": ["~Pratyush_Maini1", "~Xinyun_Chen1", "~Bo_Li19", "~Dawn_Song1"], "authors": ["Pratyush Maini", "Xinyun Chen", "Bo Li", "Dawn Song"], "keywords": ["adversarial examples", "robustness", "multiple perturbation types"], "abstract": "Despite the recent advances in $\\textit{adversarial training}$ based defenses, deep neural networks are still vulnerable to adversarial attacks outside the perturbation type they are trained to be robust against. Recent works have proposed defenses to improve the robustness of a single model against the union of multiple perturbation types. However, when evaluating the model against each individual attack, these methods still suffer significant trade-offs compared to the ones specifically trained to be robust against that perturbation type. In this work, we introduce the problem of categorizing adversarial examples based on their $\\ell_p$ perturbation types. Based on our analysis, we propose $\\textit{PROTECTOR}$, a two-stage pipeline to improve the robustness against multiple perturbation types. Instead of training a single predictor, $\\textit{PROTECTOR}$ first categorizes the perturbation type of the input, and then utilizes a predictor specifically trained against the predicted perturbation type to make the final prediction. We first theoretically show that adversarial examples created by different perturbation types constitute different distributions, which makes it possible to distinguish them. Further, we show that at test time the adversary faces a natural trade-off between fooling the perturbation type classifier and the succeeding predictor optimized with perturbation specific adversarial training. This makes it challenging for an adversary to plant strong attacks against the whole pipeline. In addition, we demonstrate the realization of this trade-off in deep networks by adding random noise to the model input at test time, enabling enhanced robustness  against strong adaptive attacks. Extensive experiments on MNIST and CIFAR-10 show that $\\textit{PROTECTOR}$ outperforms prior adversarial training based defenses by over $5\\%$, when tested against the union of $\\ell_1, \\ell_2, \\ell_\\infty$ attacks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "maini|perturbation_type_categorization_for_multiple_\\ell_p_bounded_adversarial_robustness", "one-sentence_summary": "We introduce a method that performs Perturbation Type Categorization for Robustness against multiple perturbation types", "pdf": "/pdf/03a9895c89930deb5d7fc789700e570957c0e56d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=xRbXv6cd6D", "_bibtex": "@misc{\nmaini2021perturbation,\ntitle={Perturbation Type Categorization for Multiple {\\$}{\\textbackslash}ell{\\_}p{\\$} Bounded Adversarial Robustness},\nauthor={Pratyush Maini and Xinyun Chen and Bo Li and Dawn Song},\nyear={2021},\nurl={https://openreview.net/forum?id=Oe2XI-Aft-k}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Oe2XI-Aft-k", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper778/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper778/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper778/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper778/Authors|ICLR.cc/2021/Conference/Paper778/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper778/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923867289, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper778/-/Official_Comment"}}}, {"id": "f9jxum0oodT", "original": null, "number": 7, "cdate": 1605451201392, "ddate": null, "tcdate": 1605451201392, "tmdate": 1605451565141, "tddate": null, "forum": "Oe2XI-Aft-k", "replyto": "0pMLwtEUBEy", "invitation": "ICLR.cc/2021/Conference/Paper778/-/Official_Comment", "content": {"title": "Clarifications for Reviewer 1", "comment": "Thank you for your detailing your concerns. We are happy to note that you found the idea to be interesting, while being backed up with solid experiments. We attempt to clarify all your concerns in this response, and we will also revise our paper to make these points clearer. \u200c  \n \u200c\n\nWe discuss the **separability of perturbation types** in the common response: [Non-Overlapping Nature of different perturbation regions](https://openreview.net/forum?id=Oe2XI-Aft-k&noteId=V1q5prt8RqE). The response addresses Cons (1,2) in your review. \n\nFollowing up on this discussion:   \n*The radius of l1 is eps and that of l-inf is eps/sqrt(d), meaning that the ball of linf is a subset of the ball of l1*   \nWhile the *magnitude* of the radius of the $\\ell_1$ region is larger than the $\\ell_\\infty$ region, none of the regions subsume the other. It is possible for a point in the $\\ell_\\infty$ space to exist outside the $\\ell_1$ region. Example: Consider a 100-dimensional input $\\delta = [0.3,0.3,...,0.3]$. It exists in the $\\ell_\\infty$ region of $\\epsilon_\\infty = 0.3$. However, the sum of absolute value of each component of $\\delta$, or its $\\ell_1$ norm is equal to 30. Hence, the perturbation does not belong to a region of size $\\epsilon_1 = 10$. \u200c  \n \u200c\n\n\n*In Section 5.3, you claimed ...It seems that you reckon l-inf adversarial examples are having more perturbation than l1*   \nOur discussion of the perturbation classifier does not utilize this assumption. Instead, we mean that for an $\\ell_\\infty$ adversary, if the generated adversarial perturbation is large based on the $\\ell_\\infty$ norm, then the perturbation classifier is likely to correctly classify the adversarial example as $\\ell_\\infty$. On the other hand, when the generated adversarial perturbation is small based on the $\\ell_\\infty$ norm, the perturbation classifier may not correctly categorize the perturbation type, because the perturbation may not be recognizable enough. We will revise the description of Section 5.3 to make these points clearer, and we will clarify the meaning of small and large perturbations. \u200c  \n \u200c\n\n\n### Theorem 2\n\n#### *Terminologies*\n1. Worst-case adversary refers to an adaptive adversary that has the full knowledge of the defense strategy, and makes the strongest adversarial decision given the perturbation constraints. We will add the explanation in our revision.\n2. As described in Equation 3, $\\delta$ is the perturbation induced over the original input $x$, such that $x_{adv} = x + \\delta$.\n3. $\\alpha, \\sigma, d$ are defined in the problem description in Section 4.1. \u200c  \n \u200c\n\n\n#### *High-Level Summary*\nIn Theorem 1, we showed that it is possible to distinguish between the distributions of inputs that were subjected to $\\ell_1$ and $\\ell_\\infty$ perturbations. In Theorem 2, we prove lower-bounds on the robustness of our two-stage pipeline under an adaptive attacker, and show that the adversary faces a trade-off between fooling the perturbation classifier and the second-level models.   \n \u200c\n\n\n**What does the theorem claim?**\nError rate of the *PROTECTOR* model, $P_e$ is less than 0.01 under the considered threat model.   \n**What is the problem setting?**  \n1. Data points are sampled from $\\mathcal{D}$ defined in Section 4.1. \n2. Adversarial perturbation regions considered are the $\\ell_1$ and $\\ell_\\infty$ regions. \n3. Protector pipeline consists of adversarially trained models $M_{1,\\epsilon_1}, M_{\\infty,\\epsilon_\\infty}$, and an attack classifier $C_{adv}$.\n3. The perturbation size $\\boldsymbol{\\epsilon_1} = \\alpha + 2\\sigma$ and $\\boldsymbol{\\epsilon_\\infty} = \\frac{\\alpha + 2\\sigma}{\\sqrt{d}}$. \n\nFor a more detailed description, please refer to the proof sketch of the theorem provided at the beginning of Appendix C. In a subsequent revision, we will utilize the additional page for the main paper to highlight the proof sketch, and provide a more intuitive explanation of the theorem. \u200c  \n \u200c\n\n\n### Additional Comparisons\nAs noted in the paper, we build on adversarial training baselines (Tramer & Boneh 2019, Maini et. al. 2020). However, Croce & Hein 2020, study provable robustness. As a result, the perturbation sizes considered in their work are significantly smaller than those in empirical works such as ours. For example, in case of the MNIST dataset, while we study empirical robustness at $\\epsilon_1$ = 10.0, Croce & Hein study the upper and lower bounds of the robust test error on $\\epsilon_1$ = 1.0 (see Table 1 in [Croce & Hein 2020](https://arxiv.org/abs/1905.11213)). They present a range of certified robust error rates, without empirical results as evaluated in our work & the baselines considered. Thus, the two works have different objectives and are not empirically comparable. \u200cWe will make this point clearer in our revision.  \n \u200c\n\nThank you for your time. We will shortly provide evaluation results on the uniform average of $M_p$ in a succeeding response. In the meantime, please let us know if you have any more questions. "}, "signatures": ["ICLR.cc/2021/Conference/Paper778/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper778/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Perturbation Type Categorization for Multiple $\\ell_p$ Bounded Adversarial Robustness", "authorids": ["~Pratyush_Maini1", "~Xinyun_Chen1", "~Bo_Li19", "~Dawn_Song1"], "authors": ["Pratyush Maini", "Xinyun Chen", "Bo Li", "Dawn Song"], "keywords": ["adversarial examples", "robustness", "multiple perturbation types"], "abstract": "Despite the recent advances in $\\textit{adversarial training}$ based defenses, deep neural networks are still vulnerable to adversarial attacks outside the perturbation type they are trained to be robust against. Recent works have proposed defenses to improve the robustness of a single model against the union of multiple perturbation types. However, when evaluating the model against each individual attack, these methods still suffer significant trade-offs compared to the ones specifically trained to be robust against that perturbation type. In this work, we introduce the problem of categorizing adversarial examples based on their $\\ell_p$ perturbation types. Based on our analysis, we propose $\\textit{PROTECTOR}$, a two-stage pipeline to improve the robustness against multiple perturbation types. Instead of training a single predictor, $\\textit{PROTECTOR}$ first categorizes the perturbation type of the input, and then utilizes a predictor specifically trained against the predicted perturbation type to make the final prediction. We first theoretically show that adversarial examples created by different perturbation types constitute different distributions, which makes it possible to distinguish them. Further, we show that at test time the adversary faces a natural trade-off between fooling the perturbation type classifier and the succeeding predictor optimized with perturbation specific adversarial training. This makes it challenging for an adversary to plant strong attacks against the whole pipeline. In addition, we demonstrate the realization of this trade-off in deep networks by adding random noise to the model input at test time, enabling enhanced robustness  against strong adaptive attacks. Extensive experiments on MNIST and CIFAR-10 show that $\\textit{PROTECTOR}$ outperforms prior adversarial training based defenses by over $5\\%$, when tested against the union of $\\ell_1, \\ell_2, \\ell_\\infty$ attacks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "maini|perturbation_type_categorization_for_multiple_\\ell_p_bounded_adversarial_robustness", "one-sentence_summary": "We introduce a method that performs Perturbation Type Categorization for Robustness against multiple perturbation types", "pdf": "/pdf/03a9895c89930deb5d7fc789700e570957c0e56d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=xRbXv6cd6D", "_bibtex": "@misc{\nmaini2021perturbation,\ntitle={Perturbation Type Categorization for Multiple {\\$}{\\textbackslash}ell{\\_}p{\\$} Bounded Adversarial Robustness},\nauthor={Pratyush Maini and Xinyun Chen and Bo Li and Dawn Song},\nyear={2021},\nurl={https://openreview.net/forum?id=Oe2XI-Aft-k}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Oe2XI-Aft-k", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper778/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper778/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper778/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper778/Authors|ICLR.cc/2021/Conference/Paper778/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper778/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923867289, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper778/-/Official_Comment"}}}, {"id": "jKeUfzY1hD", "original": null, "number": 6, "cdate": 1605447285651, "ddate": null, "tcdate": 1605447285651, "tmdate": 1605447285651, "tddate": null, "forum": "Oe2XI-Aft-k", "replyto": "eOZZwHJVEPX", "invitation": "ICLR.cc/2021/Conference/Paper778/-/Official_Comment", "content": {"title": "Separability of perturbation types", "comment": "We discuss the separability of perturbation types in the common response here: [Non-Overlapping nature of different perturbation regions](https://openreview.net/forum?id=Oe2XI-Aft-k&noteId=V1q5prt8RqE). The response addresses Con (3) detailed in your review.  Please refer to our [previous response](https://openreview.net/forum?id=Oe2XI-Aft-k&noteId=sIcexq1_Ibq) in this thread for discussion over other concerns."}, "signatures": ["ICLR.cc/2021/Conference/Paper778/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper778/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Perturbation Type Categorization for Multiple $\\ell_p$ Bounded Adversarial Robustness", "authorids": ["~Pratyush_Maini1", "~Xinyun_Chen1", "~Bo_Li19", "~Dawn_Song1"], "authors": ["Pratyush Maini", "Xinyun Chen", "Bo Li", "Dawn Song"], "keywords": ["adversarial examples", "robustness", "multiple perturbation types"], "abstract": "Despite the recent advances in $\\textit{adversarial training}$ based defenses, deep neural networks are still vulnerable to adversarial attacks outside the perturbation type they are trained to be robust against. Recent works have proposed defenses to improve the robustness of a single model against the union of multiple perturbation types. However, when evaluating the model against each individual attack, these methods still suffer significant trade-offs compared to the ones specifically trained to be robust against that perturbation type. In this work, we introduce the problem of categorizing adversarial examples based on their $\\ell_p$ perturbation types. Based on our analysis, we propose $\\textit{PROTECTOR}$, a two-stage pipeline to improve the robustness against multiple perturbation types. Instead of training a single predictor, $\\textit{PROTECTOR}$ first categorizes the perturbation type of the input, and then utilizes a predictor specifically trained against the predicted perturbation type to make the final prediction. We first theoretically show that adversarial examples created by different perturbation types constitute different distributions, which makes it possible to distinguish them. Further, we show that at test time the adversary faces a natural trade-off between fooling the perturbation type classifier and the succeeding predictor optimized with perturbation specific adversarial training. This makes it challenging for an adversary to plant strong attacks against the whole pipeline. In addition, we demonstrate the realization of this trade-off in deep networks by adding random noise to the model input at test time, enabling enhanced robustness  against strong adaptive attacks. Extensive experiments on MNIST and CIFAR-10 show that $\\textit{PROTECTOR}$ outperforms prior adversarial training based defenses by over $5\\%$, when tested against the union of $\\ell_1, \\ell_2, \\ell_\\infty$ attacks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "maini|perturbation_type_categorization_for_multiple_\\ell_p_bounded_adversarial_robustness", "one-sentence_summary": "We introduce a method that performs Perturbation Type Categorization for Robustness against multiple perturbation types", "pdf": "/pdf/03a9895c89930deb5d7fc789700e570957c0e56d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=xRbXv6cd6D", "_bibtex": "@misc{\nmaini2021perturbation,\ntitle={Perturbation Type Categorization for Multiple {\\$}{\\textbackslash}ell{\\_}p{\\$} Bounded Adversarial Robustness},\nauthor={Pratyush Maini and Xinyun Chen and Bo Li and Dawn Song},\nyear={2021},\nurl={https://openreview.net/forum?id=Oe2XI-Aft-k}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Oe2XI-Aft-k", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper778/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper778/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper778/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper778/Authors|ICLR.cc/2021/Conference/Paper778/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper778/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923867289, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper778/-/Official_Comment"}}}, {"id": "2_PeuMNwrse", "original": null, "number": 4, "cdate": 1605443547541, "ddate": null, "tcdate": 1605443547541, "tmdate": 1605445331766, "tddate": null, "forum": "Oe2XI-Aft-k", "replyto": "Ks_H8qg-gK", "invitation": "ICLR.cc/2021/Conference/Paper778/-/Official_Comment", "content": {"title": "Multiple perturbation types and the model structure", "comment": "Thank you for your insightful feedback on the work. We are glad you liked the clarity of the paper and our theoretical arguments that motivate the empirical results. We address your concerns and questions below, and we will revise our paper to make these points clearer. \u200c  \n \u200c\n\n### $\\ell_p$ Norm Setting\nWe agree that the problem of multiple $\\ell_p$ robustness may be a little restrictive, but it is also one of the only settings in which we have a standardized notion of attack and defense benchmarks. We believe that multiple perturbation robustness to other threat models will naturally follow in our *PROTECTOR* framework, as more attack types and individual models robust to them become standardized. However, in the absence of such attacks and defenses, it is difficult to demonstrate the efficacy of the same. We also note that the problem of multiple $\\ell_p$ norm robustness has attracted a lot of attention from the community. Apart from the citations in the main paper, there are at least the following submissions at ICLR this year that also study the same problem: \u200c  \n \n[Towards Defending Multiple Adversarial Perturbations via Gated Batch Normalization](https://openreview.net/forum?id=Utc4Yd1RD_s)   \n[Composite Adversarial Training for Multiple Adversarial Perturbations and Beyond](https://openreview.net/forum?id=H92-E4kFwbR)   \n[Learning to Generate Noise for Multi-Attack Robustness](https://openreview.net/forum?id=tv8n52XbO4p) \u200c  \n \u200c\n\n### Topological Model\n*If the attacker only cares about this input and output of this network (no pipeline here), we should have an attack strategy. How to explain this view using your analysis? Could we say this is equivalent to changing the structure of the network? From this perspective, could you tell the difference between these two views ?* \u200c  \n \u200c\n\n#### **Attack Strategy**\n\nFor both the standard attack (Eq(1)) and the softmax adaptive attack (Eq(4)), our entire pipeline (with two stages) is considered as a single model at test time. The adversary utilizes the knowledge of the input and output pairs from the entire pipeline to frame the attack strategy, as also suggested by you. In fact, the adaptive attack presented in (Eq(4)) further exploits the knowledge of the internal model structure to propagate full gradients through the two stages, and hence produces stronger adversarial examples than standard attacks that only care about model input and output.\n \nWe agree that our entire pipeline is essentially equivalent to a topological model, for which the first few layers perform perturbation type categorization, and then specialized layers for different perturbation types follow. To explain why our pipeline is supposed to be more robust to multiple perturbation types, we would like to draw your attention towards the adversarial trade-off discussed in Theorem 2 and represented in Figure 1.b. Specifically, we design this topological structure to establish a trade-off for the adversary between fooling the two stages of the topology, even when it taking a unified view of the model and *not* attacking the two stages separately. \u200c  \n \u200c\n\n#### **Training the Topological Model**   \n   \nThe key advantage that *PROTECTOR* brings is in the ability to train such a topological model in the first place. Recent work ([[ICML2020] Adversarial Robustness Against the Union of Multiple Perturbation Models](https://arxiv.org/abs/1909.04068)) has demonstrated how naive augmentation of attacks from different perturbation types leads to unstable and inconsistent training when used to train a single robust model, and often suffers from various kinds of gradient masking. On the other hand, as mentioned in Section 5.1, we observe that the training of the perturbation categorizer is stable and consistently achieves good performance.      \n   \nFurther, as also detailed in Section 5.1, these previous models take an extremely large amount of computational resources to train. For instance, the state-of-the-art network for $\\ell_\\infty$ robustness (with data augmentation) takes nearly 2 GPU days (https://github.com/yaircarmon/semisup-adv). Doing the same with augmentation of data points for different types of $\\ell_p$ attacks like in [[NeurIPS\u201919] Tramer et al., Adversarial Training and Robustness for Multiple Perturbations](https://arxiv.org/abs/1904.13000) will require 3x the time. On the other hand, the *PROTECTOR* framework is able to leverage the strong predictions of pre-existing individually robust models to train a perturbation categorizer in only a few hours.     \n   \nWe are happy to provide any further clarification about the work and thank you for your time.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper778/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper778/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Perturbation Type Categorization for Multiple $\\ell_p$ Bounded Adversarial Robustness", "authorids": ["~Pratyush_Maini1", "~Xinyun_Chen1", "~Bo_Li19", "~Dawn_Song1"], "authors": ["Pratyush Maini", "Xinyun Chen", "Bo Li", "Dawn Song"], "keywords": ["adversarial examples", "robustness", "multiple perturbation types"], "abstract": "Despite the recent advances in $\\textit{adversarial training}$ based defenses, deep neural networks are still vulnerable to adversarial attacks outside the perturbation type they are trained to be robust against. Recent works have proposed defenses to improve the robustness of a single model against the union of multiple perturbation types. However, when evaluating the model against each individual attack, these methods still suffer significant trade-offs compared to the ones specifically trained to be robust against that perturbation type. In this work, we introduce the problem of categorizing adversarial examples based on their $\\ell_p$ perturbation types. Based on our analysis, we propose $\\textit{PROTECTOR}$, a two-stage pipeline to improve the robustness against multiple perturbation types. Instead of training a single predictor, $\\textit{PROTECTOR}$ first categorizes the perturbation type of the input, and then utilizes a predictor specifically trained against the predicted perturbation type to make the final prediction. We first theoretically show that adversarial examples created by different perturbation types constitute different distributions, which makes it possible to distinguish them. Further, we show that at test time the adversary faces a natural trade-off between fooling the perturbation type classifier and the succeeding predictor optimized with perturbation specific adversarial training. This makes it challenging for an adversary to plant strong attacks against the whole pipeline. In addition, we demonstrate the realization of this trade-off in deep networks by adding random noise to the model input at test time, enabling enhanced robustness  against strong adaptive attacks. Extensive experiments on MNIST and CIFAR-10 show that $\\textit{PROTECTOR}$ outperforms prior adversarial training based defenses by over $5\\%$, when tested against the union of $\\ell_1, \\ell_2, \\ell_\\infty$ attacks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "maini|perturbation_type_categorization_for_multiple_\\ell_p_bounded_adversarial_robustness", "one-sentence_summary": "We introduce a method that performs Perturbation Type Categorization for Robustness against multiple perturbation types", "pdf": "/pdf/03a9895c89930deb5d7fc789700e570957c0e56d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=xRbXv6cd6D", "_bibtex": "@misc{\nmaini2021perturbation,\ntitle={Perturbation Type Categorization for Multiple {\\$}{\\textbackslash}ell{\\_}p{\\$} Bounded Adversarial Robustness},\nauthor={Pratyush Maini and Xinyun Chen and Bo Li and Dawn Song},\nyear={2021},\nurl={https://openreview.net/forum?id=Oe2XI-Aft-k}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Oe2XI-Aft-k", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper778/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper778/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper778/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper778/Authors|ICLR.cc/2021/Conference/Paper778/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper778/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923867289, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper778/-/Official_Comment"}}}, {"id": "7vdZA2irjkl", "original": null, "number": 3, "cdate": 1605171243282, "ddate": null, "tcdate": 1605171243282, "tmdate": 1605171336164, "tddate": null, "forum": "Oe2XI-Aft-k", "replyto": "fR5vtp3LUh", "invitation": "ICLR.cc/2021/Conference/Paper778/-/Official_Comment", "content": {"title": "Justification\u200c \u200cof\u200c \u200cAssumptions\u200c \u200cmade", "comment": "Thank\u200c \u200cyou\u200c \u200cfor\u200c \u200cyour\u200c \u200ccomment.\u200c \u200cWe\u200c \u200cwould\u200c \u200clike\u200c \u200cto\u200c \u200cpoint\u200c \u200cout\u200c \u200cthat\u200c \u200cin\u200c \u200cthe\u200c \u200cadversarial\u200c \u200cmachine\u200c \u200clearning\u200c \u200c\nliterature,\u200c \u200cpeople\u200c \u200chave\u200c \u200crelied\u200c \u200con\u200c \u200ccertain\u200c \u200cassumptions\u200c \u200cto\u200c \u200cbuild\u200c \u200ctheoretical\u200c \u200cframeworks.\u200c \u200cThough\u200c \u200c\nthese\u200c \u200ctheoretical\u200c \u200cframeworks\u200c \u200care\u200c \u200cbased\u200c \u200con\u200c \u200csimplified\u200c \u200cassumptions,\u200c \u200cthey\u200c \u200cmotivate\u200c \u200cempirical\u200c \u200c\napproaches\u200c \u200cthat\u200c \u200cwork\u200c \u200cin\u200c \u200cpractice.\u200c \u200cSpecifically,\u200c \u200cthe\u200c \u200cGaussian\u200c \u200cassumption\u200c \u200chas\u200c \u200cbeen\u200c \u200cmade\u200c \u200cin\u200c \u200ca\u200c \u200c\nnumber\u200c \u200cof\u200c \u200cprior\u200c \u200cwork\u200c \u200con\u200c \u200cadversarial\u200c \u200cmachine\u200c \u200clearning.\u200c \u200cIn\u200c \u200cthe\u200c \u200cfollowing,\u200c \u200cwe\u200c \u200clist\u200c \u200ca\u200c \u200cfew\u200c \u200cpapers\u200c \u200c\npublished\u200c \u200cin\u200c \u200crecent\u200c \u200cmachine\u200c \u200clearning\u200c \u200cconferences:\u200c   \u200c   \n\n[[NeurIPS\u201919]\u200c \u200cIlyas\u200c \u200cet\u200c \u200cal.,\u200c \u200cAdversarial\u200c \u200cExamples\u200c \u200cAre\u200c \u200cNot\u200c \u200cBugs,\u200c \u200cThey\u200c \u200cAre\u200c \u200cFeatures](https://arxiv.org/abs/1905.02175)\u200c\u200c \u200c(See\u200c \u200cSection\u200c \u200c\n4)\u200c \u200c  \n[[ICLR\u201919]\u200c \u200cTsipras\u200c \u200cet\u200c \u200cal.,\u200c \u200cRobustness\u200c \u200cMay\u200c \u200cBe\u200c \u200cat\u200c \u200cOdds\u200c \u200cwith\u200c \u200cAccuracy\u200c\u200c](https://arxiv.org/abs/1805.12152) \u200c(See\u200c \u200cSection\u200c \u200c2.1)\u200c \u200c  \n[[NeurIPS\u201919]\u200c \u200cTramer\u200c \u200cet\u200c \u200cal.,\u200c \u200cAdversarial\u200c \u200cTraining\u200c \u200cand\u200c \u200cRobustness\u200c \u200cfor\u200c \u200cMultiple\u200c \u200cPerturbations](https://arxiv.org/abs/1904.13000)\u200c\u200c \u200c(See\u200c \u200cSection\u200c \u200c2.2)\u200c \u200c  \n \u200c\n\nIn\u200c \u200cour\u200c \u200cpaper,\u200c \u200cwe\u200c \u200cmention\u200c \u200cthat\u200c \u200cthe\u200c \u200cassumptions\u200c \u200cmade\u200c \u200cin\u200c \u200cour\u200c \u200cwork\u200c \u200care\u200c \u200cadapted\u200c \u200cfrom\u200c \u200cIlyas\u200c \u200cet\u200c \u200cal.\u200c \u200c\n(Section\u200c \u200c4.1\u200c \u200cand\u200c \u200cAppendix\u200c \u200cA\u200c \u200cin\u200c \u200cour\u200c \u200cpaper).\u200c \u200cIn\u200c \u200cAppendix\u200c \u200cA\u200c \u200cof\u200c \u200cour\u200c \u200cpaper,\u200c \u200cwe\u200c \u200chave\u200c \u200ccompared\u200c \u200cour\u200c \u200c\nassumptions\u200c \u200cwith\u200c \u200cIlyas\u200c \u200cet\u200c \u200cal.\u200c \u200cWe\u200c \u200cdiscussed\u200c \u200cthat:\u200c \u200c(1)\u200c \u200cthe\u200c \u200cassumptions\u200c \u200cmade\u200c \u200cin\u200c \u200cour\u200c \u200cwork\u200c \u200cbetter\u200c \u200c\nrepresent\u200c \u200cthe\u200c \u200ccharacteristics\u200c \u200cof\u200c \u200creal-world\u200c \u200cimage\u200c \u200cdatasets;\u200c \u200cand\u200c \u200c(2)\u200c \u200cour\u200c \u200cresults\u200c \u200cnaturally\u200c \u200chold\u200c \u200cwith\u200c \u200c\nassumptions\u200c \u200cmade\u200c \u200cin\u200c \u200cIlyas\u200c \u200cet\u200c \u200cal.,\u200c \u200cwhich\u200c \u200cassumes\u200c \u200cthe\u200c \u200cpresence\u200c \u200cof\u200c \u200ca\u200c \u200cstochastic\u200c \u200cinput\u200c \u200cfeature\u200c \u200c\n(explained\u200c \u200cin\u200c \u200cAppendix\u200c \u200cA).\u200c \u200c   \n \u200c \n\nOn\u200c \u200cthe\u200c \u200cother\u200c \u200chand,\u200c \u200cwe\u200c \u200cwould\u200c \u200clike\u200c \u200cto\u200c \u200chighlight\u200c \u200cthat\u200c \u200cwhile\u200c \u200cthe\u200c \u200ctheoretical\u200c \u200cproofs\u200c \u200care\u200c \u200cimportant\u200c \u200cparts\u200c \u200c\nof\u200c \u200cthe\u200c \u200cpaper\u200c \u200cto\u200c \u200cmotivate\u200c \u200cthe\u200c \u200c\u200c*PROTECTOR* \u200cframework,\u200c \u200cthe\u200c \u200cstrong\u200c \u200cempirical\u200c \u200cresults\u200c \u200care\u200c \u200calso\u200c \u200ckey\u200c \u200c\ncontributions\u200c \u200cof\u200c \u200cour\u200c \u200cwork.\u200c \u200cIn\u200c \u200cSection\u200c \u200c6,\u200c \u200cwe\u200c \u200cshow\u200c \u200cthat\u200c \u200con\u200c \u200cadversarial\u200c \u200cexamples\u200c \u200cgenerated\u200c \u200cusing\u200c \u200c\nvarious\u200c \u200cattack\u200c \u200calgorithms,\u200c \u200cthe\u200c \u200cProtector\u200c \u200cframework\u200c \u200coutperforms\u200c \u200cthe\u200c \u200cstate-of-the-art\u200c \u200cdefenses\u200c \u200cby\u200c \u200c\nlarge\u200c \u200cmargins.\u200c \u200cWe\u200c \u200curge\u200c \u200cyou\u200c \u200cto\u200c \u200ckindly\u200c \u200calso\u200c \u200cconsider\u200c \u200cthe\u200c \u200cempirical\u200c \u200cand\u200c \u200cmethodological\u200c \u200c\ncomponents\u200c \u200cof\u200c \u200cour\u200c \u200cwork\u200c \u200cin\u200c \u200cyour\u200c \u200creview\u200c \u200cand\u200c \u200csubsequent\u200c \u200cdecision.\u200c    \u200c\n \u200c\n\nIn\u200c \u200cour\u200c \u200crevision,\u200c \u200cwe\u200c \u200cwill\u200c \u200cuse\u200c \u200cthe\u200c \u200cadditional\u200c \u200cpage\u200c \u200cto\u200c \u200cmove\u200c \u200csome\u200c \u200cdiscussion\u200c \u200cof\u200c \u200cthe\u200c \u200cdataset\u200c \u200c\nassumptions\u200c \u200cto\u200c \u200cthe\u200c \u200cmain\u200c \u200cpaper.\u200c \u200cWe\u200c \u200care\u200c \u200chappy\u200c \u200cto\u200c \u200cprovide\u200c \u200cfurther\u200c \u200cclarifications.\u200c "}, "signatures": ["ICLR.cc/2021/Conference/Paper778/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper778/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Perturbation Type Categorization for Multiple $\\ell_p$ Bounded Adversarial Robustness", "authorids": ["~Pratyush_Maini1", "~Xinyun_Chen1", "~Bo_Li19", "~Dawn_Song1"], "authors": ["Pratyush Maini", "Xinyun Chen", "Bo Li", "Dawn Song"], "keywords": ["adversarial examples", "robustness", "multiple perturbation types"], "abstract": "Despite the recent advances in $\\textit{adversarial training}$ based defenses, deep neural networks are still vulnerable to adversarial attacks outside the perturbation type they are trained to be robust against. Recent works have proposed defenses to improve the robustness of a single model against the union of multiple perturbation types. However, when evaluating the model against each individual attack, these methods still suffer significant trade-offs compared to the ones specifically trained to be robust against that perturbation type. In this work, we introduce the problem of categorizing adversarial examples based on their $\\ell_p$ perturbation types. Based on our analysis, we propose $\\textit{PROTECTOR}$, a two-stage pipeline to improve the robustness against multiple perturbation types. Instead of training a single predictor, $\\textit{PROTECTOR}$ first categorizes the perturbation type of the input, and then utilizes a predictor specifically trained against the predicted perturbation type to make the final prediction. We first theoretically show that adversarial examples created by different perturbation types constitute different distributions, which makes it possible to distinguish them. Further, we show that at test time the adversary faces a natural trade-off between fooling the perturbation type classifier and the succeeding predictor optimized with perturbation specific adversarial training. This makes it challenging for an adversary to plant strong attacks against the whole pipeline. In addition, we demonstrate the realization of this trade-off in deep networks by adding random noise to the model input at test time, enabling enhanced robustness  against strong adaptive attacks. Extensive experiments on MNIST and CIFAR-10 show that $\\textit{PROTECTOR}$ outperforms prior adversarial training based defenses by over $5\\%$, when tested against the union of $\\ell_1, \\ell_2, \\ell_\\infty$ attacks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "maini|perturbation_type_categorization_for_multiple_\\ell_p_bounded_adversarial_robustness", "one-sentence_summary": "We introduce a method that performs Perturbation Type Categorization for Robustness against multiple perturbation types", "pdf": "/pdf/03a9895c89930deb5d7fc789700e570957c0e56d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=xRbXv6cd6D", "_bibtex": "@misc{\nmaini2021perturbation,\ntitle={Perturbation Type Categorization for Multiple {\\$}{\\textbackslash}ell{\\_}p{\\$} Bounded Adversarial Robustness},\nauthor={Pratyush Maini and Xinyun Chen and Bo Li and Dawn Song},\nyear={2021},\nurl={https://openreview.net/forum?id=Oe2XI-Aft-k}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Oe2XI-Aft-k", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper778/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper778/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper778/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper778/Authors|ICLR.cc/2021/Conference/Paper778/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper778/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923867289, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper778/-/Official_Comment"}}}, {"id": "fR5vtp3LUh", "original": null, "number": 1, "cdate": 1603804845571, "ddate": null, "tcdate": 1603804845571, "tmdate": 1605024607795, "tddate": null, "forum": "Oe2XI-Aft-k", "replyto": "Oe2XI-Aft-k", "invitation": "ICLR.cc/2021/Conference/Paper778/-/Official_Review", "content": {"title": "assumptions on distribution unrealistic", "review": "In theoretical analysis, the authors conclude with an Gaussian assumption on the data distribution. \n\nThis assumption is very restrictive, and the corresponding conclusion does not provide a general view of what is happening for real datasets. Gaussian condition on dataset is very unrealistic, and its theoretical analysis should only be considered as dealing with a toy model. \n\nAuthors should remove this Gaussian assumption, or at least give strong reasons in plain natural language why Gaussianity can represent natural datasets in the adversarial study.  ", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper778/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper778/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Perturbation Type Categorization for Multiple $\\ell_p$ Bounded Adversarial Robustness", "authorids": ["~Pratyush_Maini1", "~Xinyun_Chen1", "~Bo_Li19", "~Dawn_Song1"], "authors": ["Pratyush Maini", "Xinyun Chen", "Bo Li", "Dawn Song"], "keywords": ["adversarial examples", "robustness", "multiple perturbation types"], "abstract": "Despite the recent advances in $\\textit{adversarial training}$ based defenses, deep neural networks are still vulnerable to adversarial attacks outside the perturbation type they are trained to be robust against. Recent works have proposed defenses to improve the robustness of a single model against the union of multiple perturbation types. However, when evaluating the model against each individual attack, these methods still suffer significant trade-offs compared to the ones specifically trained to be robust against that perturbation type. In this work, we introduce the problem of categorizing adversarial examples based on their $\\ell_p$ perturbation types. Based on our analysis, we propose $\\textit{PROTECTOR}$, a two-stage pipeline to improve the robustness against multiple perturbation types. Instead of training a single predictor, $\\textit{PROTECTOR}$ first categorizes the perturbation type of the input, and then utilizes a predictor specifically trained against the predicted perturbation type to make the final prediction. We first theoretically show that adversarial examples created by different perturbation types constitute different distributions, which makes it possible to distinguish them. Further, we show that at test time the adversary faces a natural trade-off between fooling the perturbation type classifier and the succeeding predictor optimized with perturbation specific adversarial training. This makes it challenging for an adversary to plant strong attacks against the whole pipeline. In addition, we demonstrate the realization of this trade-off in deep networks by adding random noise to the model input at test time, enabling enhanced robustness  against strong adaptive attacks. Extensive experiments on MNIST and CIFAR-10 show that $\\textit{PROTECTOR}$ outperforms prior adversarial training based defenses by over $5\\%$, when tested against the union of $\\ell_1, \\ell_2, \\ell_\\infty$ attacks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "maini|perturbation_type_categorization_for_multiple_\\ell_p_bounded_adversarial_robustness", "one-sentence_summary": "We introduce a method that performs Perturbation Type Categorization for Robustness against multiple perturbation types", "pdf": "/pdf/03a9895c89930deb5d7fc789700e570957c0e56d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=xRbXv6cd6D", "_bibtex": "@misc{\nmaini2021perturbation,\ntitle={Perturbation Type Categorization for Multiple {\\$}{\\textbackslash}ell{\\_}p{\\$} Bounded Adversarial Robustness},\nauthor={Pratyush Maini and Xinyun Chen and Bo Li and Dawn Song},\nyear={2021},\nurl={https://openreview.net/forum?id=Oe2XI-Aft-k}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Oe2XI-Aft-k", "replyto": "Oe2XI-Aft-k", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper778/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538135239, "tmdate": 1606915782747, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper778/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper778/-/Official_Review"}}}, {"id": "Ks_H8qg-gK", "original": null, "number": 2, "cdate": 1603842724030, "ddate": null, "tcdate": 1603842724030, "tmdate": 1605024607732, "tddate": null, "forum": "Oe2XI-Aft-k", "replyto": "Oe2XI-Aft-k", "invitation": "ICLR.cc/2021/Conference/Paper778/-/Official_Review", "content": {"title": "A good pipeline idea to improve robustness", "review": "Summary:\nThis paper proposed a pipeline method which first classify the attack type and then choose the proper predictor for that type. The authors provide both theoretical and experimental proof to support their pipeline method. \n\nPros:\n\n(1) The idea is good and reasonable. Also, the paper is well written and easy to read. The proof part is clear and natural. \n(2) The result of theorem 1 is interesting and it proved the different perturbation types (under this problem setting) is separable with a high probability. Although the setting is a little restrictive, it is still very impressive.  \n\nCons:\n(1) The method and analysis only apply to the Lp attack type. It will be good if it can be extended. \n(2) Assume that we treat the two pipelines as the whole process (end-end deep network), e.g., the first layers determines the type and then that type will determine which part to activate for training the data points, that is, you have a network with special topology and structure. If the attacker only cares about this input and output of this network (no pipeline here), we should have an attack strategy. How to explain this view using your analysis ? Could we say this is equivalent  to changing the structure of the network  ? From this perspective, could you tell the difference between these two views ?\n\n\n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper778/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper778/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Perturbation Type Categorization for Multiple $\\ell_p$ Bounded Adversarial Robustness", "authorids": ["~Pratyush_Maini1", "~Xinyun_Chen1", "~Bo_Li19", "~Dawn_Song1"], "authors": ["Pratyush Maini", "Xinyun Chen", "Bo Li", "Dawn Song"], "keywords": ["adversarial examples", "robustness", "multiple perturbation types"], "abstract": "Despite the recent advances in $\\textit{adversarial training}$ based defenses, deep neural networks are still vulnerable to adversarial attacks outside the perturbation type they are trained to be robust against. Recent works have proposed defenses to improve the robustness of a single model against the union of multiple perturbation types. However, when evaluating the model against each individual attack, these methods still suffer significant trade-offs compared to the ones specifically trained to be robust against that perturbation type. In this work, we introduce the problem of categorizing adversarial examples based on their $\\ell_p$ perturbation types. Based on our analysis, we propose $\\textit{PROTECTOR}$, a two-stage pipeline to improve the robustness against multiple perturbation types. Instead of training a single predictor, $\\textit{PROTECTOR}$ first categorizes the perturbation type of the input, and then utilizes a predictor specifically trained against the predicted perturbation type to make the final prediction. We first theoretically show that adversarial examples created by different perturbation types constitute different distributions, which makes it possible to distinguish them. Further, we show that at test time the adversary faces a natural trade-off between fooling the perturbation type classifier and the succeeding predictor optimized with perturbation specific adversarial training. This makes it challenging for an adversary to plant strong attacks against the whole pipeline. In addition, we demonstrate the realization of this trade-off in deep networks by adding random noise to the model input at test time, enabling enhanced robustness  against strong adaptive attacks. Extensive experiments on MNIST and CIFAR-10 show that $\\textit{PROTECTOR}$ outperforms prior adversarial training based defenses by over $5\\%$, when tested against the union of $\\ell_1, \\ell_2, \\ell_\\infty$ attacks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "maini|perturbation_type_categorization_for_multiple_\\ell_p_bounded_adversarial_robustness", "one-sentence_summary": "We introduce a method that performs Perturbation Type Categorization for Robustness against multiple perturbation types", "pdf": "/pdf/03a9895c89930deb5d7fc789700e570957c0e56d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=xRbXv6cd6D", "_bibtex": "@misc{\nmaini2021perturbation,\ntitle={Perturbation Type Categorization for Multiple {\\$}{\\textbackslash}ell{\\_}p{\\$} Bounded Adversarial Robustness},\nauthor={Pratyush Maini and Xinyun Chen and Bo Li and Dawn Song},\nyear={2021},\nurl={https://openreview.net/forum?id=Oe2XI-Aft-k}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Oe2XI-Aft-k", "replyto": "Oe2XI-Aft-k", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper778/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538135239, "tmdate": 1606915782747, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper778/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper778/-/Official_Review"}}}, {"id": "eOZZwHJVEPX", "original": null, "number": 4, "cdate": 1604016550713, "ddate": null, "tcdate": 1604016550713, "tmdate": 1605024607654, "tddate": null, "forum": "Oe2XI-Aft-k", "replyto": "Oe2XI-Aft-k", "invitation": "ICLR.cc/2021/Conference/Paper778/-/Official_Review", "content": {"title": "The good results may be just brought by inadequate attack evaluation.", "review": "The paper proposes a two-stage defense method to improve the adversarial robustness over different perturbation types. Specifically, it first builds a hierarchical binary classifier to differentiable the perturbation types and then uses the result to guide to its corresponding defense models.  It first proves the different types of perturbations could be separable and the adversary could be weakened to fool the binary classifier. It shows their methods achieve a clear improvement in the experiments.\n\nPros:\n1. The paper is good-written and easy to follow.\n2. The proposed idea is interesting.\n3. The experiment is detailed and comprehensive.\n\nCons:\n1. There is a major problem in their method. In the last sentence of section 5.2, it says uses the soft relaxation only in generating the adversarial example, but not for inference. It clearly caused the gradient making problem in the adversarial attack later on to test the robustness.  The gradient is blocked before reaching into the binary classifier so that the adversarial attack fails, which I think it is not truly improving the model's robustness.\n2. In my opinion, this method is just a dynamic voting based model ensemble. Just take the binary classifier as a voting procedure.  Therefore, the traditional adversarial attacks won't work in general. I would suggest using the soft relaxation in the inference as well for the adversarial attack. \n3. Also, the assumption that different norm adversarial examples could be clearly separable might be wrong. You could find the adversarial examples that satisfy both l1, l2 and l_inf constraint by just choosing the \\epsilon for every norm differently.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper778/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper778/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Perturbation Type Categorization for Multiple $\\ell_p$ Bounded Adversarial Robustness", "authorids": ["~Pratyush_Maini1", "~Xinyun_Chen1", "~Bo_Li19", "~Dawn_Song1"], "authors": ["Pratyush Maini", "Xinyun Chen", "Bo Li", "Dawn Song"], "keywords": ["adversarial examples", "robustness", "multiple perturbation types"], "abstract": "Despite the recent advances in $\\textit{adversarial training}$ based defenses, deep neural networks are still vulnerable to adversarial attacks outside the perturbation type they are trained to be robust against. Recent works have proposed defenses to improve the robustness of a single model against the union of multiple perturbation types. However, when evaluating the model against each individual attack, these methods still suffer significant trade-offs compared to the ones specifically trained to be robust against that perturbation type. In this work, we introduce the problem of categorizing adversarial examples based on their $\\ell_p$ perturbation types. Based on our analysis, we propose $\\textit{PROTECTOR}$, a two-stage pipeline to improve the robustness against multiple perturbation types. Instead of training a single predictor, $\\textit{PROTECTOR}$ first categorizes the perturbation type of the input, and then utilizes a predictor specifically trained against the predicted perturbation type to make the final prediction. We first theoretically show that adversarial examples created by different perturbation types constitute different distributions, which makes it possible to distinguish them. Further, we show that at test time the adversary faces a natural trade-off between fooling the perturbation type classifier and the succeeding predictor optimized with perturbation specific adversarial training. This makes it challenging for an adversary to plant strong attacks against the whole pipeline. In addition, we demonstrate the realization of this trade-off in deep networks by adding random noise to the model input at test time, enabling enhanced robustness  against strong adaptive attacks. Extensive experiments on MNIST and CIFAR-10 show that $\\textit{PROTECTOR}$ outperforms prior adversarial training based defenses by over $5\\%$, when tested against the union of $\\ell_1, \\ell_2, \\ell_\\infty$ attacks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "maini|perturbation_type_categorization_for_multiple_\\ell_p_bounded_adversarial_robustness", "one-sentence_summary": "We introduce a method that performs Perturbation Type Categorization for Robustness against multiple perturbation types", "pdf": "/pdf/03a9895c89930deb5d7fc789700e570957c0e56d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=xRbXv6cd6D", "_bibtex": "@misc{\nmaini2021perturbation,\ntitle={Perturbation Type Categorization for Multiple {\\$}{\\textbackslash}ell{\\_}p{\\$} Bounded Adversarial Robustness},\nauthor={Pratyush Maini and Xinyun Chen and Bo Li and Dawn Song},\nyear={2021},\nurl={https://openreview.net/forum?id=Oe2XI-Aft-k}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Oe2XI-Aft-k", "replyto": "Oe2XI-Aft-k", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper778/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538135239, "tmdate": 1606915782747, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper778/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper778/-/Official_Review"}}}], "count": 14}