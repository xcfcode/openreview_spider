{"notes": [{"tddate": null, "ddate": null, "tmdate": 1518730162910, "tcdate": 1509136080119, "number": 851, "cdate": 1518730162902, "id": "SJdCUMZAW", "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "forum": "SJdCUMZAW", "original": "BJv0UfbRZ", "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference"], "content": {"title": "Data-efficient Deep Reinforcement Learning for Dexterous Manipulation", "abstract": "Grasping an object and precisely stacking it on another is a difficult task for traditional robotic control or hand-engineered approaches. Here we examine the problem in simulation and provide techniques aimed at solving it via deep reinforcement learning. We introduce two straightforward extensions to the Deep Deterministic Policy Gradient algorithm (DDPG), which make it significantly more data-efficient and scalable. Our results show that by making extensive use of off-policy data and replay, it is possible to find high-performance control policies. Further, our results hint that it may soon be feasible to train successful stacking policies by collecting interactions on real robots.", "pdf": "/pdf/c09677c9f9200a60d9d7f9a5fff63bf107fa590d.pdf", "TL;DR": "Data-efficient deep reinforcement learning can be used to learning precise stacking policies.", "paperhash": "popov|dataefficient_deep_reinforcement_learning_for_dexterous_manipulation", "_bibtex": "@misc{\npopov2018dataefficient,\ntitle={Data-efficient Deep Reinforcement Learning for Dexterous Manipulation},\nauthor={Ivo Popov and Nicolas Heess and Timothy P. Lillicrap and Roland Hafner and Gabriel Barth-Maron and Matej Vecerik and Thomas Lampe and Tom Erez and Yuval Tassa and Martin Riedmiller},\nyear={2018},\nurl={https://openreview.net/forum?id=SJdCUMZAW},\n}", "keywords": ["Reinforcement learning", "robotics", "dexterous manipulation", "off-policy learning"], "authors": ["Ivo Popov", "Nicolas Heess", "Timothy P. Lillicrap", "Roland Hafner", "Gabriel Barth-Maron", "Matej Vecerik", "Thomas Lampe", "Tom Erez", "Yuval Tassa", "Martin Riedmiller"], "authorids": ["ivaylo.popov@hotmail.com", "heess@google.com", "countzero@google.com", "rhafner@google.com", "gabrielbm@google.com", "matejvecerik@google.com", "thomaslampe@google.com", "etom@google.com", "tassa@google.com", "riedmiller@google.com"]}, "nonreaders": [], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1506717071958, "id": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Conference"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference"]}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"authors": {"required": false, "order": 1, "values-regex": ".*", "description": "Comma separated list of author names, as they appear in the paper."}, "authorids": {"required": false, "order": 2, "values-regex": ".*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "cdate": 1506717071958}}, "tauthor": "ICLR.cc/2018/Conference"}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1517260078472, "tcdate": 1517250120704, "number": 784, "cdate": 1517250120688, "id": "HkZUU16Bz", "invitation": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "forum": "SJdCUMZAW", "replyto": "SJdCUMZAW", "signatures": ["ICLR.cc/2018/Conference/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Program_Chairs"], "content": {"decision": "Reject", "title": "ICLR 2018 Conference Acceptance Decision", "comment": "The reviewers were quite unanimous in their assessment of this paper.\n\nPROS:\n1. The paper is relatively clear and the approach makes sense\n2. The paper presents and evaluates a collection of approaches to speed learning of policies for manipulation tasks.\n3. Improving the data efficiency of learning algorithms and enabling learning across multiple robots is important for practical use in robot manipulation.\n4. The multi-stage structure of manipulation is nicely exploited in reward shaping and distribution of starting states for training.\n\nCONS\n1. Lack of novelty e.g. wrt to Finn et al. in \"Deep Spatial Autoencoders for Visuomotor Learning\"\n2. The techniques of asynchronous update and multiple replay steps may have limited novelty, building closely on previous work and applying it to this new problem.\n3. The contribution on reward shaping would benefit from a more detailed description and investigation.\n4. There is concern that results may be specific to the chosen task.\n5. Experiments using real robots are needed for practical evaluation."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Data-efficient Deep Reinforcement Learning for Dexterous Manipulation", "abstract": "Grasping an object and precisely stacking it on another is a difficult task for traditional robotic control or hand-engineered approaches. Here we examine the problem in simulation and provide techniques aimed at solving it via deep reinforcement learning. We introduce two straightforward extensions to the Deep Deterministic Policy Gradient algorithm (DDPG), which make it significantly more data-efficient and scalable. Our results show that by making extensive use of off-policy data and replay, it is possible to find high-performance control policies. Further, our results hint that it may soon be feasible to train successful stacking policies by collecting interactions on real robots.", "pdf": "/pdf/c09677c9f9200a60d9d7f9a5fff63bf107fa590d.pdf", "TL;DR": "Data-efficient deep reinforcement learning can be used to learning precise stacking policies.", "paperhash": "popov|dataefficient_deep_reinforcement_learning_for_dexterous_manipulation", "_bibtex": "@misc{\npopov2018dataefficient,\ntitle={Data-efficient Deep Reinforcement Learning for Dexterous Manipulation},\nauthor={Ivo Popov and Nicolas Heess and Timothy P. Lillicrap and Roland Hafner and Gabriel Barth-Maron and Matej Vecerik and Thomas Lampe and Tom Erez and Yuval Tassa and Martin Riedmiller},\nyear={2018},\nurl={https://openreview.net/forum?id=SJdCUMZAW},\n}", "keywords": ["Reinforcement learning", "robotics", "dexterous manipulation", "off-policy learning"], "authors": ["Ivo Popov", "Nicolas Heess", "Timothy P. Lillicrap", "Roland Hafner", "Gabriel Barth-Maron", "Matej Vecerik", "Thomas Lampe", "Tom Erez", "Yuval Tassa", "Martin Riedmiller"], "authorids": ["ivaylo.popov@hotmail.com", "heess@google.com", "countzero@google.com", "rhafner@google.com", "gabrielbm@google.com", "matejvecerik@google.com", "thomaslampe@google.com", "etom@google.com", "tassa@google.com", "riedmiller@google.com"]}, "tags": [], "invitation": {"id": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "rdate": null, "ddate": null, "expdate": 1541175629000, "duedate": null, "tmdate": 1541177635767, "tddate": null, "super": null, "final": null, "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": {"values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Conference/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Conference Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": [], "noninvitees": [], "writers": ["ICLR.cc/2018/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1541177635767}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642520936, "tcdate": 1511532892072, "number": 1, "cdate": 1511532892072, "id": "SJVDtoHef", "invitation": "ICLR.cc/2018/Conference/-/Paper851/Official_Review", "forum": "SJdCUMZAW", "replyto": "SJdCUMZAW", "signatures": ["ICLR.cc/2018/Conference/Paper851/AnonReviewer1"], "readers": ["everyone"], "content": {"title": "Neither very innovative nor very strong evaluations", "rating": "4: Ok but not good enough - rejection", "review": "I already reviewed this paper for R:SS 2017. There were no significant updates in this version, see my largely identical detailed comment in \"Official Comment\"\n\nQuality\n======\nThe proposed approaches make sense but it is unclear how task specific they are.\n\nClarity\n=====\nThe paper reads well. The authors cram 4 ideas into one paper which comes at the cost of clarity of each of them.\n\nOriginality\n=========\nThe ideas on their own are rather incremental.\n\nSignificance\n==========\nIt is unclear how widely applicable the ideas (and there combination) are an whether they would transfer to a real robot experiment. As pointed out above the ideas are not really groundbreaking on their own.\n\nPros and Cons (from the RSS AC which sums up my thoughts nicely)\n============\n+ The paper presents and evaluates a collection of approaches to speed learning of policies for manipulation tasks.\n+ Improving the data efficiency of learning algorithms and enabling learning across multiple robots is important for practical use in robot manipulation.\n+ The multi-stage structure of manipulation is nicely exploited in reward shaping and distribution of starting states for training.\n\n- The techniques of asynchronous update and multiple replay steps may have limited novelty, building closely on previous work and applying it to this new problem.\n- The contribution on reward shaping would benefit from a more detailed description and investigation.\n- There is concern that results may be specific to the chosen task.   \n- Experiments using real robots are needed for practical evaluation.\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Data-efficient Deep Reinforcement Learning for Dexterous Manipulation", "abstract": "Grasping an object and precisely stacking it on another is a difficult task for traditional robotic control or hand-engineered approaches. Here we examine the problem in simulation and provide techniques aimed at solving it via deep reinforcement learning. We introduce two straightforward extensions to the Deep Deterministic Policy Gradient algorithm (DDPG), which make it significantly more data-efficient and scalable. Our results show that by making extensive use of off-policy data and replay, it is possible to find high-performance control policies. Further, our results hint that it may soon be feasible to train successful stacking policies by collecting interactions on real robots.", "pdf": "/pdf/c09677c9f9200a60d9d7f9a5fff63bf107fa590d.pdf", "TL;DR": "Data-efficient deep reinforcement learning can be used to learning precise stacking policies.", "paperhash": "popov|dataefficient_deep_reinforcement_learning_for_dexterous_manipulation", "_bibtex": "@misc{\npopov2018dataefficient,\ntitle={Data-efficient Deep Reinforcement Learning for Dexterous Manipulation},\nauthor={Ivo Popov and Nicolas Heess and Timothy P. Lillicrap and Roland Hafner and Gabriel Barth-Maron and Matej Vecerik and Thomas Lampe and Tom Erez and Yuval Tassa and Martin Riedmiller},\nyear={2018},\nurl={https://openreview.net/forum?id=SJdCUMZAW},\n}", "keywords": ["Reinforcement learning", "robotics", "dexterous manipulation", "off-policy learning"], "authors": ["Ivo Popov", "Nicolas Heess", "Timothy P. Lillicrap", "Roland Hafner", "Gabriel Barth-Maron", "Matej Vecerik", "Thomas Lampe", "Tom Erez", "Yuval Tassa", "Martin Riedmiller"], "authorids": ["ivaylo.popov@hotmail.com", "heess@google.com", "countzero@google.com", "rhafner@google.com", "gabrielbm@google.com", "matejvecerik@google.com", "thomaslampe@google.com", "etom@google.com", "tassa@google.com", "riedmiller@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642520849, "id": "ICLR.cc/2018/Conference/-/Paper851/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper851/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper851/AnonReviewer1", "ICLR.cc/2018/Conference/Paper851/AnonReviewer3", "ICLR.cc/2018/Conference/Paper851/AnonReviewer2"], "reply": {"forum": "SJdCUMZAW", "replyto": "SJdCUMZAW", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper851/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642520849}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642520900, "tcdate": 1511650465527, "number": 2, "cdate": 1511650465527, "id": "SyFsE_Def", "invitation": "ICLR.cc/2018/Conference/-/Paper851/Official_Review", "forum": "SJdCUMZAW", "replyto": "SJdCUMZAW", "signatures": ["ICLR.cc/2018/Conference/Paper851/AnonReviewer3"], "readers": ["everyone"], "content": {"title": "The contributions already appear in prior work", "rating": "2: Strong rejection", "review": "The authors propose to learn to pick up a block and put it on another block using DDPG. A few tricks are described, which I believe already appear in prior work. The discussion of results presented in prior work also has a number of issues. The claim of \"data efficient\" learning is not really accurate, since even with demonstrations, the method requires substantially more experience than prior methods. Overall, it's hard to discern a clear contribution, either experimentally or conceptually, and the excessive claims in the paper are very off-putting. This would perhaps make a reasonable robotics paper if it had a real-world evaluation and if the claims were scoped more realistically, but as-is, I don't think this work is ready for publication.\n\nMore detailed comments:\n\nThe two main contributions -- parallel training and asynchrony -- already appear in the Gu et al. paper. In fact, that paper demonstrates learning entirely in the real world, and substantially more efficiently than described in this paper. The authors don't discuss this at all, except a passing mention of Gu et al.\n\nThe title is not appropriate for this paper. The method is data-efficient compared to what? The results don't look very data efficient: the reported result is something on the order of 160 robot-hours, and 16 robot-hours with demonstration. That's actually dramatically less efficient than prior methods.\n\n\"our results on data efficiency hint that it may soon be feasible to train successful stacking policies by collecting interactions on real robots\": Prior work already shows successful stacking policies on real robots, as well as successful pick-and-place policies and a variety of other skills. The funny thing is that many of these papers are actually cited by the authors, but they simply pretend that those works don't exist when discussing the results.\n\n\"We assess the feasibility of performing analogous experiments on real robotics hardware\": I assume this is a typo, but the paper does not actually contain any real robotics hardware experiments.\n\n\"To our knowledge our results provide the first demonstration of end-to-end learning for a complex manipulation problem involving multiple freely moving objects\": This was demonstrated by Finn et al. in \"Deep Spatial Autoencoders for Visuomotor Learning,\" with training times that are a tiny fraction of those reported in this paper, and using raw images and real hardware.\n\n\"both rely on access to a well defined and fully observed state space\": This is not true of the Finn et al. paper mentioned above.", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Data-efficient Deep Reinforcement Learning for Dexterous Manipulation", "abstract": "Grasping an object and precisely stacking it on another is a difficult task for traditional robotic control or hand-engineered approaches. Here we examine the problem in simulation and provide techniques aimed at solving it via deep reinforcement learning. We introduce two straightforward extensions to the Deep Deterministic Policy Gradient algorithm (DDPG), which make it significantly more data-efficient and scalable. Our results show that by making extensive use of off-policy data and replay, it is possible to find high-performance control policies. Further, our results hint that it may soon be feasible to train successful stacking policies by collecting interactions on real robots.", "pdf": "/pdf/c09677c9f9200a60d9d7f9a5fff63bf107fa590d.pdf", "TL;DR": "Data-efficient deep reinforcement learning can be used to learning precise stacking policies.", "paperhash": "popov|dataefficient_deep_reinforcement_learning_for_dexterous_manipulation", "_bibtex": "@misc{\npopov2018dataefficient,\ntitle={Data-efficient Deep Reinforcement Learning for Dexterous Manipulation},\nauthor={Ivo Popov and Nicolas Heess and Timothy P. Lillicrap and Roland Hafner and Gabriel Barth-Maron and Matej Vecerik and Thomas Lampe and Tom Erez and Yuval Tassa and Martin Riedmiller},\nyear={2018},\nurl={https://openreview.net/forum?id=SJdCUMZAW},\n}", "keywords": ["Reinforcement learning", "robotics", "dexterous manipulation", "off-policy learning"], "authors": ["Ivo Popov", "Nicolas Heess", "Timothy P. Lillicrap", "Roland Hafner", "Gabriel Barth-Maron", "Matej Vecerik", "Thomas Lampe", "Tom Erez", "Yuval Tassa", "Martin Riedmiller"], "authorids": ["ivaylo.popov@hotmail.com", "heess@google.com", "countzero@google.com", "rhafner@google.com", "gabrielbm@google.com", "matejvecerik@google.com", "thomaslampe@google.com", "etom@google.com", "tassa@google.com", "riedmiller@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642520849, "id": "ICLR.cc/2018/Conference/-/Paper851/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper851/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper851/AnonReviewer1", "ICLR.cc/2018/Conference/Paper851/AnonReviewer3", "ICLR.cc/2018/Conference/Paper851/AnonReviewer2"], "reply": {"forum": "SJdCUMZAW", "replyto": "SJdCUMZAW", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper851/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642520849}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642520863, "tcdate": 1511819260877, "number": 3, "cdate": 1511819260877, "id": "SkHZuZqxf", "invitation": "ICLR.cc/2018/Conference/-/Paper851/Official_Review", "forum": "SJdCUMZAW", "replyto": "SJdCUMZAW", "signatures": ["ICLR.cc/2018/Conference/Paper851/AnonReviewer2"], "readers": ["everyone"], "content": {"title": "Multiple minor extensions", "rating": "3: Clear rejection", "review": "The title is too generic and even a bit misleading. Dexterous manipulation usually refers to more complex skills, like in-hand manipulation or using the fingers to turn an object, and not simple pick and place tasks. Reinforcement learning methods are generally aiming to be data-efficient, and the method does not seem designed specifically for dexterous manipulation (which is actually a positive point, as it is more general).\n\nThe paper presents two extensions for DDPG: multiple network updates per physical interactions, and asynchronous updates from multiple robots. As the authors themselves state, these contributions are fairly straightforward, and the contributions are largely based on prior works. The  authors do evaluate the methods with different parameter settings to see the effects on learning performance. \n\nThe simulation environment is fairly basic and seems unrealistic. The hand always starts close to the blocks, which are close together, so the inverse kinematics will be close to linear. The blocks are always oriented in the same direction and they can connect easily with no need to squeeze or wiggle them together. The task seems more difficult from the description in the paper, and the authors should describe the environment in more detail.\n\nDoes the robot learn to flip the blocks over such that they can be stacked? The videos show the \nblocks turning over accidentally, but then the robot seems to give up. Having the robot learn to turn the blocks  would make for a more challenging task and a better policy.\n\nThe paper\u2019s third contribution is a recipe for constructing shaped reward functions for composite tasks. The method relies on a predefined task structure (reach-grasp-stack) and is very similar to reward shaping already used in many other reinforcement learning for manipulation papers. A comparison of different methods for defining the rewards and a more formal description of the reward generation procedure would improve the impact of this section.  The authors should also consider using tasks with longer sequences of actions, e.g., stacking four blocks. \n\nThe fourth and final listed contribution is learning from demonstrated states. Providing the robot with prior knowledge and easier partial tasks will result in faster learning. This result is not surprising. It is not clear though how applicable this approach is for a real robot system. It effectively assumes that the robot can grasp the block and pick it up, such that it can learn the stacking part, while simultaneously still learning how to grasp the block and pick it up. For testing the real robot applicability, the authors should try having the robot learn the task without simulation resets.  \n\nWhat are the actual benefits of using deep learning in this scenario? The authors mention skill representations, such as dynamic motor primitives, which employ significantly more prior knowledge than a deep network. However, as demonstrations of the task are provided, the task is divided into steps, the locations of the objects and finger tips are given, a suitable reward function is provided, and the generalization is only over the object positions, why not train a set of DMPs and optimize them with some additional reinforcement learning? The authors should consider adding a Cartesian DMP policy as a benchmark, as well as discussing the benefits of the proposed approach given the prior knowledge. ", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Data-efficient Deep Reinforcement Learning for Dexterous Manipulation", "abstract": "Grasping an object and precisely stacking it on another is a difficult task for traditional robotic control or hand-engineered approaches. Here we examine the problem in simulation and provide techniques aimed at solving it via deep reinforcement learning. We introduce two straightforward extensions to the Deep Deterministic Policy Gradient algorithm (DDPG), which make it significantly more data-efficient and scalable. Our results show that by making extensive use of off-policy data and replay, it is possible to find high-performance control policies. Further, our results hint that it may soon be feasible to train successful stacking policies by collecting interactions on real robots.", "pdf": "/pdf/c09677c9f9200a60d9d7f9a5fff63bf107fa590d.pdf", "TL;DR": "Data-efficient deep reinforcement learning can be used to learning precise stacking policies.", "paperhash": "popov|dataefficient_deep_reinforcement_learning_for_dexterous_manipulation", "_bibtex": "@misc{\npopov2018dataefficient,\ntitle={Data-efficient Deep Reinforcement Learning for Dexterous Manipulation},\nauthor={Ivo Popov and Nicolas Heess and Timothy P. Lillicrap and Roland Hafner and Gabriel Barth-Maron and Matej Vecerik and Thomas Lampe and Tom Erez and Yuval Tassa and Martin Riedmiller},\nyear={2018},\nurl={https://openreview.net/forum?id=SJdCUMZAW},\n}", "keywords": ["Reinforcement learning", "robotics", "dexterous manipulation", "off-policy learning"], "authors": ["Ivo Popov", "Nicolas Heess", "Timothy P. Lillicrap", "Roland Hafner", "Gabriel Barth-Maron", "Matej Vecerik", "Thomas Lampe", "Tom Erez", "Yuval Tassa", "Martin Riedmiller"], "authorids": ["ivaylo.popov@hotmail.com", "heess@google.com", "countzero@google.com", "rhafner@google.com", "gabrielbm@google.com", "matejvecerik@google.com", "thomaslampe@google.com", "etom@google.com", "tassa@google.com", "riedmiller@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642520849, "id": "ICLR.cc/2018/Conference/-/Paper851/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper851/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper851/AnonReviewer1", "ICLR.cc/2018/Conference/Paper851/AnonReviewer3", "ICLR.cc/2018/Conference/Paper851/AnonReviewer2"], "reply": {"forum": "SJdCUMZAW", "replyto": "SJdCUMZAW", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper851/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642520849}}}, {"tddate": null, "ddate": null, "tmdate": 1511540367933, "tcdate": 1511532299590, "number": 1, "cdate": 1511532299590, "id": "SJEzPorez", "invitation": "ICLR.cc/2018/Conference/-/Paper851/Official_Comment", "forum": "SJdCUMZAW", "replyto": "SJdCUMZAW", "signatures": ["ICLR.cc/2018/Conference/Paper851/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper851/AnonReviewer1"], "content": {"title": "Many interesting ideas, but neither very innovative nor realisitc", "comment": "The paper proposes 4 approaches to speed up deep RL: multiple replay steps, asynchronous updates, reward shaping, and selecting starting states. The various combinations of approaches are evaluated on a combined grasping and stacking task.\nI agree with the ideas related to reward shaping and selecting starting states in general. The other two approaches are rather specific to deep RL but well justified there.\nThe paper is a nice illustration that completely uninformative rewards do not work for complex, sequential tasks but that we also have to be very careful with rewards that they do not lead to the agent exploiting the reward definition in undesirable ways.\n\nBoth a strength and a weakness is that the methods proposes a whopping 4 approaches to speed up learning. It comes across as somewhat incremental for each of them and does not allow the authors to go very much in depth.\nMy main points of criticism:\n- The first 2 contributions are heavily based on prior work (as pointed out by the authors in Sect. 5, but not in the intro and summary). It is not really clear what is novel and what is just showing \"works for this type of problems as well\".\n- The contribution on reward shaping is very interesting, but then the \"meat\" is described in 10 lines which are hard to follow and to generalize to new problems.\n- The contribution on learning from starting states ends up using a pre-trained policy or demonstration. It is not really clear how much this still is RL and how much this is supervised learning. I.e., We could also pre-train the value fct. and policy based on the demonstrations in a supervised fashion. Then it would be interesting to evaluate the performance and see how much RL can additionally improve upon that.\n- I am not convinces this can really be transferred to a real robot as claimed by the authors and that the ideas are really widely applicable\n\nQuite a bit of the discussion on RL, robotics, and rendering this combination tractable could be shortened e.g. by referring to Kober, J; Bagnell, D.; Peters, J. (2013). Reinforcement Learning in Robotics: A Survey, International Journal of Robotics Research, 32, 11, pp.1238-1274.\n\nother interesting reference on parallel updates etc.\nW. Caarls and E. Schuitema, \u201cParallel Online Temporal Di\ufb00erence Learning for Motor Control,\u201d IEEE Transactions on Neural Networks and Learning Systems, vol. 27, pp. 1\u201312, Jul 2015.\n\n- If you have such a clear task decomposition (and make use of it for the reward shaping) why not learn the parts separately, e.g., in a hierarchical RL setting?\n\nMinor comments\n==============\n- Introduction: you keep talking about end-to-end but never mention what the inputs and outputs are until much later (end-to-end is typically vision+proprioception to torque, here the position and orientation of the blocks are given).\n- Section 4: what is the definition of \"physically plausible simulation\"?\n- Section 4: The observation vector is not entirely clear: 9 DoFs of the robot (position + velocity) is clear. The observations of the blocks not so much: position (3 dim?) + orientation (3 dim?), but then no velocities? And what dimension are the relative distances? Full 6 dim per block? Scalar?\n- Section 5 (multiple- mini batches):  so you make this a lot more aggressive, does re-using the samples so often not lead to overfitting?\n- Section 5 (asynchronous DPG): Any explanation why the speed-up of Grasp (8x) is significantly lower than for StackInHand (16x) for the 16 workers?\n- It would be nice to have axes label in the plots\n- \"hasn't\" => \"has not\"\n\n\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Data-efficient Deep Reinforcement Learning for Dexterous Manipulation", "abstract": "Grasping an object and precisely stacking it on another is a difficult task for traditional robotic control or hand-engineered approaches. Here we examine the problem in simulation and provide techniques aimed at solving it via deep reinforcement learning. We introduce two straightforward extensions to the Deep Deterministic Policy Gradient algorithm (DDPG), which make it significantly more data-efficient and scalable. Our results show that by making extensive use of off-policy data and replay, it is possible to find high-performance control policies. Further, our results hint that it may soon be feasible to train successful stacking policies by collecting interactions on real robots.", "pdf": "/pdf/c09677c9f9200a60d9d7f9a5fff63bf107fa590d.pdf", "TL;DR": "Data-efficient deep reinforcement learning can be used to learning precise stacking policies.", "paperhash": "popov|dataefficient_deep_reinforcement_learning_for_dexterous_manipulation", "_bibtex": "@misc{\npopov2018dataefficient,\ntitle={Data-efficient Deep Reinforcement Learning for Dexterous Manipulation},\nauthor={Ivo Popov and Nicolas Heess and Timothy P. Lillicrap and Roland Hafner and Gabriel Barth-Maron and Matej Vecerik and Thomas Lampe and Tom Erez and Yuval Tassa and Martin Riedmiller},\nyear={2018},\nurl={https://openreview.net/forum?id=SJdCUMZAW},\n}", "keywords": ["Reinforcement learning", "robotics", "dexterous manipulation", "off-policy learning"], "authors": ["Ivo Popov", "Nicolas Heess", "Timothy P. Lillicrap", "Roland Hafner", "Gabriel Barth-Maron", "Matej Vecerik", "Thomas Lampe", "Tom Erez", "Yuval Tassa", "Martin Riedmiller"], "authorids": ["ivaylo.popov@hotmail.com", "heess@google.com", "countzero@google.com", "rhafner@google.com", "gabrielbm@google.com", "matejvecerik@google.com", "thomaslampe@google.com", "etom@google.com", "tassa@google.com", "riedmiller@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825726705, "id": "ICLR.cc/2018/Conference/-/Paper851/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "SJdCUMZAW", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper851/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper851/Authors|ICLR.cc/2018/Conference/Paper851/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper851/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper851/Authors|ICLR.cc/2018/Conference/Paper851/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper851/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper851/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper851/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper851/Reviewers", "ICLR.cc/2018/Conference/Paper851/Authors", "ICLR.cc/2018/Conference/Paper851/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825726705}}}], "count": 6}