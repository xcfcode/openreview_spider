{"notes": [{"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1457649525786, "tcdate": 1457649525786, "id": "VAVwRgg9oFx0Wk76TAQv", "invitation": "ICLR.cc/2016/workshop/-/paper/188/review/12", "forum": "ROVmN8wyOSvnM0J1IpNm", "replyto": "ROVmN8wyOSvnM0J1IpNm", "signatures": ["ICLR.cc/2016/workshop/paper/188/reviewer/12"], "readers": ["everyone"], "writers": ["ICLR.cc/2016/workshop/paper/188/reviewer/12"], "content": {"title": "Interesting topic, but few results", "rating": "6: Marginally above acceptance threshold", "review": "Based on older work by Deco & Brauer (1995), in this workshop submission the authors introduce Deep Volume Conserving Neural Networks, which can be trained with many layers thanks to triagonal weight matrices. The authors offer a perspective on recent related work (Highway Nets, Residual Nets). \n\nPros: interesting topic, clearly written.\n\nCons: some concerns over the lack of content.\n\nAs stated by the authors, the underlying idea is not necessarily new (there was also other recent work at this venue exploring similar ideas: NICE: Non-linear Independent Components Estimation, Dinh et al, 2015). The question thus is whether the presented architecture works well in practice, which is not explored much.\n\nThe authors present an experiment on a variant of MNIST. They provide evidence that many layers can be trained, but report that the nets overfit to the training data. Rather than restricting the connectivity of the networks further to reduce their complexity, as suggested by the authors, I would rather recommend to tackle more challenging datasets; after all, the advantage of many layers in a feedforward net should be more expressive power.\n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Understanding Very Deep Networks via Volume Conservation", "abstract": "Recently, very deep neural networks set new records across many application domains, like Residual Networks at the ImageNet challenge and Highway Networks at language processing tasks. We expect further excellent performance improvements in different fields from these very deep networks. However these networks are still poorly understood, especially since they rely on non-standard architectures. \n\nIn this contribution we analyze the learning dynamics which are required for successfully training very deep neural networks. For the analysis we use a symplectic network architecture which inherently conserves volume when mapping a representation from one to the next layer. Therefore it avoids the vanishing gradient problem, which in turn allows to effectively train thousands of layers. We consider highway and residual networks as well as the LSTM model, all of which have approximately volume conserving mappings. \n\nWe identified two important factors for making deep architectures working:\n(1) (near) volume conserving mappings through $x = x + f(x)$ or similar (cf.\\ avoiding the vanishing gradient);\n(2) Controlling the drift effect, which increases/decreases $x$ during propagation toward the output (cf.\\ avoiding bias shifts);", "pdf": "/pdf/ROVmN8wyOSvnM0J1IpNm.pdf", "paperhash": "unterthiner|understanding_very_deep_networks_via_volume_conservation", "conflicts": ["jku.at"], "authors": ["Thomas Unterthiner", "Sepp Hochreiter"], "authorids": ["unterthiner@bioinf.jku.at", "hochreit@bioinf.jku.at"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1456580172921, "ddate": null, "super": null, "final": null, "duedate": 1460725200000, "tcdate": 1456580172921, "id": "ICLR.cc/2016/workshop/-/paper/188/review/12", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "reply": {"pdf": null, "forum": "ROVmN8wyOSvnM0J1IpNm", "replyto": "ROVmN8wyOSvnM0J1IpNm", "writers": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)"}, "signatures": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "readers": ["everyone", "ICLR.cc/2016/workshop/paper/188/reviewer/12", "ICLR.cc/2016/workshop"], "expdate": 1468501200000}}}, {"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1457645935173, "tcdate": 1457645935173, "id": "nx9215vvoF7lP3z2iomr", "invitation": "ICLR.cc/2016/workshop/-/paper/188/review/11", "forum": "ROVmN8wyOSvnM0J1IpNm", "replyto": "ROVmN8wyOSvnM0J1IpNm", "signatures": ["~Philemon_Brakel1"], "readers": ["everyone"], "writers": ["~Philemon_Brakel1"], "content": {"title": "The paper presents interesting results and an interesting hyopthesis for the success of certain neural network architectures. I didn't find the theoretical motivation very convincing and the experiments are limited. Nonetheless, I consider this paper a worthy contribution to the workshop.", "rating": "6: Marginally above acceptance threshold", "review": "The paper proposes that deep neural network architectures that work well in practice, mainly do so because of their volume preserving properties. Inspired by this, the authors propose a neural network architecture that is volume preserving by design. According to their hypothesis, this type of network can still be trained successfully after stacking hundreds of layers.\n\nI found the paper pleasant to read and very clear. The idea is relatively simple, but from a practical point of view, this can actually be considered a positive attribute.\n\nI found the theoretical motivations for the importance of volume preservation unconvincing. A mapping that preserves volume, does not necessarily preserve the norm of the vectors it acts upon. The matrix diag(5, 0.2) has a Jacobian with determinant 1, but repeatedly applying it to a vector of ones would still let the first element grow and the second element vanish at an exponential rate.\n\nAnother shortcoming of the paper is the absence of comparisons with the other architectures that are discussed. The empirical evidence for the volume preservation hypothesis would have been much stronger if the amount of success of the very deep networks could directly be related to the extent to which the volume preserving property holds for them. The highway networks and residual networks are only approximately volume preserving, so it would be very interesting to see if exactly volume preserving nets can be used to train even deeper networks.  \n\nAll in all, I still find the results and ideas interesting enough for a workshop presentation.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Understanding Very Deep Networks via Volume Conservation", "abstract": "Recently, very deep neural networks set new records across many application domains, like Residual Networks at the ImageNet challenge and Highway Networks at language processing tasks. We expect further excellent performance improvements in different fields from these very deep networks. However these networks are still poorly understood, especially since they rely on non-standard architectures. \n\nIn this contribution we analyze the learning dynamics which are required for successfully training very deep neural networks. For the analysis we use a symplectic network architecture which inherently conserves volume when mapping a representation from one to the next layer. Therefore it avoids the vanishing gradient problem, which in turn allows to effectively train thousands of layers. We consider highway and residual networks as well as the LSTM model, all of which have approximately volume conserving mappings. \n\nWe identified two important factors for making deep architectures working:\n(1) (near) volume conserving mappings through $x = x + f(x)$ or similar (cf.\\ avoiding the vanishing gradient);\n(2) Controlling the drift effect, which increases/decreases $x$ during propagation toward the output (cf.\\ avoiding bias shifts);", "pdf": "/pdf/ROVmN8wyOSvnM0J1IpNm.pdf", "paperhash": "unterthiner|understanding_very_deep_networks_via_volume_conservation", "conflicts": ["jku.at"], "authors": ["Thomas Unterthiner", "Sepp Hochreiter"], "authorids": ["unterthiner@bioinf.jku.at", "hochreit@bioinf.jku.at"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1456580173594, "ddate": null, "super": null, "final": null, "duedate": 1460725200000, "tcdate": 1456580173594, "id": "ICLR.cc/2016/workshop/-/paper/188/review/11", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "reply": {"pdf": null, "forum": "ROVmN8wyOSvnM0J1IpNm", "replyto": "ROVmN8wyOSvnM0J1IpNm", "writers": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)"}, "signatures": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "readers": ["everyone", "ICLR.cc/2016/workshop/paper/188/reviewer/11", "ICLR.cc/2016/workshop"], "expdate": 1468501200000}}}, {"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1457573170175, "tcdate": 1457573170175, "id": "GvV1QJJkoc1WDOmRiMQE", "invitation": "ICLR.cc/2016/workshop/-/paper/188/review/10", "forum": "ROVmN8wyOSvnM0J1IpNm", "replyto": "ROVmN8wyOSvnM0J1IpNm", "signatures": ["ICLR.cc/2016/workshop/paper/188/reviewer/10"], "readers": ["everyone"], "writers": ["ICLR.cc/2016/workshop/paper/188/reviewer/10"], "content": {"title": "Interesting topic", "rating": "8: Top 50% of accepted papers, clear accept", "review": "This is an important topic: models with linear pathways have been a major part of recent successes in deep learning. The factors identified sound interesting. I'd be curious to learn more; it sounds like a good workshop poster.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Understanding Very Deep Networks via Volume Conservation", "abstract": "Recently, very deep neural networks set new records across many application domains, like Residual Networks at the ImageNet challenge and Highway Networks at language processing tasks. We expect further excellent performance improvements in different fields from these very deep networks. However these networks are still poorly understood, especially since they rely on non-standard architectures. \n\nIn this contribution we analyze the learning dynamics which are required for successfully training very deep neural networks. For the analysis we use a symplectic network architecture which inherently conserves volume when mapping a representation from one to the next layer. Therefore it avoids the vanishing gradient problem, which in turn allows to effectively train thousands of layers. We consider highway and residual networks as well as the LSTM model, all of which have approximately volume conserving mappings. \n\nWe identified two important factors for making deep architectures working:\n(1) (near) volume conserving mappings through $x = x + f(x)$ or similar (cf.\\ avoiding the vanishing gradient);\n(2) Controlling the drift effect, which increases/decreases $x$ during propagation toward the output (cf.\\ avoiding bias shifts);", "pdf": "/pdf/ROVmN8wyOSvnM0J1IpNm.pdf", "paperhash": "unterthiner|understanding_very_deep_networks_via_volume_conservation", "conflicts": ["jku.at"], "authors": ["Thomas Unterthiner", "Sepp Hochreiter"], "authorids": ["unterthiner@bioinf.jku.at", "hochreit@bioinf.jku.at"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1456580173969, "ddate": null, "super": null, "final": null, "duedate": 1460725200000, "tcdate": 1456580173969, "id": "ICLR.cc/2016/workshop/-/paper/188/review/10", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "reply": {"pdf": null, "forum": "ROVmN8wyOSvnM0J1IpNm", "replyto": "ROVmN8wyOSvnM0J1IpNm", "writers": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)"}, "signatures": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "readers": ["everyone", "ICLR.cc/2016/workshop/paper/188/reviewer/10", "ICLR.cc/2016/workshop"], "expdate": 1468501200000}}}, {"tddate": null, "number": null, "replyto": null, "ddate": null, "cdate": null, "tmdate": 1455832576444, "tcdate": 1455832576444, "id": "ROVmN8wyOSvnM0J1IpNm", "invitation": "ICLR.cc/2016/workshop/-/submission", "forum": "ROVmN8wyOSvnM0J1IpNm", "signatures": ["~Thomas_Unterthiner1"], "readers": ["everyone"], "writers": ["~Thomas_Unterthiner1"], "content": {"CMT_id": "", "title": "Understanding Very Deep Networks via Volume Conservation", "abstract": "Recently, very deep neural networks set new records across many application domains, like Residual Networks at the ImageNet challenge and Highway Networks at language processing tasks. We expect further excellent performance improvements in different fields from these very deep networks. However these networks are still poorly understood, especially since they rely on non-standard architectures. \n\nIn this contribution we analyze the learning dynamics which are required for successfully training very deep neural networks. For the analysis we use a symplectic network architecture which inherently conserves volume when mapping a representation from one to the next layer. Therefore it avoids the vanishing gradient problem, which in turn allows to effectively train thousands of layers. We consider highway and residual networks as well as the LSTM model, all of which have approximately volume conserving mappings. \n\nWe identified two important factors for making deep architectures working:\n(1) (near) volume conserving mappings through $x = x + f(x)$ or similar (cf.\\ avoiding the vanishing gradient);\n(2) Controlling the drift effect, which increases/decreases $x$ during propagation toward the output (cf.\\ avoiding bias shifts);", "pdf": "/pdf/ROVmN8wyOSvnM0J1IpNm.pdf", "paperhash": "unterthiner|understanding_very_deep_networks_via_volume_conservation", "conflicts": ["jku.at"], "authors": ["Thomas Unterthiner", "Sepp Hochreiter"], "authorids": ["unterthiner@bioinf.jku.at", "hochreit@bioinf.jku.at"]}, "nonreaders": [], "details": {"replyCount": 3, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1454464564200, "ddate": null, "super": null, "final": null, "duedate": 1455833700000, "tcdate": 1454464564200, "id": "ICLR.cc/2016/workshop/-/submission", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "readers": ["everyone"], "reply": {"pdf": null, "forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"order": 4, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv.", "value-regex": "upload|http://arxiv.org/pdf/.+"}, "title": {"order": 3, "description": "Title of paper.", "value-regex": ".{0,500}"}, "abstract": {"order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"order": 1, "description": "Comma separated list of author names, as they appear in the paper.", "value-regex": "[^,\\n]+(,[^,\\n]+)*"}, "author_emails": {"order": 2, "description": "Comma separated list of author email addresses, in the same order as above.", "value-regex": "[^,\\n]+(,[^,\\n]+)*"}, "conflicts": {"order": 100, "description": "Semi-colon separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.).", "value-regex": "^([a-zA-Z0-9][a-zA-Z0-9-_]{0,61}[a-zA-Z0-9]{0,1}\\.([a-zA-Z]{1,6}|[a-zA-Z0-9-]{1,30}\\.[a-zA-Z]{2,3}))+(;[a-zA-Z0-9][a-zA-Z0-9-_]{0,61}[a-zA-Z0-9]{0,1}\\.([a-zA-Z]{1,6}|[a-zA-Z0-9-]{1,30}\\.[a-zA-Z]{2,3}))*$"}, "CMT_id": {"order": 5, "value-regex": ".*", "description": "If the paper is a resubmission from the ICLR 2016 Conference Track, enter its CMT ID; otherwise, leave blank."}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "expdate": 1463609700000}}}], "count": 4}