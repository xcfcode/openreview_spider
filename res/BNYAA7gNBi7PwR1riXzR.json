{"notes": [{"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1457896292912, "tcdate": 1457896292912, "id": "6XAgmP33RtrVp0EvsEL4", "invitation": "ICLR.cc/2016/workshop/-/paper/112/review/12", "forum": "BNYAA7gNBi7PwR1riXzR", "replyto": "BNYAA7gNBi7PwR1riXzR", "signatures": ["ICLR.cc/2016/workshop/paper/112/reviewer/12"], "readers": ["everyone"], "writers": ["ICLR.cc/2016/workshop/paper/112/reviewer/12"], "content": {"title": "Nice ideas, but poor description of the model", "rating": "5: Marginally below acceptance threshold", "review": "The paper proposes a novel use of the external memory for recording rules about how inputs should be processed. A rule consists of an input and output sequences pair with option to have a common variable in them. Given an input sequence, the model first finds an applicable rule from its memory, and uses it to extract a substring from the input using a pointer network. Then, a output sequence is generated by an RNN conditioned on the target rule and the extracted substring. This way, it is possible to generalize to unseen rules during testing, although the performance was not as good as the seen rules.\n\nThe model description Section 2 was missing lot of details, especially the part about the decoder. Some symbols in equations 1, 2 was not defined in the text, which made it hard to follow what is exactly happening. The subsection \"hybrid encoder-decoder\" definitely needs more explaining or equations. It would be nice to have another figure about the decoder similar to figure 2.\n\nAlthough the task considered is artificial, I think it is good start and exposing a flaw in current seq2seq architectures. However, it feels like the proposed model is tailored to this specific task, so might have a problem generalizing to different types of rules. \n\nAn idea of using the memory as a repository of learned skills is different from recent papers about external memory, and definitely an interesting research direction. However, the paper needs more clear and detailed description of the model to be accepted.", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Guided Sequence-to-Sequence Learning with External Rule Memory", "abstract": "External memory has been proven to be essential for the success of neural network-based systems on many tasks, including Question-Answering, classification, machine translation and reasoning. In all those models the memory is used to store instance representations of multiple levels, analogous to \u201cdata\u201d in the Von Neumann architecture of a computer, while the \u201cinstructions\u201d are stored in the weights. In this paper, we however propose to use the memory for storing part of the instructions, and more specifically, the transformation rules in sequence-to-sequence learning tasks, in an external memory attached to a neural system. This memory can be accessed both by the neural network and by the human experts, hence serving as an interface for a novel learning paradigm where not only the instances but also the rule can be taught to the neural network. Our empirical study on a synthetic but challenging dataset verifies that our model is effective.", "pdf": "/pdf/BNYAA7gNBi7PwR1riXzR.pdf", "paperhash": "gu|guided_sequencetosequence_learning_with_external_rule_memory", "conflicts": ["hku.hk"], "authors": ["Jiatao Gu", "Baotian Hu", "Zhengdong Lu", "Hang Li", "Victor O.K. Li"], "authorids": ["jiataogu@eee.hku.hk", "baotianchina@gmail.com", "lu.zhengdong@huawei.com", "hangli.hl@huawei.com", "vli@eee.hku.hk"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1456579949169, "ddate": null, "super": null, "final": null, "duedate": 1460725200000, "tcdate": 1456579949169, "id": "ICLR.cc/2016/workshop/-/paper/112/review/12", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "reply": {"pdf": null, "forum": "BNYAA7gNBi7PwR1riXzR", "replyto": "BNYAA7gNBi7PwR1riXzR", "writers": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)"}, "signatures": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "readers": ["everyone", "ICLR.cc/2016/workshop/paper/112/reviewer/12", "ICLR.cc/2016/workshop"], "expdate": 1468501200000}}}, {"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1457685138456, "tcdate": 1457685138456, "id": "L7VjNwg2vSRNGwArs4gK", "invitation": "ICLR.cc/2016/workshop/-/paper/112/review/11", "forum": "BNYAA7gNBi7PwR1riXzR", "replyto": "BNYAA7gNBi7PwR1riXzR", "signatures": ["ICLR.cc/2016/workshop/paper/112/reviewer/11"], "readers": ["everyone"], "writers": ["ICLR.cc/2016/workshop/paper/112/reviewer/11"], "content": {"title": "Nice idea, but hand-engineered to match an artificial task", "rating": "5: Marginally below acceptance threshold", "review": "The paper proposes a twist on the sequence-to-sequence learning paradigm in which an external rule-set specifies how to generate responses for inputs matching the rules. Furthermore, a recurrent encoder-decoder network with attention mechanism over the rule-set is presented as a solution to the proposed problem. The paper demonstrates that the architecture can learn to transduce sequences using the rule-set and that it is somewhat able to generalize to the case when new rules are entered into the rule-set (which causes a drop from 90% to 69% accuracy).\n\nThe rules consist of a pattern and result, the  pattern contains a string to be matched to the input and optionally a wildcard. The result contains a new string to be written and optionally a symbol to be replaced with all words matched to the wildcard.\n\nThe model merges three concepts: the attention mechanism of Bahdanau et al \"Neural machine translation by jointly learning to align and translate\", the external memory from Weston et al \"Memory networks\" and the ability to indicate individual entries in the input sequence from Vinyals et al., \"Pointer networks\". The main novelty of the paper is the task which promises to blend the power of recurrent neural network transducers with hand-engineered rule sets. \n\nThe model description is hard to comprehend. It would be helpful to expand it and write equations for all model parts.\nSome technical problems in the presentation of the model:\n- Symbols in equations are reused (e.g. the \"e\" in equation (1) and (2) refers to different functions), and are not used consistently (eq. (2) uses E to denote the rule-set, while equation (3) uses scriptR).\n- It would be helpful to label signals in Fig. 2 by symbols used in eq. (2). \n- The relationship between Fig 1. and Fig 2. is not clear. Maybe a box should be drawn in Fig. 1 indicating which part of the model is presented in Fig. 2?\n\nThe chosen task (rewriting strings using auxiliary set of rules) seems to be difficult to solve with recurrent neural networks and the proposed architecture also struggles with it. The paper list various tricks to make training feasible, including extensive use of pointer networks to support the wildcard match in rules and introduce a pre-training step with extra supervision indicating which words match with the wildcard.\n\nPros and cons:\n+ good idea to embed some form of human knowledge into neural transducers\n- the proposed task seems very artificial and limited\n- the proposed model essentially emulates an algorithm to solve string matching with wildcard capture, and requires extensive supervision to be trained. It is hard to see how the model can generalize to other tasks\n\nMinor comments:\nfor a input sequence -> for an input sequence\n\"a certain indicate vector...\" -> rewrite, the sentence is incomprehensible\n\"T(.)is used to transfer examples to their corresponding rules\" -> did you mean to transform examples with rules? Please clarify\n\"SRSS (...) has difficulty and scalability to extend to various...\" -> I presume you mean that the proposed task has the potential to mimic real-world tasks? Please clarify the sentence.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Guided Sequence-to-Sequence Learning with External Rule Memory", "abstract": "External memory has been proven to be essential for the success of neural network-based systems on many tasks, including Question-Answering, classification, machine translation and reasoning. In all those models the memory is used to store instance representations of multiple levels, analogous to \u201cdata\u201d in the Von Neumann architecture of a computer, while the \u201cinstructions\u201d are stored in the weights. In this paper, we however propose to use the memory for storing part of the instructions, and more specifically, the transformation rules in sequence-to-sequence learning tasks, in an external memory attached to a neural system. This memory can be accessed both by the neural network and by the human experts, hence serving as an interface for a novel learning paradigm where not only the instances but also the rule can be taught to the neural network. Our empirical study on a synthetic but challenging dataset verifies that our model is effective.", "pdf": "/pdf/BNYAA7gNBi7PwR1riXzR.pdf", "paperhash": "gu|guided_sequencetosequence_learning_with_external_rule_memory", "conflicts": ["hku.hk"], "authors": ["Jiatao Gu", "Baotian Hu", "Zhengdong Lu", "Hang Li", "Victor O.K. Li"], "authorids": ["jiataogu@eee.hku.hk", "baotianchina@gmail.com", "lu.zhengdong@huawei.com", "hangli.hl@huawei.com", "vli@eee.hku.hk"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1456579949858, "ddate": null, "super": null, "final": null, "duedate": 1460725200000, "tcdate": 1456579949858, "id": "ICLR.cc/2016/workshop/-/paper/112/review/11", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "reply": {"pdf": null, "forum": "BNYAA7gNBi7PwR1riXzR", "replyto": "BNYAA7gNBi7PwR1riXzR", "writers": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)"}, "signatures": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "readers": ["everyone", "ICLR.cc/2016/workshop/paper/112/reviewer/11", "ICLR.cc/2016/workshop"], "expdate": 1468501200000}}}, {"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1457618684510, "tcdate": 1457618684510, "id": "r8ljowKREf8wknpYt5yw", "invitation": "ICLR.cc/2016/workshop/-/paper/112/review/10", "forum": "BNYAA7gNBi7PwR1riXzR", "replyto": "BNYAA7gNBi7PwR1riXzR", "signatures": ["ICLR.cc/2016/workshop/paper/112/reviewer/10"], "readers": ["everyone"], "writers": ["ICLR.cc/2016/workshop/paper/112/reviewer/10"], "content": {"title": "Some good ideas, but clearer explanation of the model is needed", "rating": "6: Marginally above acceptance threshold", "review": "This paper proposes conditioning sequence to sequence models based on rules stored in an indexible differentiable memory (basically attention). The idea is interesting, although somewhat encompassed by the more general Neural Programmer-Interpreters of Reed and De Freitas (http://arxiv.org/abs/1511.06279), although that paper perhaps had not been accepted to ICLR at submission time for this one. The two main problems with this paper, which tamper my enthusiasm, are that the model is not as clearly defined as it could be in section 2. The equations suffer from not have symbols consistently defined in the text, and an example could be given (other than in the figures). Mapping the mathematical description of the model to the figures (esp. figure 1) is not evident.\n\nRegarding the evaluation, the task is synthetic and fairly small scale, so I would tone down the claims to generality of the model on its basis made in the second paragraph of section 3. A normal seq2seq baseline would have been nice. The results for the baselines are somewhat surprising (0% for the pointer network baseline). The no-rule part of the task is just copying, no? It is not clear why simple pointer networks cannot perfectly solver this, since an LSTM can with the constraint on sequence lengths being similarly distributed between training and testing. It is unclear why DNN underperforms an inner product similarity metric.\n\nOverall, this is not a bad submission for a workshop paper, but I would have liked clearer explanation of the model and some indication of what further experiments could be performed to test it.", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Guided Sequence-to-Sequence Learning with External Rule Memory", "abstract": "External memory has been proven to be essential for the success of neural network-based systems on many tasks, including Question-Answering, classification, machine translation and reasoning. In all those models the memory is used to store instance representations of multiple levels, analogous to \u201cdata\u201d in the Von Neumann architecture of a computer, while the \u201cinstructions\u201d are stored in the weights. In this paper, we however propose to use the memory for storing part of the instructions, and more specifically, the transformation rules in sequence-to-sequence learning tasks, in an external memory attached to a neural system. This memory can be accessed both by the neural network and by the human experts, hence serving as an interface for a novel learning paradigm where not only the instances but also the rule can be taught to the neural network. Our empirical study on a synthetic but challenging dataset verifies that our model is effective.", "pdf": "/pdf/BNYAA7gNBi7PwR1riXzR.pdf", "paperhash": "gu|guided_sequencetosequence_learning_with_external_rule_memory", "conflicts": ["hku.hk"], "authors": ["Jiatao Gu", "Baotian Hu", "Zhengdong Lu", "Hang Li", "Victor O.K. Li"], "authorids": ["jiataogu@eee.hku.hk", "baotianchina@gmail.com", "lu.zhengdong@huawei.com", "hangli.hl@huawei.com", "vli@eee.hku.hk"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1456579950028, "ddate": null, "super": null, "final": null, "duedate": 1460725200000, "tcdate": 1456579950028, "id": "ICLR.cc/2016/workshop/-/paper/112/review/10", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "reply": {"pdf": null, "forum": "BNYAA7gNBi7PwR1riXzR", "replyto": "BNYAA7gNBi7PwR1riXzR", "writers": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)"}, "signatures": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "readers": ["everyone", "ICLR.cc/2016/workshop/paper/112/reviewer/10", "ICLR.cc/2016/workshop"], "expdate": 1468501200000}}}, {"tddate": null, "number": null, "replyto": null, "ddate": null, "cdate": null, "tmdate": 1455818330386, "tcdate": 1455818330386, "id": "BNYAA7gNBi7PwR1riXzR", "invitation": "ICLR.cc/2016/workshop/-/submission", "forum": "BNYAA7gNBi7PwR1riXzR", "signatures": ["~Jiatao_Gu1"], "readers": ["everyone"], "writers": ["~Jiatao_Gu1"], "content": {"CMT_id": "", "title": "Guided Sequence-to-Sequence Learning with External Rule Memory", "abstract": "External memory has been proven to be essential for the success of neural network-based systems on many tasks, including Question-Answering, classification, machine translation and reasoning. In all those models the memory is used to store instance representations of multiple levels, analogous to \u201cdata\u201d in the Von Neumann architecture of a computer, while the \u201cinstructions\u201d are stored in the weights. In this paper, we however propose to use the memory for storing part of the instructions, and more specifically, the transformation rules in sequence-to-sequence learning tasks, in an external memory attached to a neural system. This memory can be accessed both by the neural network and by the human experts, hence serving as an interface for a novel learning paradigm where not only the instances but also the rule can be taught to the neural network. Our empirical study on a synthetic but challenging dataset verifies that our model is effective.", "pdf": "/pdf/BNYAA7gNBi7PwR1riXzR.pdf", "paperhash": "gu|guided_sequencetosequence_learning_with_external_rule_memory", "conflicts": ["hku.hk"], "authors": ["Jiatao Gu", "Baotian Hu", "Zhengdong Lu", "Hang Li", "Victor O.K. Li"], "authorids": ["jiataogu@eee.hku.hk", "baotianchina@gmail.com", "lu.zhengdong@huawei.com", "hangli.hl@huawei.com", "vli@eee.hku.hk"]}, "nonreaders": [], "details": {"replyCount": 3, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1454464564200, "ddate": null, "super": null, "final": null, "duedate": 1455833700000, "tcdate": 1454464564200, "id": "ICLR.cc/2016/workshop/-/submission", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "readers": ["everyone"], "reply": {"pdf": null, "forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"order": 4, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv.", "value-regex": "upload|http://arxiv.org/pdf/.+"}, "title": {"order": 3, "description": "Title of paper.", "value-regex": ".{0,500}"}, "abstract": {"order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"order": 1, "description": "Comma separated list of author names, as they appear in the paper.", "value-regex": "[^,\\n]+(,[^,\\n]+)*"}, "author_emails": {"order": 2, "description": "Comma separated list of author email addresses, in the same order as above.", "value-regex": "[^,\\n]+(,[^,\\n]+)*"}, "conflicts": {"order": 100, "description": "Semi-colon separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.).", "value-regex": "^([a-zA-Z0-9][a-zA-Z0-9-_]{0,61}[a-zA-Z0-9]{0,1}\\.([a-zA-Z]{1,6}|[a-zA-Z0-9-]{1,30}\\.[a-zA-Z]{2,3}))+(;[a-zA-Z0-9][a-zA-Z0-9-_]{0,61}[a-zA-Z0-9]{0,1}\\.([a-zA-Z]{1,6}|[a-zA-Z0-9-]{1,30}\\.[a-zA-Z]{2,3}))*$"}, "CMT_id": {"order": 5, "value-regex": ".*", "description": "If the paper is a resubmission from the ICLR 2016 Conference Track, enter its CMT ID; otherwise, leave blank."}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "expdate": 1463609700000}}}], "count": 4}