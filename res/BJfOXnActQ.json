{"notes": [{"id": "BJfOXnActQ", "original": "SJgrzW05FX", "number": 1374, "cdate": 1538087968189, "ddate": null, "tcdate": 1538087968189, "tmdate": 1551314691470, "tddate": null, "forum": "BJfOXnActQ", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Learning to Learn with Conditional Class Dependencies", "abstract": "Neural networks can learn to extract statistical properties from data, but they seldom make use of structured information from the label space to help representation learning. Although some label structure can implicitly be obtained when training on huge amounts of data, in a few-shot learning context where little data is available, making explicit use of the label structure can inform the model to reshape the representation space to reflect a global sense of class dependencies.  We propose a meta-learning framework, Conditional class-Aware Meta-Learning (CAML), that conditionally transforms feature representations based on a metric space that is trained to capture inter-class dependencies. This enables a conditional modulation of the feature representations of the base-learner to impose regularities informed by the label space. Experiments show that the conditional transformation in CAML leads to more disentangled representations and achieves competitive results on the miniImageNet benchmark.", "keywords": ["meta-learning", "learning to learn", "few-shot learning"], "authorids": ["xiang.jiang@dal.ca", "mohammad@imagia.com", "f.varno@dal.ca", "gabriel@imagia.com", "nic@imagia.com", "stan@cs.dal.ca"], "authors": ["Xiang Jiang", "Mohammad Havaei", "Farshid Varno", "Gabriel Chartrand", "Nicolas Chapados", "Stan Matwin"], "TL;DR": "CAML is an instance of MAML with conditional class dependencies.", "pdf": "/pdf/9cdd30ccaf1b969ff619df8cdb271cb4d3e2371e.pdf", "paperhash": "jiang|learning_to_learn_with_conditional_class_dependencies", "_bibtex": "@inproceedings{\njiang2018learning,\ntitle={Learning to Learn with Conditional Class Dependencies},\nauthor={Xiang Jiang and Mohammad Havaei and Farshid Varno and Gabriel Chartrand and Nicolas Chapados and Stan Matwin},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BJfOXnActQ},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "Sye7irOggE", "original": null, "number": 1, "cdate": 1544746394839, "ddate": null, "tcdate": 1544746394839, "tmdate": 1545354515642, "tddate": null, "forum": "BJfOXnActQ", "replyto": "BJfOXnActQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1374/Meta_Review", "content": {"metareview": "The reviewers think that incorporating class conditional dependencies into the metric space of a few-shot learner is a sufficiently good idea to merit acceptance. The performance isn\u2019t necessarily better than the state-of-the-art approaches like LEO, but it is nonetheless competitive. One reviewer suggests incorporating a pre-training strategy to strengthen your results. In terms of experimental details, one reviewer pointed out that the embedding network architecture is quite a bit more powerful than the base learner and would like some additional justification for this. They would also like more detail on the computing the MAML gradients in the context of this method. Beyond this, please ensure that you have incorporated all of the clarifications that were required during the discussion phase.", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Accept (Poster)", "title": "Good proposal for incorporating class dependencies in few-shot learning."}, "signatures": ["ICLR.cc/2019/Conference/Paper1374/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper1374/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Learn with Conditional Class Dependencies", "abstract": "Neural networks can learn to extract statistical properties from data, but they seldom make use of structured information from the label space to help representation learning. Although some label structure can implicitly be obtained when training on huge amounts of data, in a few-shot learning context where little data is available, making explicit use of the label structure can inform the model to reshape the representation space to reflect a global sense of class dependencies.  We propose a meta-learning framework, Conditional class-Aware Meta-Learning (CAML), that conditionally transforms feature representations based on a metric space that is trained to capture inter-class dependencies. This enables a conditional modulation of the feature representations of the base-learner to impose regularities informed by the label space. Experiments show that the conditional transformation in CAML leads to more disentangled representations and achieves competitive results on the miniImageNet benchmark.", "keywords": ["meta-learning", "learning to learn", "few-shot learning"], "authorids": ["xiang.jiang@dal.ca", "mohammad@imagia.com", "f.varno@dal.ca", "gabriel@imagia.com", "nic@imagia.com", "stan@cs.dal.ca"], "authors": ["Xiang Jiang", "Mohammad Havaei", "Farshid Varno", "Gabriel Chartrand", "Nicolas Chapados", "Stan Matwin"], "TL;DR": "CAML is an instance of MAML with conditional class dependencies.", "pdf": "/pdf/9cdd30ccaf1b969ff619df8cdb271cb4d3e2371e.pdf", "paperhash": "jiang|learning_to_learn_with_conditional_class_dependencies", "_bibtex": "@inproceedings{\njiang2018learning,\ntitle={Learning to Learn with Conditional Class Dependencies},\nauthor={Xiang Jiang and Mohammad Havaei and Farshid Varno and Gabriel Chartrand and Nicolas Chapados and Stan Matwin},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BJfOXnActQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1374/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545352862997, "tddate": null, "super": null, "final": null, "reply": {"forum": "BJfOXnActQ", "replyto": "BJfOXnActQ", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1374/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper1374/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1374/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545352862997}}}, {"id": "r1xVbbV9RQ", "original": null, "number": 4, "cdate": 1543287035620, "ddate": null, "tcdate": 1543287035620, "tmdate": 1543287035620, "tddate": null, "forum": "BJfOXnActQ", "replyto": "H1lyuO0foX", "invitation": "ICLR.cc/2019/Conference/-/Paper1374/Official_Comment", "content": {"title": "Response to reviewer 2", "comment": "Thank you for the very detailed and constructive comments.\n\n1. The motivation\n1.1 How the metric space is trained?\nThe metric space is trained in a pre-training step and it is not updated while training the base-learner. The embeddings obtained from the metric space is different from other popular pre-training techniques, e.g. in LEO the embeddings are pre-trained as a supervised classification task. The pre-trained metric space provides a representation for class dependency as it is trained to provide good separation/clustering from randomly sampled classes. This is in contrast with supervised pre-training which aims to provide discriminative feature representations.\n\n1.2 \u201cWill it introduce more information w.r.t. only using embedding space to do the classification?\u201d\nThe proposed CAML makes use of two views of the data: a global view through the metric space and a local view via the base classifier. The global view of the data, i.e., the embeddings, may not capture all the necessary information for some classification tasks, such as classifying different breeds of dogs which may have similar embeddings. In such cases, the local view of data from the pixel space could help compensate for the lack of information in the global view.\n\n3.5 \u201cHow about build MAML directly on the embedding space?\u201d\nWe are aware that meta-learning on the embedding space is a powerful idea, as shown in LEO. We have added an experiment that directly trains maml on the learned 512 dimensional metric space using three fully-connected layers. We were only able to obtain 47.43% on 1-shot tasks and 57.33% on 5-shot tasks. This suggests that applying conditional transformations on the metric space is more effective than directly using the metric space as input.\n\n2. Novelty.\nThe proposed CAML does have close relation to TADAM. However, they have three main differences.\n(i) Different goals: TADAM uses conditional transformation for metric scaling while CAML for developing better gradient-based representations.\n(ii) Task-level vs. example-level representation. TADAM uses task-level representation to modulate the inference from a task perspective, while CAML uses example-level representation to modulate the representation at the content level.\n(iii) The conditional transformation in TADAM is homogeneous in the sense that the conditional information is retrieved from the metric space and also applied to the metric space. However, the proposed CAML uses conditional transformation under the heterogeneous setup where the conditional information is retrieved from the embedding space but applied to a different base learner.\n\n3. Method details\n3.1 \u201cSince CBN is example induced, will it prone to overfitting?\u201d\nThe metric space (ResNet-12) is pre-trained and not updated while training the base learner. The gradients of the meta learner only affect the base learner and conditional transformation. We choose 30 convolutional channels out of computational considerations, and the skip connection has a bigger impact than the number of conv channels. Using 64 conv channels without the skip connection, we obtain 54.63% on 1-shot and 70.38% on 5-shot.\n\n3.2 \u201cIs this skip connection very important for this particular model?\u201d\nYes, the skip connection is very important. The use of skip connections is to improve the gradient flow. MAML unfolds the inner loop into one large graph which may cause gradient issues. Without skip connections, out model obtains 56.07% on 1-shot tasks and 71.26% on 5-shot tasks.\n\n3.3 \u201cWill the MAML objective influences the embedding network?\u201d\nWe would like to clarify that the metric is pre-trained and not updated in MAML updates. We empirically observe that training the metric space and meta-learner end-to-end is overly complex and tend to over-fit.\n\n3.4 \u201chow many epochs does MAML need?\u201d\nIt takes 50,000 episodes to train CAML, and another 30,000 episodes to pre-train the metric space.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1374/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1374/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1374/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Learn with Conditional Class Dependencies", "abstract": "Neural networks can learn to extract statistical properties from data, but they seldom make use of structured information from the label space to help representation learning. Although some label structure can implicitly be obtained when training on huge amounts of data, in a few-shot learning context where little data is available, making explicit use of the label structure can inform the model to reshape the representation space to reflect a global sense of class dependencies.  We propose a meta-learning framework, Conditional class-Aware Meta-Learning (CAML), that conditionally transforms feature representations based on a metric space that is trained to capture inter-class dependencies. This enables a conditional modulation of the feature representations of the base-learner to impose regularities informed by the label space. Experiments show that the conditional transformation in CAML leads to more disentangled representations and achieves competitive results on the miniImageNet benchmark.", "keywords": ["meta-learning", "learning to learn", "few-shot learning"], "authorids": ["xiang.jiang@dal.ca", "mohammad@imagia.com", "f.varno@dal.ca", "gabriel@imagia.com", "nic@imagia.com", "stan@cs.dal.ca"], "authors": ["Xiang Jiang", "Mohammad Havaei", "Farshid Varno", "Gabriel Chartrand", "Nicolas Chapados", "Stan Matwin"], "TL;DR": "CAML is an instance of MAML with conditional class dependencies.", "pdf": "/pdf/9cdd30ccaf1b969ff619df8cdb271cb4d3e2371e.pdf", "paperhash": "jiang|learning_to_learn_with_conditional_class_dependencies", "_bibtex": "@inproceedings{\njiang2018learning,\ntitle={Learning to Learn with Conditional Class Dependencies},\nauthor={Xiang Jiang and Mohammad Havaei and Farshid Varno and Gabriel Chartrand and Nicolas Chapados and Stan Matwin},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BJfOXnActQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1374/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621608060, "tddate": null, "super": null, "final": null, "reply": {"forum": "BJfOXnActQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1374/Authors", "ICLR.cc/2019/Conference/Paper1374/Reviewers", "ICLR.cc/2019/Conference/Paper1374/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1374/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1374/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1374/Authors|ICLR.cc/2019/Conference/Paper1374/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1374/Reviewers", "ICLR.cc/2019/Conference/Paper1374/Authors", "ICLR.cc/2019/Conference/Paper1374/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621608060}}}, {"id": "H1x2TeE90m", "original": null, "number": 3, "cdate": 1543286980160, "ddate": null, "tcdate": 1543286980160, "tmdate": 1543286980160, "tddate": null, "forum": "BJfOXnActQ", "replyto": "ByxxsLKVn7", "invitation": "ICLR.cc/2019/Conference/-/Paper1374/Official_Comment", "content": {"title": "Response to reviewer 1", "comment": "Thank you for your valuable review.\n\n1. Clarification on the metric learning step\nThank you for the suggestion. The metric is indeed learned in a K-means-flavored way and we have updated our manuscript to reflect that $\\phi$ is learned.\n\n2. How confidence intervals are constructed?\nWe sample 600 evaluation tasks from the meta-test classes and report the confidence intervals across all the evaluation tasks. We have updated our manuscript to reflect this.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1374/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1374/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1374/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Learn with Conditional Class Dependencies", "abstract": "Neural networks can learn to extract statistical properties from data, but they seldom make use of structured information from the label space to help representation learning. Although some label structure can implicitly be obtained when training on huge amounts of data, in a few-shot learning context where little data is available, making explicit use of the label structure can inform the model to reshape the representation space to reflect a global sense of class dependencies.  We propose a meta-learning framework, Conditional class-Aware Meta-Learning (CAML), that conditionally transforms feature representations based on a metric space that is trained to capture inter-class dependencies. This enables a conditional modulation of the feature representations of the base-learner to impose regularities informed by the label space. Experiments show that the conditional transformation in CAML leads to more disentangled representations and achieves competitive results on the miniImageNet benchmark.", "keywords": ["meta-learning", "learning to learn", "few-shot learning"], "authorids": ["xiang.jiang@dal.ca", "mohammad@imagia.com", "f.varno@dal.ca", "gabriel@imagia.com", "nic@imagia.com", "stan@cs.dal.ca"], "authors": ["Xiang Jiang", "Mohammad Havaei", "Farshid Varno", "Gabriel Chartrand", "Nicolas Chapados", "Stan Matwin"], "TL;DR": "CAML is an instance of MAML with conditional class dependencies.", "pdf": "/pdf/9cdd30ccaf1b969ff619df8cdb271cb4d3e2371e.pdf", "paperhash": "jiang|learning_to_learn_with_conditional_class_dependencies", "_bibtex": "@inproceedings{\njiang2018learning,\ntitle={Learning to Learn with Conditional Class Dependencies},\nauthor={Xiang Jiang and Mohammad Havaei and Farshid Varno and Gabriel Chartrand and Nicolas Chapados and Stan Matwin},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BJfOXnActQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1374/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621608060, "tddate": null, "super": null, "final": null, "reply": {"forum": "BJfOXnActQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1374/Authors", "ICLR.cc/2019/Conference/Paper1374/Reviewers", "ICLR.cc/2019/Conference/Paper1374/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1374/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1374/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1374/Authors|ICLR.cc/2019/Conference/Paper1374/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1374/Reviewers", "ICLR.cc/2019/Conference/Paper1374/Authors", "ICLR.cc/2019/Conference/Paper1374/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621608060}}}, {"id": "B1gPog4c0Q", "original": null, "number": 2, "cdate": 1543286943095, "ddate": null, "tcdate": 1543286943095, "tmdate": 1543286943095, "tddate": null, "forum": "BJfOXnActQ", "replyto": "HJxo10uFh7", "invitation": "ICLR.cc/2019/Conference/-/Paper1374/Official_Comment", "content": {"title": "Response to reviewer 3", "comment": "Thank you for your constructive review.\n\n1. Is the use of class dependency general or specific to MAML-based methods?\n(1) The benefits of class dependency is not restricted to MAML-based methods. The goal of class dependency is to provide complementary information to the meta-learner; this is especially important in few-shot learning due to insufficient data.\n(2) Relating to other SOA: (a) TADAM makes use of conditional transformations based on tasks representations for metric scaling; class dependencies can also be incorporated into TADAM with the additional benefit of capturing example-level class relationships. (b) LEO can also make use of the class dependency for improving the conditional generation of model parameters.\n\n2. The relationship between the metric space and the base-learner.\nThe proposed framework captures the dual views of a classification task: a global view that is aware of the relationships among all classes, and a local views of the current N-way K-shot classification task. The metric space, or the global view, is pre-trained in a way that is independent of the current N-way K-shot task; while the base-learner, or the local view, attempts to develop representations for the current classification task alone.\n\n3. \u201cWhat would happen if similar process keeps on? E.g., by building the third stage that modulates the features from the previous two?\u201d\nThis is a very interesting question. One can build different stages of conditional transformations associated with different granularities of class-dependency. With metric spaces trained to capture different levels of class-dependency, one could modulate the base-learner in a hierarchical manner.\n\n4. How to make use of hierarchical class structure?\nOne can employ a curriculum learning strategy to learn the metric space at different levels of the hierarchy to better train the metric space. As mentioned in 3, the hierarchical class structure can also be used to train different metric spaces and conditionally modulate representations in a hierarchical manner.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1374/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1374/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1374/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Learn with Conditional Class Dependencies", "abstract": "Neural networks can learn to extract statistical properties from data, but they seldom make use of structured information from the label space to help representation learning. Although some label structure can implicitly be obtained when training on huge amounts of data, in a few-shot learning context where little data is available, making explicit use of the label structure can inform the model to reshape the representation space to reflect a global sense of class dependencies.  We propose a meta-learning framework, Conditional class-Aware Meta-Learning (CAML), that conditionally transforms feature representations based on a metric space that is trained to capture inter-class dependencies. This enables a conditional modulation of the feature representations of the base-learner to impose regularities informed by the label space. Experiments show that the conditional transformation in CAML leads to more disentangled representations and achieves competitive results on the miniImageNet benchmark.", "keywords": ["meta-learning", "learning to learn", "few-shot learning"], "authorids": ["xiang.jiang@dal.ca", "mohammad@imagia.com", "f.varno@dal.ca", "gabriel@imagia.com", "nic@imagia.com", "stan@cs.dal.ca"], "authors": ["Xiang Jiang", "Mohammad Havaei", "Farshid Varno", "Gabriel Chartrand", "Nicolas Chapados", "Stan Matwin"], "TL;DR": "CAML is an instance of MAML with conditional class dependencies.", "pdf": "/pdf/9cdd30ccaf1b969ff619df8cdb271cb4d3e2371e.pdf", "paperhash": "jiang|learning_to_learn_with_conditional_class_dependencies", "_bibtex": "@inproceedings{\njiang2018learning,\ntitle={Learning to Learn with Conditional Class Dependencies},\nauthor={Xiang Jiang and Mohammad Havaei and Farshid Varno and Gabriel Chartrand and Nicolas Chapados and Stan Matwin},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BJfOXnActQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1374/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621608060, "tddate": null, "super": null, "final": null, "reply": {"forum": "BJfOXnActQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1374/Authors", "ICLR.cc/2019/Conference/Paper1374/Reviewers", "ICLR.cc/2019/Conference/Paper1374/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1374/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1374/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1374/Authors|ICLR.cc/2019/Conference/Paper1374/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1374/Reviewers", "ICLR.cc/2019/Conference/Paper1374/Authors", "ICLR.cc/2019/Conference/Paper1374/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621608060}}}, {"id": "Hyg7wl45AQ", "original": null, "number": 1, "cdate": 1543286874652, "ddate": null, "tcdate": 1543286874652, "tmdate": 1543286874652, "tddate": null, "forum": "BJfOXnActQ", "replyto": "BJfOXnActQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1374/Official_Comment", "content": {"title": "Updated version of the paper", "comment": "We thank the reviewers for their valuable feedback. The main changes we have made in the manuscript include:\n\n(1) Clarifications on metric learning notations and the fact that the metric space is pre-trained.\n(2) Additional discussions about the relationships between the metric space and the base classifier.\n(3) Highlight the differences between CAML and TADAM.\n(4) Hyperparameters and other small edits."}, "signatures": ["ICLR.cc/2019/Conference/Paper1374/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1374/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1374/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Learn with Conditional Class Dependencies", "abstract": "Neural networks can learn to extract statistical properties from data, but they seldom make use of structured information from the label space to help representation learning. Although some label structure can implicitly be obtained when training on huge amounts of data, in a few-shot learning context where little data is available, making explicit use of the label structure can inform the model to reshape the representation space to reflect a global sense of class dependencies.  We propose a meta-learning framework, Conditional class-Aware Meta-Learning (CAML), that conditionally transforms feature representations based on a metric space that is trained to capture inter-class dependencies. This enables a conditional modulation of the feature representations of the base-learner to impose regularities informed by the label space. Experiments show that the conditional transformation in CAML leads to more disentangled representations and achieves competitive results on the miniImageNet benchmark.", "keywords": ["meta-learning", "learning to learn", "few-shot learning"], "authorids": ["xiang.jiang@dal.ca", "mohammad@imagia.com", "f.varno@dal.ca", "gabriel@imagia.com", "nic@imagia.com", "stan@cs.dal.ca"], "authors": ["Xiang Jiang", "Mohammad Havaei", "Farshid Varno", "Gabriel Chartrand", "Nicolas Chapados", "Stan Matwin"], "TL;DR": "CAML is an instance of MAML with conditional class dependencies.", "pdf": "/pdf/9cdd30ccaf1b969ff619df8cdb271cb4d3e2371e.pdf", "paperhash": "jiang|learning_to_learn_with_conditional_class_dependencies", "_bibtex": "@inproceedings{\njiang2018learning,\ntitle={Learning to Learn with Conditional Class Dependencies},\nauthor={Xiang Jiang and Mohammad Havaei and Farshid Varno and Gabriel Chartrand and Nicolas Chapados and Stan Matwin},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BJfOXnActQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1374/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621608060, "tddate": null, "super": null, "final": null, "reply": {"forum": "BJfOXnActQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1374/Authors", "ICLR.cc/2019/Conference/Paper1374/Reviewers", "ICLR.cc/2019/Conference/Paper1374/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1374/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1374/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1374/Authors|ICLR.cc/2019/Conference/Paper1374/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1374/Reviewers", "ICLR.cc/2019/Conference/Paper1374/Authors", "ICLR.cc/2019/Conference/Paper1374/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621608060}}}, {"id": "HJxo10uFh7", "original": null, "number": 3, "cdate": 1541144035481, "ddate": null, "tcdate": 1541144035481, "tmdate": 1541533187401, "tddate": null, "forum": "BJfOXnActQ", "replyto": "BJfOXnActQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1374/Official_Review", "content": {"title": "An interesting paper with some areas yet to exploit.  ", "review": "[Summary]\nThe paper presents an enhancement to the Model-Agnostic Meta-Learning (MAML) framework to integrate class dependency into the gradient-based meta-learning procedure. Specifically, the class dependency is encoded by embedding the training examples via a clustering network into a metric space where semantic similarity is preserved via affinity under Euclidean distance. Embedding of an example in this space is further employed to modulate (scale and shift) features of the example extracted by the base-learner via a transformation network, and the final prediction is made on top of the modulated features. Experiments on min-ImageNet shows that the proposed approach improves the baseline of MAML.    \n\nPros\n- An interesting idea of leveraging class dependency in meta-learning.\n- Solid implementation with reasonable technical solutions.\n\nCons\n- Some relevant interesting areas/cases were not exploited/tested.\n- Improvement over state-of-the-arts (SOA) is marginal or none. \n\n[Originality]\nThe paper is motivated by an interesting observation that class dependency in the label space can also provide insights for meta-learning. This seems to be first introduced in the context of  meta-learning.\n\n[Quality]\nOverall the paper is well executed in some aspects, including motivation and technical implementation. There are, however, a few areas/cases I would like to see more from it so as to make a stronger case. \n\nIn terms of generalization, the proposed enhancement to MAML is claimed to be orthogonal to other SOAs that are also within the framework based on gradient-descent, e.g. LEO. It is not quite clear to me that if the use of class dependency can lead to general benefits to alike methods like LEO, or if it is just a specific case for the MAML baseline. Actually, it would be interesting to see how the proposed class-conditional modulation can help other SOA in table 1. Also, more empirical results from other use cases (e.g., other datasets or problems) also help provide more insights here. These augmentation can better justify the value or significance of this work.       \n\nIn the specific formulation of the approach in Fig 2, it looks to me that the whole system is a compounded framework that combines two classifiers with one (base-learner) producing base representation, and the second injects side-information (e.g., from class-dependency in this case) to modulates the base representation before the final prediction. I just wonder what would happen if similar process keeps on? E.g., by building the third stage that modulates the features from the previous two? Or what if we swap the roles of base-learner and the embedding from the metric space (i.e., using the base-learner to modulate the embedding)? It looks to me that the feature/embedding from both components (in Fig 5 and 6) are optimized to improve separability. The roles they play in this process are also very interesting to get more elucidation. \n \nAnother point worth discussion is that the class dependency currently imposed does not see to include hierarchical structure among classes, i.e., the label space is still flat. It would be great if this can be briefly discussed with respect to the current formulation to better inspire the future work.\n\n[Clarity]\nThe paper is generally well written and I did not have much difficulty to follow. \n\n[Significance]\nWhile the paper is built on an interesting idea, there are still a few areas for further improvement to justify its significance (the the comments above). \n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1374/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Learn with Conditional Class Dependencies", "abstract": "Neural networks can learn to extract statistical properties from data, but they seldom make use of structured information from the label space to help representation learning. Although some label structure can implicitly be obtained when training on huge amounts of data, in a few-shot learning context where little data is available, making explicit use of the label structure can inform the model to reshape the representation space to reflect a global sense of class dependencies.  We propose a meta-learning framework, Conditional class-Aware Meta-Learning (CAML), that conditionally transforms feature representations based on a metric space that is trained to capture inter-class dependencies. This enables a conditional modulation of the feature representations of the base-learner to impose regularities informed by the label space. Experiments show that the conditional transformation in CAML leads to more disentangled representations and achieves competitive results on the miniImageNet benchmark.", "keywords": ["meta-learning", "learning to learn", "few-shot learning"], "authorids": ["xiang.jiang@dal.ca", "mohammad@imagia.com", "f.varno@dal.ca", "gabriel@imagia.com", "nic@imagia.com", "stan@cs.dal.ca"], "authors": ["Xiang Jiang", "Mohammad Havaei", "Farshid Varno", "Gabriel Chartrand", "Nicolas Chapados", "Stan Matwin"], "TL;DR": "CAML is an instance of MAML with conditional class dependencies.", "pdf": "/pdf/9cdd30ccaf1b969ff619df8cdb271cb4d3e2371e.pdf", "paperhash": "jiang|learning_to_learn_with_conditional_class_dependencies", "_bibtex": "@inproceedings{\njiang2018learning,\ntitle={Learning to Learn with Conditional Class Dependencies},\nauthor={Xiang Jiang and Mohammad Havaei and Farshid Varno and Gabriel Chartrand and Nicolas Chapados and Stan Matwin},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BJfOXnActQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1374/Official_Review", "cdate": 1542234243669, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BJfOXnActQ", "replyto": "BJfOXnActQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1374/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335932914, "tmdate": 1552335932914, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1374/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "ByxxsLKVn7", "original": null, "number": 2, "cdate": 1540818584341, "ddate": null, "tcdate": 1540818584341, "tmdate": 1541533187158, "tddate": null, "forum": "BJfOXnActQ", "replyto": "BJfOXnActQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1374/Official_Review", "content": {"title": "Good paper", "review": "TL;DR. Significant contribution to meta-learning by incorporating latent metrics on labels.\n\n* Summary\n\nThe manuscript builds on the observation that using structured information from the labels space improves learning accuracy. The proposed method --CAML-- is an instance of MAML (Finn et al., 2017), where an additional embedding is used to characterize the dissimilarity among labels.\n\nWhile quite natural, the proposed method is supported by a clever metric learning step. The classes are first represented by centroids and an optimal mapping $\\phi$ is then learnt by maximizing a clustering entropy (similarly to what is performed in a K-means-flavored algorithm, though this connection is not made in the manuscript). A conditional batch normalization (Dumoulin et al., 2017) is then used to model how closeness (in the embedding space $f_\\phi$) among labels is taken into account at the meta-learning level.\n\nExisting literature is well acknowledged and I find the numerical experiments to be convincing. In my opinion, a clear accept.\n\n* Minor issues\n\n- I would suggest adding a footnote explaining why Table 1 reports confidence intervals and not just standard deviations. How are constructed those intervals?\n- Section 3.2 bears ambiguity as the manuscript reads \"We first define centroids [...]\" depending on $f_\\phi$ which is then defined as the argument of the minim of the entropy term. What appears as a circular definition is merely the effect of loose writing yet I am afraid it would confuse readers. I would suggest to rewrite this part, maybe using a pseudo-code to better make the point that $f_\\phi$ is learnt.", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1374/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Learn with Conditional Class Dependencies", "abstract": "Neural networks can learn to extract statistical properties from data, but they seldom make use of structured information from the label space to help representation learning. Although some label structure can implicitly be obtained when training on huge amounts of data, in a few-shot learning context where little data is available, making explicit use of the label structure can inform the model to reshape the representation space to reflect a global sense of class dependencies.  We propose a meta-learning framework, Conditional class-Aware Meta-Learning (CAML), that conditionally transforms feature representations based on a metric space that is trained to capture inter-class dependencies. This enables a conditional modulation of the feature representations of the base-learner to impose regularities informed by the label space. Experiments show that the conditional transformation in CAML leads to more disentangled representations and achieves competitive results on the miniImageNet benchmark.", "keywords": ["meta-learning", "learning to learn", "few-shot learning"], "authorids": ["xiang.jiang@dal.ca", "mohammad@imagia.com", "f.varno@dal.ca", "gabriel@imagia.com", "nic@imagia.com", "stan@cs.dal.ca"], "authors": ["Xiang Jiang", "Mohammad Havaei", "Farshid Varno", "Gabriel Chartrand", "Nicolas Chapados", "Stan Matwin"], "TL;DR": "CAML is an instance of MAML with conditional class dependencies.", "pdf": "/pdf/9cdd30ccaf1b969ff619df8cdb271cb4d3e2371e.pdf", "paperhash": "jiang|learning_to_learn_with_conditional_class_dependencies", "_bibtex": "@inproceedings{\njiang2018learning,\ntitle={Learning to Learn with Conditional Class Dependencies},\nauthor={Xiang Jiang and Mohammad Havaei and Farshid Varno and Gabriel Chartrand and Nicolas Chapados and Stan Matwin},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BJfOXnActQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1374/Official_Review", "cdate": 1542234243669, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BJfOXnActQ", "replyto": "BJfOXnActQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1374/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335932914, "tmdate": 1552335932914, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1374/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "H1lyuO0foX", "original": null, "number": 1, "cdate": 1539659879244, "ddate": null, "tcdate": 1539659879244, "tmdate": 1541533186956, "tddate": null, "forum": "BJfOXnActQ", "replyto": "BJfOXnActQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1374/Official_Review", "content": {"title": "A paper with clear idea for few-shot learning, but there are still some questions about the paper. ", "review": "This paper proposes a new few-shot learning method with class dependencies. To consider the structure in the label space, the authors propose to use conditional batch normalization to help change the embedding based on class-wise statistics. Based on which the final classifier can be learned by the gradient-based meta-learning method, i.e., MAML. Experiments on MiniImageNet show the proposed method can achieve high-performance, and the proposed part can be proved to be effective based on the ablation study.\n\nThere are three main concerns about this paper, and the final rating depends on the authors' response.\n1. The motivation\nThe authors claim the label structure is helpful in the few-shot learning. If the reviewer understands correctly, it is the change of embedding network based on class statistics that consider such a label structure. From the objective perspective, there are no terms related to this purpose, and the embedding space learning is also based on the same few-shot objective. Will it introduces more information w.r.t. only using embedding space to do the classification?\n\n2. The novelty.\nThis paper looks like a MAML version of TADAM. Both of the methods use the conditional batch normalization in the embedding network, while CAML uses MAML to learn another classifier based on the embedding. Although CAML uses the CBN at the example level and considers the class information in a transductive setting, it is not very novel. From the results, the proposed method uses a stronger network but does not improve a lot w.r.t. TADAM.\n\n3. Method details\n3.1 Since CBN is example induced, will it prone to overfitting?\n3.2 About the model architecture. \nCAML uses a 4*4 skip connection from input to output. It is OK to use this improve the final performance, but the authors also need to show the results without the skip connection to fairly compare with other methods. Is this skip connection very important for this particular model? Most methods use 64 channel in the convNet while 30 channels are used in this paper. Is this computational consideration or to avoid overfitting? It is a bit strange that the main network is just four layers but the conditional network is a larger and stronger resNet.\n3.3 About the MAML gradients\nHow to compute the gradient in the MAML flow? Will the embedding network be updated simultaneously? In other words, will the MAML objective influences the embedding network?\n3.4 The training details are not clear. \nThe concrete training setting is not clear. For example, does the method need model pre-train? What is the learning rate, and how to adapt it? For the MAML, we also need the inner-update learning rate. How many epochs does CAML need?\n3.5 How about build MAML directly on the embedding space?", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper1374/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Learn with Conditional Class Dependencies", "abstract": "Neural networks can learn to extract statistical properties from data, but they seldom make use of structured information from the label space to help representation learning. Although some label structure can implicitly be obtained when training on huge amounts of data, in a few-shot learning context where little data is available, making explicit use of the label structure can inform the model to reshape the representation space to reflect a global sense of class dependencies.  We propose a meta-learning framework, Conditional class-Aware Meta-Learning (CAML), that conditionally transforms feature representations based on a metric space that is trained to capture inter-class dependencies. This enables a conditional modulation of the feature representations of the base-learner to impose regularities informed by the label space. Experiments show that the conditional transformation in CAML leads to more disentangled representations and achieves competitive results on the miniImageNet benchmark.", "keywords": ["meta-learning", "learning to learn", "few-shot learning"], "authorids": ["xiang.jiang@dal.ca", "mohammad@imagia.com", "f.varno@dal.ca", "gabriel@imagia.com", "nic@imagia.com", "stan@cs.dal.ca"], "authors": ["Xiang Jiang", "Mohammad Havaei", "Farshid Varno", "Gabriel Chartrand", "Nicolas Chapados", "Stan Matwin"], "TL;DR": "CAML is an instance of MAML with conditional class dependencies.", "pdf": "/pdf/9cdd30ccaf1b969ff619df8cdb271cb4d3e2371e.pdf", "paperhash": "jiang|learning_to_learn_with_conditional_class_dependencies", "_bibtex": "@inproceedings{\njiang2018learning,\ntitle={Learning to Learn with Conditional Class Dependencies},\nauthor={Xiang Jiang and Mohammad Havaei and Farshid Varno and Gabriel Chartrand and Nicolas Chapados and Stan Matwin},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BJfOXnActQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1374/Official_Review", "cdate": 1542234243669, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BJfOXnActQ", "replyto": "BJfOXnActQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1374/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335932914, "tmdate": 1552335932914, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1374/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 9}