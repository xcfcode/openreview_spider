{"notes": [{"id": "SyG4RiR5Ym", "original": "H1g51a6qYQ", "number": 887, "cdate": 1538087884281, "ddate": null, "tcdate": 1538087884281, "tmdate": 1545355391979, "tddate": null, "forum": "SyG4RiR5Ym", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Neural Distribution Learning for generalized time-to-event prediction", "abstract": "Predicting the time to the next event is an important task in various domains. \nHowever, due to censoring and irregularly sampled sequences, time-to-event prediction has resulted in limited success only for particular tasks, architectures and data. Using recent advances in probabilistic programming and density networks, we make the case for a generalized parametric survival approach, sequentially predicting a distribution over the time to the next event. \nUnlike previous work, the proposed method can use asynchronously sampled features for censored, discrete, and multivariate data. \nFurthermore, it achieves good performance and near perfect calibration for probabilistic predictions without using rigid network-architectures, multitask approaches, complex learning schemes or non-trivial adaptations of cox-models. \nWe firmly establish that this can be achieved in the standard neural network framework by simply switching out the output layer and loss function.", "keywords": ["Deep Learning", "Survival Analysis", "Event prediction", "Time Series", "Probabilistic Programming", "Density Networks"], "authorids": ["egil.martinsson@gmail.com", "adrian.kim@navercorp.com", "jaesung.huh@navercorp.com", "jchoo@korea.ac.kr", "jungwoo.ha@navercorp.com"], "authors": ["Egil Martinsson", "Adrian Kim", "Jaesung Huh", "Jaegul Choo", "Jung-Woo Ha"], "TL;DR": "We present a general solution to event prediction that has been there all along; Discrete Time Parametric Survival Analysis.", "pdf": "/pdf/de3044062bda980272d477826dc45ef5b02cfaf1.pdf", "paperhash": "martinsson|neural_distribution_learning_for_generalized_timetoevent_prediction", "_bibtex": "@misc{\nmartinsson2019neural,\ntitle={Neural Distribution Learning for generalized time-to-event prediction},\nauthor={Egil Martinsson and Adrian Kim and Jaesung Huh and Jaegul Choo and Jung-Woo Ha},\nyear={2019},\nurl={https://openreview.net/forum?id=SyG4RiR5Ym},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "r1xg0J3lxN", "original": null, "number": 1, "cdate": 1544761288040, "ddate": null, "tcdate": 1544761288040, "tmdate": 1545354519064, "tddate": null, "forum": "SyG4RiR5Ym", "replyto": "SyG4RiR5Ym", "invitation": "ICLR.cc/2019/Conference/-/Paper887/Meta_Review", "content": {"metareview": "All reviewers agree to reject. While there were many positive points to this work, reviewers believed that it was not yet ready for acceptance.", "confidence": "5: The area chair is absolutely certain", "recommendation": "Reject", "title": "Meta-Review for Neural Distribution Learning"}, "signatures": ["ICLR.cc/2019/Conference/Paper887/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper887/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Distribution Learning for generalized time-to-event prediction", "abstract": "Predicting the time to the next event is an important task in various domains. \nHowever, due to censoring and irregularly sampled sequences, time-to-event prediction has resulted in limited success only for particular tasks, architectures and data. Using recent advances in probabilistic programming and density networks, we make the case for a generalized parametric survival approach, sequentially predicting a distribution over the time to the next event. \nUnlike previous work, the proposed method can use asynchronously sampled features for censored, discrete, and multivariate data. \nFurthermore, it achieves good performance and near perfect calibration for probabilistic predictions without using rigid network-architectures, multitask approaches, complex learning schemes or non-trivial adaptations of cox-models. \nWe firmly establish that this can be achieved in the standard neural network framework by simply switching out the output layer and loss function.", "keywords": ["Deep Learning", "Survival Analysis", "Event prediction", "Time Series", "Probabilistic Programming", "Density Networks"], "authorids": ["egil.martinsson@gmail.com", "adrian.kim@navercorp.com", "jaesung.huh@navercorp.com", "jchoo@korea.ac.kr", "jungwoo.ha@navercorp.com"], "authors": ["Egil Martinsson", "Adrian Kim", "Jaesung Huh", "Jaegul Choo", "Jung-Woo Ha"], "TL;DR": "We present a general solution to event prediction that has been there all along; Discrete Time Parametric Survival Analysis.", "pdf": "/pdf/de3044062bda980272d477826dc45ef5b02cfaf1.pdf", "paperhash": "martinsson|neural_distribution_learning_for_generalized_timetoevent_prediction", "_bibtex": "@misc{\nmartinsson2019neural,\ntitle={Neural Distribution Learning for generalized time-to-event prediction},\nauthor={Egil Martinsson and Adrian Kim and Jaesung Huh and Jaegul Choo and Jung-Woo Ha},\nyear={2019},\nurl={https://openreview.net/forum?id=SyG4RiR5Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper887/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353049481, "tddate": null, "super": null, "final": null, "reply": {"forum": "SyG4RiR5Ym", "replyto": "SyG4RiR5Ym", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper887/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper887/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper887/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353049481}}}, {"id": "H1eEWxXcCm", "original": null, "number": 7, "cdate": 1543282684394, "ddate": null, "tcdate": 1543282684394, "tmdate": 1543282684394, "tddate": null, "forum": "SyG4RiR5Ym", "replyto": "SkeMu1XcAX", "invitation": "ICLR.cc/2019/Conference/-/Paper887/Official_Comment", "content": {"title": "Why we had only one explicit baseline.", "comment": "> Also, while the HazardNet framework looks convenient, by using hazard and survival functions as discusses by the authors, it is not clear to me what are the benefits from recent works in neural temporal point processes which also define a general framework for temporal predictions of events. Approaches such at least like \"Modeling the intensity function of point process via recurrent neural networks\" should be considered in the experiments, though they do not explicitely model censoring but with slight adapations should be able to work well of experimental data.\n\nIn short, the main differences are not quantitative, and the metrics that can be used for comparing models differ between modeling strategies as the data used and type of predictions made by other methods are limited. \n\nAlso, the fact that many models would work with censored data using only slight adaptations makes it even more surprising why they seem not to take into account the particular problems that arise when having censored data.\n\nFurthermore in the cases when they would work they are are built for particular distributions, neural network architectures, probabilistic queries and types of data. The argument we make is therefore that;\n\n1) Direct comparison is uninteresting.\nWe think that there are enough qualitative differences between models and enough questions about the PSA-Approach that the typical question; \"Which model performs best on some metric?\" may not be the most interesting one to ask just yet.\n\nOur proposition is more that there's many soft qualitative aspects with our approach (simplicity of comparing multitude of neural network architectures to name one) that does not fit into a table. \n\nWe reasoned that in order to make an interesting and fair comparison we need to factor out maximum amount of confounding factors such as model architecture, which we hard or impossible as most papers are usually about coupling model architecture, data modelling technique and predicted output (Sec. 1-2, 3.4, 4, 6)\n\nThe question we focused on was more \"Is the model unbiased and calibrated?\", which we did by comparing it to the binary case. We have not found any convincing work doing this before. We neither find the answers to those questions obvious. Even for the most similar looking work such as https://arxiv.org/abs/1809.02403 they have even come to completely different conclusions, such as the need to weight the censored and the uncensored loss terms. \nWe found that Clipping log-likelihood and properly initializing output layer and making sure the assumptions of PSA holds is sufficient for numerically stable training. See discussion with reviewer #1.\n\n\n2) Comparison is unfair.\nReimplementing our generalized model for most papers using their architecture, predicted distribution, data and queries would often make that implementation mathematically identical to their model (Since they knowingly/unknowingly built upon the fundamental ideas of PSA). \nConversely; reimplementing their models for new neural network architectures and making them work for censored discrete data or the type of queries our model supports would often transform them beyond recognition and it would be hard to argue that it still is their model.\n\nIf we take DeepHit as an example. This proposes an interesting (but arguably complex) method of joining different RNNs together at different time frequencies, each specialized for their own feature type (evenly spaced, asynchronous, event sequences, etc). It predicts multiple types of output in different future timewindows and adds different weighting schemes for the varying loss-terms. They evaluate performance using MAE (which is not defined for censored data), which they base on the paper Recurrent Marked Temporal Point Process (RMTP) (See comments to reviewer #1 about this). There are 10s of examples like this.\n\nTo compare for example DeepHit's RNN tuned for their data-reshaping and evaluation method (which we critique) on our data with some neural network architecture of our choice one some metric not explicitly optimized for would be misleading at best, and we wonder which question it would answer in the first place.\n\nOther methods that have been subject of much recent debate includes temporal point processes such as Hawkes Processes, which has a hand-crafted way of mediating past history to the parameters of the (particular) current distribution of time to event.\nWe wanted to give examples on how to build new processes and distributions, not to verify this on all existing distributions manually. \nFor our model, past events is just one of many possible feature inputs, and as an example we try learning how to map it to the predicted parameters as good as possible using multilayered RNNs or CNNs as an example. \nOur only claim is that if the current predicted distribution has a Cumulative Hazard Function satisfying certain basic requirements (which a hawkes process clearly does), it works fine.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper887/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper887/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper887/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Distribution Learning for generalized time-to-event prediction", "abstract": "Predicting the time to the next event is an important task in various domains. \nHowever, due to censoring and irregularly sampled sequences, time-to-event prediction has resulted in limited success only for particular tasks, architectures and data. Using recent advances in probabilistic programming and density networks, we make the case for a generalized parametric survival approach, sequentially predicting a distribution over the time to the next event. \nUnlike previous work, the proposed method can use asynchronously sampled features for censored, discrete, and multivariate data. \nFurthermore, it achieves good performance and near perfect calibration for probabilistic predictions without using rigid network-architectures, multitask approaches, complex learning schemes or non-trivial adaptations of cox-models. \nWe firmly establish that this can be achieved in the standard neural network framework by simply switching out the output layer and loss function.", "keywords": ["Deep Learning", "Survival Analysis", "Event prediction", "Time Series", "Probabilistic Programming", "Density Networks"], "authorids": ["egil.martinsson@gmail.com", "adrian.kim@navercorp.com", "jaesung.huh@navercorp.com", "jchoo@korea.ac.kr", "jungwoo.ha@navercorp.com"], "authors": ["Egil Martinsson", "Adrian Kim", "Jaesung Huh", "Jaegul Choo", "Jung-Woo Ha"], "TL;DR": "We present a general solution to event prediction that has been there all along; Discrete Time Parametric Survival Analysis.", "pdf": "/pdf/de3044062bda980272d477826dc45ef5b02cfaf1.pdf", "paperhash": "martinsson|neural_distribution_learning_for_generalized_timetoevent_prediction", "_bibtex": "@misc{\nmartinsson2019neural,\ntitle={Neural Distribution Learning for generalized time-to-event prediction},\nauthor={Egil Martinsson and Adrian Kim and Jaesung Huh and Jaegul Choo and Jung-Woo Ha},\nyear={2019},\nurl={https://openreview.net/forum?id=SyG4RiR5Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper887/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607933, "tddate": null, "super": null, "final": null, "reply": {"forum": "SyG4RiR5Ym", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper887/Authors", "ICLR.cc/2019/Conference/Paper887/Reviewers", "ICLR.cc/2019/Conference/Paper887/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper887/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper887/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper887/Authors|ICLR.cc/2019/Conference/Paper887/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper887/Reviewers", "ICLR.cc/2019/Conference/Paper887/Authors", "ICLR.cc/2019/Conference/Paper887/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607933}}}, {"id": "SkeMu1XcAX", "original": null, "number": 6, "cdate": 1543282537806, "ddate": null, "tcdate": 1543282537806, "tmdate": 1543282537806, "tddate": null, "forum": "SyG4RiR5Ym", "replyto": "r1xplpiajm", "invitation": "ICLR.cc/2019/Conference/-/Paper887/Official_Comment", "content": {"title": "The task is generalized, with two concrete realizations (in experiment).", "comment": "> First of all, this paper should be more clear from the begining of the kind of problems it aim to tackle. \n\nWe highly value the feedback. An overarching theme was trying to be as general as possible, as we found much work being too specific when there's a much wider set of data, problems, and network architectures that can be utilized once we understand the fundamentals of predicting a parametric survival distribution.\n\nThere is however a very common problem domain that we are explicitly working on (Section 4) we also try to generalize the problem to the general task of predicting a probability distribution with possibly censored or discrete target data, and show how it can even improve on common sparse classification tasks (Section 5).\n\nCurrently we can\u2019t release implementation and all experimental data without breaking double blind. Once this is done, with corresponding visualizations and data-manipulation should be a bit easier. We tried to make some amends to make it clearer.\n\n> It the current form, it is very hard to know what are the inputs, are they sequences of various kinds of events or only one type of event per sequence. It is either not clear to me wether the censoring time is constant or not and wether it is given as input (censoring time looks to be known from section 3.4.\n\nCensoring time is used to calculate censoring indicators, itself used for training. It typically varies with time. Input to the neural network (features) can be anything, output are parameters of a distribution. \nSpecifically, for the experiment in section 4 it's sequences of features. The target (supplied during training) is a sequence of time to event (which will look like a countdown/sawtooth wave as in figure 2) and censoring indicators used for the loss function.\nCensoring time for a sequence would be the time to the end of the sequence (so it's a countdown). Censoring indicators will thus vary.\n\nIn section 5, input is a 50 ms time-window of a spectrogram. Target is bivariate; the time to event and time since event (each with their respective censoring indicators). This was supposed to exemplify that with just a change in the output dimension and feature transformation, the same model may be used for something seemingly different like making multivariate predictions.\n\n> but in that case I do not really understand the contribution : does it not correspond to a very classical problem where events from outside of the observation window should be considered during training ? classical EM approaches can be developped for this).\n\nIt is true that this corresponds closely to the classical problem of, during training, considering whether events were *not* in the observation/data window (i.e censored). While we heard of no relevant EM-methods but consider our method as exactly developing on a classical approach Parametric Survival Analysis (PSA) which we found other work not sufficiently recognizing. \nThe purpose is to say that - while there seems to be many shiny and complex solutions out there - let's first do an in-depth discussion of the classical approach.\nIt's easy to see that most other papers can be considered as derivative work of PSA, but we couldn't find anyone going into depth on the idea itself.\n\nOur general reasoning is that by thinking from the classical idea of PSA, many variations (our contributions) immediately follows and are easy to implement  such as predicting all parameters of the distribution like other Density Networks do, being able to discretize TTE, composing distributions to make other distributions, multivariate predictions, architecture agnosticism all the while making it fit well with popular probabilistic programming paradigms (Edward, PyTorch Distributions, Pyro).\n\n> The problem of unevenly spaced sequences should also be more formally defined.\n\nWe thought this was clear in the context of event-generated time series no? Asyncronous measurements vs Evenly spaced measurements for temporal models is a classic such problem we wanted to address discussing in section 3.4. In the context of TTE an additional confounding factor is that the lag between observations may be connected to what we want to predict.\n\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper887/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper887/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper887/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Distribution Learning for generalized time-to-event prediction", "abstract": "Predicting the time to the next event is an important task in various domains. \nHowever, due to censoring and irregularly sampled sequences, time-to-event prediction has resulted in limited success only for particular tasks, architectures and data. Using recent advances in probabilistic programming and density networks, we make the case for a generalized parametric survival approach, sequentially predicting a distribution over the time to the next event. \nUnlike previous work, the proposed method can use asynchronously sampled features for censored, discrete, and multivariate data. \nFurthermore, it achieves good performance and near perfect calibration for probabilistic predictions without using rigid network-architectures, multitask approaches, complex learning schemes or non-trivial adaptations of cox-models. \nWe firmly establish that this can be achieved in the standard neural network framework by simply switching out the output layer and loss function.", "keywords": ["Deep Learning", "Survival Analysis", "Event prediction", "Time Series", "Probabilistic Programming", "Density Networks"], "authorids": ["egil.martinsson@gmail.com", "adrian.kim@navercorp.com", "jaesung.huh@navercorp.com", "jchoo@korea.ac.kr", "jungwoo.ha@navercorp.com"], "authors": ["Egil Martinsson", "Adrian Kim", "Jaesung Huh", "Jaegul Choo", "Jung-Woo Ha"], "TL;DR": "We present a general solution to event prediction that has been there all along; Discrete Time Parametric Survival Analysis.", "pdf": "/pdf/de3044062bda980272d477826dc45ef5b02cfaf1.pdf", "paperhash": "martinsson|neural_distribution_learning_for_generalized_timetoevent_prediction", "_bibtex": "@misc{\nmartinsson2019neural,\ntitle={Neural Distribution Learning for generalized time-to-event prediction},\nauthor={Egil Martinsson and Adrian Kim and Jaesung Huh and Jaegul Choo and Jung-Woo Ha},\nyear={2019},\nurl={https://openreview.net/forum?id=SyG4RiR5Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper887/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607933, "tddate": null, "super": null, "final": null, "reply": {"forum": "SyG4RiR5Ym", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper887/Authors", "ICLR.cc/2019/Conference/Paper887/Reviewers", "ICLR.cc/2019/Conference/Paper887/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper887/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper887/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper887/Authors|ICLR.cc/2019/Conference/Paper887/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper887/Reviewers", "ICLR.cc/2019/Conference/Paper887/Authors", "ICLR.cc/2019/Conference/Paper887/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607933}}}, {"id": "r1la-afcAm", "original": null, "number": 5, "cdate": 1543281924667, "ddate": null, "tcdate": 1543281924667, "tmdate": 1543281924667, "tddate": null, "forum": "SyG4RiR5Ym", "replyto": "HyxXPizcRX", "invitation": "ICLR.cc/2019/Conference/-/Paper887/Official_Comment", "content": {"title": "[3] Example: Seemingly similar PSA-work with provable misconception.", "comment": "# 3.\nWe added this to Section 2. We thought it had many qualities, especially that it seems to be the first work comparing something very close to a (discrete) parametric survival approach with other approaches using what we understand as comparable NN-architectures for each loss function.\n\nIt was overlooked at first because this is one of many papers using weighting schemes for the loss function without evaluating the unbiasedness (read; calibration) of the approach. \nIn short, the paper (as many others) suggests weighting the loss with '0<a<1' as:\n\n'loss = a*uncensored_part+(1-a)*censored_part'\n\nOne can show analytically that the weighting scheme will lead to biased predictions and we didn't want to comment on it prematurely as it was just published, but let\u2019s make this point here.\n\nAs a proof, consider the case of the exponential distribution, with 'L' being the scale-parameter, 'Y~exp(L_real)', and 'a' being the proposed loss-weighting scheme, then the expected value of the gradient of the log-likelihood with censoring;\n\n'''\nd/dL E[-a*<Y<c>* log[L]-u(1-a)min(Y,c)/L] =\n  F(c)[(1-a)-a*(L_real/L)]/L = *\n'''\nWhere '<Y<c>' is the iverson bracket, being '1' for an uncensored sample.\n\nIn other terms, with 'L_real=L' then '*=(1-exp(-c/L))[1-2a]/L' which needs to be '0', so it will converge to 'L_real=L' iff 'a=0.5'. Otherwise it converges to 'L = L_real*(1-a)/a'\n\nIf the math is not convincing, let's apply that weighting scheme just for parametric learning in a simple experiment:\n\n'''\nimport torch\nimport torch.nn as nn\n\ndef train_censored(a=0.5,c=1):\n  torch.manual_seed(1)\n  # True distribution is ~Exp(1)\n  L_real = torch.ones(1)\n  dist = torch.distributions.Exponential(rate=L_real)\n  L_log = nn.Parameter(torch.randn(1))\n  optimizer = torch.optim.SGD([L_log,],lr = 1)\n  for step in range(3000):\n    y = dist.sample((1000,))\n\n    # Censored (Truncated) as [0,c)\n    y = y = torch.min(y,y*0+c)\n    u = (y<c).float() # \"non-censoring indicator\"\n    optimizer.zero_grad()\n\n    L = L_log.exp() #\n    # Exponential Log-likelihood with \"weighted loss terms\"\n    loglik = -u*a*L.log()-(1-a)*y/L\n    loss = -loglik.mean()\n\n    loss.backward()\n    optimizer.step()\n  print('Sought=',L_real.item(),'\\tResult:\\t',L.item(),'\\tExpected=',(L_real*((1-a)/a)).item())\n  \ntrain_censored(a=0.5,c=3)\n#>> Sought= 1.0 Result: 1.0122132301330566 Expected= 1.0\ntrain_censored(a=0.9,c=3)\n#>> Sought= 1.0 Result: 0.11424004286527634 Expected= 0.1111111119389534\ntrain_censored(a=0.1,c=3)\n#>> Sought= 1.0 Result: 8.986083030700684  Expected= 9.0\n'''\n\nI.e only with equal (no) weighting will it converge to a correct estimate lambda = 1.\n\nWe think this example illustrates that there still seems to be some confusion about whether the basic approach we argue for (not weighting or manipulating the loss function) works in the first place, and whether it\u2019s in fact the correct way of doing it. Considering that many of the apparent applications of these types of models are clinical, this is insufficient.\n\nThe point here is that maybe the fundamentals of survival analysis may not be as widely understood as we would like to think it is, and that there's still space to ask questions beyond comparing full implementations of models and rank them on some metric. This is what we wanted to achieve with our paper.\n\n#5.\nThank you, we corrected this."}, "signatures": ["ICLR.cc/2019/Conference/Paper887/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper887/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper887/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Distribution Learning for generalized time-to-event prediction", "abstract": "Predicting the time to the next event is an important task in various domains. \nHowever, due to censoring and irregularly sampled sequences, time-to-event prediction has resulted in limited success only for particular tasks, architectures and data. Using recent advances in probabilistic programming and density networks, we make the case for a generalized parametric survival approach, sequentially predicting a distribution over the time to the next event. \nUnlike previous work, the proposed method can use asynchronously sampled features for censored, discrete, and multivariate data. \nFurthermore, it achieves good performance and near perfect calibration for probabilistic predictions without using rigid network-architectures, multitask approaches, complex learning schemes or non-trivial adaptations of cox-models. \nWe firmly establish that this can be achieved in the standard neural network framework by simply switching out the output layer and loss function.", "keywords": ["Deep Learning", "Survival Analysis", "Event prediction", "Time Series", "Probabilistic Programming", "Density Networks"], "authorids": ["egil.martinsson@gmail.com", "adrian.kim@navercorp.com", "jaesung.huh@navercorp.com", "jchoo@korea.ac.kr", "jungwoo.ha@navercorp.com"], "authors": ["Egil Martinsson", "Adrian Kim", "Jaesung Huh", "Jaegul Choo", "Jung-Woo Ha"], "TL;DR": "We present a general solution to event prediction that has been there all along; Discrete Time Parametric Survival Analysis.", "pdf": "/pdf/de3044062bda980272d477826dc45ef5b02cfaf1.pdf", "paperhash": "martinsson|neural_distribution_learning_for_generalized_timetoevent_prediction", "_bibtex": "@misc{\nmartinsson2019neural,\ntitle={Neural Distribution Learning for generalized time-to-event prediction},\nauthor={Egil Martinsson and Adrian Kim and Jaesung Huh and Jaegul Choo and Jung-Woo Ha},\nyear={2019},\nurl={https://openreview.net/forum?id=SyG4RiR5Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper887/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607933, "tddate": null, "super": null, "final": null, "reply": {"forum": "SyG4RiR5Ym", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper887/Authors", "ICLR.cc/2019/Conference/Paper887/Reviewers", "ICLR.cc/2019/Conference/Paper887/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper887/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper887/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper887/Authors|ICLR.cc/2019/Conference/Paper887/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper887/Reviewers", "ICLR.cc/2019/Conference/Paper887/Authors", "ICLR.cc/2019/Conference/Paper887/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607933}}}, {"id": "HyxXPizcRX", "original": null, "number": 4, "cdate": 1543281499467, "ddate": null, "tcdate": 1543281499467, "tmdate": 1543281499467, "tddate": null, "forum": "SyG4RiR5Ym", "replyto": "HkgIpdGqC7", "invitation": "ICLR.cc/2019/Conference/-/Paper887/Official_Comment", "content": {"title": "[4] It's easy to use censored data - but few do it simply or correctly.", "comment": "# 4.\nWe agree fully with this and think its a strong reason motivating this work. We think that it's completely clear that most loss functions are special cases of parametric survival approach, and modifying the loss in this manner has been known at least since Moivre 1731. Yet we find no work treating it as such or answering basic questions underpinning it or follow what we find to be its implications. \nOn the contrary, there seem to be a widespread belief for the need for applying different weighting schemes, adding extra loss-terms, carefully designing the target value, the network architecture, and similar. If our approach was obvious, then\n\n- Why doesn't all work comment on the problem of censored data when it's inherent in the domain?\n- Why doesn't everybody predict *all* parameters of some distribution (instead predicting only scale-parameter or similar)?\n- Why are much work distribution-centric and network architectures designed specifically for particular distributions?\n- Why is discrete data often treated as a special case? \n- Why does the presentation/notation of the loss functions so widely differ?\n- Why is it that there are many papers who uses Neural Networks (NNs) to explicitly model the hazard function but doesn't compare to parametric density network baselines (as the one we propose) with similar NN-architectures/number of parameters?\n- Why are stochastic process (Temporal Point Process)-based models with restricted ways of taking account of the nonlinearities of feature data still considered relevant baselines to advanced NNs?\n- Why are evaluations mainly limited to pointwise predictions?\n- Why are pointwise-predictions evaluated with metrics that does not work for censored data (Ex MAE as for DeepHit)?\n- Why is there so little discussion regarding the calibration of predicted distributions?\n- Finally, why is a model with strictly increasing hazard function (RMTP [1]; \u03bb(x) ~= exp(x/L) still considered among SOTA? No hazard function that we trained which can take that form (ex Weibull) seem to want take it for similar data. Hint: It's about evaluation. If one removes censored data one cuts the right tail of the empirical distribution so hazard is *by design* increasing in the training data since each sequence now ends with an event, so the RMTP hazard function fits by design.)\n\nThe point is not to critique prior work, clearly the concepts of parametric survival analysis is widely known but maybe its general implications for how it effortlessly fits with density networks or discrete data is less known? It might have to do with Time To Event modelling being inherently complex and confusing.\n\nIn relation to whether we should have added comparisons to other work we refer to the answer we gave to reviewer #3. In short, we would have wanted to compare explicitly to other work, but this was found hard or irrelevant to fit in. Instead we limited ourselves to digging deep into whether this general (Parametric Survival Approach) is correct by considering if it's performant (vs Binary classification) and calibrated.\n\n[1] https://www.kdd.org/kdd2016/papers/files/rpp1081-duA.pdf"}, "signatures": ["ICLR.cc/2019/Conference/Paper887/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper887/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper887/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Distribution Learning for generalized time-to-event prediction", "abstract": "Predicting the time to the next event is an important task in various domains. \nHowever, due to censoring and irregularly sampled sequences, time-to-event prediction has resulted in limited success only for particular tasks, architectures and data. Using recent advances in probabilistic programming and density networks, we make the case for a generalized parametric survival approach, sequentially predicting a distribution over the time to the next event. \nUnlike previous work, the proposed method can use asynchronously sampled features for censored, discrete, and multivariate data. \nFurthermore, it achieves good performance and near perfect calibration for probabilistic predictions without using rigid network-architectures, multitask approaches, complex learning schemes or non-trivial adaptations of cox-models. \nWe firmly establish that this can be achieved in the standard neural network framework by simply switching out the output layer and loss function.", "keywords": ["Deep Learning", "Survival Analysis", "Event prediction", "Time Series", "Probabilistic Programming", "Density Networks"], "authorids": ["egil.martinsson@gmail.com", "adrian.kim@navercorp.com", "jaesung.huh@navercorp.com", "jchoo@korea.ac.kr", "jungwoo.ha@navercorp.com"], "authors": ["Egil Martinsson", "Adrian Kim", "Jaesung Huh", "Jaegul Choo", "Jung-Woo Ha"], "TL;DR": "We present a general solution to event prediction that has been there all along; Discrete Time Parametric Survival Analysis.", "pdf": "/pdf/de3044062bda980272d477826dc45ef5b02cfaf1.pdf", "paperhash": "martinsson|neural_distribution_learning_for_generalized_timetoevent_prediction", "_bibtex": "@misc{\nmartinsson2019neural,\ntitle={Neural Distribution Learning for generalized time-to-event prediction},\nauthor={Egil Martinsson and Adrian Kim and Jaesung Huh and Jaegul Choo and Jung-Woo Ha},\nyear={2019},\nurl={https://openreview.net/forum?id=SyG4RiR5Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper887/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607933, "tddate": null, "super": null, "final": null, "reply": {"forum": "SyG4RiR5Ym", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper887/Authors", "ICLR.cc/2019/Conference/Paper887/Reviewers", "ICLR.cc/2019/Conference/Paper887/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper887/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper887/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper887/Authors|ICLR.cc/2019/Conference/Paper887/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper887/Reviewers", "ICLR.cc/2019/Conference/Paper887/Authors", "ICLR.cc/2019/Conference/Paper887/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607933}}}, {"id": "HkgIpdGqC7", "original": null, "number": 3, "cdate": 1543280829945, "ddate": null, "tcdate": 1543280829945, "tmdate": 1543281345455, "tddate": null, "forum": "SyG4RiR5Ym", "replyto": "SJePeMzp2X", "invitation": "ICLR.cc/2019/Conference/-/Paper887/Official_Comment", "content": {"title": "[1-2] Clarifications on Mixtures, note on discretization.", "comment": "Thank you for your very relevant and insightful comments.  We appreciate the comments and tried small changes to tie named sections together better. Some things were originally kept orthogonal by design, as distributions (3.1-3.3) and loss functions is quite independent from what kind of feature/target engineering takes places (3.4, 4) which we found previous work being unclear about.\n \nA small comment on the summary;\n\n> The framework allows different popular architectures to learn the representation of the past events with different explicit features.\n\nWhile this is true, we mainly focus on using features to learn representations of *future* events.\n\n# 1.\nThis was also pointed out by other reviewers. We tried to fix this as our presentation was not clear. We did in fact run experiments for ParetoMixedHazards, LogisticMixedhazards, WeibullMixedHazards but did not have space to present results broken down by distributions other than in appendix. \nThe was more to show that the CHF-perspective makes it easy to compose positive distributions that effortlessly interfaces with density networks, than to look into details of individual distributions. We tried to make this clearer with some of the edits. See comment to AnonReviewer1 and AnonReviewer2. \n\nIn the accompanying github repository (awaiting publication), we will release all logs from each individual experiment. With code released, it should also be clear how this machinery looks like in practice. We did not find a good method to release it yet without breaking a double-blind policy.\n\n# 2.\nWe hope not to make a case for any optimal method of choosing the bin width or how to aggregate **features** while discretizing. We think this is an application & data-specific question and want to leave it as such, this is why we wanted to make it easy to make different choices. \n\nIn the experiments in Section 4, one of the *features* was 'log(1+count of events in prior timestep)'. While experience shows this is usually a good feature, it should be seen as an arbitrary feature generation step that we chose only because it could be done consistently for all datasets under consideration.\n\nTo aggregate/discretize **events**, we consider a timestep with *many events* as a time step with at least one event in. This also naturally fits into the discretization strategy.\n\nIf a future timestep (say in 'y' steps) has many events (i.e., is highly skewed), we hope that the current time to event distribution is predicted such that the hazard around that future timestep (equivalently;'\u039b(y+1)-\u039b(y)') is high. \nIn the presented framework, this is left to the modeller as the problem of choosing a reasonable distribution for the task, good feature engineering and the choice of neural networks. We hope our framework makes this easy. Through experiments, we can say that using our approach they should get a calibrated prediction when querying 'Pr(Y<=y)' and that this approach is better or at least as good as modelling it as the binary task of whether events will happen in 'y' timesteps.\n\nThere are many design choices that have been found through hard gotten experience that are more style than science. It was hard to motivate them in the paper due to the space limit. We put much effort into harmonizing the notation and the perspective on time to event problems. (Too)Much can be said about this. \n"}, "signatures": ["ICLR.cc/2019/Conference/Paper887/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper887/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper887/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Distribution Learning for generalized time-to-event prediction", "abstract": "Predicting the time to the next event is an important task in various domains. \nHowever, due to censoring and irregularly sampled sequences, time-to-event prediction has resulted in limited success only for particular tasks, architectures and data. Using recent advances in probabilistic programming and density networks, we make the case for a generalized parametric survival approach, sequentially predicting a distribution over the time to the next event. \nUnlike previous work, the proposed method can use asynchronously sampled features for censored, discrete, and multivariate data. \nFurthermore, it achieves good performance and near perfect calibration for probabilistic predictions without using rigid network-architectures, multitask approaches, complex learning schemes or non-trivial adaptations of cox-models. \nWe firmly establish that this can be achieved in the standard neural network framework by simply switching out the output layer and loss function.", "keywords": ["Deep Learning", "Survival Analysis", "Event prediction", "Time Series", "Probabilistic Programming", "Density Networks"], "authorids": ["egil.martinsson@gmail.com", "adrian.kim@navercorp.com", "jaesung.huh@navercorp.com", "jchoo@korea.ac.kr", "jungwoo.ha@navercorp.com"], "authors": ["Egil Martinsson", "Adrian Kim", "Jaesung Huh", "Jaegul Choo", "Jung-Woo Ha"], "TL;DR": "We present a general solution to event prediction that has been there all along; Discrete Time Parametric Survival Analysis.", "pdf": "/pdf/de3044062bda980272d477826dc45ef5b02cfaf1.pdf", "paperhash": "martinsson|neural_distribution_learning_for_generalized_timetoevent_prediction", "_bibtex": "@misc{\nmartinsson2019neural,\ntitle={Neural Distribution Learning for generalized time-to-event prediction},\nauthor={Egil Martinsson and Adrian Kim and Jaesung Huh and Jaegul Choo and Jung-Woo Ha},\nyear={2019},\nurl={https://openreview.net/forum?id=SyG4RiR5Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper887/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607933, "tddate": null, "super": null, "final": null, "reply": {"forum": "SyG4RiR5Ym", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper887/Authors", "ICLR.cc/2019/Conference/Paper887/Reviewers", "ICLR.cc/2019/Conference/Paper887/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper887/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper887/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper887/Authors|ICLR.cc/2019/Conference/Paper887/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper887/Reviewers", "ICLR.cc/2019/Conference/Paper887/Authors", "ICLR.cc/2019/Conference/Paper887/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607933}}}, {"id": "rJgnHrMqAQ", "original": null, "number": 1, "cdate": 1543279939566, "ddate": null, "tcdate": 1543279939566, "tmdate": 1543280165993, "tddate": null, "forum": "SyG4RiR5Ym", "replyto": "HklarYERom", "invitation": "ICLR.cc/2019/Conference/-/Paper887/Official_Comment", "content": {"title": "Binary predictions is a relevant baseline. Mixtures is just a part of our results.", "comment": "Thank you for your feedback.\n\n> If we are measuring the classification accuracy, there is a little justification for using survival analysis; we could use just a classification algorithm instead.\n\nA theme in the paper is to point out that predicting a distribution CDF is the same thing as making *all* classification predictions 'Pr(Y<y)' for every timestep 'y>0' ahead.\n\nWe think that our experimental results shows (surprisingly) that our survival model outperforms the classification algorithm on the classification task. Both for the arguably contrieved task of predicting specific timesteps ahead (Section 4) or whether a certain timeframe contains an event (Predicting zero steps ahead, section 5). The latter is a well known binary task in its domain which we solve with an arguably novel multivariate survival-formulation.\n\n> The evaluation is quite sub-par. Instead of reporting the standard ranking/concordance metrics, the authors report the accuracy of binary classification in certain future timestamps ahead.\n\nWe make a point of our evaluation approach to be non-standard, but we hope that our arguments for it and our critique against the standard evaluation methods for censored sequential problems (Section 3.4, Section 4 and results Section 6) makes sense. \n\nWe understand the standard Concordance Index (CI) to estimate how well two predictions are expected to be ordered. A good metric for the dominant paradigm of pointwise-predicting TTE (regression/ranking). \nIn contrast, our model predicts a distribution so to answer questions of performance and calibration its arguably not a relevant/helpful metric in its commonly known form. \nTo this goal we found the Binary model a good choice of baseline (Section 4), and since CI is not defined for binary predictions, we omitted it. \nAs a sidenote, evaluating AUC on different predicted time-windows ahead as we do is tightly related to the time-specific AUC [1][2][3], in turn related to CI. \n\nWhile possible (but non-standard), we could have generalized CI to compare two predicted distributions directly as 'Pr(Y_i < Y_j)' with ground truth 'y_i<y_j' when available (\"concordant\"). This however restrics the parametric form of the distributions and is hence less general.\n\n> The authors also don\u2019t report the result for non-mixture versions, so we cannot see the true advantages of the proposed mixture modeling.\n\nThis critique was pointed out by other reviewers, and we tried to edit to make this clearer.\n\nThe main questions we wanted to answer was not which network architecture or distributions where the best. It was more;\n\n- Does our Parametric Survival model produce calibrated & good predictions independent of choice of architecture and distribution?\n- Is the this approach better or at least as good as explicitly modeling its binary subqueries? (classification approach)\n\nThe conclusions was a resounding *yes*.\n\nWe tested (but didn't report) all of the following:\n\n(3 datasets) x\n(4 evaluation thresholds ) x\n(3 network architectures) x\n[ Binary x (Use last timesteps or not),  \nHazardNet x (4 distributions) x (MixedHazards or not)]\n\nSome per-distribution results can be found in Appendix (see Figure 9) but we could report all tabular statistics broken up by distributions too if interesting. \n\nWhile not the main question, one conclusion was no significant improvement (see Appendix fig 9) for the more complex multimodal MixedHazards-distributions. \nThe reasons can only be speculated about, but it may give hints about the need for predicting fine grained/expressive target distributions, which seems to be quite a concern of current research (Consider DeepHit, Luck et al [0], etc).\n\nWhile we found this interesting and surprising in its own right, the main benefit we wanted to show was the ease of testing this in the first place using our framework.\n\n> the authors do not compare to the many existing deep hazard model such as Deep Survival [1], DeepSurv [2], DeepHit [3], or many variations based on deep point process modeling.\n\nWe do not explicitly test against these, see answer to reviewer #3. \nIt should be noted on the other hand, that one of our findings - that binary models will be biased unless removing last timesteps - has consequences for all methods employing what we call \"classification\" or \"multitask\" approaches (Ex DeepHit, [0] and more). We tried to clarify this in the revised version.\nOur results implies that unless they preprocessed data as we suggest, their results risks being heavily biased and uncalibrated. If not evaluated as we suggests they won't see this. We can't find any paper commenting on this issue. \n \n[0] https://arxiv.org/abs/1705.10245\n[1] https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5384160/\n[2] https://academic.oup.com/bib/article/16/1/153/238328\n[3] https://www.mayo.edu/research/documents/biostat-80pdf/doc-10027891\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper887/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper887/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper887/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Distribution Learning for generalized time-to-event prediction", "abstract": "Predicting the time to the next event is an important task in various domains. \nHowever, due to censoring and irregularly sampled sequences, time-to-event prediction has resulted in limited success only for particular tasks, architectures and data. Using recent advances in probabilistic programming and density networks, we make the case for a generalized parametric survival approach, sequentially predicting a distribution over the time to the next event. \nUnlike previous work, the proposed method can use asynchronously sampled features for censored, discrete, and multivariate data. \nFurthermore, it achieves good performance and near perfect calibration for probabilistic predictions without using rigid network-architectures, multitask approaches, complex learning schemes or non-trivial adaptations of cox-models. \nWe firmly establish that this can be achieved in the standard neural network framework by simply switching out the output layer and loss function.", "keywords": ["Deep Learning", "Survival Analysis", "Event prediction", "Time Series", "Probabilistic Programming", "Density Networks"], "authorids": ["egil.martinsson@gmail.com", "adrian.kim@navercorp.com", "jaesung.huh@navercorp.com", "jchoo@korea.ac.kr", "jungwoo.ha@navercorp.com"], "authors": ["Egil Martinsson", "Adrian Kim", "Jaesung Huh", "Jaegul Choo", "Jung-Woo Ha"], "TL;DR": "We present a general solution to event prediction that has been there all along; Discrete Time Parametric Survival Analysis.", "pdf": "/pdf/de3044062bda980272d477826dc45ef5b02cfaf1.pdf", "paperhash": "martinsson|neural_distribution_learning_for_generalized_timetoevent_prediction", "_bibtex": "@misc{\nmartinsson2019neural,\ntitle={Neural Distribution Learning for generalized time-to-event prediction},\nauthor={Egil Martinsson and Adrian Kim and Jaesung Huh and Jaegul Choo and Jung-Woo Ha},\nyear={2019},\nurl={https://openreview.net/forum?id=SyG4RiR5Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper887/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607933, "tddate": null, "super": null, "final": null, "reply": {"forum": "SyG4RiR5Ym", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper887/Authors", "ICLR.cc/2019/Conference/Paper887/Reviewers", "ICLR.cc/2019/Conference/Paper887/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper887/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper887/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper887/Authors|ICLR.cc/2019/Conference/Paper887/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper887/Reviewers", "ICLR.cc/2019/Conference/Paper887/Authors", "ICLR.cc/2019/Conference/Paper887/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607933}}}, {"id": "HygO5BG9CX", "original": null, "number": 2, "cdate": 1543280016463, "ddate": null, "tcdate": 1543280016463, "tmdate": 1543280016463, "tddate": null, "forum": "SyG4RiR5Ym", "replyto": "rJgnHrMqAQ", "invitation": "ICLR.cc/2019/Conference/-/Paper887/Official_Comment", "content": {"title": "final note on non-parametrics", "comment": "> A major baseline for mixture modeling is always non-parametric modeling.\n\nThis is true, which we agree with (see Section 6 and answer to reviewer #3). \nWe do comment on the qualitative differences (Sec. 2,6) but could not find a reliable method of quantitative comparison. \nIf we would have made a neural-architecture-independent, scalable semi-parametric model working for discrete (read *heavily tied*) sequential data (we found no such work), that would have been the main topic of the paper rather than treating it as a baseline. Instead we propose this as future work and would be very interested to see it. \nFor particular model architectures and data, there's already plenty of such comparisons as you note. \n\nIn summary, we think that our paper may be an interesting and subtly controversial addition to the discussion on temporal point processes, survival analysis and neural networks and we hope that we've answered your questions."}, "signatures": ["ICLR.cc/2019/Conference/Paper887/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper887/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper887/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Distribution Learning for generalized time-to-event prediction", "abstract": "Predicting the time to the next event is an important task in various domains. \nHowever, due to censoring and irregularly sampled sequences, time-to-event prediction has resulted in limited success only for particular tasks, architectures and data. Using recent advances in probabilistic programming and density networks, we make the case for a generalized parametric survival approach, sequentially predicting a distribution over the time to the next event. \nUnlike previous work, the proposed method can use asynchronously sampled features for censored, discrete, and multivariate data. \nFurthermore, it achieves good performance and near perfect calibration for probabilistic predictions without using rigid network-architectures, multitask approaches, complex learning schemes or non-trivial adaptations of cox-models. \nWe firmly establish that this can be achieved in the standard neural network framework by simply switching out the output layer and loss function.", "keywords": ["Deep Learning", "Survival Analysis", "Event prediction", "Time Series", "Probabilistic Programming", "Density Networks"], "authorids": ["egil.martinsson@gmail.com", "adrian.kim@navercorp.com", "jaesung.huh@navercorp.com", "jchoo@korea.ac.kr", "jungwoo.ha@navercorp.com"], "authors": ["Egil Martinsson", "Adrian Kim", "Jaesung Huh", "Jaegul Choo", "Jung-Woo Ha"], "TL;DR": "We present a general solution to event prediction that has been there all along; Discrete Time Parametric Survival Analysis.", "pdf": "/pdf/de3044062bda980272d477826dc45ef5b02cfaf1.pdf", "paperhash": "martinsson|neural_distribution_learning_for_generalized_timetoevent_prediction", "_bibtex": "@misc{\nmartinsson2019neural,\ntitle={Neural Distribution Learning for generalized time-to-event prediction},\nauthor={Egil Martinsson and Adrian Kim and Jaesung Huh and Jaegul Choo and Jung-Woo Ha},\nyear={2019},\nurl={https://openreview.net/forum?id=SyG4RiR5Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper887/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607933, "tddate": null, "super": null, "final": null, "reply": {"forum": "SyG4RiR5Ym", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper887/Authors", "ICLR.cc/2019/Conference/Paper887/Reviewers", "ICLR.cc/2019/Conference/Paper887/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper887/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper887/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper887/Authors|ICLR.cc/2019/Conference/Paper887/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper887/Reviewers", "ICLR.cc/2019/Conference/Paper887/Authors", "ICLR.cc/2019/Conference/Paper887/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607933}}}, {"id": "SJePeMzp2X", "original": null, "number": 3, "cdate": 1541378543178, "ddate": null, "tcdate": 1541378543178, "tmdate": 1542137579953, "tddate": null, "forum": "SyG4RiR5Ym", "replyto": "SyG4RiR5Ym", "invitation": "ICLR.cc/2019/Conference/-/Paper887/Official_Review", "content": {"title": "Re: Neural Distribution Learning for generalized time-to-event prediction", "review": "The authors propose a parametric framework (HazardNet) for survival analysis with deep learning where they mainly focus on the discrete-time case. The framework allows different popular architectures to learn the representation of the past events with different explicit features. Then, it considers a bunch of parametric families and their mixtures for the distribution of inter-event time. Experiments include a comprehensive comparison between HazardNet and different binary classifiers trained separately at each target time duration. \n\nOverall, the paper is well-written and easy to follow. It seeks to build a strong baseline for the deep survival analysis, which is an hot ongoing research topic recently in literature. However, there are a few weaknesses that should be addressed. \n\n1. In the beginning, the paper motivates the mixtures of distributions from MDN. Because most existing work focuses on the formulation of the intensity function, it is very interesting to approach the problem from the cumulative intensity function instead. Originally, it looks like the paper seeks to formulate a general parametric form based on MDN. However, it is disappointing that in the experiments, it still only considers a few classic parametric distributions. There is lack of solid technical connection between Sec 3.1, 3.2 and Sec 4.\n\n2. The discretization discussion of Sec 3.4 is not clear. Normally, the major motivation for discretization is application-driven, say, in hospital, the doctor regularly triggers the inspection event. However, how to optimally choose a bin-size and how to aggregate the multiple events within each bin is still not clear, which is not sufficiently discussed in the paper. Why is taking the summation of the events in a bin a proper way of aggregation? What if we have highly skewed bins?\n\n3. Although the comparison and experimental setting in Figure 4 is comprehensive, the paper misses a very related work \"Deep Recurrent Survival Analysis, https://arxiv.org/abs/1809.02403\", which also considers the discrete-time version of survival analysis. Only comparing with the binary classifiers is not quite convincing without referring to other survival analysis work.\n\n4. Finally, the authors state that existing temporal point process work \"have little meaning without taking into account censored data\". However, if inspecting the loss function of these work closely, we can see there is a survival term exactly the same as the log-cumulative hazard in Equation 3 that handles the censored case.\n\n5. A typo on the bottom of page 3, should be p(t) = F(t + 1) - F(t)\n", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper887/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Neural Distribution Learning for generalized time-to-event prediction", "abstract": "Predicting the time to the next event is an important task in various domains. \nHowever, due to censoring and irregularly sampled sequences, time-to-event prediction has resulted in limited success only for particular tasks, architectures and data. Using recent advances in probabilistic programming and density networks, we make the case for a generalized parametric survival approach, sequentially predicting a distribution over the time to the next event. \nUnlike previous work, the proposed method can use asynchronously sampled features for censored, discrete, and multivariate data. \nFurthermore, it achieves good performance and near perfect calibration for probabilistic predictions without using rigid network-architectures, multitask approaches, complex learning schemes or non-trivial adaptations of cox-models. \nWe firmly establish that this can be achieved in the standard neural network framework by simply switching out the output layer and loss function.", "keywords": ["Deep Learning", "Survival Analysis", "Event prediction", "Time Series", "Probabilistic Programming", "Density Networks"], "authorids": ["egil.martinsson@gmail.com", "adrian.kim@navercorp.com", "jaesung.huh@navercorp.com", "jchoo@korea.ac.kr", "jungwoo.ha@navercorp.com"], "authors": ["Egil Martinsson", "Adrian Kim", "Jaesung Huh", "Jaegul Choo", "Jung-Woo Ha"], "TL;DR": "We present a general solution to event prediction that has been there all along; Discrete Time Parametric Survival Analysis.", "pdf": "/pdf/de3044062bda980272d477826dc45ef5b02cfaf1.pdf", "paperhash": "martinsson|neural_distribution_learning_for_generalized_timetoevent_prediction", "_bibtex": "@misc{\nmartinsson2019neural,\ntitle={Neural Distribution Learning for generalized time-to-event prediction},\nauthor={Egil Martinsson and Adrian Kim and Jaesung Huh and Jaegul Choo and Jung-Woo Ha},\nyear={2019},\nurl={https://openreview.net/forum?id=SyG4RiR5Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper887/Official_Review", "cdate": 1542234354360, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SyG4RiR5Ym", "replyto": "SyG4RiR5Ym", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper887/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335825757, "tmdate": 1552335825757, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper887/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "HklarYERom", "original": null, "number": 2, "cdate": 1540405572902, "ddate": null, "tcdate": 1540405572902, "tmdate": 1541533607132, "tddate": null, "forum": "SyG4RiR5Ym", "replyto": "SyG4RiR5Ym", "invitation": "ICLR.cc/2019/Conference/-/Paper887/Official_Review", "content": {"title": "Deep Hazard Modeling with Mixture of Distributions ", "review": "This paper proposes to use a mixture of distributions for hazard modeling. They use the standard censored loss and binning-based discretization for handling irregularities in the time series. \n\nThe evaluation is quite sub-par. Instead of reporting the standard ranking/concordance metrics, the authors report the accuracy of binary classification in certain future timestamps ahead. If we are measuring the classification accuracy, there is a little justification for using survival analysis; we could use just a classification algorithm instead. Moreover, the authors do not compare to the many existing deep hazard model such as Deep Survival [1], DeepSurv [2], DeepHit [3], or many variations based on deep point process modeling. The authors also don\u2019t report the result for non-mixture versions, so we cannot see the true advantages of the proposed mixture modeling.\n\nA major baseline for mixture modeling is always non-parametric modeling. In this case, given that there are existing works on deep Cox hazard modeling, the authors need to show the advantages of their proposed mixture modeling against deep Cox models.\n\nOverall, the methodology in this paper is quite limited and the evaluation is non-standard. Thus, I vote for rejection of the paper.\n\n\n[1] Ranganath, Rajesh, et al. \"Deep Survival Analysis.\" Machine Learning for Healthcare Conference. 2016.\n\n[2] Katzman, Jared L., et al. \"DeepSurv: personalized treatment recommender system using a Cox proportional hazards deep neural network.\" BMC medical research methodology 18.1 (2018): 24.\n\n[3] Lee, Changhee, et al. \"Deephit: A deep learning approach to survival analysis with competing risks.\" AAAI, 2018.", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper887/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Distribution Learning for generalized time-to-event prediction", "abstract": "Predicting the time to the next event is an important task in various domains. \nHowever, due to censoring and irregularly sampled sequences, time-to-event prediction has resulted in limited success only for particular tasks, architectures and data. Using recent advances in probabilistic programming and density networks, we make the case for a generalized parametric survival approach, sequentially predicting a distribution over the time to the next event. \nUnlike previous work, the proposed method can use asynchronously sampled features for censored, discrete, and multivariate data. \nFurthermore, it achieves good performance and near perfect calibration for probabilistic predictions without using rigid network-architectures, multitask approaches, complex learning schemes or non-trivial adaptations of cox-models. \nWe firmly establish that this can be achieved in the standard neural network framework by simply switching out the output layer and loss function.", "keywords": ["Deep Learning", "Survival Analysis", "Event prediction", "Time Series", "Probabilistic Programming", "Density Networks"], "authorids": ["egil.martinsson@gmail.com", "adrian.kim@navercorp.com", "jaesung.huh@navercorp.com", "jchoo@korea.ac.kr", "jungwoo.ha@navercorp.com"], "authors": ["Egil Martinsson", "Adrian Kim", "Jaesung Huh", "Jaegul Choo", "Jung-Woo Ha"], "TL;DR": "We present a general solution to event prediction that has been there all along; Discrete Time Parametric Survival Analysis.", "pdf": "/pdf/de3044062bda980272d477826dc45ef5b02cfaf1.pdf", "paperhash": "martinsson|neural_distribution_learning_for_generalized_timetoevent_prediction", "_bibtex": "@misc{\nmartinsson2019neural,\ntitle={Neural Distribution Learning for generalized time-to-event prediction},\nauthor={Egil Martinsson and Adrian Kim and Jaesung Huh and Jaegul Choo and Jung-Woo Ha},\nyear={2019},\nurl={https://openreview.net/forum?id=SyG4RiR5Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper887/Official_Review", "cdate": 1542234354360, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SyG4RiR5Ym", "replyto": "SyG4RiR5Ym", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper887/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335825757, "tmdate": 1552335825757, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper887/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "r1xplpiajm", "original": null, "number": 1, "cdate": 1540369652701, "ddate": null, "tcdate": 1540369652701, "tmdate": 1541533606924, "tddate": null, "forum": "SyG4RiR5Ym", "replyto": "SyG4RiR5Ym", "invitation": "ICLR.cc/2019/Conference/-/Paper887/Official_Review", "content": {"title": "Important clarifications should be given about the task and the model ", "review": "The paper \"Neural Distribution Learning for generalized time-to-event prediction\" proposes HazardNet, a neural network framework for time-to-event prediction with right-censored data. \n \nFirst of all, this paper should be more clear from the begining of the kind of problems it aim to tackle. The tasks the proposal is able to consider is not easy to realize, at least before the experiments part. The problem should be clearly formalized in the begining of the paper (for instance in the introduction of section 3). It the current form, it is very hard to know what are the inputs, are they sequences of various kinds of events or only one type of event per sequence. It is either not clear to me wether the censoring time is constant or not and wether it is given as input (censoring time looks to be known from section 3.4 but in that case I do not really understand the contribution : does it not correspond to a very classical problem where events from outside of the observation window should be considered during training ? classical EM approaches can be developped for this). The problem of unevenly spaced sequences should also be more formally defined. \n\nAlso, while the HazardNet framework looks convenient, by using hazard and survival functions as discusses by the authors, it is not clear to me what are the benefits from recent works in neural temporal point processes which also define a general framework for temporal predictions of events. Approaches such at least like \"Modeling the intensity function of point process via recurrent neural networks\" should be considered in the experiments, though they do not explicitely model censoring but  with slight adapations should be able to work well of experimental data. \n\n\n", "rating": "3: Clear rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper887/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Distribution Learning for generalized time-to-event prediction", "abstract": "Predicting the time to the next event is an important task in various domains. \nHowever, due to censoring and irregularly sampled sequences, time-to-event prediction has resulted in limited success only for particular tasks, architectures and data. Using recent advances in probabilistic programming and density networks, we make the case for a generalized parametric survival approach, sequentially predicting a distribution over the time to the next event. \nUnlike previous work, the proposed method can use asynchronously sampled features for censored, discrete, and multivariate data. \nFurthermore, it achieves good performance and near perfect calibration for probabilistic predictions without using rigid network-architectures, multitask approaches, complex learning schemes or non-trivial adaptations of cox-models. \nWe firmly establish that this can be achieved in the standard neural network framework by simply switching out the output layer and loss function.", "keywords": ["Deep Learning", "Survival Analysis", "Event prediction", "Time Series", "Probabilistic Programming", "Density Networks"], "authorids": ["egil.martinsson@gmail.com", "adrian.kim@navercorp.com", "jaesung.huh@navercorp.com", "jchoo@korea.ac.kr", "jungwoo.ha@navercorp.com"], "authors": ["Egil Martinsson", "Adrian Kim", "Jaesung Huh", "Jaegul Choo", "Jung-Woo Ha"], "TL;DR": "We present a general solution to event prediction that has been there all along; Discrete Time Parametric Survival Analysis.", "pdf": "/pdf/de3044062bda980272d477826dc45ef5b02cfaf1.pdf", "paperhash": "martinsson|neural_distribution_learning_for_generalized_timetoevent_prediction", "_bibtex": "@misc{\nmartinsson2019neural,\ntitle={Neural Distribution Learning for generalized time-to-event prediction},\nauthor={Egil Martinsson and Adrian Kim and Jaesung Huh and Jaegul Choo and Jung-Woo Ha},\nyear={2019},\nurl={https://openreview.net/forum?id=SyG4RiR5Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper887/Official_Review", "cdate": 1542234354360, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SyG4RiR5Ym", "replyto": "SyG4RiR5Ym", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper887/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335825757, "tmdate": 1552335825757, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper887/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 12}