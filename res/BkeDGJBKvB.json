{"notes": [{"id": "BkeDGJBKvB", "original": "H1lZtz3OvB", "number": 1583, "cdate": 1569439503117, "ddate": null, "tcdate": 1569439503117, "tmdate": 1577168289922, "tddate": null, "forum": "BkeDGJBKvB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["maximilian.igl@gmail.com", "gambs@robots.ox.ac.uk", "jinkehe1996@gmail.com", "nantas@robots.ox.ac.uk", "nsid@robots.ox.ac.uk", "wendelin.boehmer@cs.ox.ac.uk", "shimon.whiteson@cs.ox.ac.uk"], "title": "Multitask Soft Option Learning", "authors": ["Maximilian Igl", "Andrew Gambardella", "Jinke He", "Nantas Nardelli", "N. Siddharth", "Wendelin B\u00f6hmer", "Shimon Whiteson"], "pdf": "/pdf/5e7c12118369a575ab52f9fb553689c36468b080.pdf", "TL;DR": "In Hierarchical RL, we introduce the notion of a 'soft', i.e. adaptable, option and show that this helps learning in multitask settings.", "abstract": "We present Multitask Soft Option Learning (MSOL), a hierarchical multi-task framework based on Planning-as-Inference. MSOL extends the concept of Options, using separate variational posteriors for each task, regularized by a shared prior. The learned soft-options are temporally extended, allowing a higher-level master policy to train faster on new tasks by making decisions with lower frequency. Additionally, MSOL allows fine-tuning of soft-options for new tasks without unlearning previously useful behavior, and avoids problems with local minima in multitask training. We demonstrate empirically that MSOL significantly outperforms both hierarchical and flat transfer-learning baselines in challenging multi-task environments.", "keywords": ["Hierarchical Reinforcement Learning", "Reinforcement Learning", "Control as Inference", "Options", "Multitask Learning"], "paperhash": "igl|multitask_soft_option_learning", "original_pdf": "/attachment/9be82fd541efe5e82d10afd0fee469b328a0ebe5.pdf", "_bibtex": "@misc{\nigl2020multitask,\ntitle={Multitask Soft Option Learning},\nauthor={Maximilian Igl and Andrew Gambardella and Jinke He and Nantas Nardelli and N. Siddharth and Wendelin B{\\\"o}hmer and Shimon Whiteson},\nyear={2020},\nurl={https://openreview.net/forum?id=BkeDGJBKvB}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "V9jLYhsgcK", "original": null, "number": 1, "cdate": 1576798727179, "ddate": null, "tcdate": 1576798727179, "tmdate": 1576800909325, "tddate": null, "forum": "BkeDGJBKvB", "replyto": "BkeDGJBKvB", "invitation": "ICLR.cc/2020/Conference/Paper1583/-/Decision", "content": {"decision": "Reject", "comment": "Apologies for only receiving two reviews. R2 gave a WR and R3 gave an A. Given the lack of 3rd review and split nature of the scores, the AC has closely scrutinized the paper/reviews/comments/rebuttal. Thoughts:\n - Paper is on interesting topic.\n - AC agrees with R2's concern about the evaluation not using more complex environments like Mujoco. Without evaluation on a standard benchmark, it is difficult to know objectively if the approach works. \n - AC agrees with authors that the DISTRAL approach forms a strong baseline. \n - Nevertheless, the experiments aren't super compelling either.\n - AC has some concerns about scaling issues w.r.t. model size & #tasks. \n\nThe paper is very borderline, but the AC sides with R2's concerns and unfortunately feels the paper cannot be accepted without a stronger evaluation. With this, it would make a compelling paper. ", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["maximilian.igl@gmail.com", "gambs@robots.ox.ac.uk", "jinkehe1996@gmail.com", "nantas@robots.ox.ac.uk", "nsid@robots.ox.ac.uk", "wendelin.boehmer@cs.ox.ac.uk", "shimon.whiteson@cs.ox.ac.uk"], "title": "Multitask Soft Option Learning", "authors": ["Maximilian Igl", "Andrew Gambardella", "Jinke He", "Nantas Nardelli", "N. Siddharth", "Wendelin B\u00f6hmer", "Shimon Whiteson"], "pdf": "/pdf/5e7c12118369a575ab52f9fb553689c36468b080.pdf", "TL;DR": "In Hierarchical RL, we introduce the notion of a 'soft', i.e. adaptable, option and show that this helps learning in multitask settings.", "abstract": "We present Multitask Soft Option Learning (MSOL), a hierarchical multi-task framework based on Planning-as-Inference. MSOL extends the concept of Options, using separate variational posteriors for each task, regularized by a shared prior. The learned soft-options are temporally extended, allowing a higher-level master policy to train faster on new tasks by making decisions with lower frequency. Additionally, MSOL allows fine-tuning of soft-options for new tasks without unlearning previously useful behavior, and avoids problems with local minima in multitask training. We demonstrate empirically that MSOL significantly outperforms both hierarchical and flat transfer-learning baselines in challenging multi-task environments.", "keywords": ["Hierarchical Reinforcement Learning", "Reinforcement Learning", "Control as Inference", "Options", "Multitask Learning"], "paperhash": "igl|multitask_soft_option_learning", "original_pdf": "/attachment/9be82fd541efe5e82d10afd0fee469b328a0ebe5.pdf", "_bibtex": "@misc{\nigl2020multitask,\ntitle={Multitask Soft Option Learning},\nauthor={Maximilian Igl and Andrew Gambardella and Jinke He and Nantas Nardelli and N. Siddharth and Wendelin B{\\\"o}hmer and Shimon Whiteson},\nyear={2020},\nurl={https://openreview.net/forum?id=BkeDGJBKvB}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "BkeDGJBKvB", "replyto": "BkeDGJBKvB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795720348, "tmdate": 1576800271162, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1583/-/Decision"}}}, {"id": "Bkgbl8_niH", "original": null, "number": 6, "cdate": 1573844457492, "ddate": null, "tcdate": 1573844457492, "tmdate": 1573844457492, "tddate": null, "forum": "BkeDGJBKvB", "replyto": "BJeiYdShiB", "invitation": "ICLR.cc/2020/Conference/Paper1583/-/Official_Comment", "content": {"title": "Question", "comment": "Thank you for your comments. \n\n>Nonetheless, while we agree in principle with your observation that additional high-dimensional experiments would further strengthen our claims, we observe that practically, the amount of compute required to be able to do is beyond our abilities.\n\nWould it be possible to comment on how much time does one run of the algorithm takes in an environment like Ant-Gather for example? I am curious to understand how hard is it to train with the proposed approach.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1583/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1583/AnonReviewer2", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["maximilian.igl@gmail.com", "gambs@robots.ox.ac.uk", "jinkehe1996@gmail.com", "nantas@robots.ox.ac.uk", "nsid@robots.ox.ac.uk", "wendelin.boehmer@cs.ox.ac.uk", "shimon.whiteson@cs.ox.ac.uk"], "title": "Multitask Soft Option Learning", "authors": ["Maximilian Igl", "Andrew Gambardella", "Jinke He", "Nantas Nardelli", "N. Siddharth", "Wendelin B\u00f6hmer", "Shimon Whiteson"], "pdf": "/pdf/5e7c12118369a575ab52f9fb553689c36468b080.pdf", "TL;DR": "In Hierarchical RL, we introduce the notion of a 'soft', i.e. adaptable, option and show that this helps learning in multitask settings.", "abstract": "We present Multitask Soft Option Learning (MSOL), a hierarchical multi-task framework based on Planning-as-Inference. MSOL extends the concept of Options, using separate variational posteriors for each task, regularized by a shared prior. The learned soft-options are temporally extended, allowing a higher-level master policy to train faster on new tasks by making decisions with lower frequency. Additionally, MSOL allows fine-tuning of soft-options for new tasks without unlearning previously useful behavior, and avoids problems with local minima in multitask training. We demonstrate empirically that MSOL significantly outperforms both hierarchical and flat transfer-learning baselines in challenging multi-task environments.", "keywords": ["Hierarchical Reinforcement Learning", "Reinforcement Learning", "Control as Inference", "Options", "Multitask Learning"], "paperhash": "igl|multitask_soft_option_learning", "original_pdf": "/attachment/9be82fd541efe5e82d10afd0fee469b328a0ebe5.pdf", "_bibtex": "@misc{\nigl2020multitask,\ntitle={Multitask Soft Option Learning},\nauthor={Maximilian Igl and Andrew Gambardella and Jinke He and Nantas Nardelli and N. Siddharth and Wendelin B{\\\"o}hmer and Shimon Whiteson},\nyear={2020},\nurl={https://openreview.net/forum?id=BkeDGJBKvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BkeDGJBKvB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1583/Authors", "ICLR.cc/2020/Conference/Paper1583/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1583/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1583/Reviewers", "ICLR.cc/2020/Conference/Paper1583/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1583/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1583/Authors|ICLR.cc/2020/Conference/Paper1583/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504153873, "tmdate": 1576860531308, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1583/Authors", "ICLR.cc/2020/Conference/Paper1583/Reviewers", "ICLR.cc/2020/Conference/Paper1583/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1583/-/Official_Comment"}}}, {"id": "BJeiYdShiB", "original": null, "number": 5, "cdate": 1573832835503, "ddate": null, "tcdate": 1573832835503, "tmdate": 1573832835503, "tddate": null, "forum": "BkeDGJBKvB", "replyto": "BJx8fVtqiH", "invitation": "ICLR.cc/2020/Conference/Paper1583/-/Official_Comment", "content": {"title": "Thank you for your prompt reply", "comment": "Thank you for your prompt reply and for valuing the contribution of our work.\n\nWe would first like to note that on Swimmer MSOL performs comparably with Distral, which is itself a strong multitask baseline.\nFurthermore, we argue that hierarchical methods such as ours are best suited to domains that exhibit a strong compositional structure, like the Taxi domain does. And while Swimmer is indeed a higher dimensional domain, it does not exhibit such compositional structure. As a consequence, comparable performance against Distral is a positive mark of MSOL's general utility with higher-dimensional tasks, with the expectation that where compositional structure may be leveraged, it can do so (cf. Taxi experiments).\n\nNonetheless, while we agree in principle with your observation that additional high-dimensional experiments would further strengthen our claims, we observe that practically, the amount of compute required to be able to do is beyond our abilities. Note that it is not simply a case of being able to run a new environment, but to be able to control for all the confounds in the form of hyperparameters and general variance in RL experiments, to be able to properly assign credit/blame to our proposed changes. Particularly, compositional tasks when combined with continuous control, tend to become extremely complex, requiring a lot of resources to train---as in Ant-Gather, for example.\n\nAltogether, we believe that the experiments we perform provide good evidence for the suitability and value of MSOL for multitask learning, and hope that the lack of more complex experiments, which were not possible due to resource constraints, do not adversely affect the value of our work.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1583/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1583/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["maximilian.igl@gmail.com", "gambs@robots.ox.ac.uk", "jinkehe1996@gmail.com", "nantas@robots.ox.ac.uk", "nsid@robots.ox.ac.uk", "wendelin.boehmer@cs.ox.ac.uk", "shimon.whiteson@cs.ox.ac.uk"], "title": "Multitask Soft Option Learning", "authors": ["Maximilian Igl", "Andrew Gambardella", "Jinke He", "Nantas Nardelli", "N. Siddharth", "Wendelin B\u00f6hmer", "Shimon Whiteson"], "pdf": "/pdf/5e7c12118369a575ab52f9fb553689c36468b080.pdf", "TL;DR": "In Hierarchical RL, we introduce the notion of a 'soft', i.e. adaptable, option and show that this helps learning in multitask settings.", "abstract": "We present Multitask Soft Option Learning (MSOL), a hierarchical multi-task framework based on Planning-as-Inference. MSOL extends the concept of Options, using separate variational posteriors for each task, regularized by a shared prior. The learned soft-options are temporally extended, allowing a higher-level master policy to train faster on new tasks by making decisions with lower frequency. Additionally, MSOL allows fine-tuning of soft-options for new tasks without unlearning previously useful behavior, and avoids problems with local minima in multitask training. We demonstrate empirically that MSOL significantly outperforms both hierarchical and flat transfer-learning baselines in challenging multi-task environments.", "keywords": ["Hierarchical Reinforcement Learning", "Reinforcement Learning", "Control as Inference", "Options", "Multitask Learning"], "paperhash": "igl|multitask_soft_option_learning", "original_pdf": "/attachment/9be82fd541efe5e82d10afd0fee469b328a0ebe5.pdf", "_bibtex": "@misc{\nigl2020multitask,\ntitle={Multitask Soft Option Learning},\nauthor={Maximilian Igl and Andrew Gambardella and Jinke He and Nantas Nardelli and N. Siddharth and Wendelin B{\\\"o}hmer and Shimon Whiteson},\nyear={2020},\nurl={https://openreview.net/forum?id=BkeDGJBKvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BkeDGJBKvB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1583/Authors", "ICLR.cc/2020/Conference/Paper1583/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1583/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1583/Reviewers", "ICLR.cc/2020/Conference/Paper1583/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1583/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1583/Authors|ICLR.cc/2020/Conference/Paper1583/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504153873, "tmdate": 1576860531308, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1583/Authors", "ICLR.cc/2020/Conference/Paper1583/Reviewers", "ICLR.cc/2020/Conference/Paper1583/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1583/-/Official_Comment"}}}, {"id": "BJx8fVtqiH", "original": null, "number": 3, "cdate": 1573717005899, "ddate": null, "tcdate": 1573717005899, "tmdate": 1573717005899, "tddate": null, "forum": "BkeDGJBKvB", "replyto": "HkgLRS7dir", "invitation": "ICLR.cc/2020/Conference/Paper1583/-/Official_Comment", "content": {"title": "Follow up comments", "comment": "Thank you for the clarifications.\n\n>Fairness of comparison:\nThank you, that is useful. \n\n>Our goal with the Swimmer environment was to show that our algorithm is applicable to continuous tasks. Applying the algorithm in practice to much more complex multi-task domains in MuJoCo was infeasible given our limited computational resources.\nConsidering swimmer is the only high dimensional experiment, and that the performance in swimmer is not very convincing; I find it hard to convince myself that the approach fairs well in high dimensional domains. It would add a lot of value to this work to validate the proposed approach in other Mujoco tasks, 3D navigation environments. As presented, it does  but only in Taxi domain and marginally in Swimmer. \n\n>learning good options and termination functions, robustly, just from a set of tasks is already a significant enough contribution.\nI agree that the paper presents a useful contribution. However the experiments are limited and therefore other high-dimensional environments could add value and strengthen the contributions further. "}, "signatures": ["ICLR.cc/2020/Conference/Paper1583/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1583/AnonReviewer2", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["maximilian.igl@gmail.com", "gambs@robots.ox.ac.uk", "jinkehe1996@gmail.com", "nantas@robots.ox.ac.uk", "nsid@robots.ox.ac.uk", "wendelin.boehmer@cs.ox.ac.uk", "shimon.whiteson@cs.ox.ac.uk"], "title": "Multitask Soft Option Learning", "authors": ["Maximilian Igl", "Andrew Gambardella", "Jinke He", "Nantas Nardelli", "N. Siddharth", "Wendelin B\u00f6hmer", "Shimon Whiteson"], "pdf": "/pdf/5e7c12118369a575ab52f9fb553689c36468b080.pdf", "TL;DR": "In Hierarchical RL, we introduce the notion of a 'soft', i.e. adaptable, option and show that this helps learning in multitask settings.", "abstract": "We present Multitask Soft Option Learning (MSOL), a hierarchical multi-task framework based on Planning-as-Inference. MSOL extends the concept of Options, using separate variational posteriors for each task, regularized by a shared prior. The learned soft-options are temporally extended, allowing a higher-level master policy to train faster on new tasks by making decisions with lower frequency. Additionally, MSOL allows fine-tuning of soft-options for new tasks without unlearning previously useful behavior, and avoids problems with local minima in multitask training. We demonstrate empirically that MSOL significantly outperforms both hierarchical and flat transfer-learning baselines in challenging multi-task environments.", "keywords": ["Hierarchical Reinforcement Learning", "Reinforcement Learning", "Control as Inference", "Options", "Multitask Learning"], "paperhash": "igl|multitask_soft_option_learning", "original_pdf": "/attachment/9be82fd541efe5e82d10afd0fee469b328a0ebe5.pdf", "_bibtex": "@misc{\nigl2020multitask,\ntitle={Multitask Soft Option Learning},\nauthor={Maximilian Igl and Andrew Gambardella and Jinke He and Nantas Nardelli and N. Siddharth and Wendelin B{\\\"o}hmer and Shimon Whiteson},\nyear={2020},\nurl={https://openreview.net/forum?id=BkeDGJBKvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BkeDGJBKvB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1583/Authors", "ICLR.cc/2020/Conference/Paper1583/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1583/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1583/Reviewers", "ICLR.cc/2020/Conference/Paper1583/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1583/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1583/Authors|ICLR.cc/2020/Conference/Paper1583/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504153873, "tmdate": 1576860531308, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1583/Authors", "ICLR.cc/2020/Conference/Paper1583/Reviewers", "ICLR.cc/2020/Conference/Paper1583/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1583/-/Official_Comment"}}}, {"id": "HJxXf8mOjB", "original": null, "number": 2, "cdate": 1573561867393, "ddate": null, "tcdate": 1573561867393, "tmdate": 1573561867393, "tddate": null, "forum": "BkeDGJBKvB", "replyto": "S1xF8IXHtH", "invitation": "ICLR.cc/2020/Conference/Paper1583/-/Official_Comment", "content": {"title": "Response to Review #3", "comment": "Thank you for the detailed reading of our work and your feedback. We hope we are able to address your comments below and welcome additional questions or comments. \n\nEquation 6:\nThe incentive to deviate from the option-prior comes from the first term in equation 6 (which we didn\u2019t assign a number to), the reward $r_i$. \nSo overall, the posterior tries to optimize the reward (which might try to pull the posterior away from the prior) while deviating from the three priors as little as possible, which induces the various effects for terms 1,2,3.\n\nEqual value of $\\beta$ for terms 1,2,3:\nWe chose to restrict ourselves to the same value of $\\beta$ for all three terms to show that our algorithm works well even in this restricted case.\nHowever, you are right that this is restrictive and it is quite likely that it is possible to further improve the results of MSOL by fine-tuning all three values separately. However, this would make the comparison to Distral less fair as it only has one regularization hyperparameter. \nConsequently, we believe that showing superior performance for equal values of $\\beta$ is a stronger argument in favor of MSOL. \n\nXi in Term 3: Yes, thank you!\n\nDelta(z_t-z_{t-1}):\nThank you for pointing this out; we included a definition in the text.\nSince we assume $z_t$ to be discrete, the delta here has the meaning of a Kronecker delta.\nFor continuous $z_t$, it would correspond to the Dirac Delta distribution. \nIn both cases we mean to express that $z_t=z_{t-1}$ with probability 1. In other words, if $b_t=0$, i.e. when the option doesn\u2019t terminate, we don\u2019t change the active option.\n\nTypo:\nThank you for pointing this out.\n\nAppendix C.1:\nWe believe you are referring to equation 9? \nWe chose not to include the superscript dash here because we are using $A_{\\phi_i}$ twice, once in $\\mathcal{L}_A$ in which case it is indeed assumed constant. We are also using it in $\\mathcal{L}_V$, in which case it is not constant as it is optimized w.r.t $V_{\\phi_i}(s_t, z_{t-1})$, i.e. the last term in equation 9. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1583/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1583/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["maximilian.igl@gmail.com", "gambs@robots.ox.ac.uk", "jinkehe1996@gmail.com", "nantas@robots.ox.ac.uk", "nsid@robots.ox.ac.uk", "wendelin.boehmer@cs.ox.ac.uk", "shimon.whiteson@cs.ox.ac.uk"], "title": "Multitask Soft Option Learning", "authors": ["Maximilian Igl", "Andrew Gambardella", "Jinke He", "Nantas Nardelli", "N. Siddharth", "Wendelin B\u00f6hmer", "Shimon Whiteson"], "pdf": "/pdf/5e7c12118369a575ab52f9fb553689c36468b080.pdf", "TL;DR": "In Hierarchical RL, we introduce the notion of a 'soft', i.e. adaptable, option and show that this helps learning in multitask settings.", "abstract": "We present Multitask Soft Option Learning (MSOL), a hierarchical multi-task framework based on Planning-as-Inference. MSOL extends the concept of Options, using separate variational posteriors for each task, regularized by a shared prior. The learned soft-options are temporally extended, allowing a higher-level master policy to train faster on new tasks by making decisions with lower frequency. Additionally, MSOL allows fine-tuning of soft-options for new tasks without unlearning previously useful behavior, and avoids problems with local minima in multitask training. We demonstrate empirically that MSOL significantly outperforms both hierarchical and flat transfer-learning baselines in challenging multi-task environments.", "keywords": ["Hierarchical Reinforcement Learning", "Reinforcement Learning", "Control as Inference", "Options", "Multitask Learning"], "paperhash": "igl|multitask_soft_option_learning", "original_pdf": "/attachment/9be82fd541efe5e82d10afd0fee469b328a0ebe5.pdf", "_bibtex": "@misc{\nigl2020multitask,\ntitle={Multitask Soft Option Learning},\nauthor={Maximilian Igl and Andrew Gambardella and Jinke He and Nantas Nardelli and N. Siddharth and Wendelin B{\\\"o}hmer and Shimon Whiteson},\nyear={2020},\nurl={https://openreview.net/forum?id=BkeDGJBKvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BkeDGJBKvB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1583/Authors", "ICLR.cc/2020/Conference/Paper1583/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1583/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1583/Reviewers", "ICLR.cc/2020/Conference/Paper1583/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1583/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1583/Authors|ICLR.cc/2020/Conference/Paper1583/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504153873, "tmdate": 1576860531308, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1583/Authors", "ICLR.cc/2020/Conference/Paper1583/Reviewers", "ICLR.cc/2020/Conference/Paper1583/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1583/-/Official_Comment"}}}, {"id": "HkgLRS7dir", "original": null, "number": 1, "cdate": 1573561806092, "ddate": null, "tcdate": 1573561806092, "tmdate": 1573561806092, "tddate": null, "forum": "BkeDGJBKvB", "replyto": "H1li36yCYS", "invitation": "ICLR.cc/2020/Conference/Paper1583/-/Official_Comment", "content": {"title": "Response to Review #2", "comment": "Thank you for your detailed review and feedback. We welcome any further comments and questions. \n\nScalability:\nWe choose to use one network per task, but this is not strictly necessary if one wanted to scale to more tasks. For example, in the non-hierarchical case with prior/posterior networks, Galashov et al. use just one posterior network, providing task information as additional input. Their findings could be applied to our hierarchical approach. \nFurthermore, we show that the derived loss function incentivises specialized options during training, i.e. options for which the prior and posterior are the same. Consequently, given sufficient options, eventually one can forget the posteriors as the option priors should be good enough to solve the tasks encountered so far. \n\nFairness of comparison:\nBoth MSOL and Distral use one network per task so this comparison is fair.\nWe also included MSOL(frozen) which, once the prior options are trained, only uses this one network without further adaptation for transfer, allowing a fair comparison to MLSH, which it still outperforms. \n\nEnvironmental complexity and additional baselines: \nOne key goal of MSOL is to find good (i.e. transferable) options and terminations without any additional human-designed prior knowledge regarding subgoals. This is challenging because during option-learning the algorithm not only needs to solve each task, but needs to find a way to segment tasks into reusable sub-tasks - all without additional human information. This is a main difference to many previous algorithms which receive additional information, for example in the form of landmarks, policy sketches or designated sub-tasks for each option. \nOur goal with the Swimmer environment was to show that our algorithm is applicable to continuous tasks. Applying the algorithm in practice to much more complex multi-task domains in MuJoCo was infeasible given our limited computational resources. Hierarchical approaches applied to such settings often receive additional prior information (like sub-tasks or goals per option as in Tessler et. al) or are not constrained to learn transferable options (like in Option Critic). \n\nOption initiation sets:\nWe agree that learning to restrict the initiation sets of options is an interesting avenue for future research, but believe that learning good options and termination functions, robustly, just from a set of tasks is already a significant enough contribution.\n \nIntra-option and termination learning: \nWe agree and have rephrased this sentence. We have also extended the related work section. The main difference to our work is that those approaches do not rely on a multitask setting to find good termination functions, but on other ideas, like landmarks, bottleneck states or predictability. \n\nTerm 1 being nonzero: \nWhen b_t=0, i.e. when we do not terminate, both the prior p^H and the posterior q^H are, by definition, delta(z_t-z_{t-1}), i.e. both assign a probability of 1 to the last active option. Consequently, in this case the fraction becomes one and we have for the term: beta * ln 1 = 0\nOn the other hand, if b_t=1, then the prior is uniform and the posterior is the learned posterior, leading in general to a non-zero term. \n\nComparison to Distral:\nFinal performance: We would like to note that because Distal, similarly to MSOL, learns a separate posterior policy which is regularized against the prior, it will always ultimately achieve optimal performance for weak enough regularization. So it is to be expected that both MSOL and Distral achieve the same final performance. \nOur experiments show two things:\nFor learning a hierarchy, we have a more robust optimization algorithm than MLSH\nLearning a hierarchy (compared to a flat prior as in Distral or compared to a heuristic hierarchy as in Distral+Action) is useful because it can accelerate learning.\n\nWe intentionally included the non-directional Taxi domain to show in which cases a simpler architecture (here Distral+Action) is sufficient for optimal transfer, so its strong performance is expected.\nThe main difference of this domain is that here, passing the last action is sufficiently informative to predict the likely future behavior. It is like walking in a corridor with only one door at either end: Knowing which direction we are walking in carries the same amount of information as knowing which door we want to walk towards. \nHowever, in directional Taxi, knowing the last action is not as informative (because it might involve a rotation) and in Moving Bandits, one could infer the intended goal from the last action if one takes the goal positions into account. However, we found that Distral+Action was unable to learn this more complex relationship. In those cases a learned hierarchy in which options carry learned semantics outperforms Distral+Action in terms of transfer speed.\n\nRelated literature:\nThank you for the pointers to additional literature. We have included them in the related work section.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1583/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1583/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["maximilian.igl@gmail.com", "gambs@robots.ox.ac.uk", "jinkehe1996@gmail.com", "nantas@robots.ox.ac.uk", "nsid@robots.ox.ac.uk", "wendelin.boehmer@cs.ox.ac.uk", "shimon.whiteson@cs.ox.ac.uk"], "title": "Multitask Soft Option Learning", "authors": ["Maximilian Igl", "Andrew Gambardella", "Jinke He", "Nantas Nardelli", "N. Siddharth", "Wendelin B\u00f6hmer", "Shimon Whiteson"], "pdf": "/pdf/5e7c12118369a575ab52f9fb553689c36468b080.pdf", "TL;DR": "In Hierarchical RL, we introduce the notion of a 'soft', i.e. adaptable, option and show that this helps learning in multitask settings.", "abstract": "We present Multitask Soft Option Learning (MSOL), a hierarchical multi-task framework based on Planning-as-Inference. MSOL extends the concept of Options, using separate variational posteriors for each task, regularized by a shared prior. The learned soft-options are temporally extended, allowing a higher-level master policy to train faster on new tasks by making decisions with lower frequency. Additionally, MSOL allows fine-tuning of soft-options for new tasks without unlearning previously useful behavior, and avoids problems with local minima in multitask training. We demonstrate empirically that MSOL significantly outperforms both hierarchical and flat transfer-learning baselines in challenging multi-task environments.", "keywords": ["Hierarchical Reinforcement Learning", "Reinforcement Learning", "Control as Inference", "Options", "Multitask Learning"], "paperhash": "igl|multitask_soft_option_learning", "original_pdf": "/attachment/9be82fd541efe5e82d10afd0fee469b328a0ebe5.pdf", "_bibtex": "@misc{\nigl2020multitask,\ntitle={Multitask Soft Option Learning},\nauthor={Maximilian Igl and Andrew Gambardella and Jinke He and Nantas Nardelli and N. Siddharth and Wendelin B{\\\"o}hmer and Shimon Whiteson},\nyear={2020},\nurl={https://openreview.net/forum?id=BkeDGJBKvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BkeDGJBKvB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1583/Authors", "ICLR.cc/2020/Conference/Paper1583/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1583/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1583/Reviewers", "ICLR.cc/2020/Conference/Paper1583/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1583/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1583/Authors|ICLR.cc/2020/Conference/Paper1583/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504153873, "tmdate": 1576860531308, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1583/Authors", "ICLR.cc/2020/Conference/Paper1583/Reviewers", "ICLR.cc/2020/Conference/Paper1583/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1583/-/Official_Comment"}}}, {"id": "S1xF8IXHtH", "original": null, "number": 1, "cdate": 1571268176893, "ddate": null, "tcdate": 1571268176893, "tmdate": 1572972449624, "tddate": null, "forum": "BkeDGJBKvB", "replyto": "BkeDGJBKvB", "invitation": "ICLR.cc/2020/Conference/Paper1583/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper is about learning hierarchical multitask policies over options.  The hierarchical prior policy is shared amongst all tasks, while the hierarchical posterior policies are allowed to adapt to specific tasks.  Once the prior is learned, it is fixed.   The parameters of the posterior policies are adjusted via an adapted version of A2C.  \n\nI liked the flow and the organization of this paper.\n\nComments and questions:\n* I see that term 1 and term 3 of equation (6) working together to ensure some kind of exploration and exploitation.  Term 2 controls how the option posterior deviates from the prior.  However, when the ratio is 1 or less than 1, the value of (6) would increase, and both cases would have made the posterior more like the prior.  There seems to be no other term that incentivizes the option posterior to deviate, and I do not see how the options are adapting to tasks.  \n\n* The term 1,2,3 in (6) are weighted equally by beta and cannot be fine-tuned to desired trade-offs.  \n\n* Should there be \\xi(i) multiplying the last term in (3)?  \n\n* What does delta(z_t - z_{t-1}) mean in section 3.1?  It was not defined anywhere.  \n\nMisspelling and typos:\n* page 5: optimized is misspelled in \"Details on how we optimiszed...\" \n\n* Appendix C.1: should there not be a superscript dash on A_{\\pi_i} since the superscript dash carries the meaning that the term is a constant.  "}, "signatures": ["ICLR.cc/2020/Conference/Paper1583/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1583/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["maximilian.igl@gmail.com", "gambs@robots.ox.ac.uk", "jinkehe1996@gmail.com", "nantas@robots.ox.ac.uk", "nsid@robots.ox.ac.uk", "wendelin.boehmer@cs.ox.ac.uk", "shimon.whiteson@cs.ox.ac.uk"], "title": "Multitask Soft Option Learning", "authors": ["Maximilian Igl", "Andrew Gambardella", "Jinke He", "Nantas Nardelli", "N. Siddharth", "Wendelin B\u00f6hmer", "Shimon Whiteson"], "pdf": "/pdf/5e7c12118369a575ab52f9fb553689c36468b080.pdf", "TL;DR": "In Hierarchical RL, we introduce the notion of a 'soft', i.e. adaptable, option and show that this helps learning in multitask settings.", "abstract": "We present Multitask Soft Option Learning (MSOL), a hierarchical multi-task framework based on Planning-as-Inference. MSOL extends the concept of Options, using separate variational posteriors for each task, regularized by a shared prior. The learned soft-options are temporally extended, allowing a higher-level master policy to train faster on new tasks by making decisions with lower frequency. Additionally, MSOL allows fine-tuning of soft-options for new tasks without unlearning previously useful behavior, and avoids problems with local minima in multitask training. We demonstrate empirically that MSOL significantly outperforms both hierarchical and flat transfer-learning baselines in challenging multi-task environments.", "keywords": ["Hierarchical Reinforcement Learning", "Reinforcement Learning", "Control as Inference", "Options", "Multitask Learning"], "paperhash": "igl|multitask_soft_option_learning", "original_pdf": "/attachment/9be82fd541efe5e82d10afd0fee469b328a0ebe5.pdf", "_bibtex": "@misc{\nigl2020multitask,\ntitle={Multitask Soft Option Learning},\nauthor={Maximilian Igl and Andrew Gambardella and Jinke He and Nantas Nardelli and N. Siddharth and Wendelin B{\\\"o}hmer and Shimon Whiteson},\nyear={2020},\nurl={https://openreview.net/forum?id=BkeDGJBKvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BkeDGJBKvB", "replyto": "BkeDGJBKvB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1583/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1583/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575441962409, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1583/Reviewers"], "noninvitees": [], "tcdate": 1570237735267, "tmdate": 1575441962420, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1583/-/Official_Review"}}}, {"id": "H1li36yCYS", "original": null, "number": 2, "cdate": 1571843507156, "ddate": null, "tcdate": 1571843507156, "tmdate": 1572972449582, "tddate": null, "forum": "BkeDGJBKvB", "replyto": "BkeDGJBKvB", "invitation": "ICLR.cc/2020/Conference/Paper1583/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary:\nThe authors propose a method for learning hierarchical policies in a multi-task setting built on options and casts the concepts into the Planning as Inference framework. They claim that the method brings several advantages such as:\n-Stabilize end-to-end mtl,\n-Leads to coordination b/w master policies,\n-Allows fine-tuning of options\n-Learn termination policies naturally\n\nThe proposed approach derives the reward Eq.6 by extending the graph to options for Levine\u2019s tutorial. Eq.6 is simply the extension of the reward of maximum entropy RL to the options framework. The ideas presented in the paper are interesting, but I have concerns about the scalability of such an approach. Please see the detailed comments below.  Additionally, please note that although I have marked a weak reject, I am open to adjusting my score if the rebuttal addresses enough issues.\n\nDetailed Comments:\nA primary weakness of this approach is that it seems like there is one network that learns the options and is shared across all task (that would be the prior) and then there is a task-specific network for all options (posterior), wouldn\u2019t this be very difficult to scale if we want to learn reusable options over the lifetime of an agent? If there are n tasks, do you need to use n different networks? \n\nThe authors assume that all options are present everywhere i.e. I \u2286 S. I think the work could benefit from removing this assumption.\n\nThe authors mention that unlike (Frans et al., 2018), they learn both intra-option and termination policies: there is definitely more work that aims to learn both the skill and its termination. It would be more complete to cite additional references here that learn both of these or rephrase this sentence. \n\nIt does not seem clear why \u201cterm 1 of the regularization is only nonzero whenever we terminate an option and the master policy needs to select another to execute.\u201d This doesn\u2019t seem true as this is a ratio of the two probabilities and not just the instantiation of the random variable. \n\nThe results in moving bandits alone are very convincing. However, in Taxi (2b) distral+action seems to be as good as/even better MSOL. In directional Taxi (2c) Distral(+action) manages to reach the same final performance (if we care about that), can you please comment on this.\n\nSome parts of the experiments section does not seem clear to me, Does the proposed approach use a network per task? if yes, then it is obvious that their method could improve over learning on 12 tasks with one set of network. Please clarify. \n\nOne major concern is that the only high dimensional experiment is a swimmer and it is not immediately clear how much do we gain there. Distral is relatively closer in performance to both MSOL and MSOL frozen. I would recommend evaluation in a variety of high-dimensional domains such as other instances in Mujoco, and visual domains. In particular, the proposed ideas would make a stronger case if the baselines included other multitask hierarchical agents such as [4] for example. A discussion including some of the missing relevant related multi-task literature would also be helpful [1,2,4,5,6].\n\n[1] Mann, T. A., Mannor, S., & Precup, D. (2015). Approximate value iteration with temporally extended actions. JAIR, 53, 375-438.\n[2] Konidaris, G., & Barto, A. G. (2007). Building Portable Options: Skill Transfer in Reinforcement Learning. In IJCAI,\n[3] Andreas, J., Klein, D., & Levine, S. (2017). Modular multitask reinforcement learning with policy sketches. ICML\n[4] Tessler, Chen, et al. \"A deep hierarchical approach to lifelong learning in minecraft.\" Thirty-First AAAI Conference on Artificial Intelligence. 2017.\n[5] Ammar, Haitham Bou, et al. \"Online multi-task learning for policy gradient methods.\" International Conference on Machine Learning. 2014.\n[6] Mankowitz, Daniel J., Timothy A. Mann, and Shie Mannor. \"Adaptive skills adaptive partitions (ASAP).\" Advances in Neural Information Processing Systems. 2016."}, "signatures": ["ICLR.cc/2020/Conference/Paper1583/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1583/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["maximilian.igl@gmail.com", "gambs@robots.ox.ac.uk", "jinkehe1996@gmail.com", "nantas@robots.ox.ac.uk", "nsid@robots.ox.ac.uk", "wendelin.boehmer@cs.ox.ac.uk", "shimon.whiteson@cs.ox.ac.uk"], "title": "Multitask Soft Option Learning", "authors": ["Maximilian Igl", "Andrew Gambardella", "Jinke He", "Nantas Nardelli", "N. Siddharth", "Wendelin B\u00f6hmer", "Shimon Whiteson"], "pdf": "/pdf/5e7c12118369a575ab52f9fb553689c36468b080.pdf", "TL;DR": "In Hierarchical RL, we introduce the notion of a 'soft', i.e. adaptable, option and show that this helps learning in multitask settings.", "abstract": "We present Multitask Soft Option Learning (MSOL), a hierarchical multi-task framework based on Planning-as-Inference. MSOL extends the concept of Options, using separate variational posteriors for each task, regularized by a shared prior. The learned soft-options are temporally extended, allowing a higher-level master policy to train faster on new tasks by making decisions with lower frequency. Additionally, MSOL allows fine-tuning of soft-options for new tasks without unlearning previously useful behavior, and avoids problems with local minima in multitask training. We demonstrate empirically that MSOL significantly outperforms both hierarchical and flat transfer-learning baselines in challenging multi-task environments.", "keywords": ["Hierarchical Reinforcement Learning", "Reinforcement Learning", "Control as Inference", "Options", "Multitask Learning"], "paperhash": "igl|multitask_soft_option_learning", "original_pdf": "/attachment/9be82fd541efe5e82d10afd0fee469b328a0ebe5.pdf", "_bibtex": "@misc{\nigl2020multitask,\ntitle={Multitask Soft Option Learning},\nauthor={Maximilian Igl and Andrew Gambardella and Jinke He and Nantas Nardelli and N. Siddharth and Wendelin B{\\\"o}hmer and Shimon Whiteson},\nyear={2020},\nurl={https://openreview.net/forum?id=BkeDGJBKvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BkeDGJBKvB", "replyto": "BkeDGJBKvB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1583/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1583/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575441962409, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1583/Reviewers"], "noninvitees": [], "tcdate": 1570237735267, "tmdate": 1575441962420, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1583/-/Official_Review"}}}], "count": 9}