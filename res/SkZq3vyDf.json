{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1528124429452, "tcdate": 1518464137084, "number": 232, "cdate": 1518464137084, "id": "SkZq3vyDf", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "SkZq3vyDf", "signatures": ["~Daniel_Fojo1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "Comparing Fixed and Adaptive Computation Time for Recurrent Neural Networks", "abstract": "Deep networks commonly perform better than shallow ones, but allocating the proper amount of computation for each particular input sample remains an open problem. This issue is particularly challenging in sequential tasks, where the required complexity may vary for different tokens in the input sequence. Adaptive Computation Time (ACT) was proposed as a method for dynamically adapting the computation at each step for Recurrent Neural Networks (RNN). ACT introduces two main modifications to the regular RNN formulation: (1) more than one RNN steps may be executed between an input sample is fed to the layer and and this layer generates an output,  and (2) this number of steps is dynamically predicted depending on the input token and the hidden state of the network. In our work, we aim at gaining intuition about the contribution of these two factors to the overall performance boost observed when augmenting RNNs with ACT. We design a new baseline, Repeat-RNN, which performs a constant number of RNN state updates larger than one before generating an output. Surprisingly, such uniform distribution of the computational resources matches the performance of ACT in the studied tasks. We hope that this finding motivates new research efforts towards designing RNN architectures that are able to dynamically allocate computational resources.", "paperhash": "fojo|comparing_fixed_and_adaptive_computation_time_for_recurrent_neural_networks", "keywords": ["Neural Network", "Recurrent Neural Network", "RNN", "Adaptive Computation Time", "ACT", "Variable Computation"], "_bibtex": "@misc{\n  fojo2018comparing,\n  title={Comparing Fixed and Adaptive Computation Time for Recurrent Neural Networks},\n  author={Daniel Fojo and V\u00edctor Campos and Xavier Gir\u00f3-i-Nieto},\n  year={2018},\n  url={https://openreview.net/forum?id=SkZq3vyDf}\n}", "authorids": ["daniel.fojo@estudiant.upc.edu", "victor.campos@bsc.es", "xavier.giro@upc.edu"], "authors": ["Daniel Fojo", "V\u00edctor Campos", "Xavier Gir\u00f3-i-Nieto"], "TL;DR": "Comparison of Adaptive Computation Time with Fixed computation time for RNN's gives surprising results.", "pdf": "/pdf/50016666bbc236b19e5962593238f4fa4d620d0e.pdf"}, "nonreaders": [], "details": {"replyCount": 6, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582832034, "tcdate": 1520603632113, "number": 1, "cdate": 1520603632113, "id": "HyOgMGgYM", "invitation": "ICLR.cc/2018/Workshop/-/Paper232/Official_Review", "forum": "SkZq3vyDf", "replyto": "SkZq3vyDf", "signatures": ["ICLR.cc/2018/Workshop/Paper232/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper232/AnonReviewer1"], "content": {"title": "nice ablation test, more tasks would be great", "rating": "7: Good paper, accept", "review": "The paper performs an ablation test for the Adaptive Computation Time model (ACT). Instead of learning to predict how many steps of computation are necessary for each input token, the proposed model (Repeat-RNN) performs a fixed number of steps. This number of steps is treated as a hyperparameter. Repeat-RNN is evaluated on two tasks from the ACT paper (parity and addition) and it is shown that it is better or not worse than in ACT in terms of both training and inference time. \n\nThe paper is clearly written, and I think that it makes a valuable contribution by  showing how a simple method can be just as good as a fancy one. I think we need such papers at the ICLR workshop.\n\nA few suggestions for the authors:\n- There was 5 tasks in the original ACT paper, and so far you only evaluated on 2. Would be great if you tried others as well.\n- Consider citing https://arxiv.org/abs/1312.6026\n- There is a bit of confusion in the text between notions of \"input sample\" and \"input token\", see e.g. item (1) in the intro\n- item (1) in the intro has broken grammar\n\n\n\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Comparing Fixed and Adaptive Computation Time for Recurrent Neural Networks", "abstract": "Deep networks commonly perform better than shallow ones, but allocating the proper amount of computation for each particular input sample remains an open problem. This issue is particularly challenging in sequential tasks, where the required complexity may vary for different tokens in the input sequence. Adaptive Computation Time (ACT) was proposed as a method for dynamically adapting the computation at each step for Recurrent Neural Networks (RNN). ACT introduces two main modifications to the regular RNN formulation: (1) more than one RNN steps may be executed between an input sample is fed to the layer and and this layer generates an output,  and (2) this number of steps is dynamically predicted depending on the input token and the hidden state of the network. In our work, we aim at gaining intuition about the contribution of these two factors to the overall performance boost observed when augmenting RNNs with ACT. We design a new baseline, Repeat-RNN, which performs a constant number of RNN state updates larger than one before generating an output. Surprisingly, such uniform distribution of the computational resources matches the performance of ACT in the studied tasks. We hope that this finding motivates new research efforts towards designing RNN architectures that are able to dynamically allocate computational resources.", "paperhash": "fojo|comparing_fixed_and_adaptive_computation_time_for_recurrent_neural_networks", "keywords": ["Neural Network", "Recurrent Neural Network", "RNN", "Adaptive Computation Time", "ACT", "Variable Computation"], "_bibtex": "@misc{\n  fojo2018comparing,\n  title={Comparing Fixed and Adaptive Computation Time for Recurrent Neural Networks},\n  author={Daniel Fojo and V\u00edctor Campos and Xavier Gir\u00f3-i-Nieto},\n  year={2018},\n  url={https://openreview.net/forum?id=SkZq3vyDf}\n}", "authorids": ["daniel.fojo@estudiant.upc.edu", "victor.campos@bsc.es", "xavier.giro@upc.edu"], "authors": ["Daniel Fojo", "V\u00edctor Campos", "Xavier Gir\u00f3-i-Nieto"], "TL;DR": "Comparison of Adaptive Computation Time with Fixed computation time for RNN's gives surprising results.", "pdf": "/pdf/50016666bbc236b19e5962593238f4fa4d620d0e.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582831802, "id": "ICLR.cc/2018/Workshop/-/Paper232/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper232/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper232/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper232/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper232/AnonReviewer3"], "reply": {"forum": "SkZq3vyDf", "replyto": "SkZq3vyDf", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper232/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper232/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582831802}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582726667, "tcdate": 1520674190443, "number": 2, "cdate": 1520674190443, "id": "ByI9HXWtf", "invitation": "ICLR.cc/2018/Workshop/-/Paper232/Official_Review", "forum": "SkZq3vyDf", "replyto": "SkZq3vyDf", "signatures": ["ICLR.cc/2018/Workshop/Paper232/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper232/AnonReviewer2"], "content": {"title": "Small demonstration that the important part of ACT is simply more steps.", "rating": "5: Marginally below acceptance threshold", "review": "The paper presents an ablation on the ACT (Graves 2016) architecture in which a constant number of ponder steps is performed between RNN steps. As in the original ACT, the ponder steps share weights.\n\nEvaluation is done on two tasks from the original ACT paper - parity and addition, both tasks are not solved using a regular LSTM and require intermediate computation between RNN steps.\n\nThe idea of simply making the transition of an LSTM network more powerful is not new, as both adding more LSTM layers and more transitions (possibly with weight tying) was reported e.g. in http://proceedings.mlr.press/v70/zilly17a/zilly17a.pdf. \n\nPros:\n- Very simple architecture\n\nCons:\n- Limited novelty of the approach\n- Lack of comparison with other approaches that add more compute steps, such as adding depth or the recurrent highway RNNs\n\n", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Comparing Fixed and Adaptive Computation Time for Recurrent Neural Networks", "abstract": "Deep networks commonly perform better than shallow ones, but allocating the proper amount of computation for each particular input sample remains an open problem. This issue is particularly challenging in sequential tasks, where the required complexity may vary for different tokens in the input sequence. Adaptive Computation Time (ACT) was proposed as a method for dynamically adapting the computation at each step for Recurrent Neural Networks (RNN). ACT introduces two main modifications to the regular RNN formulation: (1) more than one RNN steps may be executed between an input sample is fed to the layer and and this layer generates an output,  and (2) this number of steps is dynamically predicted depending on the input token and the hidden state of the network. In our work, we aim at gaining intuition about the contribution of these two factors to the overall performance boost observed when augmenting RNNs with ACT. We design a new baseline, Repeat-RNN, which performs a constant number of RNN state updates larger than one before generating an output. Surprisingly, such uniform distribution of the computational resources matches the performance of ACT in the studied tasks. We hope that this finding motivates new research efforts towards designing RNN architectures that are able to dynamically allocate computational resources.", "paperhash": "fojo|comparing_fixed_and_adaptive_computation_time_for_recurrent_neural_networks", "keywords": ["Neural Network", "Recurrent Neural Network", "RNN", "Adaptive Computation Time", "ACT", "Variable Computation"], "_bibtex": "@misc{\n  fojo2018comparing,\n  title={Comparing Fixed and Adaptive Computation Time for Recurrent Neural Networks},\n  author={Daniel Fojo and V\u00edctor Campos and Xavier Gir\u00f3-i-Nieto},\n  year={2018},\n  url={https://openreview.net/forum?id=SkZq3vyDf}\n}", "authorids": ["daniel.fojo@estudiant.upc.edu", "victor.campos@bsc.es", "xavier.giro@upc.edu"], "authors": ["Daniel Fojo", "V\u00edctor Campos", "Xavier Gir\u00f3-i-Nieto"], "TL;DR": "Comparison of Adaptive Computation Time with Fixed computation time for RNN's gives surprising results.", "pdf": "/pdf/50016666bbc236b19e5962593238f4fa4d620d0e.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582831802, "id": "ICLR.cc/2018/Workshop/-/Paper232/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper232/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper232/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper232/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper232/AnonReviewer3"], "reply": {"forum": "SkZq3vyDf", "replyto": "SkZq3vyDf", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper232/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper232/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582831802}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582611737, "tcdate": 1520878728344, "number": 3, "cdate": 1520878728344, "id": "Hye5ESVKz", "invitation": "ICLR.cc/2018/Workshop/-/Paper232/Official_Review", "forum": "SkZq3vyDf", "replyto": "SkZq3vyDf", "signatures": ["ICLR.cc/2018/Workshop/Paper232/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper232/AnonReviewer3"], "content": {"title": "Ok, but more depth would be great", "rating": "5: Marginally below acceptance threshold", "review": "Your paper attempts to compare the speed and accuracy of an RNN when performing multiple recurrent steps for each input vs an adaptive number of steps for each input (ACT). This is generally OK but really not incredibly novel or difficult although some will find these limited results somewhat interesting -- however, many more experimental and theoretical directions could have been explored for a workshop paper. \n\nComments:  \n- Your title claims to compare fixed and adaptive computation time, however I don't see a single graph or table comparing computation times in sec, min etc.? Why not?\n- The fact that running a fixed number of steps instead of an adaptive number of steps (which I think summarizes your contribution) and getting the same or better accuracy is to me (and I believe to many others) not surprising since you have more freedom when running more recurrent steps on average.\n- I am surprised to read that updating the state repeatedly gives better accuracy remains an open question -- you are technically running through more layers which improves the modeling capacity, this is not really an open question.\n- The E-Mail addresses and some of the graphs seem to go over the boundary of where text is allowed -- that probably has to be fixed.\n   ", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Comparing Fixed and Adaptive Computation Time for Recurrent Neural Networks", "abstract": "Deep networks commonly perform better than shallow ones, but allocating the proper amount of computation for each particular input sample remains an open problem. This issue is particularly challenging in sequential tasks, where the required complexity may vary for different tokens in the input sequence. Adaptive Computation Time (ACT) was proposed as a method for dynamically adapting the computation at each step for Recurrent Neural Networks (RNN). ACT introduces two main modifications to the regular RNN formulation: (1) more than one RNN steps may be executed between an input sample is fed to the layer and and this layer generates an output,  and (2) this number of steps is dynamically predicted depending on the input token and the hidden state of the network. In our work, we aim at gaining intuition about the contribution of these two factors to the overall performance boost observed when augmenting RNNs with ACT. We design a new baseline, Repeat-RNN, which performs a constant number of RNN state updates larger than one before generating an output. Surprisingly, such uniform distribution of the computational resources matches the performance of ACT in the studied tasks. We hope that this finding motivates new research efforts towards designing RNN architectures that are able to dynamically allocate computational resources.", "paperhash": "fojo|comparing_fixed_and_adaptive_computation_time_for_recurrent_neural_networks", "keywords": ["Neural Network", "Recurrent Neural Network", "RNN", "Adaptive Computation Time", "ACT", "Variable Computation"], "_bibtex": "@misc{\n  fojo2018comparing,\n  title={Comparing Fixed and Adaptive Computation Time for Recurrent Neural Networks},\n  author={Daniel Fojo and V\u00edctor Campos and Xavier Gir\u00f3-i-Nieto},\n  year={2018},\n  url={https://openreview.net/forum?id=SkZq3vyDf}\n}", "authorids": ["daniel.fojo@estudiant.upc.edu", "victor.campos@bsc.es", "xavier.giro@upc.edu"], "authors": ["Daniel Fojo", "V\u00edctor Campos", "Xavier Gir\u00f3-i-Nieto"], "TL;DR": "Comparison of Adaptive Computation Time with Fixed computation time for RNN's gives surprising results.", "pdf": "/pdf/50016666bbc236b19e5962593238f4fa4d620d0e.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582831802, "id": "ICLR.cc/2018/Workshop/-/Paper232/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper232/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper232/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper232/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper232/AnonReviewer3"], "reply": {"forum": "SkZq3vyDf", "replyto": "SkZq3vyDf", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper232/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper232/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582831802}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521573580881, "tcdate": 1521573580881, "number": 161, "cdate": 1521573580538, "id": "HkS00CCtG", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "SkZq3vyDf", "replyto": "SkZq3vyDf", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Accept", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "Congratulations, your paper was accepted to the ICLR workshop."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Comparing Fixed and Adaptive Computation Time for Recurrent Neural Networks", "abstract": "Deep networks commonly perform better than shallow ones, but allocating the proper amount of computation for each particular input sample remains an open problem. This issue is particularly challenging in sequential tasks, where the required complexity may vary for different tokens in the input sequence. Adaptive Computation Time (ACT) was proposed as a method for dynamically adapting the computation at each step for Recurrent Neural Networks (RNN). ACT introduces two main modifications to the regular RNN formulation: (1) more than one RNN steps may be executed between an input sample is fed to the layer and and this layer generates an output,  and (2) this number of steps is dynamically predicted depending on the input token and the hidden state of the network. In our work, we aim at gaining intuition about the contribution of these two factors to the overall performance boost observed when augmenting RNNs with ACT. We design a new baseline, Repeat-RNN, which performs a constant number of RNN state updates larger than one before generating an output. Surprisingly, such uniform distribution of the computational resources matches the performance of ACT in the studied tasks. We hope that this finding motivates new research efforts towards designing RNN architectures that are able to dynamically allocate computational resources.", "paperhash": "fojo|comparing_fixed_and_adaptive_computation_time_for_recurrent_neural_networks", "keywords": ["Neural Network", "Recurrent Neural Network", "RNN", "Adaptive Computation Time", "ACT", "Variable Computation"], "_bibtex": "@misc{\n  fojo2018comparing,\n  title={Comparing Fixed and Adaptive Computation Time for Recurrent Neural Networks},\n  author={Daniel Fojo and V\u00edctor Campos and Xavier Gir\u00f3-i-Nieto},\n  year={2018},\n  url={https://openreview.net/forum?id=SkZq3vyDf}\n}", "authorids": ["daniel.fojo@estudiant.upc.edu", "victor.campos@bsc.es", "xavier.giro@upc.edu"], "authors": ["Daniel Fojo", "V\u00edctor Campos", "Xavier Gir\u00f3-i-Nieto"], "TL;DR": "Comparison of Adaptive Computation Time with Fixed computation time for RNN's gives surprising results.", "pdf": "/pdf/50016666bbc236b19e5962593238f4fa4d620d0e.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}, {"tddate": null, "ddate": null, "tmdate": 1518879259979, "tcdate": 1518879259979, "number": 2, "cdate": 1518879259979, "id": "Bk47fpHwM", "invitation": "ICLR.cc/2018/Workshop/-/Paper232/Public_Comment", "forum": "SkZq3vyDf", "replyto": "HJTjVsrPf", "signatures": ["~Daniel_Fojo1"], "readers": ["everyone"], "writers": ["~Daniel_Fojo1"], "content": {"title": "Re: Formatting Violation", "comment": "We have sent the fixed version to the email, thank you."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Comparing Fixed and Adaptive Computation Time for Recurrent Neural Networks", "abstract": "Deep networks commonly perform better than shallow ones, but allocating the proper amount of computation for each particular input sample remains an open problem. This issue is particularly challenging in sequential tasks, where the required complexity may vary for different tokens in the input sequence. Adaptive Computation Time (ACT) was proposed as a method for dynamically adapting the computation at each step for Recurrent Neural Networks (RNN). ACT introduces two main modifications to the regular RNN formulation: (1) more than one RNN steps may be executed between an input sample is fed to the layer and and this layer generates an output,  and (2) this number of steps is dynamically predicted depending on the input token and the hidden state of the network. In our work, we aim at gaining intuition about the contribution of these two factors to the overall performance boost observed when augmenting RNNs with ACT. We design a new baseline, Repeat-RNN, which performs a constant number of RNN state updates larger than one before generating an output. Surprisingly, such uniform distribution of the computational resources matches the performance of ACT in the studied tasks. We hope that this finding motivates new research efforts towards designing RNN architectures that are able to dynamically allocate computational resources.", "paperhash": "fojo|comparing_fixed_and_adaptive_computation_time_for_recurrent_neural_networks", "keywords": ["Neural Network", "Recurrent Neural Network", "RNN", "Adaptive Computation Time", "ACT", "Variable Computation"], "_bibtex": "@misc{\n  fojo2018comparing,\n  title={Comparing Fixed and Adaptive Computation Time for Recurrent Neural Networks},\n  author={Daniel Fojo and V\u00edctor Campos and Xavier Gir\u00f3-i-Nieto},\n  year={2018},\n  url={https://openreview.net/forum?id=SkZq3vyDf}\n}", "authorids": ["daniel.fojo@estudiant.upc.edu", "victor.campos@bsc.es", "xavier.giro@upc.edu"], "authors": ["Daniel Fojo", "V\u00edctor Campos", "Xavier Gir\u00f3-i-Nieto"], "TL;DR": "Comparison of Adaptive Computation Time with Fixed computation time for RNN's gives surprising results.", "pdf": "/pdf/50016666bbc236b19e5962593238f4fa4d620d0e.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518712624521, "id": "ICLR.cc/2018/Workshop/-/Paper232/Public_Comment", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper232/Reviewers"], "reply": {"replyto": null, "forum": "SkZq3vyDf", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1518712624521}}}, {"tddate": null, "ddate": null, "tmdate": 1518871717937, "tcdate": 1518871717937, "number": 1, "cdate": 1518871717937, "id": "HJTjVsrPf", "invitation": "ICLR.cc/2018/Workshop/-/Paper232/Public_Comment", "forum": "SkZq3vyDf", "replyto": "SkZq3vyDf", "signatures": ["~Oriol_Vinyals1"], "readers": ["everyone"], "writers": ["~Oriol_Vinyals1"], "content": {"title": "Formatting Violation", "comment": "Your paper violates the formatting posted here https://iclr.cc/Conferences/2018/CallForWorkshops. Please send us a fixed version of your PDF at iclr2018.programchairs@gmail.com by the end of Monday, February 19th, or else we will reject your paper.\n\nThanks,\nICLR2018 Program Chairs"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Comparing Fixed and Adaptive Computation Time for Recurrent Neural Networks", "abstract": "Deep networks commonly perform better than shallow ones, but allocating the proper amount of computation for each particular input sample remains an open problem. This issue is particularly challenging in sequential tasks, where the required complexity may vary for different tokens in the input sequence. Adaptive Computation Time (ACT) was proposed as a method for dynamically adapting the computation at each step for Recurrent Neural Networks (RNN). ACT introduces two main modifications to the regular RNN formulation: (1) more than one RNN steps may be executed between an input sample is fed to the layer and and this layer generates an output,  and (2) this number of steps is dynamically predicted depending on the input token and the hidden state of the network. In our work, we aim at gaining intuition about the contribution of these two factors to the overall performance boost observed when augmenting RNNs with ACT. We design a new baseline, Repeat-RNN, which performs a constant number of RNN state updates larger than one before generating an output. Surprisingly, such uniform distribution of the computational resources matches the performance of ACT in the studied tasks. We hope that this finding motivates new research efforts towards designing RNN architectures that are able to dynamically allocate computational resources.", "paperhash": "fojo|comparing_fixed_and_adaptive_computation_time_for_recurrent_neural_networks", "keywords": ["Neural Network", "Recurrent Neural Network", "RNN", "Adaptive Computation Time", "ACT", "Variable Computation"], "_bibtex": "@misc{\n  fojo2018comparing,\n  title={Comparing Fixed and Adaptive Computation Time for Recurrent Neural Networks},\n  author={Daniel Fojo and V\u00edctor Campos and Xavier Gir\u00f3-i-Nieto},\n  year={2018},\n  url={https://openreview.net/forum?id=SkZq3vyDf}\n}", "authorids": ["daniel.fojo@estudiant.upc.edu", "victor.campos@bsc.es", "xavier.giro@upc.edu"], "authors": ["Daniel Fojo", "V\u00edctor Campos", "Xavier Gir\u00f3-i-Nieto"], "TL;DR": "Comparison of Adaptive Computation Time with Fixed computation time for RNN's gives surprising results.", "pdf": "/pdf/50016666bbc236b19e5962593238f4fa4d620d0e.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518712624521, "id": "ICLR.cc/2018/Workshop/-/Paper232/Public_Comment", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper232/Reviewers"], "reply": {"replyto": null, "forum": "SkZq3vyDf", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1518712624521}}}], "count": 7}