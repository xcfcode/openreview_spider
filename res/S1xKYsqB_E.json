{"notes": [{"id": "S1xKYsqB_E", "original": "B1g7gH5rOV", "number": 63, "cdate": 1553472385170, "ddate": null, "tcdate": 1553472385170, "tmdate": 1562082111869, "tddate": null, "forum": "S1xKYsqB_E", "replyto": null, "invitation": "ICLR.cc/2019/Workshop/LLD/-/Blind_Submission", "content": {"title": "Learning Spatial Common Sense with Geometry-Aware Recurrent Networks", "authors": ["Hsiao-Yu Tung", "Ricson Cheng", "Katerina Fragkiadaki"], "authorids": ["htung@cs.cmu.edu", "ricsonc@andrew.cmu.edu", "katef@cs.cmu.edu"], "keywords": [], "abstract": "We integrate two powerful ideas, geometry and deep visual representation learning, into recurrent network architectures for mobile visual scene understanding. The proposed networks learn to \u201clift\u201d 2D visual features and integrate them over time into latent 3D feature maps of the scene.   They are equipped with differentiable geometric operations,  such as projection,  unprojection,  egomotion stabilization, in order to compute a geometrically-consistent mapping between the world scene and their 3D latent feature space.  We train the proposed architectures to predict novel image views given short frame sequences as input.  Their predictions strongly generalize to scenes with a novel number of objects, appearances and configurations, and greatly outperform predictions of previous works that do not consider egomotion stabilization or a space-aware latent feature space. Our experiments suggest the proposed space-aware latent feature arrangement and egomotion-stabilized convolutions are essential architectural choices for spatial common sense to emerge in artificial embodied visual agents.", "pdf": "/pdf/452c5bfdc9427ffdc90309110ecdd6bc39b68ddd.pdf", "paperhash": "tung|learning_spatial_common_sense_with_geometryaware_recurrent_networks"}, "signatures": ["ICLR.cc/2019/Workshop/LLD"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/LLD"], "details": {"replyCount": 3, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/LLD/-/Blind_Submission", "cdate": 1548689671889, "reply": {"forum": null, "replyto": null, "readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2019/Workshop/LLD"]}, "signatures": {"values": ["ICLR.cc/2019/Workshop/LLD"]}, "content": {"authors": {"values-regex": ".*"}, "authorids": {"values-regex": ".*"}}}, "tcdate": 1548689671889, "tmdate": 1557933709646, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/LLD"], "invitees": ["~"], "signatures": ["ICLR.cc/2019/Workshop/LLD"], "details": {"writable": true}}}, "tauthor": "OpenReview.net"}, {"id": "rJgwL3sDKN", "original": null, "number": 1, "cdate": 1554656334990, "ddate": null, "tcdate": 1554656334990, "tmdate": 1555512019345, "tddate": null, "forum": "S1xKYsqB_E", "replyto": "S1xKYsqB_E", "invitation": "ICLR.cc/2019/Workshop/LLD/-/Paper63/Official_Review", "content": {"title": "Good design, great results", "review": "Summary:\nThe authors design an architecture for building a geometry-aware latent representation of a scene, and test this by querying a view from the latent representation and comparing it with the actual view.\n\nThe paper is structured well, it is clear on what the authors would like to say. Recently geometry-aware architectures have gained importance, and the task for which the authors have shown results is promising.\n\nThe authors designed a geometry-aware deep neural network, and trained it on multiple (but limited) viewpoints of multiple scenes to make a 3D latent representation of each scene. They then query a novel viewpoint of each scene and make a prediction loss on that to train the network in a self-supervised manner.\n\nAlthough it is \"self-supervised\", it is not clear whether the \"novel\" viewpoints used while training are within the training set, because if they are not, then the training data is actually as big as the number of iterations used to train. It would be helpful if the authors could clarify on this.\n\nThe authors show that their architecture is able to give quality results for scenes with more objects than those on which the network was trained, which provides evidence to the generalizability of the network. This is a very good result, provided we are clear on the \"limited\"-ness of the training data.\n\nThe authors have compared their results with those of another recent architecture that tried to tackle the same problem. The results seem to be in favour of this paper, especially in the case of more objects in the scene. It is worth noting that the other method was not geometry-aware by design, as this paper is.", "rating": "4: Top 50% of accepted papers, clear accept", "confidence": "2: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Workshop/LLD/Paper63/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/LLD/Paper63/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Spatial Common Sense with Geometry-Aware Recurrent Networks", "authors": ["Hsiao-Yu Tung", "Ricson Cheng", "Katerina Fragkiadaki"], "authorids": ["htung@cs.cmu.edu", "ricsonc@andrew.cmu.edu", "katef@cs.cmu.edu"], "keywords": [], "abstract": "We integrate two powerful ideas, geometry and deep visual representation learning, into recurrent network architectures for mobile visual scene understanding. The proposed networks learn to \u201clift\u201d 2D visual features and integrate them over time into latent 3D feature maps of the scene.   They are equipped with differentiable geometric operations,  such as projection,  unprojection,  egomotion stabilization, in order to compute a geometrically-consistent mapping between the world scene and their 3D latent feature space.  We train the proposed architectures to predict novel image views given short frame sequences as input.  Their predictions strongly generalize to scenes with a novel number of objects, appearances and configurations, and greatly outperform predictions of previous works that do not consider egomotion stabilization or a space-aware latent feature space. Our experiments suggest the proposed space-aware latent feature arrangement and egomotion-stabilized convolutions are essential architectural choices for spatial common sense to emerge in artificial embodied visual agents.", "pdf": "/pdf/452c5bfdc9427ffdc90309110ecdd6bc39b68ddd.pdf", "paperhash": "tung|learning_spatial_common_sense_with_geometryaware_recurrent_networks"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/LLD/-/Paper63/Official_Review", "cdate": 1553713410908, "expdate": 1555718400000, "duedate": 1554681600000, "reply": {"forum": "S1xKYsqB_E", "replyto": "S1xKYsqB_E", "writers": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2019/Workshop/LLD/Paper63/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2019/Workshop/LLD/Paper63/AnonReviewer[0-9]+"}, "readers": {"values": ["everyone"], "description": "The users who will be allowed to read the above content."}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["5: Top 15% of accepted papers, strong accept", "4: Top 50% of accepted papers, clear accept", "3: Marginally above acceptance threshold", "2: Marginally below acceptance threshold", "1: Strong rejection"], "required": true}, "confidence": {"order": 4, "value-radio": ["3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "2: The reviewer is fairly confident that the evaluation is correct", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "tcdate": 1553713410908, "tmdate": 1555511821077, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/LLD"], "invitees": ["ICLR.cc/2019/Workshop/LLD/Paper63/Reviewers"], "signatures": ["ICLR.cc/2019/Workshop/LLD"]}}}, {"id": "HyeSRogOYV", "original": null, "number": 2, "cdate": 1554676684749, "ddate": null, "tcdate": 1554676684749, "tmdate": 1555511888393, "tddate": null, "forum": "S1xKYsqB_E", "replyto": "S1xKYsqB_E", "invitation": "ICLR.cc/2019/Workshop/LLD/-/Paper63/Official_Review", "content": {"title": "Article evaluates an embedding of 2d-images that has learnt to predict the change in viewpoint (Rotation+translation) of 3D surfaces. The learning is done on synthetic 3D datasets.", "review": "Interesting article on evaluating camera's egoview based embedding of images into a 3D latent space that can reconstruct the 3D viewpoint of surfaces in the scene. Article quite short and dense and difficult to understand all the details.\n\n- How does the 3D-Unet evaluate the Ego-motion of the camera viewpoint ? Given the article is short, explanation on the architecture are quite fundamental.\n- Does the 3D latent space resemble any shape or surface ? Or is the latent space representation abstract ?\n- How are the intrinsic and extrinsic parameters of the camera evaluated in this setup ? We require these parameters to be estimated when evaluating a homography between two views. \n- Though the number of labels during training required are small, we still require a rich set of 3D object datsets and different viewpoints generated from them. Can you provide an idea of how this architecture generalizes to new classes and shapes of objects in images.\n- An idea of memory consumption for such architectures would be quite useful. ", "rating": "4: Top 50% of accepted papers, clear accept", "confidence": "2: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Workshop/LLD/Paper63/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/LLD/Paper63/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Spatial Common Sense with Geometry-Aware Recurrent Networks", "authors": ["Hsiao-Yu Tung", "Ricson Cheng", "Katerina Fragkiadaki"], "authorids": ["htung@cs.cmu.edu", "ricsonc@andrew.cmu.edu", "katef@cs.cmu.edu"], "keywords": [], "abstract": "We integrate two powerful ideas, geometry and deep visual representation learning, into recurrent network architectures for mobile visual scene understanding. The proposed networks learn to \u201clift\u201d 2D visual features and integrate them over time into latent 3D feature maps of the scene.   They are equipped with differentiable geometric operations,  such as projection,  unprojection,  egomotion stabilization, in order to compute a geometrically-consistent mapping between the world scene and their 3D latent feature space.  We train the proposed architectures to predict novel image views given short frame sequences as input.  Their predictions strongly generalize to scenes with a novel number of objects, appearances and configurations, and greatly outperform predictions of previous works that do not consider egomotion stabilization or a space-aware latent feature space. Our experiments suggest the proposed space-aware latent feature arrangement and egomotion-stabilized convolutions are essential architectural choices for spatial common sense to emerge in artificial embodied visual agents.", "pdf": "/pdf/452c5bfdc9427ffdc90309110ecdd6bc39b68ddd.pdf", "paperhash": "tung|learning_spatial_common_sense_with_geometryaware_recurrent_networks"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/LLD/-/Paper63/Official_Review", "cdate": 1553713410908, "expdate": 1555718400000, "duedate": 1554681600000, "reply": {"forum": "S1xKYsqB_E", "replyto": "S1xKYsqB_E", "writers": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2019/Workshop/LLD/Paper63/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2019/Workshop/LLD/Paper63/AnonReviewer[0-9]+"}, "readers": {"values": ["everyone"], "description": "The users who will be allowed to read the above content."}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["5: Top 15% of accepted papers, strong accept", "4: Top 50% of accepted papers, clear accept", "3: Marginally above acceptance threshold", "2: Marginally below acceptance threshold", "1: Strong rejection"], "required": true}, "confidence": {"order": 4, "value-radio": ["3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "2: The reviewer is fairly confident that the evaluation is correct", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "tcdate": 1553713410908, "tmdate": 1555511821077, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/LLD"], "invitees": ["ICLR.cc/2019/Workshop/LLD/Paper63/Reviewers"], "signatures": ["ICLR.cc/2019/Workshop/LLD"]}}}, {"id": "S1xKyLyKKE", "original": null, "number": 1, "cdate": 1554736608943, "ddate": null, "tcdate": 1554736608943, "tmdate": 1555510988034, "tddate": null, "forum": "S1xKYsqB_E", "replyto": "S1xKYsqB_E", "invitation": "ICLR.cc/2019/Workshop/LLD/-/Paper63/Decision", "content": {"title": "Acceptance Decision", "decision": "Accept"}, "signatures": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Spatial Common Sense with Geometry-Aware Recurrent Networks", "authors": ["Hsiao-Yu Tung", "Ricson Cheng", "Katerina Fragkiadaki"], "authorids": ["htung@cs.cmu.edu", "ricsonc@andrew.cmu.edu", "katef@cs.cmu.edu"], "keywords": [], "abstract": "We integrate two powerful ideas, geometry and deep visual representation learning, into recurrent network architectures for mobile visual scene understanding. The proposed networks learn to \u201clift\u201d 2D visual features and integrate them over time into latent 3D feature maps of the scene.   They are equipped with differentiable geometric operations,  such as projection,  unprojection,  egomotion stabilization, in order to compute a geometrically-consistent mapping between the world scene and their 3D latent feature space.  We train the proposed architectures to predict novel image views given short frame sequences as input.  Their predictions strongly generalize to scenes with a novel number of objects, appearances and configurations, and greatly outperform predictions of previous works that do not consider egomotion stabilization or a space-aware latent feature space. Our experiments suggest the proposed space-aware latent feature arrangement and egomotion-stabilized convolutions are essential architectural choices for spatial common sense to emerge in artificial embodied visual agents.", "pdf": "/pdf/452c5bfdc9427ffdc90309110ecdd6bc39b68ddd.pdf", "paperhash": "tung|learning_spatial_common_sense_with_geometryaware_recurrent_networks"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/LLD/-/Paper63/Decision", "cdate": 1554736072372, "reply": {"forum": "S1xKYsqB_E", "replyto": "S1xKYsqB_E", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-regex": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "description": "How your identity will be displayed."}, "signatures": {"values": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"title": {"order": 1, "required": true, "value": "Acceptance Decision"}, "decision": {"order": 2, "required": true, "value-radio": ["Accept", "Reject"], "description": "Acceptance decision"}, "comment": {"order": 3, "required": false, "value-regex": "[\\S\\s]{0,5000}", "description": ""}}}, "tcdate": 1554736072372, "tmdate": 1555510966445, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/LLD"], "invitees": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "signatures": ["ICLR.cc/2019/Workshop/LLD"]}}}], "count": 4}