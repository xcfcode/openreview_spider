{"notes": [{"id": "Byl_ciRcY7", "original": "rJg6lJ7tYQ", "number": 549, "cdate": 1538087824341, "ddate": null, "tcdate": 1538087824341, "tmdate": 1545355440429, "tddate": null, "forum": "Byl_ciRcY7", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "ON BREIMAN\u2019S DILEMMA IN NEURAL NETWORKS: SUCCESS AND FAILURE OF NORMALIZED MARGINS", "abstract": "A belief persists long in machine learning that enlargement of margins over training data accounts for the resistance of models to overfitting by increasing the robustness. Yet Breiman shows a dilemma (Breiman, 1999) that a uniform improvement on margin distribution \\emph{does not} necessarily reduces generalization error. In this paper, we revisit Breiman's dilemma in deep neural networks with recently proposed normalized margins using Lipschitz constant bound by spectral norm products. With both simplified theory and extensive experiments, Breiman's dilemma is shown to rely on dynamics of normalized margin distributions, that reflects the trade-off between model expression power and data complexity. When the complexity of data is comparable to the model expression power in the sense that training and test data share similar phase transitions in normalized margin dynamics, two efficient ways are derived via classic margin-based generalization bounds to successfully predict the trend of generalization error. On the other hand, over-expressed models that exhibit uniform improvements on training normalized margins may lose such a prediction power and fail to prevent the overfitting. \n", "keywords": ["Bregman's Dilemma", "Generalization Error", "Margin", "Spectral normalization"], "authorids": ["yhuangcc@ust.hk", "yuany@ust.hk", "wzhuai@connect.ust.hk"], "authors": ["Yifei HUANG", "Yuan YAO", "Weizhi ZHU"], "TL;DR": "Bregman's dilemma is shown in deep learning that improvement of margins of over-parameterized models may result in overfitting, and dynamics of normalized margin distributions are proposed to predict generalization error and identify such a dilemma. ", "pdf": "/pdf/6c3005549b4ac8119f8edeb787a903e3e738bf5e.pdf", "paperhash": "huang|on_breimans_dilemma_in_neural_networks_success_and_failure_of_normalized_margins", "_bibtex": "@misc{\nhuang2019on,\ntitle={{ON} {BREIMAN}\u2019S {DILEMMA} {IN} {NEURAL} {NETWORKS}: {SUCCESS} {AND} {FAILURE} {OF} {NORMALIZED} {MARGINS}},\nauthor={Yifei HUANG and Yuan YAO and Weizhi ZHU},\nyear={2019},\nurl={https://openreview.net/forum?id=Byl_ciRcY7},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "H1lgs0qUg4", "original": null, "number": 1, "cdate": 1545150103947, "ddate": null, "tcdate": 1545150103947, "tmdate": 1545354476958, "tddate": null, "forum": "Byl_ciRcY7", "replyto": "Byl_ciRcY7", "invitation": "ICLR.cc/2019/Conference/-/Paper549/Meta_Review", "content": {"metareview": "The reviewers reached a consensus that the paper is not ready for publication in ICLR. (see more details in the reviews below. )", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Reject", "title": "meta-review"}, "signatures": ["ICLR.cc/2019/Conference/Paper549/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper549/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ON BREIMAN\u2019S DILEMMA IN NEURAL NETWORKS: SUCCESS AND FAILURE OF NORMALIZED MARGINS", "abstract": "A belief persists long in machine learning that enlargement of margins over training data accounts for the resistance of models to overfitting by increasing the robustness. Yet Breiman shows a dilemma (Breiman, 1999) that a uniform improvement on margin distribution \\emph{does not} necessarily reduces generalization error. In this paper, we revisit Breiman's dilemma in deep neural networks with recently proposed normalized margins using Lipschitz constant bound by spectral norm products. With both simplified theory and extensive experiments, Breiman's dilemma is shown to rely on dynamics of normalized margin distributions, that reflects the trade-off between model expression power and data complexity. When the complexity of data is comparable to the model expression power in the sense that training and test data share similar phase transitions in normalized margin dynamics, two efficient ways are derived via classic margin-based generalization bounds to successfully predict the trend of generalization error. On the other hand, over-expressed models that exhibit uniform improvements on training normalized margins may lose such a prediction power and fail to prevent the overfitting. \n", "keywords": ["Bregman's Dilemma", "Generalization Error", "Margin", "Spectral normalization"], "authorids": ["yhuangcc@ust.hk", "yuany@ust.hk", "wzhuai@connect.ust.hk"], "authors": ["Yifei HUANG", "Yuan YAO", "Weizhi ZHU"], "TL;DR": "Bregman's dilemma is shown in deep learning that improvement of margins of over-parameterized models may result in overfitting, and dynamics of normalized margin distributions are proposed to predict generalization error and identify such a dilemma. ", "pdf": "/pdf/6c3005549b4ac8119f8edeb787a903e3e738bf5e.pdf", "paperhash": "huang|on_breimans_dilemma_in_neural_networks_success_and_failure_of_normalized_margins", "_bibtex": "@misc{\nhuang2019on,\ntitle={{ON} {BREIMAN}\u2019S {DILEMMA} {IN} {NEURAL} {NETWORKS}: {SUCCESS} {AND} {FAILURE} {OF} {NORMALIZED} {MARGINS}},\nauthor={Yifei HUANG and Yuan YAO and Weizhi ZHU},\nyear={2019},\nurl={https://openreview.net/forum?id=Byl_ciRcY7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper549/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353175411, "tddate": null, "super": null, "final": null, "reply": {"forum": "Byl_ciRcY7", "replyto": "Byl_ciRcY7", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper549/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper549/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper549/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353175411}}}, {"id": "rJeA1UCq2X", "original": null, "number": 3, "cdate": 1541232102389, "ddate": null, "tcdate": 1541232102389, "tmdate": 1543276118391, "tddate": null, "forum": "Byl_ciRcY7", "replyto": "Byl_ciRcY7", "invitation": "ICLR.cc/2019/Conference/-/Paper549/Official_Review", "content": {"title": "The technical contribution is minor", "review": "Summary: \nThe authors investigate the Breiman\u2019s dilemma in the context of deep learning. They show generalization bounds in terms of the margin distribution. They also perform experiments showing the Breiman\u2019s dilemma.\n\nComments: \nI am afraid the authors miss an important related paper:\n\nLev Reyzin, Robert E. Schapire:\nHow boosting the margin can also boost classifier complexity. ICML 2006: 753-760\n\nReyzin and Schapire explain the Breiman\u2019s dilemma based on base classifiers\u2019 complexity. In particular, their experiments show that arc-gv tends to use more complex decision trees than AdaBoost while it achieves better margin distribution over sample. That is, not only margin distribution, but also the complexity of base classifiers\u2019 class matters. This is already explained by known Rademacher complexity based margin bounds.\n\nAs for quiantile-based analyses on margin bounds the following result is known:\n\nLiwei Wang et al: A Refined Margin Analysis for Boosting Algorithms via Equilibrium Margin, \nJournal of Machine Learning Research 12 (2011) 1835-1863.\n\nThey proved a shaper bound using the notion of equibrium margin. The authors should compare the presented results with this. \n\nThe technical results of the paper look quite similar to known margin bounds and I am afraid the contribution is minor or redundant.\n\nAfter the rebuttal:\nI read the authors' comments and understand more the technical results. I raised my score. But I still feel that the techniccal contribution is a bit weak.\n\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper549/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "ON BREIMAN\u2019S DILEMMA IN NEURAL NETWORKS: SUCCESS AND FAILURE OF NORMALIZED MARGINS", "abstract": "A belief persists long in machine learning that enlargement of margins over training data accounts for the resistance of models to overfitting by increasing the robustness. Yet Breiman shows a dilemma (Breiman, 1999) that a uniform improvement on margin distribution \\emph{does not} necessarily reduces generalization error. In this paper, we revisit Breiman's dilemma in deep neural networks with recently proposed normalized margins using Lipschitz constant bound by spectral norm products. With both simplified theory and extensive experiments, Breiman's dilemma is shown to rely on dynamics of normalized margin distributions, that reflects the trade-off between model expression power and data complexity. When the complexity of data is comparable to the model expression power in the sense that training and test data share similar phase transitions in normalized margin dynamics, two efficient ways are derived via classic margin-based generalization bounds to successfully predict the trend of generalization error. On the other hand, over-expressed models that exhibit uniform improvements on training normalized margins may lose such a prediction power and fail to prevent the overfitting. \n", "keywords": ["Bregman's Dilemma", "Generalization Error", "Margin", "Spectral normalization"], "authorids": ["yhuangcc@ust.hk", "yuany@ust.hk", "wzhuai@connect.ust.hk"], "authors": ["Yifei HUANG", "Yuan YAO", "Weizhi ZHU"], "TL;DR": "Bregman's dilemma is shown in deep learning that improvement of margins of over-parameterized models may result in overfitting, and dynamics of normalized margin distributions are proposed to predict generalization error and identify such a dilemma. ", "pdf": "/pdf/6c3005549b4ac8119f8edeb787a903e3e738bf5e.pdf", "paperhash": "huang|on_breimans_dilemma_in_neural_networks_success_and_failure_of_normalized_margins", "_bibtex": "@misc{\nhuang2019on,\ntitle={{ON} {BREIMAN}\u2019S {DILEMMA} {IN} {NEURAL} {NETWORKS}: {SUCCESS} {AND} {FAILURE} {OF} {NORMALIZED} {MARGINS}},\nauthor={Yifei HUANG and Yuan YAO and Weizhi ZHU},\nyear={2019},\nurl={https://openreview.net/forum?id=Byl_ciRcY7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper549/Official_Review", "cdate": 1542234436039, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "Byl_ciRcY7", "replyto": "Byl_ciRcY7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper549/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335749377, "tmdate": 1552335749377, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper549/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "rkxPipBfAQ", "original": null, "number": 3, "cdate": 1542770079077, "ddate": null, "tcdate": 1542770079077, "tmdate": 1542770079077, "tddate": null, "forum": "Byl_ciRcY7", "replyto": "SJgqQ5vNn7", "invitation": "ICLR.cc/2019/Conference/-/Paper549/Official_Comment", "content": {"title": "Response to AnonReviewer1", "comment": "We thank you for your comments. We give a response to your concerns below.\n\n-- Geometric margin\nThanks for the suggestion on geometric margin, whose definition is currently not available for deep neural networks. On the other hand, functional margins and networks Lipschitz via spectral norm products have been recently used to control the network complexity (Bartlett et al. 2017). Our results show that only a control on spectral norm and the functional margin is not sufficient to prevent the Breiman\u2019s dilemma. Other weight norms like 2-1 norm might be worth to explore as our next direction. \n\n-- Is Breiman\u2019s dilemma solved by re-defining margin properly?\nThis is a good question. Although we don\u2019t have a geometric margin in hand as in linear networks yet, different normalizations on functional margins may affect when Breiman\u2019s dilemma appears as model complexity increases. On the other hand, we have seen that Breiman\u2019s dilemma is ubiquitous in neural networks as the expressive power grows as in boosting, and what we lack is a proper complexity control at this moment that needs to be explored in the future. \n"}, "signatures": ["ICLR.cc/2019/Conference/Paper549/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper549/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper549/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ON BREIMAN\u2019S DILEMMA IN NEURAL NETWORKS: SUCCESS AND FAILURE OF NORMALIZED MARGINS", "abstract": "A belief persists long in machine learning that enlargement of margins over training data accounts for the resistance of models to overfitting by increasing the robustness. Yet Breiman shows a dilemma (Breiman, 1999) that a uniform improvement on margin distribution \\emph{does not} necessarily reduces generalization error. In this paper, we revisit Breiman's dilemma in deep neural networks with recently proposed normalized margins using Lipschitz constant bound by spectral norm products. With both simplified theory and extensive experiments, Breiman's dilemma is shown to rely on dynamics of normalized margin distributions, that reflects the trade-off between model expression power and data complexity. When the complexity of data is comparable to the model expression power in the sense that training and test data share similar phase transitions in normalized margin dynamics, two efficient ways are derived via classic margin-based generalization bounds to successfully predict the trend of generalization error. On the other hand, over-expressed models that exhibit uniform improvements on training normalized margins may lose such a prediction power and fail to prevent the overfitting. \n", "keywords": ["Bregman's Dilemma", "Generalization Error", "Margin", "Spectral normalization"], "authorids": ["yhuangcc@ust.hk", "yuany@ust.hk", "wzhuai@connect.ust.hk"], "authors": ["Yifei HUANG", "Yuan YAO", "Weizhi ZHU"], "TL;DR": "Bregman's dilemma is shown in deep learning that improvement of margins of over-parameterized models may result in overfitting, and dynamics of normalized margin distributions are proposed to predict generalization error and identify such a dilemma. ", "pdf": "/pdf/6c3005549b4ac8119f8edeb787a903e3e738bf5e.pdf", "paperhash": "huang|on_breimans_dilemma_in_neural_networks_success_and_failure_of_normalized_margins", "_bibtex": "@misc{\nhuang2019on,\ntitle={{ON} {BREIMAN}\u2019S {DILEMMA} {IN} {NEURAL} {NETWORKS}: {SUCCESS} {AND} {FAILURE} {OF} {NORMALIZED} {MARGINS}},\nauthor={Yifei HUANG and Yuan YAO and Weizhi ZHU},\nyear={2019},\nurl={https://openreview.net/forum?id=Byl_ciRcY7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper549/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621615567, "tddate": null, "super": null, "final": null, "reply": {"forum": "Byl_ciRcY7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper549/Authors", "ICLR.cc/2019/Conference/Paper549/Reviewers", "ICLR.cc/2019/Conference/Paper549/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper549/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper549/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper549/Authors|ICLR.cc/2019/Conference/Paper549/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper549/Reviewers", "ICLR.cc/2019/Conference/Paper549/Authors", "ICLR.cc/2019/Conference/Paper549/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621615567}}}, {"id": "H1gIu6BMCm", "original": null, "number": 2, "cdate": 1542770029575, "ddate": null, "tcdate": 1542770029575, "tmdate": 1542770029575, "tddate": null, "forum": "Byl_ciRcY7", "replyto": "rke-fUAYnX", "invitation": "ICLR.cc/2019/Conference/-/Paper549/Official_Comment", "content": {"title": "Response to AnonReviewer3", "comment": "We thank you for your comments. We give a response to your two major questions below.\n\n-- It\u2019s difficult to judge phase transition in the evolution.\nPhase transitions of normalized margin evolutions can be judged via the rank correlations between training- and test- margin dynamics, shown in the second row (heatmaps) of Figure. When these heatmaps exhibit a block diagonal structure, as the left and middle ones, training- and test- margin dynamics share a similar phase transition; on the other hand, the left-right two-block structure in the right heatmap indicates the distinct phase transitions in training- and test- margin evolutions (training margins undergo a uniform improvements while test margins undergo an increase-decrease phase transition). Moreover, the cross-over in the training margin dynamics also marks the occurrence of phase-transition as well, as illustrated by the right two figures in Figure 1.\n\n-- The bound is not direct to support the results of the experiments.\nUnfortunately, existing Rademacher complexity based bounds are too loose to quantitatively calibrate the success case and failure case given models and datasets. They only provide a qualitative explanation such that: 1) when training margin dynamics share a similar phase transition with test margin dynamics, Rademacher complexity of normalized networks can be regarded as a constant without affecting the successful prediction of test error trend by training margins, 2) when training margin dynamics are uniformly improved as a distinct phase transition to test margin dynamics, such a prediction fails, Breiman\u2019s dilemma happens, and Rademacher complexity of normalized networks blows up. Better complexity upper bounds with tighter explanation power are our future pursuit.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper549/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper549/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper549/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ON BREIMAN\u2019S DILEMMA IN NEURAL NETWORKS: SUCCESS AND FAILURE OF NORMALIZED MARGINS", "abstract": "A belief persists long in machine learning that enlargement of margins over training data accounts for the resistance of models to overfitting by increasing the robustness. Yet Breiman shows a dilemma (Breiman, 1999) that a uniform improvement on margin distribution \\emph{does not} necessarily reduces generalization error. In this paper, we revisit Breiman's dilemma in deep neural networks with recently proposed normalized margins using Lipschitz constant bound by spectral norm products. With both simplified theory and extensive experiments, Breiman's dilemma is shown to rely on dynamics of normalized margin distributions, that reflects the trade-off between model expression power and data complexity. When the complexity of data is comparable to the model expression power in the sense that training and test data share similar phase transitions in normalized margin dynamics, two efficient ways are derived via classic margin-based generalization bounds to successfully predict the trend of generalization error. On the other hand, over-expressed models that exhibit uniform improvements on training normalized margins may lose such a prediction power and fail to prevent the overfitting. \n", "keywords": ["Bregman's Dilemma", "Generalization Error", "Margin", "Spectral normalization"], "authorids": ["yhuangcc@ust.hk", "yuany@ust.hk", "wzhuai@connect.ust.hk"], "authors": ["Yifei HUANG", "Yuan YAO", "Weizhi ZHU"], "TL;DR": "Bregman's dilemma is shown in deep learning that improvement of margins of over-parameterized models may result in overfitting, and dynamics of normalized margin distributions are proposed to predict generalization error and identify such a dilemma. ", "pdf": "/pdf/6c3005549b4ac8119f8edeb787a903e3e738bf5e.pdf", "paperhash": "huang|on_breimans_dilemma_in_neural_networks_success_and_failure_of_normalized_margins", "_bibtex": "@misc{\nhuang2019on,\ntitle={{ON} {BREIMAN}\u2019S {DILEMMA} {IN} {NEURAL} {NETWORKS}: {SUCCESS} {AND} {FAILURE} {OF} {NORMALIZED} {MARGINS}},\nauthor={Yifei HUANG and Yuan YAO and Weizhi ZHU},\nyear={2019},\nurl={https://openreview.net/forum?id=Byl_ciRcY7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper549/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621615567, "tddate": null, "super": null, "final": null, "reply": {"forum": "Byl_ciRcY7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper549/Authors", "ICLR.cc/2019/Conference/Paper549/Reviewers", "ICLR.cc/2019/Conference/Paper549/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper549/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper549/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper549/Authors|ICLR.cc/2019/Conference/Paper549/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper549/Reviewers", "ICLR.cc/2019/Conference/Paper549/Authors", "ICLR.cc/2019/Conference/Paper549/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621615567}}}, {"id": "BkebW6rfA7", "original": null, "number": 1, "cdate": 1542769912819, "ddate": null, "tcdate": 1542769912819, "tmdate": 1542769912819, "tddate": null, "forum": "Byl_ciRcY7", "replyto": "rJeA1UCq2X", "invitation": "ICLR.cc/2019/Conference/-/Paper549/Official_Comment", "content": {"title": "Response to AnonReviewer2", "comment": "We thank you for your comments. \n\nFirst of all, let\u2019s recapture our main contribution to clarifying the distinctions to existing works in boosting that mentioned by the reviewer. For neural networks, we show that Breiman\u2019s Dilemma in neural networks can be judged by the dynamics of normalized margin distributions and phase transitions therein:\n\n(A) When dynamics of training- and test- normalized margins share a similar phase transition of increase-decrease dynamics, one can predict the trend (e.g. early stopping regularization) of test margins/errors from training margins via two simplifications of traditional Rademacher complexity based bounds (e.g. Bartlett et al. 2017), by regarding as a small constant the Rademacher complexity of spectrally normalized networks. \n(B) On the other hand, when training normalized margins are uniformly improved which indicates the over-expressiveness of the models against data, such a prediction fails and the Rademacher complexity of spectrally normalized networks might be too large to make the bounds trivial. This is a similar observation in boosting reported by Breiman (1999) that a uniform improvement of margin distribution does not necessarily mean improvement in generalization performance. \n \nThe two references you mentioned are good in explaining Breiman\u2019s dilemma in boosting algorithms, but NOT applicable to deep neural networks that will be explained as follows.\n\n1) Reyzin and Schapire (2006) shows that arc-gv (Breiman, 1999) has larger base classifier complexities (in terms of tree depth) than AdaBoost that explains its drop of generalization performance. This work has the same spirit as what we have observed for deep neural networks. But complexity measure of neural networks, where we turn to a simplification of recent spectral-normalization bounds by Bartlett et al. 2017, is different to that of convex combinations of voting classifiers in boosting. \n2) Liwei Wang et al. (2011) proposes a generalization bound for boosting based on VC-dimensions and optimized quantile margins. First, such optimized quantile margins become trivial or do not work when training margin distributions are uniformly improved in (B) scenario above, even in boosting set. Second, most of the models in deep neural networks like the ones we studied in this paper (CNN.400, AlexNet, VGG16, ResNet), are overparameterized that VC-dimension is too loose to give trivial bound. Lately, since Bartlett (1997), neural network society has been exploring weight-norm based or size-independent bounds on Rademacher complexities, e.g. the bounds in Bartlett et al. (2017). In this paper, we are not aiming to provide a tighter estimate of such bounds; instead, we show that under scenario (A) above, Rademacher complexity can be simplified to be a constant which does not affect the prediction of test margins/errors via training margins, in a similar spirit of Liao et al. 2018 in the study of cross-entropy loss.  \n\nReference:\n[1] Bartlett, Peter L. \"For valid generalization the size of the weights is more important than the size of the network.\"\u00a0Advances in neural information processing systems. 1997.\n[2] Liao, Qianli, et al. \"A surprising linear relationship predicts test performance in deep networks.\"\u00a0arXiv preprint arXiv:1807.09659\u00a0(2018).\n[3] Bartlett, Peter L., Dylan J. Foster, and Matus J. Telgarsky. \"Spectrally-normalized margin bounds for neural networks.\"\u00a0Advances in Neural Information Processing Systems. 2017."}, "signatures": ["ICLR.cc/2019/Conference/Paper549/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper549/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper549/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ON BREIMAN\u2019S DILEMMA IN NEURAL NETWORKS: SUCCESS AND FAILURE OF NORMALIZED MARGINS", "abstract": "A belief persists long in machine learning that enlargement of margins over training data accounts for the resistance of models to overfitting by increasing the robustness. Yet Breiman shows a dilemma (Breiman, 1999) that a uniform improvement on margin distribution \\emph{does not} necessarily reduces generalization error. In this paper, we revisit Breiman's dilemma in deep neural networks with recently proposed normalized margins using Lipschitz constant bound by spectral norm products. With both simplified theory and extensive experiments, Breiman's dilemma is shown to rely on dynamics of normalized margin distributions, that reflects the trade-off between model expression power and data complexity. When the complexity of data is comparable to the model expression power in the sense that training and test data share similar phase transitions in normalized margin dynamics, two efficient ways are derived via classic margin-based generalization bounds to successfully predict the trend of generalization error. On the other hand, over-expressed models that exhibit uniform improvements on training normalized margins may lose such a prediction power and fail to prevent the overfitting. \n", "keywords": ["Bregman's Dilemma", "Generalization Error", "Margin", "Spectral normalization"], "authorids": ["yhuangcc@ust.hk", "yuany@ust.hk", "wzhuai@connect.ust.hk"], "authors": ["Yifei HUANG", "Yuan YAO", "Weizhi ZHU"], "TL;DR": "Bregman's dilemma is shown in deep learning that improvement of margins of over-parameterized models may result in overfitting, and dynamics of normalized margin distributions are proposed to predict generalization error and identify such a dilemma. ", "pdf": "/pdf/6c3005549b4ac8119f8edeb787a903e3e738bf5e.pdf", "paperhash": "huang|on_breimans_dilemma_in_neural_networks_success_and_failure_of_normalized_margins", "_bibtex": "@misc{\nhuang2019on,\ntitle={{ON} {BREIMAN}\u2019S {DILEMMA} {IN} {NEURAL} {NETWORKS}: {SUCCESS} {AND} {FAILURE} {OF} {NORMALIZED} {MARGINS}},\nauthor={Yifei HUANG and Yuan YAO and Weizhi ZHU},\nyear={2019},\nurl={https://openreview.net/forum?id=Byl_ciRcY7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper549/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621615567, "tddate": null, "super": null, "final": null, "reply": {"forum": "Byl_ciRcY7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper549/Authors", "ICLR.cc/2019/Conference/Paper549/Reviewers", "ICLR.cc/2019/Conference/Paper549/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper549/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper549/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper549/Authors|ICLR.cc/2019/Conference/Paper549/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper549/Reviewers", "ICLR.cc/2019/Conference/Paper549/Authors", "ICLR.cc/2019/Conference/Paper549/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621615567}}}, {"id": "rke-fUAYnX", "original": null, "number": 2, "cdate": 1541166600795, "ddate": null, "tcdate": 1541166600795, "tmdate": 1541533899120, "tddate": null, "forum": "Byl_ciRcY7", "replyto": "Byl_ciRcY7", "invitation": "ICLR.cc/2019/Conference/-/Paper549/Official_Review", "content": {"title": "This paper started from Breiman's dilemma and showed that it relies on dynamics of normalized margin distribution.", "review": "The authors found that general generalization bounds fail to capture the ramp loss. However, once the network scaled by its Lipschitz constant, it becomes efficient to get an upper bound of generalization error, while also needs to trade-off the constant in the margin error. Due to the limitation of fixing the constant in margin error, the authors  tried to use the quantile margin to change the bound, which is easy to tune the hyper-parameter. They also conducted the experiments that the quantile margin generalization bound could be used to predict the tendency of loss curve both in training and test in some sense.\n\nIt's really an interesting work to provide a way for early stopping and to show the quantile margin maybe a substitution of tendency in training error as well as test error.\n\nQuestions: \n\nIt could be difficult to judge from the phase transition, if exists, in the evolution of normalized margin distributions curve. Maybe  some quantitative descriptions are needed. \n\nBesides, the authors' quantile margin bound (Theorem 2) shows the upper bound of margin (or say margin error). But the bound is not direct to support the powerful experiments results, the relationship between the tendency of quantile margin, training and test error.\n\nTypos:\n In Eqn. (10), the first $f_t$ should be $\\widetilde{f}_t$ .\nIn Eqn. (9) and (11), there is  $1$.\nIn Proof in Lemma A.1, the convolution operator is $x(v)$ not $x(u)$, since Lemma is also true.\nIn Proof in Lemma D.4, though the proof is same in the book `Foundations of machine learning' by Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar, please check the typo.\nIn Proof of Proposition 1, lack $\\frac{1}{n}$ in Rademacher complexity.\nIn Proof of Theorem 1, maybe you should take $\\mathbb{E}$ not $\\mathbb{P}$ before $\\ell_{\\gamma_1,\\gamma_2}(\\xi(\\widetilde{f}(x,y))))$.\nIn Proof of Theorem 2, page 18 the last line in the equation, why can the second term after divided by $L_f$ bounded by $L$, maybe need some conditions or I missed something.\n\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper549/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ON BREIMAN\u2019S DILEMMA IN NEURAL NETWORKS: SUCCESS AND FAILURE OF NORMALIZED MARGINS", "abstract": "A belief persists long in machine learning that enlargement of margins over training data accounts for the resistance of models to overfitting by increasing the robustness. Yet Breiman shows a dilemma (Breiman, 1999) that a uniform improvement on margin distribution \\emph{does not} necessarily reduces generalization error. In this paper, we revisit Breiman's dilemma in deep neural networks with recently proposed normalized margins using Lipschitz constant bound by spectral norm products. With both simplified theory and extensive experiments, Breiman's dilemma is shown to rely on dynamics of normalized margin distributions, that reflects the trade-off between model expression power and data complexity. When the complexity of data is comparable to the model expression power in the sense that training and test data share similar phase transitions in normalized margin dynamics, two efficient ways are derived via classic margin-based generalization bounds to successfully predict the trend of generalization error. On the other hand, over-expressed models that exhibit uniform improvements on training normalized margins may lose such a prediction power and fail to prevent the overfitting. \n", "keywords": ["Bregman's Dilemma", "Generalization Error", "Margin", "Spectral normalization"], "authorids": ["yhuangcc@ust.hk", "yuany@ust.hk", "wzhuai@connect.ust.hk"], "authors": ["Yifei HUANG", "Yuan YAO", "Weizhi ZHU"], "TL;DR": "Bregman's dilemma is shown in deep learning that improvement of margins of over-parameterized models may result in overfitting, and dynamics of normalized margin distributions are proposed to predict generalization error and identify such a dilemma. ", "pdf": "/pdf/6c3005549b4ac8119f8edeb787a903e3e738bf5e.pdf", "paperhash": "huang|on_breimans_dilemma_in_neural_networks_success_and_failure_of_normalized_margins", "_bibtex": "@misc{\nhuang2019on,\ntitle={{ON} {BREIMAN}\u2019S {DILEMMA} {IN} {NEURAL} {NETWORKS}: {SUCCESS} {AND} {FAILURE} {OF} {NORMALIZED} {MARGINS}},\nauthor={Yifei HUANG and Yuan YAO and Weizhi ZHU},\nyear={2019},\nurl={https://openreview.net/forum?id=Byl_ciRcY7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper549/Official_Review", "cdate": 1542234436039, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "Byl_ciRcY7", "replyto": "Byl_ciRcY7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper549/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335749377, "tmdate": 1552335749377, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper549/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SJgqQ5vNn7", "original": null, "number": 1, "cdate": 1540811298114, "ddate": null, "tcdate": 1540811298114, "tmdate": 1541533898911, "tddate": null, "forum": "Byl_ciRcY7", "replyto": "Byl_ciRcY7", "invitation": "ICLR.cc/2019/Conference/-/Paper549/Official_Review", "content": {"title": ".", "review": "The submission explores Breiman's dilemma: training margin is not always a good predictor of test error.\n\nIn particular, the authors show that:\n\n- For under-parametrized CNNs, the training prediction margin is a good predictor of the test error.\n- For over-parametrized CNNs, the training prediction margin is not a good predictor of the test error.\n\nThroughout the submission, I suspect that the authors compute the \"functional margin\", that is, the difference between the largest label score and the second largest score, for correctly classified examples. Functional margins ignore the smoothness of the underlying function, a critical factor for generalization. For instance, the function f(x) = 1[x > 0] has large functional margin, but any perturbation around the x-origin would drastically change the prediction. For this reason, I think the authors should consider the \"geometrical margin\" instead, which is unfortunately difficult to compute for general neural networks. Their theory tries to reflect on this issue by using spectrally-normalized bounds, but the practice ignores this issue completely (as far as I can tell).\n\nTherefore, we may be looking at the wrong statistic to predict generalization error. Is Breiman dilemma solved by re-defining margin properly? Geometrical margin can be computed in closed-form for linear classifiers, so perhaps this would be a first step in this investigation.\n\n", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper549/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ON BREIMAN\u2019S DILEMMA IN NEURAL NETWORKS: SUCCESS AND FAILURE OF NORMALIZED MARGINS", "abstract": "A belief persists long in machine learning that enlargement of margins over training data accounts for the resistance of models to overfitting by increasing the robustness. Yet Breiman shows a dilemma (Breiman, 1999) that a uniform improvement on margin distribution \\emph{does not} necessarily reduces generalization error. In this paper, we revisit Breiman's dilemma in deep neural networks with recently proposed normalized margins using Lipschitz constant bound by spectral norm products. With both simplified theory and extensive experiments, Breiman's dilemma is shown to rely on dynamics of normalized margin distributions, that reflects the trade-off between model expression power and data complexity. When the complexity of data is comparable to the model expression power in the sense that training and test data share similar phase transitions in normalized margin dynamics, two efficient ways are derived via classic margin-based generalization bounds to successfully predict the trend of generalization error. On the other hand, over-expressed models that exhibit uniform improvements on training normalized margins may lose such a prediction power and fail to prevent the overfitting. \n", "keywords": ["Bregman's Dilemma", "Generalization Error", "Margin", "Spectral normalization"], "authorids": ["yhuangcc@ust.hk", "yuany@ust.hk", "wzhuai@connect.ust.hk"], "authors": ["Yifei HUANG", "Yuan YAO", "Weizhi ZHU"], "TL;DR": "Bregman's dilemma is shown in deep learning that improvement of margins of over-parameterized models may result in overfitting, and dynamics of normalized margin distributions are proposed to predict generalization error and identify such a dilemma. ", "pdf": "/pdf/6c3005549b4ac8119f8edeb787a903e3e738bf5e.pdf", "paperhash": "huang|on_breimans_dilemma_in_neural_networks_success_and_failure_of_normalized_margins", "_bibtex": "@misc{\nhuang2019on,\ntitle={{ON} {BREIMAN}\u2019S {DILEMMA} {IN} {NEURAL} {NETWORKS}: {SUCCESS} {AND} {FAILURE} {OF} {NORMALIZED} {MARGINS}},\nauthor={Yifei HUANG and Yuan YAO and Weizhi ZHU},\nyear={2019},\nurl={https://openreview.net/forum?id=Byl_ciRcY7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper549/Official_Review", "cdate": 1542234436039, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "Byl_ciRcY7", "replyto": "Byl_ciRcY7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper549/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335749377, "tmdate": 1552335749377, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper549/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 8}