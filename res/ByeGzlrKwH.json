{"notes": [{"id": "ByeGzlrKwH", "original": "ryeA2ZxFvr", "number": 2165, "cdate": 1569439754103, "ddate": null, "tcdate": 1569439754103, "tmdate": 1583912040227, "tddate": null, "forum": "ByeGzlrKwH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["taiji@mist.i.u-tokyo.ac.jp", "abe@ipride.co.jp", "tomoaki.nishimura@nttdata.com"], "title": "Compression based bound for non-compressed network: unified generalization error analysis of large compressible deep neural network", "authors": ["Taiji Suzuki", "Hiroshi Abe", "Tomoaki Nishimura"], "pdf": "/pdf/82b66e53e3b33e8854c767c5ec1223b2d3eab3a2.pdf", "abstract": "One of the biggest issues in deep learning theory is the generalization ability of networks with huge model size. \nThe classical learning theory suggests that overparameterized models cause overfitting.\nHowever, practically used large deep models avoid overfitting, which is not well explained by the classical approaches.\nTo resolve this issue, several attempts have been made. \nAmong them, the compression based bound is one of the promising approaches. \nHowever, the compression based bound can be applied only to a compressed network, and it is not applicable to the non-compressed original network. \nIn this paper, we give a unified frame-work that can convert compression based bounds to those for non-compressed original networks.\nThe bound gives even better rate than the one for the compressed network by improving the bias term.\nBy establishing the unified frame-work, we can obtain a data dependent generalization error bound which gives a tighter evaluation than the data independent ones.\n", "keywords": ["Generalization error", "compression based bound", "local Rademacher complexity"], "paperhash": "suzuki|compression_based_bound_for_noncompressed_network_unified_generalization_error_analysis_of_large_compressible_deep_neural_network", "_bibtex": "@inproceedings{\nSuzuki2020Compression,\ntitle={Compression based bound for non-compressed network: unified generalization error analysis of large compressible deep neural network},\nauthor={Taiji Suzuki and Hiroshi Abe and Tomoaki Nishimura},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=ByeGzlrKwH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/337879b9857962b93e0280dad73662756f75770b.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "AlBAgatu-w", "original": null, "number": 1, "cdate": 1576798742183, "ddate": null, "tcdate": 1576798742183, "tmdate": 1576800894034, "tddate": null, "forum": "ByeGzlrKwH", "replyto": "ByeGzlrKwH", "invitation": "ICLR.cc/2020/Conference/Paper2165/-/Decision", "content": {"decision": "Accept (Spotlight)", "comment": "This paper has a few interesting contributions: (a) a bound for un-compressed networks in terms of the compressed network (this is in contrast to some prior work, which only gives bounds on the compressed network); (b) the use of local Rademacher complexity to try to squeeze as much as possible out of the connection; (c) an application of the bound to a specific interesting favorable condition, namely low-rank structure.\n\nAs a minor suggestion, I'd like to recommend that the authors go ahead and use their allowed 10th body page!", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["taiji@mist.i.u-tokyo.ac.jp", "abe@ipride.co.jp", "tomoaki.nishimura@nttdata.com"], "title": "Compression based bound for non-compressed network: unified generalization error analysis of large compressible deep neural network", "authors": ["Taiji Suzuki", "Hiroshi Abe", "Tomoaki Nishimura"], "pdf": "/pdf/82b66e53e3b33e8854c767c5ec1223b2d3eab3a2.pdf", "abstract": "One of the biggest issues in deep learning theory is the generalization ability of networks with huge model size. \nThe classical learning theory suggests that overparameterized models cause overfitting.\nHowever, practically used large deep models avoid overfitting, which is not well explained by the classical approaches.\nTo resolve this issue, several attempts have been made. \nAmong them, the compression based bound is one of the promising approaches. \nHowever, the compression based bound can be applied only to a compressed network, and it is not applicable to the non-compressed original network. \nIn this paper, we give a unified frame-work that can convert compression based bounds to those for non-compressed original networks.\nThe bound gives even better rate than the one for the compressed network by improving the bias term.\nBy establishing the unified frame-work, we can obtain a data dependent generalization error bound which gives a tighter evaluation than the data independent ones.\n", "keywords": ["Generalization error", "compression based bound", "local Rademacher complexity"], "paperhash": "suzuki|compression_based_bound_for_noncompressed_network_unified_generalization_error_analysis_of_large_compressible_deep_neural_network", "_bibtex": "@inproceedings{\nSuzuki2020Compression,\ntitle={Compression based bound for non-compressed network: unified generalization error analysis of large compressible deep neural network},\nauthor={Taiji Suzuki and Hiroshi Abe and Tomoaki Nishimura},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=ByeGzlrKwH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/337879b9857962b93e0280dad73662756f75770b.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "ByeGzlrKwH", "replyto": "ByeGzlrKwH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795711777, "tmdate": 1576800261038, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2165/-/Decision"}}}, {"id": "SkgfGuL3or", "original": null, "number": 8, "cdate": 1573836810312, "ddate": null, "tcdate": 1573836810312, "tmdate": 1573836810312, "tddate": null, "forum": "ByeGzlrKwH", "replyto": "BJeq9Wt9jH", "invitation": "ICLR.cc/2020/Conference/Paper2165/-/Official_Comment", "content": {"title": "Comments on the revised submission", "comment": "Thank you for clarifying the notation and answering my questions. I found the new empirical evaluation of intrinsic dimensionality and comparison to Arora et al. bound to be a valuable contribution (Appendix D). However, I do believe that the presentation of the theoretical results and the notation in the main text could be made easier to follow still (I personally think that it is confusing to denote quite different quantities with the same letter but different subscripts or superscripts). \n\nOverall, I think the paper improved and therefore I increased my score to \"accept\"."}, "signatures": ["ICLR.cc/2020/Conference/Paper2165/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2165/AnonReviewer1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["taiji@mist.i.u-tokyo.ac.jp", "abe@ipride.co.jp", "tomoaki.nishimura@nttdata.com"], "title": "Compression based bound for non-compressed network: unified generalization error analysis of large compressible deep neural network", "authors": ["Taiji Suzuki", "Hiroshi Abe", "Tomoaki Nishimura"], "pdf": "/pdf/82b66e53e3b33e8854c767c5ec1223b2d3eab3a2.pdf", "abstract": "One of the biggest issues in deep learning theory is the generalization ability of networks with huge model size. \nThe classical learning theory suggests that overparameterized models cause overfitting.\nHowever, practically used large deep models avoid overfitting, which is not well explained by the classical approaches.\nTo resolve this issue, several attempts have been made. \nAmong them, the compression based bound is one of the promising approaches. \nHowever, the compression based bound can be applied only to a compressed network, and it is not applicable to the non-compressed original network. \nIn this paper, we give a unified frame-work that can convert compression based bounds to those for non-compressed original networks.\nThe bound gives even better rate than the one for the compressed network by improving the bias term.\nBy establishing the unified frame-work, we can obtain a data dependent generalization error bound which gives a tighter evaluation than the data independent ones.\n", "keywords": ["Generalization error", "compression based bound", "local Rademacher complexity"], "paperhash": "suzuki|compression_based_bound_for_noncompressed_network_unified_generalization_error_analysis_of_large_compressible_deep_neural_network", "_bibtex": "@inproceedings{\nSuzuki2020Compression,\ntitle={Compression based bound for non-compressed network: unified generalization error analysis of large compressible deep neural network},\nauthor={Taiji Suzuki and Hiroshi Abe and Tomoaki Nishimura},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=ByeGzlrKwH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/337879b9857962b93e0280dad73662756f75770b.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "ByeGzlrKwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2165/Authors", "ICLR.cc/2020/Conference/Paper2165/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2165/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2165/Reviewers", "ICLR.cc/2020/Conference/Paper2165/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2165/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2165/Authors|ICLR.cc/2020/Conference/Paper2165/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504145384, "tmdate": 1576860555373, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2165/Authors", "ICLR.cc/2020/Conference/Paper2165/Reviewers", "ICLR.cc/2020/Conference/Paper2165/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2165/-/Official_Comment"}}}, {"id": "Bkg4y5FCYS", "original": null, "number": 2, "cdate": 1571883484302, "ddate": null, "tcdate": 1571883484302, "tmdate": 1573836351400, "tddate": null, "forum": "ByeGzlrKwH", "replyto": "ByeGzlrKwH", "invitation": "ICLR.cc/2020/Conference/Paper2165/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "8: Accept", "review_assessment:_checking_correctness_of_experiments": "N/A", "review_assessment:_thoroughness_in_paper_reading": "N/A", "title": "Official Blind Review #1", "review": "The paper presents novel theoretical results on generalization bounds via compression. Similar ideas in the last few years appeared, but only bounds on a compressed network were obtained. In contrast, the current submission gives a bound on the original (uncompressed) network in terms of the complexity of the compressed network class.\n\nOverall, the paper seems to be well-written. I appreciate that the outlines of the proofs are included in the main text, which helps the reader follow the ideas. The result is novel and quite interesting. The new bounds seem to be still quite far from giving tight generalization theory, but I believe the paper provides some nice theoretical results for other researchers to improve upon. I think the paper could be improved immensely by some empirical analysis of the rank of compressed standard vision networks and rank of activation covariance matrices.  There are also some citation issues (see detailed comments below).\n\nCitation issues:\nIn the introduction, paragraph 2, the authors cite Neyshabur et al. 2019 for the observation that networks generalize well despite being overparameterized. It seems like an odd choice. Why is Barttlet\u2019s \u201899 paper [\u201cSize of the weights\u2026\u201d]  not cited? Or at least Neyshabur et al. 2015? \nThen the authors mention that classical learning theory cannot explain the phenomena mentioned above, and classical theory \u201c.. suggests \u201d that overparameterized models cause overfitting\u2026\u201d. The authors need to be more precise and add citations (I am assuming that the authors are talking about VC bounds for worst-case ERM generalization).\nIn the third paragraph, where the authors talk about norm-based bounds being lose, it seems that Nagarajan and Kolter 2019 should be cited (not only at the end), as well as Dzigaite and Roy 2017 (they look into the looseness of path-norm and margin-based bounds).\n\nCould the authors comment more on how the bound in Theorem 2 is superior to VC dimension bound and whether conditions under which the bound is tight are realistic for standard compressed vision networks. Having weight matrices to be close to rank 1 seems unrealistic.I would like to see some sort of empirical evidence if the authors believe that this is the case. And for larger ranks, the bound seems to be close to VC bound.\n\nIn general, I found the notation a bit hard to follow and had to constantly be looking through the paper to find the definitions of various quantities. Having three different r\u2019s, multiple mu\u2019s with dots, bars, stars, etc., was definitely confusing and required extra attention to detail.\n\nOther minor comments:\n\nIn section 2, marginal distributions over x and y are introduced. Are those used in the main text?\nIs that a definition of \\mu with the dot on top in assumption 5, or is this mu with the dot defined earlier? Using notation := would make it clearer whether the quantity is being defined.\nIn Section 3, \u201cThe main difference from the\u2026\u201d paragraph, there is \\Psi(\\dot r) used. Where is that defined?", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper2165/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2165/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["taiji@mist.i.u-tokyo.ac.jp", "abe@ipride.co.jp", "tomoaki.nishimura@nttdata.com"], "title": "Compression based bound for non-compressed network: unified generalization error analysis of large compressible deep neural network", "authors": ["Taiji Suzuki", "Hiroshi Abe", "Tomoaki Nishimura"], "pdf": "/pdf/82b66e53e3b33e8854c767c5ec1223b2d3eab3a2.pdf", "abstract": "One of the biggest issues in deep learning theory is the generalization ability of networks with huge model size. \nThe classical learning theory suggests that overparameterized models cause overfitting.\nHowever, practically used large deep models avoid overfitting, which is not well explained by the classical approaches.\nTo resolve this issue, several attempts have been made. \nAmong them, the compression based bound is one of the promising approaches. \nHowever, the compression based bound can be applied only to a compressed network, and it is not applicable to the non-compressed original network. \nIn this paper, we give a unified frame-work that can convert compression based bounds to those for non-compressed original networks.\nThe bound gives even better rate than the one for the compressed network by improving the bias term.\nBy establishing the unified frame-work, we can obtain a data dependent generalization error bound which gives a tighter evaluation than the data independent ones.\n", "keywords": ["Generalization error", "compression based bound", "local Rademacher complexity"], "paperhash": "suzuki|compression_based_bound_for_noncompressed_network_unified_generalization_error_analysis_of_large_compressible_deep_neural_network", "_bibtex": "@inproceedings{\nSuzuki2020Compression,\ntitle={Compression based bound for non-compressed network: unified generalization error analysis of large compressible deep neural network},\nauthor={Taiji Suzuki and Hiroshi Abe and Tomoaki Nishimura},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=ByeGzlrKwH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/337879b9857962b93e0280dad73662756f75770b.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "ByeGzlrKwH", "replyto": "ByeGzlrKwH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2165/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2165/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574779148146, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2165/Reviewers"], "noninvitees": [], "tcdate": 1570237726767, "tmdate": 1574779148161, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2165/-/Official_Review"}}}, {"id": "HJgKa3Rijr", "original": null, "number": 7, "cdate": 1573805248678, "ddate": null, "tcdate": 1573805248678, "tmdate": 1573805248678, "tddate": null, "forum": "ByeGzlrKwH", "replyto": "BJl6G22osr", "invitation": "ICLR.cc/2020/Conference/Paper2165/-/Official_Comment", "content": {"title": "Thanks for the response", "comment": "It helps a lot!"}, "signatures": ["ICLR.cc/2020/Conference/Paper2165/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2165/AnonReviewer4", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["taiji@mist.i.u-tokyo.ac.jp", "abe@ipride.co.jp", "tomoaki.nishimura@nttdata.com"], "title": "Compression based bound for non-compressed network: unified generalization error analysis of large compressible deep neural network", "authors": ["Taiji Suzuki", "Hiroshi Abe", "Tomoaki Nishimura"], "pdf": "/pdf/82b66e53e3b33e8854c767c5ec1223b2d3eab3a2.pdf", "abstract": "One of the biggest issues in deep learning theory is the generalization ability of networks with huge model size. \nThe classical learning theory suggests that overparameterized models cause overfitting.\nHowever, practically used large deep models avoid overfitting, which is not well explained by the classical approaches.\nTo resolve this issue, several attempts have been made. \nAmong them, the compression based bound is one of the promising approaches. \nHowever, the compression based bound can be applied only to a compressed network, and it is not applicable to the non-compressed original network. \nIn this paper, we give a unified frame-work that can convert compression based bounds to those for non-compressed original networks.\nThe bound gives even better rate than the one for the compressed network by improving the bias term.\nBy establishing the unified frame-work, we can obtain a data dependent generalization error bound which gives a tighter evaluation than the data independent ones.\n", "keywords": ["Generalization error", "compression based bound", "local Rademacher complexity"], "paperhash": "suzuki|compression_based_bound_for_noncompressed_network_unified_generalization_error_analysis_of_large_compressible_deep_neural_network", "_bibtex": "@inproceedings{\nSuzuki2020Compression,\ntitle={Compression based bound for non-compressed network: unified generalization error analysis of large compressible deep neural network},\nauthor={Taiji Suzuki and Hiroshi Abe and Tomoaki Nishimura},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=ByeGzlrKwH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/337879b9857962b93e0280dad73662756f75770b.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "ByeGzlrKwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2165/Authors", "ICLR.cc/2020/Conference/Paper2165/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2165/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2165/Reviewers", "ICLR.cc/2020/Conference/Paper2165/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2165/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2165/Authors|ICLR.cc/2020/Conference/Paper2165/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504145384, "tmdate": 1576860555373, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2165/Authors", "ICLR.cc/2020/Conference/Paper2165/Reviewers", "ICLR.cc/2020/Conference/Paper2165/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2165/-/Official_Comment"}}}, {"id": "BJl6G22osr", "original": null, "number": 6, "cdate": 1573796884656, "ddate": null, "tcdate": 1573796884656, "tmdate": 1573797139628, "tddate": null, "forum": "ByeGzlrKwH", "replyto": "Hyepa7Msir", "invitation": "ICLR.cc/2020/Conference/Paper2165/-/Official_Comment", "content": {"title": "We have fixed the issue about Eq.(5)", "comment": "Thank you for your thorough exposition.\nWe have realized that you are absolutely correct.\nThis issue can be easily fixed by replacing $\\dot{R}_{r}(\\widehat{\\mathcal{F}} - \\widehat{\\mathcal{G}})$ with $\\dot{R}_{r}(\\psi(\\widehat{\\mathcal{F}}) - \\psi(\\widehat{\\mathcal{G}}))$ as an upper bound of Eq.(5). This is further bounded by\n$$\n\\dot{R}_{r}(\\psi(\\widehat{\\mathcal{F}}) -\\psi(\\widehat{\\mathcal{G}})) \\leq\n\\frac{C}{n}\n+\nC \\mathrm{E}_{D_n}\\left[ \\int_{1/n}^{\\hat{\\gamma}_n} \\sqrt{\\frac{\\log(N( \\widehat{\\mathcal{G}},\\|\\cdot\\|_{n},\\epsilon/2))}{n}} d \\epsilon\n+\n\\int_{1/n}^{\\hat{\\gamma}_n} \\sqrt{\\frac{\\log(N( \\widehat{\\mathcal{G}},\\|\\cdot\\|_{n},\\epsilon/2))}{n}} d\\epsilon \\right].\n$$\nPlease check Eq.(6) of the revised version. We also used this upper bound to bound $\\dot{R}_{r}(\\widehat{\\mathcal{F}} - \\widehat{\\mathcal{G}})$ in the previous version and all the remaining arguments (Theorems 2,3 and 4) are derived from the the Dudley integral bound appearing in the right hand side instead of $\\dot{R}_r$ itself. Therefore, this modification does not affect the remaining arguments. According to this modification, we fixed the main text and the proofs. They are just minor modifications.\n\nAlthough we used only the Dudley integral bound to show Theorems 2,3 and 4, we used the local Rademacher complexity $\\dot{R}_{\\dot{r}}(\\widehat{\\mathcal{F}} - \\widehat{\\mathcal{G}})$ in Theorem 1 to avoid a heavy notation related to the covering number appearing in the Dudley integral. Unexpectedly, this caused a mistake, but the $\\dot{R}_{\\dot{r}}$ term can be replaced by the Dudley integral anyway.\n\nFinally, we would like to remark that the local Rademacher complexity $\\dot{R}_{r}(\\widehat{\\mathcal{F}} - \\widehat{\\mathcal{G}})$ is still required to bridge $\\|f-g\\|_n$ and $\\|f-g\\|_{L_2}$. Thus, it remains in the main text.\n\nWe appreciate your insightful comment."}, "signatures": ["ICLR.cc/2020/Conference/Paper2165/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2165/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["taiji@mist.i.u-tokyo.ac.jp", "abe@ipride.co.jp", "tomoaki.nishimura@nttdata.com"], "title": "Compression based bound for non-compressed network: unified generalization error analysis of large compressible deep neural network", "authors": ["Taiji Suzuki", "Hiroshi Abe", "Tomoaki Nishimura"], "pdf": "/pdf/82b66e53e3b33e8854c767c5ec1223b2d3eab3a2.pdf", "abstract": "One of the biggest issues in deep learning theory is the generalization ability of networks with huge model size. \nThe classical learning theory suggests that overparameterized models cause overfitting.\nHowever, practically used large deep models avoid overfitting, which is not well explained by the classical approaches.\nTo resolve this issue, several attempts have been made. \nAmong them, the compression based bound is one of the promising approaches. \nHowever, the compression based bound can be applied only to a compressed network, and it is not applicable to the non-compressed original network. \nIn this paper, we give a unified frame-work that can convert compression based bounds to those for non-compressed original networks.\nThe bound gives even better rate than the one for the compressed network by improving the bias term.\nBy establishing the unified frame-work, we can obtain a data dependent generalization error bound which gives a tighter evaluation than the data independent ones.\n", "keywords": ["Generalization error", "compression based bound", "local Rademacher complexity"], "paperhash": "suzuki|compression_based_bound_for_noncompressed_network_unified_generalization_error_analysis_of_large_compressible_deep_neural_network", "_bibtex": "@inproceedings{\nSuzuki2020Compression,\ntitle={Compression based bound for non-compressed network: unified generalization error analysis of large compressible deep neural network},\nauthor={Taiji Suzuki and Hiroshi Abe and Tomoaki Nishimura},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=ByeGzlrKwH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/337879b9857962b93e0280dad73662756f75770b.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "ByeGzlrKwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2165/Authors", "ICLR.cc/2020/Conference/Paper2165/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2165/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2165/Reviewers", "ICLR.cc/2020/Conference/Paper2165/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2165/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2165/Authors|ICLR.cc/2020/Conference/Paper2165/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504145384, "tmdate": 1576860555373, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2165/Authors", "ICLR.cc/2020/Conference/Paper2165/Reviewers", "ICLR.cc/2020/Conference/Paper2165/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2165/-/Official_Comment"}}}, {"id": "Hyepa7Msir", "original": null, "number": 5, "cdate": 1573753797419, "ddate": null, "tcdate": 1573753797419, "tmdate": 1573756773051, "tddate": null, "forum": "ByeGzlrKwH", "replyto": "BkgeHbYcjr", "invitation": "ICLR.cc/2020/Conference/Paper2165/-/Official_Comment", "content": {"title": "How to prove the covering number inequality used in Eq. (5)?", "comment": "Thanks for the response. I agree that the Lipschitz continuity of $\\psi$ implies $|\\psi(f(x))-\\psi(g(x))|\\le|f(x)-g(x)|$ and $\\|\\psi(f)-\\psi(g)\\|_n\\le\\|f-g\\|_n$. However, to prove Eq. (5), it looks like the following inequality is used:\n$$\\mathcal{N}(\\{\\psi(f)-\\psi(g)|f\\in\\widehat{\\mathcal{F}},g\\in\\widehat{\\mathcal{G}},\\|f-g\\|_{L_2}\\le r\\},\\|\\cdot\\|_n,\\epsilon)\\le\\mathcal{N}(\\{f-g|f\\in\\widehat{\\mathcal{F}},g\\in\\widehat{\\mathcal{G}},\\|f-g\\|_{L_2}\\le r\\},\\|\\cdot\\|_n,\\epsilon).$$\nI do not see how to prove it? It is not enough to only use $\\|\\psi(f)-\\psi(g)\\|_n\\le\\|f-g\\|_n$; what we need to show should be something like given $h$ such that $\\|(f-g)-h\\|_n\\le\\epsilon$, it also holds that $\\|(\\psi(f)-\\psi(g))-\\phi(h)\\|_n\\le\\epsilon$ for some transformation $\\phi$.\n\nTo put it simply, I agree that given a function class $\\mathcal{F}$ and a Lipschitz function $\\psi$, the covering number of $\\psi(\\mathcal{F})$ is bounded by the covering number of $\\mathcal{F}$; however, I do not see why the above inequality is true, since we are considering $\\psi(f)-\\psi(g)$, not $\\psi(f-g)$.\n\nIn detail, in the original review I gave a special example where the above covering number inequality is not true. Consider the case $n=1$, and the set $A:=\\{(z+1,z)|z\\in[-1,+1]\\}$. Then $\\{x-y|(x,y)\\in A\\}$ only contains a single number $1$, and thus has covering number $1$. On the other hand, let $\\psi$ denote the sigmoid function $e^x/(1+e^x)$ which is Lipschitz, then $\\{\\psi(x)-\\psi(y)|(x,y)\\in A\\}=[\\frac{1}{2}-\\frac{1}{1+e},\\frac{\\sqrt{e}-1}{\\sqrt{e}+1}]$, whose covering number is larger than $1$. This example is too special since in $A$, the two coordinates are not independent, and probably we can prove the above covering number inequality when $f$ and $g$ are freely chosen from $\\widehat{\\mathcal{F}}$ and $\\widehat{\\mathcal{G}}$; however they further need to satisfy the condition $\\|f-g\\|_{L_2}\\le r$, which makes the situation more complicated.\n\nIn addition, it looks to me that Eq. (5) is important to bound the bias term, which is after all the key term this paper tries to bound, as the title suggests \"compression based bound for non-compressed networks\". Therefore the discussion of Eq. (5) is not just a technical one, but could affect the big picture."}, "signatures": ["ICLR.cc/2020/Conference/Paper2165/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2165/AnonReviewer4", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["taiji@mist.i.u-tokyo.ac.jp", "abe@ipride.co.jp", "tomoaki.nishimura@nttdata.com"], "title": "Compression based bound for non-compressed network: unified generalization error analysis of large compressible deep neural network", "authors": ["Taiji Suzuki", "Hiroshi Abe", "Tomoaki Nishimura"], "pdf": "/pdf/82b66e53e3b33e8854c767c5ec1223b2d3eab3a2.pdf", "abstract": "One of the biggest issues in deep learning theory is the generalization ability of networks with huge model size. \nThe classical learning theory suggests that overparameterized models cause overfitting.\nHowever, practically used large deep models avoid overfitting, which is not well explained by the classical approaches.\nTo resolve this issue, several attempts have been made. \nAmong them, the compression based bound is one of the promising approaches. \nHowever, the compression based bound can be applied only to a compressed network, and it is not applicable to the non-compressed original network. \nIn this paper, we give a unified frame-work that can convert compression based bounds to those for non-compressed original networks.\nThe bound gives even better rate than the one for the compressed network by improving the bias term.\nBy establishing the unified frame-work, we can obtain a data dependent generalization error bound which gives a tighter evaluation than the data independent ones.\n", "keywords": ["Generalization error", "compression based bound", "local Rademacher complexity"], "paperhash": "suzuki|compression_based_bound_for_noncompressed_network_unified_generalization_error_analysis_of_large_compressible_deep_neural_network", "_bibtex": "@inproceedings{\nSuzuki2020Compression,\ntitle={Compression based bound for non-compressed network: unified generalization error analysis of large compressible deep neural network},\nauthor={Taiji Suzuki and Hiroshi Abe and Tomoaki Nishimura},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=ByeGzlrKwH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/337879b9857962b93e0280dad73662756f75770b.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "ByeGzlrKwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2165/Authors", "ICLR.cc/2020/Conference/Paper2165/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2165/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2165/Reviewers", "ICLR.cc/2020/Conference/Paper2165/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2165/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2165/Authors|ICLR.cc/2020/Conference/Paper2165/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504145384, "tmdate": 1576860555373, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2165/Authors", "ICLR.cc/2020/Conference/Paper2165/Reviewers", "ICLR.cc/2020/Conference/Paper2165/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2165/-/Official_Comment"}}}, {"id": "SJeWa-KcoB", "original": null, "number": 4, "cdate": 1573716408840, "ddate": null, "tcdate": 1573716408840, "tmdate": 1573716408840, "tddate": null, "forum": "ByeGzlrKwH", "replyto": "Byevc7PaYS", "invitation": "ICLR.cc/2020/Conference/Paper2165/-/Official_Comment", "content": {"title": "Reply from authors", "comment": "Thank you for your suggestive comments, which clarify our paper's contribution.\n\n> One thing I hope the authors could clarify is the novelty in the proof of Theorem 1, because it seems the techniques used here such as entropy integral and peeling are all well-known. It would be better if the authors could give a comparison between this paper and papers with similar techniques.\n\nIn the literature, the local Rademacher complexity has been used to derive a fast leaning rate for a model the complexity of which is pre-determined. Howerver, the current setting is that the complexity of the trained model is more data-dependent, and the local Rademacher complexity is used to \"bridge\" the complexity of the trained network and the set of the small sized networks, G, while typical usage of the technique is directly bound the population excess risk. Hence, the usage of the local Rademacher complexity is quite different from the existing studies. In particular, we need to use the ratio type empirical process. Moreover, the bound is not restricted to the empirical risk minimization and it can be applied to any estimator as long as it produces a compressible network. We think our usage of the technique is interesting and this technique has not been employed in the literature of the generalization error analysis of overparameterized neural networks. Hence, although using the local Rademacher complexity might seem classical, we believe that it still provides an important insight to understand the generalization error analysis of deep learning.\n\n> Another question is, in the statement of Theorem 1, the term  is marked as part of 'main term' and the term  is marked as part of 'fast term'. I hope the authors could give a more detailed explanation of why \\dot{r} could be seen as a faster term than a constant, which seems not sensible to me.\n\nIf \\hat{r} is fixed independent of the sample size, then \\dot{r} is just a constant. However, by balancing the bias and variance terms, we may decrease \\hat{r} as the sample size increases. Indeed, we took $\\hat{r} =(\\sum_l m_l/L)^{-\\frac{1}{4/\\beta + 2(1-1/2\\alpha)}}$ in Theorem 4 which can be small if $m_l$ is relatively large (in particular, m_l increases as n goes up). However, we also realized that the terminology \"fast term\" would be confusing. We changed this to \"bias term.\"\n\n> Also, I hope the authors could explain why $\\Phi(\\dot{r})$ is a 'faster term', because it seems this term is not faster than $\\sqrt{1/n}$.\nLemma 2 shows that the main term of $\\Phi(r)$ is proportional to $O(r^{1-q}\\sqrt{1/n})$ under some assumptions. Thus, if $\\dot{r}$ is much smaller than 1, then $\\Phi(\\dot{r})$ can be smaller than \\sqrt{1/n}. At least, we can show r^{*2} is o(1/\\sqrt{n}) in a typical setting. This is why we used the terminology of \"faster term.\"\n\nFor the reasoning we have mentioned above, we employed the terminology \"faster term.\" However, by balancing the bias and variance term, some part of the faster term becomes the same rate as the main term (this is mainly due to the term related to $\\hat{r}$). We thought this terminology facilitates the understanding, but it could cause some confusion. Hence, we have changed the faster term to \"bias term.\" We also modified the expositions after Theorem 1 so that there does not appear confusion.\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2165/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2165/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["taiji@mist.i.u-tokyo.ac.jp", "abe@ipride.co.jp", "tomoaki.nishimura@nttdata.com"], "title": "Compression based bound for non-compressed network: unified generalization error analysis of large compressible deep neural network", "authors": ["Taiji Suzuki", "Hiroshi Abe", "Tomoaki Nishimura"], "pdf": "/pdf/82b66e53e3b33e8854c767c5ec1223b2d3eab3a2.pdf", "abstract": "One of the biggest issues in deep learning theory is the generalization ability of networks with huge model size. \nThe classical learning theory suggests that overparameterized models cause overfitting.\nHowever, practically used large deep models avoid overfitting, which is not well explained by the classical approaches.\nTo resolve this issue, several attempts have been made. \nAmong them, the compression based bound is one of the promising approaches. \nHowever, the compression based bound can be applied only to a compressed network, and it is not applicable to the non-compressed original network. \nIn this paper, we give a unified frame-work that can convert compression based bounds to those for non-compressed original networks.\nThe bound gives even better rate than the one for the compressed network by improving the bias term.\nBy establishing the unified frame-work, we can obtain a data dependent generalization error bound which gives a tighter evaluation than the data independent ones.\n", "keywords": ["Generalization error", "compression based bound", "local Rademacher complexity"], "paperhash": "suzuki|compression_based_bound_for_noncompressed_network_unified_generalization_error_analysis_of_large_compressible_deep_neural_network", "_bibtex": "@inproceedings{\nSuzuki2020Compression,\ntitle={Compression based bound for non-compressed network: unified generalization error analysis of large compressible deep neural network},\nauthor={Taiji Suzuki and Hiroshi Abe and Tomoaki Nishimura},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=ByeGzlrKwH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/337879b9857962b93e0280dad73662756f75770b.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "ByeGzlrKwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2165/Authors", "ICLR.cc/2020/Conference/Paper2165/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2165/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2165/Reviewers", "ICLR.cc/2020/Conference/Paper2165/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2165/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2165/Authors|ICLR.cc/2020/Conference/Paper2165/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504145384, "tmdate": 1576860555373, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2165/Authors", "ICLR.cc/2020/Conference/Paper2165/Reviewers", "ICLR.cc/2020/Conference/Paper2165/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2165/-/Official_Comment"}}}, {"id": "BJeq9Wt9jH", "original": null, "number": 3, "cdate": 1573716370419, "ddate": null, "tcdate": 1573716370419, "tmdate": 1573716370419, "tddate": null, "forum": "ByeGzlrKwH", "replyto": "Bkg4y5FCYS", "invitation": "ICLR.cc/2020/Conference/Paper2165/-/Official_Comment", "content": {"title": "Reply from authors ", "comment": "Thank you very much for your several comments. We have revised our paper according to your comments.\n\n> I appreciate that the outlines of the proofs are included in the main text, which helps the reader follow the ideas.\nThank you very much for your suggestion. We will definitely add the outline of the proof in the final version.\n\n> I think the paper could be improved immensely by some empirical analysis of the rank of compressed standard vision networks and rank of activation covariance matrices.\nThank you for pointing out this. We have added a numerical experiments about the eigenvalue distributions and intrinsic dimensionality of practically used VGG-19 network in Appendix D. We can see that the eigenvalues of the covariance and weight matrices decrease rapidly and the intrinsic dimensionality can be much smaller than the actual number of parameters. We also included a comparison with Arora et al. 2018. Our suggested quantity gives favorably tight evaluation compared with their numerical results.\n\nCitation issues:\n> In the introduction, paragraph 2, the authors cite Neyshabur et al. 2019 for the observation that networks generalize well despite being overparameterized. It seems like an odd choice.\n> Why is Barttlet\u2019s \u201899 paper [\u201cSize of the weights\u2026\u201d]  not cited? Or at least Neyshabur et al. 2015?\n\nThank you very much for pointing out the citation issue. We cited Neyshabur et al. 2019 as a good pointer to the recent literature of generalization error analysis and numerical experiments on overparameterized networks. However, we agree with your opinion that the papers you mentioned should be cited. We have cited them in the revised version.\n\n> Then the authors mention that classical learning theory cannot explain the phenomena mentioned above, [...]\n> The authors need to be more precise and add citations (I am assuming that the authors are talking about VC bounds for worst-case ERM generalization).\n\nYes, we intended that the \"classical learning theory\" is the VC-dimension type worst case analysis. The VC-dimension of networks with depth L and width W is lower bounded by L^2W^2, which yields O(\\sqrt{L^2W^2/n}) of the generalization error bound (Harvey et al. 2017). We have modified this part as \"well explained by a classical VC-dimension type theory (Harvey et al. 2017))\" by citing the paper, Harvey et al. (2017).\n\n> In the third paragraph, where the authors talk about norm-based bounds being lose, it seems that Nagarajan and Kolter 2019 should be cited (not only at the end), as well as Dzigaite and Roy 2017 (they look into the looseness of path-norm and margin-based bounds).\n\nThank you very much for the citation information. We could have missed some relevant papers, but we have cited them in the revised version.\n\n> Could the authors comment more on how the bound in Theorem 2 is superior to VC dimension bound and whether conditions under which the bound is tight are realistic for standard compressed vision networks.\n\nOur bound is always tighter than VC-dimension bound, but VC-dimension bound is recovered if we let \\alpha and \\beta = 0 small as an extreme case (this is not directly obtained by the presented theorems, but can be seen from the proof). As long as the singular value decay satisfies the assumptions, our bound can be tighter than VC-dimension bound (please remark that this does not necessarily imply the matrices are close to rank \"1\"). To show this is realistic, we have conducted numerical experiments (Appendix D). We can see that both of the weight matrices and the covariance matrices show rapid decrease of spectrum.\n\n\n> In general, I found the notation a bit hard to follow.\nThank you very much for reading our paper in details. We will do our best to make the notation more concise in the final version.\n\n> Other minor comments:\n> In section 2, marginal distributions over x and y are introduced. Are those used in the main text?\nThank you for pointing out this. They are used only in Assumptions 1 and 2 for clarifying the support of the distributions. We have moved the definition just before the assumptions.\n\n> Is that a definition of \\mu with the dot on top in assumption 5, or is this mu with the dot defined earlier? Using notation := would make it clearer whether the quantity is being defined.\nYes, you are absolutely correct. We have added \":=\"\" in the right hand side.\n\n> In Section 3, \u201cThe main difference from the\u2026\u201d paragraph, there is \\Psi(\\dot r) used. Where is that defined?\nWe appreciate your correction. This was typo. $\\Psi(\\dot r)$ is defined in Appendix A, but this was not defined before Section 3. We replaced this by $\\dot{R}_{\\dot{r}}(\\widehat{\\mathcal{F}} - \\widehat{\\mathcal{G}})$ in the revised version.\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2165/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2165/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["taiji@mist.i.u-tokyo.ac.jp", "abe@ipride.co.jp", "tomoaki.nishimura@nttdata.com"], "title": "Compression based bound for non-compressed network: unified generalization error analysis of large compressible deep neural network", "authors": ["Taiji Suzuki", "Hiroshi Abe", "Tomoaki Nishimura"], "pdf": "/pdf/82b66e53e3b33e8854c767c5ec1223b2d3eab3a2.pdf", "abstract": "One of the biggest issues in deep learning theory is the generalization ability of networks with huge model size. \nThe classical learning theory suggests that overparameterized models cause overfitting.\nHowever, practically used large deep models avoid overfitting, which is not well explained by the classical approaches.\nTo resolve this issue, several attempts have been made. \nAmong them, the compression based bound is one of the promising approaches. \nHowever, the compression based bound can be applied only to a compressed network, and it is not applicable to the non-compressed original network. \nIn this paper, we give a unified frame-work that can convert compression based bounds to those for non-compressed original networks.\nThe bound gives even better rate than the one for the compressed network by improving the bias term.\nBy establishing the unified frame-work, we can obtain a data dependent generalization error bound which gives a tighter evaluation than the data independent ones.\n", "keywords": ["Generalization error", "compression based bound", "local Rademacher complexity"], "paperhash": "suzuki|compression_based_bound_for_noncompressed_network_unified_generalization_error_analysis_of_large_compressible_deep_neural_network", "_bibtex": "@inproceedings{\nSuzuki2020Compression,\ntitle={Compression based bound for non-compressed network: unified generalization error analysis of large compressible deep neural network},\nauthor={Taiji Suzuki and Hiroshi Abe and Tomoaki Nishimura},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=ByeGzlrKwH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/337879b9857962b93e0280dad73662756f75770b.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "ByeGzlrKwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2165/Authors", "ICLR.cc/2020/Conference/Paper2165/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2165/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2165/Reviewers", "ICLR.cc/2020/Conference/Paper2165/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2165/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2165/Authors|ICLR.cc/2020/Conference/Paper2165/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504145384, "tmdate": 1576860555373, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2165/Authors", "ICLR.cc/2020/Conference/Paper2165/Reviewers", "ICLR.cc/2020/Conference/Paper2165/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2165/-/Official_Comment"}}}, {"id": "BkgeHbYcjr", "original": null, "number": 2, "cdate": 1573716279541, "ddate": null, "tcdate": 1573716279541, "tmdate": 1573716301715, "tddate": null, "forum": "ByeGzlrKwH", "replyto": "BygRLlp3cS", "invitation": "ICLR.cc/2020/Conference/Paper2165/-/Official_Comment", "content": {"title": "Reply from authors", "comment": "Thank you for your suggestive comments.\n\n(1)\nQ: How to derive Eq. (5)?\nA: Thank you for clarifying the details. This is due to the Lipschitz continuity of the loss $\\psi$. Indeed, we have that $|\\psi(f(x)) - \\psi(g(x))| \\leq |f(x) - g(x)|$ which yields $\\|\\psi(f) - \\psi(g)\\|_n \\leq \\|f - g\\|_n$. Then, we obtain Eq.(5). This is explained just after the equation as \"where we used 1-Lipschitz continuity of the loss function $\\psi$ in the \ufb01rst inequality\". However, we have added more detailed exposition to clarify this point in the revised version.\nAs you pointed out, this is not true in general if it does not have Lipschitz continuity. The condition \"|f-g|_{L_2} <= r\" is not used to derive Eq.(5) itself, but only the Lipschitz continuity is used.\n\n(2)\nQ: it looks like the local Rademacher complexity of F can be controlled directly using Lemma 2. The question then is how compression helps in the analysis?\nA: Yes, you are right. The local Rademacher complexity of F appears only in the bias term. The variance term (the main term) is controlled by the (global) Rademacher complexity of G. If we don't consider compression, the main term (the global Rademacher complexity of G) must be replaced by that of \"F\" which is much larger than that of G. However, through compression, this becomes the complexity of G, which yields large improvement. We would like to notice that a naive analysis without local Rademacher complexity produces an additional term of a global Rademacher complexity of F to bound the bias term. Our technique resolves this issue by using the ratio type empirical process.\n\n\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2165/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2165/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["taiji@mist.i.u-tokyo.ac.jp", "abe@ipride.co.jp", "tomoaki.nishimura@nttdata.com"], "title": "Compression based bound for non-compressed network: unified generalization error analysis of large compressible deep neural network", "authors": ["Taiji Suzuki", "Hiroshi Abe", "Tomoaki Nishimura"], "pdf": "/pdf/82b66e53e3b33e8854c767c5ec1223b2d3eab3a2.pdf", "abstract": "One of the biggest issues in deep learning theory is the generalization ability of networks with huge model size. \nThe classical learning theory suggests that overparameterized models cause overfitting.\nHowever, practically used large deep models avoid overfitting, which is not well explained by the classical approaches.\nTo resolve this issue, several attempts have been made. \nAmong them, the compression based bound is one of the promising approaches. \nHowever, the compression based bound can be applied only to a compressed network, and it is not applicable to the non-compressed original network. \nIn this paper, we give a unified frame-work that can convert compression based bounds to those for non-compressed original networks.\nThe bound gives even better rate than the one for the compressed network by improving the bias term.\nBy establishing the unified frame-work, we can obtain a data dependent generalization error bound which gives a tighter evaluation than the data independent ones.\n", "keywords": ["Generalization error", "compression based bound", "local Rademacher complexity"], "paperhash": "suzuki|compression_based_bound_for_noncompressed_network_unified_generalization_error_analysis_of_large_compressible_deep_neural_network", "_bibtex": "@inproceedings{\nSuzuki2020Compression,\ntitle={Compression based bound for non-compressed network: unified generalization error analysis of large compressible deep neural network},\nauthor={Taiji Suzuki and Hiroshi Abe and Tomoaki Nishimura},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=ByeGzlrKwH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/337879b9857962b93e0280dad73662756f75770b.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "ByeGzlrKwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2165/Authors", "ICLR.cc/2020/Conference/Paper2165/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2165/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2165/Reviewers", "ICLR.cc/2020/Conference/Paper2165/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2165/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2165/Authors|ICLR.cc/2020/Conference/Paper2165/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504145384, "tmdate": 1576860555373, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2165/Authors", "ICLR.cc/2020/Conference/Paper2165/Reviewers", "ICLR.cc/2020/Conference/Paper2165/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2165/-/Official_Comment"}}}, {"id": "SkeOxWYcjS", "original": null, "number": 1, "cdate": 1573716208321, "ddate": null, "tcdate": 1573716208321, "tmdate": 1573716208321, "tddate": null, "forum": "ByeGzlrKwH", "replyto": "ByeGzlrKwH", "invitation": "ICLR.cc/2020/Conference/Paper2165/-/Official_Comment", "content": {"title": "Revised version has been uploaded", "comment": "Dear reviewers,\n\nThank you very much for your insightful comments. We have revised our manuscript according to your comments. The main modifications are as follows:\n1. We have added numerical evaluation in Appendix D. It evaluates the eigenvalue distribution of VGG-19 trained on CIFAR-10 and computed the intrinsic dimensionality of that.\n2. We have added some missing important citations.\n3. We have modified the intuitive explanations of the general bound (Theorem 1).\n\nSincerely yours,\nAuthors."}, "signatures": ["ICLR.cc/2020/Conference/Paper2165/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2165/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["taiji@mist.i.u-tokyo.ac.jp", "abe@ipride.co.jp", "tomoaki.nishimura@nttdata.com"], "title": "Compression based bound for non-compressed network: unified generalization error analysis of large compressible deep neural network", "authors": ["Taiji Suzuki", "Hiroshi Abe", "Tomoaki Nishimura"], "pdf": "/pdf/82b66e53e3b33e8854c767c5ec1223b2d3eab3a2.pdf", "abstract": "One of the biggest issues in deep learning theory is the generalization ability of networks with huge model size. \nThe classical learning theory suggests that overparameterized models cause overfitting.\nHowever, practically used large deep models avoid overfitting, which is not well explained by the classical approaches.\nTo resolve this issue, several attempts have been made. \nAmong them, the compression based bound is one of the promising approaches. \nHowever, the compression based bound can be applied only to a compressed network, and it is not applicable to the non-compressed original network. \nIn this paper, we give a unified frame-work that can convert compression based bounds to those for non-compressed original networks.\nThe bound gives even better rate than the one for the compressed network by improving the bias term.\nBy establishing the unified frame-work, we can obtain a data dependent generalization error bound which gives a tighter evaluation than the data independent ones.\n", "keywords": ["Generalization error", "compression based bound", "local Rademacher complexity"], "paperhash": "suzuki|compression_based_bound_for_noncompressed_network_unified_generalization_error_analysis_of_large_compressible_deep_neural_network", "_bibtex": "@inproceedings{\nSuzuki2020Compression,\ntitle={Compression based bound for non-compressed network: unified generalization error analysis of large compressible deep neural network},\nauthor={Taiji Suzuki and Hiroshi Abe and Tomoaki Nishimura},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=ByeGzlrKwH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/337879b9857962b93e0280dad73662756f75770b.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "ByeGzlrKwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2165/Authors", "ICLR.cc/2020/Conference/Paper2165/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2165/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2165/Reviewers", "ICLR.cc/2020/Conference/Paper2165/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2165/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2165/Authors|ICLR.cc/2020/Conference/Paper2165/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504145384, "tmdate": 1576860555373, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2165/Authors", "ICLR.cc/2020/Conference/Paper2165/Reviewers", "ICLR.cc/2020/Conference/Paper2165/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2165/-/Official_Comment"}}}, {"id": "Byevc7PaYS", "original": null, "number": 1, "cdate": 1571808143293, "ddate": null, "tcdate": 1571808143293, "tmdate": 1572972374660, "tddate": null, "forum": "ByeGzlrKwH", "replyto": "ByeGzlrKwH", "invitation": "ICLR.cc/2020/Conference/Paper2165/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "N/A", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper provides generalization bound based on the compression arguments. A key contribution is that instead of bounding the\npopulation risk for the compressed model, this paper manages to give bounds on the non-compressed network following a unified analysis framework. Also, this paper applies the unified framework to low-rank assumption on weigh matrices and covariance matrices.\n\nOverall, I believe this paper should be accepted because of its contribution to our understanding of generalization theory. The central contribution here is using localized Rademacher complexity to bound the $L_2$ norm between original network and compressed network. However, I think there are several issues unclear and in need of clarification.\n\nOne thing I hope the authors could clarify is the novelty in the proof of Theorem 1, because it seems the techniques used here such as entropy integral and peeling are all well-known. It would be better if the authors could give a comparison between this paper and papers with similar techniques.\n\nAnother question is, in the statement of Theorem 1, the term $\\sqrt{M \\frac{2t}{n}}$ is marked as part of 'main term' and the term $C \\dot r \\sqrt{\\frac{t}{n}}$ is marked as part of 'fast term'. I hope the authors could give a more detailed explanation of why $\\dot r$ could be seen as a faster term than a constant, which seems not sensible to me. \n\nAlso, I hope the authors could explain why $\\Phi(\\sqrt{2(\\hat r ^2 + r ^2 _*)})$ is a 'faster term', because it seems this term is not faster than $\\sqrt{\\frac{1}{n}}$.\n\nThe questions above essentially concern how sharp this general bound could be. I would appreciate it if the authors could give a thorough response to questions mentioned above. This can help me achieve a better understanding and a more precise evaluation on this paper."}, "signatures": ["ICLR.cc/2020/Conference/Paper2165/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2165/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["taiji@mist.i.u-tokyo.ac.jp", "abe@ipride.co.jp", "tomoaki.nishimura@nttdata.com"], "title": "Compression based bound for non-compressed network: unified generalization error analysis of large compressible deep neural network", "authors": ["Taiji Suzuki", "Hiroshi Abe", "Tomoaki Nishimura"], "pdf": "/pdf/82b66e53e3b33e8854c767c5ec1223b2d3eab3a2.pdf", "abstract": "One of the biggest issues in deep learning theory is the generalization ability of networks with huge model size. \nThe classical learning theory suggests that overparameterized models cause overfitting.\nHowever, practically used large deep models avoid overfitting, which is not well explained by the classical approaches.\nTo resolve this issue, several attempts have been made. \nAmong them, the compression based bound is one of the promising approaches. \nHowever, the compression based bound can be applied only to a compressed network, and it is not applicable to the non-compressed original network. \nIn this paper, we give a unified frame-work that can convert compression based bounds to those for non-compressed original networks.\nThe bound gives even better rate than the one for the compressed network by improving the bias term.\nBy establishing the unified frame-work, we can obtain a data dependent generalization error bound which gives a tighter evaluation than the data independent ones.\n", "keywords": ["Generalization error", "compression based bound", "local Rademacher complexity"], "paperhash": "suzuki|compression_based_bound_for_noncompressed_network_unified_generalization_error_analysis_of_large_compressible_deep_neural_network", "_bibtex": "@inproceedings{\nSuzuki2020Compression,\ntitle={Compression based bound for non-compressed network: unified generalization error analysis of large compressible deep neural network},\nauthor={Taiji Suzuki and Hiroshi Abe and Tomoaki Nishimura},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=ByeGzlrKwH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/337879b9857962b93e0280dad73662756f75770b.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "ByeGzlrKwH", "replyto": "ByeGzlrKwH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2165/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2165/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574779148146, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2165/Reviewers"], "noninvitees": [], "tcdate": 1570237726767, "tmdate": 1574779148161, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2165/-/Official_Review"}}}, {"id": "BygRLlp3cS", "original": null, "number": 3, "cdate": 1572814934382, "ddate": null, "tcdate": 1572814934382, "tmdate": 1572972374569, "tddate": null, "forum": "ByeGzlrKwH", "replyto": "ByeGzlrKwH", "invitation": "ICLR.cc/2020/Conference/Paper2165/-/Official_Review", "content": {"rating": "6: Weak Accept", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "N/A", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper obtains a compression-based generalization bound (Theorem 1) for the original network, while prior work gives bounds for the compressed network. The general bound given by Theorem 1 is further applied to networks with low-rank weight matrices (Theorem 2 and Corollary 1) or low-rank covariance matrices (Theorem 3 and 4). In some cases, the bound given by Theorem 1 for the original network could be better than the bound for the compressed network.\n\nIn terms of proof techniques, Lemma 2 is a general result to control the local Rademacher complexity using upper bounds on the covering numbers, which is interesting and could be useful in other problems.\n\nOn the other hand, there are two technical concerns.\n(1) In eq. (5), the covering number of {\\phi(f)-\\phi(g)} is bounded by the covering number of {f-g}, which is not necessarily true. For example, in the 1-dimensional case, it is possible that f-g is always 1, while \\phi(f)-\\phi(g) is not a constant. This example might appear since f and g are not freely chosen from F and G; they further need to satisfy the condition that |f-g|_{L_2} is bounded by r. If the claim in eq. (5) is indeed true, a proof is needed.\n(2) Despite the issue in (1), many bounds in the paper may actually be okay, since in the proofs the covering numbers of F (the original networks) are used (e.g., in eq. (6) and Lemma 2). Therefore it looks like the local Rademacher complexity of F can be controlled directly using Lemma 2. The question then is how compression helps in the analysis?\n\nI hope the above points can be clarified, and I would like to participate in the discussion."}, "signatures": ["ICLR.cc/2020/Conference/Paper2165/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2165/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["taiji@mist.i.u-tokyo.ac.jp", "abe@ipride.co.jp", "tomoaki.nishimura@nttdata.com"], "title": "Compression based bound for non-compressed network: unified generalization error analysis of large compressible deep neural network", "authors": ["Taiji Suzuki", "Hiroshi Abe", "Tomoaki Nishimura"], "pdf": "/pdf/82b66e53e3b33e8854c767c5ec1223b2d3eab3a2.pdf", "abstract": "One of the biggest issues in deep learning theory is the generalization ability of networks with huge model size. \nThe classical learning theory suggests that overparameterized models cause overfitting.\nHowever, practically used large deep models avoid overfitting, which is not well explained by the classical approaches.\nTo resolve this issue, several attempts have been made. \nAmong them, the compression based bound is one of the promising approaches. \nHowever, the compression based bound can be applied only to a compressed network, and it is not applicable to the non-compressed original network. \nIn this paper, we give a unified frame-work that can convert compression based bounds to those for non-compressed original networks.\nThe bound gives even better rate than the one for the compressed network by improving the bias term.\nBy establishing the unified frame-work, we can obtain a data dependent generalization error bound which gives a tighter evaluation than the data independent ones.\n", "keywords": ["Generalization error", "compression based bound", "local Rademacher complexity"], "paperhash": "suzuki|compression_based_bound_for_noncompressed_network_unified_generalization_error_analysis_of_large_compressible_deep_neural_network", "_bibtex": "@inproceedings{\nSuzuki2020Compression,\ntitle={Compression based bound for non-compressed network: unified generalization error analysis of large compressible deep neural network},\nauthor={Taiji Suzuki and Hiroshi Abe and Tomoaki Nishimura},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=ByeGzlrKwH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/337879b9857962b93e0280dad73662756f75770b.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "ByeGzlrKwH", "replyto": "ByeGzlrKwH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2165/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2165/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574779148146, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2165/Reviewers"], "noninvitees": [], "tcdate": 1570237726767, "tmdate": 1574779148161, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2165/-/Official_Review"}}}], "count": 13}