{"notes": [{"id": "HJxEhREKDH", "original": "Skek_DK_vr", "number": 1353, "cdate": 1569439404129, "ddate": null, "tcdate": 1569439404129, "tmdate": 1583912044452, "tddate": null, "forum": "HJxEhREKDH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["knowzou@ucla.edu", "plong@google.com", "qgu@cs.ucla.edu"], "title": "On the Global Convergence  of Training Deep Linear ResNets", "authors": ["Difan Zou", "Philip M. Long", "Quanquan Gu"], "pdf": "/pdf/40f991e7d57fcf3070e9cc9829c477216a40cc72.pdf", "TL;DR": "Under certain condition on the input and output linear transformations, both GD and SGD can achieve global convergence for training deep linear ResNets.", "abstract": "We study the convergence of gradient descent (GD) and stochastic gradient descent (SGD) for training $L$-hidden-layer linear residual networks (ResNets). We prove that for training deep residual networks with certain linear transformations at input and output layers, which are fixed throughout training, both GD and SGD with zero initialization on all hidden weights can converge to the global minimum of the training loss. Moreover, when specializing to appropriate Gaussian random linear transformations, GD and SGD provably optimize wide enough deep linear ResNets. Compared with the global convergence result of GD for training standard deep linear networks \\citep{du2019width}, our condition on the neural network width is sharper by a factor of $O(\\kappa L)$, where $\\kappa$ denotes the condition number of the covariance matrix of the training data. We further propose a modified identity input and output transformations, and show that a $(d+k)$-wide neural network is sufficient to guarantee the global convergence of GD/SGD, where $d,k$ are the input and output dimensions respectively.", "keywords": [], "paperhash": "zou|on_the_global_convergence_of_training_deep_linear_resnets", "_bibtex": "@inproceedings{\nZou2020On,\ntitle={On the Global Convergence  of Training Deep Linear ResNets},\nauthor={Difan Zou and Philip M. Long and Quanquan Gu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJxEhREKDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e9f06e9d1384a7cd6235301dbc5b970844fe863a.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 13, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "4iv2us5Wol", "original": null, "number": 1, "cdate": 1576798721291, "ddate": null, "tcdate": 1576798721291, "tmdate": 1576800915306, "tddate": null, "forum": "HJxEhREKDH", "replyto": "HJxEhREKDH", "invitation": "ICLR.cc/2020/Conference/Paper1353/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "This paper provides further analysis of convergence in deep linear networks. I recommend acceptance. ", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["knowzou@ucla.edu", "plong@google.com", "qgu@cs.ucla.edu"], "title": "On the Global Convergence  of Training Deep Linear ResNets", "authors": ["Difan Zou", "Philip M. Long", "Quanquan Gu"], "pdf": "/pdf/40f991e7d57fcf3070e9cc9829c477216a40cc72.pdf", "TL;DR": "Under certain condition on the input and output linear transformations, both GD and SGD can achieve global convergence for training deep linear ResNets.", "abstract": "We study the convergence of gradient descent (GD) and stochastic gradient descent (SGD) for training $L$-hidden-layer linear residual networks (ResNets). We prove that for training deep residual networks with certain linear transformations at input and output layers, which are fixed throughout training, both GD and SGD with zero initialization on all hidden weights can converge to the global minimum of the training loss. Moreover, when specializing to appropriate Gaussian random linear transformations, GD and SGD provably optimize wide enough deep linear ResNets. Compared with the global convergence result of GD for training standard deep linear networks \\citep{du2019width}, our condition on the neural network width is sharper by a factor of $O(\\kappa L)$, where $\\kappa$ denotes the condition number of the covariance matrix of the training data. We further propose a modified identity input and output transformations, and show that a $(d+k)$-wide neural network is sufficient to guarantee the global convergence of GD/SGD, where $d,k$ are the input and output dimensions respectively.", "keywords": [], "paperhash": "zou|on_the_global_convergence_of_training_deep_linear_resnets", "_bibtex": "@inproceedings{\nZou2020On,\ntitle={On the Global Convergence  of Training Deep Linear ResNets},\nauthor={Difan Zou and Philip M. Long and Quanquan Gu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJxEhREKDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e9f06e9d1384a7cd6235301dbc5b970844fe863a.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "HJxEhREKDH", "replyto": "HJxEhREKDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795720908, "tmdate": 1576800271825, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1353/-/Decision"}}}, {"id": "SyluQ28J9S", "original": null, "number": 3, "cdate": 1571937312059, "ddate": null, "tcdate": 1571937312059, "tmdate": 1574527740942, "tddate": null, "forum": "HJxEhREKDH", "replyto": "HJxEhREKDH", "invitation": "ICLR.cc/2020/Conference/Paper1353/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #2", "review": "\n*Summary* \nThis paper deals with the global convergence of deep linear ResNets. The author show that under some initialization conditions for the first and the last layer (that are not optimized !) GD and SGD does converge to a global minimum of the min squared error. The closed related work seems to be Bartlett et al. 2019 that study the convergence of GD in the case of linear networks.  \n \n*Decision* \nOn issue for Bartlett et al. 2019 was that they required a condition on the initial suboptimality to be small in order to insure convergence. This work shows that in the case of linear ResNets, with a well chosen initialization, a similar condition holds with high probability. \nI think this paper is interesting for the ICLR community and seems to provide good contributions (like for instance the analysis for SGD). However I have some question that I would like the authors to answer.\n*Questions*\n- In Proposition 3.3 you show an upperbound on $\\sigma_{\\min}(B)$ in order to show in Corollary 3.4  that the condition to apply Theorem 3.1 is true. However, it seems to me that you need a lower bound on  $\\sigma_{\\min}(B)$ to prove that (A.3) is true.\n- To what extent the proof of Theorem 3.1 uses the proof technique of Bartlett et al. 2019 ?\n- With you small enough conditions, are you in the lazy regime described by Chizat, Lenaic, Edouard Oyallon, and Francis Bach. \"On Lazy Training in Differentiable Programming.\" (2019). NeurIPS\n- In Theorem 3.1 What is $e$ ?\n- Could you prove the same result as Theorem  3.1 and 3.6 but with an inequality constraint on the step size? It seems very restrictive to me to ask a stepsize to be exactly equal to a quantity. If you cannot relax you equality constraint into an inequality constraint, can you at least show that your result hold for step size in an interval?\n\n\n=== After rebuttal ===\nThank you for this detailed answer. It confirms that this paper is of interest to the ICLR community.\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1353/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1353/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["knowzou@ucla.edu", "plong@google.com", "qgu@cs.ucla.edu"], "title": "On the Global Convergence  of Training Deep Linear ResNets", "authors": ["Difan Zou", "Philip M. Long", "Quanquan Gu"], "pdf": "/pdf/40f991e7d57fcf3070e9cc9829c477216a40cc72.pdf", "TL;DR": "Under certain condition on the input and output linear transformations, both GD and SGD can achieve global convergence for training deep linear ResNets.", "abstract": "We study the convergence of gradient descent (GD) and stochastic gradient descent (SGD) for training $L$-hidden-layer linear residual networks (ResNets). We prove that for training deep residual networks with certain linear transformations at input and output layers, which are fixed throughout training, both GD and SGD with zero initialization on all hidden weights can converge to the global minimum of the training loss. Moreover, when specializing to appropriate Gaussian random linear transformations, GD and SGD provably optimize wide enough deep linear ResNets. Compared with the global convergence result of GD for training standard deep linear networks \\citep{du2019width}, our condition on the neural network width is sharper by a factor of $O(\\kappa L)$, where $\\kappa$ denotes the condition number of the covariance matrix of the training data. We further propose a modified identity input and output transformations, and show that a $(d+k)$-wide neural network is sufficient to guarantee the global convergence of GD/SGD, where $d,k$ are the input and output dimensions respectively.", "keywords": [], "paperhash": "zou|on_the_global_convergence_of_training_deep_linear_resnets", "_bibtex": "@inproceedings{\nZou2020On,\ntitle={On the Global Convergence  of Training Deep Linear ResNets},\nauthor={Difan Zou and Philip M. Long and Quanquan Gu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJxEhREKDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e9f06e9d1384a7cd6235301dbc5b970844fe863a.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJxEhREKDH", "replyto": "HJxEhREKDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1353/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1353/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575833652820, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1353/Reviewers"], "noninvitees": [], "tcdate": 1570237738610, "tmdate": 1575833652836, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1353/-/Official_Review"}}}, {"id": "rkgnmE0ptS", "original": null, "number": 2, "cdate": 1571836964251, "ddate": null, "tcdate": 1571836964251, "tmdate": 1574418232897, "tddate": null, "forum": "HJxEhREKDH", "replyto": "HJxEhREKDH", "invitation": "ICLR.cc/2020/Conference/Paper1353/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #1", "review": "This paper studies the convergence properties of GD and SGD on deep linear resnets. The authors prove that, under certain conditions on the input and output transformations and with zero initialization, GD and SGD converges to global minima. The results derived in this paper show that the condition on the width of a deep linear resnet is less strict (by a factor O(L kappa) where L is the depth and kappa the condition number of the training data) that a network without residual connection. Overall, the paper is well-written and the analysis is fairly standard and easy to follow (I checked most of the theorems except for Lemma A.2 and the results for stochastic gradients). The authors do not clearly contrast their results to prior work (especially Bartlett et al. (2019) and Arora et al. (2019a)) and I\u2019m therefore not convinced the final result brings any new insight. Please address this issue in your rebuttal. I will reconsider my score if the authors can provide a satisfactory answer.\n\nComparison to Du & Hu\nThe authors claim that their bound shows an improvement by a factor of O(kappa L) over the deep linear network (without residual connections) analyzed in Du & Hu.\n1) I don\u2019t think this is explicitly stated in the paper but are the initialization conditions and the assumed distance to the optimum the same in both papers?\n2) These results are obviously worst-case bounds and the analysis used in both papers is different to some extent. Couldn\u2019t you re-derived this result by adapting your own analysis?\n3) Is there any theoretical proof or empirical evidence showing that resnets do indeed scale better w.r.t. to the condition number of the data?\n\nComparison to prior work\nThe authors mention the work of Bartlett et al. (2019) and Arora et al. (2019a) but it is never very clear what the real differences are.\n1) Regarding Bartlett et al. (2019), you say that \u201cTheorem 3.1 can imply the convergence result in Bartlett et al. (2019).\u201d. Does the result of Theorem 3.1 provides a tighter bound in terms of L or kappa (when using the same initialization conditions)? \n2) Arora et al. (2019a): You explain that they showed that \u201cGD converges under substantially weaker conditions\u201d. How much weaker are these conditions? How do they compare to the \u201cmodified identity transformation\u201d that allows you to obtain a global convergence rate. How does your final result compare to Arora et al. (2019a)? The analysis of Arora et al. (2019a) requires a balanced-ness condition, is this substituted by a different condition in your analysis?\n3) Finally, Allen-Zhu et al. (2019) already has some results on deep resnets although their analysis is for a heavily over-parametrized regime. Can you still comment on how their results differ from yours?\n\nA & B are fixed matrices initialized from a Gaussian distribution. How essential is this condition? Assuming for simplicity that A and B are square matrices. Can I not set A=B=identity and still satisfy the condition in Theorem 1? The term on the RHS would be 1 so then the initialization would need to scale as a function of the spectrum of the data matrix X.\n\nResNet vs LinearNet\nWhere does your proof break down for a linear net without residual connections, i.e. where does the proof absolutely need the identify matrices. Looking at the proof, it seems to me, one could change the following in order to still obtain a similar result:\n1) If we consider a network B \\prod_l W_l AX, the proof of proposition 3.3 could be unchanged if the W matrices are initialized to identify instead of zero.\n2) The lower bound on \\sigma^2_min(I + \\tau W_l) would instead be replaced by a lower on \\sigma^2_min(W_l)=\\lambda_min(W_l). Is this where  one can see the benefit of having residual connections?\n\nProposition 3.3\nThe bound on the singular values of the matrices A and B is vacuous for square matrices, in which case I believe the theorem does not hold. Can you comment on this?\n\nExtension stochastic setting\nI only skimmed at the proof but the extension looks fairly straightforward. Can you comment on how difficult this derivation is compared to the deterministic setting?\n\nExtensions\nYou claim \"can potentially provide meaningful insights to the convergence analysis of deep non-linear ResNets.\" although this is not obvious as the landscape of such networks can be very different. Can you elaborate?\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1353/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1353/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["knowzou@ucla.edu", "plong@google.com", "qgu@cs.ucla.edu"], "title": "On the Global Convergence  of Training Deep Linear ResNets", "authors": ["Difan Zou", "Philip M. Long", "Quanquan Gu"], "pdf": "/pdf/40f991e7d57fcf3070e9cc9829c477216a40cc72.pdf", "TL;DR": "Under certain condition on the input and output linear transformations, both GD and SGD can achieve global convergence for training deep linear ResNets.", "abstract": "We study the convergence of gradient descent (GD) and stochastic gradient descent (SGD) for training $L$-hidden-layer linear residual networks (ResNets). We prove that for training deep residual networks with certain linear transformations at input and output layers, which are fixed throughout training, both GD and SGD with zero initialization on all hidden weights can converge to the global minimum of the training loss. Moreover, when specializing to appropriate Gaussian random linear transformations, GD and SGD provably optimize wide enough deep linear ResNets. Compared with the global convergence result of GD for training standard deep linear networks \\citep{du2019width}, our condition on the neural network width is sharper by a factor of $O(\\kappa L)$, where $\\kappa$ denotes the condition number of the covariance matrix of the training data. We further propose a modified identity input and output transformations, and show that a $(d+k)$-wide neural network is sufficient to guarantee the global convergence of GD/SGD, where $d,k$ are the input and output dimensions respectively.", "keywords": [], "paperhash": "zou|on_the_global_convergence_of_training_deep_linear_resnets", "_bibtex": "@inproceedings{\nZou2020On,\ntitle={On the Global Convergence  of Training Deep Linear ResNets},\nauthor={Difan Zou and Philip M. Long and Quanquan Gu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJxEhREKDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e9f06e9d1384a7cd6235301dbc5b970844fe863a.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJxEhREKDH", "replyto": "HJxEhREKDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1353/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1353/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575833652820, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1353/Reviewers"], "noninvitees": [], "tcdate": 1570237738610, "tmdate": 1575833652836, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1353/-/Official_Review"}}}, {"id": "HJg_GdOTKr", "original": null, "number": 1, "cdate": 1571813391974, "ddate": null, "tcdate": 1571813391974, "tmdate": 1574261628550, "tddate": null, "forum": "HJxEhREKDH", "replyto": "HJxEhREKDH", "invitation": "ICLR.cc/2020/Conference/Paper1353/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #3", "review": "In this paper, the authors study the convergence of (stochastic) gradient descent in training deep linear residual networks, where linear transformation at input and output layers are fixed and matrices in other layers are trained. They first establish a global convergence of GD/SGD under some conditions on the fixed linear transformations. They they showed that for Gaussian random input and output transformation, global convergence still holds under conditions on the width of networks strictly milder than the literature. Linear convergence rate of SG/SGD are also established.\n\nThe paper is well written. The results seem novel and interesting.\n\nIt would be nice if the authors can give intuition why the input and output layer transformations need to be fixed in the analysis. What if happen if these matrices vary along the optimization process?\n\n----------------------\nAfter rebuttal:\n\nI have read the authors' response. I would like to keep my original score.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1353/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1353/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["knowzou@ucla.edu", "plong@google.com", "qgu@cs.ucla.edu"], "title": "On the Global Convergence  of Training Deep Linear ResNets", "authors": ["Difan Zou", "Philip M. Long", "Quanquan Gu"], "pdf": "/pdf/40f991e7d57fcf3070e9cc9829c477216a40cc72.pdf", "TL;DR": "Under certain condition on the input and output linear transformations, both GD and SGD can achieve global convergence for training deep linear ResNets.", "abstract": "We study the convergence of gradient descent (GD) and stochastic gradient descent (SGD) for training $L$-hidden-layer linear residual networks (ResNets). We prove that for training deep residual networks with certain linear transformations at input and output layers, which are fixed throughout training, both GD and SGD with zero initialization on all hidden weights can converge to the global minimum of the training loss. Moreover, when specializing to appropriate Gaussian random linear transformations, GD and SGD provably optimize wide enough deep linear ResNets. Compared with the global convergence result of GD for training standard deep linear networks \\citep{du2019width}, our condition on the neural network width is sharper by a factor of $O(\\kappa L)$, where $\\kappa$ denotes the condition number of the covariance matrix of the training data. We further propose a modified identity input and output transformations, and show that a $(d+k)$-wide neural network is sufficient to guarantee the global convergence of GD/SGD, where $d,k$ are the input and output dimensions respectively.", "keywords": [], "paperhash": "zou|on_the_global_convergence_of_training_deep_linear_resnets", "_bibtex": "@inproceedings{\nZou2020On,\ntitle={On the Global Convergence  of Training Deep Linear ResNets},\nauthor={Difan Zou and Philip M. Long and Quanquan Gu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJxEhREKDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e9f06e9d1384a7cd6235301dbc5b970844fe863a.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJxEhREKDH", "replyto": "HJxEhREKDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1353/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1353/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575833652820, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1353/Reviewers"], "noninvitees": [], "tcdate": 1570237738610, "tmdate": 1575833652836, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1353/-/Official_Review"}}}, {"id": "rJllHnhsiH", "original": null, "number": 9, "cdate": 1573796920170, "ddate": null, "tcdate": 1573796920170, "tmdate": 1573797185794, "tddate": null, "forum": "HJxEhREKDH", "replyto": "B1eGf3FFir", "invitation": "ICLR.cc/2020/Conference/Paper1353/-/Official_Comment", "content": {"title": "Re:  Q5 (Arora et al.)", "comment": "Thanks for your further comments.\n\n**Clarification on the initial loss condition:\n\nFirst, we would like to clarify that the statement \u201cthe initial loss is smaller than any loss obtainable with rank deficiencies\u201d is quoted from the introduction of https://arxiv.org/pdf/1810.02281.pdf (see the third paragraph on page 2 for details). Second, this condition actually says that $\\|W_{1:N}(0)-\\Phi\\|_F\\le \\sigma_{min}(\\Phi)-c$, as stated in the first sentence of Theorem 1 in  Arora et al. (2018), which is not the condition on T in Theorem 1.\nThird, we do not see that our initialization trivially has a positive deficiency margin.  For example, for the case of the modified identity transformations for $A$ and $B$, we have $B(I + \\tau W_{L}^{(0)})\\dots (I + \\tau W_{1}^{(0)})A = 0$. Then the margin deficiency condition would require $\\| B(I + \\tau W_{L}^{(0)})\\dots (I + \\tau W_{1}^{(0)})A -\\Phi \\|_F = \\|\\Phi \\|_F \\leq \\sigma_{min}(\\Phi)-c$ for some positive $c$, which apparently doesn't hold. So our initialization scheme does not necessarily satisfy margin deficiency condition imposed in Arora et al. (2018).\n\n\n**Comparison with Arora et al. in terms of the condition on the input and output weight matrices:\n\nIt is true that we require certain conditions on the input and output weights matrices, which are stated in all theorems. However, Arora et al. 2018 also require conditions on the input and output weights matrices, i.e., balancedness condition (actually this condition is required for all layers in Arora et al. 2018, while in our paper, as we answered in A5.3, only the weight matrices at hidden layers satisfy this condition.) To our knowledge, the balancedness condition and our condition on the input and output weight matrices are not directly comparable, so it is unfair to say that we require \u201cadditional\u201d condition to prove the global convergence."}, "signatures": ["ICLR.cc/2020/Conference/Paper1353/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1353/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["knowzou@ucla.edu", "plong@google.com", "qgu@cs.ucla.edu"], "title": "On the Global Convergence  of Training Deep Linear ResNets", "authors": ["Difan Zou", "Philip M. Long", "Quanquan Gu"], "pdf": "/pdf/40f991e7d57fcf3070e9cc9829c477216a40cc72.pdf", "TL;DR": "Under certain condition on the input and output linear transformations, both GD and SGD can achieve global convergence for training deep linear ResNets.", "abstract": "We study the convergence of gradient descent (GD) and stochastic gradient descent (SGD) for training $L$-hidden-layer linear residual networks (ResNets). We prove that for training deep residual networks with certain linear transformations at input and output layers, which are fixed throughout training, both GD and SGD with zero initialization on all hidden weights can converge to the global minimum of the training loss. Moreover, when specializing to appropriate Gaussian random linear transformations, GD and SGD provably optimize wide enough deep linear ResNets. Compared with the global convergence result of GD for training standard deep linear networks \\citep{du2019width}, our condition on the neural network width is sharper by a factor of $O(\\kappa L)$, where $\\kappa$ denotes the condition number of the covariance matrix of the training data. We further propose a modified identity input and output transformations, and show that a $(d+k)$-wide neural network is sufficient to guarantee the global convergence of GD/SGD, where $d,k$ are the input and output dimensions respectively.", "keywords": [], "paperhash": "zou|on_the_global_convergence_of_training_deep_linear_resnets", "_bibtex": "@inproceedings{\nZou2020On,\ntitle={On the Global Convergence  of Training Deep Linear ResNets},\nauthor={Difan Zou and Philip M. Long and Quanquan Gu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJxEhREKDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e9f06e9d1384a7cd6235301dbc5b970844fe863a.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJxEhREKDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1353/Authors", "ICLR.cc/2020/Conference/Paper1353/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1353/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1353/Reviewers", "ICLR.cc/2020/Conference/Paper1353/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1353/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1353/Authors|ICLR.cc/2020/Conference/Paper1353/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504157287, "tmdate": 1576860531768, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1353/Authors", "ICLR.cc/2020/Conference/Paper1353/Reviewers", "ICLR.cc/2020/Conference/Paper1353/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1353/-/Official_Comment"}}}, {"id": "SyxL6inoiH", "original": null, "number": 8, "cdate": 1573796798253, "ddate": null, "tcdate": 1573796798253, "tmdate": 1573796798253, "tddate": null, "forum": "HJxEhREKDH", "replyto": "HJlVjoYFsr", "invitation": "ICLR.cc/2020/Conference/Paper1353/-/Official_Comment", "content": {"title": "Re: A11", "comment": "Thank you for your further comments.\n\nWe would like to clarify that Lemma A.2 implicitly requires smoothness since we directly leverage the formula of the loss function (which is smooth) in the proof. For non-linear networks there are generally two cases: using smooth activation functions (e.g., sigmoid, softplus, ELU) or nonsmooth activation functions (e.g., ReLU, LeakyReLU). For the smooth activation function case, we can easily derive a similar result as Lemma A.2. For the non-smooth activation function such as ReLU, we can prove a variant of Lemma A.2, which states the semi-smoothness property of the training loss function, as what was done in Allen-Zhu et al. (2019) and Frei et al. (2019).\n\nRegarding extending the analysis of our initialization scheme to nonlinear ResNets, the key is to prove two major results based on our initialization: 1) Residual connection with zero initialization can stabilize the forward feature propagation across layers, and 2)  Using Residual connection with zero initialization, one can provide a better characterization of the backward error propagation across layers, and a more refined optimization analysis in terms of all hidden layers, which cannot be easily done for deep nonlinear networks without residual connection.  We believe that by proving these two main results, the benefit of residual connection with zero initialization can be adapted to non-linear ResNets.   \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1353/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1353/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["knowzou@ucla.edu", "plong@google.com", "qgu@cs.ucla.edu"], "title": "On the Global Convergence  of Training Deep Linear ResNets", "authors": ["Difan Zou", "Philip M. Long", "Quanquan Gu"], "pdf": "/pdf/40f991e7d57fcf3070e9cc9829c477216a40cc72.pdf", "TL;DR": "Under certain condition on the input and output linear transformations, both GD and SGD can achieve global convergence for training deep linear ResNets.", "abstract": "We study the convergence of gradient descent (GD) and stochastic gradient descent (SGD) for training $L$-hidden-layer linear residual networks (ResNets). We prove that for training deep residual networks with certain linear transformations at input and output layers, which are fixed throughout training, both GD and SGD with zero initialization on all hidden weights can converge to the global minimum of the training loss. Moreover, when specializing to appropriate Gaussian random linear transformations, GD and SGD provably optimize wide enough deep linear ResNets. Compared with the global convergence result of GD for training standard deep linear networks \\citep{du2019width}, our condition on the neural network width is sharper by a factor of $O(\\kappa L)$, where $\\kappa$ denotes the condition number of the covariance matrix of the training data. We further propose a modified identity input and output transformations, and show that a $(d+k)$-wide neural network is sufficient to guarantee the global convergence of GD/SGD, where $d,k$ are the input and output dimensions respectively.", "keywords": [], "paperhash": "zou|on_the_global_convergence_of_training_deep_linear_resnets", "_bibtex": "@inproceedings{\nZou2020On,\ntitle={On the Global Convergence  of Training Deep Linear ResNets},\nauthor={Difan Zou and Philip M. Long and Quanquan Gu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJxEhREKDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e9f06e9d1384a7cd6235301dbc5b970844fe863a.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJxEhREKDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1353/Authors", "ICLR.cc/2020/Conference/Paper1353/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1353/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1353/Reviewers", "ICLR.cc/2020/Conference/Paper1353/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1353/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1353/Authors|ICLR.cc/2020/Conference/Paper1353/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504157287, "tmdate": 1576860531768, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1353/Authors", "ICLR.cc/2020/Conference/Paper1353/Reviewers", "ICLR.cc/2020/Conference/Paper1353/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1353/-/Official_Comment"}}}, {"id": "HkxmYG-isB", "original": null, "number": 7, "cdate": 1573749371339, "ddate": null, "tcdate": 1573749371339, "tmdate": 1573749371339, "tddate": null, "forum": "HJxEhREKDH", "replyto": "rJlPx1ODoH", "invitation": "ICLR.cc/2020/Conference/Paper1353/-/Official_Comment", "content": {"title": "Thank you for your response", "comment": "Thank you for this detailed answer. It confirms that this paper is of interest to the ICLR community."}, "signatures": ["ICLR.cc/2020/Conference/Paper1353/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1353/AnonReviewer2", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["knowzou@ucla.edu", "plong@google.com", "qgu@cs.ucla.edu"], "title": "On the Global Convergence  of Training Deep Linear ResNets", "authors": ["Difan Zou", "Philip M. Long", "Quanquan Gu"], "pdf": "/pdf/40f991e7d57fcf3070e9cc9829c477216a40cc72.pdf", "TL;DR": "Under certain condition on the input and output linear transformations, both GD and SGD can achieve global convergence for training deep linear ResNets.", "abstract": "We study the convergence of gradient descent (GD) and stochastic gradient descent (SGD) for training $L$-hidden-layer linear residual networks (ResNets). We prove that for training deep residual networks with certain linear transformations at input and output layers, which are fixed throughout training, both GD and SGD with zero initialization on all hidden weights can converge to the global minimum of the training loss. Moreover, when specializing to appropriate Gaussian random linear transformations, GD and SGD provably optimize wide enough deep linear ResNets. Compared with the global convergence result of GD for training standard deep linear networks \\citep{du2019width}, our condition on the neural network width is sharper by a factor of $O(\\kappa L)$, where $\\kappa$ denotes the condition number of the covariance matrix of the training data. We further propose a modified identity input and output transformations, and show that a $(d+k)$-wide neural network is sufficient to guarantee the global convergence of GD/SGD, where $d,k$ are the input and output dimensions respectively.", "keywords": [], "paperhash": "zou|on_the_global_convergence_of_training_deep_linear_resnets", "_bibtex": "@inproceedings{\nZou2020On,\ntitle={On the Global Convergence  of Training Deep Linear ResNets},\nauthor={Difan Zou and Philip M. Long and Quanquan Gu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJxEhREKDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e9f06e9d1384a7cd6235301dbc5b970844fe863a.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJxEhREKDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1353/Authors", "ICLR.cc/2020/Conference/Paper1353/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1353/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1353/Reviewers", "ICLR.cc/2020/Conference/Paper1353/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1353/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1353/Authors|ICLR.cc/2020/Conference/Paper1353/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504157287, "tmdate": 1576860531768, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1353/Authors", "ICLR.cc/2020/Conference/Paper1353/Reviewers", "ICLR.cc/2020/Conference/Paper1353/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1353/-/Official_Comment"}}}, {"id": "B1eGf3FFir", "original": null, "number": 6, "cdate": 1573653514364, "ddate": null, "tcdate": 1573653514364, "tmdate": 1573653514364, "tddate": null, "forum": "HJxEhREKDH", "replyto": "H1lutMuvsH", "invitation": "ICLR.cc/2020/Conference/Paper1353/-/Official_Comment", "content": {"title": "Q5 (Arora et al.)", "comment": "You said \"the initial loss is smaller than any loss obtainable with rank deficiencies.\"\nAre you referring to the condition on T in Theorem 1 in https://arxiv.org/pdf/1810.02281.pdf? Please clarify.\n\nThe theorem from Arora et al. also requires a condition on the margin deficiency of the initial weights, which is trivially met due to your initialization. You do have an additional condition on the input and output weights matrices, but this is not needed in Arora et al. if we consider the zero initialization of the weight matrices, can you please confirm?"}, "signatures": ["ICLR.cc/2020/Conference/Paper1353/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1353/AnonReviewer1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["knowzou@ucla.edu", "plong@google.com", "qgu@cs.ucla.edu"], "title": "On the Global Convergence  of Training Deep Linear ResNets", "authors": ["Difan Zou", "Philip M. Long", "Quanquan Gu"], "pdf": "/pdf/40f991e7d57fcf3070e9cc9829c477216a40cc72.pdf", "TL;DR": "Under certain condition on the input and output linear transformations, both GD and SGD can achieve global convergence for training deep linear ResNets.", "abstract": "We study the convergence of gradient descent (GD) and stochastic gradient descent (SGD) for training $L$-hidden-layer linear residual networks (ResNets). We prove that for training deep residual networks with certain linear transformations at input and output layers, which are fixed throughout training, both GD and SGD with zero initialization on all hidden weights can converge to the global minimum of the training loss. Moreover, when specializing to appropriate Gaussian random linear transformations, GD and SGD provably optimize wide enough deep linear ResNets. Compared with the global convergence result of GD for training standard deep linear networks \\citep{du2019width}, our condition on the neural network width is sharper by a factor of $O(\\kappa L)$, where $\\kappa$ denotes the condition number of the covariance matrix of the training data. We further propose a modified identity input and output transformations, and show that a $(d+k)$-wide neural network is sufficient to guarantee the global convergence of GD/SGD, where $d,k$ are the input and output dimensions respectively.", "keywords": [], "paperhash": "zou|on_the_global_convergence_of_training_deep_linear_resnets", "_bibtex": "@inproceedings{\nZou2020On,\ntitle={On the Global Convergence  of Training Deep Linear ResNets},\nauthor={Difan Zou and Philip M. Long and Quanquan Gu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJxEhREKDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e9f06e9d1384a7cd6235301dbc5b970844fe863a.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJxEhREKDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1353/Authors", "ICLR.cc/2020/Conference/Paper1353/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1353/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1353/Reviewers", "ICLR.cc/2020/Conference/Paper1353/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1353/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1353/Authors|ICLR.cc/2020/Conference/Paper1353/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504157287, "tmdate": 1576860531768, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1353/Authors", "ICLR.cc/2020/Conference/Paper1353/Reviewers", "ICLR.cc/2020/Conference/Paper1353/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1353/-/Official_Comment"}}}, {"id": "HJlVjoYFsr", "original": null, "number": 5, "cdate": 1573653403985, "ddate": null, "tcdate": 1573653403985, "tmdate": 1573653403985, "tddate": null, "forum": "HJxEhREKDH", "replyto": "Hylp8Q_vjS", "invitation": "ICLR.cc/2020/Conference/Paper1353/-/Official_Comment", "content": {"title": "A11", "comment": "Regarding your answer to Q11: Can you elaborate on this? I was expecting a slightly more technical answer. Take for instance Lemma A.2: looking at the  proof, it seems to me it does not require smoothness so it could potentially be adapted to non-linear networks? What would then be the difficulty in the initialization scheme that you mentioned in your answer?"}, "signatures": ["ICLR.cc/2020/Conference/Paper1353/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1353/AnonReviewer1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["knowzou@ucla.edu", "plong@google.com", "qgu@cs.ucla.edu"], "title": "On the Global Convergence  of Training Deep Linear ResNets", "authors": ["Difan Zou", "Philip M. Long", "Quanquan Gu"], "pdf": "/pdf/40f991e7d57fcf3070e9cc9829c477216a40cc72.pdf", "TL;DR": "Under certain condition on the input and output linear transformations, both GD and SGD can achieve global convergence for training deep linear ResNets.", "abstract": "We study the convergence of gradient descent (GD) and stochastic gradient descent (SGD) for training $L$-hidden-layer linear residual networks (ResNets). We prove that for training deep residual networks with certain linear transformations at input and output layers, which are fixed throughout training, both GD and SGD with zero initialization on all hidden weights can converge to the global minimum of the training loss. Moreover, when specializing to appropriate Gaussian random linear transformations, GD and SGD provably optimize wide enough deep linear ResNets. Compared with the global convergence result of GD for training standard deep linear networks \\citep{du2019width}, our condition on the neural network width is sharper by a factor of $O(\\kappa L)$, where $\\kappa$ denotes the condition number of the covariance matrix of the training data. We further propose a modified identity input and output transformations, and show that a $(d+k)$-wide neural network is sufficient to guarantee the global convergence of GD/SGD, where $d,k$ are the input and output dimensions respectively.", "keywords": [], "paperhash": "zou|on_the_global_convergence_of_training_deep_linear_resnets", "_bibtex": "@inproceedings{\nZou2020On,\ntitle={On the Global Convergence  of Training Deep Linear ResNets},\nauthor={Difan Zou and Philip M. Long and Quanquan Gu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJxEhREKDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e9f06e9d1384a7cd6235301dbc5b970844fe863a.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJxEhREKDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1353/Authors", "ICLR.cc/2020/Conference/Paper1353/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1353/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1353/Reviewers", "ICLR.cc/2020/Conference/Paper1353/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1353/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1353/Authors|ICLR.cc/2020/Conference/Paper1353/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504157287, "tmdate": 1576860531768, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1353/Authors", "ICLR.cc/2020/Conference/Paper1353/Reviewers", "ICLR.cc/2020/Conference/Paper1353/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1353/-/Official_Comment"}}}, {"id": "Hylp8Q_vjS", "original": null, "number": 3, "cdate": 1573516116634, "ddate": null, "tcdate": 1573516116634, "tmdate": 1573516709680, "tddate": null, "forum": "HJxEhREKDH", "replyto": "rkgnmE0ptS", "invitation": "ICLR.cc/2020/Conference/Paper1353/-/Official_Comment", "content": {"title": "Response to Review #1: part 2 ", "comment": "Q6: \u201cFinally, Allen-Zhu et al. (2019) already has some results on deep resnets although their analysis is for a heavily over-parametrized regime. Can you still comment on how their results differ from yours?\u201d\nA6: It is true that Allen-Zhu et al. (2019) studied the convergence of ResNets for nonlinear activation functions. Besides the linear versus nonlinear activation functions, the additional differences include: (1) they require that the neural network to be vastly over-parameterized  and the width has a worse dependency on the sample size n and depth L, while our results for deep linear ResNets give a much better condition on the width (only logarithmic in n and with no dependence on L); and (2) Allen-Zhu et al. (2019) require that all data points are separated by a positive distance and have unit norm, our results have no assumption on the training data. We have added more comments in the related work of our revision. \n\n\nQ7.1: \u201cA & B are fixed matrices initialized from a Gaussian distribution. How essential is this condition? Assuming for simplicity that A and B are square matrices. \u201d\nA7.1: First, we would like to clarify that this condition is not essential and Gaussian linear transformations are just one example we discussed. Second, while we have not worked out the details, we believe our theory can also be established if A and B are also trained. The high-level idea to prove this is: with proper initialization of A and B (e.g., Gaussian initialization with proper variance), we can show that they will stay close to their initial values similarly to the way that other layers were treated, and, given that they are close to their initial values, their singular values can be bounded above, and bounded away from zero, so that the objective function stays smooth, and they continue to propagate gradients effectively to the other layers.\n\nQ7.2: \u201cCan I not set A=B=identity and still satisfy the condition in Theorem 1? The term on the RHS would be 1 so then the initialization would need to scale as a function of the spectrum of the data matrix X.\u201d\nA7.2: Theorem 3.1 establishes convergence for any choices of A and B that satisfy a condition relating their singular values to the quality of the initial solution obtained from them. We have discussed the case that A=B=I in Remark 3.2 and Section 4. In this case, Bartlett, et al. showed that GD sometimes fails to converge to the optimum.\n\n\nQ8:  \u201d1) If we consider ...  initialized to identify instead of zero.\n2) The lower bound ... . Is this where  one can see the benefit of having residual connections?\u201c\nA8: Yes, you are right. We have already mentioned that our initialization is equivalent to the identity initialization for standard deep linear networks in Section 2. Thus it is also true that the  $\\sigma^2_{min}(I + \\tau W_l)$ can be replaced by $\\lambda_{min}(W_l)$. We choose to present our results in the context of \u201cdeep linear ResNets\u201d because we want to highlight the benefit of having residual connections.\n\nQ9: \u201dThe bound ... the theorem does not hold. Can you comment on this?\u201c\nA9: In Proposition 3.3, we require that $m\\ge C(d+k+\\log(1/\\delta))$ for some absolute constant C, which means that A and B cannot be square matrices since $m\\ge d$ and $m\\ge k$. Then our theorem can indeed hold since the smallest singular values of A and B can be lower bounded with high probability.   \n\nQ10: \u201dCan you comment on how difficult this derivation is compared to the deterministic setting?\u201c\nA10: As we pointed out in the introduction section, we need to guarantee that the restricted gradient bounds and a smoothness property hold along the algorithm trajectory. Compared with the deterministic setting, the trajectory of SGD is difficult to characterize due to the randomness caused by stochastic gradients. Therefore, a martingale based proof technique is needed to characterize the trajectory of SGD in order to prove the global convergence. In addition, it is also highly non-trivial to prove the linear rate of convergence for SGD as we did in Theorem 3.8 because in general SGD cannot achieve this.\n\nQ11: \u201dYou claim ... this is not obvious as the landscape of such networks can be very different. Can you elaborate?\u201c\nA11: Deep linear networks are an idealized setting that includes some aspects of the non-linear case, including the potential for exploding and vanishing gradients, and a non-convex objective function, while abstracting away others.  Study of this clean setting has inspired research into the nonlinear case in the past.  The fact that stronger results are possible for residual networks in the linear case raises the hope that they may lead to stronger results with non-linearities.  We also were surprised by the results on effective initialization in this case discussed in Section 4;  we hope (and expect) that similar initialization schemes can be shown to work very efficiently in the presence of non-linearities.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1353/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1353/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["knowzou@ucla.edu", "plong@google.com", "qgu@cs.ucla.edu"], "title": "On the Global Convergence  of Training Deep Linear ResNets", "authors": ["Difan Zou", "Philip M. Long", "Quanquan Gu"], "pdf": "/pdf/40f991e7d57fcf3070e9cc9829c477216a40cc72.pdf", "TL;DR": "Under certain condition on the input and output linear transformations, both GD and SGD can achieve global convergence for training deep linear ResNets.", "abstract": "We study the convergence of gradient descent (GD) and stochastic gradient descent (SGD) for training $L$-hidden-layer linear residual networks (ResNets). We prove that for training deep residual networks with certain linear transformations at input and output layers, which are fixed throughout training, both GD and SGD with zero initialization on all hidden weights can converge to the global minimum of the training loss. Moreover, when specializing to appropriate Gaussian random linear transformations, GD and SGD provably optimize wide enough deep linear ResNets. Compared with the global convergence result of GD for training standard deep linear networks \\citep{du2019width}, our condition on the neural network width is sharper by a factor of $O(\\kappa L)$, where $\\kappa$ denotes the condition number of the covariance matrix of the training data. We further propose a modified identity input and output transformations, and show that a $(d+k)$-wide neural network is sufficient to guarantee the global convergence of GD/SGD, where $d,k$ are the input and output dimensions respectively.", "keywords": [], "paperhash": "zou|on_the_global_convergence_of_training_deep_linear_resnets", "_bibtex": "@inproceedings{\nZou2020On,\ntitle={On the Global Convergence  of Training Deep Linear ResNets},\nauthor={Difan Zou and Philip M. Long and Quanquan Gu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJxEhREKDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e9f06e9d1384a7cd6235301dbc5b970844fe863a.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJxEhREKDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1353/Authors", "ICLR.cc/2020/Conference/Paper1353/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1353/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1353/Reviewers", "ICLR.cc/2020/Conference/Paper1353/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1353/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1353/Authors|ICLR.cc/2020/Conference/Paper1353/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504157287, "tmdate": 1576860531768, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1353/Authors", "ICLR.cc/2020/Conference/Paper1353/Reviewers", "ICLR.cc/2020/Conference/Paper1353/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1353/-/Official_Comment"}}}, {"id": "H1grcQOvsS", "original": null, "number": 4, "cdate": 1573516172805, "ddate": null, "tcdate": 1573516172805, "tmdate": 1573516172805, "tddate": null, "forum": "HJxEhREKDH", "replyto": "HJg_GdOTKr", "invitation": "ICLR.cc/2020/Conference/Paper1353/-/Official_Comment", "content": {"title": "Response to Review #3", "comment": "Thanks for your supportive comments. \n\nQ1: \u201cIt would be nice if the authors can give intuition why the input and output layer transformations need to be fixed in the analysis. What if happen if these matrices vary along the optimization process?\u201d\nA1: We believe that our analysis can be modified to address the case that A and B are trained (i.e., vary along the optimization process).  The high-level idea to prove this is: with proper initialization of A and B (e.g., Gaussian initialization with proper variance), we can show that they will stay close to their initial values similarly to the way that other layers were treated, and, given that they are close to their initial values, their singular values can be bounded above, and bounded away from zero, so that the objective function stays smooth, and they continue to propagate gradients effectively to the other layers.\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1353/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1353/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["knowzou@ucla.edu", "plong@google.com", "qgu@cs.ucla.edu"], "title": "On the Global Convergence  of Training Deep Linear ResNets", "authors": ["Difan Zou", "Philip M. Long", "Quanquan Gu"], "pdf": "/pdf/40f991e7d57fcf3070e9cc9829c477216a40cc72.pdf", "TL;DR": "Under certain condition on the input and output linear transformations, both GD and SGD can achieve global convergence for training deep linear ResNets.", "abstract": "We study the convergence of gradient descent (GD) and stochastic gradient descent (SGD) for training $L$-hidden-layer linear residual networks (ResNets). We prove that for training deep residual networks with certain linear transformations at input and output layers, which are fixed throughout training, both GD and SGD with zero initialization on all hidden weights can converge to the global minimum of the training loss. Moreover, when specializing to appropriate Gaussian random linear transformations, GD and SGD provably optimize wide enough deep linear ResNets. Compared with the global convergence result of GD for training standard deep linear networks \\citep{du2019width}, our condition on the neural network width is sharper by a factor of $O(\\kappa L)$, where $\\kappa$ denotes the condition number of the covariance matrix of the training data. We further propose a modified identity input and output transformations, and show that a $(d+k)$-wide neural network is sufficient to guarantee the global convergence of GD/SGD, where $d,k$ are the input and output dimensions respectively.", "keywords": [], "paperhash": "zou|on_the_global_convergence_of_training_deep_linear_resnets", "_bibtex": "@inproceedings{\nZou2020On,\ntitle={On the Global Convergence  of Training Deep Linear ResNets},\nauthor={Difan Zou and Philip M. Long and Quanquan Gu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJxEhREKDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e9f06e9d1384a7cd6235301dbc5b970844fe863a.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJxEhREKDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1353/Authors", "ICLR.cc/2020/Conference/Paper1353/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1353/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1353/Reviewers", "ICLR.cc/2020/Conference/Paper1353/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1353/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1353/Authors|ICLR.cc/2020/Conference/Paper1353/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504157287, "tmdate": 1576860531768, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1353/Authors", "ICLR.cc/2020/Conference/Paper1353/Reviewers", "ICLR.cc/2020/Conference/Paper1353/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1353/-/Official_Comment"}}}, {"id": "H1lutMuvsH", "original": null, "number": 2, "cdate": 1573515904466, "ddate": null, "tcdate": 1573515904466, "tmdate": 1573515904466, "tddate": null, "forum": "HJxEhREKDH", "replyto": "rkgnmE0ptS", "invitation": "ICLR.cc/2020/Conference/Paper1353/-/Official_Comment", "content": {"title": "Response to Review #1: part 1", "comment": "Thanks for your valuable and helpful comments.\n\nQ1:  \u201cI don\u2019t think this is explicitly stated in the paper but are the initialization conditions and the assumed distance to the optimum the same in both papers?\u201d\nA1: Regarding the initialization, we consider random Gaussian transformations at both input and output layers and use zero initialization for all rest layers (which is equivalent to identity initialization for standard deep linear networks), while Du&Hu consider random Gaussian initialization for all layers. Therefore, the initialization scheme in our paper is different from that in Du&Hu.  A main finding of our paper is that the initialization considered here enables much stronger convergence guarantees.  In addition, both our paper and Du & Hu do not assume the distance to the optimum. We would also like to point out that the global convergence of GD (and SGD) established in our paper and in  Du&Hu do not require any condition on the distance to the optimum.\n\nQ2: \u201cCouldn\u2019t you re-derived this result by adapting your own analysis?\u201d\nA2: We believe that the techniques of our paper could be generalized to other initialization schemes, such as the one considered by Du & Hu.\n\nQ3: \u201cIs there any theoretical proof or empirical evidence showing that resnets do indeed scale better w.r.t. to the condition number of the data?\u201d\nA3: We have proved a stronger upper bound on the required width in terms of the condition number;  we agree that lower bounds for other initialization schemes, or experimental comparisons, would be an interesting topic for future work.\n\n\nQ4: \u201cDoes the result of Theorem 3.1 provides a tighter bound in terms of L or kappa (when using the same initialization conditions)? \u201d\nA4:  Bartlett et al, only consider the case in which the hidden layers have the same width as the input and the output, so we cannot compare our bounds on the required width with theirs.  Our remark was that we can recover the main result of Bartlett, et al\u2019s paper by specializing Theorem 3.1 to the case that $A=B=I$, and all hidden layers have the same number of hidden units as the input and output layers.\n\nQ5.1: \u201cArora et al. (2019a): You explain that they showed that \u201cGD converges under substantially weaker conditions\u201d. How much weaker are these conditions?\u201d\nA5.1: We would like to first clarify that the statement \u201c Arora et al. (2019a) proves global convergence of GD under substantially weaker conditions\u201d  is made for the comparison between  Arora et al. (2019a) and Bartlett et al. (2019), rather than our paper. Specifically, in order to establish the global convergence of GD, Arora et al. (2019a) requires (1) the widths of hidden layers are greater than or equal to the minimum between the dimensions of input and output; (2) layers are initialized to be approximately balanced; and (3) the initial loss is smaller than any loss obtainable with rank deficiencies. These conditions are weaker than that in Bartlett et al. (2019) as Bartlett et al. (2019) requires that the initial loss is much smaller. In contrast to Arora et al. (2019a) and Bartlett et al. (2019), our result does not require any assumption on the initial loss, and therefore holds under weaker conditions.\n\nQ5.2: \u201cHow do they (Arora et al. 2019a) compare to the \u201cmodified identity transformation\u201d that allows you to obtain a global convergence rate.  How does your final result compare to Arora et al. (2019a)? \u201d\nA5.2: We would like to clarify that the global convergence of GD/SGD established in our paper is not limited to \u201cmodified identity transformation\u201d. In fact, our paper proves a generic condition for input and output linear transforms such that the global convergence can be guaranteed. For example, in Corollaries 3.4, 3.7, 3.9, we show that Gaussian random linear transformation can guarantee global convergence. In addition, we also want to point out that the proposed modified identity transformation is just a possible choice of A and B (instead of random A and B) that can guarantee the global convergence of GD/SGD. In particular, as long as the width of the neural network is greater than or equal to $d+k$, modified identity transformation satisfies the aforementioned generic condition and therefore the global convergence of GD/SGD also holds. We have added this result in Section 4 of our revision. \nAs we mentioned in A5.1, since our convergence guarantee does not require any condition on the initial loss while the result in Arora et al. (2019a) requires, we believe our result is stronger than that in Arora et al. (2019a). \n\nQ5.3: \u201cThe analysis of Arora et al. (2019a) requires a balanced-ness condition, is this substituted by a different condition in your analysis?\u201d\nA5.3: Since in our paper all the hidden layers are simply initialized by identity matrix, they automatically satisfy the balanced-ness condition. We have clarified this in Section 2 of our revision.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1353/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1353/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["knowzou@ucla.edu", "plong@google.com", "qgu@cs.ucla.edu"], "title": "On the Global Convergence  of Training Deep Linear ResNets", "authors": ["Difan Zou", "Philip M. Long", "Quanquan Gu"], "pdf": "/pdf/40f991e7d57fcf3070e9cc9829c477216a40cc72.pdf", "TL;DR": "Under certain condition on the input and output linear transformations, both GD and SGD can achieve global convergence for training deep linear ResNets.", "abstract": "We study the convergence of gradient descent (GD) and stochastic gradient descent (SGD) for training $L$-hidden-layer linear residual networks (ResNets). We prove that for training deep residual networks with certain linear transformations at input and output layers, which are fixed throughout training, both GD and SGD with zero initialization on all hidden weights can converge to the global minimum of the training loss. Moreover, when specializing to appropriate Gaussian random linear transformations, GD and SGD provably optimize wide enough deep linear ResNets. Compared with the global convergence result of GD for training standard deep linear networks \\citep{du2019width}, our condition on the neural network width is sharper by a factor of $O(\\kappa L)$, where $\\kappa$ denotes the condition number of the covariance matrix of the training data. We further propose a modified identity input and output transformations, and show that a $(d+k)$-wide neural network is sufficient to guarantee the global convergence of GD/SGD, where $d,k$ are the input and output dimensions respectively.", "keywords": [], "paperhash": "zou|on_the_global_convergence_of_training_deep_linear_resnets", "_bibtex": "@inproceedings{\nZou2020On,\ntitle={On the Global Convergence  of Training Deep Linear ResNets},\nauthor={Difan Zou and Philip M. Long and Quanquan Gu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJxEhREKDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e9f06e9d1384a7cd6235301dbc5b970844fe863a.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJxEhREKDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1353/Authors", "ICLR.cc/2020/Conference/Paper1353/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1353/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1353/Reviewers", "ICLR.cc/2020/Conference/Paper1353/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1353/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1353/Authors|ICLR.cc/2020/Conference/Paper1353/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504157287, "tmdate": 1576860531768, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1353/Authors", "ICLR.cc/2020/Conference/Paper1353/Reviewers", "ICLR.cc/2020/Conference/Paper1353/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1353/-/Official_Comment"}}}, {"id": "rJlPx1ODoH", "original": null, "number": 1, "cdate": 1573514990801, "ddate": null, "tcdate": 1573514990801, "tmdate": 1573515388772, "tddate": null, "forum": "HJxEhREKDH", "replyto": "SyluQ28J9S", "invitation": "ICLR.cc/2020/Conference/Paper1353/-/Official_Comment", "content": {"title": "Response to Review #2", "comment": "Thanks for your positive and constructive comments.\n\nQ1: \u201cIn Proposition 3.3 you show an upper bound on $\\sigma_{min}(B)$ in order to show in Corollary 3.4  that the condition to apply Theorem 3.1 is true. However, it seems to me that you need a lower bound on  to prove that (A.3) is true.\u201d\nA1: Thank for catching this.  We have actually proved the lower bound of $\\sigma_{min}(B)$ in the proof of Proposition 3.3, but we did not properly present it in the statement of Proposition 3.3. We have modified the statement of Proposition 3.3 in the revision.\n\nQ2: \u201cTo what extent the proof of Theorem 3.1 uses the proof technique of Bartlett et al. 2019 ?\u201d\nA2: There are a number of significant differences, but we would like to highlight Lemma A.2, which establishes the smoothness of the objective function.  Bartlett, et al established smoothness through an upper bound on operator norm of the Hessian in terms of parameters of the problem like the depth and the size of the input that held for all solutions inside a ball centered at the initial solution.  This was in turn used to bound how much the objective function changes as GD makes its step.  We instead directly bound this change in the objective.  Crucially, our bound on the smoothness parameter is tighter, which depends on the loss function value at a specific $\\mathbf{W}$ (See the term $\\sqrt{eL(\\mathbf{W})}$ on the R.H.S. of the equation in Lemma A.2.).  This gives us a stronger bound after training has proceeded for a while ($L(\\mathbf{W})$ is decreasing when the training makes progress).  We also achieved a quantitatively stronger dependence on the other parameters (e.g., input dimension $d$).  \n\n\nQ3: \u201cWith you small enough conditions, are you in the lazy regime described by Chizat, Lenaic, Edouard Oyallon, and Francis Bach. \"On Lazy Training in Differentiable Programming.\" (2019). NeurIPS\u201d\nA3: Thanks for pointing this out. We would like to clarify that when applying Gaussian random transformations and the neural network width goes to infinity, our results are indeed in the lazy regime since the neural network weights will barely change. However, our theoretical results are not limited to this. Specifically, we provide a generic condition on the linear transformations at input and output layers, which can be satisfied in multiple ways.  For example, if the initial training loss is close to the global optimum, we do not require super large m, and the neural network does not need to behave like a linear model, which is not in the lazy regime. We have commented on this work in the additional related work section of our revision.\n\nQ4: \u201cIn Theorem 3.1 What is e?\u201d\nA4: It is the base of the natural logarithm.\n\nQ5: \u201cCould you prove the same result as Theorem  3.1 and 3.6 but with an inequality constraint on the step size? It seems very restrictive to me to ask a stepsize to be exactly equal to a quantity. If you cannot relax you equality constraint into an inequality constraint, can you at least show that your result holds for step size in an interval?\u201d\nA5: Thanks for your suggestion. Actually, the quantities we provide in Theorem 3.1 and 3.6 are only the upper bound of step size (which corresponds to the optimal iteration complexity), our theoretical results still hold for the smaller step size. We have restated the condition on the stepsize to be $\\eta \\leq \\dots$ in all theorems of the revision.\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1353/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1353/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["knowzou@ucla.edu", "plong@google.com", "qgu@cs.ucla.edu"], "title": "On the Global Convergence  of Training Deep Linear ResNets", "authors": ["Difan Zou", "Philip M. Long", "Quanquan Gu"], "pdf": "/pdf/40f991e7d57fcf3070e9cc9829c477216a40cc72.pdf", "TL;DR": "Under certain condition on the input and output linear transformations, both GD and SGD can achieve global convergence for training deep linear ResNets.", "abstract": "We study the convergence of gradient descent (GD) and stochastic gradient descent (SGD) for training $L$-hidden-layer linear residual networks (ResNets). We prove that for training deep residual networks with certain linear transformations at input and output layers, which are fixed throughout training, both GD and SGD with zero initialization on all hidden weights can converge to the global minimum of the training loss. Moreover, when specializing to appropriate Gaussian random linear transformations, GD and SGD provably optimize wide enough deep linear ResNets. Compared with the global convergence result of GD for training standard deep linear networks \\citep{du2019width}, our condition on the neural network width is sharper by a factor of $O(\\kappa L)$, where $\\kappa$ denotes the condition number of the covariance matrix of the training data. We further propose a modified identity input and output transformations, and show that a $(d+k)$-wide neural network is sufficient to guarantee the global convergence of GD/SGD, where $d,k$ are the input and output dimensions respectively.", "keywords": [], "paperhash": "zou|on_the_global_convergence_of_training_deep_linear_resnets", "_bibtex": "@inproceedings{\nZou2020On,\ntitle={On the Global Convergence  of Training Deep Linear ResNets},\nauthor={Difan Zou and Philip M. Long and Quanquan Gu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJxEhREKDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e9f06e9d1384a7cd6235301dbc5b970844fe863a.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJxEhREKDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1353/Authors", "ICLR.cc/2020/Conference/Paper1353/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1353/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1353/Reviewers", "ICLR.cc/2020/Conference/Paper1353/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1353/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1353/Authors|ICLR.cc/2020/Conference/Paper1353/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504157287, "tmdate": 1576860531768, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1353/Authors", "ICLR.cc/2020/Conference/Paper1353/Reviewers", "ICLR.cc/2020/Conference/Paper1353/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1353/-/Official_Comment"}}}], "count": 14}