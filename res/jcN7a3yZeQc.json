{"notes": [{"id": "jcN7a3yZeQc", "original": "cQuqMEPyU1b", "number": 2391, "cdate": 1601308263804, "ddate": null, "tcdate": 1601308263804, "tmdate": 1614985639453, "tddate": null, "forum": "jcN7a3yZeQc", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Decorrelated Double Q-learning", "authorids": ["~GANG_CHEN1"], "authors": ["GANG CHEN"], "keywords": ["q-learning", "control variates", "reinforcement learning"], "abstract": "Q-learning with value function approximation may have the poor performance because of overestimation bias and imprecise estimate. Specifically, overestimation bias is from the maximum operator over noise estimate, which is exaggerated using the estimate of a subsequent state. Inspired by the recent advance of deep reinforcement learning and Double Q-learning, we introduce the decorrelated double Q-learning (D2Q). Specifically, we introduce the decorrelated regularization item to reduce the correlation between value function approximators, which can lead to less biased estimation and low variance. The experimental results on a suite of MuJoCo continuous control tasks demonstrate that our decorrelated double Q-learning can effectively improve the performance.", "one-sentence_summary": "This paper proposes a decorrelated Double Q-learning for continuous task control", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chen|decorrelated_double_qlearning", "pdf": "/pdf/b0fb7882290e819c004f6b91a03606e5326dde8d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=n3vEJtQg2", "_bibtex": "@misc{\nchen2021decorrelated,\ntitle={Decorrelated Double Q-learning},\nauthor={GANG CHEN},\nyear={2021},\nurl={https://openreview.net/forum?id=jcN7a3yZeQc}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 6, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "2kMBaEAcr6", "original": null, "number": 1, "cdate": 1610040522102, "ddate": null, "tcdate": 1610040522102, "tmdate": 1610474130942, "tddate": null, "forum": "jcN7a3yZeQc", "replyto": "jcN7a3yZeQc", "invitation": "ICLR.cc/2021/Conference/Paper2391/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "This paper investigates some variants of the double Q-learning algorithm and develops theoretical guarantees. In particular, it focuses on how to reduce the correlation between the two trajectories employed in the double Q-learning strategy, in the hope of rigorously addressing the overestimation bias issue that arises due to the max operator in Q-learning. However, the reviewers point out that the proofs are hard to parse (and often hand-waving with important details omitted). The experimental results are also not convincing enough.\u00a0 \u00a0 \u00a0"}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Decorrelated Double Q-learning", "authorids": ["~GANG_CHEN1"], "authors": ["GANG CHEN"], "keywords": ["q-learning", "control variates", "reinforcement learning"], "abstract": "Q-learning with value function approximation may have the poor performance because of overestimation bias and imprecise estimate. Specifically, overestimation bias is from the maximum operator over noise estimate, which is exaggerated using the estimate of a subsequent state. Inspired by the recent advance of deep reinforcement learning and Double Q-learning, we introduce the decorrelated double Q-learning (D2Q). Specifically, we introduce the decorrelated regularization item to reduce the correlation between value function approximators, which can lead to less biased estimation and low variance. The experimental results on a suite of MuJoCo continuous control tasks demonstrate that our decorrelated double Q-learning can effectively improve the performance.", "one-sentence_summary": "This paper proposes a decorrelated Double Q-learning for continuous task control", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chen|decorrelated_double_qlearning", "pdf": "/pdf/b0fb7882290e819c004f6b91a03606e5326dde8d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=n3vEJtQg2", "_bibtex": "@misc{\nchen2021decorrelated,\ntitle={Decorrelated Double Q-learning},\nauthor={GANG CHEN},\nyear={2021},\nurl={https://openreview.net/forum?id=jcN7a3yZeQc}\n}"}, "tags": [], "invitation": {"reply": {"forum": "jcN7a3yZeQc", "replyto": "jcN7a3yZeQc", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040522088, "tmdate": 1610474130926, "id": "ICLR.cc/2021/Conference/Paper2391/-/Decision"}}}, {"id": "YaZoTQj2J9n", "original": null, "number": 1, "cdate": 1605094527073, "ddate": null, "tcdate": 1605094527073, "tmdate": 1605094527073, "tddate": null, "forum": "jcN7a3yZeQc", "replyto": "jcN7a3yZeQc", "invitation": "ICLR.cc/2021/Conference/Paper2391/-/Public_Comment", "content": {"title": "Good idea, lack of citation.", "comment": "Your idea looks good. However, the comparison experiments lack similar methods which reduce the estimation error.\nRecommend the author compare with the following method:  [1]Li Z, Hou X. Mixing Update Q-value for Deep Reinforcement Learning[C]//2019 International Joint Conference on Neural Networks (IJCNN). IEEE, 2019: 1-6. [2]He Q, Hou X. Reducing Estimation Bias via Weighted Delayed Deep Deterministic Policy Gradient[J]. arXiv preprint arXiv:2006.12622, 2020."}, "signatures": ["~Qiang_He1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "~Qiang_He1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Decorrelated Double Q-learning", "authorids": ["~GANG_CHEN1"], "authors": ["GANG CHEN"], "keywords": ["q-learning", "control variates", "reinforcement learning"], "abstract": "Q-learning with value function approximation may have the poor performance because of overestimation bias and imprecise estimate. Specifically, overestimation bias is from the maximum operator over noise estimate, which is exaggerated using the estimate of a subsequent state. Inspired by the recent advance of deep reinforcement learning and Double Q-learning, we introduce the decorrelated double Q-learning (D2Q). Specifically, we introduce the decorrelated regularization item to reduce the correlation between value function approximators, which can lead to less biased estimation and low variance. The experimental results on a suite of MuJoCo continuous control tasks demonstrate that our decorrelated double Q-learning can effectively improve the performance.", "one-sentence_summary": "This paper proposes a decorrelated Double Q-learning for continuous task control", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chen|decorrelated_double_qlearning", "pdf": "/pdf/b0fb7882290e819c004f6b91a03606e5326dde8d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=n3vEJtQg2", "_bibtex": "@misc{\nchen2021decorrelated,\ntitle={Decorrelated Double Q-learning},\nauthor={GANG CHEN},\nyear={2021},\nurl={https://openreview.net/forum?id=jcN7a3yZeQc}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "jcN7a3yZeQc", "readers": {"description": "User groups that will be able to read this comment.", "values": ["everyone"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed."}}, "expdate": 1605630600000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2391/Authors", "ICLR.cc/2021/Conference/Paper2391/Reviewers", "ICLR.cc/2021/Conference/Paper2391/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1605024963208, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2391/-/Public_Comment"}}}, {"id": "qAUWHDfCDQa", "original": null, "number": 1, "cdate": 1603672128398, "ddate": null, "tcdate": 1603672128398, "tmdate": 1605024221880, "tddate": null, "forum": "jcN7a3yZeQc", "replyto": "jcN7a3yZeQc", "invitation": "ICLR.cc/2021/Conference/Paper2391/-/Official_Review", "content": {"title": "Improving double Q-learning with decorrelated regularization and control variates", "review": "The proposed \"decorrelated double Q-learning\" algorithm combines a few techniques to improve the performance of model-free RL, including control variates for reducing variance, decorrelated regularization for reducing bias, and a technique from TD3 for stabilizing learning. \n\nOverall, the ideas of this work are interesting and bring some insights for tackling the overestimation issue of Q-learning. Empirically, the proposed method shows some improvements over the existing ones. However, a few major concerns are as follows.\n\n- The theoretical analysis of convergence seems hand-waving and confuses me. For example, does the analysis only apply to the tabular case? (The authors don't seem to state this explicitly.) How does Eq. (17) follow from Eq. (9) (are we missing the gradient of the decorrelated regularization term)?\n\n- All experimental results are about reward vs. iteration curves, which are not convincing or insightful enough. For example, is there empirical evidence showing that the proposed algorithm does learn two decorrelated critics? \n\n- The structure of Section 3 may need some adjustment. In particular, in the current version, the formal definition of the correlation term (Sec 3.2) and the description of the full algorithm itself (Sec 3.3) appear after the convergence analysis of the algorithm (Sec 3.1), which looks weird.\n\nBased on the above comments, I think substantial improvements are needed for publication of this work.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2391/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2391/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Decorrelated Double Q-learning", "authorids": ["~GANG_CHEN1"], "authors": ["GANG CHEN"], "keywords": ["q-learning", "control variates", "reinforcement learning"], "abstract": "Q-learning with value function approximation may have the poor performance because of overestimation bias and imprecise estimate. Specifically, overestimation bias is from the maximum operator over noise estimate, which is exaggerated using the estimate of a subsequent state. Inspired by the recent advance of deep reinforcement learning and Double Q-learning, we introduce the decorrelated double Q-learning (D2Q). Specifically, we introduce the decorrelated regularization item to reduce the correlation between value function approximators, which can lead to less biased estimation and low variance. The experimental results on a suite of MuJoCo continuous control tasks demonstrate that our decorrelated double Q-learning can effectively improve the performance.", "one-sentence_summary": "This paper proposes a decorrelated Double Q-learning for continuous task control", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chen|decorrelated_double_qlearning", "pdf": "/pdf/b0fb7882290e819c004f6b91a03606e5326dde8d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=n3vEJtQg2", "_bibtex": "@misc{\nchen2021decorrelated,\ntitle={Decorrelated Double Q-learning},\nauthor={GANG CHEN},\nyear={2021},\nurl={https://openreview.net/forum?id=jcN7a3yZeQc}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "jcN7a3yZeQc", "replyto": "jcN7a3yZeQc", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2391/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538097392, "tmdate": 1606915806329, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2391/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2391/-/Official_Review"}}}, {"id": "rwAmhWey89L", "original": null, "number": 2, "cdate": 1603688466273, "ddate": null, "tcdate": 1603688466273, "tmdate": 1605024221816, "tddate": null, "forum": "jcN7a3yZeQc", "replyto": "jcN7a3yZeQc", "invitation": "ICLR.cc/2021/Conference/Paper2391/-/Official_Review", "content": {"title": "Strange theory and unconvincing experiments", "review": "Summary\nThe paper suggests an improvement over double-Q learning by applying the control variates technique to the target Q, in the form of $(q1 - \\beta (q2 - E(q2))$ (eqn (8)). To minimize the variance, it suggests minimizing the correlation between $q1$ and $q2$. In addition, it applies the TD3 trick. The resulting algorithm, D2Q, outperforms DDPG and competes with TD3.\n\nRecommendation\nI hope I haven\u2019t misunderstood this paper, but I\u2019ve found neither the theory nor the experiment convincing. Therefore I recommend a rejection.\n\nStrengths\nThe proposed algorithm is simple and straightforward to use.\n\nWeaknesses\n1. Theory\n(a) Minimizing the variance of eqn (8) requires maximizing the correlation between q1 and q2. If they are independent, what\u2019s the point of including q2? Check out https://en.wikipedia.org/wiki/Control_variates\n(b) $E(q2))$ is the \u201caverage over all possible runs\u201d. It\u2019s unclear how it\u2019s calculated. Maybe run a few identical RL experiments with different random seeds, just to get $E(q2)$? Feels wasteful to me.\n(c) Why would minimizing the squared cosine between last layer feature vectors lead to minimum correlation? If the feature for q2 is obtained from that of q1 through a deterministic 90\u00ba rotation, wouldn\u2019t that result in a zero cosine but really strong correlation?\n(d) Why is it ok to ignore $var(q1)$ while computing $\\beta$? No theory is given here.\n\n2. Experiments\nIn Fig 1 - 3, D2Q sometimes outperforms and sometimes underperforms TD3. Because these two algorithms are so similar, I can\u2019t tell whether the comparison is statistically significant.\n\nOther feedbacks\nPlease address questions raised above. Perform additional experiments to make the paper more convincing. \n", "rating": "3: Clear rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2391/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2391/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Decorrelated Double Q-learning", "authorids": ["~GANG_CHEN1"], "authors": ["GANG CHEN"], "keywords": ["q-learning", "control variates", "reinforcement learning"], "abstract": "Q-learning with value function approximation may have the poor performance because of overestimation bias and imprecise estimate. Specifically, overestimation bias is from the maximum operator over noise estimate, which is exaggerated using the estimate of a subsequent state. Inspired by the recent advance of deep reinforcement learning and Double Q-learning, we introduce the decorrelated double Q-learning (D2Q). Specifically, we introduce the decorrelated regularization item to reduce the correlation between value function approximators, which can lead to less biased estimation and low variance. The experimental results on a suite of MuJoCo continuous control tasks demonstrate that our decorrelated double Q-learning can effectively improve the performance.", "one-sentence_summary": "This paper proposes a decorrelated Double Q-learning for continuous task control", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chen|decorrelated_double_qlearning", "pdf": "/pdf/b0fb7882290e819c004f6b91a03606e5326dde8d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=n3vEJtQg2", "_bibtex": "@misc{\nchen2021decorrelated,\ntitle={Decorrelated Double Q-learning},\nauthor={GANG CHEN},\nyear={2021},\nurl={https://openreview.net/forum?id=jcN7a3yZeQc}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "jcN7a3yZeQc", "replyto": "jcN7a3yZeQc", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2391/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538097392, "tmdate": 1606915806329, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2391/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2391/-/Official_Review"}}}, {"id": "fftMr1r4arM", "original": null, "number": 4, "cdate": 1603900769204, "ddate": null, "tcdate": 1603900769204, "tmdate": 1605024221742, "tddate": null, "forum": "jcN7a3yZeQc", "replyto": "jcN7a3yZeQc", "invitation": "ICLR.cc/2021/Conference/Paper2391/-/Official_Review", "content": {"title": "This paper designs a variant of Double Q-learning to deal with overestimation and high variance, and provides experiment results to compare the new algorithm with previous ones.", "review": "Overall, I vote for rejecting. The core idea of the new algorithm looks interesting, but this paper does not provide convincing evidence theoretically and empirically. \n\nPros:\n\n1. This paper introduces a new variant of Double Q-learning to reduce the overestimation risk and reduce variance. \n2. In part of the experiments, the new algorithm demonstrates better performance than previous ones.\n\nCons:\n\n1. This paper does not provides enough insightful intuition and theoretical guarantees on the design of the new algorithm. There should be more explanation and evidence to help reader understand why for instance, the definition of Q-value in equation (8) makes sense. \n2. This paper is not well-written. There are missing references links, typos and missing definitions of some notations. For example, in the second paragraph of introduction, \u201cLillicrap et al.\u201d is just text and not linked to a paper in the references and there are many other cases like this. Typos can be seen in many places as well, such as in the first paragraph of section 3.3, it should be \u2018two-fold\u2019 instead of \u2018two folder\u2019; in the last paragraph of section 5, \u2018categories\u2019 should be \u2018categorised\u2019. Moreover, in the convergence analysis, the meaning of many notations are not explained at all (e.g. $\\alpha_t$). The author only says \u201cwe provide sketch of proof which borrows heavily from the proof of convergence of Double Q-learning and TD3\u201d, but without the definitions of the notations, the completeness of the paper is greatly undermined. ", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2391/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2391/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Decorrelated Double Q-learning", "authorids": ["~GANG_CHEN1"], "authors": ["GANG CHEN"], "keywords": ["q-learning", "control variates", "reinforcement learning"], "abstract": "Q-learning with value function approximation may have the poor performance because of overestimation bias and imprecise estimate. Specifically, overestimation bias is from the maximum operator over noise estimate, which is exaggerated using the estimate of a subsequent state. Inspired by the recent advance of deep reinforcement learning and Double Q-learning, we introduce the decorrelated double Q-learning (D2Q). Specifically, we introduce the decorrelated regularization item to reduce the correlation between value function approximators, which can lead to less biased estimation and low variance. The experimental results on a suite of MuJoCo continuous control tasks demonstrate that our decorrelated double Q-learning can effectively improve the performance.", "one-sentence_summary": "This paper proposes a decorrelated Double Q-learning for continuous task control", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chen|decorrelated_double_qlearning", "pdf": "/pdf/b0fb7882290e819c004f6b91a03606e5326dde8d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=n3vEJtQg2", "_bibtex": "@misc{\nchen2021decorrelated,\ntitle={Decorrelated Double Q-learning},\nauthor={GANG CHEN},\nyear={2021},\nurl={https://openreview.net/forum?id=jcN7a3yZeQc}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "jcN7a3yZeQc", "replyto": "jcN7a3yZeQc", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2391/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538097392, "tmdate": 1606915806329, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2391/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2391/-/Official_Review"}}}, {"id": "iLayvWD1I87", "original": null, "number": 3, "cdate": 1603752744844, "ddate": null, "tcdate": 1603752744844, "tmdate": 1605024221676, "tddate": null, "forum": "jcN7a3yZeQc", "replyto": "jcN7a3yZeQc", "invitation": "ICLR.cc/2021/Conference/Paper2391/-/Official_Review", "content": {"title": "Proof is hard to go through", "review": "The paper proposes a method that modifies double Q-learning by eliminating a linearly correlated part of one Q. I am not familiar with the proof of Double Q-learning and TD3, and thus find the proof of this paper hard to read as it omits the majority of proof by claiming it is similar to the proof of the aforementioned two algorithms. To name some of the part that confused me while reading: what is the definition of F^Q_t and c_t in (14)? Why does a small delta_2 exist in (16)? Why does Delta_t converge to zero as claimed in the line after (16)? Why is the randomness of s_{t+1} not mentioned in the subscripts of E's in (5) and (9)? etc. Therefore, I suggest the author(s) write a thorough proof and put it in the appendix to make the convergence analysis readable. I am also curious how the de-correlation term helps to improve the convergence in the analysis as it is the main contribution of this paper. Besides,  double Q-learning and TD are mostly used in function approximations. I wonder if the analysis can extend to some simple case of parameterized Q functions, e.g. linear approximations. The experiment part looks good to me as it compares D2Q with several sota algorithms and get satisfying results.", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2391/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2391/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Decorrelated Double Q-learning", "authorids": ["~GANG_CHEN1"], "authors": ["GANG CHEN"], "keywords": ["q-learning", "control variates", "reinforcement learning"], "abstract": "Q-learning with value function approximation may have the poor performance because of overestimation bias and imprecise estimate. Specifically, overestimation bias is from the maximum operator over noise estimate, which is exaggerated using the estimate of a subsequent state. Inspired by the recent advance of deep reinforcement learning and Double Q-learning, we introduce the decorrelated double Q-learning (D2Q). Specifically, we introduce the decorrelated regularization item to reduce the correlation between value function approximators, which can lead to less biased estimation and low variance. The experimental results on a suite of MuJoCo continuous control tasks demonstrate that our decorrelated double Q-learning can effectively improve the performance.", "one-sentence_summary": "This paper proposes a decorrelated Double Q-learning for continuous task control", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chen|decorrelated_double_qlearning", "pdf": "/pdf/b0fb7882290e819c004f6b91a03606e5326dde8d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=n3vEJtQg2", "_bibtex": "@misc{\nchen2021decorrelated,\ntitle={Decorrelated Double Q-learning},\nauthor={GANG CHEN},\nyear={2021},\nurl={https://openreview.net/forum?id=jcN7a3yZeQc}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "jcN7a3yZeQc", "replyto": "jcN7a3yZeQc", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2391/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538097392, "tmdate": 1606915806329, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2391/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2391/-/Official_Review"}}}], "count": 7}