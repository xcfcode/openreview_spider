{"notes": [{"id": "Byxl-04KvH", "original": "rkeE4JEdPr", "number": 953, "cdate": 1569439224397, "ddate": null, "tcdate": 1569439224397, "tmdate": 1577168218957, "tddate": null, "forum": "Byxl-04KvH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "NESTED LEARNING FOR MULTI-GRANULAR TASKS", "authors": ["Rapha\u00ebl Achddou", "J. Matias Di Martino", "Guillermo Sapiro"], "authorids": ["raphael.achddou@telecom-paristech.fr", "matiasdm@fing.edu.uy", "guillermo.sapiro@duke.edu"], "keywords": ["Nested learning"], "TL;DR": "DNNs can learn nested representations and output different quality of predictions with their respective confidence.", "abstract": "Standard deep neural networks (DNNs) used for classification are trained in an end-to-end fashion for very specific tasks - object recognition, face identification, character recognition, etc. This specificity often leads to overconfident models that generalize poorly to samples that are not from the original training distribution. Moreover, they do not allow to leverage information from heterogeneously annotated data, where for example, labels may be provided with different levels of granularity. Finally, standard DNNs do not produce results with simultaneous different levels of confidence for different levels of detail, they are most commonly an all or nothing approach. To address these challenges, we introduce the problem of nested learning: how to obtain a hierarchical representation of the input such that a coarse label can be extracted first, and sequentially refine this representation to obtain successively refined predictions, all of them with the corresponding confidence. We explicitly enforce this behaviour by creating a sequence of nested information bottlenecks. Looking at the problem of nested learning from an in formation theory perspective, we design a network topology with two important properties. First, a sequence of low dimensional (nested) feature embeddings are enforced. Then we show how the explicit combination of nested outputs can improve both robustness and finer predictions. Experimental results on CIFAR-10, MNIST, and FASHION-MNIST demonstrate that nested learning outperforms the same network trained in the standard end-to-end fashion. Since the network can be naturally trained with mixed data labeled at different levels of nested details, we also study what is the most efficient way of annotating data, when a fixed training budget is given and the cost of labels increases with the levels in the nested hierarchy.", "pdf": "/pdf/11f08f8e506ed4972802ee27f73d6e9536de337e.pdf", "code": "https://github.com/nestedlearning2019", "paperhash": "achddou|nested_learning_for_multigranular_tasks", "original_pdf": "/attachment/0cc3d72f27c4cffed7d915fd91793ee1ab5981f8.pdf", "_bibtex": "@misc{\nachddou2020nested,\ntitle={{\\{}NESTED{\\}} {\\{}LEARNING{\\}} {\\{}FOR{\\}} {\\{}MULTI{\\}}-{\\{}GRANULAR{\\}} {\\{}TASKS{\\}}},\nauthor={Rapha{\\\"e}l Achddou and J. Matias Di Martino and Guillermo Sapiro},\nyear={2020},\nurl={https://openreview.net/forum?id=Byxl-04KvH}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "a1zJS2QinU", "original": null, "number": 1, "cdate": 1576798710570, "ddate": null, "tcdate": 1576798710570, "tmdate": 1576800925776, "tddate": null, "forum": "Byxl-04KvH", "replyto": "Byxl-04KvH", "invitation": "ICLR.cc/2020/Conference/Paper953/-/Decision", "content": {"decision": "Reject", "comment": "This paper proposes a model architecture and training procedure for multiple nested label sets of varying granularities and shows improvements in efficiency over simple baselines in the number of fine-grained training labels needed to reach a given level of performance.\n\nReviewers did not raise any serious concerns about the method that was presented, but they were also not convinced that it represented a sufficiently novel or impactful contribution to an open problem. Without any reviewer advocating for the paper, even after discussion, I have no choice but to recommend rejection.\n\nI'm open to the possibility that there is substantial technical value here, but I think this work would be well served by more extensive comparisons and a potentially revamped motivation to try to make the case for it that value more directly.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NESTED LEARNING FOR MULTI-GRANULAR TASKS", "authors": ["Rapha\u00ebl Achddou", "J. Matias Di Martino", "Guillermo Sapiro"], "authorids": ["raphael.achddou@telecom-paristech.fr", "matiasdm@fing.edu.uy", "guillermo.sapiro@duke.edu"], "keywords": ["Nested learning"], "TL;DR": "DNNs can learn nested representations and output different quality of predictions with their respective confidence.", "abstract": "Standard deep neural networks (DNNs) used for classification are trained in an end-to-end fashion for very specific tasks - object recognition, face identification, character recognition, etc. This specificity often leads to overconfident models that generalize poorly to samples that are not from the original training distribution. Moreover, they do not allow to leverage information from heterogeneously annotated data, where for example, labels may be provided with different levels of granularity. Finally, standard DNNs do not produce results with simultaneous different levels of confidence for different levels of detail, they are most commonly an all or nothing approach. To address these challenges, we introduce the problem of nested learning: how to obtain a hierarchical representation of the input such that a coarse label can be extracted first, and sequentially refine this representation to obtain successively refined predictions, all of them with the corresponding confidence. We explicitly enforce this behaviour by creating a sequence of nested information bottlenecks. Looking at the problem of nested learning from an in formation theory perspective, we design a network topology with two important properties. First, a sequence of low dimensional (nested) feature embeddings are enforced. Then we show how the explicit combination of nested outputs can improve both robustness and finer predictions. Experimental results on CIFAR-10, MNIST, and FASHION-MNIST demonstrate that nested learning outperforms the same network trained in the standard end-to-end fashion. Since the network can be naturally trained with mixed data labeled at different levels of nested details, we also study what is the most efficient way of annotating data, when a fixed training budget is given and the cost of labels increases with the levels in the nested hierarchy.", "pdf": "/pdf/11f08f8e506ed4972802ee27f73d6e9536de337e.pdf", "code": "https://github.com/nestedlearning2019", "paperhash": "achddou|nested_learning_for_multigranular_tasks", "original_pdf": "/attachment/0cc3d72f27c4cffed7d915fd91793ee1ab5981f8.pdf", "_bibtex": "@misc{\nachddou2020nested,\ntitle={{\\{}NESTED{\\}} {\\{}LEARNING{\\}} {\\{}FOR{\\}} {\\{}MULTI{\\}}-{\\{}GRANULAR{\\}} {\\{}TASKS{\\}}},\nauthor={Rapha{\\\"e}l Achddou and J. Matias Di Martino and Guillermo Sapiro},\nyear={2020},\nurl={https://openreview.net/forum?id=Byxl-04KvH}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "Byxl-04KvH", "replyto": "Byxl-04KvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795708178, "tmdate": 1576800256534, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper953/-/Decision"}}}, {"id": "B1giGrB0FS", "original": null, "number": 2, "cdate": 1571865875108, "ddate": null, "tcdate": 1571865875108, "tmdate": 1574198750667, "tddate": null, "forum": "Byxl-04KvH", "replyto": "Byxl-04KvH", "invitation": "ICLR.cc/2020/Conference/Paper953/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #1", "review": "I read the author response, thank you for responding to my questions.\n\nOriginal review:\n\nThis paper presents a hierarchical learning approach that trains neural network classifiers on a known hierarchy of labels.  The experiments show that the approach makes a network more robust to distortion compared to standard end-to-end learning.  The approach in this paper and its formalization of the task are interesting, but the significance of the techniques is somewhat unclear and the experiments could be more thorough in terms of the baselines and data sets considered.\n\nFrom the related work section, the relationship between this paper and the previous work is somewhat complicated -- it is hard for a reader not deeply familiar with these previous works to understand the unique contribution made in the submission, and assess why it is significant.  For example, in the last paragraph of that section, many of the distinctions drawn between the submitted work and previous methods seem minor (including a confidence measure for a certain prediction, for example) or seemingly subjective (about whether an operation with a previous method was \u201cnatural\u201d or could be done \u201ctransparently\u201d).  Making crisper, less ambiguous distinctions between this work and previous work would help.\n\nLikewise, the experimental results here do not compare against any of the hierarchical learning approaches discussed in the related work section.  The results show that the paper\u2019s approach is more robust to distortion compared to standard end-to-end learning.  However, I was unclear on why it was not appropriate to compare against the other hierarchical learning methods from previous work.  Also, if the claim is improved robustness, I feel that evaluating against adversarial training (Madry et al., ICLR 2017) or similar approaches is necessary to understand the practical relevance of these improvements.\n\nFinally, experiments that consider larger hierarchies (here, the number of target classes tends to be small, CIFAR-10 and MNIST each have ten classes, meaning the hierarchies are not very rich) would help illustrate the potential power of the techniques.\n\nMinor\nI didn\u2019t understand the following statement, and given that it\u2019s a fairly bold claim I would rephrase it or explain it better in the paper body rather than referring the reader to the appendix:\n\u201cA standard DNN unknowingly uses low quality data also to train higher layers, even if there is no high level information in the data.\u201d\n\nI don\u2019t understand what the right arrow operator on the top of page 5 means.  From earlier statements it seems that I(f_i(X), Y_i) -> I(X, Y_i) means that the left quantity is approximately equal to the right.  But I\u2019m not sure why to use an arrow for that rather than an \\approx symbol.  The arrow would seem to imply that the left approaches the right in the limit, but if that is what you mean you should tell us in the limit of what.  Later the arrow is used to represent links in a Markov chain, adding further confusion for me.\n\nI think it would be helpful if before Equation 1, you mentioned this holds for strictly nested Y_i\u2019s (since earlier in the paper, Y_i referred to more general things).\n \nI assume the ECE is computed over held-out validation data (i.e., not training data)?  The paper should say this.\n\nPage 7: lineal -> linear", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper953/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper953/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NESTED LEARNING FOR MULTI-GRANULAR TASKS", "authors": ["Rapha\u00ebl Achddou", "J. Matias Di Martino", "Guillermo Sapiro"], "authorids": ["raphael.achddou@telecom-paristech.fr", "matiasdm@fing.edu.uy", "guillermo.sapiro@duke.edu"], "keywords": ["Nested learning"], "TL;DR": "DNNs can learn nested representations and output different quality of predictions with their respective confidence.", "abstract": "Standard deep neural networks (DNNs) used for classification are trained in an end-to-end fashion for very specific tasks - object recognition, face identification, character recognition, etc. This specificity often leads to overconfident models that generalize poorly to samples that are not from the original training distribution. Moreover, they do not allow to leverage information from heterogeneously annotated data, where for example, labels may be provided with different levels of granularity. Finally, standard DNNs do not produce results with simultaneous different levels of confidence for different levels of detail, they are most commonly an all or nothing approach. To address these challenges, we introduce the problem of nested learning: how to obtain a hierarchical representation of the input such that a coarse label can be extracted first, and sequentially refine this representation to obtain successively refined predictions, all of them with the corresponding confidence. We explicitly enforce this behaviour by creating a sequence of nested information bottlenecks. Looking at the problem of nested learning from an in formation theory perspective, we design a network topology with two important properties. First, a sequence of low dimensional (nested) feature embeddings are enforced. Then we show how the explicit combination of nested outputs can improve both robustness and finer predictions. Experimental results on CIFAR-10, MNIST, and FASHION-MNIST demonstrate that nested learning outperforms the same network trained in the standard end-to-end fashion. Since the network can be naturally trained with mixed data labeled at different levels of nested details, we also study what is the most efficient way of annotating data, when a fixed training budget is given and the cost of labels increases with the levels in the nested hierarchy.", "pdf": "/pdf/11f08f8e506ed4972802ee27f73d6e9536de337e.pdf", "code": "https://github.com/nestedlearning2019", "paperhash": "achddou|nested_learning_for_multigranular_tasks", "original_pdf": "/attachment/0cc3d72f27c4cffed7d915fd91793ee1ab5981f8.pdf", "_bibtex": "@misc{\nachddou2020nested,\ntitle={{\\{}NESTED{\\}} {\\{}LEARNING{\\}} {\\{}FOR{\\}} {\\{}MULTI{\\}}-{\\{}GRANULAR{\\}} {\\{}TASKS{\\}}},\nauthor={Rapha{\\\"e}l Achddou and J. Matias Di Martino and Guillermo Sapiro},\nyear={2020},\nurl={https://openreview.net/forum?id=Byxl-04KvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Byxl-04KvH", "replyto": "Byxl-04KvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper953/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper953/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575422130754, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper953/Reviewers"], "noninvitees": [], "tcdate": 1570237744561, "tmdate": 1575422130768, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper953/-/Official_Review"}}}, {"id": "Skl-AqPojB", "original": null, "number": 1, "cdate": 1573776073506, "ddate": null, "tcdate": 1573776073506, "tmdate": 1573776683030, "tddate": null, "forum": "Byxl-04KvH", "replyto": "HygywE9K5B", "invitation": "ICLR.cc/2020/Conference/Paper953/-/Official_Comment", "content": {"title": "Answers to Review #4", "comment": "We would like to start by sincerely thanking the reviewer for his/her time and dedication reviewing this paper. He/she presents interesting comments and questions that we address in the following. \n\nQ: Perhaps show ....\nA: We performed additional experiments following this suggestions (see Section F.3, Figures 13 and Table 8). \n\nQ: What do you think is the role ....\nA: Yes, it is definitely worth exploring different training regimes as they play an important role in the effectiveness and efficiency of the training of nested solutions. We included additional experiments in the updated version of the paper showing the impact on training of different training protocols in (see new Section F.3, and in particular, the experiments presented in Figure 13 and Table 8). \n\nQ: Generalization to any number of nested ....\nA: We do demonstrate that our ideas can be applied in practice with two and three nested levels. This is substantially more general than previous work that is developed exclusively for two nested levels, see e.g. (Yan et al. 2015). In addition as the proposed method is general we think testing it in problems with more hierarchies -when data becomes available- is going to be straightforward (our code is also already available online). From a theoretical perspective, we do develop all our ideas with a general number of nested labels in mind (in contrast with previous work). The proposed guidelines for the network architecture, the combination of the outputs, the training protocol and so forth are generalize to an arbitrary number of nested levels. \n\nQ: The empirical demonstration of skip connections ....\nA: The reviewer is absolutely right. We showed in the original submission the impact of  skipped connections in the overall performance of the network (page 8 Table 2), but we did not explicitly validate empirically the impact of skipped connections in the flow of information (as we argue in page 4 Section 3). In the updated version of the manuscript we added experiments showing the impact of skipped connections in the amount of mutual information between features above and below an information bottlenecks (see the updated appendix, Section F.2, and Figure 12). \n\nQ: Role of attention ...\nA: We think is an interesting idea we did not explore it in this work, we think it could be discussed in future work and merged with the ideas we present  as they are complementary. \n \nQ: Not clear why ....\nA: We believe the review is confused here due to a poor choice we made at defining the name of some classes (we corrected this issue in the updated version, and explain the confusion in the following). \nIn the experiment shown in Figure 1, the nested classes F1:Vehicle, F2: Wheels, and F3: Truck mean:\n- F1: The object is a vehicle (we name this class \"vehicle\")\n- F2: The object is a vehicle with wheels (in opposition, for example, to a boat or a plane). We named this class \"wheels\" to group ground vehicles, but not because we are actually detect or classify wheels specifically. \nWe hope this explanation makes the Figure 1 more clear, and evacuate the reviewers confusion.  We also changed the categories name in the paper to avoid this confusion in the future. \n\nQ: Not clear why .....\nA: The term \"output calibration\" is not introduced or defined by us, we followed the most common nomenclature convention (see, e.g., Zadronzny & Elkan (2002)), we clarified this in the updated version of the manuscript (pag 5.).\n\nQ: Why not use, for example, regularization ....\nThe proposed method is certainly an effective way of mitigating the problem of prediction overconfidence and is complementary to regularization methods. Indeed, the models we implemented include layers of batch normalization with has been proven to act as a weight regularization (see Luo et al. (2018)). We discussed this in more detail in the updated version of the manuscript (section B.1).  \n\nQ: Table 1 ....  \nA: This is a very stable observation. Indeed, we obtained consistent results on several runs and under different conditions. To provide a quantitatively measure of the stability of the results reported, we repeated 10 times the experiments presented in Table 1 and included in the updated version a bound on the standard deviation of the results.\n\nQ:Why is the marginal accuracy ....\nA: We believe that has to do with the amount of data, budget 2 is exactly two times budget 1 and that explains the large improvement between this two levels. On the other hand, after a certain level of accuracy is achieved the effect of adding data start to saturate, and that is why between budget 2 and 4 the relative improvement is more modest.  In regards to the claim that the nested model \"gradually breaks\" an important distinction should be made. More precisely, by \"gradually breaks\" we mean that as the quality of the input data deteriorates, the prediction accuracy starts to gradually decrease from the finner to the coarser outputs (as illustrated in the examples in Figure 5)\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper953/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper953/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NESTED LEARNING FOR MULTI-GRANULAR TASKS", "authors": ["Rapha\u00ebl Achddou", "J. Matias Di Martino", "Guillermo Sapiro"], "authorids": ["raphael.achddou@telecom-paristech.fr", "matiasdm@fing.edu.uy", "guillermo.sapiro@duke.edu"], "keywords": ["Nested learning"], "TL;DR": "DNNs can learn nested representations and output different quality of predictions with their respective confidence.", "abstract": "Standard deep neural networks (DNNs) used for classification are trained in an end-to-end fashion for very specific tasks - object recognition, face identification, character recognition, etc. This specificity often leads to overconfident models that generalize poorly to samples that are not from the original training distribution. Moreover, they do not allow to leverage information from heterogeneously annotated data, where for example, labels may be provided with different levels of granularity. Finally, standard DNNs do not produce results with simultaneous different levels of confidence for different levels of detail, they are most commonly an all or nothing approach. To address these challenges, we introduce the problem of nested learning: how to obtain a hierarchical representation of the input such that a coarse label can be extracted first, and sequentially refine this representation to obtain successively refined predictions, all of them with the corresponding confidence. We explicitly enforce this behaviour by creating a sequence of nested information bottlenecks. Looking at the problem of nested learning from an in formation theory perspective, we design a network topology with two important properties. First, a sequence of low dimensional (nested) feature embeddings are enforced. Then we show how the explicit combination of nested outputs can improve both robustness and finer predictions. Experimental results on CIFAR-10, MNIST, and FASHION-MNIST demonstrate that nested learning outperforms the same network trained in the standard end-to-end fashion. Since the network can be naturally trained with mixed data labeled at different levels of nested details, we also study what is the most efficient way of annotating data, when a fixed training budget is given and the cost of labels increases with the levels in the nested hierarchy.", "pdf": "/pdf/11f08f8e506ed4972802ee27f73d6e9536de337e.pdf", "code": "https://github.com/nestedlearning2019", "paperhash": "achddou|nested_learning_for_multigranular_tasks", "original_pdf": "/attachment/0cc3d72f27c4cffed7d915fd91793ee1ab5981f8.pdf", "_bibtex": "@misc{\nachddou2020nested,\ntitle={{\\{}NESTED{\\}} {\\{}LEARNING{\\}} {\\{}FOR{\\}} {\\{}MULTI{\\}}-{\\{}GRANULAR{\\}} {\\{}TASKS{\\}}},\nauthor={Rapha{\\\"e}l Achddou and J. Matias Di Martino and Guillermo Sapiro},\nyear={2020},\nurl={https://openreview.net/forum?id=Byxl-04KvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Byxl-04KvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper953/Authors", "ICLR.cc/2020/Conference/Paper953/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper953/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper953/Reviewers", "ICLR.cc/2020/Conference/Paper953/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper953/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper953/Authors|ICLR.cc/2020/Conference/Paper953/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504163552, "tmdate": 1576860559553, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper953/Authors", "ICLR.cc/2020/Conference/Paper953/Reviewers", "ICLR.cc/2020/Conference/Paper953/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper953/-/Official_Comment"}}}, {"id": "rkeDzhwjjH", "original": null, "number": 2, "cdate": 1573776399230, "ddate": null, "tcdate": 1573776399230, "tmdate": 1573776649639, "tddate": null, "forum": "Byxl-04KvH", "replyto": "B1giGrB0FS", "invitation": "ICLR.cc/2020/Conference/Paper953/-/Official_Comment", "content": {"title": "Answers to Review #1", "comment": "We would like to start by sincerely thanking the reviewer for his/her time and dedication reviewing this paper. He/she presents interesting comments and questions that we address in the following.\n\nQ: From the related .... \nA: We reformulated the discussion of the related work and provided more crisper and precise details and comparisons between our formulation and previous work. This updated discussion is provided in the new version of the manuscript, last paragraph of Section 2, pag. 3.  \n\nQ: Likewise, the experimental results ....\nA: The proposed work focus on achieving a nested representation capable of leveraging data labeled at different levels of granularity and at the same time, capable of producing multiple nested outputs. As we show, as a consequence we obtain more robust and versatile solutions, though we do not explicitly aim at improving robustness as adversarial training does. In that sense, we think the ideas here proposed are actually complementary to those in adversarial learning. Moreover, it would be a very interesting future work to see how the ideas proposed here and standard techniques of adversarial training can be combined to improve even further the robustness of nested learning. In addition, we added an experiment in the updated version of the manuscript comparing the proposed approach with multi-task learning (see Section F.1. in the updated version of the supplementary material). For example, as shown in the new set of experiments presented in Table 7, the proposed architecture leads to a more robust representation compared to MTL (for a network trained on the same data and with approximately the same number of parameters). Again, this suggests that learning feature representations in a nested fashion inherently encourages models robustness (even though we do not explicitly enforce it as in adversarial learning). \n\nQ: I didn\u2019t understand ...\nA: We rephrase this claim and now it reads: \" ... when heterogeneous data with different quality and granularity of annotations (as in the example illustrated in Figure 1  (left)) is provided for training, low quality samples with coarse labels can help us to understand the structure of the coarser distributions (person, under 50) while simultaneously data with finer labels can contribute to the coarse and fine tasks. \" . \n\nQ: I don\u2019t understand what the right arrow ...\nA: We fixed this issue and make the notation more clear. \n\nQ: I think it would be helpful ....\nA: The reviewer is correct, we added this information in the updated version of the manuscript (last paragraph of Section 3, pag. 5). \n\nQ: Page 7: lineal -> linear\nWe fixed this. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper953/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper953/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NESTED LEARNING FOR MULTI-GRANULAR TASKS", "authors": ["Rapha\u00ebl Achddou", "J. Matias Di Martino", "Guillermo Sapiro"], "authorids": ["raphael.achddou@telecom-paristech.fr", "matiasdm@fing.edu.uy", "guillermo.sapiro@duke.edu"], "keywords": ["Nested learning"], "TL;DR": "DNNs can learn nested representations and output different quality of predictions with their respective confidence.", "abstract": "Standard deep neural networks (DNNs) used for classification are trained in an end-to-end fashion for very specific tasks - object recognition, face identification, character recognition, etc. This specificity often leads to overconfident models that generalize poorly to samples that are not from the original training distribution. Moreover, they do not allow to leverage information from heterogeneously annotated data, where for example, labels may be provided with different levels of granularity. Finally, standard DNNs do not produce results with simultaneous different levels of confidence for different levels of detail, they are most commonly an all or nothing approach. To address these challenges, we introduce the problem of nested learning: how to obtain a hierarchical representation of the input such that a coarse label can be extracted first, and sequentially refine this representation to obtain successively refined predictions, all of them with the corresponding confidence. We explicitly enforce this behaviour by creating a sequence of nested information bottlenecks. Looking at the problem of nested learning from an in formation theory perspective, we design a network topology with two important properties. First, a sequence of low dimensional (nested) feature embeddings are enforced. Then we show how the explicit combination of nested outputs can improve both robustness and finer predictions. Experimental results on CIFAR-10, MNIST, and FASHION-MNIST demonstrate that nested learning outperforms the same network trained in the standard end-to-end fashion. Since the network can be naturally trained with mixed data labeled at different levels of nested details, we also study what is the most efficient way of annotating data, when a fixed training budget is given and the cost of labels increases with the levels in the nested hierarchy.", "pdf": "/pdf/11f08f8e506ed4972802ee27f73d6e9536de337e.pdf", "code": "https://github.com/nestedlearning2019", "paperhash": "achddou|nested_learning_for_multigranular_tasks", "original_pdf": "/attachment/0cc3d72f27c4cffed7d915fd91793ee1ab5981f8.pdf", "_bibtex": "@misc{\nachddou2020nested,\ntitle={{\\{}NESTED{\\}} {\\{}LEARNING{\\}} {\\{}FOR{\\}} {\\{}MULTI{\\}}-{\\{}GRANULAR{\\}} {\\{}TASKS{\\}}},\nauthor={Rapha{\\\"e}l Achddou and J. Matias Di Martino and Guillermo Sapiro},\nyear={2020},\nurl={https://openreview.net/forum?id=Byxl-04KvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Byxl-04KvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper953/Authors", "ICLR.cc/2020/Conference/Paper953/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper953/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper953/Reviewers", "ICLR.cc/2020/Conference/Paper953/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper953/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper953/Authors|ICLR.cc/2020/Conference/Paper953/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504163552, "tmdate": 1576860559553, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper953/Authors", "ICLR.cc/2020/Conference/Paper953/Reviewers", "ICLR.cc/2020/Conference/Paper953/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper953/-/Official_Comment"}}}, {"id": "Hkg8xpwsjS", "original": null, "number": 3, "cdate": 1573776622293, "ddate": null, "tcdate": 1573776622293, "tmdate": 1573776622293, "tddate": null, "forum": "Byxl-04KvH", "replyto": "SyeESNS6YH", "invitation": "ICLR.cc/2020/Conference/Paper953/-/Official_Comment", "content": {"title": "Answers to Review #2", "comment": "We would like to start by sincerely thanking the reviewer for his/her time and dedication reviewing this paper. He/she presents interesting comments and questions that we address in the following.\n\nQ: The paper justifies the ... \nA: Following reviewers suggestion, we added three experiment to provide additional empirical evidence to the theoretical ideas presented in Section 3. (i) We compare the proposed nested architecture with a multi task learning (MTL) approach (see Section F.1 in the updated version of the supplementary material, and in particular the results shown in Table 7); (ii) We added experiments showing the impact of skipped connections in the amount of mutual information between features above and below an information bottlenecks (see supplementary material section F.2., Figure 12 and equations (6)-(7)); and (iii) We tested alternative training protocols as suggested by reviewer nr. 4 (Section F.3).  \n\nThe experiments show how the proposed nested learning provides more robust feature extraction for nested problems compared with its MTL counterpart. In addition we show that when skipped connections are included, more information flows to from the first layers to the features associated to finner classification. To this end, we used MINE algorithm (Belghazi et al. 2018) to obtain an empirical estimation of the mutual information of different features extracted in the network. The experiments provide additional empirical evidence that when skipped connections are included, more information flows from the first layers to the features associated to finer classification (approximately by a factor of two). This provides empirical support to the ideas discussed from a theoretical perspective in the Section 3 pag. 4.  \n\nQ: The novelty of the proposed ... \nA: To make more clear the novelty of this work, we substantially modified Section 2 to make more explicit the connection and differences between our work previous literature. In particular we highlight the main differences between our work and the work that is closer to ours: Kim et al.(2018), Yan et al. (2015), Triguero & Vens (2016), Wehrmann et al. (2018). There are key differences between our work previous approaches (please see the updated version of the manuscript for a detailed discussion, in particular, the last paragraph of Section 2). To the best of our knowledge the important framework of nested training and testing has not been explicitly proposed before.\n\nQ: it would be better if the proposed ...\nA: Following the reviewer\u2019s suggestion we compared the proposed architecture with a standard multi-task learning network. The results are presented in the updated version of the manuscript (Section F.1, Figure 11 and Table 7). The architecture of the MTL network is described in Figure 11. Both architectures (design to have roughly the same number of parameters) can be trained to achieve the simultaneous classification of nested labels, however as illustrates Table 7 (page 19) enforcing a nested feature representation significantly improves the robustness of the model.\n\nQ: Minor details Some citations should be enclosed in brackets\nA: We fixed this issue. \n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper953/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper953/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NESTED LEARNING FOR MULTI-GRANULAR TASKS", "authors": ["Rapha\u00ebl Achddou", "J. Matias Di Martino", "Guillermo Sapiro"], "authorids": ["raphael.achddou@telecom-paristech.fr", "matiasdm@fing.edu.uy", "guillermo.sapiro@duke.edu"], "keywords": ["Nested learning"], "TL;DR": "DNNs can learn nested representations and output different quality of predictions with their respective confidence.", "abstract": "Standard deep neural networks (DNNs) used for classification are trained in an end-to-end fashion for very specific tasks - object recognition, face identification, character recognition, etc. This specificity often leads to overconfident models that generalize poorly to samples that are not from the original training distribution. Moreover, they do not allow to leverage information from heterogeneously annotated data, where for example, labels may be provided with different levels of granularity. Finally, standard DNNs do not produce results with simultaneous different levels of confidence for different levels of detail, they are most commonly an all or nothing approach. To address these challenges, we introduce the problem of nested learning: how to obtain a hierarchical representation of the input such that a coarse label can be extracted first, and sequentially refine this representation to obtain successively refined predictions, all of them with the corresponding confidence. We explicitly enforce this behaviour by creating a sequence of nested information bottlenecks. Looking at the problem of nested learning from an in formation theory perspective, we design a network topology with two important properties. First, a sequence of low dimensional (nested) feature embeddings are enforced. Then we show how the explicit combination of nested outputs can improve both robustness and finer predictions. Experimental results on CIFAR-10, MNIST, and FASHION-MNIST demonstrate that nested learning outperforms the same network trained in the standard end-to-end fashion. Since the network can be naturally trained with mixed data labeled at different levels of nested details, we also study what is the most efficient way of annotating data, when a fixed training budget is given and the cost of labels increases with the levels in the nested hierarchy.", "pdf": "/pdf/11f08f8e506ed4972802ee27f73d6e9536de337e.pdf", "code": "https://github.com/nestedlearning2019", "paperhash": "achddou|nested_learning_for_multigranular_tasks", "original_pdf": "/attachment/0cc3d72f27c4cffed7d915fd91793ee1ab5981f8.pdf", "_bibtex": "@misc{\nachddou2020nested,\ntitle={{\\{}NESTED{\\}} {\\{}LEARNING{\\}} {\\{}FOR{\\}} {\\{}MULTI{\\}}-{\\{}GRANULAR{\\}} {\\{}TASKS{\\}}},\nauthor={Rapha{\\\"e}l Achddou and J. Matias Di Martino and Guillermo Sapiro},\nyear={2020},\nurl={https://openreview.net/forum?id=Byxl-04KvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Byxl-04KvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper953/Authors", "ICLR.cc/2020/Conference/Paper953/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper953/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper953/Reviewers", "ICLR.cc/2020/Conference/Paper953/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper953/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper953/Authors|ICLR.cc/2020/Conference/Paper953/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504163552, "tmdate": 1576860559553, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper953/Authors", "ICLR.cc/2020/Conference/Paper953/Reviewers", "ICLR.cc/2020/Conference/Paper953/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper953/-/Official_Comment"}}}, {"id": "SyeESNS6YH", "original": null, "number": 1, "cdate": 1571800123938, "ddate": null, "tcdate": 1571800123938, "tmdate": 1572972531233, "tddate": null, "forum": "Byxl-04KvH", "replyto": "Byxl-04KvH", "invitation": "ICLR.cc/2020/Conference/Paper953/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Nested learning for multi-granular tasks\n\n1. Summary\nThe paper considers a framework for classification with a hierarchy of labels [from coarse to fine]. The paper proposes a network architecture with multiple bottleneck layers, one for each label level, and skip connections. The objective function is the standard classification loss. The experiments show that coarse labels help learning and can improve label efficiency, i.e. don\u2019t need all fine labels to get good classification performance.\n\n2. Opinion and rationales\n\nWhilst I think the execution of ideas is good and the motivation is very practical, I\u2019m leaning towards \u201creject\u201d for this paper due to the reasons below. I welcome the authors\u2019 clarification and am willing to reconsider my view.\n\ni. The paper justifies the proposed architecture as successive information compression into label embeddings using some entropy based criteria (as in information bottleneck literature). This ensures the relationship of the entropies between the label layers. However, I\u2019m not sure this justification is necessary (albeit being a valid one) given that the training/later sections do not come back to this justification.\n\nii. The novelty of the proposed architecture and training approach is low. The network is a nested structure of successive classifiers. The proposed calibration using rejection class and temperature scaling is not new.\n\niii. it would be better if the proposed architecture + method are validated on more real-world datasets with a more natural label grouping scheme, and compared to alternative architectures, e.g. multi-task learning with a shared network except the output layer.\n\n3. Minor details\n\nSome citations should be enclosed in brackets"}, "signatures": ["ICLR.cc/2020/Conference/Paper953/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper953/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NESTED LEARNING FOR MULTI-GRANULAR TASKS", "authors": ["Rapha\u00ebl Achddou", "J. Matias Di Martino", "Guillermo Sapiro"], "authorids": ["raphael.achddou@telecom-paristech.fr", "matiasdm@fing.edu.uy", "guillermo.sapiro@duke.edu"], "keywords": ["Nested learning"], "TL;DR": "DNNs can learn nested representations and output different quality of predictions with their respective confidence.", "abstract": "Standard deep neural networks (DNNs) used for classification are trained in an end-to-end fashion for very specific tasks - object recognition, face identification, character recognition, etc. This specificity often leads to overconfident models that generalize poorly to samples that are not from the original training distribution. Moreover, they do not allow to leverage information from heterogeneously annotated data, where for example, labels may be provided with different levels of granularity. Finally, standard DNNs do not produce results with simultaneous different levels of confidence for different levels of detail, they are most commonly an all or nothing approach. To address these challenges, we introduce the problem of nested learning: how to obtain a hierarchical representation of the input such that a coarse label can be extracted first, and sequentially refine this representation to obtain successively refined predictions, all of them with the corresponding confidence. We explicitly enforce this behaviour by creating a sequence of nested information bottlenecks. Looking at the problem of nested learning from an in formation theory perspective, we design a network topology with two important properties. First, a sequence of low dimensional (nested) feature embeddings are enforced. Then we show how the explicit combination of nested outputs can improve both robustness and finer predictions. Experimental results on CIFAR-10, MNIST, and FASHION-MNIST demonstrate that nested learning outperforms the same network trained in the standard end-to-end fashion. Since the network can be naturally trained with mixed data labeled at different levels of nested details, we also study what is the most efficient way of annotating data, when a fixed training budget is given and the cost of labels increases with the levels in the nested hierarchy.", "pdf": "/pdf/11f08f8e506ed4972802ee27f73d6e9536de337e.pdf", "code": "https://github.com/nestedlearning2019", "paperhash": "achddou|nested_learning_for_multigranular_tasks", "original_pdf": "/attachment/0cc3d72f27c4cffed7d915fd91793ee1ab5981f8.pdf", "_bibtex": "@misc{\nachddou2020nested,\ntitle={{\\{}NESTED{\\}} {\\{}LEARNING{\\}} {\\{}FOR{\\}} {\\{}MULTI{\\}}-{\\{}GRANULAR{\\}} {\\{}TASKS{\\}}},\nauthor={Rapha{\\\"e}l Achddou and J. Matias Di Martino and Guillermo Sapiro},\nyear={2020},\nurl={https://openreview.net/forum?id=Byxl-04KvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Byxl-04KvH", "replyto": "Byxl-04KvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper953/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper953/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575422130754, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper953/Reviewers"], "noninvitees": [], "tcdate": 1570237744561, "tmdate": 1575422130768, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper953/-/Official_Review"}}}, {"id": "HygywE9K5B", "original": null, "number": 3, "cdate": 1572607062834, "ddate": null, "tcdate": 1572607062834, "tmdate": 1572972531137, "tddate": null, "forum": "Byxl-04KvH", "replyto": "Byxl-04KvH", "invitation": "ICLR.cc/2020/Conference/Paper953/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "Summary: The problem addressed is how to train a DNN to learn a hierarchical representation of the input that has heterogeneously annotated labels at various levels of granularity. After training the network\u2019s goal is to emit sequentially finer grained labels with corresponding confidence. The authors use information theory to propose a network typology of information bottlenecks with skip connections to achieve this nested learning problem.  \n\nDecision: weak reject. \n\nReason: The proposal of learning a hierarchical representation is not new. Nor is the architecture. I think the interesting points (that are not really flesh out as much, but appear to be in the auxiliary material - section B.2) is the training regime. I would\u2019ve liked to have seen more of what the role of the training regime is on the outcomes and how the network\u2019s gradient\u2019s behave in different regimes. But in general, the paper is well organized and argued, although there is a little belaboring of ideas of entropy and mutual information only to use it to buttress a point that is made and left hanging. The paper also defines too many concepts for every proposition it wants to support, making it cognitively costly to follow. \n\nFeedback:\nPleasure reading the paper. Few points of feedback:\n\n- Perhaps show the network gradients to help understand the dynamics of the learning\n- What do you think is the role of the training regime (section B.2) on the outcomes you are observing? Do you think it would be worth observing the effect of changing the learning regime (and the accompanying gradient) on the outcomes?\n- Generalization to any number of nested labels is not demonstrated\n- The empirical demonstration of contribution of skip connections is not too  \n- Corollary to above, what do you think would be the role of attention in the hierarchical representation learning?\n\nQuestions:\n1- Not clear why complementarity is a necessary condition of learning (section 3, before definition 1). Take for example the vehicle, wheels, truck example in figure 1. Learning F2 (wheels) features isn\u2019t conditioned on correctly learning F1 (vehicle). In fact, could it be satisfactory, to first order approximation, to assume that learning finer-grained features first (roundness of wheels) and then combining lower level features (in what you refer to as Markov chain) may result in equal if not better accuracies? Is it possible to test this?\n\n2- Not clear why calibration of outputs (section \u201ccombination of nested outputs\u201d in p 5) is an approximation of P(Y_i=q).\n\n3- Its not clear to me why the proposed rejection calibration method as a way of handling overconfidence is the right approach. Why this solution? Why not use, for example, regularization instead?\n\n4- Table 1 of results. As the distortions increases the relative improvements of the nested learner appears to increase in CIFAR-10 results. Can you demonstrate this is not an artifact of the distortion generation strategy and is indeed a stable observation?\n\n5- why is the marginal accuracy improvements so much larger for going from a budget of 1 to 2 than 2 to 5 in all cost functions? Does this not refute the claim that the nested model \u201cgradually breaks\u201d?"}, "signatures": ["ICLR.cc/2020/Conference/Paper953/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper953/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NESTED LEARNING FOR MULTI-GRANULAR TASKS", "authors": ["Rapha\u00ebl Achddou", "J. Matias Di Martino", "Guillermo Sapiro"], "authorids": ["raphael.achddou@telecom-paristech.fr", "matiasdm@fing.edu.uy", "guillermo.sapiro@duke.edu"], "keywords": ["Nested learning"], "TL;DR": "DNNs can learn nested representations and output different quality of predictions with their respective confidence.", "abstract": "Standard deep neural networks (DNNs) used for classification are trained in an end-to-end fashion for very specific tasks - object recognition, face identification, character recognition, etc. This specificity often leads to overconfident models that generalize poorly to samples that are not from the original training distribution. Moreover, they do not allow to leverage information from heterogeneously annotated data, where for example, labels may be provided with different levels of granularity. Finally, standard DNNs do not produce results with simultaneous different levels of confidence for different levels of detail, they are most commonly an all or nothing approach. To address these challenges, we introduce the problem of nested learning: how to obtain a hierarchical representation of the input such that a coarse label can be extracted first, and sequentially refine this representation to obtain successively refined predictions, all of them with the corresponding confidence. We explicitly enforce this behaviour by creating a sequence of nested information bottlenecks. Looking at the problem of nested learning from an in formation theory perspective, we design a network topology with two important properties. First, a sequence of low dimensional (nested) feature embeddings are enforced. Then we show how the explicit combination of nested outputs can improve both robustness and finer predictions. Experimental results on CIFAR-10, MNIST, and FASHION-MNIST demonstrate that nested learning outperforms the same network trained in the standard end-to-end fashion. Since the network can be naturally trained with mixed data labeled at different levels of nested details, we also study what is the most efficient way of annotating data, when a fixed training budget is given and the cost of labels increases with the levels in the nested hierarchy.", "pdf": "/pdf/11f08f8e506ed4972802ee27f73d6e9536de337e.pdf", "code": "https://github.com/nestedlearning2019", "paperhash": "achddou|nested_learning_for_multigranular_tasks", "original_pdf": "/attachment/0cc3d72f27c4cffed7d915fd91793ee1ab5981f8.pdf", "_bibtex": "@misc{\nachddou2020nested,\ntitle={{\\{}NESTED{\\}} {\\{}LEARNING{\\}} {\\{}FOR{\\}} {\\{}MULTI{\\}}-{\\{}GRANULAR{\\}} {\\{}TASKS{\\}}},\nauthor={Rapha{\\\"e}l Achddou and J. Matias Di Martino and Guillermo Sapiro},\nyear={2020},\nurl={https://openreview.net/forum?id=Byxl-04KvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Byxl-04KvH", "replyto": "Byxl-04KvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper953/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper953/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575422130754, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper953/Reviewers"], "noninvitees": [], "tcdate": 1570237744561, "tmdate": 1575422130768, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper953/-/Official_Review"}}}], "count": 8}