{"notes": [{"tddate": null, "ddate": null, "cdate": null, "original": null, "tmdate": 1490028627677, "tcdate": 1490028627677, "number": 1, "id": "BynU_tTog", "invitation": "ICLR.cc/2017/workshop/-/paper143/acceptance", "forum": "BkyScySKl", "replyto": "BkyScySKl", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Accept", "title": "ICLR committee final decision"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Joint Embeddings of Scene Graphs and Images", "abstract": "Multimodal representations of text and images have become popular in recent years. Text however has inherent ambiguities when describing visual scenes, leading to the recent development of datasets with detailed graphical descriptions in the form of scene graphs. We consider the task of joint representation of semantically precise scene graphs and images. We propose models for representing scene graphs and aligning them with images.  We investigate methods based on bag-of-words, subpath representations, as well as neural networks. Our investigation proposes and contrasts several models which can address this task and highlights some unique challenges in both designing models and evaluation.", "pdf": "/pdf/20108157e891d08f1d3b5c2c286e73b5870e9a74.pdf", "TL;DR": "We propose models for embedding scene graphs  in a joint space with images", "paperhash": "belilovsky|joint_embeddings_of_scene_graphs_and_images", "keywords": [], "conflicts": ["inria.fr", "centralesupelec.fr", "kuleuven.be", "cs.toronto.edu"], "authors": ["Eugene Belilovsky", "Matthew Blaschko", "Jamie Ryan Kiros", "Raquel Urtasun", "Richard Zemel"], "authorids": ["eugene.belilovsky@inria.fr", "matthew.blaschko@esat.kuleuven.be", "rkiros@cs.toronto.edu", "urtasun@cs.toronto.edu", "zemel@cs.toronto.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1490028628224, "id": "ICLR.cc/2017/workshop/-/paper143/acceptance", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "BkyScySKl", "replyto": "BkyScySKl", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept", "Reject"]}}}, "nonreaders": [], "cdate": 1490028628224}}}, {"tddate": null, "tmdate": 1489173782988, "tcdate": 1489173782988, "number": 2, "id": "S1y76deie", "invitation": "ICLR.cc/2017/workshop/-/paper143/official/review", "forum": "BkyScySKl", "replyto": "BkyScySKl", "signatures": ["ICLR.cc/2017/workshop/paper143/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper143/AnonReviewer1"], "content": {"title": "Review", "rating": "7: Good paper, accept", "review": "This paper investigates a set of simple models for generating scene and image graph embeddings. Scene graphs are represented either with count features on their constituent nodes, count features on their constituent nodes and short paths, or convolutionally. A margin objective is then used to learn projections from the space of image features and graph representations into a joint embedding space. This paper finds that on a dataset of scene graphs, the representation based on path counts outperforms the other two approaches both in identifying similar images to the one for which the graph was annotated, and in retrieving the target image.\n\nThis is a clean, focused, and well presented contribution. It's an interesting result that the approach based on path counts outperforms the convolutional / GraphNN approach---it seems like count-based models have generally been on the way out (at least in machine translation and language modeling). Presumably it's the relatively small size of the training data that makes them still useful here. It might be useful to mention how you think this approach might scale to larger datasets. How big is the object vocabulary? How many subpaths occur in the test set but not the training set? Are you doing anything else (e.g. smoothing, backoff) to deal with count sparsity?", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Joint Embeddings of Scene Graphs and Images", "abstract": "Multimodal representations of text and images have become popular in recent years. Text however has inherent ambiguities when describing visual scenes, leading to the recent development of datasets with detailed graphical descriptions in the form of scene graphs. We consider the task of joint representation of semantically precise scene graphs and images. We propose models for representing scene graphs and aligning them with images.  We investigate methods based on bag-of-words, subpath representations, as well as neural networks. Our investigation proposes and contrasts several models which can address this task and highlights some unique challenges in both designing models and evaluation.", "pdf": "/pdf/20108157e891d08f1d3b5c2c286e73b5870e9a74.pdf", "TL;DR": "We propose models for embedding scene graphs  in a joint space with images", "paperhash": "belilovsky|joint_embeddings_of_scene_graphs_and_images", "keywords": [], "conflicts": ["inria.fr", "centralesupelec.fr", "kuleuven.be", "cs.toronto.edu"], "authors": ["Eugene Belilovsky", "Matthew Blaschko", "Jamie Ryan Kiros", "Raquel Urtasun", "Richard Zemel"], "authorids": ["eugene.belilovsky@inria.fr", "matthew.blaschko@esat.kuleuven.be", "rkiros@cs.toronto.edu", "urtasun@cs.toronto.edu", "zemel@cs.toronto.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489173783625, "id": "ICLR.cc/2017/workshop/-/paper143/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper143/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper143/AnonReviewer2", "ICLR.cc/2017/workshop/paper143/AnonReviewer1"], "reply": {"forum": "BkyScySKl", "replyto": "BkyScySKl", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper143/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper143/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489173783625}}}, {"tddate": null, "tmdate": 1488103804323, "tcdate": 1488103804323, "number": 1, "id": "S1VYt7lcl", "invitation": "ICLR.cc/2017/workshop/-/paper143/official/review", "forum": "BkyScySKl", "replyto": "BkyScySKl", "signatures": ["ICLR.cc/2017/workshop/paper143/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper143/AnonReviewer2"], "content": {"title": "Strong baselines for scene graph prediction", "rating": "8: Top 50% of accepted papers, clear accept", "review": "The submission studies scene graph prediction via joint embeddings of images and graphs. It evaluates two different embeddings (one of which is a simple baseline). The graph embeddings are evaluated in ranking experiments. Interestingly, a representation that is essentially a \"bag of small subgraphs\" performs very competitively; it substantially outperforms graph network representations. \n\nScene graph prediction will likely become an increasingly important topic in computer vision, and this submission presents some string baselines for the problem. Having such baselines is extremely important: so, even though the paper does not introduce new algorithms, I would recommend that this submission is accepted.\n\nIt would be interesting to see to what extent these results generalize to larger datasets that have a long tail of relationship and node types, such as VisualGenome; I encourage the authors to perform such experiments for a future full version of this paper.\n\nThe submission should probably cite this related work: https://arxiv.org/pdf/1701.02426.pdf\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Joint Embeddings of Scene Graphs and Images", "abstract": "Multimodal representations of text and images have become popular in recent years. Text however has inherent ambiguities when describing visual scenes, leading to the recent development of datasets with detailed graphical descriptions in the form of scene graphs. We consider the task of joint representation of semantically precise scene graphs and images. We propose models for representing scene graphs and aligning them with images.  We investigate methods based on bag-of-words, subpath representations, as well as neural networks. Our investigation proposes and contrasts several models which can address this task and highlights some unique challenges in both designing models and evaluation.", "pdf": "/pdf/20108157e891d08f1d3b5c2c286e73b5870e9a74.pdf", "TL;DR": "We propose models for embedding scene graphs  in a joint space with images", "paperhash": "belilovsky|joint_embeddings_of_scene_graphs_and_images", "keywords": [], "conflicts": ["inria.fr", "centralesupelec.fr", "kuleuven.be", "cs.toronto.edu"], "authors": ["Eugene Belilovsky", "Matthew Blaschko", "Jamie Ryan Kiros", "Raquel Urtasun", "Richard Zemel"], "authorids": ["eugene.belilovsky@inria.fr", "matthew.blaschko@esat.kuleuven.be", "rkiros@cs.toronto.edu", "urtasun@cs.toronto.edu", "zemel@cs.toronto.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489173783625, "id": "ICLR.cc/2017/workshop/-/paper143/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper143/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper143/AnonReviewer2", "ICLR.cc/2017/workshop/paper143/AnonReviewer1"], "reply": {"forum": "BkyScySKl", "replyto": "BkyScySKl", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper143/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper143/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489173783625}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1487514628217, "tcdate": 1487366710871, "number": 143, "id": "BkyScySKl", "invitation": "ICLR.cc/2017/workshop/-/submission", "forum": "BkyScySKl", "signatures": ["~Eugene_Belilovsky1"], "readers": ["everyone"], "content": {"title": "Joint Embeddings of Scene Graphs and Images", "abstract": "Multimodal representations of text and images have become popular in recent years. Text however has inherent ambiguities when describing visual scenes, leading to the recent development of datasets with detailed graphical descriptions in the form of scene graphs. We consider the task of joint representation of semantically precise scene graphs and images. We propose models for representing scene graphs and aligning them with images.  We investigate methods based on bag-of-words, subpath representations, as well as neural networks. Our investigation proposes and contrasts several models which can address this task and highlights some unique challenges in both designing models and evaluation.", "pdf": "/pdf/20108157e891d08f1d3b5c2c286e73b5870e9a74.pdf", "TL;DR": "We propose models for embedding scene graphs  in a joint space with images", "paperhash": "belilovsky|joint_embeddings_of_scene_graphs_and_images", "keywords": [], "conflicts": ["inria.fr", "centralesupelec.fr", "kuleuven.be", "cs.toronto.edu"], "authors": ["Eugene Belilovsky", "Matthew Blaschko", "Jamie Ryan Kiros", "Raquel Urtasun", "Richard Zemel"], "authorids": ["eugene.belilovsky@inria.fr", "matthew.blaschko@esat.kuleuven.be", "rkiros@cs.toronto.edu", "urtasun@cs.toronto.edu", "zemel@cs.toronto.edu"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 3, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1487690420000, "tmdate": 1484242559574, "id": "ICLR.cc/2017/workshop/-/submission", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1495466420000, "cdate": 1484242559574}}}], "count": 4}