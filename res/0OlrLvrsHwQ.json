{"notes": [{"id": "0OlrLvrsHwQ", "original": "LEbVP_N6ane", "number": 1178, "cdate": 1601308132238, "ddate": null, "tcdate": 1601308132238, "tmdate": 1615208486393, "tddate": null, "forum": "0OlrLvrsHwQ", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Learning Parametrised Graph Shift Operators", "authorids": ["~George_Dasoulas1", "~Johannes_F._Lutzeyer1", "~Michalis_Vazirgiannis1"], "authors": ["George Dasoulas", "Johannes F. Lutzeyer", "Michalis Vazirgiannis"], "keywords": ["graph neural networks", "graph shift operators", "graph classification", "node classification", "graph representation learning"], "abstract": "In many domains data is currently represented as graphs and therefore, the graph representation of this data becomes increasingly important in machine learning. Network data is, implicitly or explicitly, always represented using a graph shift operator (GSO) with the most common choices being the adjacency, Laplacian matrices and their normalisations. In this paper, a novel parametrised GSO (PGSO) is proposed, where specific parameter values result in the most commonly used GSOs and message-passing operators in graph neural network (GNN) frameworks. The PGSO is suggested as a replacement of the standard GSOs that are used in state-of-the-art GNN architectures and the optimisation of the PGSO parameters is seamlessly included in the model training. It is proved that the PGSO has real eigenvalues and a set of real eigenvectors independent of the parameter values and spectral bounds on the PGSO are derived. PGSO parameters are shown to adapt to the sparsity of the graph structure in a study on stochastic blockmodel networks, where they are found to automatically replicate the GSO regularisation found in the literature. On several real-world datasets the accuracy of state-of-the-art GNN architectures is improved by the inclusion of the PGSO in both node- and graph-classification tasks. ", "one-sentence_summary": "We propose a parametrised graph shift operator (PGSO) to encode graph structure, providing a unified view of the most common GSOs, and improve GNN performance by incorporating the PGSO into the model training in an end-to-end manner.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "dasoulas|learning_parametrised_graph_shift_operators", "pdf": "/pdf/e6905a755d143d191885c3b5fef08eb1af58ebb1.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ndasoulas2021learning,\ntitle={Learning Parametrised Graph Shift Operators},\nauthor={George Dasoulas and Johannes F. Lutzeyer and Michalis Vazirgiannis},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=0OlrLvrsHwQ}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "rHtRn3plr0y", "original": null, "number": 1, "cdate": 1610040448357, "ddate": null, "tcdate": 1610040448357, "tmdate": 1610474050232, "tddate": null, "forum": "0OlrLvrsHwQ", "replyto": "0OlrLvrsHwQ", "invitation": "ICLR.cc/2021/Conference/Paper1178/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Poster)", "comment": "Summary:\nThe authors observe that a range of Laplacian-type operators used in\ngraph neural networks can be embedded in a parametric family, so that\nthe precise form of the Laplacian used can be determined by the\nlearning process. Empirical evaluation and some (limited) theoretical\nanalysis are provided. \n\nDiscussion:\nThe authors have provided detailed replies and also additional\nexperiments. That has addressed major concerns, and most reviewers now\nagree the paper is good. One reviewer is more skeptical, mostly\nregarding presentation. I agree with some of the points raised in this\nregard, but see them as less of an issue - I would consider the\npresentation improvable, but acceptable.\n\nOne weakness I should mention is that the two theorems provided are\nfrankly trivial. I appreciate this is 'only' a conference\nsubmission,\nbut I would nonetheless call the fact that symmetric matrices have real\neigenvalues (theorem 1) an observation, not a result. \nThat similarly holds for any direct consequence\nof Gershgorin's theorem (theorem 2). The entire page used to state this could perhaps be put to\nbetter use for additional empirical results.\n\n\nRecommendation:\nThe program committee (the AC and program chairs) were hesitating about this paper but decided to recommend acceptance. The idea is neat and simple, presentation and empirical evaluation are fine, if improvable (we strongly recommend the authors to invest time). What is phrased as theory is trivial, but also admittedly not the main focus of the paper. \n\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Parametrised Graph Shift Operators", "authorids": ["~George_Dasoulas1", "~Johannes_F._Lutzeyer1", "~Michalis_Vazirgiannis1"], "authors": ["George Dasoulas", "Johannes F. Lutzeyer", "Michalis Vazirgiannis"], "keywords": ["graph neural networks", "graph shift operators", "graph classification", "node classification", "graph representation learning"], "abstract": "In many domains data is currently represented as graphs and therefore, the graph representation of this data becomes increasingly important in machine learning. Network data is, implicitly or explicitly, always represented using a graph shift operator (GSO) with the most common choices being the adjacency, Laplacian matrices and their normalisations. In this paper, a novel parametrised GSO (PGSO) is proposed, where specific parameter values result in the most commonly used GSOs and message-passing operators in graph neural network (GNN) frameworks. The PGSO is suggested as a replacement of the standard GSOs that are used in state-of-the-art GNN architectures and the optimisation of the PGSO parameters is seamlessly included in the model training. It is proved that the PGSO has real eigenvalues and a set of real eigenvectors independent of the parameter values and spectral bounds on the PGSO are derived. PGSO parameters are shown to adapt to the sparsity of the graph structure in a study on stochastic blockmodel networks, where they are found to automatically replicate the GSO regularisation found in the literature. On several real-world datasets the accuracy of state-of-the-art GNN architectures is improved by the inclusion of the PGSO in both node- and graph-classification tasks. ", "one-sentence_summary": "We propose a parametrised graph shift operator (PGSO) to encode graph structure, providing a unified view of the most common GSOs, and improve GNN performance by incorporating the PGSO into the model training in an end-to-end manner.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "dasoulas|learning_parametrised_graph_shift_operators", "pdf": "/pdf/e6905a755d143d191885c3b5fef08eb1af58ebb1.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ndasoulas2021learning,\ntitle={Learning Parametrised Graph Shift Operators},\nauthor={George Dasoulas and Johannes F. Lutzeyer and Michalis Vazirgiannis},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=0OlrLvrsHwQ}\n}"}, "tags": [], "invitation": {"reply": {"forum": "0OlrLvrsHwQ", "replyto": "0OlrLvrsHwQ", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040448343, "tmdate": 1610474050216, "id": "ICLR.cc/2021/Conference/Paper1178/-/Decision"}}}, {"id": "oHPDDJ75-sN", "original": null, "number": 4, "cdate": 1603900470199, "ddate": null, "tcdate": 1603900470199, "tmdate": 1606926576779, "tddate": null, "forum": "0OlrLvrsHwQ", "replyto": "0OlrLvrsHwQ", "invitation": "ICLR.cc/2021/Conference/Paper1178/-/Official_Review", "content": {"title": "A simple but useful idea", "review": "##########################################################################\nSummary:\n\nThe paper proposes a parametric form for a matrix representation of a graph to be used as a building block within graph neural networks (GNNs). In essence, people use different normalized versions of the adjacency and Laplacian matrices within GNNs. The authors, in turn, propose a generic parametrized version that encompasses those normalizations and that can be learned from data.\n\n##########################################################################\nReasons for score:\u00a0\n\u00a0\nMy overall evaluation is slightly positive. Although the methodological contribution is minor, my positive recommendation is based on the wide applicability of the simple idea proposed. The parametric graph shift operator can be readily used in existing applications of GNNs possibly boosting performance.\n\u00a0\n##########################################################################\n\nPros:\u00a0\n\n1. Simple contribution explained in a straightforward manner. This can help adoption by practitioners.\n\n2. Numerical experiments rightly illustrate the value of the contribution.\n\u00a0\n\n##########################################################################\n\nCons:\u00a0\n\n1. There is no clear justification for the specific parametric form in (1), other than it can recover existing choices of the GSO. This, of course, can be obtained with other conceivable parametrizations. The authors try to justify this by saying that (1) is the \"most general affine form of the adjacency matrix\", but this does not seem to be a rigorous statement. Most general in what sense? What is the space of all affine forms of matrices?\n\n2. The value of the theoretical analysis is unclear. The paragraph after Theorem 1 is supposed to highlight the value of having real eigenvalues. The authors mention \"without having to worry about complex values\", worrying in what sense? Why would it be difficult to implement the examples at the end of page 4 if the eigenvalues would be complex? The authors then mention the difficulty of doing spectral clustering with complex eigenvalues, but the connection with the ongoing discussion is quite loose.\n\n3. Similarly, the value of Theorem 2 is unclear to me. Why are these bounds useful? The discussion after Theorem 2 again tries to enlighten in that direction, but it falls short.\n\u00a0\u00a0\n\n#########################################################################\n\nSome typos:\u00a0\n(1) In Remark 1, \"absence of an edge\" should be \"existence of an edge\".\n\n#########################################################################\n\nEdit after author response: We thank the authors for their response to my concerns and those of other reviewers. I have updated my score from 6 to 7 based on their changes.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1178/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1178/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Parametrised Graph Shift Operators", "authorids": ["~George_Dasoulas1", "~Johannes_F._Lutzeyer1", "~Michalis_Vazirgiannis1"], "authors": ["George Dasoulas", "Johannes F. Lutzeyer", "Michalis Vazirgiannis"], "keywords": ["graph neural networks", "graph shift operators", "graph classification", "node classification", "graph representation learning"], "abstract": "In many domains data is currently represented as graphs and therefore, the graph representation of this data becomes increasingly important in machine learning. Network data is, implicitly or explicitly, always represented using a graph shift operator (GSO) with the most common choices being the adjacency, Laplacian matrices and their normalisations. In this paper, a novel parametrised GSO (PGSO) is proposed, where specific parameter values result in the most commonly used GSOs and message-passing operators in graph neural network (GNN) frameworks. The PGSO is suggested as a replacement of the standard GSOs that are used in state-of-the-art GNN architectures and the optimisation of the PGSO parameters is seamlessly included in the model training. It is proved that the PGSO has real eigenvalues and a set of real eigenvectors independent of the parameter values and spectral bounds on the PGSO are derived. PGSO parameters are shown to adapt to the sparsity of the graph structure in a study on stochastic blockmodel networks, where they are found to automatically replicate the GSO regularisation found in the literature. On several real-world datasets the accuracy of state-of-the-art GNN architectures is improved by the inclusion of the PGSO in both node- and graph-classification tasks. ", "one-sentence_summary": "We propose a parametrised graph shift operator (PGSO) to encode graph structure, providing a unified view of the most common GSOs, and improve GNN performance by incorporating the PGSO into the model training in an end-to-end manner.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "dasoulas|learning_parametrised_graph_shift_operators", "pdf": "/pdf/e6905a755d143d191885c3b5fef08eb1af58ebb1.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ndasoulas2021learning,\ntitle={Learning Parametrised Graph Shift Operators},\nauthor={George Dasoulas and Johannes F. Lutzeyer and Michalis Vazirgiannis},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=0OlrLvrsHwQ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "0OlrLvrsHwQ", "replyto": "0OlrLvrsHwQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1178/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538124898, "tmdate": 1606915786146, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1178/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1178/-/Official_Review"}}}, {"id": "OZfy56V1qN7", "original": null, "number": 3, "cdate": 1603878405143, "ddate": null, "tcdate": 1603878405143, "tmdate": 1606767106672, "tddate": null, "forum": "0OlrLvrsHwQ", "replyto": "0OlrLvrsHwQ", "invitation": "ICLR.cc/2021/Conference/Paper1178/-/Official_Review", "content": {"title": "An interesting idea with some promising empirical results which should be made more comprehensive", "review": "The authors consider the problem of learning a parametrized graph shift operator (or message passing operator) in the context of graph neural networks. They consider a family of GSO (that they name PGSO) based on seven scalar parameters, and show that it includes most commonly used operators such as the adjacency matrix or the laplacian. The spectral properties of the PGSO are analyzed. Finally, some empirical results are provided demonstrating the PGSO as a drop-in replacement for standard GSO in GNN architectures.\n\nGraph neural networks have gathered a large amount of interest in the past years, and found widespread use in adapted problems. However, recent analyses have shown that typical architectures may have limited expressiveness (e.g. bounded by the WL test). The paper is overall well-written, and proposes a method which could be of interest to practitioners. On the other hand, the paper has some minor weaknesses, in that the proposed parametrization is redundant with some existing techniques (e.g. $m_3$ corresponds to residual connections, which are widely used), and, given the simplicity of the proposed method, a more comprehensive empirical evaluation could be beneficial. Additionally, the proposed operator is still restricted to 1-neighborhoods, and hence cannot by itself solve the expressiveness problems encountered by graph neural networks. This lack of expressiveness, in conjunction with the mixed empirical results (which show overall good performance improvement but with overlapping error bars), potentially lower the impact of this contribution.\n\nGiven the potential interest this method could hold for practitioners, I believe that the paper could be substantially improved by making the empirical evaluation more comprehensive. In particular, a more comprehensive analysis of the training dynamics (as in figure 1) as well as the effect of initialization would give a clearer picture of the effect of the PGSO in practice. Specifically, one might wonder about the importance of initialization for the $m_2$ parameter, given that it controls the influence of the edge information, and for example whether it is possible for it to change sign during training. Perhaps an informative experiment could be to initialize the PGSO at different commonly used GSO parameters (e.g. adjacency, Laplacian, normalized Laplacian), and observe how such initialization affects performance.\n\n==============================\n\nEdit after author response: we thank the authors for their response and providing some more empirical information. Overall I feel that this paper presents a neat idea that could be of interest to some people in the community, and I have modified my score from 6 to 7. It would be great for the authors to discuss the importance of initialization, as in particular, it seems to me that the sign of $m_2$ can never change  (from its initial value), indicating perhaps that practitioners should try initialization at either and select the better performing model.\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1178/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1178/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Parametrised Graph Shift Operators", "authorids": ["~George_Dasoulas1", "~Johannes_F._Lutzeyer1", "~Michalis_Vazirgiannis1"], "authors": ["George Dasoulas", "Johannes F. Lutzeyer", "Michalis Vazirgiannis"], "keywords": ["graph neural networks", "graph shift operators", "graph classification", "node classification", "graph representation learning"], "abstract": "In many domains data is currently represented as graphs and therefore, the graph representation of this data becomes increasingly important in machine learning. Network data is, implicitly or explicitly, always represented using a graph shift operator (GSO) with the most common choices being the adjacency, Laplacian matrices and their normalisations. In this paper, a novel parametrised GSO (PGSO) is proposed, where specific parameter values result in the most commonly used GSOs and message-passing operators in graph neural network (GNN) frameworks. The PGSO is suggested as a replacement of the standard GSOs that are used in state-of-the-art GNN architectures and the optimisation of the PGSO parameters is seamlessly included in the model training. It is proved that the PGSO has real eigenvalues and a set of real eigenvectors independent of the parameter values and spectral bounds on the PGSO are derived. PGSO parameters are shown to adapt to the sparsity of the graph structure in a study on stochastic blockmodel networks, where they are found to automatically replicate the GSO regularisation found in the literature. On several real-world datasets the accuracy of state-of-the-art GNN architectures is improved by the inclusion of the PGSO in both node- and graph-classification tasks. ", "one-sentence_summary": "We propose a parametrised graph shift operator (PGSO) to encode graph structure, providing a unified view of the most common GSOs, and improve GNN performance by incorporating the PGSO into the model training in an end-to-end manner.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "dasoulas|learning_parametrised_graph_shift_operators", "pdf": "/pdf/e6905a755d143d191885c3b5fef08eb1af58ebb1.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ndasoulas2021learning,\ntitle={Learning Parametrised Graph Shift Operators},\nauthor={George Dasoulas and Johannes F. Lutzeyer and Michalis Vazirgiannis},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=0OlrLvrsHwQ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "0OlrLvrsHwQ", "replyto": "0OlrLvrsHwQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1178/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538124898, "tmdate": 1606915786146, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1178/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1178/-/Official_Review"}}}, {"id": "55dBb5beyA", "original": null, "number": 2, "cdate": 1603840379412, "ddate": null, "tcdate": 1603840379412, "tmdate": 1606235939501, "tddate": null, "forum": "0OlrLvrsHwQ", "replyto": "0OlrLvrsHwQ", "invitation": "ICLR.cc/2021/Conference/Paper1178/-/Official_Review", "content": {"title": "Not ready for publication", "review": "This paper describes a parameterized family of \"graph shift operators\", defined for any graph on n vertices as an n x n matrix where the i,j-th entry is 0 whenever edge (i,j) does not appear in the graph. The paper studies some spectral properties of the parameterized family, and experiments with using them as components of graph neural networks.\n\nThis paper appears to me to be a hammer in search of a nail. It was not clear what problem is meant to be solved here. Nor does it seem that the topic is fundamental, scientific investigation. \n\nThe paper is generally poorly written and was difficult to follow. I suspect much of that relates to the lack of clarity in the problem. However, the writing is also unfocused at the paragraph level.\n\nIn short: this paper needs substantial work before it's ready for publication. ", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1178/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1178/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Parametrised Graph Shift Operators", "authorids": ["~George_Dasoulas1", "~Johannes_F._Lutzeyer1", "~Michalis_Vazirgiannis1"], "authors": ["George Dasoulas", "Johannes F. Lutzeyer", "Michalis Vazirgiannis"], "keywords": ["graph neural networks", "graph shift operators", "graph classification", "node classification", "graph representation learning"], "abstract": "In many domains data is currently represented as graphs and therefore, the graph representation of this data becomes increasingly important in machine learning. Network data is, implicitly or explicitly, always represented using a graph shift operator (GSO) with the most common choices being the adjacency, Laplacian matrices and their normalisations. In this paper, a novel parametrised GSO (PGSO) is proposed, where specific parameter values result in the most commonly used GSOs and message-passing operators in graph neural network (GNN) frameworks. The PGSO is suggested as a replacement of the standard GSOs that are used in state-of-the-art GNN architectures and the optimisation of the PGSO parameters is seamlessly included in the model training. It is proved that the PGSO has real eigenvalues and a set of real eigenvectors independent of the parameter values and spectral bounds on the PGSO are derived. PGSO parameters are shown to adapt to the sparsity of the graph structure in a study on stochastic blockmodel networks, where they are found to automatically replicate the GSO regularisation found in the literature. On several real-world datasets the accuracy of state-of-the-art GNN architectures is improved by the inclusion of the PGSO in both node- and graph-classification tasks. ", "one-sentence_summary": "We propose a parametrised graph shift operator (PGSO) to encode graph structure, providing a unified view of the most common GSOs, and improve GNN performance by incorporating the PGSO into the model training in an end-to-end manner.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "dasoulas|learning_parametrised_graph_shift_operators", "pdf": "/pdf/e6905a755d143d191885c3b5fef08eb1af58ebb1.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ndasoulas2021learning,\ntitle={Learning Parametrised Graph Shift Operators},\nauthor={George Dasoulas and Johannes F. Lutzeyer and Michalis Vazirgiannis},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=0OlrLvrsHwQ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "0OlrLvrsHwQ", "replyto": "0OlrLvrsHwQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1178/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538124898, "tmdate": 1606915786146, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1178/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1178/-/Official_Review"}}}, {"id": "UJlZ52Hq4H_", "original": null, "number": 11, "cdate": 1606235914090, "ddate": null, "tcdate": 1606235914090, "tmdate": 1606235914090, "tddate": null, "forum": "0OlrLvrsHwQ", "replyto": "QTvjE0FPZ_u", "invitation": "ICLR.cc/2021/Conference/Paper1178/-/Official_Comment", "content": {"title": "Clarifying remarks", "comment": ">Research problem: The problem that we address in this work is that most of the standard graph learning models are utilising a fixed, non-learnable graph shift operator (GSO). As you correctly defined, the GSO is a matrix which encodes neighbourhood topology in graph-related tasks. We are answering the question whether there is a single GSO which optimally represents graph structures in all tasks and datasets and whether such an optimal GSO can be learned in a computationally efficient and numerically stable way. Our motivation behind this work is the observation that in different graph learning algorithms, different GSOs have been chosen without a clear motivation for this choice, e.g  [1], [2],  [3]. Our objective is to introduce a parametrised GSO that can change during the learning procedure providing a flexible data-dependent representation of the neighbourhood topology, which allows for more expressive graph learning models.\n\n\nWhat you're describing here is a solution, not a problem. Prior work chooses GSOs arbitrarily: fine. What issues do we expect that to cause? What is unsatisfactory about the existing choices?\n\nI understand the story as this: \n1. there is an arbitrary choice in most graph neural net approaches\n2. we can instead learn the arbitrary choice\n3. that might do good things\n\nAnd the unclear point is: what exactly are the good things we expect? If it's mainly a speculative endeavour, so the purpose of the experiments is to find out what will happen, this should be clarified and the experiments should be designed to really isolate the effect of distinct GSOs (as opposed to, e.g., the effects of extra parameters or extensive hyperparameter search). For instance, a good start would be training multiple models that differ by fixed GSO structures and showing this induces some kind of predictable variation in their behavior. \n\nMy read of the experiments is: learning the GSO sometimes has benefits for classification tasks. Though, it's not clear to me whether those benefits are due to a superior choice of shift operator, or simply because there are a some extra degrees of freedom in the model. The experiments showing that the learned operator is sensitivity to sparsity in the underlying seem promising, though it's not clear to me how this relates to 'optimality' of the representation.\n\n \nI don't have time to give line-by-line comments on the writing---though honestly I found it less off putting on the second read. However,  a general useful principle would be to restructure the paper so that you have clear theses and the results you present are in service of each thesis. E.g., the theory section could be written as: It's useful for the GSO to have real-valued eigenvalues that lie in a known range. This is for [some clearly explained reason]. We prove the result holds.\n\n\nI don't feel very strongly about this paper, so I'm not eager to be the main reason for accept or reject.  I continue to think it could benefit from extensive reworking, but I'll raise my vote to a weak accept to reflect that I'm merely lukewarm about acceptance rather than clearly opposed."}, "signatures": ["ICLR.cc/2021/Conference/Paper1178/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1178/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Parametrised Graph Shift Operators", "authorids": ["~George_Dasoulas1", "~Johannes_F._Lutzeyer1", "~Michalis_Vazirgiannis1"], "authors": ["George Dasoulas", "Johannes F. Lutzeyer", "Michalis Vazirgiannis"], "keywords": ["graph neural networks", "graph shift operators", "graph classification", "node classification", "graph representation learning"], "abstract": "In many domains data is currently represented as graphs and therefore, the graph representation of this data becomes increasingly important in machine learning. Network data is, implicitly or explicitly, always represented using a graph shift operator (GSO) with the most common choices being the adjacency, Laplacian matrices and their normalisations. In this paper, a novel parametrised GSO (PGSO) is proposed, where specific parameter values result in the most commonly used GSOs and message-passing operators in graph neural network (GNN) frameworks. The PGSO is suggested as a replacement of the standard GSOs that are used in state-of-the-art GNN architectures and the optimisation of the PGSO parameters is seamlessly included in the model training. It is proved that the PGSO has real eigenvalues and a set of real eigenvectors independent of the parameter values and spectral bounds on the PGSO are derived. PGSO parameters are shown to adapt to the sparsity of the graph structure in a study on stochastic blockmodel networks, where they are found to automatically replicate the GSO regularisation found in the literature. On several real-world datasets the accuracy of state-of-the-art GNN architectures is improved by the inclusion of the PGSO in both node- and graph-classification tasks. ", "one-sentence_summary": "We propose a parametrised graph shift operator (PGSO) to encode graph structure, providing a unified view of the most common GSOs, and improve GNN performance by incorporating the PGSO into the model training in an end-to-end manner.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "dasoulas|learning_parametrised_graph_shift_operators", "pdf": "/pdf/e6905a755d143d191885c3b5fef08eb1af58ebb1.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ndasoulas2021learning,\ntitle={Learning Parametrised Graph Shift Operators},\nauthor={George Dasoulas and Johannes F. Lutzeyer and Michalis Vazirgiannis},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=0OlrLvrsHwQ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "0OlrLvrsHwQ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1178/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1178/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1178/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1178/Authors|ICLR.cc/2021/Conference/Paper1178/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1178/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923862756, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1178/-/Official_Comment"}}}, {"id": "n8Di5ZF-FMB", "original": null, "number": 8, "cdate": 1606059269081, "ddate": null, "tcdate": 1606059269081, "tmdate": 1606059269081, "tddate": null, "forum": "0OlrLvrsHwQ", "replyto": "hXcOKMmHx02", "invitation": "ICLR.cc/2021/Conference/Paper1178/-/Official_Comment", "content": {"title": "Response to Reviewer 3: Two new experiment settings", "comment": "We are very thankful for your review. Your comments have enabled us to make substantial additions to the experimental section of the paper. Specifically, we included two new experimental evaluations and a discussion of the PGSO parameter learning behaviour as well as the train/validation accuracy evolution throughout training.  You may find these additions in the revised manuscript we have uploaded in Sections 5.2 and 5.3 and Appendix B.4 and B.5. We would now like to more directly address your comments in the order that you presented them in.\n\n1. As you correctly noticed, the term \u2018absence\u2019 in Remark 1 was a typo and has been replaced with the term \u2018existence\u2019 in the revised paper.\n\n\n2. Our initial thought was that Sections 4.2 and 5.1 could provide information regarding the PGSO parameter behaviour on a real world dataset ( Section 4.2 and Figure 1b) and in a synthetic scenario of varying sparsity ( Section 5.1 and Figure 2 ). However, we agree with you that we should provide a more explicit analysis of the parameter evolution through epochs. For this reason, we set up another experiment on the Cora dataset using a GCN-PGSO model for 5 different initialisations (GCN, Adjacency Matrix, Symmetric Normalised Laplacian, Random-Walk Normalised Laplacian and all-zeros initial configurations) and monitored the PGSO parameters evolution over 150 epochs. For all scenarios, the $m_1,m_3,e_1,e_2,e_3$ parameters increase monotonically until they converge. Moreover, parameters $m_2$ and $a$ initially increase for the GCN, Adjacency and all-zeros initialisations, while they initially decrease for Random-Walk normalised Laplacian and Symmetric normalised Laplacian. We observe that the achieved accuracy is not very sensitive to the different initialisations. The aforementioned experimentation setup and results are included in the newly added Section 5.2 and in Appendix B.4 in the revised paper.\n\n\n3. We thank you very much for your suggestion of the further experimentation on the accuracy evolution. Following your comments, we monitored the train/validation accuracy and the loss convergence throughout training in 2 experiments: \n    * The node classification task on Cora, where we compared the achieved accuracy and loss minimisation of the standard GCN model and the GCN-PGSO model throughout training. In both cases, we set the same hyper-parameter configurations for a fair comparison.\n    * The graph classification task on PTC-MR, where we compared again the accuracy and loss of the standard GIN model and GIN-PGSO model. Again, we used the same hyper-parameter configurations for a fair comparison.\nWe observed a faster convergence of the train and validation accuracy and the cross-entropy loss to better values when using our proposed PGSO in both tasks. We included these results in Section 5.3 and the newly added Appendix B.5 in the revised paper.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1178/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1178/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Parametrised Graph Shift Operators", "authorids": ["~George_Dasoulas1", "~Johannes_F._Lutzeyer1", "~Michalis_Vazirgiannis1"], "authors": ["George Dasoulas", "Johannes F. Lutzeyer", "Michalis Vazirgiannis"], "keywords": ["graph neural networks", "graph shift operators", "graph classification", "node classification", "graph representation learning"], "abstract": "In many domains data is currently represented as graphs and therefore, the graph representation of this data becomes increasingly important in machine learning. Network data is, implicitly or explicitly, always represented using a graph shift operator (GSO) with the most common choices being the adjacency, Laplacian matrices and their normalisations. In this paper, a novel parametrised GSO (PGSO) is proposed, where specific parameter values result in the most commonly used GSOs and message-passing operators in graph neural network (GNN) frameworks. The PGSO is suggested as a replacement of the standard GSOs that are used in state-of-the-art GNN architectures and the optimisation of the PGSO parameters is seamlessly included in the model training. It is proved that the PGSO has real eigenvalues and a set of real eigenvectors independent of the parameter values and spectral bounds on the PGSO are derived. PGSO parameters are shown to adapt to the sparsity of the graph structure in a study on stochastic blockmodel networks, where they are found to automatically replicate the GSO regularisation found in the literature. On several real-world datasets the accuracy of state-of-the-art GNN architectures is improved by the inclusion of the PGSO in both node- and graph-classification tasks. ", "one-sentence_summary": "We propose a parametrised graph shift operator (PGSO) to encode graph structure, providing a unified view of the most common GSOs, and improve GNN performance by incorporating the PGSO into the model training in an end-to-end manner.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "dasoulas|learning_parametrised_graph_shift_operators", "pdf": "/pdf/e6905a755d143d191885c3b5fef08eb1af58ebb1.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ndasoulas2021learning,\ntitle={Learning Parametrised Graph Shift Operators},\nauthor={George Dasoulas and Johannes F. Lutzeyer and Michalis Vazirgiannis},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=0OlrLvrsHwQ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "0OlrLvrsHwQ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1178/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1178/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1178/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1178/Authors|ICLR.cc/2021/Conference/Paper1178/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1178/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923862756, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1178/-/Official_Comment"}}}, {"id": "E80OzR1mxyk", "original": null, "number": 7, "cdate": 1605908056840, "ddate": null, "tcdate": 1605908056840, "tmdate": 1605908056840, "tddate": null, "forum": "0OlrLvrsHwQ", "replyto": "OZfy56V1qN7", "invitation": "ICLR.cc/2021/Conference/Paper1178/-/Official_Comment", "content": {"title": "Response to Reviewer 1: A new series of experiments", "comment": " We are very grateful for your evaluation and your constructive comments. These have allowed us to significantly improve the paper by adding a new series of experiments. These experiments provide a more comprehensive analysis of the PGSO parameter evolution throughout training and of the sensitivity to initialisation. Please find these changes in the revised manuscript we have uploaded. We now proceed by giving more detailed answers to the weaknesses you identified.\n\n> *\u201cthe paper has some minor weaknesses, in that the proposed parametrization is redundant with some existing techniques (e.g. $m_3$  corresponds to residual connections, which are widely used)\u201d*\n\n\n\nWe agree that PGSO can likely be related to elements of several different techniques; thank you very much for pointing this out. Our aim is to provide a unified view of message passing operations under the GSO formulation and as a result the strengths of several different techniques can be automatically included ad hoc by the model itself. \nAlthough initially it appears that $m_3$ corresponds to the residual connections, after a detailed comparison we found that there are in fact significant differences. \nThe term $m_3 I_n $ of $\\gamma(A,S)$ in a standard GNN model is going to be fed into a non-linear function (such as a MLP), providing, then, the node representation. Contrariwise, to our knowledge, residual connections are used to ensure that a part of the input avoids non-linear transformation, that could cause gradient explosion phenomena. So, in a setting with residual blocks, the skip connections transfer the input after the output of a GNN block, avoiding non-linear functions. In our scenario the $m_3 I_n$ term transfers the input after the aggregation of the neighborhood information, that is usually fed into a non-linear function (e.g in the Graph Isomorphism Network, the aggregated input is fed into a MLP).\nThe inclusion of the trainable parameter $m_3$ in the optimisation allows for an \u201cadaptable\u201d incorporation of the $ m_3 I_n $ term in the model. Hence, $ m_3 I_n $ is changing through the training epochs, in contrast to residual blocks that, to our knowledge, are fixed throughout training.\n\n> *\u201c the proposed operator is still restricted to 1-neighborhoods, and hence cannot by itself solve the expressiveness problems encountered by graph neural networks.\u201c*\n\nThe proposed GNN-PGSO model (Section 3.3) utilises the **same** parametrised operator $\\gamma(A,S)$ in all GNN layers. Hence, the PGSO parameters are **shared** through all layers and depend on information from larger neighborhoods. So while the application of the operator only propagates node features in 1-neighborhoods, the GNN-PGSO learns representations from larger neighborhoods, due to the PGSO parameter sharing between the layers. Therefore, the expressiveness of GNNs is improved by the incorporation of the PGSO. Furthermore, as we note in Section 3.2, the PGSO has the further potential to address the expressiveness problem of GNNs if, instead of the adjacency matrix, it is defined on the basis of the diffusion operator $S$ [1], that has a larger receptive field, i.e has information from larger neighborhoods.\n\n> *\u201cthe paper could be substantially improved by making the empirical evaluation more comprehensive. In particular, a more comprehensive analysis of the training dynamics (as in figure 1) as well as the effect of initialization would give a clearer picture of the effect of the PGSO in practice.\u201d*\n\nThank you very much for your useful suggestions on the experimental evaluation. Following your comments, we performed a series of experiments using the GCN-PGSO model in the node classification task (Cora dataset) for 5 different PGSO parameter initialisations (GCN, Adjacency Matrix,  Symmetric Normalised Laplacian, Random-Walk Normalised Laplacian initialisations and a degenerate all-zeros initialisation). For all initialisations, $m_1,m_3,e_1,e_2,e_3$ parameters monotonically increased until they converged. Moreover, $m_2$ and $a$ parameters initially increased until they converged for GCN, Adjacency matrix and all-zeros configurations, while for the normalised Laplacians, they initially decreased. Independent of the initialisations the parameters varied smoothly throughout training. Finally, it is important to note that the achieved accuracy was not very sensitive to the different initialisations and, more specifically, the accuracy from the two normalised Laplacian initialisations was slightly lower than the one achieved from the remaining three initialisations. We included these experimental results in the paper in the newly added Section 5.2 and in Appendix B4.\n\n\n[1] Johannes Klicpera, Stefan Wei\u00dfenberger, and Stephan G\u00fcnnemann. Diffusion improves graph learning. In Advances in Neural Information Processing Systems, 2019.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1178/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1178/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Parametrised Graph Shift Operators", "authorids": ["~George_Dasoulas1", "~Johannes_F._Lutzeyer1", "~Michalis_Vazirgiannis1"], "authors": ["George Dasoulas", "Johannes F. Lutzeyer", "Michalis Vazirgiannis"], "keywords": ["graph neural networks", "graph shift operators", "graph classification", "node classification", "graph representation learning"], "abstract": "In many domains data is currently represented as graphs and therefore, the graph representation of this data becomes increasingly important in machine learning. Network data is, implicitly or explicitly, always represented using a graph shift operator (GSO) with the most common choices being the adjacency, Laplacian matrices and their normalisations. In this paper, a novel parametrised GSO (PGSO) is proposed, where specific parameter values result in the most commonly used GSOs and message-passing operators in graph neural network (GNN) frameworks. The PGSO is suggested as a replacement of the standard GSOs that are used in state-of-the-art GNN architectures and the optimisation of the PGSO parameters is seamlessly included in the model training. It is proved that the PGSO has real eigenvalues and a set of real eigenvectors independent of the parameter values and spectral bounds on the PGSO are derived. PGSO parameters are shown to adapt to the sparsity of the graph structure in a study on stochastic blockmodel networks, where they are found to automatically replicate the GSO regularisation found in the literature. On several real-world datasets the accuracy of state-of-the-art GNN architectures is improved by the inclusion of the PGSO in both node- and graph-classification tasks. ", "one-sentence_summary": "We propose a parametrised graph shift operator (PGSO) to encode graph structure, providing a unified view of the most common GSOs, and improve GNN performance by incorporating the PGSO into the model training in an end-to-end manner.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "dasoulas|learning_parametrised_graph_shift_operators", "pdf": "/pdf/e6905a755d143d191885c3b5fef08eb1af58ebb1.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ndasoulas2021learning,\ntitle={Learning Parametrised Graph Shift Operators},\nauthor={George Dasoulas and Johannes F. Lutzeyer and Michalis Vazirgiannis},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=0OlrLvrsHwQ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "0OlrLvrsHwQ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1178/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1178/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1178/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1178/Authors|ICLR.cc/2021/Conference/Paper1178/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1178/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923862756, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1178/-/Official_Comment"}}}, {"id": "8zZpGnsXbTw", "original": null, "number": 6, "cdate": 1605907306456, "ddate": null, "tcdate": 1605907306456, "tmdate": 1605907306456, "tddate": null, "forum": "0OlrLvrsHwQ", "replyto": "oHPDDJ75-sN", "invitation": "ICLR.cc/2021/Conference/Paper1178/-/Official_Comment", "content": {"title": "Response to Reviewer 2: Adjustments of Sections 3 and 4 ", "comment": "We would like to thank you very much for your careful review. Your comments have allowed us to improve the justification of the theoretical aspects of our work. We have made changes in Sections 3 and 4 to address the shortcomings which you rightfully identified and also corrected the typo you pointed out and we have uploaded a revised version of the paper. Below, we give brief responses to your main concerns:\n\n1. As you correctly identified our motivation of the PGSO is not rigorous and tries to capture the fact that this definition includes the majority of commonly used GSOs as specific parameter choices. In the revised manuscript we have replaced the non-rigorous claim by a more specific statement of our motivation to clarify this ambiguity.\nOf course one could more formally parametrise and explore the space of affine matrix transforms of the adjacency matrix ($f(A) = m_1 A + m_0 I$) and extend this transformation by left and/or right multiplying the degree matrix raised to trainable power ($D^{e_i}$).  However, in the submitted manuscript our aim is instead to span the space of commonly used GSOs and to learn where in this space the optimal GSO lies.\n\n\n2. We have extended the discussion of the result in Theorem 1 to address your rightfully identified shortcomings. While trying to not overstate the importance of Theorem 1, we specify that it is of both practical and theoretical significance. In practice Theorem 1 helps practitioners identify numerical errors in spectral calculations (which are not part of training a GNN). The theoretical significance of Theorem 1 stems from the fact that there exists a lot of powerful theoretical tools which apply to symmetric matrices with a real spectrum. In the revised paper, we discuss Cauchy\u2019s interlacing theorem as an example of such a result. Through the proof of Theorem 1 it becomes apparent that these theoretical tools apply to our PGSO independent of the parameter choice and thereby Theorem 1 lays a strong foundation for further theoretical analysis of methodology, such as GNNs, making use of the PGSO. In direct response to your questions, GNNs can be implemented without issues using GSOs with complex spectra. However, theoretically there are fewer results which apply to such a GSO and hence the theoretical understanding of such models is less advanced. We agree that the link to spectral clustering here is tenuous and hence, we have removed it from this part of the discussion. \n\n\n3. We agree that the discussion was insufficient. In Section 4.2 we demonstrate the value of Theorem 2 in an empirical study. We have now included a statement following Theorem 2, which links these sections and thereby clarifies the value of the result proven in Theorem 2. Furthermore, we have added a theoretical discussion of the value of Theorem 2. In essence, the bounds proven in Theorem 2 allow the observation of the spectral bounds for the many GSOs which can be obtained via specific parameter choices made in our PGSO formulation. In the context of GNNs such bounds are of value since they allow statements about numerical stability and vanishing/exploding gradients to be made. For example in the paper by Kipf and Welling 2017 [1] the observation that the spectrum of the symmetric normalised Laplacian is contained in the interval $[0,2]$ motivated them to make use of the \u201crenormalisation trick\u201d to stabilise their computations. The fact that the PGSO is learned means that its spectral support varies throughout training and the bounds proven in Theorem 2 enable us to monitor bounds on the spectral support in a numerically efficient manner, avoiding the computation of eigenvalues at each iteration. \n\n\n\n[1] Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional net-works. In 5th International Conference on Learning Representations (ICLR), 2017.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1178/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1178/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Parametrised Graph Shift Operators", "authorids": ["~George_Dasoulas1", "~Johannes_F._Lutzeyer1", "~Michalis_Vazirgiannis1"], "authors": ["George Dasoulas", "Johannes F. Lutzeyer", "Michalis Vazirgiannis"], "keywords": ["graph neural networks", "graph shift operators", "graph classification", "node classification", "graph representation learning"], "abstract": "In many domains data is currently represented as graphs and therefore, the graph representation of this data becomes increasingly important in machine learning. Network data is, implicitly or explicitly, always represented using a graph shift operator (GSO) with the most common choices being the adjacency, Laplacian matrices and their normalisations. In this paper, a novel parametrised GSO (PGSO) is proposed, where specific parameter values result in the most commonly used GSOs and message-passing operators in graph neural network (GNN) frameworks. The PGSO is suggested as a replacement of the standard GSOs that are used in state-of-the-art GNN architectures and the optimisation of the PGSO parameters is seamlessly included in the model training. It is proved that the PGSO has real eigenvalues and a set of real eigenvectors independent of the parameter values and spectral bounds on the PGSO are derived. PGSO parameters are shown to adapt to the sparsity of the graph structure in a study on stochastic blockmodel networks, where they are found to automatically replicate the GSO regularisation found in the literature. On several real-world datasets the accuracy of state-of-the-art GNN architectures is improved by the inclusion of the PGSO in both node- and graph-classification tasks. ", "one-sentence_summary": "We propose a parametrised graph shift operator (PGSO) to encode graph structure, providing a unified view of the most common GSOs, and improve GNN performance by incorporating the PGSO into the model training in an end-to-end manner.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "dasoulas|learning_parametrised_graph_shift_operators", "pdf": "/pdf/e6905a755d143d191885c3b5fef08eb1af58ebb1.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ndasoulas2021learning,\ntitle={Learning Parametrised Graph Shift Operators},\nauthor={George Dasoulas and Johannes F. Lutzeyer and Michalis Vazirgiannis},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=0OlrLvrsHwQ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "0OlrLvrsHwQ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1178/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1178/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1178/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1178/Authors|ICLR.cc/2021/Conference/Paper1178/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1178/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923862756, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1178/-/Official_Comment"}}}, {"id": "QTvjE0FPZ_u", "original": null, "number": 5, "cdate": 1605211060031, "ddate": null, "tcdate": 1605211060031, "tmdate": 1605211060031, "tddate": null, "forum": "0OlrLvrsHwQ", "replyto": "55dBb5beyA", "invitation": "ICLR.cc/2021/Conference/Paper1178/-/Official_Comment", "content": {"title": "Clarifications of the raised concerns", "comment": "Dear reviewer,\nThank you very much for your evaluation. We regret that you had to find that the paper is currently not ready for publication.  We are very much interested in improving the quality and presentation of our work. Therefore, we would like to ask you to please give us more specific feedback, which allows us to improve aspects of our work. First, please allow us to further elaborate on  the issues that you raised: the clarity of the research problem and your issues following the text structure.\n \n**Research problem:**\nThe problem that we address in this work is that most of the standard graph learning models are utilising a fixed, non-learnable graph shift operator (GSO). As you correctly defined, the GSO is a matrix which encodes neighbourhood topology in graph-related tasks. We are answering the question whether there is a single GSO which optimally represents graph structures in all tasks and datasets and whether such an optimal GSO can be learned in a computationally efficient and numerically stable way. Our motivation behind this work is the observation that in different graph learning algorithms, different GSOs have been chosen without a clear motivation for this choice, e.g $D^{-1}A$ [1], $D_1^{-1/2}A_1D_1^{-1/2}$ [2], $D^{-1} L$ [3].  Our objective is to introduce a parametrised GSO that can change during the learning procedure providing a flexible data-dependent representation of the neighbourhood topology, which allows for more expressive graph learning models.\n\n**Text structure:**\nTo clarify the focus of our sections and enable you to follow the content of our paper better, we provide a brief summary of the section content.\n- In Section 1 ( Introduction ) we state our two research questions. These are to firstly, find out whether there is a single graph shift operator which optimally represents graph structures in all tasks and datasets. Secondly, we aim to find out whether such an optimal representation can be learned in a computationally efficient and numerically stable way.\n- In Section 2 ( Related Work ) we firstly present examples of applications of GSOs in different ML scenarios and previous work that raised similar questions regarding the impact of choice of GSO. Secondly, we discuss examples of state-of-the-art GNNs that are using specific GSOs. Lastly, we provide a more detailed account of the work by Klicpera and highlight common themes and differences between our work and theirs. \n- In Section 3 ( Method ) we present our contribution. In Sections 3.1, 3.2, we define the parametrised GSO (PGSO) and in Section 3.3 we introduce the methods GNN-PGSO and GNN-mPGSO, where we show how we can replace the originally used GSO in GNN models with our PGSO.\n- In Section 4 ( Spectral Analysis ) we study spectral properties of our PGSO. The main objective of this Section is to show that the application of the PGSO results in numerically stable computations (as a result of the computation of the spectral bounds) and that is therefore a reliable alternative to standard and fixed GSOs (e.g a Laplacian operator). \n- In Section 5 ( Experiments ) we show empirical results of our contribution. In Section 5.1 our goal is to show how the PGSO parameters adapt to the sparsity level of a graph. In Section 5.2, we show the performance boost of standard GNN models that is achieved using the GNN-PGSO and GNN-mPGSO methods in node classification and graph classification scenarios.\n\nWe hope that this summary can help address your concerns. We would be very grateful if you could provide further clarification to your review in the following areas:\n\n1. *\u201cIt was not clear what problem is meant to be solved here. Nor does it seem that the topic is fundamental, scientific investigation.\u201d*\nWe hope that our further explanation has clarified our research problem and scientific foundation of this investigation. It would be useful if you could point out to us which aspect of the problem was lacking.\n2. *\u201cThe paper is generally poorly written and was difficult to follow. I suspect much of that relates to the lack of clarity in the problem. However, the writing is also unfocused at the paragraph level.\u201d*\nIt would be incredibly helpful for us if you could provide specific sections of the text or aspects of the presented idea, which you thought were poorly worded or explained, so that we are able to improve the quality of this paper. We agree that the writing in the appendix can be improved and we plan to provide an improved version in our upcoming set of corrections.\n\n[1] Keyulu Xu, Weihua Hu, Jure Leskovec and Stefanie Jegelka. How powerful are graph neural networks?  In 7th International Conference on Learning Representations (ICLR), 2019.\n[2] Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional net-works. In 5th International Conference on Learning Representations (ICLR), 2017.\n[3] Ulrike von Luxburg. A tutorial on spectral clustering. Statistics and Computing, pp. 395 \u2013 416, 2007.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1178/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1178/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Parametrised Graph Shift Operators", "authorids": ["~George_Dasoulas1", "~Johannes_F._Lutzeyer1", "~Michalis_Vazirgiannis1"], "authors": ["George Dasoulas", "Johannes F. Lutzeyer", "Michalis Vazirgiannis"], "keywords": ["graph neural networks", "graph shift operators", "graph classification", "node classification", "graph representation learning"], "abstract": "In many domains data is currently represented as graphs and therefore, the graph representation of this data becomes increasingly important in machine learning. Network data is, implicitly or explicitly, always represented using a graph shift operator (GSO) with the most common choices being the adjacency, Laplacian matrices and their normalisations. In this paper, a novel parametrised GSO (PGSO) is proposed, where specific parameter values result in the most commonly used GSOs and message-passing operators in graph neural network (GNN) frameworks. The PGSO is suggested as a replacement of the standard GSOs that are used in state-of-the-art GNN architectures and the optimisation of the PGSO parameters is seamlessly included in the model training. It is proved that the PGSO has real eigenvalues and a set of real eigenvectors independent of the parameter values and spectral bounds on the PGSO are derived. PGSO parameters are shown to adapt to the sparsity of the graph structure in a study on stochastic blockmodel networks, where they are found to automatically replicate the GSO regularisation found in the literature. On several real-world datasets the accuracy of state-of-the-art GNN architectures is improved by the inclusion of the PGSO in both node- and graph-classification tasks. ", "one-sentence_summary": "We propose a parametrised graph shift operator (PGSO) to encode graph structure, providing a unified view of the most common GSOs, and improve GNN performance by incorporating the PGSO into the model training in an end-to-end manner.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "dasoulas|learning_parametrised_graph_shift_operators", "pdf": "/pdf/e6905a755d143d191885c3b5fef08eb1af58ebb1.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ndasoulas2021learning,\ntitle={Learning Parametrised Graph Shift Operators},\nauthor={George Dasoulas and Johannes F. Lutzeyer and Michalis Vazirgiannis},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=0OlrLvrsHwQ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "0OlrLvrsHwQ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1178/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1178/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1178/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1178/Authors|ICLR.cc/2021/Conference/Paper1178/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1178/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923862756, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1178/-/Official_Comment"}}}, {"id": "hXcOKMmHx02", "original": null, "number": 1, "cdate": 1603834227551, "ddate": null, "tcdate": 1603834227551, "tmdate": 1605024510478, "tddate": null, "forum": "0OlrLvrsHwQ", "replyto": "0OlrLvrsHwQ", "invitation": "ICLR.cc/2021/Conference/Paper1178/-/Official_Review", "content": {"title": "Parametrized GSOs in GNNs", "review": "The paper proposes a parameterized graph shift operator (PGSO) as a replacement of fixed, hand-picked GSOs for application to GNN architectures and the experiments illustrate that PGSOs automatically adapt to the regularized GSOs for different settings. The contribution is of interest to the ML community as existing studies show significant improvements in performance of GNNs by using certain GSOs in specific contexts and therefore, a data driven characterization of GSO helps introduce adaptivity to the network representation in GNNs.\n\nPros: The paper is well written and easy to follow. The theoretical and empirical spectral analysis of PGSO reveals its applicability to many existing GNN architectures. The experiments are sufficient to illustrate the utility of using PGSO over a constant GSO for different settings. \n\nCons: \n1. Remark 1 seems to contradict Definition 1. Do the authors mean 'presence' and not 'absence' of an edge in Remark 1?\n\n2. An explicit discussion on how PGSO parameters are learned or updated in every epoch is missing. \n\n3. The authors compare the final accuracy results of GNNS with PGSOs and those with constants GSOs. The evolution of accuracy with the number of epochs for some of these results will help evaluate the cost of learning the parameters of PGSOs. ", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1178/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1178/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Parametrised Graph Shift Operators", "authorids": ["~George_Dasoulas1", "~Johannes_F._Lutzeyer1", "~Michalis_Vazirgiannis1"], "authors": ["George Dasoulas", "Johannes F. Lutzeyer", "Michalis Vazirgiannis"], "keywords": ["graph neural networks", "graph shift operators", "graph classification", "node classification", "graph representation learning"], "abstract": "In many domains data is currently represented as graphs and therefore, the graph representation of this data becomes increasingly important in machine learning. Network data is, implicitly or explicitly, always represented using a graph shift operator (GSO) with the most common choices being the adjacency, Laplacian matrices and their normalisations. In this paper, a novel parametrised GSO (PGSO) is proposed, where specific parameter values result in the most commonly used GSOs and message-passing operators in graph neural network (GNN) frameworks. The PGSO is suggested as a replacement of the standard GSOs that are used in state-of-the-art GNN architectures and the optimisation of the PGSO parameters is seamlessly included in the model training. It is proved that the PGSO has real eigenvalues and a set of real eigenvectors independent of the parameter values and spectral bounds on the PGSO are derived. PGSO parameters are shown to adapt to the sparsity of the graph structure in a study on stochastic blockmodel networks, where they are found to automatically replicate the GSO regularisation found in the literature. On several real-world datasets the accuracy of state-of-the-art GNN architectures is improved by the inclusion of the PGSO in both node- and graph-classification tasks. ", "one-sentence_summary": "We propose a parametrised graph shift operator (PGSO) to encode graph structure, providing a unified view of the most common GSOs, and improve GNN performance by incorporating the PGSO into the model training in an end-to-end manner.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "dasoulas|learning_parametrised_graph_shift_operators", "pdf": "/pdf/e6905a755d143d191885c3b5fef08eb1af58ebb1.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ndasoulas2021learning,\ntitle={Learning Parametrised Graph Shift Operators},\nauthor={George Dasoulas and Johannes F. Lutzeyer and Michalis Vazirgiannis},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=0OlrLvrsHwQ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "0OlrLvrsHwQ", "replyto": "0OlrLvrsHwQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1178/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538124898, "tmdate": 1606915786146, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1178/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1178/-/Official_Review"}}}], "count": 11}