{"notes": [{"id": "H1xSNiRcF7", "original": "HkltNFR5Ym", "number": 1, "cdate": 1538087725357, "ddate": null, "tcdate": 1538087725357, "tmdate": 1550884085382, "tddate": null, "forum": "H1xSNiRcF7", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Smoothing the Geometry of Probabilistic Box Embeddings", "abstract": "There is growing interest in geometrically-inspired embeddings for learning hierarchies, partial orders, and lattice structures, with natural applications to transitive relational data such as entailment graphs. Recent work has extended these ideas beyond deterministic hierarchies to probabilistically calibrated models, which enable learning from uncertain supervision and inferring soft-inclusions among concepts, while maintaining the geometric inductive bias of hierarchical embedding models. We build on the Box Lattice model of Vilnis et al. (2018), which showed promising results in modeling soft-inclusions through an overlapping hierarchy of sets, parameterized as high-dimensional hyperrectangles (boxes). However, the hard edges of the boxes present difficulties for standard gradient based optimization; that work employed a special surrogate function for the disjoint case, but we find this method to be fragile.  In this work, we present a novel hierarchical embedding model, inspired by a relaxation of box embeddings into parameterized density functions using Gaussian convolutions over the boxes. Our approach provides an alternative surrogate to the original lattice measure that improves the robustness of optimization in the disjoint case, while also preserving the desirable properties with respect to the original lattice. We demonstrate increased or matching performance on WordNet hypernymy prediction, Flickr caption entailment, and a MovieLens-based market basket dataset. We show especially marked improvements in the case of sparse data, where many conditional probabilities should be low, and thus boxes should be nearly disjoint.", "keywords": ["embeddings", "order embeddings", "knowledge graph embedding", "relational learning"], "authorids": ["xiangl@cs.umass.edu", "luke@cs.umass.edu", "dongxuzhang@cs.umass.edu", "mboratko@math.umass.edu", "mccallum@cs.umass.edu"], "authors": ["Xiang Li", "Luke Vilnis", "Dongxu Zhang", "Michael Boratko", "Andrew McCallum"], "TL;DR": "Improve hierarchical embedding models using kernel smoothing", "pdf": "/pdf/30f1421b94869cfbadcc02bf5c259dcd2d51501e.pdf", "paperhash": "li|smoothing_the_geometry_of_probabilistic_box_embeddings", "_bibtex": "@inproceedings{\nli2018smoothing,\ntitle={Smoothing the Geometry of Probabilistic Box Embeddings},\nauthor={Xiang Li and Luke Vilnis and Dongxu Zhang and Michael Boratko and Andrew McCallum},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=H1xSNiRcF7},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "Bygv35mggN", "original": null, "number": 1, "cdate": 1544727215426, "ddate": null, "tcdate": 1544727215426, "tmdate": 1545354511123, "tddate": null, "forum": "H1xSNiRcF7", "replyto": "H1xSNiRcF7", "invitation": "ICLR.cc/2019/Conference/-/Paper1/Meta_Review", "content": {"metareview": "The manuscript presents a promising new algorithm for learning geometrically-inspired embeddings for learning hierarchies, partial orders, and lattice structures. The manuscript builds on the build on the box lattice model, extending prior work by relaxing the box embeddings via Gaussian convolutions. This is shown to be particularly effective for non-overlapping boxes, where the previous method fail.\n\nThe primary weakness identified by reviewers was the writing, which was thought to be lacking some context, and may be difficult to approach for the non-domain expert. This can be improved by including an additional general introduction. Otherwise, the manuscript was well written.\n\nOverall, reviewers and AC agree that the general problem statement is timely and interesting, and well executed. In our opinion, this paper is a clear accept.", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Accept (Oral)", "title": "Metareview"}, "signatures": ["ICLR.cc/2019/Conference/Paper1/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper1/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Smoothing the Geometry of Probabilistic Box Embeddings", "abstract": "There is growing interest in geometrically-inspired embeddings for learning hierarchies, partial orders, and lattice structures, with natural applications to transitive relational data such as entailment graphs. Recent work has extended these ideas beyond deterministic hierarchies to probabilistically calibrated models, which enable learning from uncertain supervision and inferring soft-inclusions among concepts, while maintaining the geometric inductive bias of hierarchical embedding models. We build on the Box Lattice model of Vilnis et al. (2018), which showed promising results in modeling soft-inclusions through an overlapping hierarchy of sets, parameterized as high-dimensional hyperrectangles (boxes). However, the hard edges of the boxes present difficulties for standard gradient based optimization; that work employed a special surrogate function for the disjoint case, but we find this method to be fragile.  In this work, we present a novel hierarchical embedding model, inspired by a relaxation of box embeddings into parameterized density functions using Gaussian convolutions over the boxes. Our approach provides an alternative surrogate to the original lattice measure that improves the robustness of optimization in the disjoint case, while also preserving the desirable properties with respect to the original lattice. We demonstrate increased or matching performance on WordNet hypernymy prediction, Flickr caption entailment, and a MovieLens-based market basket dataset. We show especially marked improvements in the case of sparse data, where many conditional probabilities should be low, and thus boxes should be nearly disjoint.", "keywords": ["embeddings", "order embeddings", "knowledge graph embedding", "relational learning"], "authorids": ["xiangl@cs.umass.edu", "luke@cs.umass.edu", "dongxuzhang@cs.umass.edu", "mboratko@math.umass.edu", "mccallum@cs.umass.edu"], "authors": ["Xiang Li", "Luke Vilnis", "Dongxu Zhang", "Michael Boratko", "Andrew McCallum"], "TL;DR": "Improve hierarchical embedding models using kernel smoothing", "pdf": "/pdf/30f1421b94869cfbadcc02bf5c259dcd2d51501e.pdf", "paperhash": "li|smoothing_the_geometry_of_probabilistic_box_embeddings", "_bibtex": "@inproceedings{\nli2018smoothing,\ntitle={Smoothing the Geometry of Probabilistic Box Embeddings},\nauthor={Xiang Li and Luke Vilnis and Dongxu Zhang and Michael Boratko and Andrew McCallum},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=H1xSNiRcF7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353373789, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1xSNiRcF7", "replyto": "H1xSNiRcF7", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper1/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353373789}}}, {"id": "H1xPEJOVsm", "original": null, "number": 1, "cdate": 1539764015088, "ddate": null, "tcdate": 1539764015088, "tmdate": 1543562954695, "tddate": null, "forum": "H1xSNiRcF7", "replyto": "H1xSNiRcF7", "invitation": "ICLR.cc/2019/Conference/-/Paper1/Official_Review", "content": {"title": "review", "review": "Post-rebuttal revision: All my concerns were adressed by the authors. This is a great paper and should be accepted.\n\n------\n\nThe paper presents smoothing probabilistic box embeddings with softplus functions, which make the optimization landscape continuous, while also presenting the theoretical background of the proposed method well. The paper presents the overall idea beautifully and is very easy to follow. The overall idea of smoothed sotfplus boxes is well-founded, elegant and practical. The results on standard WordNet do not improve upon state-of-the-art, however imbalanced WordNet with abundance of negative examples gain remarkable improvements. Similarly in Flickr and MovieLens the method performs well. This paper presents a novel, theoretically well-justified idea with excellent results, and is likely going to be a high-impact paper. \n\nAn illustrating figure would still be nice to include, also for the convolutions of eq 2. The paper does not comment on running times, some kind of scalability comparison should be included since the paper claims that the model is easier to train.\n\nThe paper should clarify that the \\prod in 3.3. meet and join definitions seems to refer to a set product, while the p(a) equation has a standard product (or does it?). What is the \u201ca\u201d in the p(a), should it be \"p(x)\u201d ? \n\nI have trouble understanding eq 1: the difference inside the function is always negative, while the hinge function seems to clip negative values away. The definition of the m(x) is too clever, please clarify the function in more conventional notation.  ", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Smoothing the Geometry of Probabilistic Box Embeddings", "abstract": "There is growing interest in geometrically-inspired embeddings for learning hierarchies, partial orders, and lattice structures, with natural applications to transitive relational data such as entailment graphs. Recent work has extended these ideas beyond deterministic hierarchies to probabilistically calibrated models, which enable learning from uncertain supervision and inferring soft-inclusions among concepts, while maintaining the geometric inductive bias of hierarchical embedding models. We build on the Box Lattice model of Vilnis et al. (2018), which showed promising results in modeling soft-inclusions through an overlapping hierarchy of sets, parameterized as high-dimensional hyperrectangles (boxes). However, the hard edges of the boxes present difficulties for standard gradient based optimization; that work employed a special surrogate function for the disjoint case, but we find this method to be fragile.  In this work, we present a novel hierarchical embedding model, inspired by a relaxation of box embeddings into parameterized density functions using Gaussian convolutions over the boxes. Our approach provides an alternative surrogate to the original lattice measure that improves the robustness of optimization in the disjoint case, while also preserving the desirable properties with respect to the original lattice. We demonstrate increased or matching performance on WordNet hypernymy prediction, Flickr caption entailment, and a MovieLens-based market basket dataset. We show especially marked improvements in the case of sparse data, where many conditional probabilities should be low, and thus boxes should be nearly disjoint.", "keywords": ["embeddings", "order embeddings", "knowledge graph embedding", "relational learning"], "authorids": ["xiangl@cs.umass.edu", "luke@cs.umass.edu", "dongxuzhang@cs.umass.edu", "mboratko@math.umass.edu", "mccallum@cs.umass.edu"], "authors": ["Xiang Li", "Luke Vilnis", "Dongxu Zhang", "Michael Boratko", "Andrew McCallum"], "TL;DR": "Improve hierarchical embedding models using kernel smoothing", "pdf": "/pdf/30f1421b94869cfbadcc02bf5c259dcd2d51501e.pdf", "paperhash": "li|smoothing_the_geometry_of_probabilistic_box_embeddings", "_bibtex": "@inproceedings{\nli2018smoothing,\ntitle={Smoothing the Geometry of Probabilistic Box Embeddings},\nauthor={Xiang Li and Luke Vilnis and Dongxu Zhang and Michael Boratko and Andrew McCallum},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=H1xSNiRcF7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1/Official_Review", "cdate": 1542234560585, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "H1xSNiRcF7", "replyto": "H1xSNiRcF7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335626702, "tmdate": 1552335626702, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "Bye2jf25Rm", "original": null, "number": 6, "cdate": 1543320227835, "ddate": null, "tcdate": 1543320227835, "tmdate": 1543320227835, "tddate": null, "forum": "H1xSNiRcF7", "replyto": "H1xPEJOVsm", "invitation": "ICLR.cc/2019/Conference/-/Paper1/Official_Comment", "content": {"title": "Response", "comment": "Thank you for your thoughtful review. Responses are included inline:\n\n> An illustrating figure would still be nice to include, also for the convolutions of eq 2. \n\nWe agree that such a rendering will be helpful, and will add it to the paper.\n\n> The paper does not comment on running times, some kind of scalability comparison should be included since the paper claims that the model is easier to train.\n\nThe ease of training leads to better results on certain data, rather than increased scalability --- both methods are applicable to large scale data, similar to other embedding methods. We added a new series of experiments testing robustness to different initialization regimes for the two models, which are included in the draft and detailed in our response to Reviewer #2.\n\n> The paper should clarify that the \\prod in 3.3. meet and join definitions seems to refer to a set product, while the p(a) equation has a standard product (or does it?). What is the \u201ca\u201d in the p(a), should it be \"p(x)\u201d ? \n\nYour interpretation of the products is correct, and \"a\" was indeed a typo for \"x.\" Thanks! We have fixed this in the draft and changed the definition to clarify the meaning of the products.\n\n> I have trouble understanding eq 1: the difference inside the function is always negative, while the hinge function seems to clip negative values away. \n> The definition of the m(x) is too clever, please clarify the function in more conventional notation. \n\nThank you, there was a sign error. In the updated formula, the quantity inside the function can be positive or negative (negative if the hard boundaries of the boxes don't overlap at all). We've also switched the definition to use \u201cmin\u201d and \u201cmax\u201d rather than \\wedge and \\vee symbols, so it should be much clearer.\n\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Smoothing the Geometry of Probabilistic Box Embeddings", "abstract": "There is growing interest in geometrically-inspired embeddings for learning hierarchies, partial orders, and lattice structures, with natural applications to transitive relational data such as entailment graphs. Recent work has extended these ideas beyond deterministic hierarchies to probabilistically calibrated models, which enable learning from uncertain supervision and inferring soft-inclusions among concepts, while maintaining the geometric inductive bias of hierarchical embedding models. We build on the Box Lattice model of Vilnis et al. (2018), which showed promising results in modeling soft-inclusions through an overlapping hierarchy of sets, parameterized as high-dimensional hyperrectangles (boxes). However, the hard edges of the boxes present difficulties for standard gradient based optimization; that work employed a special surrogate function for the disjoint case, but we find this method to be fragile.  In this work, we present a novel hierarchical embedding model, inspired by a relaxation of box embeddings into parameterized density functions using Gaussian convolutions over the boxes. Our approach provides an alternative surrogate to the original lattice measure that improves the robustness of optimization in the disjoint case, while also preserving the desirable properties with respect to the original lattice. We demonstrate increased or matching performance on WordNet hypernymy prediction, Flickr caption entailment, and a MovieLens-based market basket dataset. We show especially marked improvements in the case of sparse data, where many conditional probabilities should be low, and thus boxes should be nearly disjoint.", "keywords": ["embeddings", "order embeddings", "knowledge graph embedding", "relational learning"], "authorids": ["xiangl@cs.umass.edu", "luke@cs.umass.edu", "dongxuzhang@cs.umass.edu", "mboratko@math.umass.edu", "mccallum@cs.umass.edu"], "authors": ["Xiang Li", "Luke Vilnis", "Dongxu Zhang", "Michael Boratko", "Andrew McCallum"], "TL;DR": "Improve hierarchical embedding models using kernel smoothing", "pdf": "/pdf/30f1421b94869cfbadcc02bf5c259dcd2d51501e.pdf", "paperhash": "li|smoothing_the_geometry_of_probabilistic_box_embeddings", "_bibtex": "@inproceedings{\nli2018smoothing,\ntitle={Smoothing the Geometry of Probabilistic Box Embeddings},\nauthor={Xiang Li and Luke Vilnis and Dongxu Zhang and Michael Boratko and Andrew McCallum},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=H1xSNiRcF7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621604481, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1xSNiRcF7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1/Authors", "ICLR.cc/2019/Conference/Paper1/Reviewers", "ICLR.cc/2019/Conference/Paper1/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1/Authors|ICLR.cc/2019/Conference/Paper1/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1/Reviewers", "ICLR.cc/2019/Conference/Paper1/Authors", "ICLR.cc/2019/Conference/Paper1/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621604481}}}, {"id": "rJeo2-39Rm", "original": null, "number": 5, "cdate": 1543319987034, "ddate": null, "tcdate": 1543319987034, "tmdate": 1543319987034, "tddate": null, "forum": "H1xSNiRcF7", "replyto": "SylyqW25Cm", "invitation": "ICLR.cc/2019/Conference/-/Paper1/Official_Comment", "content": {"title": "Response Continued", "comment": "\n> There's a strong emphasis on how smoothing makes training easier. Do you have any metrics to directly support this, such as variance under random restarts?\n\nWe do not see much variance in terms of outcome when changing only the random seed. In terms of ease/robustness of training, our experiments on imbalanced wordnet give evidence that the soft box model is more robust in the regime of sparse *training* data. However, we have updated the draft with a new series of experiments on MovieLens. In the appendix, we\u2019ve added experiments that demonstrate the greatly decreased sensitivity of the soft box model when picking distributions for box initialization such that the boxes start off with roughly 0%, 20%, 50%, and 100% of boxes disjoint --- regimes in which the hard box model experiences much greater degradation in performance. Although we can control our initialization, we can't necessarily control the intermediate stages of learning, during which boxes may become disjoint, so this may give some useful insight.\n\nNOTE: When performing this comparison, we found a difference between the criteria to establish development set convergence in the POE, box, and soft box experiments on MovieLens and the criteria used by the (complex) bilinear baseline models. These criteria (number of steps without development set improvement) are given in the appendix. This led us to update the results (in Table 5) for POE, box, and soft box, with the best performing model (our proposed soft box model) improving by an absolute point of Spearman and Pearson's rho compared to the old tuning regime. Additionally, the hard box model outperforms all other models besides the soft box model. The soft box model outperforms it in KL and Pearson by a similar absolute margin as before, but its previous advantage of ~2.9 points of Spearman's rho over the hard box model is now only ~2.1 points. This difference in development set stopping criteria was not present in any other experiments.\n\n\n> In the abstract and introduction, it's easy to gloss over \"inspired by\" and assume that the actual model is a Gaussian convolution. Could be more direct here that it's a softplus approximation.\n\nThe model is also modified to take pointwise min and max inside the softplus, in order to maintain idempotency, as described in the second half of section 5.2. We updated this section with a clearer description. Since we not only approximate the Gaussian with a logistic, but also modify the equation to preserve the necessary idempotency (by analogy to the zero-temperature limit), \"softplus approximation\" might not be sufficient to describe the entire model. We should still try to make this part of the abstract clearer in some way."}, "signatures": ["ICLR.cc/2019/Conference/Paper1/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Smoothing the Geometry of Probabilistic Box Embeddings", "abstract": "There is growing interest in geometrically-inspired embeddings for learning hierarchies, partial orders, and lattice structures, with natural applications to transitive relational data such as entailment graphs. Recent work has extended these ideas beyond deterministic hierarchies to probabilistically calibrated models, which enable learning from uncertain supervision and inferring soft-inclusions among concepts, while maintaining the geometric inductive bias of hierarchical embedding models. We build on the Box Lattice model of Vilnis et al. (2018), which showed promising results in modeling soft-inclusions through an overlapping hierarchy of sets, parameterized as high-dimensional hyperrectangles (boxes). However, the hard edges of the boxes present difficulties for standard gradient based optimization; that work employed a special surrogate function for the disjoint case, but we find this method to be fragile.  In this work, we present a novel hierarchical embedding model, inspired by a relaxation of box embeddings into parameterized density functions using Gaussian convolutions over the boxes. Our approach provides an alternative surrogate to the original lattice measure that improves the robustness of optimization in the disjoint case, while also preserving the desirable properties with respect to the original lattice. We demonstrate increased or matching performance on WordNet hypernymy prediction, Flickr caption entailment, and a MovieLens-based market basket dataset. We show especially marked improvements in the case of sparse data, where many conditional probabilities should be low, and thus boxes should be nearly disjoint.", "keywords": ["embeddings", "order embeddings", "knowledge graph embedding", "relational learning"], "authorids": ["xiangl@cs.umass.edu", "luke@cs.umass.edu", "dongxuzhang@cs.umass.edu", "mboratko@math.umass.edu", "mccallum@cs.umass.edu"], "authors": ["Xiang Li", "Luke Vilnis", "Dongxu Zhang", "Michael Boratko", "Andrew McCallum"], "TL;DR": "Improve hierarchical embedding models using kernel smoothing", "pdf": "/pdf/30f1421b94869cfbadcc02bf5c259dcd2d51501e.pdf", "paperhash": "li|smoothing_the_geometry_of_probabilistic_box_embeddings", "_bibtex": "@inproceedings{\nli2018smoothing,\ntitle={Smoothing the Geometry of Probabilistic Box Embeddings},\nauthor={Xiang Li and Luke Vilnis and Dongxu Zhang and Michael Boratko and Andrew McCallum},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=H1xSNiRcF7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621604481, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1xSNiRcF7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1/Authors", "ICLR.cc/2019/Conference/Paper1/Reviewers", "ICLR.cc/2019/Conference/Paper1/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1/Authors|ICLR.cc/2019/Conference/Paper1/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1/Reviewers", "ICLR.cc/2019/Conference/Paper1/Authors", "ICLR.cc/2019/Conference/Paper1/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621604481}}}, {"id": "SylyqW25Cm", "original": null, "number": 4, "cdate": 1543319943511, "ddate": null, "tcdate": 1543319943511, "tmdate": 1543319943511, "tddate": null, "forum": "H1xSNiRcF7", "replyto": "SylZ79N5hm", "invitation": "ICLR.cc/2019/Conference/-/Paper1/Official_Comment", "content": {"title": "Response", "comment": "Thank you for the review. We will reply in detail to each point inline:\n\n> Missing citation / comparison: https://arxiv.org/pdf/1804.01882.pdf (Ganea et al. 2018) is an alternative way of generalizing order embeddings. \n> They also report very high numbers on WordNet, though I'm not sure they are directly comparable.\n\nThis is indeed a very related paper. Our work differs from hyperbolic embeddings in a couple of ways. First, by virtue of being a probabilistic model, the box model can score complex multivariate queries including negated variables. Secondly, the box structure is more suitable for general DAG embedding, as opposed to a hyperbolic model where the constant negative curvature strongly biases the model towards trees. The numbers are not directly comparable, but we will add this to related work, thank you.\n\n> The Gaussian relaxation (Eq. (2) and (3)) defines a particular length scale, \\sigma. \n> It's not clear if this is also implicit in the softplus derivation (by analogy with Eq. (4), should we assume that it approximates the \\sigma = 1 case?). \n> What effect does this have on the embedding space? Without it, it would seem that the normal BL model is scale invariant, which might be a desirable property for representing hierarchical data.\n\nThe \\sigma parameter is absorbed into the constant \\rho in the softplus approximation to the Gaussian (Proposition 1), which differs from \\sigma by the factor 1/1.702 given there.  In practice, this is tuned as a global temperature for the softplus, but it is not particularly important when normalizing the space by the global coordinatewise minimum and maximum, as explained at the end of section 4.2 (this detail is probably the most important practical answer to your question). The scale invariance question is interesting. In order to solve the problem of sparse gradients, our solution sacrifices scale invariance. While scale invariance is desirable in theory, it has been known to cause instability in other contexts, such as perceptron vs. hinge loss learning, and perhaps the \u201cscale\u201d of the \u201csoft edges\u201d could be viewed as a type of margin, as well as solving the problem of sparse gradients.\n\n> The main thrust of section 5.2 is that smoothed box embeddings retain better performance with increasing numbers of negatives. \n> Could you include the ratio of positive / negative examples on the Flickr dataset, and some measure of the distribution of P(A|B) values on MovieLens to get a sense of how these datasets compare?\n\nSince the Flickr dataset consists of denotational entailment probabilities between (possibly unseen) pairs of sentences, none of the train or test probabilities are exactly 0 (negative examples). However, many such pairs have a conditional probability below 0.1, with a ratio of about 13:1. Movielens is similarly pseudosparse, not truly sparse, with a similarly large majority of its probabilities taking values below 0.1. We have added a histogram showing the distribution of these probabilities in the appendix.\n\n> Flickr data: what is the encoder model that produces the embeddings here, and how does it handle unseen captions? (Why would we expect the smoothed box model to handle unseen captions better?)\n\nThe encoder model is a single-layer LSTM with the same specifications as used in Lai and Hockenmaier 2017 and Vilnis et al. 2018. It handles unseen captions by composing token embeddings with the RNN. We have updated the draft to make this clear. As for why the soft box model improves on unseen captions more than the other tasks, it may simply be a question of there being more room to improve (the previous SOTA held-out KL divergence is about twice as large for unseen captions than for the other categories, for example.) It would be interesting to explore this further.\n\n(continued in next comment)"}, "signatures": ["ICLR.cc/2019/Conference/Paper1/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Smoothing the Geometry of Probabilistic Box Embeddings", "abstract": "There is growing interest in geometrically-inspired embeddings for learning hierarchies, partial orders, and lattice structures, with natural applications to transitive relational data such as entailment graphs. Recent work has extended these ideas beyond deterministic hierarchies to probabilistically calibrated models, which enable learning from uncertain supervision and inferring soft-inclusions among concepts, while maintaining the geometric inductive bias of hierarchical embedding models. We build on the Box Lattice model of Vilnis et al. (2018), which showed promising results in modeling soft-inclusions through an overlapping hierarchy of sets, parameterized as high-dimensional hyperrectangles (boxes). However, the hard edges of the boxes present difficulties for standard gradient based optimization; that work employed a special surrogate function for the disjoint case, but we find this method to be fragile.  In this work, we present a novel hierarchical embedding model, inspired by a relaxation of box embeddings into parameterized density functions using Gaussian convolutions over the boxes. Our approach provides an alternative surrogate to the original lattice measure that improves the robustness of optimization in the disjoint case, while also preserving the desirable properties with respect to the original lattice. We demonstrate increased or matching performance on WordNet hypernymy prediction, Flickr caption entailment, and a MovieLens-based market basket dataset. We show especially marked improvements in the case of sparse data, where many conditional probabilities should be low, and thus boxes should be nearly disjoint.", "keywords": ["embeddings", "order embeddings", "knowledge graph embedding", "relational learning"], "authorids": ["xiangl@cs.umass.edu", "luke@cs.umass.edu", "dongxuzhang@cs.umass.edu", "mboratko@math.umass.edu", "mccallum@cs.umass.edu"], "authors": ["Xiang Li", "Luke Vilnis", "Dongxu Zhang", "Michael Boratko", "Andrew McCallum"], "TL;DR": "Improve hierarchical embedding models using kernel smoothing", "pdf": "/pdf/30f1421b94869cfbadcc02bf5c259dcd2d51501e.pdf", "paperhash": "li|smoothing_the_geometry_of_probabilistic_box_embeddings", "_bibtex": "@inproceedings{\nli2018smoothing,\ntitle={Smoothing the Geometry of Probabilistic Box Embeddings},\nauthor={Xiang Li and Luke Vilnis and Dongxu Zhang and Michael Boratko and Andrew McCallum},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=H1xSNiRcF7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621604481, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1xSNiRcF7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1/Authors", "ICLR.cc/2019/Conference/Paper1/Reviewers", "ICLR.cc/2019/Conference/Paper1/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1/Authors|ICLR.cc/2019/Conference/Paper1/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1/Reviewers", "ICLR.cc/2019/Conference/Paper1/Authors", "ICLR.cc/2019/Conference/Paper1/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621604481}}}, {"id": "BkejJghqAQ", "original": null, "number": 3, "cdate": 1543319522996, "ddate": null, "tcdate": 1543319522996, "tmdate": 1543319522996, "tddate": null, "forum": "H1xSNiRcF7", "replyto": "rJglCnOshm", "invitation": "ICLR.cc/2019/Conference/-/Paper1/Official_Comment", "content": {"title": "Response", "comment": "Thank you for the thoughtful review.\n\n - We use Adam to perform the optimization, using the default settings given in the Adam paper for momentum / decay / ridge terms, with learning rates given in the appendix of the submission. We have also updated the appendix with more hyperparameter details, and plan to release code before publication. \n\n - The temperature / bandwidth hyperparameter is always set equal to 1.0. We address this also in our response to Reviewer #2 --- since in all experiments aside from Flickr, we divide each dimension by the global maximum across boxes, this seems to avoid scale issues. \n\n - We agree that a figure illustrating the geometric intuition would be helpful, and will add a rendering in a future draft."}, "signatures": ["ICLR.cc/2019/Conference/Paper1/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Smoothing the Geometry of Probabilistic Box Embeddings", "abstract": "There is growing interest in geometrically-inspired embeddings for learning hierarchies, partial orders, and lattice structures, with natural applications to transitive relational data such as entailment graphs. Recent work has extended these ideas beyond deterministic hierarchies to probabilistically calibrated models, which enable learning from uncertain supervision and inferring soft-inclusions among concepts, while maintaining the geometric inductive bias of hierarchical embedding models. We build on the Box Lattice model of Vilnis et al. (2018), which showed promising results in modeling soft-inclusions through an overlapping hierarchy of sets, parameterized as high-dimensional hyperrectangles (boxes). However, the hard edges of the boxes present difficulties for standard gradient based optimization; that work employed a special surrogate function for the disjoint case, but we find this method to be fragile.  In this work, we present a novel hierarchical embedding model, inspired by a relaxation of box embeddings into parameterized density functions using Gaussian convolutions over the boxes. Our approach provides an alternative surrogate to the original lattice measure that improves the robustness of optimization in the disjoint case, while also preserving the desirable properties with respect to the original lattice. We demonstrate increased or matching performance on WordNet hypernymy prediction, Flickr caption entailment, and a MovieLens-based market basket dataset. We show especially marked improvements in the case of sparse data, where many conditional probabilities should be low, and thus boxes should be nearly disjoint.", "keywords": ["embeddings", "order embeddings", "knowledge graph embedding", "relational learning"], "authorids": ["xiangl@cs.umass.edu", "luke@cs.umass.edu", "dongxuzhang@cs.umass.edu", "mboratko@math.umass.edu", "mccallum@cs.umass.edu"], "authors": ["Xiang Li", "Luke Vilnis", "Dongxu Zhang", "Michael Boratko", "Andrew McCallum"], "TL;DR": "Improve hierarchical embedding models using kernel smoothing", "pdf": "/pdf/30f1421b94869cfbadcc02bf5c259dcd2d51501e.pdf", "paperhash": "li|smoothing_the_geometry_of_probabilistic_box_embeddings", "_bibtex": "@inproceedings{\nli2018smoothing,\ntitle={Smoothing the Geometry of Probabilistic Box Embeddings},\nauthor={Xiang Li and Luke Vilnis and Dongxu Zhang and Michael Boratko and Andrew McCallum},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=H1xSNiRcF7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621604481, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1xSNiRcF7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1/Authors", "ICLR.cc/2019/Conference/Paper1/Reviewers", "ICLR.cc/2019/Conference/Paper1/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1/Authors|ICLR.cc/2019/Conference/Paper1/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1/Reviewers", "ICLR.cc/2019/Conference/Paper1/Authors", "ICLR.cc/2019/Conference/Paper1/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621604481}}}, {"id": "rJglCnOshm", "original": null, "number": 3, "cdate": 1541274824287, "ddate": null, "tcdate": 1541274824287, "tmdate": 1541534371550, "tddate": null, "forum": "H1xSNiRcF7", "replyto": "H1xSNiRcF7", "invitation": "ICLR.cc/2019/Conference/-/Paper1/Official_Review", "content": {"title": "Nice idea and good improvement on benchmarks", "review": "The paper proposes a method for learning embedding of hierarchies. Specifically, the paper builds on a a geometrically inspired embedding method using box representations. The key contribution of the paper is facilitating optimization of these models by gradient based methods, which eventually leads to improved accuracy on relevant benchmark data (on par or beyond SOTA). The observation is that when two boxes are disjoint in the model but have overlap in the ground truth, no gradient can flow to the model to correct the problem (which is happens in case of sparse-data.\n\nTo alleviate the above problem, the paper proposes smoothing the model. That is, transforming the original model constructed from indicator functions (hence difficult to optimize by gradient based method) to a smooth differentiable function by diffusing the landscape. The diffusion process corresponds to convincing the objective function with the Gaussian kernel.\n\nI find the idea of converting such combinatorial problems to differentiable, specially when gradient methods can succeed in optimizing them afterward, very fascinating. I believe this paper is taking a theoretically sound path to construct the differentiable form of the originally non-differentiable problem. As the authors find, the smoothed function leads to improved performance against SOTA on relevant benchmark data such as WordNet hypernymy, Flick caption entailment and MovieLnes market basket data.\n\nOne downside of the current submission is that the details of optimization are now provided at all. What algorithm do you use to optimize the objective function? What are the hyper parameters? What value of sigma (for diffusion) do you use [or maybe you use the continuation method to gradually anneal sigma from large toward zero?). These are important details that I ask the authors to include.\n\nAlso, I think some graphical illustration of the embedding would be very helpful, perhaps something like Figure 2 of \"Probabilistic Embedding of Knowledge Graphs with Box Lattice Measures\". I hope such illustration is added to the submission.", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Smoothing the Geometry of Probabilistic Box Embeddings", "abstract": "There is growing interest in geometrically-inspired embeddings for learning hierarchies, partial orders, and lattice structures, with natural applications to transitive relational data such as entailment graphs. Recent work has extended these ideas beyond deterministic hierarchies to probabilistically calibrated models, which enable learning from uncertain supervision and inferring soft-inclusions among concepts, while maintaining the geometric inductive bias of hierarchical embedding models. We build on the Box Lattice model of Vilnis et al. (2018), which showed promising results in modeling soft-inclusions through an overlapping hierarchy of sets, parameterized as high-dimensional hyperrectangles (boxes). However, the hard edges of the boxes present difficulties for standard gradient based optimization; that work employed a special surrogate function for the disjoint case, but we find this method to be fragile.  In this work, we present a novel hierarchical embedding model, inspired by a relaxation of box embeddings into parameterized density functions using Gaussian convolutions over the boxes. Our approach provides an alternative surrogate to the original lattice measure that improves the robustness of optimization in the disjoint case, while also preserving the desirable properties with respect to the original lattice. We demonstrate increased or matching performance on WordNet hypernymy prediction, Flickr caption entailment, and a MovieLens-based market basket dataset. We show especially marked improvements in the case of sparse data, where many conditional probabilities should be low, and thus boxes should be nearly disjoint.", "keywords": ["embeddings", "order embeddings", "knowledge graph embedding", "relational learning"], "authorids": ["xiangl@cs.umass.edu", "luke@cs.umass.edu", "dongxuzhang@cs.umass.edu", "mboratko@math.umass.edu", "mccallum@cs.umass.edu"], "authors": ["Xiang Li", "Luke Vilnis", "Dongxu Zhang", "Michael Boratko", "Andrew McCallum"], "TL;DR": "Improve hierarchical embedding models using kernel smoothing", "pdf": "/pdf/30f1421b94869cfbadcc02bf5c259dcd2d51501e.pdf", "paperhash": "li|smoothing_the_geometry_of_probabilistic_box_embeddings", "_bibtex": "@inproceedings{\nli2018smoothing,\ntitle={Smoothing the Geometry of Probabilistic Box Embeddings},\nauthor={Xiang Li and Luke Vilnis and Dongxu Zhang and Michael Boratko and Andrew McCallum},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=H1xSNiRcF7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1/Official_Review", "cdate": 1542234560585, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "H1xSNiRcF7", "replyto": "H1xSNiRcF7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335626702, "tmdate": 1552335626702, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SylZ79N5hm", "original": null, "number": 2, "cdate": 1541192216792, "ddate": null, "tcdate": 1541192216792, "tmdate": 1541534371337, "tddate": null, "forum": "H1xSNiRcF7", "replyto": "H1xSNiRcF7", "invitation": "ICLR.cc/2019/Conference/-/Paper1/Official_Review", "content": {"title": "review", "review": "This paper proposes a soft relaxation of the box lattice (BL) model of Vilnis et al. 2018 and applies it to several graph prediction tasks. Results are comparable to the BL model on existing artificially-balanced data but significantly better on more natural unbalanced data with a large number of negatives. The paper assumes some familiarity with the problem domain and existing works (there is not a lot of exposition for an unfamilar reader), but should be of strong interest to anyone working on embeddings or graph prediction.\n\nThe paper is well-written, with clear explanations of the desired properties of the model and a concise set of experiments that are easy to follow. The strongest result is that on unbalanced WordNet, while the Flickr and MovieLens results are a little less clear but do show that this technique does not cause any loss in performance.\n\nA few points of feedback:\n\n- Missing citation / comparison: https://arxiv.org/pdf/1804.01882.pdf (Ganea et al. 2018) is an alternative way of generalizing order embeddings. They also report very high numbers on WordNet, though I'm not sure they are directly comparable.\n\n- The Gaussian relaxation (Eq. (2) and (3)) defines a particular length scale, \\sigma. It's not clear if this is also implicit in the softplus derivation (by analogy with Eq. (4), should we assume that it approximates the \\sigma = 1 case?). What effect does this have on the embedding space? Without it, it would seem that the normal BL model is scale invariant, which might be a desirable property for representing hierarchical data.\n\n- The main thrust of section 5.2 is that smoothed box embeddings retain better performance with increasing numbers of negatives. Could you include the ratio of positive / negative examples on the Flickr dataset, and some measure of the distribution of P(A|B) values on MovieLens to get a sense of how these datasets compare?\n\n- Flickr data: what is the encoder model that produces the embeddings here, and how does it handle unseen captions? (Why would we expect the smoothed box model to handle unseen captions better?)\n\n- There's a strong emphasis on how smoothing makes training easier. Do you have any metrics to directly support this, such as variance under random restarts?\n\n- In the abstract and introduction, it's easy to gloss over \"inspired by\" and assume that the actual model is a Gaussian convolution. Could be more direct here that it's a softplus approximation.", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Smoothing the Geometry of Probabilistic Box Embeddings", "abstract": "There is growing interest in geometrically-inspired embeddings for learning hierarchies, partial orders, and lattice structures, with natural applications to transitive relational data such as entailment graphs. Recent work has extended these ideas beyond deterministic hierarchies to probabilistically calibrated models, which enable learning from uncertain supervision and inferring soft-inclusions among concepts, while maintaining the geometric inductive bias of hierarchical embedding models. We build on the Box Lattice model of Vilnis et al. (2018), which showed promising results in modeling soft-inclusions through an overlapping hierarchy of sets, parameterized as high-dimensional hyperrectangles (boxes). However, the hard edges of the boxes present difficulties for standard gradient based optimization; that work employed a special surrogate function for the disjoint case, but we find this method to be fragile.  In this work, we present a novel hierarchical embedding model, inspired by a relaxation of box embeddings into parameterized density functions using Gaussian convolutions over the boxes. Our approach provides an alternative surrogate to the original lattice measure that improves the robustness of optimization in the disjoint case, while also preserving the desirable properties with respect to the original lattice. We demonstrate increased or matching performance on WordNet hypernymy prediction, Flickr caption entailment, and a MovieLens-based market basket dataset. We show especially marked improvements in the case of sparse data, where many conditional probabilities should be low, and thus boxes should be nearly disjoint.", "keywords": ["embeddings", "order embeddings", "knowledge graph embedding", "relational learning"], "authorids": ["xiangl@cs.umass.edu", "luke@cs.umass.edu", "dongxuzhang@cs.umass.edu", "mboratko@math.umass.edu", "mccallum@cs.umass.edu"], "authors": ["Xiang Li", "Luke Vilnis", "Dongxu Zhang", "Michael Boratko", "Andrew McCallum"], "TL;DR": "Improve hierarchical embedding models using kernel smoothing", "pdf": "/pdf/30f1421b94869cfbadcc02bf5c259dcd2d51501e.pdf", "paperhash": "li|smoothing_the_geometry_of_probabilistic_box_embeddings", "_bibtex": "@inproceedings{\nli2018smoothing,\ntitle={Smoothing the Geometry of Probabilistic Box Embeddings},\nauthor={Xiang Li and Luke Vilnis and Dongxu Zhang and Michael Boratko and Andrew McCallum},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=H1xSNiRcF7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1/Official_Review", "cdate": 1542234560585, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "H1xSNiRcF7", "replyto": "H1xSNiRcF7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335626702, "tmdate": 1552335626702, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "HylK0EWqhX", "original": null, "number": 1, "cdate": 1541178577469, "ddate": null, "tcdate": 1541178577469, "tmdate": 1541178577469, "tddate": null, "forum": "H1xSNiRcF7", "replyto": "r1lircRdnQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1/Official_Comment", "content": {"title": "Reply to Missing Reference", "comment": "Hi, thanks for the comment! We actually do cite the Hierarchical Density Order Embedding paper from Athiwarakun et al. in the introduction section. The original box lattice paper from Vilnis et al. also reports the density model result of 92.3 accuracy in their wordnet table. Box embeddings get a very similar score on this task, so we only include that result, since the aim of the experiment is to compare the softbox and hard box models and not to demonstrate a new state of the art. There are also some questions about whether the density embeddings use exactly the same dataset split, or just the same method of generating negative examples, which we have not been able to determine. Hope this helps! "}, "signatures": ["ICLR.cc/2019/Conference/Paper1/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Smoothing the Geometry of Probabilistic Box Embeddings", "abstract": "There is growing interest in geometrically-inspired embeddings for learning hierarchies, partial orders, and lattice structures, with natural applications to transitive relational data such as entailment graphs. Recent work has extended these ideas beyond deterministic hierarchies to probabilistically calibrated models, which enable learning from uncertain supervision and inferring soft-inclusions among concepts, while maintaining the geometric inductive bias of hierarchical embedding models. We build on the Box Lattice model of Vilnis et al. (2018), which showed promising results in modeling soft-inclusions through an overlapping hierarchy of sets, parameterized as high-dimensional hyperrectangles (boxes). However, the hard edges of the boxes present difficulties for standard gradient based optimization; that work employed a special surrogate function for the disjoint case, but we find this method to be fragile.  In this work, we present a novel hierarchical embedding model, inspired by a relaxation of box embeddings into parameterized density functions using Gaussian convolutions over the boxes. Our approach provides an alternative surrogate to the original lattice measure that improves the robustness of optimization in the disjoint case, while also preserving the desirable properties with respect to the original lattice. We demonstrate increased or matching performance on WordNet hypernymy prediction, Flickr caption entailment, and a MovieLens-based market basket dataset. We show especially marked improvements in the case of sparse data, where many conditional probabilities should be low, and thus boxes should be nearly disjoint.", "keywords": ["embeddings", "order embeddings", "knowledge graph embedding", "relational learning"], "authorids": ["xiangl@cs.umass.edu", "luke@cs.umass.edu", "dongxuzhang@cs.umass.edu", "mboratko@math.umass.edu", "mccallum@cs.umass.edu"], "authors": ["Xiang Li", "Luke Vilnis", "Dongxu Zhang", "Michael Boratko", "Andrew McCallum"], "TL;DR": "Improve hierarchical embedding models using kernel smoothing", "pdf": "/pdf/30f1421b94869cfbadcc02bf5c259dcd2d51501e.pdf", "paperhash": "li|smoothing_the_geometry_of_probabilistic_box_embeddings", "_bibtex": "@inproceedings{\nli2018smoothing,\ntitle={Smoothing the Geometry of Probabilistic Box Embeddings},\nauthor={Xiang Li and Luke Vilnis and Dongxu Zhang and Michael Boratko and Andrew McCallum},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=H1xSNiRcF7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621604481, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1xSNiRcF7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1/Authors", "ICLR.cc/2019/Conference/Paper1/Reviewers", "ICLR.cc/2019/Conference/Paper1/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1/Authors|ICLR.cc/2019/Conference/Paper1/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1/Reviewers", "ICLR.cc/2019/Conference/Paper1/Authors", "ICLR.cc/2019/Conference/Paper1/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621604481}}}, {"id": "r1lircRdnQ", "original": null, "number": 1, "cdate": 1541102146937, "ddate": null, "tcdate": 1541102146937, "tmdate": 1541102194751, "tddate": null, "forum": "H1xSNiRcF7", "replyto": "H1xSNiRcF7", "invitation": "ICLR.cc/2019/Conference/-/Paper1/Public_Comment", "content": {"comment": "This paper seems like a great idea. However, I believe the paper misses an important reference on the WordNet task. According to Hierarchical Density Order Embeddings (Athiwaratkun, 2018) https://arxiv.org/pdf/1804.09843.pdf, their score for hypernym prediction on WordNet test split is 92.3 which is a bit higher than the paper's reported scores. ", "title": "Missing Reference"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Smoothing the Geometry of Probabilistic Box Embeddings", "abstract": "There is growing interest in geometrically-inspired embeddings for learning hierarchies, partial orders, and lattice structures, with natural applications to transitive relational data such as entailment graphs. Recent work has extended these ideas beyond deterministic hierarchies to probabilistically calibrated models, which enable learning from uncertain supervision and inferring soft-inclusions among concepts, while maintaining the geometric inductive bias of hierarchical embedding models. We build on the Box Lattice model of Vilnis et al. (2018), which showed promising results in modeling soft-inclusions through an overlapping hierarchy of sets, parameterized as high-dimensional hyperrectangles (boxes). However, the hard edges of the boxes present difficulties for standard gradient based optimization; that work employed a special surrogate function for the disjoint case, but we find this method to be fragile.  In this work, we present a novel hierarchical embedding model, inspired by a relaxation of box embeddings into parameterized density functions using Gaussian convolutions over the boxes. Our approach provides an alternative surrogate to the original lattice measure that improves the robustness of optimization in the disjoint case, while also preserving the desirable properties with respect to the original lattice. We demonstrate increased or matching performance on WordNet hypernymy prediction, Flickr caption entailment, and a MovieLens-based market basket dataset. We show especially marked improvements in the case of sparse data, where many conditional probabilities should be low, and thus boxes should be nearly disjoint.", "keywords": ["embeddings", "order embeddings", "knowledge graph embedding", "relational learning"], "authorids": ["xiangl@cs.umass.edu", "luke@cs.umass.edu", "dongxuzhang@cs.umass.edu", "mboratko@math.umass.edu", "mccallum@cs.umass.edu"], "authors": ["Xiang Li", "Luke Vilnis", "Dongxu Zhang", "Michael Boratko", "Andrew McCallum"], "TL;DR": "Improve hierarchical embedding models using kernel smoothing", "pdf": "/pdf/30f1421b94869cfbadcc02bf5c259dcd2d51501e.pdf", "paperhash": "li|smoothing_the_geometry_of_probabilistic_box_embeddings", "_bibtex": "@inproceedings{\nli2018smoothing,\ntitle={Smoothing the Geometry of Probabilistic Box Embeddings},\nauthor={Xiang Li and Luke Vilnis and Dongxu Zhang and Michael Boratko and Andrew McCallum},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=H1xSNiRcF7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311942883, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "H1xSNiRcF7", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1/Authors", "ICLR.cc/2019/Conference/Paper1/Reviewers", "ICLR.cc/2019/Conference/Paper1/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1/Authors", "ICLR.cc/2019/Conference/Paper1/Reviewers", "ICLR.cc/2019/Conference/Paper1/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311942883}}}], "count": 11}