{"notes": [{"id": "QubpWYfdNry", "original": "qOjMG1re4vx", "number": 1735, "cdate": 1601308191673, "ddate": null, "tcdate": 1601308191673, "tmdate": 1615235884374, "tddate": null, "forum": "QubpWYfdNry", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Domain-Robust Visual Imitation Learning with Mutual Information Constraints", "authorids": ["~Edoardo_Cetin1", "~Oya_Celiktutan2"], "authors": ["Edoardo Cetin", "Oya Celiktutan"], "keywords": ["Imitation Learning", "Reinforcement Learning", "Observational Imitation", "Third-Person Imitation", "Mutual Information", "Domain Adaption", "Machine Learning"], "abstract": "Human beings are able to understand objectives and learn by simply observing others perform a task. Imitation learning methods aim to replicate such capabilities, however, they generally depend on access to a full set of optimal states and actions taken with the agent's actuators and from the agent's point of view. In this paper, we introduce a new algorithm - called Disentangling Generative Adversarial Imitation Learning (DisentanGAIL) - with the purpose of bypassing such constraints. Our algorithm enables autonomous agents to learn directly from high dimensional observations of an expert performing a task, by making use of adversarial learning with a latent representation inside the discriminator network. Such latent representation is regularized through mutual information constraints to incentivize learning only features that encode information about the completion levels of the task being demonstrated. This allows to obtain a shared feature space to successfully perform imitation while disregarding the differences between the expert's and the agent's domains. Empirically, our algorithm is able to efficiently imitate in a diverse range of control problems including balancing, manipulation and locomotive tasks, while being robust to various domain differences in terms of both environment appearance and agent embodiment.", "one-sentence_summary": "Imitation of visual expert demonstrations robust to appearance and embodiment mismatch, working for high dimensional control problems.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "cetin|domainrobust_visual_imitation_learning_with_mutual_information_constraints", "supplementary_material": "/attachment/a2f45d7daa76f701540a30f84a144a328ff20c3a.zip", "pdf": "/pdf/ffeade3d551ddc81dea27db706160b3bc6510cec.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ncetin2021domainrobust,\ntitle={Domain-Robust Visual Imitation Learning with Mutual Information Constraints},\nauthor={Edoardo Cetin and Oya Celiktutan},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=QubpWYfdNry}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 14, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "R4etEXlox4B", "original": null, "number": 1, "cdate": 1610040454797, "ddate": null, "tcdate": 1610040454797, "tmdate": 1610474057337, "tddate": null, "forum": "QubpWYfdNry", "replyto": "QubpWYfdNry", "invitation": "ICLR.cc/2021/Conference/Paper1735/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Poster)", "comment": "The reviewers raised a number of concerns about the novelty of the paper and comparisons. The authors were able to address the concerns regarding the comparisons in the response, and the reviewers unanimously agree that the paper should be published. I do think however that this paper is quite borderline. I agree with the reviewers that the updated experiments are convincing in terms of the provided comparisons. However, the reservations I have about the work can perhaps best be stated as follows: There is quite a bit of work in the area of imitation from observations, which makes a range of different assumptions and utilizes a variety of different domain adaptation techniques. Much of this work is in the robotics domain (which is cited in the paper), and much of it demonstrates results in fairly realistic settings, often with real humans and real robots. In comparison, the experiments in this paper are quite simplistic, using toy domains and \"demonstrations\" obtained from a computational oracle (i.e., another policy). Given the maturity of this field and the current state of the art, I am skeptical of this evaluation, and I think TPIL is a very weak baseline. That said, I would  defer to the reviewers in this case -- I do think the particular technical contributions that the paper makes are a valuable addition to the literature, though somewhat incremental. I am also sympathetic to the authors in that much of the more successful prior work in this area that does evaluate under realistic conditions makes subtly different assumptions, or utilizes different techniques for which it is difficult to provide an apples-to-apples comparison.\n\nOne thing I would request of the authors for the camera ready though is: Please tone down the claims. \"Human-like 7 DOF Striker\" is not human-like, it's a crudely simulated robotic arm that was recolored. It would of course be better to have a realistic evaluation (as many prior papers in this field indeed have), but in the absence of that, it is best not to overclaim and be upfront that the evaluation is on relatively simple simulated tasks under conditions that are not necessarily realistic (and have nothing to do with actual humans), but meant rather to evaluate in an apples-to-apples manner the particular algorithmic innovations in the method."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Domain-Robust Visual Imitation Learning with Mutual Information Constraints", "authorids": ["~Edoardo_Cetin1", "~Oya_Celiktutan2"], "authors": ["Edoardo Cetin", "Oya Celiktutan"], "keywords": ["Imitation Learning", "Reinforcement Learning", "Observational Imitation", "Third-Person Imitation", "Mutual Information", "Domain Adaption", "Machine Learning"], "abstract": "Human beings are able to understand objectives and learn by simply observing others perform a task. Imitation learning methods aim to replicate such capabilities, however, they generally depend on access to a full set of optimal states and actions taken with the agent's actuators and from the agent's point of view. In this paper, we introduce a new algorithm - called Disentangling Generative Adversarial Imitation Learning (DisentanGAIL) - with the purpose of bypassing such constraints. Our algorithm enables autonomous agents to learn directly from high dimensional observations of an expert performing a task, by making use of adversarial learning with a latent representation inside the discriminator network. Such latent representation is regularized through mutual information constraints to incentivize learning only features that encode information about the completion levels of the task being demonstrated. This allows to obtain a shared feature space to successfully perform imitation while disregarding the differences between the expert's and the agent's domains. Empirically, our algorithm is able to efficiently imitate in a diverse range of control problems including balancing, manipulation and locomotive tasks, while being robust to various domain differences in terms of both environment appearance and agent embodiment.", "one-sentence_summary": "Imitation of visual expert demonstrations robust to appearance and embodiment mismatch, working for high dimensional control problems.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "cetin|domainrobust_visual_imitation_learning_with_mutual_information_constraints", "supplementary_material": "/attachment/a2f45d7daa76f701540a30f84a144a328ff20c3a.zip", "pdf": "/pdf/ffeade3d551ddc81dea27db706160b3bc6510cec.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ncetin2021domainrobust,\ntitle={Domain-Robust Visual Imitation Learning with Mutual Information Constraints},\nauthor={Edoardo Cetin and Oya Celiktutan},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=QubpWYfdNry}\n}"}, "tags": [], "invitation": {"reply": {"forum": "QubpWYfdNry", "replyto": "QubpWYfdNry", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040454783, "tmdate": 1610474057321, "id": "ICLR.cc/2021/Conference/Paper1735/-/Decision"}}}, {"id": "K433eqZVgM", "original": null, "number": 3, "cdate": 1604034968872, "ddate": null, "tcdate": 1604034968872, "tmdate": 1606789727029, "tddate": null, "forum": "QubpWYfdNry", "replyto": "QubpWYfdNry", "invitation": "ICLR.cc/2021/Conference/Paper1735/-/Official_Review", "content": {"title": "Review #2", "review": "This paper proposes a visual imitation learning algorithm that can handle domain shifts between the expert demonstrations and the data generated by the agent. The proposed method is built upon GAIL and scales to image inputs. The authors handle the domain shift problem by learning a domain-invariant discriminator and a statistics network. The invariant discriminator takes in a concatenation of a sequence of latent representations of the observations and tries to predict without relying on the domain information. The statistics network is used for estimating the mutual information constraint between the latent representation and the domain labels, which the authors are trying to minimize in order to attain domain invariant predictions. The authors optimize the standard GAIL objectives along with the mutual information constraints as regularizations jointly. The experiments are done in both discrete and continuous control tasks in Mujoco and the proposed approach, DisentanGAIL, outperforms the prior method TPIL.\\\n\nFor pros, I think the experimental setup is reasonable and the authors conduct relatively thorough ablation studies to show the usefulness of various components such as the prior data constraint, double statistics network, and spectral normalization regularization, which is helpful for understanding the method. The paper is also well written and easy to understand.\n\nHowever, I have a few concerns about this approach, which I will list as follows.\n\n1. Regarding the novelty of this paper, I feel like the approach is not particularly novel. The method is similar to TPIL (Stadie et al. 2017) and the main difference between DisentanGAIL and TPIL is that in DisentanGAIL, the authors set the mutual information constraint to be less than 1 bit, and in TPIL, the mutual information is constrained to 0. I can definitely see why the softer regularization works better, but I wonder if the contribution of this work is substantial given such a simple tweak. Moreover, the authors also propose many techniques to make DisentanGAIL work better, such as using unsupervised data with some regularization, double statistics networks, spectral norm, and etc.. However, these seem to be more like implementation tricks rather than major contributions and I'm unsure if they make the contribution substantial enough for the standard of an ICLR paper. Moreover, adding these additional components definitely seem to make the approach much more complex and might require much more tuning to get it to work, which is another concern.\n\n2. Another point related to novelty and related works, there are several papers that also consider domain-adaptive imitation learning that seem pretty similar to this paper, such as [1, 2, 3]. [1, 2] consider learning domain-invariant features, while [3] proposes a unifying theoretical framework for domain-adaptive imitation learning. I think these methods could serve as comparisons to DisentanGAIL.\n\n3. Furthermore, since the approach seems to be a bit incremental, it would be nice to have the theoretical analysis that could justify the incremental changes and guarantee the convergence to the optimal policy (e.g. attaining similar results in [3]). Unfortunately, this is missing in the paper.\n\n4. The challenging, high-dimensional environments used in the paper are all in locomotion tasks in Mujoco. It would be nice to see more realistic environments such as robotic manipulation tasks like ROBEL and  Adroit etc., which would make the domain adaptation more appealing. \n\nOverall, based on the arguments above, I would recommend a reject for this paper.\n\n------------------------------------------------------------------------------------------------------------------\nPost-rebuttal updates:\n\nAfter reading the author response and other reviews, I agree that the difference between the paper and prior works is now much more clear and the empirical evidence shows that the new method works well, though I'm still concerned about the part where the authors add many components and make the algorithm much more complex and potentially hard to work in practice. Nevertheless, I've increased my score to a 6.\n\n[1] Okumura, Ryo, Masashi Okada, and Tadahiro Taniguchi. \"Domain-Adversarial and-Conditional State Space Model for Imitation Learning.\" arXiv preprint arXiv:2001.11628 (2020).\n[2] Lu, Yiren, and Jonathan Tompson. \"ADAIL: Adaptive Adversarial Imitation Learning.\" arXiv preprint arXiv:2008.12647 (2020).\n[3] Kim, Kuno, et al. \"Domain adaptive imitation learning.\" arXiv preprint arXiv:1910.00105 (2019).", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1735/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1735/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Domain-Robust Visual Imitation Learning with Mutual Information Constraints", "authorids": ["~Edoardo_Cetin1", "~Oya_Celiktutan2"], "authors": ["Edoardo Cetin", "Oya Celiktutan"], "keywords": ["Imitation Learning", "Reinforcement Learning", "Observational Imitation", "Third-Person Imitation", "Mutual Information", "Domain Adaption", "Machine Learning"], "abstract": "Human beings are able to understand objectives and learn by simply observing others perform a task. Imitation learning methods aim to replicate such capabilities, however, they generally depend on access to a full set of optimal states and actions taken with the agent's actuators and from the agent's point of view. In this paper, we introduce a new algorithm - called Disentangling Generative Adversarial Imitation Learning (DisentanGAIL) - with the purpose of bypassing such constraints. Our algorithm enables autonomous agents to learn directly from high dimensional observations of an expert performing a task, by making use of adversarial learning with a latent representation inside the discriminator network. Such latent representation is regularized through mutual information constraints to incentivize learning only features that encode information about the completion levels of the task being demonstrated. This allows to obtain a shared feature space to successfully perform imitation while disregarding the differences between the expert's and the agent's domains. Empirically, our algorithm is able to efficiently imitate in a diverse range of control problems including balancing, manipulation and locomotive tasks, while being robust to various domain differences in terms of both environment appearance and agent embodiment.", "one-sentence_summary": "Imitation of visual expert demonstrations robust to appearance and embodiment mismatch, working for high dimensional control problems.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "cetin|domainrobust_visual_imitation_learning_with_mutual_information_constraints", "supplementary_material": "/attachment/a2f45d7daa76f701540a30f84a144a328ff20c3a.zip", "pdf": "/pdf/ffeade3d551ddc81dea27db706160b3bc6510cec.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ncetin2021domainrobust,\ntitle={Domain-Robust Visual Imitation Learning with Mutual Information Constraints},\nauthor={Edoardo Cetin and Oya Celiktutan},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=QubpWYfdNry}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "QubpWYfdNry", "replyto": "QubpWYfdNry", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1735/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538111851, "tmdate": 1606915788038, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1735/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1735/-/Official_Review"}}}, {"id": "9lA4fOmgn9P", "original": null, "number": 4, "cdate": 1604194067862, "ddate": null, "tcdate": 1604194067862, "tmdate": 1606786677137, "tddate": null, "forum": "QubpWYfdNry", "replyto": "QubpWYfdNry", "invitation": "ICLR.cc/2021/Conference/Paper1735/-/Official_Review", "content": {"title": "Learning domain-invariant representations for observational imitation learning", "review": "This paper studies observational imitation learning, a problem setting in which the agent wishes to learn from expert observations, but the state, action, and observational spaces of the expert and agent domains can vary. They introduce DisentanGAIL to tackle this problem by introducing two mutual information constraints in the GAIL framework to learn domain-invariant representations of observations. In particular, DisentanGAIL aims to discard domain information in the presentation, while retaining relevant goal completion information. \n\nOverall, the problem setting is very practical. The performance gain is greatest in the domains where the camera angle changes, which is promising. I like the intuition provided in Appendix A and think some of it should be included in the main text. It motivates well the distinction between domain information and goal-completion, and highlights why previous methods like domain confusion loss can fail. Moreover, I would\u2019ve liked to see some analysis of the experiments to support this hypothesis. For example, a version of Appendix D1 would be valuable to include, and perhaps also another visualization demonstrating that other baselines fail to encode goal completion information. Some of the implementation details in Section 5 can be shortened/moved to the appendix instead.\n\nSome questions:\n\n- What is the difference between DisentanGAIL with DCL and TPIL? The text says that the DCL substitutes the mutual information constraints, so the only difference I see would be learning a distribution over latent representations rather than a deterministic feature extractor. Any intuition why this alone leads to such a big difference in Table 1?\n\n- Without any regularization, I\u2019m surprised DisentanGAIL learns anything at all. Wouldn\u2019t the discriminator then be able to tell the two domains apart (especially if one of the differences is color) and fail to provide a meaningful learning signal to the agent?\n\n- If I understand Figure 3 correctly, the orange line is without any regularization (lower bound) and the green is learning without domain differences (upper bound). How come DisentanGAIL can eventually outperform the green line in some of the experiments?\n\n- What\u2019s the gap between DisentanGAIL and the expert policy? Does the expert always achieve a reward of 1?\n\nOther comments\n\n- Please clean up all Figures and Tables. The text is too small and impossible to read without zooming in very close. \n\n- \u201cLearning\u201d typo in last line of page 2.\n\n----------\n\nPost-rebuttal comments\n\nThank you for answering my all questions and updating the manuscript with some of my suggestions. The additions in section 4 clarify the motivation much better and also highlight the differences with prior work. I've increased my score from 6 to 7.\n\n", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1735/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1735/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Domain-Robust Visual Imitation Learning with Mutual Information Constraints", "authorids": ["~Edoardo_Cetin1", "~Oya_Celiktutan2"], "authors": ["Edoardo Cetin", "Oya Celiktutan"], "keywords": ["Imitation Learning", "Reinforcement Learning", "Observational Imitation", "Third-Person Imitation", "Mutual Information", "Domain Adaption", "Machine Learning"], "abstract": "Human beings are able to understand objectives and learn by simply observing others perform a task. Imitation learning methods aim to replicate such capabilities, however, they generally depend on access to a full set of optimal states and actions taken with the agent's actuators and from the agent's point of view. In this paper, we introduce a new algorithm - called Disentangling Generative Adversarial Imitation Learning (DisentanGAIL) - with the purpose of bypassing such constraints. Our algorithm enables autonomous agents to learn directly from high dimensional observations of an expert performing a task, by making use of adversarial learning with a latent representation inside the discriminator network. Such latent representation is regularized through mutual information constraints to incentivize learning only features that encode information about the completion levels of the task being demonstrated. This allows to obtain a shared feature space to successfully perform imitation while disregarding the differences between the expert's and the agent's domains. Empirically, our algorithm is able to efficiently imitate in a diverse range of control problems including balancing, manipulation and locomotive tasks, while being robust to various domain differences in terms of both environment appearance and agent embodiment.", "one-sentence_summary": "Imitation of visual expert demonstrations robust to appearance and embodiment mismatch, working for high dimensional control problems.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "cetin|domainrobust_visual_imitation_learning_with_mutual_information_constraints", "supplementary_material": "/attachment/a2f45d7daa76f701540a30f84a144a328ff20c3a.zip", "pdf": "/pdf/ffeade3d551ddc81dea27db706160b3bc6510cec.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ncetin2021domainrobust,\ntitle={Domain-Robust Visual Imitation Learning with Mutual Information Constraints},\nauthor={Edoardo Cetin and Oya Celiktutan},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=QubpWYfdNry}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "QubpWYfdNry", "replyto": "QubpWYfdNry", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1735/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538111851, "tmdate": 1606915788038, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1735/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1735/-/Official_Review"}}}, {"id": "ZmGA5_PnZX", "original": null, "number": 2, "cdate": 1603949764597, "ddate": null, "tcdate": 1603949764597, "tmdate": 1606755935640, "tddate": null, "forum": "QubpWYfdNry", "replyto": "QubpWYfdNry", "invitation": "ICLR.cc/2021/Conference/Paper1735/-/Official_Review", "content": {"title": "Review", "review": "Summary\n--\nThis paper proposes a method for performing observational imitation learning -- an existing task that seeks to enable an agent to learn from visual observations of expert behavior in order to roughly imitate the expert behavior. The method employs an adversarial imitation learning objective function that incorporates proposed mutual information constraints that are intended to force the representation space to be invariant to the domain of the data sources, and instead only encode goal-completion information.\n\nExperiments illustrate that the method was able to learn and be performant despite visual and embodiment differences in the expert and agent domain on various mujoco environments, exhibiting substantially better performance to a different observational IL method. Experiments also illustrate good performance on a slightly higher dimensional task.\n\nClarity and Correctness\n--\nThe paper has a few somewhat painful clarity issues that make it difficult to understand.\n\n- Section 4 starts with a subsection that describes components of the architecture, when instead it should start with a clear high-level description of the approach.\n\n- S4.2 The relationship between B_P.E and B_P.\\pi and B_E and B_\\pi is unclear. The former two terms were never defined in the preliminaries, so it's not clear whether they constitute additional data that was not mentioned in the assumptions. This ties into my comment about about Section 4 needing an introductory paragraph to lay out the high-level assumptions, inputs and outputs, and idea of the method. The notion of \"prior data\" is used in the paper, but I cannot find a clear description of it anywhere.\n\nOriginality\n--\nAs mentioned in the paper, this paper is closely related to Stadie et al. (2017). There's description of the differences to it in various places throughout the paper, but it would be nice if there were a clear section on the comparison of objective functions.\n\nSignificance\n--\nThe significance of this paper is that it demonstrates a more performant method for performing observational imitation learning, which is potentially more applicable than standard IL approaches that require state or state-action traces.\n\n- Why was TPIL not used as a method of comparison in Fig 3?\n\n- The results shown in Fig 2 have rather short x-axes, with a maximum of 20,000 steps of training. The comparisons would be more informative if training were run for more steps (e.g. 1e6 or 1e7). It is possible that the other approaches end up matching expert performance as well, without any great loss in efficiency.\n\nOther comments\n--\n- S4.2 prior data constraint -- the inequality is backwards (MI is nonnegative)\n- S3.2, trajectories are sequences, use () not {}\n- The logical and notation in S4.2 is very confusing -- the statements inside the parentheses that define d_i aren't boolean truth values, so it makes the definition of d_i confusion. My suggestion is just to use d_i = 1(o \\in B_e), since B_e and B_\\pi are disjoint.\n- Be precise / define what you mean by \"high dimensional\". \"High\" is subjective, and is perhaps not the most appropriate adjective to describe 7 dimensional tasks. Also, make it clear that the high dimensionality is in the action space.\n\nPost-rebuttal comments\n--\nAfter reading the authors' response and the updated components of the manuscript, I thank the authors for addressing nearly all of my concerns. The inclusion of a clearer motivation, more discussion w.r.t. TPIL, and comparison to TPIL, all enhance my understanding of the contributions of the paper beyond my original review enough for me to increase my score from a 6 to a 7.\n", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1735/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1735/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Domain-Robust Visual Imitation Learning with Mutual Information Constraints", "authorids": ["~Edoardo_Cetin1", "~Oya_Celiktutan2"], "authors": ["Edoardo Cetin", "Oya Celiktutan"], "keywords": ["Imitation Learning", "Reinforcement Learning", "Observational Imitation", "Third-Person Imitation", "Mutual Information", "Domain Adaption", "Machine Learning"], "abstract": "Human beings are able to understand objectives and learn by simply observing others perform a task. Imitation learning methods aim to replicate such capabilities, however, they generally depend on access to a full set of optimal states and actions taken with the agent's actuators and from the agent's point of view. In this paper, we introduce a new algorithm - called Disentangling Generative Adversarial Imitation Learning (DisentanGAIL) - with the purpose of bypassing such constraints. Our algorithm enables autonomous agents to learn directly from high dimensional observations of an expert performing a task, by making use of adversarial learning with a latent representation inside the discriminator network. Such latent representation is regularized through mutual information constraints to incentivize learning only features that encode information about the completion levels of the task being demonstrated. This allows to obtain a shared feature space to successfully perform imitation while disregarding the differences between the expert's and the agent's domains. Empirically, our algorithm is able to efficiently imitate in a diverse range of control problems including balancing, manipulation and locomotive tasks, while being robust to various domain differences in terms of both environment appearance and agent embodiment.", "one-sentence_summary": "Imitation of visual expert demonstrations robust to appearance and embodiment mismatch, working for high dimensional control problems.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "cetin|domainrobust_visual_imitation_learning_with_mutual_information_constraints", "supplementary_material": "/attachment/a2f45d7daa76f701540a30f84a144a328ff20c3a.zip", "pdf": "/pdf/ffeade3d551ddc81dea27db706160b3bc6510cec.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ncetin2021domainrobust,\ntitle={Domain-Robust Visual Imitation Learning with Mutual Information Constraints},\nauthor={Edoardo Cetin and Oya Celiktutan},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=QubpWYfdNry}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "QubpWYfdNry", "replyto": "QubpWYfdNry", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1735/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538111851, "tmdate": 1606915788038, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1735/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1735/-/Official_Review"}}}, {"id": "xYa7oPKsaKj", "original": null, "number": 1, "cdate": 1603705714837, "ddate": null, "tcdate": 1603705714837, "tmdate": 1606489100778, "tddate": null, "forum": "QubpWYfdNry", "replyto": "QubpWYfdNry", "invitation": "ICLR.cc/2021/Conference/Paper1735/-/Official_Review", "content": {"title": "Strong paper, needs some additional comparisons", "review": "## Summary\nThe paper proposes a novel approach for third-person visual imitation learning from observations, ie for imitating a different agent in a potentially different environment purely from visual demonstrations. The main difference over prior work (eg Stadie et al, 2017) that used domain confusion objectives is the introduction of new regularization objectives on the learned representation of the visual scene. Empirically, the paper shows that the proposed regularizations can improve performance across a wide range of third-person visual imitation tasks, transferring between simulated agents with different appearances and/or morphologies.\n\n\n## Strengths\n- the paper is very clearly written and easy to follow, necessary background is adequately covered, all components of the model are explained in detail\n- all novel design choices are ablated in the experimental section\n- experiments are conducted across multiple environments and source/target variations, both appearance and morphology -- empirical performance improvements are demonstrated\n- experiments include higher-dimensional, robotic manipulation environments, more complex than those tested in prior work\n\n\n## Weaknesses\n- **not fully fair comparison to baselines**: since the main novelty lies in the introduction of novel regularization objectives, the \"DisentanGAIL w/ domain confusion loss\" is the main comparison method since the only difference to the proposed method is the representation regularization function. However, this baseline does not have access to the additional collected prior datasets used for the introduced \"prior regularization\" loss. Looking at Figure 2, it seems that the performance of DisentanGAIL w/o Prior Data and DisentanGAIL w/ domain confusion loss are near-equivalent which suggests that the access to the additional prior data might be the main factor that contributes to DisentanGAIL's superior performance --> an additional baseline \"DisentanGAIL w/ domain confusion loss & prior data\" is needed, which additionally trains the domain confusion objective on the prior data collected for the DisentanGAIL prior regularization objective, to allow for fair comparison of both regularization approaches with access to the same data\n- **missing baseline results on harder tasks**: on the harder tasks shown in Fig 3 there is no evaluation of the baseline methods, which makes it hard to judge how hard these tasks actually are for prior third-person visual imitation approaches\n\n\n## Questions\n- from my understanding the issue of prior work that constrains the domain-related MI to 0 (described in Appendix, section A) appears for *any* two domains (since, particularly in the beginning of training, there will always be differences in the goal-reaching distributions of expert and policy data). However, it seems that DisentanGAIL w/ domain confusion loss, which applies the MI=0 loss from Stadie et al. 2017, works well on many of the tested domains. How does that fit to the explanation made in Section A + the quant results in Fig 5?\n- what differences result in the substantial performance difference between TPIL and DisentanGAIL w/ domain confusion loss? \n- why does the paper report the \"max cumulative reward so far\" not the expected cumulative reward of the current episode? the latter could give a better idea of the training stability of the different algorithms\n- out of curiosity: the shown scenarios are visually still pretty similar between source and target (eg background etc) --> how do you think an approach like the proposed one would scale to visually substantially different environments, eg from one kitchen to another?\n\n## Suggestions to improve the paper\n- add an additional baseline \"DisentanGAIL w/ domain confusion loss & prior data\", as discussed in the \"weaknesses\" section, particularly for the transfer tasks on the bottom right of Fig 2 in which the discrepancy between DisentanGAIL and the baselines is the largest\n- add evaluation of baselines (particularly DisentanGAIL w/ domain confusion loss (w/ and w/o prior data)) to the harder manipulation environments in Fig 3 to show the benefits of the introduced regularizations\n- since the proposed method addresses a concrete problem of prior work (as explained in appendix Section A) with a clear intuition, it could be nice to add a toy experiment early in the paper that demonstrates this effect empirically for an easy-to-analyze imitation problem, showing that for MI=0 the agent cannot properly learn to imitate since it is unable to capture the relevant information --> such an experiment could help to further motivate the need for the new regularizations\n- the additional assumption of a \"prior dataset\" collected in both domains for additionally constraining the latent representation is first mentioned in section 4.2 --> since this is a different assumption from prior work on cross-domain imitation it would be good to mention this earlier, maybe in a dedicated \"Problem Statement\" section\n- for qualitative matching results in Fig 4 in the appendix it would be nice to show the corresponding matches found when using the domain confusion loss instead of the proposed regularizations to see whether some of the failure cases are interpretable\n- I wonder whether it would be possible to show imitation across agents with more drastic morphology differences in the most challenging 7DOF robotic manipulation tasks. Right now, while there are differences in the link dimensions, the main difference still seems to be in the visual appearance (at least from looking at the provided pictures) --> maybe one could try to eg try to transfer between robots with different gripper morphologies (eg one agent has U-shaped gripper vs another agent with T-shaped \"gripper\" for the pusher task), or from a 4DOF arm to a 7DOF arm etc.\n- as mentioned in one question above: while the agent appearance and morphology changes between the experiments, the largest part of the observation, the background, is usually constant across source and target environment --> it would be interesting to see experiments with different background appearances to see the robustness of the method\n\n## Overall Recommendation\nThe paper is well written and the experiments are covering a wide array of third-person visual imitation problems on which the proposed method shows strong results. The experimental evaluation seems thorough, all design choices are ablated. My main doubts are about the fairness of comparison to the baselines (particularly with regard to the additional data available to the proposed method), and some lacking baseline results on the harder tasks. Therefore I cannot fully support acceptance yet, but if the authors are able to provide the additional evaluations and answer the questions posed above adequately, I am willing to increase my score and vote for acceptance.\n\n\n## Post-Rebuttal Comments\nI thank the authors for their detailed feedback. Particularly the clarifications about the usage of prior data in the baselines were very helpful and the added results with background differences are interesting!\n\nI am not sure whether it is standard to show the learning curves with the \"max achieved return so far\" in the GAIL literature, but if not I still think the true reward per episode should be shown to properly reflect the training stability of the algorithm. Potentially, the stability could also be increased by adding a learning rate decay schedule?\n\nOverall, the rebuttal addressed my questions and incorporated many of the suggestions, therefore I am increasing my score and vote for acceptance.\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1735/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1735/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Domain-Robust Visual Imitation Learning with Mutual Information Constraints", "authorids": ["~Edoardo_Cetin1", "~Oya_Celiktutan2"], "authors": ["Edoardo Cetin", "Oya Celiktutan"], "keywords": ["Imitation Learning", "Reinforcement Learning", "Observational Imitation", "Third-Person Imitation", "Mutual Information", "Domain Adaption", "Machine Learning"], "abstract": "Human beings are able to understand objectives and learn by simply observing others perform a task. Imitation learning methods aim to replicate such capabilities, however, they generally depend on access to a full set of optimal states and actions taken with the agent's actuators and from the agent's point of view. In this paper, we introduce a new algorithm - called Disentangling Generative Adversarial Imitation Learning (DisentanGAIL) - with the purpose of bypassing such constraints. Our algorithm enables autonomous agents to learn directly from high dimensional observations of an expert performing a task, by making use of adversarial learning with a latent representation inside the discriminator network. Such latent representation is regularized through mutual information constraints to incentivize learning only features that encode information about the completion levels of the task being demonstrated. This allows to obtain a shared feature space to successfully perform imitation while disregarding the differences between the expert's and the agent's domains. Empirically, our algorithm is able to efficiently imitate in a diverse range of control problems including balancing, manipulation and locomotive tasks, while being robust to various domain differences in terms of both environment appearance and agent embodiment.", "one-sentence_summary": "Imitation of visual expert demonstrations robust to appearance and embodiment mismatch, working for high dimensional control problems.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "cetin|domainrobust_visual_imitation_learning_with_mutual_information_constraints", "supplementary_material": "/attachment/a2f45d7daa76f701540a30f84a144a328ff20c3a.zip", "pdf": "/pdf/ffeade3d551ddc81dea27db706160b3bc6510cec.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ncetin2021domainrobust,\ntitle={Domain-Robust Visual Imitation Learning with Mutual Information Constraints},\nauthor={Edoardo Cetin and Oya Celiktutan},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=QubpWYfdNry}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "QubpWYfdNry", "replyto": "QubpWYfdNry", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1735/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538111851, "tmdate": 1606915788038, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1735/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1735/-/Official_Review"}}}, {"id": "yDFC97f5bs", "original": null, "number": 12, "cdate": 1605979491361, "ddate": null, "tcdate": 1605979491361, "tmdate": 1605980033363, "tddate": null, "forum": "QubpWYfdNry", "replyto": "-w50btq2MZm", "invitation": "ICLR.cc/2021/Conference/Paper1735/-/Official_Comment", "content": {"title": "Responses to AnonReviewer3 - part 3", "comment": "**Suggestions**\n\n4. the additional assumption of a \"prior dataset\" collected in both domains for additionally constraining the latent representation is first mentioned in section 4.2 --> since this is a different assumption from prior work on cross-domain imitation it would be good to mention this earlier, maybe in a dedicated \"Problem Statement\" section\n\nWe have now added an introductory paragraph in Section 4 that introduces DisentanGAIL and defines prior data before delving into the algorithm's details.\n\n5. for qualitative matching results in Fig 4 in the appendix it would be nice to show the corresponding matches found when using the domain confusion loss instead of the proposed regularizations to see whether some of the failure cases are interpretable\n\nIn the latest revision, we have expanded section D.1 to report additional results showing the observations couplings produced by DisentanGAIL with domain confusion loss.\n\n6. I wonder whether it would be possible to show imitation across agents with more drastic morphology differences in the most challenging 7DOF robotic manipulation tasks. Right now, while there are differences in the link dimensions, the main difference still seems to be in the visual appearance (at least from looking at the provided pictures) --> maybe one could try to eg try to transfer between robots with different gripper morphologies (eg one agent has U-shaped gripper vs another agent with T-shaped \"gripper\" for the pusher task), or from a 4DOF arm to a 7DOF arm etc.\n\nWe realize that we originally omitted from Appendix C the fact that the gripper morphologies do differ in the Pusher task (the Human-like 7DOF-Pusher has a 4-linked thick gripper while the original 7DOF-Pusher has a 3-linked thin gripper). We have now included this information in the current revision. The visualizations in Appendix D.1, regarding the manipulation tasks, indicate that DisentanGAIL effectively learns to encode information about the objects\u2019 positions, which should be unaffected by the agents\u2019 morphology differences. Therefore, we hypothesize that observational imitation between increasingly different agents should be possible. However, DisentanGAIL would have fewer meaningful correspondences to learn the appropriate motion to move the object towards the target, making the initial exploration stage harder. \n\n7. as mentioned in one question above: while the agent appearance and morphology changes between the experiments, the largest part of the observation, the background, is usually constant across source and target environment --> it would be interesting to see experiments with different background appearances to see the robustness of the method\n\nFollowing the reviewer's suggestion, in the latest revision we added section D.4 to the Appendix, where we test DisentanGAIL on two alternative target environments in the Hopper and 7DOF-Pusher environment realms, with very distinct backgrounds from the relative source environments: \n-\tthe new target environment in the Hopper realm has a much darker floor, where the tiles are difficult to discern. \n-\tthe new target environment in the 7DOF-Pusher realm includes a green table and a white floor, both of which differ greatly in appearance to the grey table and black floor of the source environment. \n\nThe obtained results show that background domain differences have a minor effect on DisentanGAIL\u2019s final performance. However, they have a more prominent effect on DisentanGAIL\u2019s efficiency, making our algorithm converge in an increased number of epochs. This is particularly noticeable in the Hopper realm\u2019s results. We hypothesize this is because in these experiments there is a greater amount of domain information that requires to be \u2018disentangled\u2019 from the useful goal-completion information. For example, in the locomotion realms, the pre-processor encodes goal-completion information about the relative position of the agent with respect to the floor tiles in the two environments (as empirically suggested in Section D.1). In the new target environment of the Hopper realm, this information needs to be also disentangled from domain information regarding the tiles\u2019 appearance.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1735/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1735/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Domain-Robust Visual Imitation Learning with Mutual Information Constraints", "authorids": ["~Edoardo_Cetin1", "~Oya_Celiktutan2"], "authors": ["Edoardo Cetin", "Oya Celiktutan"], "keywords": ["Imitation Learning", "Reinforcement Learning", "Observational Imitation", "Third-Person Imitation", "Mutual Information", "Domain Adaption", "Machine Learning"], "abstract": "Human beings are able to understand objectives and learn by simply observing others perform a task. Imitation learning methods aim to replicate such capabilities, however, they generally depend on access to a full set of optimal states and actions taken with the agent's actuators and from the agent's point of view. In this paper, we introduce a new algorithm - called Disentangling Generative Adversarial Imitation Learning (DisentanGAIL) - with the purpose of bypassing such constraints. Our algorithm enables autonomous agents to learn directly from high dimensional observations of an expert performing a task, by making use of adversarial learning with a latent representation inside the discriminator network. Such latent representation is regularized through mutual information constraints to incentivize learning only features that encode information about the completion levels of the task being demonstrated. This allows to obtain a shared feature space to successfully perform imitation while disregarding the differences between the expert's and the agent's domains. Empirically, our algorithm is able to efficiently imitate in a diverse range of control problems including balancing, manipulation and locomotive tasks, while being robust to various domain differences in terms of both environment appearance and agent embodiment.", "one-sentence_summary": "Imitation of visual expert demonstrations robust to appearance and embodiment mismatch, working for high dimensional control problems.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "cetin|domainrobust_visual_imitation_learning_with_mutual_information_constraints", "supplementary_material": "/attachment/a2f45d7daa76f701540a30f84a144a328ff20c3a.zip", "pdf": "/pdf/ffeade3d551ddc81dea27db706160b3bc6510cec.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ncetin2021domainrobust,\ntitle={Domain-Robust Visual Imitation Learning with Mutual Information Constraints},\nauthor={Edoardo Cetin and Oya Celiktutan},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=QubpWYfdNry}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "QubpWYfdNry", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1735/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1735/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1735/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1735/Authors|ICLR.cc/2021/Conference/Paper1735/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1735/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923856314, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1735/-/Official_Comment"}}}, {"id": "-w50btq2MZm", "original": null, "number": 11, "cdate": 1605979420608, "ddate": null, "tcdate": 1605979420608, "tmdate": 1605980008696, "tddate": null, "forum": "QubpWYfdNry", "replyto": "nM7TjQ-AnrO", "invitation": "ICLR.cc/2021/Conference/Paper1735/-/Official_Comment", "content": {"title": "Responses to AnonReviewer3 - part 2", "comment": "**Questions**\n\n3. why does the paper report the \"max cumulative reward so far\" not the expected cumulative reward of the current episode? the latter could give a better idea of the training stability of the different algorithms\n\nOnce GAIL-based methods achieve optimality, the distribution of behavior in B_\\pi and B_E become semantically similar. This causes instabilities since the discriminator still tries to perfectly discern the two distributions but cannot effectively use goal-completion information to do so. This made some of the performance curves harder to discern and, hence, harder to evaluate. Therefore, for visual clarity, we opted to display the described metric.\n\n4. out of curiosity: the shown scenarios are visually still pretty similar between source and target (eg background etc) --> how do you think an approach like the proposed one would scale to visually substantially different environments, eg from one kitchen to another?\n\nObservational imitation is still a very much ill-posed problem and hence, we hypothesize that many of the recovered solutions are also possible thanks to the structural inductive biases of our model. Particularly, we believe convolutions play a big role in guiding the optimization towards looking for locally consistent shared elements across the expert and agent observations. Preliminary unsuccessful experiments in environments with few correspondences between the agent and expert domains (e.g., different objects to be moved to different targets on different tables) seemed to be in line with this hypothesis. Hence, moving forward, we believe data augmentation and meta-learning can play a key role in combating some of these issues. Particularly, data-augmentation can help the model understand which characteristics of the observation are not a consequence of the expert's behavior and should therefore be disregarded. Similarly, meta-learning could help the model build a prior over which features contain 'plausible' information about the goal completion levels within a distribution of observational imitation problems.\n\n\n**Suggestions**\n\n1. add an additional baseline \"DisentanGAIL w/ domain confusion loss & prior data\", as discussed in the \"weaknesses\" section...\n\n(From answer to Weaknesses 1) We would like to clarify that the algorithms making use of the domain confusion loss (TPIL and DisentanGAIL with DCL) do make use of prior data as additional training inputs with class label 0, and the correct domain label. This is analogous to the way the original TPIL algorithm made use of 'failure data', sampled by failure policies in both expert and agent domains. Enforcing only the expert demonstration constraint outperforms the domain confusion loss on most examined problems, even as this version of DisentanGAIL does not have access to prior data. Following the reviewer's comments, we realize how this information might have been unclear and added it explicitly in the latest revision (see Appendix B.1).\n\n2. add evaluation of baselines (particularly DisentanGAIL w/ domain confusion loss (w/ and w/o prior data)) to the harder manipulation environments in Fig 3 to show the benefits of the introduced regularizations\n\n(From answer to Weaknesses 2) We collected the results and added the relative performance curves for all the remaining baselines in the high dimensional observational imitation problems. We also added Table 2 in Section 6 summarizing these results. \n\n3. since the proposed method addresses a concrete problem of prior work (as explained in appendix Section A) with a clear intuition, it could be nice to add a toy experiment early in the paper that demonstrates this effect empirically for an easy-to-analyze imitation problem, showing that for MI=0 the agent cannot properly learn to imitate since it is unable to capture the relevant information --> such an experiment could help to further motivate the need for the new regularizations\n\nSuch an experiment could surely solidify the proposed arguments for aiming to enforce a different constraint. However, we believe that making a stable algorithm which truly enforces MI=0 is not a trivial task. In particular, both a domain confusion loss with a very high weighting coefficient and a dual penalty with I_{max}=0 might lead to some minimal domain information leaking into the latent representations, simply due to the stochastic nature of the training process. On the other hand, we think that the empirical issues of enforcing a low mutual information constraint are also somewhat shown by the experiments in section D.3 of the Appendix. Here, setting I_{max}=0.01 for the expert demonstration constraint leads to a consistent reduction in performance in any experiment considering domain differences. We expanded Section 4.2 in the main text and section A in the Appendix to more thoroughly discuss the issues related to the constraint proposed by Stadie et al. and the effects of the domain confusion loss.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1735/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1735/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Domain-Robust Visual Imitation Learning with Mutual Information Constraints", "authorids": ["~Edoardo_Cetin1", "~Oya_Celiktutan2"], "authors": ["Edoardo Cetin", "Oya Celiktutan"], "keywords": ["Imitation Learning", "Reinforcement Learning", "Observational Imitation", "Third-Person Imitation", "Mutual Information", "Domain Adaption", "Machine Learning"], "abstract": "Human beings are able to understand objectives and learn by simply observing others perform a task. Imitation learning methods aim to replicate such capabilities, however, they generally depend on access to a full set of optimal states and actions taken with the agent's actuators and from the agent's point of view. In this paper, we introduce a new algorithm - called Disentangling Generative Adversarial Imitation Learning (DisentanGAIL) - with the purpose of bypassing such constraints. Our algorithm enables autonomous agents to learn directly from high dimensional observations of an expert performing a task, by making use of adversarial learning with a latent representation inside the discriminator network. Such latent representation is regularized through mutual information constraints to incentivize learning only features that encode information about the completion levels of the task being demonstrated. This allows to obtain a shared feature space to successfully perform imitation while disregarding the differences between the expert's and the agent's domains. Empirically, our algorithm is able to efficiently imitate in a diverse range of control problems including balancing, manipulation and locomotive tasks, while being robust to various domain differences in terms of both environment appearance and agent embodiment.", "one-sentence_summary": "Imitation of visual expert demonstrations robust to appearance and embodiment mismatch, working for high dimensional control problems.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "cetin|domainrobust_visual_imitation_learning_with_mutual_information_constraints", "supplementary_material": "/attachment/a2f45d7daa76f701540a30f84a144a328ff20c3a.zip", "pdf": "/pdf/ffeade3d551ddc81dea27db706160b3bc6510cec.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ncetin2021domainrobust,\ntitle={Domain-Robust Visual Imitation Learning with Mutual Information Constraints},\nauthor={Edoardo Cetin and Oya Celiktutan},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=QubpWYfdNry}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "QubpWYfdNry", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1735/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1735/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1735/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1735/Authors|ICLR.cc/2021/Conference/Paper1735/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1735/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923856314, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1735/-/Official_Comment"}}}, {"id": "nM7TjQ-AnrO", "original": null, "number": 10, "cdate": 1605979313628, "ddate": null, "tcdate": 1605979313628, "tmdate": 1605979977354, "tddate": null, "forum": "QubpWYfdNry", "replyto": "xYa7oPKsaKj", "invitation": "ICLR.cc/2021/Conference/Paper1735/-/Official_Comment", "content": {"title": "Responses to AnonReviewer3 - part 1", "comment": "**Weaknesses**\n\n1. not fully fair comparison to baselines: since the main novelty lies in the introduction of novel regularization objectives, the \"DisentanGAIL w/ domain confusion loss\" is the main comparison method since the only difference to the proposed method is the representation regularization function. However, this baseline does not have access to the additional collected prior datasets used for the introduced \"prior regularization\" loss. Looking at Figure 2, it seems that the performance of DisentanGAIL w/o Prior Data and DisentanGAIL w/ domain confusion loss are near-equivalent which suggests that the access to the additional prior data might be the main factor that contributes to DisentanGAIL's superior performance --> an additional baseline \"DisentanGAIL w/ domain confusion loss & prior data\" is needed, which additionally trains the domain confusion objective on the prior data collected for the DisentanGAIL prior regularization objective, to allow for fair comparison of both regularization approaches with access to the same data\n\nWe understand the reviewer's concerns and we would like to clarify that the algorithms making use of the domain confusion loss (TPIL and DisentanGAIL with DCL) do make use of prior data as additional training inputs with class label 0, and the correct domain label. This is analogous to the way the original TPIL algorithm made use of 'failure data', sampled by failure policies in both expert and agent domains. Enforcing only the expert demonstration constraint outperforms the domain confusion loss on most examined problems, even if this version of DisentanGAIL does not have access to prior data. Following the reviewer's comments, we realize how this information might have been unclear and have therefore added it explicitly in the latest revision (see Appendix B.1).\n\n2. missing baseline results on harder tasks: on the harder tasks shown in Fig 3 there is no evaluation of the baseline methods, which makes it hard to judge how hard these tasks actually are for prior third-person visual imitation approaches\n\nIn the latest revision, we collected the results and added the relative performance curves for all the remaining baselines in the high dimensional observational imitation problems. We also added Table 2 in Section 6 summarizing these results. We did not originally include them because some algorithms, such as TPIL, yielded very suboptimal results in preliminary experiments. Therefore, we believed their inclusion would provide little information.\n\n**Questions**\n\n1. from my understanding the issue of prior work that constrains the domain-related MI to 0 (described in Appendix, section A) appears for any two domains (since, particularly in the beginning of training, there will always be differences in the goal-reaching distributions of expert and policy data). However, it seems that DisentanGAIL w/ domain confusion loss, which applies the MI=0 loss from Stadie et al. 2017, works well on many of the tested domains. How does that fit to the explanation made in Section A + the quant results in Fig 5?\n\nWhile Stadie et al. aim to constrain the mutual information of some latent representation with the domain labels to 0, in practice, their algorithm does not try to enforce this precisely. Particularly, the domain confusion loss with a fixed weight coefficient acts only as a heuristic penalty related to some quantity proportional to the mutual information contained in the latent representations. In the current revision, we expanded our discussion about these implications in Appendix A.\n\n2. what differences result in the substantial performance difference between TPIL and DisentanGAIL w/ domain confusion loss?\n\nWhile DisentanGAIL with domain confusion loss shares most of the implementation details with DisentanGAIL (described in Appendix B), we obtained the TPIL results by adapting the authors' original code. The main distinctive features of DisentanGAIL with domain confusion loss, which likely had a significant effect on performance are:\n- Off-policy maximum-entropy actor - enabling more sample-efficient learning than TRPO and incentivizing the agent to explore even when an uninformative learning signal is provided. \n- Discriminator architecture and processing of the pre-processor's stochastic output (as described in Appendix B) - providing some implicit regularization and permitting to easily diminish the information contained in any of the independent dimensions of the Gaussian representations. \n- Invariant discriminator regularization - providing further regularization for the expressivity of the invariant discriminator.\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1735/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1735/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Domain-Robust Visual Imitation Learning with Mutual Information Constraints", "authorids": ["~Edoardo_Cetin1", "~Oya_Celiktutan2"], "authors": ["Edoardo Cetin", "Oya Celiktutan"], "keywords": ["Imitation Learning", "Reinforcement Learning", "Observational Imitation", "Third-Person Imitation", "Mutual Information", "Domain Adaption", "Machine Learning"], "abstract": "Human beings are able to understand objectives and learn by simply observing others perform a task. Imitation learning methods aim to replicate such capabilities, however, they generally depend on access to a full set of optimal states and actions taken with the agent's actuators and from the agent's point of view. In this paper, we introduce a new algorithm - called Disentangling Generative Adversarial Imitation Learning (DisentanGAIL) - with the purpose of bypassing such constraints. Our algorithm enables autonomous agents to learn directly from high dimensional observations of an expert performing a task, by making use of adversarial learning with a latent representation inside the discriminator network. Such latent representation is regularized through mutual information constraints to incentivize learning only features that encode information about the completion levels of the task being demonstrated. This allows to obtain a shared feature space to successfully perform imitation while disregarding the differences between the expert's and the agent's domains. Empirically, our algorithm is able to efficiently imitate in a diverse range of control problems including balancing, manipulation and locomotive tasks, while being robust to various domain differences in terms of both environment appearance and agent embodiment.", "one-sentence_summary": "Imitation of visual expert demonstrations robust to appearance and embodiment mismatch, working for high dimensional control problems.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "cetin|domainrobust_visual_imitation_learning_with_mutual_information_constraints", "supplementary_material": "/attachment/a2f45d7daa76f701540a30f84a144a328ff20c3a.zip", "pdf": "/pdf/ffeade3d551ddc81dea27db706160b3bc6510cec.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ncetin2021domainrobust,\ntitle={Domain-Robust Visual Imitation Learning with Mutual Information Constraints},\nauthor={Edoardo Cetin and Oya Celiktutan},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=QubpWYfdNry}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "QubpWYfdNry", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1735/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1735/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1735/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1735/Authors|ICLR.cc/2021/Conference/Paper1735/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1735/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923856314, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1735/-/Official_Comment"}}}, {"id": "B79v87pQlV4", "original": null, "number": 9, "cdate": 1605978410178, "ddate": null, "tcdate": 1605978410178, "tmdate": 1605979925783, "tddate": null, "forum": "QubpWYfdNry", "replyto": "ZmGA5_PnZX", "invitation": "ICLR.cc/2021/Conference/Paper1735/-/Official_Comment", "content": {"title": "Responses to AnonReviewer1", "comment": "**Clarity and Correctness**\n\n1. Section 4 starts with a subsection that describes components of the architecture, when instead it should start with a clear high-level description of the approach.\n2. The relationship between B_{P.E} and B_{P.\\pi} and B_E and B_\\pi is unclear. The former two terms were never defined in the preliminaries, so it's not clear whether they constitute additional data that was not mentioned in the assumptions. This ties into my comment about Section 4 needing an introductory paragraph to lay out the high-level assumptions, inputs and outputs, and idea of the method. The notion of \"prior data\" is used in the paper, but I cannot find a clear description of it anywhere.\n\nIn the current revision, we added a paragraph at the beginning of Section 4 that provides a high-level overview and explains the objectives of DisentanGAIL, referencing the previously introduced notation. In the same paragraph, we also describe the nature of prior data as representing unsupervisedly-collected observations of both expert\u2019s and agent\u2019s domains and define its notation.\n\n**Originality**\n\n1. As mentioned in the paper, this paper is closely related to Stadie et al. (2017). There's description of the differences to it in various places throughout the paper, but it would be nice if there were a clear section on the comparison of objective functions.\n\nWe extended Section 4.2 with a paragraph discussing the implications of the constraint from Stadie et al. and its implementation, as compared to ours.\n\n\n**Significance**\n\n1. Why was TPIL not used as a method of comparison in Fig 3?\n\nIn the latest revision, we collected the results and added the relative performance curves for all the remaining baselines in the high dimensional observational imitation problems. We also added Table 2 in Section 6 summarizing these results. We did not originally include these results because some algorithms, such as TPIL, yielded very suboptimal results in preliminary experiments. Therefore, we believed their inclusion would provide little information.\n\n2. The results shown in Fig 2 have rather short x-axes, with a maximum of 20,000 steps of training. The comparisons would be more informative if training were run for more steps (e.g. 1e6 or 1e7). It is possible that the other approaches end up matching expert performance as well, without any great loss in efficiency.\n\nWhile we agree that running for a much greater number of steps would be more informative, in the low dimensional experiments we wanted to focus on providing a wide range of results, testing a multitude of methods and domain differences. This would not have been possible using our computational resources if we collected orders of magnitude of additional training steps per experiment. Moreover, one of the goals of our experiments was evaluating the algorithms for their efficiency, in order to understand their potential real-world applicability. Hence, in this context, we believe that 20,000 frames of experience are a fair allowance for solving the low dimensional task.\n\n**Other comments**\n\nWe addressed the reviewer\u2019s additional comments in the latest version of the paper. Particularly, we fixed the inexact notation, changed the definition of d_i and explicitly defined the notion of 'high dimensional tasks' in Section 6, as suggested.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1735/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1735/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Domain-Robust Visual Imitation Learning with Mutual Information Constraints", "authorids": ["~Edoardo_Cetin1", "~Oya_Celiktutan2"], "authors": ["Edoardo Cetin", "Oya Celiktutan"], "keywords": ["Imitation Learning", "Reinforcement Learning", "Observational Imitation", "Third-Person Imitation", "Mutual Information", "Domain Adaption", "Machine Learning"], "abstract": "Human beings are able to understand objectives and learn by simply observing others perform a task. Imitation learning methods aim to replicate such capabilities, however, they generally depend on access to a full set of optimal states and actions taken with the agent's actuators and from the agent's point of view. In this paper, we introduce a new algorithm - called Disentangling Generative Adversarial Imitation Learning (DisentanGAIL) - with the purpose of bypassing such constraints. Our algorithm enables autonomous agents to learn directly from high dimensional observations of an expert performing a task, by making use of adversarial learning with a latent representation inside the discriminator network. Such latent representation is regularized through mutual information constraints to incentivize learning only features that encode information about the completion levels of the task being demonstrated. This allows to obtain a shared feature space to successfully perform imitation while disregarding the differences between the expert's and the agent's domains. Empirically, our algorithm is able to efficiently imitate in a diverse range of control problems including balancing, manipulation and locomotive tasks, while being robust to various domain differences in terms of both environment appearance and agent embodiment.", "one-sentence_summary": "Imitation of visual expert demonstrations robust to appearance and embodiment mismatch, working for high dimensional control problems.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "cetin|domainrobust_visual_imitation_learning_with_mutual_information_constraints", "supplementary_material": "/attachment/a2f45d7daa76f701540a30f84a144a328ff20c3a.zip", "pdf": "/pdf/ffeade3d551ddc81dea27db706160b3bc6510cec.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ncetin2021domainrobust,\ntitle={Domain-Robust Visual Imitation Learning with Mutual Information Constraints},\nauthor={Edoardo Cetin and Oya Celiktutan},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=QubpWYfdNry}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "QubpWYfdNry", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1735/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1735/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1735/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1735/Authors|ICLR.cc/2021/Conference/Paper1735/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1735/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923856314, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1735/-/Official_Comment"}}}, {"id": "lyA_XbD3HhE", "original": null, "number": 7, "cdate": 1605978117578, "ddate": null, "tcdate": 1605978117578, "tmdate": 1605979850518, "tddate": null, "forum": "QubpWYfdNry", "replyto": "K433eqZVgM", "invitation": "ICLR.cc/2021/Conference/Paper1735/-/Official_Comment", "content": {"title": "Responses to AnonReviewer2 - part 1", "comment": "**Concerns:**\n\n1. Regarding the novelty of this paper, I feel like the approach is not particularly novel. The method is similar to TPIL (Stadie et al. 2017) and the main difference between DisentanGAIL and TPIL is that in DisentanGAIL, the authors set the mutual information constraint to be less than 1 bit, and in TPIL, the mutual information is constrained to 0. I can definitely see why the softer regularization works better, but I wonder if the contribution of this work is substantial given such a simple tweak. Moreover, the authors also propose many techniques to make DisentanGAIL work better, such as using unsupervised data with some regularization, double statistics networks, spectral norm, and etc.. However, these seem to be more like implementation tricks rather than major contributions and I'm unsure if they make the contribution substantial enough for the standard of an ICLR paper. Moreover, adding these additional components definitely seem to make the approach much more complex and might require much more tuning to get it to work, which is another concern.\n\nThere are several core differences between DisentanGAIL and prior works, which make our approach concretely novel. In particular, while Stadie et al. argue for constraining the mutual information of some representation with the domain labels to be 0, in practice they try to enforce this very loosely via an additional domain confusion loss with a fixed weight coefficient. This is very different, in both aims and implementation, from our approach of calculating the mutual information explicitly and applying the adaptive and dual penalization terms to precisely ensure the enforcement of the proposed constraints. Moreover, we argue that the identification of the domain information disguising phenomenon is also a valuable contribution to solve the problem of observational imitation. In this regard, the proposed double statistics network and invariant discriminator regularization can become standard practice for future efforts to ensure its prevention. While we understand the reviewer's concern about the increased complexity, we would like to emphasize that all the above-mentioned practices did not add any hyper-parameter which required tuning across our diverse set of experiments. Thus, we do not think this should hinder the applicability of our algorithm. We believe our contribution beyond prior works is also substantiated by our empirical results. Our algorithm can be successfully applied to high dimensional environments, under limited experience regimes, even for problems considering major domain appearance and embodiment differences. In contrast, to the best of our knowledge, prior works showed algorithms based on the domain confusion loss to be successful solely in low dimensional environments (where agents had maximum 2 degrees of freedom) for problems only considering domain appearance differences. Taking into account the reviewer's concerns, we extended section 4.2 in the main text and section A in the Appendix to more thoroughly discuss the work from Stadie et al. and its differences with ours.\n\n2. Another point related to novelty and related works, there are several papers that also consider domain-adaptive imitation learning that seem pretty similar to this paper, such as [1, 2, 3]. [1, 2] consider learning domain-invariant features, while [3] proposes a unifying theoretical framework for domain-adaptive imitation learning. I think these methods could serve as comparisons to DisentanGAIL.\n\nWe thank the reviewer for bringing to our attention DAC-SSM [1] from Okumura et al. In this work, the authors combine the domain confusion loss from Stadie et al. with a modified optimization scheme and a model-based agent. However, as in the original Third-Person Imitation Learning paper, the authors show successful applications exclusively in low dimensional environments considering only domain appearance differences. We now added a reference of DAC-SSM in the Related work section. We are not able to provide a direct comparison as the authors do not provide a publicly accessible implementation of their algorithm or their environments.\n\nIn the Related work section, we did already mention the connection of our work with DAIL [3] from Kim et al. In particular, this work assumes access to aligned demonstrations from environments where both agent and expert already achieved expertise, in both the agent's and the expert's point of view. In contrast, we do not make such an assumption and tackle a similar but more general problem-setting.  \n\nADAIL [3], from Lu et al., tackles a problem which can be described as agent-centric imitation under differences in the expert's and agent's domains dynamics. While this problem is surely related, we believe it to be too distant from the problem of observational imitation tackled in our work."}, "signatures": ["ICLR.cc/2021/Conference/Paper1735/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1735/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Domain-Robust Visual Imitation Learning with Mutual Information Constraints", "authorids": ["~Edoardo_Cetin1", "~Oya_Celiktutan2"], "authors": ["Edoardo Cetin", "Oya Celiktutan"], "keywords": ["Imitation Learning", "Reinforcement Learning", "Observational Imitation", "Third-Person Imitation", "Mutual Information", "Domain Adaption", "Machine Learning"], "abstract": "Human beings are able to understand objectives and learn by simply observing others perform a task. Imitation learning methods aim to replicate such capabilities, however, they generally depend on access to a full set of optimal states and actions taken with the agent's actuators and from the agent's point of view. In this paper, we introduce a new algorithm - called Disentangling Generative Adversarial Imitation Learning (DisentanGAIL) - with the purpose of bypassing such constraints. Our algorithm enables autonomous agents to learn directly from high dimensional observations of an expert performing a task, by making use of adversarial learning with a latent representation inside the discriminator network. Such latent representation is regularized through mutual information constraints to incentivize learning only features that encode information about the completion levels of the task being demonstrated. This allows to obtain a shared feature space to successfully perform imitation while disregarding the differences between the expert's and the agent's domains. Empirically, our algorithm is able to efficiently imitate in a diverse range of control problems including balancing, manipulation and locomotive tasks, while being robust to various domain differences in terms of both environment appearance and agent embodiment.", "one-sentence_summary": "Imitation of visual expert demonstrations robust to appearance and embodiment mismatch, working for high dimensional control problems.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "cetin|domainrobust_visual_imitation_learning_with_mutual_information_constraints", "supplementary_material": "/attachment/a2f45d7daa76f701540a30f84a144a328ff20c3a.zip", "pdf": "/pdf/ffeade3d551ddc81dea27db706160b3bc6510cec.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ncetin2021domainrobust,\ntitle={Domain-Robust Visual Imitation Learning with Mutual Information Constraints},\nauthor={Edoardo Cetin and Oya Celiktutan},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=QubpWYfdNry}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "QubpWYfdNry", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1735/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1735/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1735/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1735/Authors|ICLR.cc/2021/Conference/Paper1735/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1735/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923856314, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1735/-/Official_Comment"}}}, {"id": "-CO-T_GjEXI", "original": null, "number": 5, "cdate": 1605977924790, "ddate": null, "tcdate": 1605977924790, "tmdate": 1605979808711, "tddate": null, "forum": "QubpWYfdNry", "replyto": "9lA4fOmgn9P", "invitation": "ICLR.cc/2021/Conference/Paper1735/-/Official_Comment", "content": {"title": "Responses to AnonReviewer4", "comment": "**General comments:**\n\n1. I like the intuition provided in Appendix A and think some of it should be included in the main text.\n\nIn the current revision, we extended Section 4.2 in the main text to include an explanation of the downsides of the constraint proposed by Stadie et al. We also expanded Appendix A to give further details of the implications of their practical implementation. \n\n2. a version of Appendix D1 would be valuable to include, and perhaps also another visualization demonstrating that other baselines fail to encode goal completion information.\n\nIn the current revision, we expanded Appendix D.1 with an additional visualization, showing the observation couplings produced by DisentanGAIL with DCL. Due to space constraints, we were not able to include a version of this section in the main text and we opted to keep it as part of the Appendix.\n\n**Questions:**\n\n1. What is the difference between DisentanGAIL with DCL and TPIL?\n\nWhile DisentanGAIL with DCL shares most of the implementation details with DisentanGAIL (described in Appendix B), we obtained the TPIL results by adapting the authors' original code. The main distinctive features of DisentanGAIL with DCL, which likely had a significant effect on performance are:\n- Off-policy maximum-entropy actor - enabling more sample-efficient learning than TRPO and incentivizing the agent to explore even when an uninformative learning signal is provided. \n- Discriminator architecture and processing of the pre-processor's stochastic output (as described in Appendix B) - providing some implicit regularization and permitting to easily diminish the information contained in any of the independent dimensions of the Gaussian representations. \n- Invariant discriminator regularization - providing further regularization for the expressivity of the invariant discriminator.\n\n2. Without any regularization, I\u2019m surprised DisentanGAIL learns anything at all. Wouldn\u2019t the discriminator then be able to tell the two domains apart (especially if one of the differences is color) and fail to provide a meaningful learning signal to the agent?\n\nEven without any regularization, DisentanGAIL still appears to provide some meaningful learning signal to the actor, outperforming TPIL on many problems. We believe that this is possible thanks to the same components allowing DisentanGAIL with DCL to substantially outperform TPIL. For example, the Gaussian representation of the pre-processor's output should always provide some soft regularization. When this is combined with the inherent stochasticity of the training process, it can lead to the invariant discriminator to make use of some useful goal-completion information while providing rewards to the agent's transitions. However, in most examined problems, these components alone are not enough to effectively reproduce the demonstrated task, as highlighted by the significant performance gap with the other DisentanGAIL ablations. \n\n3. If I understand Figure 3 correctly, the orange line is without any regularization (lower bound) and the green is learning without domain differences (upper bound). How come DisentanGAIL can eventually outperform the green line in some of the experiments?\n\nThe environments tested in the high dimensional experiments pose challenges even in the setting of imitation without domain differences. For the manipulation tasks, we observed that the proposed mutual information constraints (which the algorithm learning without domain differences does not apply) are actually beneficial regardless of the presence of domain differences. We hypothesize that this is due to the general regularizing effect that they have on the discriminator, counteracting the manipulation environments' inherent stochasticity. Particularly, the expert data constraint has the side effect of limiting information flow in the discriminator (similarly to Peng et al., 2018), while the prior data constraint can help to identify which information is irrelevant for achieving the demonstrated objective (similarly to Zolna et al., 2019). We hypothesize that these factors allow the full DisentanGAIL algorithm to slightly exceed the performance of the source environment baseline in the 7DOF-Striker realm.\n\n4. What\u2019s the gap between DisentanGAIL and the expert policy? Does the expert always achieve a reward of 1?\n\nWe normalize the reported performance such that 1 represents the episode returns achieved by the expert agent. DisentanGAIL always recovers a comparable performance to the expert policy, with a larger gap in the high dimensional locomotion environments (as discussed in Section 6).\n\n**Other comments**\n\n1. Please clean up all Figures and Tables. The text is too small and impossible to read without zooming in very close.\n\nIn the current revision, we have increased the size of the figures and tables, together with their corresponding text.\n\n2. \u201cLearning\u201d typo in last line of page 2.\n\nFixed.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1735/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1735/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Domain-Robust Visual Imitation Learning with Mutual Information Constraints", "authorids": ["~Edoardo_Cetin1", "~Oya_Celiktutan2"], "authors": ["Edoardo Cetin", "Oya Celiktutan"], "keywords": ["Imitation Learning", "Reinforcement Learning", "Observational Imitation", "Third-Person Imitation", "Mutual Information", "Domain Adaption", "Machine Learning"], "abstract": "Human beings are able to understand objectives and learn by simply observing others perform a task. Imitation learning methods aim to replicate such capabilities, however, they generally depend on access to a full set of optimal states and actions taken with the agent's actuators and from the agent's point of view. In this paper, we introduce a new algorithm - called Disentangling Generative Adversarial Imitation Learning (DisentanGAIL) - with the purpose of bypassing such constraints. Our algorithm enables autonomous agents to learn directly from high dimensional observations of an expert performing a task, by making use of adversarial learning with a latent representation inside the discriminator network. Such latent representation is regularized through mutual information constraints to incentivize learning only features that encode information about the completion levels of the task being demonstrated. This allows to obtain a shared feature space to successfully perform imitation while disregarding the differences between the expert's and the agent's domains. Empirically, our algorithm is able to efficiently imitate in a diverse range of control problems including balancing, manipulation and locomotive tasks, while being robust to various domain differences in terms of both environment appearance and agent embodiment.", "one-sentence_summary": "Imitation of visual expert demonstrations robust to appearance and embodiment mismatch, working for high dimensional control problems.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "cetin|domainrobust_visual_imitation_learning_with_mutual_information_constraints", "supplementary_material": "/attachment/a2f45d7daa76f701540a30f84a144a328ff20c3a.zip", "pdf": "/pdf/ffeade3d551ddc81dea27db706160b3bc6510cec.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ncetin2021domainrobust,\ntitle={Domain-Robust Visual Imitation Learning with Mutual Information Constraints},\nauthor={Edoardo Cetin and Oya Celiktutan},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=QubpWYfdNry}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "QubpWYfdNry", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1735/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1735/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1735/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1735/Authors|ICLR.cc/2021/Conference/Paper1735/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1735/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923856314, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1735/-/Official_Comment"}}}, {"id": "X2ommJPobwF", "original": null, "number": 13, "cdate": 1605979706781, "ddate": null, "tcdate": 1605979706781, "tmdate": 1605979706781, "tddate": null, "forum": "QubpWYfdNry", "replyto": "QubpWYfdNry", "invitation": "ICLR.cc/2021/Conference/Paper1735/-/Official_Comment", "content": {"title": "Update overview", "comment": "We would like to truly thank all the reviewers for their constructive feedback and impactful suggestions. We have incorporated many of these suggestions into our paper, helping us considerably improve the completeness and readability of this work. We have posted responses to the individual reviews, addressing each question and concern in detail. Below, we provide an overview of the major changes we have made in the current revision of our paper:\n\n\u2022\tWe added an introductory paragraph to Section 4 which gives a high-level overview of DisentanGAIL and defines the notion of prior data.\n\n\u2022\tWe extended Section 4.2  by including some of the intuition from Appendix A, to provide a comparison of our methodology with the constraint proposed by Stadie et al. and its implementation.\n\n\u2022\tWe specified the criterion used to characterize the Hopper, Half-Cheetah, 7DOF-Pusher and 7DOF-Striker environments as 'high dimensional' in Section 6.\n\n\u2022\tWe collected the results and added the relative performance curves for all the remaining baselines in the high dimensional observational imitation problems. We also added Table 2 in Section 6 summarizing these results.\n\n\u2022\tWe extended Appendix A by further discussing practical implications of the domain confusion loss proposed by Stadie et al.\n\n\u2022\tWe clarified how and when prior data is used by the different ablations in Appendix B.1.\n\n\u2022\tWe expanded section D.1 of the Appendix to report additional results showing the latent representations couplings produced by DisentanGAIL with domain confusion loss.\n\n\u2022\tWe added Section D.4 in the Appendix, where we evaluate DisentanGAIL on two alternative target environments in the Hopper and 7DOF-Pusher realms with very distinct backgrounds from the relative source environments.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1735/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1735/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Domain-Robust Visual Imitation Learning with Mutual Information Constraints", "authorids": ["~Edoardo_Cetin1", "~Oya_Celiktutan2"], "authors": ["Edoardo Cetin", "Oya Celiktutan"], "keywords": ["Imitation Learning", "Reinforcement Learning", "Observational Imitation", "Third-Person Imitation", "Mutual Information", "Domain Adaption", "Machine Learning"], "abstract": "Human beings are able to understand objectives and learn by simply observing others perform a task. Imitation learning methods aim to replicate such capabilities, however, they generally depend on access to a full set of optimal states and actions taken with the agent's actuators and from the agent's point of view. In this paper, we introduce a new algorithm - called Disentangling Generative Adversarial Imitation Learning (DisentanGAIL) - with the purpose of bypassing such constraints. Our algorithm enables autonomous agents to learn directly from high dimensional observations of an expert performing a task, by making use of adversarial learning with a latent representation inside the discriminator network. Such latent representation is regularized through mutual information constraints to incentivize learning only features that encode information about the completion levels of the task being demonstrated. This allows to obtain a shared feature space to successfully perform imitation while disregarding the differences between the expert's and the agent's domains. Empirically, our algorithm is able to efficiently imitate in a diverse range of control problems including balancing, manipulation and locomotive tasks, while being robust to various domain differences in terms of both environment appearance and agent embodiment.", "one-sentence_summary": "Imitation of visual expert demonstrations robust to appearance and embodiment mismatch, working for high dimensional control problems.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "cetin|domainrobust_visual_imitation_learning_with_mutual_information_constraints", "supplementary_material": "/attachment/a2f45d7daa76f701540a30f84a144a328ff20c3a.zip", "pdf": "/pdf/ffeade3d551ddc81dea27db706160b3bc6510cec.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ncetin2021domainrobust,\ntitle={Domain-Robust Visual Imitation Learning with Mutual Information Constraints},\nauthor={Edoardo Cetin and Oya Celiktutan},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=QubpWYfdNry}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "QubpWYfdNry", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1735/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1735/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1735/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1735/Authors|ICLR.cc/2021/Conference/Paper1735/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1735/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923856314, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1735/-/Official_Comment"}}}, {"id": "y2Hp5f7jMOO", "original": null, "number": 8, "cdate": 1605978195291, "ddate": null, "tcdate": 1605978195291, "tmdate": 1605978195291, "tddate": null, "forum": "QubpWYfdNry", "replyto": "lyA_XbD3HhE", "invitation": "ICLR.cc/2021/Conference/Paper1735/-/Official_Comment", "content": {"title": "Responses to AnonReviewer2 - part 2", "comment": "3. Furthermore, since the approach seems to be a bit incremental, it would be nice to have the theoretical analysis that could justify the incremental changes and guarantee the convergence to the optimal policy (e.g. attaining similar results in [3]). Unfortunately, this is missing in the paper.\n\nWe argue that our work makes already a significant contribution towards solving the problem of observational imitation, as highlighted in our responses above. In their derivations, Kim et al. consider a limited class of alignable MDPs and only show that a mapping between optimal policies in this scenario should exist. On the other hand, imitation across non-alignable POMDPs is a majorly ill-posed problem and making rigorous claims would require major assumptions about the tasks and the expert demonstrations. While we understand the importance of such analysis, we leave this to future works focusing on more restricted classes of observational imitation problems, in which such assumptions can apply.\n\n4. The challenging, high-dimensional environments used in the paper are all in locomotion tasks in Mujoco. It would be nice to see more realistic environments such as robotic manipulation tasks like ROBEL and Adroit etc., which would make the domain adaptation more appealing.\n\nOur high dimensional environments do not solely involve locomotion tasks but also two robotic manipulation tasks, namely within the 7DOF-Pusher and 7DOF-Striker environment realms. Particularly, these manipulation tasks examine observational imitation from a human-like agent to a robot-like agent based on the PR2, with major domain differences in terms of both agent appearance and embodiment (as described in Appendix C). We have released our environment suite as part of our submission, which already provides a more challenging and diverse set of observational imitation problems than what was previously attempted by prior related works. However, we agree with the reviewer regarding the importance of even more realistic and challenging problems and we hope that future works will extend our efforts and add additional realistic robotics manipulation problems to this suite.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1735/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1735/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Domain-Robust Visual Imitation Learning with Mutual Information Constraints", "authorids": ["~Edoardo_Cetin1", "~Oya_Celiktutan2"], "authors": ["Edoardo Cetin", "Oya Celiktutan"], "keywords": ["Imitation Learning", "Reinforcement Learning", "Observational Imitation", "Third-Person Imitation", "Mutual Information", "Domain Adaption", "Machine Learning"], "abstract": "Human beings are able to understand objectives and learn by simply observing others perform a task. Imitation learning methods aim to replicate such capabilities, however, they generally depend on access to a full set of optimal states and actions taken with the agent's actuators and from the agent's point of view. In this paper, we introduce a new algorithm - called Disentangling Generative Adversarial Imitation Learning (DisentanGAIL) - with the purpose of bypassing such constraints. Our algorithm enables autonomous agents to learn directly from high dimensional observations of an expert performing a task, by making use of adversarial learning with a latent representation inside the discriminator network. Such latent representation is regularized through mutual information constraints to incentivize learning only features that encode information about the completion levels of the task being demonstrated. This allows to obtain a shared feature space to successfully perform imitation while disregarding the differences between the expert's and the agent's domains. Empirically, our algorithm is able to efficiently imitate in a diverse range of control problems including balancing, manipulation and locomotive tasks, while being robust to various domain differences in terms of both environment appearance and agent embodiment.", "one-sentence_summary": "Imitation of visual expert demonstrations robust to appearance and embodiment mismatch, working for high dimensional control problems.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "cetin|domainrobust_visual_imitation_learning_with_mutual_information_constraints", "supplementary_material": "/attachment/a2f45d7daa76f701540a30f84a144a328ff20c3a.zip", "pdf": "/pdf/ffeade3d551ddc81dea27db706160b3bc6510cec.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ncetin2021domainrobust,\ntitle={Domain-Robust Visual Imitation Learning with Mutual Information Constraints},\nauthor={Edoardo Cetin and Oya Celiktutan},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=QubpWYfdNry}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "QubpWYfdNry", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1735/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1735/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1735/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1735/Authors|ICLR.cc/2021/Conference/Paper1735/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1735/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923856314, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1735/-/Official_Comment"}}}, {"id": "Z06WNs9n-yb", "original": null, "number": 6, "cdate": 1605977946315, "ddate": null, "tcdate": 1605977946315, "tmdate": 1605977946315, "tddate": null, "forum": "QubpWYfdNry", "replyto": "-CO-T_GjEXI", "invitation": "ICLR.cc/2021/Conference/Paper1735/-/Official_Comment", "content": {"title": "References", "comment": "Bradly C Stadie, Pieter Abbeel, and Ilya Sutskever. Third-person imitation learning. arXiv preprint\narXiv:1703.01703, 2017.\n\nXue Bin Peng, Angjoo Kanazawa, Sam Toyer, Pieter Abbeel, and Sergey Levine. Variational discriminator\nbottleneck: Improving imitation learning, inverse rl, and gans by constraining information\nflow. arXiv preprint arXiv:1810.00821, 2018.\n\nKonrad Zolna, Scott Reed, Alexander Novikov, Sergio Gomez Colmenarej, David Budden, Serkan\nCabi, Misha Denil, Nando de Freitas, and Ziyu Wang. Task-relevant adversarial imitation learning.\narXiv preprint arXiv:1910.01077, 2019."}, "signatures": ["ICLR.cc/2021/Conference/Paper1735/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1735/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Domain-Robust Visual Imitation Learning with Mutual Information Constraints", "authorids": ["~Edoardo_Cetin1", "~Oya_Celiktutan2"], "authors": ["Edoardo Cetin", "Oya Celiktutan"], "keywords": ["Imitation Learning", "Reinforcement Learning", "Observational Imitation", "Third-Person Imitation", "Mutual Information", "Domain Adaption", "Machine Learning"], "abstract": "Human beings are able to understand objectives and learn by simply observing others perform a task. Imitation learning methods aim to replicate such capabilities, however, they generally depend on access to a full set of optimal states and actions taken with the agent's actuators and from the agent's point of view. In this paper, we introduce a new algorithm - called Disentangling Generative Adversarial Imitation Learning (DisentanGAIL) - with the purpose of bypassing such constraints. Our algorithm enables autonomous agents to learn directly from high dimensional observations of an expert performing a task, by making use of adversarial learning with a latent representation inside the discriminator network. Such latent representation is regularized through mutual information constraints to incentivize learning only features that encode information about the completion levels of the task being demonstrated. This allows to obtain a shared feature space to successfully perform imitation while disregarding the differences between the expert's and the agent's domains. Empirically, our algorithm is able to efficiently imitate in a diverse range of control problems including balancing, manipulation and locomotive tasks, while being robust to various domain differences in terms of both environment appearance and agent embodiment.", "one-sentence_summary": "Imitation of visual expert demonstrations robust to appearance and embodiment mismatch, working for high dimensional control problems.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "cetin|domainrobust_visual_imitation_learning_with_mutual_information_constraints", "supplementary_material": "/attachment/a2f45d7daa76f701540a30f84a144a328ff20c3a.zip", "pdf": "/pdf/ffeade3d551ddc81dea27db706160b3bc6510cec.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ncetin2021domainrobust,\ntitle={Domain-Robust Visual Imitation Learning with Mutual Information Constraints},\nauthor={Edoardo Cetin and Oya Celiktutan},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=QubpWYfdNry}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "QubpWYfdNry", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1735/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1735/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1735/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1735/Authors|ICLR.cc/2021/Conference/Paper1735/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1735/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923856314, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1735/-/Official_Comment"}}}], "count": 15}