{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396655024, "tcdate": 1486396655024, "number": 1, "id": "SyPxafLOx", "invitation": "ICLR.cc/2017/conference/-/paper521/acceptance", "forum": "Bkul3t9ee", "replyto": "Bkul3t9ee", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "Quality, Clarity:\n \n The work is well motivated and clearly written -- no issues there.\n \n Originality, Significance:\n \n The idea is simple and well motivated, i.e., the learning of reward functions based on feature selection from identified subtasks in videos.\n \n pros:\n - the problem is difficult and relevant: good solutions would have impact\n \n cons:\n - the benefit with respect to other baselines for various choices, although the latest version does contain updated baselines\n - the influence of the initial controller on the results\n - the work may gain better appreciation at a robotics conference\n \n I am very much on the fence for this paper.\n It straddles a number of recent advances in video segmentation, robotics, and RL, which makes the specific technical contributions harder to identify. I do think that a robotics conference would be appreciative of the work, but better learning of reward functions is surely a bottleneck and therefore of interest to ICLR.\n Given the lukewarm support for this paper by reviewers, the PCs decided not to accept the paper, but invite the authors to present it in the workshop track.", "decision": "Invite to Workshop Track"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Perceptual Rewards for Imitation Learning", "abstract": "Reward function design and exploration time are arguably the biggest obstacles to the deployment of reinforcement learning (RL) agents in the real world. In many real-world tasks, designing a suitable reward function takes considerable manual engineering and often requires additional and potentially visible sensors to be installed just to measure whether the task has been executed successfully. Furthermore, many interesting tasks consist of multiple steps that must be executed in sequence. Even when the final outcome can be measured, it does not necessarily provide useful feedback on these implicit intermediate steps or sub-goals.\nTo address these issues, we propose leveraging the abstraction power of intermediate visual representations learned by deep models to quickly infer perceptual reward functions from small numbers of demonstrations. We present a method that is able to identify the key intermediate steps of a task from only a handful of demonstration sequences, and automatically identify the most discriminative features for identifying these steps. This method makes use of the features in a pre-trained deep model, but does not require any explicit sub-goal supervision. The resulting reward functions, which are dense and smooth, can then be used by an RL agent to learn to perform the task in real-world settings. To evaluate the learned reward functions, we present qualitative results on two real-world tasks and a quantitative evaluation against a human-designed reward function. We also demonstrate that our method can be used to learn a complex real-world door opening skill using a real robot, even when the demonstration used for reward learning is provided by a human using their own hand. To our knowledge, these are the first results showing that complex robotic manipulation skills can be learned directly and without supervised labels from a video of a human performing the task.\n", "pdf": "/pdf/939de21e00049bd95fa78820f7e7e99cc37df62b.pdf", "TL;DR": "Real robots learn new tasks from observing a few human demonstrations.", "paperhash": "sermanet|unsupervised_perceptual_rewards_for_imitation_learning", "keywords": ["Computer vision", "Deep learning", "Unsupervised Learning", "Reinforcement Learning", "Transfer Learning"], "conflicts": ["google.com", "berkeley.edu", "washington.edu"], "authors": ["Pierre Sermanet", "Kelvin Xu", "Sergey Levine"], "authorids": ["sermanet@google.com", "kelvinxx@google.com", "slevine@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396655538, "id": "ICLR.cc/2017/conference/-/paper521/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "Bkul3t9ee", "replyto": "Bkul3t9ee", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396655538}}}, {"tddate": null, "tmdate": 1484945018169, "tcdate": 1484945018169, "number": 2, "id": "SJfYUxlwx", "invitation": "ICLR.cc/2017/conference/-/paper521/official/comment", "forum": "Bkul3t9ee", "replyto": "r1E16EPLl", "signatures": ["ICLR.cc/2017/conference/paper521/AnonReviewer5"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper521/AnonReviewer5"], "content": {"title": "Response to rebuttal", "comment": "Thank you for carrying out additional experiments to test alternative baselines, they add to the overall quality of the paper.\nUnfortunately, overall I still feel that the contribution of the paper at hand leaves it at the border of acceptance. On the one hand it serves as a well written account demonstrating that reward functions can be learned even from few demonstrations. On the other hand, the \"representation learning\" part of the paper is rather small and it \"only\" provides a two data-points (e.g. two instances in which the method worked) that are fairly similar to each other. Ultimately I think a decision by the area chair is needed. In my opinion the paper could well be accepted to the conference but it might be better suited for a robotics conference. I personally would also welcome an open discussion about this but it seems the schedule for acceptance is rather restricted.\n\nRegarding the rebuttal: \n\n1) Thanks this was useful for providing an intuition and justifies the taken approach. I am still wondering how well the simple distance based weighting would do / whether one cannot design an even simpler method.\n2) I of course understand this and my comment was not meant as a criticism of your choice of methods. Rather I worry that your choice of methods and the restricted setting (two experiments with the same robot arm) might induce a bias in the evaluation.\n3) Thanks, as I wrote above I appreciate this and it does address my concern.\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Perceptual Rewards for Imitation Learning", "abstract": "Reward function design and exploration time are arguably the biggest obstacles to the deployment of reinforcement learning (RL) agents in the real world. In many real-world tasks, designing a suitable reward function takes considerable manual engineering and often requires additional and potentially visible sensors to be installed just to measure whether the task has been executed successfully. Furthermore, many interesting tasks consist of multiple steps that must be executed in sequence. Even when the final outcome can be measured, it does not necessarily provide useful feedback on these implicit intermediate steps or sub-goals.\nTo address these issues, we propose leveraging the abstraction power of intermediate visual representations learned by deep models to quickly infer perceptual reward functions from small numbers of demonstrations. We present a method that is able to identify the key intermediate steps of a task from only a handful of demonstration sequences, and automatically identify the most discriminative features for identifying these steps. This method makes use of the features in a pre-trained deep model, but does not require any explicit sub-goal supervision. The resulting reward functions, which are dense and smooth, can then be used by an RL agent to learn to perform the task in real-world settings. To evaluate the learned reward functions, we present qualitative results on two real-world tasks and a quantitative evaluation against a human-designed reward function. We also demonstrate that our method can be used to learn a complex real-world door opening skill using a real robot, even when the demonstration used for reward learning is provided by a human using their own hand. To our knowledge, these are the first results showing that complex robotic manipulation skills can be learned directly and without supervised labels from a video of a human performing the task.\n", "pdf": "/pdf/939de21e00049bd95fa78820f7e7e99cc37df62b.pdf", "TL;DR": "Real robots learn new tasks from observing a few human demonstrations.", "paperhash": "sermanet|unsupervised_perceptual_rewards_for_imitation_learning", "keywords": ["Computer vision", "Deep learning", "Unsupervised Learning", "Reinforcement Learning", "Transfer Learning"], "conflicts": ["google.com", "berkeley.edu", "washington.edu"], "authors": ["Pierre Sermanet", "Kelvin Xu", "Sergey Levine"], "authorids": ["sermanet@google.com", "kelvinxx@google.com", "slevine@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287538588, "id": "ICLR.cc/2017/conference/-/paper521/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "Bkul3t9ee", "writers": {"values-regex": "ICLR.cc/2017/conference/paper521/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper521/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper521/reviewers", "ICLR.cc/2017/conference/paper521/areachairs"], "cdate": 1485287538588}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1484374902804, "tcdate": 1478298607629, "number": 521, "id": "Bkul3t9ee", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "Bkul3t9ee", "signatures": ["~Pierre_Sermanet1"], "readers": ["everyone"], "content": {"title": "Unsupervised Perceptual Rewards for Imitation Learning", "abstract": "Reward function design and exploration time are arguably the biggest obstacles to the deployment of reinforcement learning (RL) agents in the real world. In many real-world tasks, designing a suitable reward function takes considerable manual engineering and often requires additional and potentially visible sensors to be installed just to measure whether the task has been executed successfully. Furthermore, many interesting tasks consist of multiple steps that must be executed in sequence. Even when the final outcome can be measured, it does not necessarily provide useful feedback on these implicit intermediate steps or sub-goals.\nTo address these issues, we propose leveraging the abstraction power of intermediate visual representations learned by deep models to quickly infer perceptual reward functions from small numbers of demonstrations. We present a method that is able to identify the key intermediate steps of a task from only a handful of demonstration sequences, and automatically identify the most discriminative features for identifying these steps. This method makes use of the features in a pre-trained deep model, but does not require any explicit sub-goal supervision. The resulting reward functions, which are dense and smooth, can then be used by an RL agent to learn to perform the task in real-world settings. To evaluate the learned reward functions, we present qualitative results on two real-world tasks and a quantitative evaluation against a human-designed reward function. We also demonstrate that our method can be used to learn a complex real-world door opening skill using a real robot, even when the demonstration used for reward learning is provided by a human using their own hand. To our knowledge, these are the first results showing that complex robotic manipulation skills can be learned directly and without supervised labels from a video of a human performing the task.\n", "pdf": "/pdf/939de21e00049bd95fa78820f7e7e99cc37df62b.pdf", "TL;DR": "Real robots learn new tasks from observing a few human demonstrations.", "paperhash": "sermanet|unsupervised_perceptual_rewards_for_imitation_learning", "keywords": ["Computer vision", "Deep learning", "Unsupervised Learning", "Reinforcement Learning", "Transfer Learning"], "conflicts": ["google.com", "berkeley.edu", "washington.edu"], "authors": ["Pierre Sermanet", "Kelvin Xu", "Sergey Levine"], "authorids": ["sermanet@google.com", "kelvinxx@google.com", "slevine@google.com"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 13, "writable": false, "overwriting": ["Byf3mmNFl"], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "tmdate": 1484373212084, "tcdate": 1484373212084, "number": 5, "id": "r1E16EPLl", "invitation": "ICLR.cc/2017/conference/-/paper521/public/comment", "forum": "Bkul3t9ee", "replyto": "SkhCBlGNl", "signatures": ["~Pierre_Sermanet1"], "readers": ["everyone"], "writers": ["~Pierre_Sermanet1"], "content": {"title": "Rebuttal", "comment": "Thank you for your review and suggestions! We incorporated your comments in the paper and additionally address them here:\n\n1) We added an analysis of the accuracy of feature selection by varying the number of features (n) in figure 4. The rightmost part of the graph is the equivalent of the suggested experiment to use all features, this collapses to 0% accuracy when n > 8192, and is best in the range [32, 64].\n\n2) For fairness, the kinesthetic demonstration was necessary for comparing to the baseline system developed by Chebotar et al (2016). We would note that the real world robotics tasks that we evaluated our method on are very difficult and no known prior RL method has demonstrated door opening of this sort from scratch without such a demonstration.\n\n3) We have added a learned linear classifier on top of the fixed pre-trained features and believe this should help address your concerns by providing a model for comparison (details of this analysis can be found in sections 2.3 and 3.1.2)."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Perceptual Rewards for Imitation Learning", "abstract": "Reward function design and exploration time are arguably the biggest obstacles to the deployment of reinforcement learning (RL) agents in the real world. In many real-world tasks, designing a suitable reward function takes considerable manual engineering and often requires additional and potentially visible sensors to be installed just to measure whether the task has been executed successfully. Furthermore, many interesting tasks consist of multiple steps that must be executed in sequence. Even when the final outcome can be measured, it does not necessarily provide useful feedback on these implicit intermediate steps or sub-goals.\nTo address these issues, we propose leveraging the abstraction power of intermediate visual representations learned by deep models to quickly infer perceptual reward functions from small numbers of demonstrations. We present a method that is able to identify the key intermediate steps of a task from only a handful of demonstration sequences, and automatically identify the most discriminative features for identifying these steps. This method makes use of the features in a pre-trained deep model, but does not require any explicit sub-goal supervision. The resulting reward functions, which are dense and smooth, can then be used by an RL agent to learn to perform the task in real-world settings. To evaluate the learned reward functions, we present qualitative results on two real-world tasks and a quantitative evaluation against a human-designed reward function. We also demonstrate that our method can be used to learn a complex real-world door opening skill using a real robot, even when the demonstration used for reward learning is provided by a human using their own hand. To our knowledge, these are the first results showing that complex robotic manipulation skills can be learned directly and without supervised labels from a video of a human performing the task.\n", "pdf": "/pdf/939de21e00049bd95fa78820f7e7e99cc37df62b.pdf", "TL;DR": "Real robots learn new tasks from observing a few human demonstrations.", "paperhash": "sermanet|unsupervised_perceptual_rewards_for_imitation_learning", "keywords": ["Computer vision", "Deep learning", "Unsupervised Learning", "Reinforcement Learning", "Transfer Learning"], "conflicts": ["google.com", "berkeley.edu", "washington.edu"], "authors": ["Pierre Sermanet", "Kelvin Xu", "Sergey Levine"], "authorids": ["sermanet@google.com", "kelvinxx@google.com", "slevine@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287538717, "id": "ICLR.cc/2017/conference/-/paper521/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Bkul3t9ee", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper521/reviewers", "ICLR.cc/2017/conference/paper521/areachairs"], "cdate": 1485287538717}}}, {"tddate": null, "tmdate": 1484373094739, "tcdate": 1484373094739, "number": 4, "id": "rkkuhNPUg", "invitation": "ICLR.cc/2017/conference/-/paper521/public/comment", "forum": "Bkul3t9ee", "replyto": "BJ05lbz4e", "signatures": ["~Pierre_Sermanet1"], "readers": ["everyone"], "writers": ["~Pierre_Sermanet1"], "content": {"title": "Rebuttal", "comment": "Thank you your review and suggestions! We agree that adding more baselines for doing feature extraction would expand the experimental contribution of the paper. We have added a learned classifier baseline and a more thorough analysis comparing these alternatives in sections 2.3 and 3.1.2 which we hope helps address your concerns."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Perceptual Rewards for Imitation Learning", "abstract": "Reward function design and exploration time are arguably the biggest obstacles to the deployment of reinforcement learning (RL) agents in the real world. In many real-world tasks, designing a suitable reward function takes considerable manual engineering and often requires additional and potentially visible sensors to be installed just to measure whether the task has been executed successfully. Furthermore, many interesting tasks consist of multiple steps that must be executed in sequence. Even when the final outcome can be measured, it does not necessarily provide useful feedback on these implicit intermediate steps or sub-goals.\nTo address these issues, we propose leveraging the abstraction power of intermediate visual representations learned by deep models to quickly infer perceptual reward functions from small numbers of demonstrations. We present a method that is able to identify the key intermediate steps of a task from only a handful of demonstration sequences, and automatically identify the most discriminative features for identifying these steps. This method makes use of the features in a pre-trained deep model, but does not require any explicit sub-goal supervision. The resulting reward functions, which are dense and smooth, can then be used by an RL agent to learn to perform the task in real-world settings. To evaluate the learned reward functions, we present qualitative results on two real-world tasks and a quantitative evaluation against a human-designed reward function. We also demonstrate that our method can be used to learn a complex real-world door opening skill using a real robot, even when the demonstration used for reward learning is provided by a human using their own hand. To our knowledge, these are the first results showing that complex robotic manipulation skills can be learned directly and without supervised labels from a video of a human performing the task.\n", "pdf": "/pdf/939de21e00049bd95fa78820f7e7e99cc37df62b.pdf", "TL;DR": "Real robots learn new tasks from observing a few human demonstrations.", "paperhash": "sermanet|unsupervised_perceptual_rewards_for_imitation_learning", "keywords": ["Computer vision", "Deep learning", "Unsupervised Learning", "Reinforcement Learning", "Transfer Learning"], "conflicts": ["google.com", "berkeley.edu", "washington.edu"], "authors": ["Pierre Sermanet", "Kelvin Xu", "Sergey Levine"], "authorids": ["sermanet@google.com", "kelvinxx@google.com", "slevine@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287538717, "id": "ICLR.cc/2017/conference/-/paper521/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Bkul3t9ee", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper521/reviewers", "ICLR.cc/2017/conference/paper521/areachairs"], "cdate": 1485287538717}}}, {"tddate": null, "tmdate": 1484372848716, "tcdate": 1484372848716, "number": 3, "id": "S1K_iNPLl", "invitation": "ICLR.cc/2017/conference/-/paper521/public/comment", "forum": "Bkul3t9ee", "replyto": "HJ-Dj_fVe", "signatures": ["~Pierre_Sermanet1"], "readers": ["everyone"], "writers": ["~Pierre_Sermanet1"], "content": {"title": "Rebuttal", "comment": "First of all, thank you for your constructive comments and your review. We have made an effort to incorporate your remarks in the paper and also address them here:\n\n1. and 3. These references are useful and we have added them to the paper. We would emphasize that the main contribution of the paper is a step towards learning reward functions from videos that allow a real-world robot to perform a difficult manipulation task. We started with very simple components and aim to use more sophisticated ones like the ones you suggested in future work. But because the main contribution of our paper was demonstrating that we could learn reward functions from videos, and because performing experiments on real robots is generally quite labor intensive, we feel implementing comparisons to multiple videos segmentation baselines would be out of scope here. \n\n2. While our current video segmentation is simple and does not make use of commonalities across demonstrations, we absolutely agree that using a method which leverages this would yield much more robust step-discovery in less constrained setups. In fact, the feature selection experiments were in part designed to test the existence of small but discriminative subsets of features in our pre-trained features, in order to reduce the complexity of the problem. Based on the suggestions of R5 and your own, we added a graph which shows empirically how classification performance rapidly deteriorate when all the features are used (see fig.4, Section 3.1.2). \n\nIn general, with millions of features to compare, the unsupervised search for common patterns across videos leads to a combinatorial explosion. But if we find a small set of good candidates, then the problem becomes tractable and we can use an optimization method for video alignment such as the one described in the video co-localization work of Tang et al. ECCV14. Our experiments are a first step towards a more sophisticated and tractable unsupervised alignment. We discuss this at length in Sections 2.3 and 3.1.2.\n\n4. Thanks for the suggestion! We have added a linear classifier for step classification and compared it with feature selection. Our initial intuition was that using all the pre-trained features for classification would not be effective, which we confirmed empirically in Section 3.1.2 (see fig.4). It turns out that our assumption that a linear classifier would simply overfit given the large number of trainable parameters and the low-data regime was untrue (thanks!). The classifier performed surprisingly well. However the hypothesis that a small subset of very discriminative features could be also be used (our experiments show it performs comparably and has lower variance) is true and could potentially be used in future video segmentation methods which scale poorly to large dimensions as discussed above. \n\nWe would point out however, that under the modeling assumptions which we discuss in the inverse RL interpretation (see Section 2.1), our naive Bayes classifier should use all of the features. Since this is highly ineffective as we show, this motivates using a feature selection criteria (or as you suggest a learned classifier) as a way to rectify the issue. Both methods perform similarly, with the classifier being slightly better.\n\nLastly, we would like to emphasize that both of the feature selection/classifier build upon the unsupervised video segmentation and as such is one part of our method. Therefore, using a classifier should be viewed overall as a variant of our method, rather than an pre-existing baseline approach to learning rewards from videos. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Perceptual Rewards for Imitation Learning", "abstract": "Reward function design and exploration time are arguably the biggest obstacles to the deployment of reinforcement learning (RL) agents in the real world. In many real-world tasks, designing a suitable reward function takes considerable manual engineering and often requires additional and potentially visible sensors to be installed just to measure whether the task has been executed successfully. Furthermore, many interesting tasks consist of multiple steps that must be executed in sequence. Even when the final outcome can be measured, it does not necessarily provide useful feedback on these implicit intermediate steps or sub-goals.\nTo address these issues, we propose leveraging the abstraction power of intermediate visual representations learned by deep models to quickly infer perceptual reward functions from small numbers of demonstrations. We present a method that is able to identify the key intermediate steps of a task from only a handful of demonstration sequences, and automatically identify the most discriminative features for identifying these steps. This method makes use of the features in a pre-trained deep model, but does not require any explicit sub-goal supervision. The resulting reward functions, which are dense and smooth, can then be used by an RL agent to learn to perform the task in real-world settings. To evaluate the learned reward functions, we present qualitative results on two real-world tasks and a quantitative evaluation against a human-designed reward function. We also demonstrate that our method can be used to learn a complex real-world door opening skill using a real robot, even when the demonstration used for reward learning is provided by a human using their own hand. To our knowledge, these are the first results showing that complex robotic manipulation skills can be learned directly and without supervised labels from a video of a human performing the task.\n", "pdf": "/pdf/939de21e00049bd95fa78820f7e7e99cc37df62b.pdf", "TL;DR": "Real robots learn new tasks from observing a few human demonstrations.", "paperhash": "sermanet|unsupervised_perceptual_rewards_for_imitation_learning", "keywords": ["Computer vision", "Deep learning", "Unsupervised Learning", "Reinforcement Learning", "Transfer Learning"], "conflicts": ["google.com", "berkeley.edu", "washington.edu"], "authors": ["Pierre Sermanet", "Kelvin Xu", "Sergey Levine"], "authorids": ["sermanet@google.com", "kelvinxx@google.com", "slevine@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287538717, "id": "ICLR.cc/2017/conference/-/paper521/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Bkul3t9ee", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper521/reviewers", "ICLR.cc/2017/conference/paper521/areachairs"], "cdate": 1485287538717}}}, {"tddate": null, "tmdate": 1481964377011, "tcdate": 1481964377011, "number": 3, "id": "HJ-Dj_fVe", "invitation": "ICLR.cc/2017/conference/-/paper521/official/review", "forum": "Bkul3t9ee", "replyto": "Bkul3t9ee", "signatures": ["ICLR.cc/2017/conference/paper521/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper521/AnonReviewer3"], "content": {"title": "Simple well motivated approach, but requires better references and comparisons to existing methods", "rating": "4: Ok but not good enough - rejection", "review": "The paper tries to present a first step towards solving the difficult problem of \"learning from limited number of demonstrations\". The paper tries to present 3 contributions towards this effort:\n1. unsupervised segmentation of videos to identify intermediate steps in a process\n2. define reward function based on feature selection for each sub-task\n\nPros:\n+ The paper is a first attempt to solve a very challenging problem, where a robot is taught real-world tasks with very few visual demonstrations and without further retraining.\n+ The method is well motivated and tries to transfer the priors learned from object classification task (through deep network features) to address the problem of limited training examples.\n+ As demonstrated in Fig. 3, the reward functions could be more interpretable and correlate with transitions between subtasks.\n+ Breaking a video into subtasks helps a video demonstration-based method achieve comparable performance with a method which requires full instrumentation for complex real-world tasks like door opening.\n\nCons:\n1. Unsupervised video segmentation can serve as a good starting point to identify subtasks. However, there are multiple prior works in this domain which need to be referenced and compared with. Particularly, video shot detection and shot segmentation works try to identify abrupt change in video to break it into visually diverse shots. These methods could be easily augmented with CNN-features.\n(Note that there are multiple papers in this domain, eg. refer to survey in Yuan et al. Trans. on Circuits and Systems for video tech. 2007)\n\n2. The authors claim that they did not find it necessary to identify commonalities across demonstrations. This limits the scope of the problem drastically and requires the demonstrations to follow very specific set of constraints. Again, it is to be noted that there is past literature (video co-segmentation, eg. Tang et al. ECCV'14) which uses these commonalities to perform unsupervised video segmentation.\n\n3. The unsupervised temporal video segmentation approach in the paper is only compared to a very simple random baseline for a few sample videos. However, given the large amount of literature in this domain, it is difficult to judge the novelty and significance of the proposed approach from these experiments.\n\n4. The authors hypothesize that \"sparse independent features exists which can discriminate a wide range of unseen inputs\" and encode this intuition through the feature selection strategy. Again, the validity of the hypothesis is not experimentally well demonstrated. For instance, comparison to a simple linear classifier for subtasks would have been useful.\n\nOverall, the paper presents a simple approach based on the idea that recognizing sub-goals in an unsupervised fashion would help in learning from few visual demonstrations. This is well motivated as a first-step towards a difficult task. However, the methods and claims presented in the paper need to be analyzed and compared with better baselines.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Perceptual Rewards for Imitation Learning", "abstract": "Reward function design and exploration time are arguably the biggest obstacles to the deployment of reinforcement learning (RL) agents in the real world. In many real-world tasks, designing a suitable reward function takes considerable manual engineering and often requires additional and potentially visible sensors to be installed just to measure whether the task has been executed successfully. Furthermore, many interesting tasks consist of multiple steps that must be executed in sequence. Even when the final outcome can be measured, it does not necessarily provide useful feedback on these implicit intermediate steps or sub-goals.\nTo address these issues, we propose leveraging the abstraction power of intermediate visual representations learned by deep models to quickly infer perceptual reward functions from small numbers of demonstrations. We present a method that is able to identify the key intermediate steps of a task from only a handful of demonstration sequences, and automatically identify the most discriminative features for identifying these steps. This method makes use of the features in a pre-trained deep model, but does not require any explicit sub-goal supervision. The resulting reward functions, which are dense and smooth, can then be used by an RL agent to learn to perform the task in real-world settings. To evaluate the learned reward functions, we present qualitative results on two real-world tasks and a quantitative evaluation against a human-designed reward function. We also demonstrate that our method can be used to learn a complex real-world door opening skill using a real robot, even when the demonstration used for reward learning is provided by a human using their own hand. To our knowledge, these are the first results showing that complex robotic manipulation skills can be learned directly and without supervised labels from a video of a human performing the task.\n", "pdf": "/pdf/939de21e00049bd95fa78820f7e7e99cc37df62b.pdf", "TL;DR": "Real robots learn new tasks from observing a few human demonstrations.", "paperhash": "sermanet|unsupervised_perceptual_rewards_for_imitation_learning", "keywords": ["Computer vision", "Deep learning", "Unsupervised Learning", "Reinforcement Learning", "Transfer Learning"], "conflicts": ["google.com", "berkeley.edu", "washington.edu"], "authors": ["Pierre Sermanet", "Kelvin Xu", "Sergey Levine"], "authorids": ["sermanet@google.com", "kelvinxx@google.com", "slevine@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512555432, "id": "ICLR.cc/2017/conference/-/paper521/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper521/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper521/AnonReviewer5", "ICLR.cc/2017/conference/paper521/AnonReviewer4", "ICLR.cc/2017/conference/paper521/AnonReviewer3"], "reply": {"forum": "Bkul3t9ee", "replyto": "Bkul3t9ee", "writers": {"values-regex": "ICLR.cc/2017/conference/paper521/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper521/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512555432}}}, {"tddate": null, "tmdate": 1481933013225, "tcdate": 1481932950098, "number": 2, "id": "BJ05lbz4e", "invitation": "ICLR.cc/2017/conference/-/paper521/official/review", "forum": "Bkul3t9ee", "replyto": "Bkul3t9ee", "signatures": ["ICLR.cc/2017/conference/paper521/AnonReviewer4"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper521/AnonReviewer4"], "content": {"title": "Nice idea but more baselines are needed.", "rating": "6: Marginally above acceptance threshold", "review": "This paper proposes a novel method to learn vision feature as intermediate rewards to guide the robot training in the real world. Since there are only a few sequences of human demonstrations, the paper first segments the sequences into fragments so that the features are roughly invariant on the corresponding fragments across sequences, then clusters and finds most discriminative features on those fragments, and uses them as the reward function. The features are from pre-trained deep models.\n\nThe idea is simple and seems quite effective in picking the right reward functions. Fig. 6 is a good comparison (although it could be better with error bars). However, some baselines are not strong, in particular vision related baselines. For example, the random reward (\"simply outputs true or false\") in Tbl. 2 seems quite arbitrary and may not serve as a good baseline (but its performance is still not that bad, surprisingly.). A better baseline would be to use random/simpler feature extraction on the image, e.g., binning features and simply picking the most frequent ones, which might not be as discriminative as the proposed feature. I wonder whether a simpler vision-based approach would lead to a similarly performed reward function. If so, then these delicate steps (segment, etc) altogether.  ", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Perceptual Rewards for Imitation Learning", "abstract": "Reward function design and exploration time are arguably the biggest obstacles to the deployment of reinforcement learning (RL) agents in the real world. In many real-world tasks, designing a suitable reward function takes considerable manual engineering and often requires additional and potentially visible sensors to be installed just to measure whether the task has been executed successfully. Furthermore, many interesting tasks consist of multiple steps that must be executed in sequence. Even when the final outcome can be measured, it does not necessarily provide useful feedback on these implicit intermediate steps or sub-goals.\nTo address these issues, we propose leveraging the abstraction power of intermediate visual representations learned by deep models to quickly infer perceptual reward functions from small numbers of demonstrations. We present a method that is able to identify the key intermediate steps of a task from only a handful of demonstration sequences, and automatically identify the most discriminative features for identifying these steps. This method makes use of the features in a pre-trained deep model, but does not require any explicit sub-goal supervision. The resulting reward functions, which are dense and smooth, can then be used by an RL agent to learn to perform the task in real-world settings. To evaluate the learned reward functions, we present qualitative results on two real-world tasks and a quantitative evaluation against a human-designed reward function. We also demonstrate that our method can be used to learn a complex real-world door opening skill using a real robot, even when the demonstration used for reward learning is provided by a human using their own hand. To our knowledge, these are the first results showing that complex robotic manipulation skills can be learned directly and without supervised labels from a video of a human performing the task.\n", "pdf": "/pdf/939de21e00049bd95fa78820f7e7e99cc37df62b.pdf", "TL;DR": "Real robots learn new tasks from observing a few human demonstrations.", "paperhash": "sermanet|unsupervised_perceptual_rewards_for_imitation_learning", "keywords": ["Computer vision", "Deep learning", "Unsupervised Learning", "Reinforcement Learning", "Transfer Learning"], "conflicts": ["google.com", "berkeley.edu", "washington.edu"], "authors": ["Pierre Sermanet", "Kelvin Xu", "Sergey Levine"], "authorids": ["sermanet@google.com", "kelvinxx@google.com", "slevine@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512555432, "id": "ICLR.cc/2017/conference/-/paper521/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper521/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper521/AnonReviewer5", "ICLR.cc/2017/conference/paper521/AnonReviewer4", "ICLR.cc/2017/conference/paper521/AnonReviewer3"], "reply": {"forum": "Bkul3t9ee", "replyto": "Bkul3t9ee", "writers": {"values-regex": "ICLR.cc/2017/conference/paper521/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper521/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512555432}}}, {"tddate": null, "tmdate": 1481930250540, "tcdate": 1481930250540, "number": 3, "id": "SyffLxMVl", "invitation": "ICLR.cc/2017/conference/-/paper521/pre-review/question", "forum": "Bkul3t9ee", "replyto": "Bkul3t9ee", "signatures": ["ICLR.cc/2017/conference/paper521/AnonReviewer5"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper521/AnonReviewer5"], "content": {"title": "See review", "question": "See the review for main questions, no minor questions came to mind."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Perceptual Rewards for Imitation Learning", "abstract": "Reward function design and exploration time are arguably the biggest obstacles to the deployment of reinforcement learning (RL) agents in the real world. In many real-world tasks, designing a suitable reward function takes considerable manual engineering and often requires additional and potentially visible sensors to be installed just to measure whether the task has been executed successfully. Furthermore, many interesting tasks consist of multiple steps that must be executed in sequence. Even when the final outcome can be measured, it does not necessarily provide useful feedback on these implicit intermediate steps or sub-goals.\nTo address these issues, we propose leveraging the abstraction power of intermediate visual representations learned by deep models to quickly infer perceptual reward functions from small numbers of demonstrations. We present a method that is able to identify the key intermediate steps of a task from only a handful of demonstration sequences, and automatically identify the most discriminative features for identifying these steps. This method makes use of the features in a pre-trained deep model, but does not require any explicit sub-goal supervision. The resulting reward functions, which are dense and smooth, can then be used by an RL agent to learn to perform the task in real-world settings. To evaluate the learned reward functions, we present qualitative results on two real-world tasks and a quantitative evaluation against a human-designed reward function. We also demonstrate that our method can be used to learn a complex real-world door opening skill using a real robot, even when the demonstration used for reward learning is provided by a human using their own hand. To our knowledge, these are the first results showing that complex robotic manipulation skills can be learned directly and without supervised labels from a video of a human performing the task.\n", "pdf": "/pdf/939de21e00049bd95fa78820f7e7e99cc37df62b.pdf", "TL;DR": "Real robots learn new tasks from observing a few human demonstrations.", "paperhash": "sermanet|unsupervised_perceptual_rewards_for_imitation_learning", "keywords": ["Computer vision", "Deep learning", "Unsupervised Learning", "Reinforcement Learning", "Transfer Learning"], "conflicts": ["google.com", "berkeley.edu", "washington.edu"], "authors": ["Pierre Sermanet", "Kelvin Xu", "Sergey Levine"], "authorids": ["sermanet@google.com", "kelvinxx@google.com", "slevine@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481930251142, "id": "ICLR.cc/2017/conference/-/paper521/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper521/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper521/AnonReviewer4", "ICLR.cc/2017/conference/paper521/AnonReviewer3", "ICLR.cc/2017/conference/paper521/AnonReviewer5"], "reply": {"forum": "Bkul3t9ee", "replyto": "Bkul3t9ee", "writers": {"values-regex": "ICLR.cc/2017/conference/paper521/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper521/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481930251142}}}, {"tddate": null, "tmdate": 1481930196309, "tcdate": 1481930196309, "number": 1, "id": "SkhCBlGNl", "invitation": "ICLR.cc/2017/conference/-/paper521/official/review", "forum": "Bkul3t9ee", "replyto": "Bkul3t9ee", "signatures": ["ICLR.cc/2017/conference/paper521/AnonReviewer5"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper521/AnonReviewer5"], "content": {"title": "Well writen paper on a simple idea for extracting reward functions in real-world scenarios. Is missing some baselines.", "rating": "6: Marginally above acceptance threshold", "review": "The paper explores a simple approach to learning reward functions for reinforcement learning from visual observations of expert trajectories for cases were only little training data is available. To obtain descriptive rewards even under such challenging conditions the method re-uses a pre-trained neural network as feature extractor (this is similar to a large body of work on task transfer with neural nets in the area of computer vision) and represents the reward function as a weighted distance to features for automatically extracted \"key-frames\" of the provided expert trajectories.\n\nThe paper is well written and explains all involved concepts clearly while also embedding the presented approach in the literature on inverse reinforcement learning (IRL). The resulting algorithm is appealing due to its simplicity and could prove useful for many real world robotic applications. I have three main issues with the paper in its current form, if these can be addressed I believe the paper would be significantly strengthened:\n1) Although the recursive splitting approach for extracting the \"key-frames\" seems reasonable and the feature selection is well motivated I am missing two baselines in the experiments:\n   - what happens if the feature selection is disabled and the distance between all features is used ? will this immediately break the procedure ? If not, what is the trade-off here ? \n   - an even simpler baseline than what is proposed in the paper would be the following procedure: simply use all frames of the recorded trajectories, calculate the distance to them in feature space and weights them according to their time as in the approach proposed in the paper. How well would that work ?\n2) I understand the desire to combine the extracted reward function with a simple RL method but believe the used simple controller could potentially introduce a significant bias in the experiments since it requires initialization from an expert trajectory. As a direct consequence of this initialization the RL procedure is already started close to a good solution and the extracted reward function is potentially only queried in a small region around what was observed in the initial set of images (perhaps with the exception of the human demonstrations). Without an additional experiment it is thus unclear how well the presented approach will work in combination with other RL methods for training the controller.\n3) I understand that the low number of available images excludes training a deep neural net directly for the task at hand but one has to wonder how other baselines would do. What happens if one uses a random projection of the images to form a feature vector? How well would a distance measure using raw images (e.g. L2 norm of image differences) or a distance measure based on the first principal components work? It seems that occlusions etc. would exclude them from working well but without empirical evidence it is hard to confirm this.\n\nMinor issues:\n- Page 1: \"make use of ideas about imitation\" reads a bit awkwardly\n- Page 3: \"We use the Inception network pre-trained ImageNet\" -> pre-trained for ImageNet classification\n- Page 4: the definition of the transition function for the stochastic case seems broken\n- Page 6: \"efficient enough to evaluate\" a bit strangely written sentence\n\nAdditional comments rather than real issues:\n- The paper is mainly of empirical nature, little actual learning is performed to obtain the reward function and no theoretical advances are needed. This is not necessarily bad but makes the empirical evaluation all the more important. \n- While I liked the clear exposition the approach -- in the end -- boils down to computing quadratic distances to features of pre-extracted \"key-frames\", it is nice that you make a connection to standard IRL approaches in Section 2.1 but one could argue that this derivation is not strictly necessary.\n", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Perceptual Rewards for Imitation Learning", "abstract": "Reward function design and exploration time are arguably the biggest obstacles to the deployment of reinforcement learning (RL) agents in the real world. In many real-world tasks, designing a suitable reward function takes considerable manual engineering and often requires additional and potentially visible sensors to be installed just to measure whether the task has been executed successfully. Furthermore, many interesting tasks consist of multiple steps that must be executed in sequence. Even when the final outcome can be measured, it does not necessarily provide useful feedback on these implicit intermediate steps or sub-goals.\nTo address these issues, we propose leveraging the abstraction power of intermediate visual representations learned by deep models to quickly infer perceptual reward functions from small numbers of demonstrations. We present a method that is able to identify the key intermediate steps of a task from only a handful of demonstration sequences, and automatically identify the most discriminative features for identifying these steps. This method makes use of the features in a pre-trained deep model, but does not require any explicit sub-goal supervision. The resulting reward functions, which are dense and smooth, can then be used by an RL agent to learn to perform the task in real-world settings. To evaluate the learned reward functions, we present qualitative results on two real-world tasks and a quantitative evaluation against a human-designed reward function. We also demonstrate that our method can be used to learn a complex real-world door opening skill using a real robot, even when the demonstration used for reward learning is provided by a human using their own hand. To our knowledge, these are the first results showing that complex robotic manipulation skills can be learned directly and without supervised labels from a video of a human performing the task.\n", "pdf": "/pdf/939de21e00049bd95fa78820f7e7e99cc37df62b.pdf", "TL;DR": "Real robots learn new tasks from observing a few human demonstrations.", "paperhash": "sermanet|unsupervised_perceptual_rewards_for_imitation_learning", "keywords": ["Computer vision", "Deep learning", "Unsupervised Learning", "Reinforcement Learning", "Transfer Learning"], "conflicts": ["google.com", "berkeley.edu", "washington.edu"], "authors": ["Pierre Sermanet", "Kelvin Xu", "Sergey Levine"], "authorids": ["sermanet@google.com", "kelvinxx@google.com", "slevine@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512555432, "id": "ICLR.cc/2017/conference/-/paper521/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper521/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper521/AnonReviewer5", "ICLR.cc/2017/conference/paper521/AnonReviewer4", "ICLR.cc/2017/conference/paper521/AnonReviewer3"], "reply": {"forum": "Bkul3t9ee", "replyto": "Bkul3t9ee", "writers": {"values-regex": "ICLR.cc/2017/conference/paper521/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper521/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512555432}}}, {"tddate": null, "tmdate": 1481585683430, "tcdate": 1481585683422, "number": 2, "id": "HkoMN327x", "invitation": "ICLR.cc/2017/conference/-/paper521/public/comment", "forum": "Bkul3t9ee", "replyto": "H1DSZf17l", "signatures": ["~Pierre_Sermanet1"], "readers": ["everyone"], "writers": ["~Pierre_Sermanet1"], "content": {"title": "Response", "comment": "Thank you for your questions.\n\nQ: It would be great if the authors could provide more thorough ablation analysis on the parameter chosen (e.g, number of splits, number of discriminative features selected, mixing coefficients alpha in Eqn. 1). It would be great if the authors could provide with the reasoning process how they are chosen.\nA: We are working on providing more analysis and will update the paper in the coming days.\n\nQ: In Section 2.2, it is not clear where exactly the features come from, though \"mid to high-level features\" are mentioned. Ideally, these features, if visualized at the original images, should be concentrated at the key locations (e.g., door handle), which will make the approach more convincing.  \nA: We are investigating what analysis of the features we can provide and will update the paper in the coming days.\n\nQ: Finally, could the authors show failure cases with learned features, e.g., when the image is misclassified, leading to over-pessimistic/over-optimistic rewards. \nA: Failure cases were already available in the text of the original paper but we have added a dedicated section in 3.1.1 for failure cases to make it more clear."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Perceptual Rewards for Imitation Learning", "abstract": "Reward function design and exploration time are arguably the biggest obstacles to the deployment of reinforcement learning (RL) agents in the real world. In many real-world tasks, designing a suitable reward function takes considerable manual engineering and often requires additional and potentially visible sensors to be installed just to measure whether the task has been executed successfully. Furthermore, many interesting tasks consist of multiple steps that must be executed in sequence. Even when the final outcome can be measured, it does not necessarily provide useful feedback on these implicit intermediate steps or sub-goals.\nTo address these issues, we propose leveraging the abstraction power of intermediate visual representations learned by deep models to quickly infer perceptual reward functions from small numbers of demonstrations. We present a method that is able to identify the key intermediate steps of a task from only a handful of demonstration sequences, and automatically identify the most discriminative features for identifying these steps. This method makes use of the features in a pre-trained deep model, but does not require any explicit sub-goal supervision. The resulting reward functions, which are dense and smooth, can then be used by an RL agent to learn to perform the task in real-world settings. To evaluate the learned reward functions, we present qualitative results on two real-world tasks and a quantitative evaluation against a human-designed reward function. We also demonstrate that our method can be used to learn a complex real-world door opening skill using a real robot, even when the demonstration used for reward learning is provided by a human using their own hand. To our knowledge, these are the first results showing that complex robotic manipulation skills can be learned directly and without supervised labels from a video of a human performing the task.\n", "pdf": "/pdf/939de21e00049bd95fa78820f7e7e99cc37df62b.pdf", "TL;DR": "Real robots learn new tasks from observing a few human demonstrations.", "paperhash": "sermanet|unsupervised_perceptual_rewards_for_imitation_learning", "keywords": ["Computer vision", "Deep learning", "Unsupervised Learning", "Reinforcement Learning", "Transfer Learning"], "conflicts": ["google.com", "berkeley.edu", "washington.edu"], "authors": ["Pierre Sermanet", "Kelvin Xu", "Sergey Levine"], "authorids": ["sermanet@google.com", "kelvinxx@google.com", "slevine@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287538717, "id": "ICLR.cc/2017/conference/-/paper521/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Bkul3t9ee", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper521/reviewers", "ICLR.cc/2017/conference/paper521/areachairs"], "cdate": 1485287538717}}}, {"tddate": null, "tmdate": 1481585374446, "tcdate": 1481585374432, "number": 1, "id": "HyIkm23mg", "invitation": "ICLR.cc/2017/conference/-/paper521/public/comment", "forum": "Bkul3t9ee", "replyto": "B1m9I6V7l", "signatures": ["~Pierre_Sermanet1"], "readers": ["everyone"], "writers": ["~Pierre_Sermanet1"], "content": {"title": "Response", "comment": "Thank you for the clarification questions, we have updated the paper to answer them.\n\nQ: Should Algo. 1 be returning both min_std and min_split at the end?\nA: The algorithm hadn't been translated into pseudo-code correctly, it is fixed now.\n\nQ: In the line avg_std <- .. , isn't the model double counting the std from previous runs as well? Also is prev_std + std1 + std2 set addition or value addition?\nA: This line does not recompute the average standard deviation, it just averages the list of standard deviations already computed. prev_std + std1 + std2 was actually a list merging, we changed the code to reflect this using a Join() function.\n\nQ: Is there a dynamic prog. variant of optimally implementing the same method? What is the current complexity of this method in terms of video frames?\nA: Dynamic programming is not applicable because sub-problems depend on the preceding decisions. More details can be found in Section 2.2. The complexity is O(n^m) with n the number of frames and m the number of splits. We added a non-exact but efficient version of the algorithm as a greedy binary optimization, which runs in O(n^2 log m).\n\nQ: What is the advantage of the feature selection method in Sec. 2.2. Why not train a simple linear classifier for each subgoal and use the score of the classifier to define reward functions?\nA: Our initial intuition is that training a classifier with such little data would badly overfit, so we first tried to select individual features directly. We were planning to provide a comparison to a trained classifier, it is under way and we will update the paper with it in the coming days.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Perceptual Rewards for Imitation Learning", "abstract": "Reward function design and exploration time are arguably the biggest obstacles to the deployment of reinforcement learning (RL) agents in the real world. In many real-world tasks, designing a suitable reward function takes considerable manual engineering and often requires additional and potentially visible sensors to be installed just to measure whether the task has been executed successfully. Furthermore, many interesting tasks consist of multiple steps that must be executed in sequence. Even when the final outcome can be measured, it does not necessarily provide useful feedback on these implicit intermediate steps or sub-goals.\nTo address these issues, we propose leveraging the abstraction power of intermediate visual representations learned by deep models to quickly infer perceptual reward functions from small numbers of demonstrations. We present a method that is able to identify the key intermediate steps of a task from only a handful of demonstration sequences, and automatically identify the most discriminative features for identifying these steps. This method makes use of the features in a pre-trained deep model, but does not require any explicit sub-goal supervision. The resulting reward functions, which are dense and smooth, can then be used by an RL agent to learn to perform the task in real-world settings. To evaluate the learned reward functions, we present qualitative results on two real-world tasks and a quantitative evaluation against a human-designed reward function. We also demonstrate that our method can be used to learn a complex real-world door opening skill using a real robot, even when the demonstration used for reward learning is provided by a human using their own hand. To our knowledge, these are the first results showing that complex robotic manipulation skills can be learned directly and without supervised labels from a video of a human performing the task.\n", "pdf": "/pdf/939de21e00049bd95fa78820f7e7e99cc37df62b.pdf", "TL;DR": "Real robots learn new tasks from observing a few human demonstrations.", "paperhash": "sermanet|unsupervised_perceptual_rewards_for_imitation_learning", "keywords": ["Computer vision", "Deep learning", "Unsupervised Learning", "Reinforcement Learning", "Transfer Learning"], "conflicts": ["google.com", "berkeley.edu", "washington.edu"], "authors": ["Pierre Sermanet", "Kelvin Xu", "Sergey Levine"], "authorids": ["sermanet@google.com", "kelvinxx@google.com", "slevine@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287538717, "id": "ICLR.cc/2017/conference/-/paper521/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Bkul3t9ee", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper521/reviewers", "ICLR.cc/2017/conference/paper521/areachairs"], "cdate": 1485287538717}}}, {"tddate": null, "tmdate": 1481066123524, "tcdate": 1481066123519, "number": 2, "id": "B1m9I6V7l", "invitation": "ICLR.cc/2017/conference/-/paper521/pre-review/question", "forum": "Bkul3t9ee", "replyto": "Bkul3t9ee", "signatures": ["ICLR.cc/2017/conference/paper521/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper521/AnonReviewer3"], "content": {"title": "Clarifications of unsupervised video segmentation algorithm", "question": "1. Some clarifications of Algo. 1: \n- Should Algo. 1 be returning both min_std and min_split at the end?\n- In the line avg_std <- .. , isn't the model double counting the std from previous runs as well? Also is prev_std + std1 + std2 set addition or value addition?\n- Is there a dynamic prog. variant of optimally implementing the same method? What is the current complexity of this method in terms of video frames?\n\n2. What is the advantage of the feature selection method in Sec. 2.2. Why not train a simple linear classifier for each subgoal and use the score of the classifier to define reward functions?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Perceptual Rewards for Imitation Learning", "abstract": "Reward function design and exploration time are arguably the biggest obstacles to the deployment of reinforcement learning (RL) agents in the real world. In many real-world tasks, designing a suitable reward function takes considerable manual engineering and often requires additional and potentially visible sensors to be installed just to measure whether the task has been executed successfully. Furthermore, many interesting tasks consist of multiple steps that must be executed in sequence. Even when the final outcome can be measured, it does not necessarily provide useful feedback on these implicit intermediate steps or sub-goals.\nTo address these issues, we propose leveraging the abstraction power of intermediate visual representations learned by deep models to quickly infer perceptual reward functions from small numbers of demonstrations. We present a method that is able to identify the key intermediate steps of a task from only a handful of demonstration sequences, and automatically identify the most discriminative features for identifying these steps. This method makes use of the features in a pre-trained deep model, but does not require any explicit sub-goal supervision. The resulting reward functions, which are dense and smooth, can then be used by an RL agent to learn to perform the task in real-world settings. To evaluate the learned reward functions, we present qualitative results on two real-world tasks and a quantitative evaluation against a human-designed reward function. We also demonstrate that our method can be used to learn a complex real-world door opening skill using a real robot, even when the demonstration used for reward learning is provided by a human using their own hand. To our knowledge, these are the first results showing that complex robotic manipulation skills can be learned directly and without supervised labels from a video of a human performing the task.\n", "pdf": "/pdf/939de21e00049bd95fa78820f7e7e99cc37df62b.pdf", "TL;DR": "Real robots learn new tasks from observing a few human demonstrations.", "paperhash": "sermanet|unsupervised_perceptual_rewards_for_imitation_learning", "keywords": ["Computer vision", "Deep learning", "Unsupervised Learning", "Reinforcement Learning", "Transfer Learning"], "conflicts": ["google.com", "berkeley.edu", "washington.edu"], "authors": ["Pierre Sermanet", "Kelvin Xu", "Sergey Levine"], "authorids": ["sermanet@google.com", "kelvinxx@google.com", "slevine@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481930251142, "id": "ICLR.cc/2017/conference/-/paper521/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper521/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper521/AnonReviewer4", "ICLR.cc/2017/conference/paper521/AnonReviewer3", "ICLR.cc/2017/conference/paper521/AnonReviewer5"], "reply": {"forum": "Bkul3t9ee", "replyto": "Bkul3t9ee", "writers": {"values-regex": "ICLR.cc/2017/conference/paper521/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper521/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481930251142}}}, {"tddate": null, "tmdate": 1480692031312, "tcdate": 1480692031307, "number": 1, "id": "H1DSZf17l", "invitation": "ICLR.cc/2017/conference/-/paper521/pre-review/question", "forum": "Bkul3t9ee", "replyto": "Bkul3t9ee", "signatures": ["ICLR.cc/2017/conference/paper521/AnonReviewer4"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper521/AnonReviewer4"], "content": {"title": "More explanation and ablation analysis.", "question": "It would be great if the authors could provide more thorough ablation analysis on the parameter chosen (e.g, number of splits, number of discriminative features selected, mixing coefficients alpha in Eqn. 1). It would be great if the authors could provide with the reasoning process how they are chosen. \n\nIn Section 2.2, it is not clear where exactly the features come from, though \"mid to high-level features\" are mentioned. Ideally, these features, if visualized at the original images, should be concentrated at the key locations (e.g., door handle), which will make the approach more convincing.  \n\nFinally, could the authors show failure cases with learned features, e.g., when the image is misclassified, leading to over-pessimistic/over-optimistic rewards. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Perceptual Rewards for Imitation Learning", "abstract": "Reward function design and exploration time are arguably the biggest obstacles to the deployment of reinforcement learning (RL) agents in the real world. In many real-world tasks, designing a suitable reward function takes considerable manual engineering and often requires additional and potentially visible sensors to be installed just to measure whether the task has been executed successfully. Furthermore, many interesting tasks consist of multiple steps that must be executed in sequence. Even when the final outcome can be measured, it does not necessarily provide useful feedback on these implicit intermediate steps or sub-goals.\nTo address these issues, we propose leveraging the abstraction power of intermediate visual representations learned by deep models to quickly infer perceptual reward functions from small numbers of demonstrations. We present a method that is able to identify the key intermediate steps of a task from only a handful of demonstration sequences, and automatically identify the most discriminative features for identifying these steps. This method makes use of the features in a pre-trained deep model, but does not require any explicit sub-goal supervision. The resulting reward functions, which are dense and smooth, can then be used by an RL agent to learn to perform the task in real-world settings. To evaluate the learned reward functions, we present qualitative results on two real-world tasks and a quantitative evaluation against a human-designed reward function. We also demonstrate that our method can be used to learn a complex real-world door opening skill using a real robot, even when the demonstration used for reward learning is provided by a human using their own hand. To our knowledge, these are the first results showing that complex robotic manipulation skills can be learned directly and without supervised labels from a video of a human performing the task.\n", "pdf": "/pdf/939de21e00049bd95fa78820f7e7e99cc37df62b.pdf", "TL;DR": "Real robots learn new tasks from observing a few human demonstrations.", "paperhash": "sermanet|unsupervised_perceptual_rewards_for_imitation_learning", "keywords": ["Computer vision", "Deep learning", "Unsupervised Learning", "Reinforcement Learning", "Transfer Learning"], "conflicts": ["google.com", "berkeley.edu", "washington.edu"], "authors": ["Pierre Sermanet", "Kelvin Xu", "Sergey Levine"], "authorids": ["sermanet@google.com", "kelvinxx@google.com", "slevine@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481930251142, "id": "ICLR.cc/2017/conference/-/paper521/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper521/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper521/AnonReviewer4", "ICLR.cc/2017/conference/paper521/AnonReviewer3", "ICLR.cc/2017/conference/paper521/AnonReviewer5"], "reply": {"forum": "Bkul3t9ee", "replyto": "Bkul3t9ee", "writers": {"values-regex": "ICLR.cc/2017/conference/paper521/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper521/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481930251142}}}], "count": 14}