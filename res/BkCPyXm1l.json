{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396303288, "tcdate": 1486396303288, "number": 1, "id": "SJvcsfL_e", "invitation": "ICLR.cc/2017/conference/-/paper3/acceptance", "forum": "BkCPyXm1l", "replyto": "BkCPyXm1l", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "The reviewers unanimously recommend rejection."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "SoftTarget Regularization: An Effective Technique to Reduce Over-Fitting in Neural Networks", "abstract": "Deep neural networks are learning models with a very high capacity and therefore prone to over-fitting. Many regularization techniques such as Dropout, DropConnect, and weight decay all attempt to solve the problem of over-fitting by reducing the capacity of their respective models (Srivastava et al., 2014), (Wan et al., 2013), (Krogh & Hertz, 1992). In this paper we introduce a new form of regularization that guides the learning problem in a way that reduces over-fitting without sacrificing the capacity of the model. The mistakes that models make in early stages of training carry information about the learning problem. By adjusting the labels of the current epoch of training through a weighted average of the real labels, and an exponential average of the past soft-targets we achieved a regularization scheme as powerful as Dropout without necessarily reducing the capacity of the model, and simplified the complexity of the learning problem. SoftTarget regularization proved to be an effective tool in various neural network architectures.", "pdf": "/pdf/defec2079755d4334e3a2f4bb456b672e8cba34a.pdf", "paperhash": "aghajanyan|softtarget_regularization_an_effective_technique_to_reduce_overfitting_in_neural_networks", "author_emails": "armen.ag@live.com", "conflicts": ["dimensionalmechanics.com"], "authors": ["Armen Aghajanyan"], "authorids": ["armen.ag@live.com"], "keywords": ["Deep learning", "Optimization", "Computer vision"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396304196, "id": "ICLR.cc/2017/conference/-/paper3/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "BkCPyXm1l", "replyto": "BkCPyXm1l", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396304196}}}, {"tddate": null, "tmdate": 1485212508387, "tcdate": 1485212508387, "number": 7, "id": "BkNvo-NPl", "invitation": "ICLR.cc/2017/conference/-/paper3/official/comment", "forum": "BkCPyXm1l", "replyto": "SJ9I--JLe", "signatures": ["ICLR.cc/2017/conference/paper3/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper3/AnonReviewer2"], "content": {"title": "weak baseline", "comment": "A particular naive but seemly related baseline is to compare with a model regularized by a linear combination of hard labels and uniform soft targets( 1/#classes ) instead of the previous model predictions. The uniform soft target has the maximum co-label similarity. If this naive baseline also work well as an effective regularizer, the authors need to rethink about the proposed methods in this paper. Alternatively, the authors can directly penalize the negative co-label similarity in the loss function. Overall, the baseline is weak and there is very little insight into why the proposed method provide generalization benefits. I stay by my original review."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "SoftTarget Regularization: An Effective Technique to Reduce Over-Fitting in Neural Networks", "abstract": "Deep neural networks are learning models with a very high capacity and therefore prone to over-fitting. Many regularization techniques such as Dropout, DropConnect, and weight decay all attempt to solve the problem of over-fitting by reducing the capacity of their respective models (Srivastava et al., 2014), (Wan et al., 2013), (Krogh & Hertz, 1992). In this paper we introduce a new form of regularization that guides the learning problem in a way that reduces over-fitting without sacrificing the capacity of the model. The mistakes that models make in early stages of training carry information about the learning problem. By adjusting the labels of the current epoch of training through a weighted average of the real labels, and an exponential average of the past soft-targets we achieved a regularization scheme as powerful as Dropout without necessarily reducing the capacity of the model, and simplified the complexity of the learning problem. SoftTarget regularization proved to be an effective tool in various neural network architectures.", "pdf": "/pdf/defec2079755d4334e3a2f4bb456b672e8cba34a.pdf", "paperhash": "aghajanyan|softtarget_regularization_an_effective_technique_to_reduce_overfitting_in_neural_networks", "author_emails": "armen.ag@live.com", "conflicts": ["dimensionalmechanics.com"], "authors": ["Armen Aghajanyan"], "authorids": ["armen.ag@live.com"], "keywords": ["Deep learning", "Optimization", "Computer vision"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287766741, "id": "ICLR.cc/2017/conference/-/paper3/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "BkCPyXm1l", "writers": {"values-regex": "ICLR.cc/2017/conference/paper3/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper3/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper3/reviewers", "ICLR.cc/2017/conference/paper3/areachairs"], "cdate": 1485287766741}}}, {"tddate": null, "tmdate": 1485162171936, "tcdate": 1485162171936, "number": 6, "id": "Sy4pUHmwx", "invitation": "ICLR.cc/2017/conference/-/paper3/official/comment", "forum": "BkCPyXm1l", "replyto": "Bk454VSNe", "signatures": ["ICLR.cc/2017/conference/paper3/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper3/AnonReviewer3"], "content": {"title": "response to rebuttal", "comment": "My decision did not change after rebuttal since the author could not provide convincing arguments/improvements to concerns above. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "SoftTarget Regularization: An Effective Technique to Reduce Over-Fitting in Neural Networks", "abstract": "Deep neural networks are learning models with a very high capacity and therefore prone to over-fitting. Many regularization techniques such as Dropout, DropConnect, and weight decay all attempt to solve the problem of over-fitting by reducing the capacity of their respective models (Srivastava et al., 2014), (Wan et al., 2013), (Krogh & Hertz, 1992). In this paper we introduce a new form of regularization that guides the learning problem in a way that reduces over-fitting without sacrificing the capacity of the model. The mistakes that models make in early stages of training carry information about the learning problem. By adjusting the labels of the current epoch of training through a weighted average of the real labels, and an exponential average of the past soft-targets we achieved a regularization scheme as powerful as Dropout without necessarily reducing the capacity of the model, and simplified the complexity of the learning problem. SoftTarget regularization proved to be an effective tool in various neural network architectures.", "pdf": "/pdf/defec2079755d4334e3a2f4bb456b672e8cba34a.pdf", "paperhash": "aghajanyan|softtarget_regularization_an_effective_technique_to_reduce_overfitting_in_neural_networks", "author_emails": "armen.ag@live.com", "conflicts": ["dimensionalmechanics.com"], "authors": ["Armen Aghajanyan"], "authorids": ["armen.ag@live.com"], "keywords": ["Deep learning", "Optimization", "Computer vision"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287766741, "id": "ICLR.cc/2017/conference/-/paper3/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "BkCPyXm1l", "writers": {"values-regex": "ICLR.cc/2017/conference/paper3/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper3/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper3/reviewers", "ICLR.cc/2017/conference/paper3/areachairs"], "cdate": 1485287766741}}}, {"tddate": null, "tmdate": 1485162148206, "tcdate": 1482142860434, "number": 2, "id": "Bk454VSNe", "invitation": "ICLR.cc/2017/conference/-/paper3/official/review", "forum": "BkCPyXm1l", "replyto": "BkCPyXm1l", "signatures": ["ICLR.cc/2017/conference/paper3/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper3/AnonReviewer3"], "content": {"title": "the empirical results are not satisfactory", "rating": "3: Clear rejection", "review": "Inspired by the analysis on the effect of the co-label similarity (Hinton et al., 2015), this paper proposes a soft-target regularization that iteratively trains the network using weighted average of the exponential moving average of past labels and hard labels as target argument of loss. They claim that this prevents the disappearing of co-label similarity after early training and  yields a competitive regularization to dropout without sacrificing network capacity.\n\nIn order to make a fair comparison to dropout,  the dropout should be tuned carefully. Showing that it performs better than dropout regularization for some particular values of dropout (Table 2) does not demonstrate a convincing advantage. It is possible that dropout performs better after a reasonable tuning with cross-validation.\n\nThe baseline architectures used in the experiments do not belong the recent state of art methods thus yielding significantly lower accuracy. It seems also that experiment setup does not involve any data augmentation, the results can also change with augmentation. It is not clear why number of epochs are set to a small number like 100 without putting some convergence tests.. Therefore the significance of the method is not convincingly demonstrated in empirical study.\n\nCo-label similarities could be calculated using softmax results at final layer rather than using predicted labels.  The advantage over dropout is not clear in Figure 4, the dropout is set to 0.2 without any cross-validation.  \n\n\nRegularizing by enforcing the training steps to keep co-label similarities is interesting idea but not very novel and the results are not significant.\n\nPros : \n- provides an investigation of regularization on co-label similarity during training\n\nCons:\n-The empirical results do not support the intuitive claims regarding proposed procedure\nIterative version can be unstable in practice\n\n\n", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "SoftTarget Regularization: An Effective Technique to Reduce Over-Fitting in Neural Networks", "abstract": "Deep neural networks are learning models with a very high capacity and therefore prone to over-fitting. Many regularization techniques such as Dropout, DropConnect, and weight decay all attempt to solve the problem of over-fitting by reducing the capacity of their respective models (Srivastava et al., 2014), (Wan et al., 2013), (Krogh & Hertz, 1992). In this paper we introduce a new form of regularization that guides the learning problem in a way that reduces over-fitting without sacrificing the capacity of the model. The mistakes that models make in early stages of training carry information about the learning problem. By adjusting the labels of the current epoch of training through a weighted average of the real labels, and an exponential average of the past soft-targets we achieved a regularization scheme as powerful as Dropout without necessarily reducing the capacity of the model, and simplified the complexity of the learning problem. SoftTarget regularization proved to be an effective tool in various neural network architectures.", "pdf": "/pdf/defec2079755d4334e3a2f4bb456b672e8cba34a.pdf", "paperhash": "aghajanyan|softtarget_regularization_an_effective_technique_to_reduce_overfitting_in_neural_networks", "author_emails": "armen.ag@live.com", "conflicts": ["dimensionalmechanics.com"], "authors": ["Armen Aghajanyan"], "authorids": ["armen.ag@live.com"], "keywords": ["Deep learning", "Optimization", "Computer vision"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512728773, "id": "ICLR.cc/2017/conference/-/paper3/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper3/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper3/AnonReviewer1", "ICLR.cc/2017/conference/paper3/AnonReviewer3", "ICLR.cc/2017/conference/paper3/AnonReviewer2"], "reply": {"forum": "BkCPyXm1l", "replyto": "BkCPyXm1l", "writers": {"values-regex": "ICLR.cc/2017/conference/paper3/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper3/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512728773}}}, {"tddate": null, "tmdate": 1485148395306, "tcdate": 1485148395306, "number": 5, "id": "SJmeWMXwg", "invitation": "ICLR.cc/2017/conference/-/paper3/official/comment", "forum": "BkCPyXm1l", "replyto": "B1wUf-yLg", "signatures": ["ICLR.cc/2017/conference/paper3/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper3/AnonReviewer3"], "content": {"title": "response", "comment": "Showing results from 0.2 to 0.8 with increments of 0.2 is not satisfactory as changes smaller than 0.2 can effect results when the parameters are tuned with random sampling or spearmint.\n\nRegularization effect of softlabels is discussed in Section 3 and Section 6 in Hinton et.al. (2015). Using this for the network itself is an incremantal approach. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "SoftTarget Regularization: An Effective Technique to Reduce Over-Fitting in Neural Networks", "abstract": "Deep neural networks are learning models with a very high capacity and therefore prone to over-fitting. Many regularization techniques such as Dropout, DropConnect, and weight decay all attempt to solve the problem of over-fitting by reducing the capacity of their respective models (Srivastava et al., 2014), (Wan et al., 2013), (Krogh & Hertz, 1992). In this paper we introduce a new form of regularization that guides the learning problem in a way that reduces over-fitting without sacrificing the capacity of the model. The mistakes that models make in early stages of training carry information about the learning problem. By adjusting the labels of the current epoch of training through a weighted average of the real labels, and an exponential average of the past soft-targets we achieved a regularization scheme as powerful as Dropout without necessarily reducing the capacity of the model, and simplified the complexity of the learning problem. SoftTarget regularization proved to be an effective tool in various neural network architectures.", "pdf": "/pdf/defec2079755d4334e3a2f4bb456b672e8cba34a.pdf", "paperhash": "aghajanyan|softtarget_regularization_an_effective_technique_to_reduce_overfitting_in_neural_networks", "author_emails": "armen.ag@live.com", "conflicts": ["dimensionalmechanics.com"], "authors": ["Armen Aghajanyan"], "authorids": ["armen.ag@live.com"], "keywords": ["Deep learning", "Optimization", "Computer vision"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287766741, "id": "ICLR.cc/2017/conference/-/paper3/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "BkCPyXm1l", "writers": {"values-regex": "ICLR.cc/2017/conference/paper3/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper3/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper3/reviewers", "ICLR.cc/2017/conference/paper3/areachairs"], "cdate": 1485287766741}}}, {"tddate": null, "tmdate": 1483833935393, "tcdate": 1483833935393, "number": 9, "id": "B1wUf-yLg", "invitation": "ICLR.cc/2017/conference/-/paper3/public/comment", "forum": "BkCPyXm1l", "replyto": "Bk454VSNe", "signatures": ["~Armen_Aghajanyan1"], "readers": ["everyone"], "writers": ["~Armen_Aghajanyan1"], "content": {"title": "reply", "comment": "Thank you for your comments.\n\nWe tested dropout extensively in multiple experiments, by electing dropout values ranging from 0.2-0.8. \n\n\"Regularizing by enforcing the training steps to keep co-label similarities is interesting idea but not very novel\": I don't believe I have seen any paper that approach regularization from this perspective, papers that build up to our approach are mentioned and talked about extensively. Could you please reference a paper that does this?\n\nI would kindly ask you to reconsider. SoftTarget outperformed any other regularization in virtually all of our experiments."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "SoftTarget Regularization: An Effective Technique to Reduce Over-Fitting in Neural Networks", "abstract": "Deep neural networks are learning models with a very high capacity and therefore prone to over-fitting. Many regularization techniques such as Dropout, DropConnect, and weight decay all attempt to solve the problem of over-fitting by reducing the capacity of their respective models (Srivastava et al., 2014), (Wan et al., 2013), (Krogh & Hertz, 1992). In this paper we introduce a new form of regularization that guides the learning problem in a way that reduces over-fitting without sacrificing the capacity of the model. The mistakes that models make in early stages of training carry information about the learning problem. By adjusting the labels of the current epoch of training through a weighted average of the real labels, and an exponential average of the past soft-targets we achieved a regularization scheme as powerful as Dropout without necessarily reducing the capacity of the model, and simplified the complexity of the learning problem. SoftTarget regularization proved to be an effective tool in various neural network architectures.", "pdf": "/pdf/defec2079755d4334e3a2f4bb456b672e8cba34a.pdf", "paperhash": "aghajanyan|softtarget_regularization_an_effective_technique_to_reduce_overfitting_in_neural_networks", "author_emails": "armen.ag@live.com", "conflicts": ["dimensionalmechanics.com"], "authors": ["Armen Aghajanyan"], "authorids": ["armen.ag@live.com"], "keywords": ["Deep learning", "Optimization", "Computer vision"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287766873, "id": "ICLR.cc/2017/conference/-/paper3/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BkCPyXm1l", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper3/reviewers", "ICLR.cc/2017/conference/paper3/areachairs"], "cdate": 1485287766873}}}, {"tddate": null, "tmdate": 1483833682363, "tcdate": 1483833682363, "number": 8, "id": "SJ9I--JLe", "invitation": "ICLR.cc/2017/conference/-/paper3/public/comment", "forum": "BkCPyXm1l", "replyto": "SkLtfa8El", "signatures": ["~Armen_Aghajanyan1"], "readers": ["everyone"], "writers": ["~Armen_Aghajanyan1"], "content": {"title": "reply", "comment": "Thank you for your comment.\n\nHyper-parameter optimization was done with uniform initialization on all of the various architectures tested. The purpose of all the tests was to show relative comparisons between varying architectures and the fact that SoftTarget regularization performed better in virtually all the cases builds a strong case for the effectiveness of SoftTarget.\n\nI believe the paper you are referring to is Hinton's distillation paper, which I reference and have a subsection dedicated too. Hinton et al, describe a method to distil knowledge from bigger networks to smaller networks but don't create a framework for regularizing a single network through soft-targets. This paper creates that framework. \n\nI would ask kindly to reconsider.\n\nThank you."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "SoftTarget Regularization: An Effective Technique to Reduce Over-Fitting in Neural Networks", "abstract": "Deep neural networks are learning models with a very high capacity and therefore prone to over-fitting. Many regularization techniques such as Dropout, DropConnect, and weight decay all attempt to solve the problem of over-fitting by reducing the capacity of their respective models (Srivastava et al., 2014), (Wan et al., 2013), (Krogh & Hertz, 1992). In this paper we introduce a new form of regularization that guides the learning problem in a way that reduces over-fitting without sacrificing the capacity of the model. The mistakes that models make in early stages of training carry information about the learning problem. By adjusting the labels of the current epoch of training through a weighted average of the real labels, and an exponential average of the past soft-targets we achieved a regularization scheme as powerful as Dropout without necessarily reducing the capacity of the model, and simplified the complexity of the learning problem. SoftTarget regularization proved to be an effective tool in various neural network architectures.", "pdf": "/pdf/defec2079755d4334e3a2f4bb456b672e8cba34a.pdf", "paperhash": "aghajanyan|softtarget_regularization_an_effective_technique_to_reduce_overfitting_in_neural_networks", "author_emails": "armen.ag@live.com", "conflicts": ["dimensionalmechanics.com"], "authors": ["Armen Aghajanyan"], "authorids": ["armen.ag@live.com"], "keywords": ["Deep learning", "Optimization", "Computer vision"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287766873, "id": "ICLR.cc/2017/conference/-/paper3/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BkCPyXm1l", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper3/reviewers", "ICLR.cc/2017/conference/paper3/areachairs"], "cdate": 1485287766873}}}, {"tddate": null, "tmdate": 1482244733619, "tcdate": 1482244733619, "number": 3, "id": "SkLtfa8El", "invitation": "ICLR.cc/2017/conference/-/paper3/official/review", "forum": "BkCPyXm1l", "replyto": "BkCPyXm1l", "signatures": ["ICLR.cc/2017/conference/paper3/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper3/AnonReviewer2"], "content": {"title": "Review", "rating": "4: Ok but not good enough - rejection", "review": "The paper introduced a regularization scheme through soft-target that are produced by mixing between the true hard label and the current model prediction. Very similar method was proposed in Section 6 from (Hinton et al. 2016, Distilling the Knowledge in a Neural Network). \n\nPros: \n+ Comprehensive analysis on the co-label similarity.\n\nCons:\n- Weak baselines. I am not sure the authors have found the best hyper-parameters in their experiments. I just trained a 5 layer fully connected MNIST model with 512 hidden units without any regularizer and achieved 0.986 acc. using Adam and He initialization, where the paper reported 0.981 for such architecture. \n- The authors failed to bring the novel idea. It is very similar to (Hinton et al. 2016). This is probably not enough for ICLR.", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "SoftTarget Regularization: An Effective Technique to Reduce Over-Fitting in Neural Networks", "abstract": "Deep neural networks are learning models with a very high capacity and therefore prone to over-fitting. Many regularization techniques such as Dropout, DropConnect, and weight decay all attempt to solve the problem of over-fitting by reducing the capacity of their respective models (Srivastava et al., 2014), (Wan et al., 2013), (Krogh & Hertz, 1992). In this paper we introduce a new form of regularization that guides the learning problem in a way that reduces over-fitting without sacrificing the capacity of the model. The mistakes that models make in early stages of training carry information about the learning problem. By adjusting the labels of the current epoch of training through a weighted average of the real labels, and an exponential average of the past soft-targets we achieved a regularization scheme as powerful as Dropout without necessarily reducing the capacity of the model, and simplified the complexity of the learning problem. SoftTarget regularization proved to be an effective tool in various neural network architectures.", "pdf": "/pdf/defec2079755d4334e3a2f4bb456b672e8cba34a.pdf", "paperhash": "aghajanyan|softtarget_regularization_an_effective_technique_to_reduce_overfitting_in_neural_networks", "author_emails": "armen.ag@live.com", "conflicts": ["dimensionalmechanics.com"], "authors": ["Armen Aghajanyan"], "authorids": ["armen.ag@live.com"], "keywords": ["Deep learning", "Optimization", "Computer vision"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512728773, "id": "ICLR.cc/2017/conference/-/paper3/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper3/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper3/AnonReviewer1", "ICLR.cc/2017/conference/paper3/AnonReviewer3", "ICLR.cc/2017/conference/paper3/AnonReviewer2"], "reply": {"forum": "BkCPyXm1l", "replyto": "BkCPyXm1l", "writers": {"values-regex": "ICLR.cc/2017/conference/paper3/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper3/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512728773}}}, {"tddate": null, "tmdate": 1481842985965, "tcdate": 1481842985959, "number": 1, "id": "SJzN-ilVl", "invitation": "ICLR.cc/2017/conference/-/paper3/official/review", "forum": "BkCPyXm1l", "replyto": "BkCPyXm1l", "signatures": ["ICLR.cc/2017/conference/paper3/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper3/AnonReviewer1"], "content": {"title": "An interesting approach, but I'm unconvinced.", "rating": "4: Ok but not good enough - rejection", "review": "This manuscript tries to tackle neural network regularization by blending the target distribution with predictions of the model itself. In this sense it is similar in spirit to scheduled sampling (Bengio et al) and SEARN (Daume et al) DAgger (Ross et al) which consider a \"roll-in\" mixture of the target and model distributions during training. It was clarified in the pre-review questions that these targets are generated on-line rather than from a lagged distribution, which I think makes the algorithm pseudocode somewhat misleading if I understand it correctly.\n\nThis is an incremental improvement on the idea of label softening/smoothing that has recently been revived, and so the novelty is not that high. The author points out that co-label similarity is better preserved by this method but it doesn't follow that this is causal re: regularization; a natural baseline would be a fixed, soft label distribution, as well as one where the softening/temperature of the label distribution is gradually reduced (as one would expect for this method to do as the model gets closer and closer to reproducing the target distribution).\n\nIt's an interesting and somewhat appealing idea but the case is not clearly made that this is all that useful. The dropout baselines for MNIST seem quite far from results already in the literature (Srivastava et al 2014 achieves 1.06% with a 3x1024 MLP with dropout and a simple max norm constraint; the dropout baselines here fail to break 1.3% which is rather high by contemporary standards on the permutation-invariant task), and results for CIFAR10 are quite far from the current state of the art, making it difficult to judge the contribution in light of other innovations. The largest benchmark considered is SVHN where the reported accuracies are quite bad indeed; SOTA for single net performance has been less than half the reported error rates for 3-4 years now. It's unclear what conclusions can be drawn about how this would help (or even hurt) in a better-tuned setting.\n\nI have remaining reservations about data hygiene, namely reporting minimum test loss/maximum test accuracy rather than an unbiased method for model selection (minimum validation set error, for example). Relatedly, the regularization potential of early stopping on a validation set is not considered. See, e.g. the protocol in Goodfellow et al (2013).", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "SoftTarget Regularization: An Effective Technique to Reduce Over-Fitting in Neural Networks", "abstract": "Deep neural networks are learning models with a very high capacity and therefore prone to over-fitting. Many regularization techniques such as Dropout, DropConnect, and weight decay all attempt to solve the problem of over-fitting by reducing the capacity of their respective models (Srivastava et al., 2014), (Wan et al., 2013), (Krogh & Hertz, 1992). In this paper we introduce a new form of regularization that guides the learning problem in a way that reduces over-fitting without sacrificing the capacity of the model. The mistakes that models make in early stages of training carry information about the learning problem. By adjusting the labels of the current epoch of training through a weighted average of the real labels, and an exponential average of the past soft-targets we achieved a regularization scheme as powerful as Dropout without necessarily reducing the capacity of the model, and simplified the complexity of the learning problem. SoftTarget regularization proved to be an effective tool in various neural network architectures.", "pdf": "/pdf/defec2079755d4334e3a2f4bb456b672e8cba34a.pdf", "paperhash": "aghajanyan|softtarget_regularization_an_effective_technique_to_reduce_overfitting_in_neural_networks", "author_emails": "armen.ag@live.com", "conflicts": ["dimensionalmechanics.com"], "authors": ["Armen Aghajanyan"], "authorids": ["armen.ag@live.com"], "keywords": ["Deep learning", "Optimization", "Computer vision"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512728773, "id": "ICLR.cc/2017/conference/-/paper3/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper3/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper3/AnonReviewer1", "ICLR.cc/2017/conference/paper3/AnonReviewer3", "ICLR.cc/2017/conference/paper3/AnonReviewer2"], "reply": {"forum": "BkCPyXm1l", "replyto": "BkCPyXm1l", "writers": {"values-regex": "ICLR.cc/2017/conference/paper3/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper3/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512728773}}}, {"tddate": null, "tmdate": 1480831130637, "tcdate": 1480831130631, "number": 7, "id": "ryQieNZQe", "invitation": "ICLR.cc/2017/conference/-/paper3/public/comment", "forum": "BkCPyXm1l", "replyto": "H1vO8K1Qe", "signatures": ["~Armen_Aghajanyan1"], "readers": ["everyone"], "writers": ["~Armen_Aghajanyan1"], "content": {"title": "revision 1:", "comment": "Hello,\n\nI have added accuracy reports in the paper. SoftTarget models performs better than other models in terms of accuracy as well. The paper also compares Batch Normalization as a regularization scheme. \n\nThank you.\nArmen A."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "SoftTarget Regularization: An Effective Technique to Reduce Over-Fitting in Neural Networks", "abstract": "Deep neural networks are learning models with a very high capacity and therefore prone to over-fitting. Many regularization techniques such as Dropout, DropConnect, and weight decay all attempt to solve the problem of over-fitting by reducing the capacity of their respective models (Srivastava et al., 2014), (Wan et al., 2013), (Krogh & Hertz, 1992). In this paper we introduce a new form of regularization that guides the learning problem in a way that reduces over-fitting without sacrificing the capacity of the model. The mistakes that models make in early stages of training carry information about the learning problem. By adjusting the labels of the current epoch of training through a weighted average of the real labels, and an exponential average of the past soft-targets we achieved a regularization scheme as powerful as Dropout without necessarily reducing the capacity of the model, and simplified the complexity of the learning problem. SoftTarget regularization proved to be an effective tool in various neural network architectures.", "pdf": "/pdf/defec2079755d4334e3a2f4bb456b672e8cba34a.pdf", "paperhash": "aghajanyan|softtarget_regularization_an_effective_technique_to_reduce_overfitting_in_neural_networks", "author_emails": "armen.ag@live.com", "conflicts": ["dimensionalmechanics.com"], "authors": ["Armen Aghajanyan"], "authorids": ["armen.ag@live.com"], "keywords": ["Deep learning", "Optimization", "Computer vision"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287766873, "id": "ICLR.cc/2017/conference/-/paper3/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BkCPyXm1l", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper3/reviewers", "ICLR.cc/2017/conference/paper3/areachairs"], "cdate": 1485287766873}}}, {"tddate": null, "tmdate": 1480830974538, "tcdate": 1480830974533, "number": 6, "id": "SyU-eEZQg", "invitation": "ICLR.cc/2017/conference/-/paper3/public/comment", "forum": "BkCPyXm1l", "replyto": "BkQsA_CMe", "signatures": ["~Armen_Aghajanyan1"], "readers": ["everyone"], "writers": ["~Armen_Aghajanyan1"], "content": {"title": "revision 1", "comment": "Hello,\n\nI wanted to comment on the new revisions made to the paper. I ran a small brute force grid search on SoftTarget hyperparameters as well. I also described the use of small numbers in n_t and n_b. I also added a mention in the \"similarities to other methods\" of the interpretation of SoftTarget as a stateful implementation of distillation. \n\nI also wanted to quickly comment on your question on tuning the lambda parameter. The lambda parameter can be seen as the general weighting for any reguralizer. Therefore I did not include a mention on an algorithm on how to tune it since it depends on problem set up. Lambda was found through a small grid-search.\n\nThank you for your comments."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "SoftTarget Regularization: An Effective Technique to Reduce Over-Fitting in Neural Networks", "abstract": "Deep neural networks are learning models with a very high capacity and therefore prone to over-fitting. Many regularization techniques such as Dropout, DropConnect, and weight decay all attempt to solve the problem of over-fitting by reducing the capacity of their respective models (Srivastava et al., 2014), (Wan et al., 2013), (Krogh & Hertz, 1992). In this paper we introduce a new form of regularization that guides the learning problem in a way that reduces over-fitting without sacrificing the capacity of the model. The mistakes that models make in early stages of training carry information about the learning problem. By adjusting the labels of the current epoch of training through a weighted average of the real labels, and an exponential average of the past soft-targets we achieved a regularization scheme as powerful as Dropout without necessarily reducing the capacity of the model, and simplified the complexity of the learning problem. SoftTarget regularization proved to be an effective tool in various neural network architectures.", "pdf": "/pdf/defec2079755d4334e3a2f4bb456b672e8cba34a.pdf", "paperhash": "aghajanyan|softtarget_regularization_an_effective_technique_to_reduce_overfitting_in_neural_networks", "author_emails": "armen.ag@live.com", "conflicts": ["dimensionalmechanics.com"], "authors": ["Armen Aghajanyan"], "authorids": ["armen.ag@live.com"], "keywords": ["Deep learning", "Optimization", "Computer vision"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287766873, "id": "ICLR.cc/2017/conference/-/paper3/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BkCPyXm1l", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper3/reviewers", "ICLR.cc/2017/conference/paper3/areachairs"], "cdate": 1485287766873}}}, {"tddate": null, "tmdate": 1480830643807, "tcdate": 1480830643802, "number": 5, "id": "r12n0XW7e", "invitation": "ICLR.cc/2017/conference/-/paper3/public/comment", "forum": "BkCPyXm1l", "replyto": "ByA7iECGx", "signatures": ["~Armen_Aghajanyan1"], "readers": ["everyone"], "writers": ["~Armen_Aghajanyan1"], "content": {"title": "Revision 1:", "comment": "Hello,\n\nI have reported accuracy terms for all my experiments, as well as a few comments on choice of hyper-parameters. Overall SoftTarget regularized networks outperformed other networks in terms of accuracy as well.\n\nThank you.\nArmen. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "SoftTarget Regularization: An Effective Technique to Reduce Over-Fitting in Neural Networks", "abstract": "Deep neural networks are learning models with a very high capacity and therefore prone to over-fitting. Many regularization techniques such as Dropout, DropConnect, and weight decay all attempt to solve the problem of over-fitting by reducing the capacity of their respective models (Srivastava et al., 2014), (Wan et al., 2013), (Krogh & Hertz, 1992). In this paper we introduce a new form of regularization that guides the learning problem in a way that reduces over-fitting without sacrificing the capacity of the model. The mistakes that models make in early stages of training carry information about the learning problem. By adjusting the labels of the current epoch of training through a weighted average of the real labels, and an exponential average of the past soft-targets we achieved a regularization scheme as powerful as Dropout without necessarily reducing the capacity of the model, and simplified the complexity of the learning problem. SoftTarget regularization proved to be an effective tool in various neural network architectures.", "pdf": "/pdf/defec2079755d4334e3a2f4bb456b672e8cba34a.pdf", "paperhash": "aghajanyan|softtarget_regularization_an_effective_technique_to_reduce_overfitting_in_neural_networks", "author_emails": "armen.ag@live.com", "conflicts": ["dimensionalmechanics.com"], "authors": ["Armen Aghajanyan"], "authorids": ["armen.ag@live.com"], "keywords": ["Deep learning", "Optimization", "Computer vision"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287766873, "id": "ICLR.cc/2017/conference/-/paper3/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BkCPyXm1l", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper3/reviewers", "ICLR.cc/2017/conference/paper3/areachairs"], "cdate": 1485287766873}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1480830405166, "tcdate": 1476763494052, "number": 3, "id": "BkCPyXm1l", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "BkCPyXm1l", "signatures": ["~Armen_Aghajanyan1"], "readers": ["everyone"], "content": {"TL;DR": "", "title": "SoftTarget Regularization: An Effective Technique to Reduce Over-Fitting in Neural Networks", "abstract": "Deep neural networks are learning models with a very high capacity and therefore prone to over-fitting. Many regularization techniques such as Dropout, DropConnect, and weight decay all attempt to solve the problem of over-fitting by reducing the capacity of their respective models (Srivastava et al., 2014), (Wan et al., 2013), (Krogh & Hertz, 1992). In this paper we introduce a new form of regularization that guides the learning problem in a way that reduces over-fitting without sacrificing the capacity of the model. The mistakes that models make in early stages of training carry information about the learning problem. By adjusting the labels of the current epoch of training through a weighted average of the real labels, and an exponential average of the past soft-targets we achieved a regularization scheme as powerful as Dropout without necessarily reducing the capacity of the model, and simplified the complexity of the learning problem. SoftTarget regularization proved to be an effective tool in various neural network architectures.", "pdf": "/pdf/defec2079755d4334e3a2f4bb456b672e8cba34a.pdf", "paperhash": "aghajanyan|softtarget_regularization_an_effective_technique_to_reduce_overfitting_in_neural_networks", "author_emails": "armen.ag@live.com", "conflicts": ["dimensionalmechanics.com"], "authors": ["Armen Aghajanyan"], "authorids": ["armen.ag@live.com"], "keywords": ["Deep learning", "Optimization", "Computer vision"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 20, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "tmdate": 1480726370920, "tcdate": 1480726370915, "number": 4, "id": "H1ivwck7g", "invitation": "ICLR.cc/2017/conference/-/paper3/public/comment", "forum": "BkCPyXm1l", "replyto": "H1vO8K1Qe", "signatures": ["~Armen_Aghajanyan1"], "readers": ["everyone"], "writers": ["~Armen_Aghajanyan1"], "content": {"title": "weight decay: result", "comment": "Thank you for reading. \n\nI'll run weight-decay experiments as soon as I can.\n\nThe next edits for this paper will include hyper-parameter optimization and accuracy reports. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "SoftTarget Regularization: An Effective Technique to Reduce Over-Fitting in Neural Networks", "abstract": "Deep neural networks are learning models with a very high capacity and therefore prone to over-fitting. Many regularization techniques such as Dropout, DropConnect, and weight decay all attempt to solve the problem of over-fitting by reducing the capacity of their respective models (Srivastava et al., 2014), (Wan et al., 2013), (Krogh & Hertz, 1992). In this paper we introduce a new form of regularization that guides the learning problem in a way that reduces over-fitting without sacrificing the capacity of the model. The mistakes that models make in early stages of training carry information about the learning problem. By adjusting the labels of the current epoch of training through a weighted average of the real labels, and an exponential average of the past soft-targets we achieved a regularization scheme as powerful as Dropout without necessarily reducing the capacity of the model, and simplified the complexity of the learning problem. SoftTarget regularization proved to be an effective tool in various neural network architectures.", "pdf": "/pdf/defec2079755d4334e3a2f4bb456b672e8cba34a.pdf", "paperhash": "aghajanyan|softtarget_regularization_an_effective_technique_to_reduce_overfitting_in_neural_networks", "author_emails": "armen.ag@live.com", "conflicts": ["dimensionalmechanics.com"], "authors": ["Armen Aghajanyan"], "authorids": ["armen.ag@live.com"], "keywords": ["Deep learning", "Optimization", "Computer vision"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287766873, "id": "ICLR.cc/2017/conference/-/paper3/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BkCPyXm1l", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper3/reviewers", "ICLR.cc/2017/conference/paper3/areachairs"], "cdate": 1485287766873}}}, {"tddate": null, "tmdate": 1480726090513, "tcdate": 1480726090507, "number": 3, "id": "SyzLL517l", "invitation": "ICLR.cc/2017/conference/-/paper3/public/comment", "forum": "BkCPyXm1l", "replyto": "BkQsA_CMe", "signatures": ["~Armen_Aghajanyan1"], "readers": ["everyone"], "writers": ["~Armen_Aghajanyan1"], "content": {"title": "Modeling and setup: reply", "comment": "Thank you for comments.\n\nI am editing the paper right now. I will include hyper-parameter optimization for soft-target, and also explanations for the choice of parameters. \n\nTo refer to the distillation paper, distillation was described as a way to transfer knowledge from large models to smaller models utilizing soft-targets. Soft-Target regularization can be thought of doing weighted distillation where the donor model is the state of the previous training, and the weighting target are the hard-targets.\n\nThank you for reading.  "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "SoftTarget Regularization: An Effective Technique to Reduce Over-Fitting in Neural Networks", "abstract": "Deep neural networks are learning models with a very high capacity and therefore prone to over-fitting. Many regularization techniques such as Dropout, DropConnect, and weight decay all attempt to solve the problem of over-fitting by reducing the capacity of their respective models (Srivastava et al., 2014), (Wan et al., 2013), (Krogh & Hertz, 1992). In this paper we introduce a new form of regularization that guides the learning problem in a way that reduces over-fitting without sacrificing the capacity of the model. The mistakes that models make in early stages of training carry information about the learning problem. By adjusting the labels of the current epoch of training through a weighted average of the real labels, and an exponential average of the past soft-targets we achieved a regularization scheme as powerful as Dropout without necessarily reducing the capacity of the model, and simplified the complexity of the learning problem. SoftTarget regularization proved to be an effective tool in various neural network architectures.", "pdf": "/pdf/defec2079755d4334e3a2f4bb456b672e8cba34a.pdf", "paperhash": "aghajanyan|softtarget_regularization_an_effective_technique_to_reduce_overfitting_in_neural_networks", "author_emails": "armen.ag@live.com", "conflicts": ["dimensionalmechanics.com"], "authors": ["Armen Aghajanyan"], "authorids": ["armen.ag@live.com"], "keywords": ["Deep learning", "Optimization", "Computer vision"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287766873, "id": "ICLR.cc/2017/conference/-/paper3/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BkCPyXm1l", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper3/reviewers", "ICLR.cc/2017/conference/paper3/areachairs"], "cdate": 1485287766873}}}, {"tddate": null, "tmdate": 1480722031200, "tcdate": 1480722031196, "number": 2, "id": "H1vO8K1Qe", "invitation": "ICLR.cc/2017/conference/-/paper3/pre-review/question", "forum": "BkCPyXm1l", "replyto": "BkCPyXm1l", "signatures": ["ICLR.cc/2017/conference/paper3/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper3/AnonReviewer2"], "content": {"title": "Comparison with the simple weight decay?", "question": "It seems the only baseline that author has compared against is dropout. Weight decay can be very effective regularizer for the negative log likelihood of a model. Author should at least do some comparison with the weight decay. \n\nAlso, ultimately we care about the classification accuracy of a model. A terrible negative log likelihood on the test set does not always translate to overfitting in terms of the classification performance. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "SoftTarget Regularization: An Effective Technique to Reduce Over-Fitting in Neural Networks", "abstract": "Deep neural networks are learning models with a very high capacity and therefore prone to over-fitting. Many regularization techniques such as Dropout, DropConnect, and weight decay all attempt to solve the problem of over-fitting by reducing the capacity of their respective models (Srivastava et al., 2014), (Wan et al., 2013), (Krogh & Hertz, 1992). In this paper we introduce a new form of regularization that guides the learning problem in a way that reduces over-fitting without sacrificing the capacity of the model. The mistakes that models make in early stages of training carry information about the learning problem. By adjusting the labels of the current epoch of training through a weighted average of the real labels, and an exponential average of the past soft-targets we achieved a regularization scheme as powerful as Dropout without necessarily reducing the capacity of the model, and simplified the complexity of the learning problem. SoftTarget regularization proved to be an effective tool in various neural network architectures.", "pdf": "/pdf/defec2079755d4334e3a2f4bb456b672e8cba34a.pdf", "paperhash": "aghajanyan|softtarget_regularization_an_effective_technique_to_reduce_overfitting_in_neural_networks", "author_emails": "armen.ag@live.com", "conflicts": ["dimensionalmechanics.com"], "authors": ["Armen Aghajanyan"], "authorids": ["armen.ag@live.com"], "keywords": ["Deep learning", "Optimization", "Computer vision"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959512455, "id": "ICLR.cc/2017/conference/-/paper3/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper3/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper3/AnonReviewer1", "ICLR.cc/2017/conference/paper3/AnonReviewer2"], "reply": {"forum": "BkCPyXm1l", "replyto": "BkCPyXm1l", "writers": {"values-regex": "ICLR.cc/2017/conference/paper3/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper3/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959512455}}}, {"tddate": null, "tmdate": 1480654490685, "tcdate": 1480654490680, "number": 3, "id": "BkQsA_CMe", "invitation": "ICLR.cc/2017/conference/-/paper3/official/comment", "forum": "BkCPyXm1l", "replyto": "BkCPyXm1l", "signatures": ["ICLR.cc/2017/conference/paper3/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper3/AnonReviewer3"], "content": {"title": "Modeling and setup", "comment": "\n- You definitely need to report misclassification error results on test data for obvious reasons related to losses and final test misclassification error. Currently comparisons are not conclusive.\n\n-  Can you explain better the reason for using the particular updates in (3) and (4) better? Why don't you do for example totally corrective update, e.g. take convex combination of all \\cal{F}'s (or some portion) up to current iteration in (3)? Therefore \\beta and \\gamma should be tuned reasonably well to see whether (3) and (4) is really helping or not and the range for cross validation should be reported.\n\n- The reason to set n_t n_b is not satisfactory.  It is crucial to cross-validate such parameters. Isn't  n_t = {1,2} unreasonably small number that can cause unstable results? why all n_b and n_t are equal?Are there results on other n_b and n_t's that were tried?\n\n- It is stated that colabel similarities disappear when network starts to overfit. However distillation ( Hinton et.al. ,2015 ) captures colabel similarities after training a model and using distillation. This method seems an iterative extension of distillation without using a bigger teacher model. Does proposed method gives better results then a two step version of distillation ?  \n\n- How do you tune \\lambda for weight decay? \n\n- From paper: \"We considered a frozen set of hyper-parameters for the SoftTarget regularization to show that SoftTarget regularization can still work without a having to conduct a large grid search\". This argument is not valid in ML, maybe if you did a reasonable search, you would get worse results (since you should not look test error until you finish the cross-validation).   Why a common hyper parameter tuning procedure is not used e.g. random search (Bergstra and Bengio, JMLR 2012) or Bayesian optimization (Snoek et al ,NIPS 2012) ?  Setting the hyper parameters to some numbers without searching a range or set can dramatically ruin fair comparison. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "SoftTarget Regularization: An Effective Technique to Reduce Over-Fitting in Neural Networks", "abstract": "Deep neural networks are learning models with a very high capacity and therefore prone to over-fitting. Many regularization techniques such as Dropout, DropConnect, and weight decay all attempt to solve the problem of over-fitting by reducing the capacity of their respective models (Srivastava et al., 2014), (Wan et al., 2013), (Krogh & Hertz, 1992). In this paper we introduce a new form of regularization that guides the learning problem in a way that reduces over-fitting without sacrificing the capacity of the model. The mistakes that models make in early stages of training carry information about the learning problem. By adjusting the labels of the current epoch of training through a weighted average of the real labels, and an exponential average of the past soft-targets we achieved a regularization scheme as powerful as Dropout without necessarily reducing the capacity of the model, and simplified the complexity of the learning problem. SoftTarget regularization proved to be an effective tool in various neural network architectures.", "pdf": "/pdf/defec2079755d4334e3a2f4bb456b672e8cba34a.pdf", "paperhash": "aghajanyan|softtarget_regularization_an_effective_technique_to_reduce_overfitting_in_neural_networks", "author_emails": "armen.ag@live.com", "conflicts": ["dimensionalmechanics.com"], "authors": ["Armen Aghajanyan"], "authorids": ["armen.ag@live.com"], "keywords": ["Deep learning", "Optimization", "Computer vision"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287766741, "id": "ICLR.cc/2017/conference/-/paper3/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "BkCPyXm1l", "writers": {"values-regex": "ICLR.cc/2017/conference/paper3/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper3/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper3/reviewers", "ICLR.cc/2017/conference/paper3/areachairs"], "cdate": 1485287766741}}}, {"tddate": null, "tmdate": 1480637717389, "tcdate": 1480637717384, "number": 2, "id": "SyazT4RGe", "invitation": "ICLR.cc/2017/conference/-/paper3/public/comment", "forum": "BkCPyXm1l", "replyto": "ByA7iECGx", "signatures": ["~Armen_Aghajanyan1"], "readers": ["everyone"], "writers": ["~Armen_Aghajanyan1"], "content": {"title": "Computational feasibility and comparisons: answers.", "comment": "1. In this case, you would only keep in memory the predicted classes the subset of images from ImageNet that are contained in memory at that time.\n\n2. I agree with you on this point. SoftTarget with stopping would be more optimal as seen from the minimal loss.\n\n3. I see what you mean. I will update the paper to include accuracy rates as well. The majority of architectures picked were from standard papers (such as the ResNet), others were standard convolution -> pooling networks that are usually applied as baseline architectures. I will do some edits and will update the paper.\n\nThank you once again for the comments.\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "SoftTarget Regularization: An Effective Technique to Reduce Over-Fitting in Neural Networks", "abstract": "Deep neural networks are learning models with a very high capacity and therefore prone to over-fitting. Many regularization techniques such as Dropout, DropConnect, and weight decay all attempt to solve the problem of over-fitting by reducing the capacity of their respective models (Srivastava et al., 2014), (Wan et al., 2013), (Krogh & Hertz, 1992). In this paper we introduce a new form of regularization that guides the learning problem in a way that reduces over-fitting without sacrificing the capacity of the model. The mistakes that models make in early stages of training carry information about the learning problem. By adjusting the labels of the current epoch of training through a weighted average of the real labels, and an exponential average of the past soft-targets we achieved a regularization scheme as powerful as Dropout without necessarily reducing the capacity of the model, and simplified the complexity of the learning problem. SoftTarget regularization proved to be an effective tool in various neural network architectures.", "pdf": "/pdf/defec2079755d4334e3a2f4bb456b672e8cba34a.pdf", "paperhash": "aghajanyan|softtarget_regularization_an_effective_technique_to_reduce_overfitting_in_neural_networks", "author_emails": "armen.ag@live.com", "conflicts": ["dimensionalmechanics.com"], "authors": ["Armen Aghajanyan"], "authorids": ["armen.ag@live.com"], "keywords": ["Deep learning", "Optimization", "Computer vision"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287766873, "id": "ICLR.cc/2017/conference/-/paper3/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BkCPyXm1l", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper3/reviewers", "ICLR.cc/2017/conference/paper3/areachairs"], "cdate": 1485287766873}}}, {"tddate": null, "tmdate": 1480637221961, "tcdate": 1480637221955, "number": 2, "id": "ByA7iECGx", "invitation": "ICLR.cc/2017/conference/-/paper3/official/comment", "forum": "BkCPyXm1l", "replyto": "S1ClVE0fg", "signatures": ["ICLR.cc/2017/conference/paper3/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper3/AnonReviewer1"], "content": {"title": "Replies to author response", "comment": "1. As I understand it you accumulate all synthetic targets for the entire training set and then engage in an epoch of training. Is this not the case? (If it isn't the case and you are generating de novo targets on-the-fly as you process each minibatch, then are you backpropagating through the targets in the weighting term?). \n\nIn the ImageNet case, this amounts to about 5 GB of storage in float32 that needs to be frequently updated (and therefore is most practically stored in memory), whereas training data is usually streamed read-only from disk in such a setting.\n\n2. I agree that early stopping may not be sufficient on its own (for this choice of architecture and hyperparameters) but early stopping is usually combined with other regularization methods where necessary.\n\n3. Average test loss can get bigger without a corresponding increase in misclassification rate if the model simply becomes more confident in a few of its already incorrect predictions. Thus I'd consider it important to compare test error, for practical purposes.\n\nRelatedly, the paper would be stronger if efficacy were demonstrated on architectures that aren't arbitrarily introduced in this paper but have been drawn from the literature and were chosen by previous authors for their favorable performance in the control setting. With that baseline established, a gain in accuracy with the same architecture (or in test loss without a noticeable decrease in accuracy), or a gain in these metrics with an architecture that is somehow preferable (less computation, less parameters, etc.) would solidify the empirical claim."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "SoftTarget Regularization: An Effective Technique to Reduce Over-Fitting in Neural Networks", "abstract": "Deep neural networks are learning models with a very high capacity and therefore prone to over-fitting. Many regularization techniques such as Dropout, DropConnect, and weight decay all attempt to solve the problem of over-fitting by reducing the capacity of their respective models (Srivastava et al., 2014), (Wan et al., 2013), (Krogh & Hertz, 1992). In this paper we introduce a new form of regularization that guides the learning problem in a way that reduces over-fitting without sacrificing the capacity of the model. The mistakes that models make in early stages of training carry information about the learning problem. By adjusting the labels of the current epoch of training through a weighted average of the real labels, and an exponential average of the past soft-targets we achieved a regularization scheme as powerful as Dropout without necessarily reducing the capacity of the model, and simplified the complexity of the learning problem. SoftTarget regularization proved to be an effective tool in various neural network architectures.", "pdf": "/pdf/defec2079755d4334e3a2f4bb456b672e8cba34a.pdf", "paperhash": "aghajanyan|softtarget_regularization_an_effective_technique_to_reduce_overfitting_in_neural_networks", "author_emails": "armen.ag@live.com", "conflicts": ["dimensionalmechanics.com"], "authors": ["Armen Aghajanyan"], "authorids": ["armen.ag@live.com"], "keywords": ["Deep learning", "Optimization", "Computer vision"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287766741, "id": "ICLR.cc/2017/conference/-/paper3/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "BkCPyXm1l", "writers": {"values-regex": "ICLR.cc/2017/conference/paper3/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper3/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper3/reviewers", "ICLR.cc/2017/conference/paper3/areachairs"], "cdate": 1485287766741}}}, {"tddate": null, "tmdate": 1480635382513, "tcdate": 1480635382508, "number": 1, "id": "S1ClVE0fg", "invitation": "ICLR.cc/2017/conference/-/paper3/public/comment", "forum": "BkCPyXm1l", "replyto": "S1ayxXAfx", "signatures": ["~Armen_Aghajanyan1"], "readers": ["everyone"], "writers": ["~Armen_Aghajanyan1"], "content": {"title": "Computational feasibility and comparisons: answers.", "comment": "Thank you for reading the paper. \n\n1. The memory growth is rather marginal as the size we have to store in memory is equal to (# of samples kept in memory, # of classes). When training large networks on a large amount of data (such as ImageNet) data is usually fed in, in the form of a generator, and therefore is soft on memory. With respect to the amount memory images takes (# of samples kept in memory, # of channels, row size,column size), this amount of memory is marginal. Optimization routines that store past gradients take up much more memory than our approach.\n\n2. This is a very good question. Early stopping is an effective technique to stop overfitting but tends to produce suboptimal results. This is why methods such as dropout/drop connect have been really popular. Our experiments also show that the results obtained with soft-target regularization have been better than standard vanilla networks (check out the minimum loss on all the tasks). \n\n3. The reason the paper only shows loss results and not accuracy is because the loss is what we are directly optimizing over, even though it acts as a surrogate for accuracy. This was a conscious decision not to include accuracy in the paper. \n\nWould you suggest I include the accuracy graphs in the paper?\n\nThank you again for reading my paper.\nArmen."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "SoftTarget Regularization: An Effective Technique to Reduce Over-Fitting in Neural Networks", "abstract": "Deep neural networks are learning models with a very high capacity and therefore prone to over-fitting. Many regularization techniques such as Dropout, DropConnect, and weight decay all attempt to solve the problem of over-fitting by reducing the capacity of their respective models (Srivastava et al., 2014), (Wan et al., 2013), (Krogh & Hertz, 1992). In this paper we introduce a new form of regularization that guides the learning problem in a way that reduces over-fitting without sacrificing the capacity of the model. The mistakes that models make in early stages of training carry information about the learning problem. By adjusting the labels of the current epoch of training through a weighted average of the real labels, and an exponential average of the past soft-targets we achieved a regularization scheme as powerful as Dropout without necessarily reducing the capacity of the model, and simplified the complexity of the learning problem. SoftTarget regularization proved to be an effective tool in various neural network architectures.", "pdf": "/pdf/defec2079755d4334e3a2f4bb456b672e8cba34a.pdf", "paperhash": "aghajanyan|softtarget_regularization_an_effective_technique_to_reduce_overfitting_in_neural_networks", "author_emails": "armen.ag@live.com", "conflicts": ["dimensionalmechanics.com"], "authors": ["Armen Aghajanyan"], "authorids": ["armen.ag@live.com"], "keywords": ["Deep learning", "Optimization", "Computer vision"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287766873, "id": "ICLR.cc/2017/conference/-/paper3/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BkCPyXm1l", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper3/reviewers", "ICLR.cc/2017/conference/paper3/areachairs"], "cdate": 1485287766873}}}, {"tddate": null, "tmdate": 1480630245239, "tcdate": 1480630245231, "number": 1, "id": "S1ayxXAfx", "invitation": "ICLR.cc/2017/conference/-/paper3/pre-review/question", "forum": "BkCPyXm1l", "replyto": "BkCPyXm1l", "signatures": ["ICLR.cc/2017/conference/paper3/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper3/AnonReviewer1"], "content": {"title": "Computational feasibility and comparisons", "question": "- The method as proposed requires memory that grows linearly both with the number of training samples and the output dimensionality (i.e. number of target classes). How would you propose to scale this to large datasets?\n- A commonly used regularization scheme is early stopping, whereby you monitor a validation objective of interest, persist the parameters (e.g. to disk) whenever a sufficiently good new configuration is found, and terminate training when the validation objective either stagnates or gets worse. What advantage does this method offer over the conceptually simpler (and computationally less demanding) early stopping formulation?\n- Your benchmarks involve exclusively classification. Does this method meaningfully impact test set *accuracy*? The value of the loss function on test data is less informative close to a minimum of the empirical loss as the model doubling down on its confidence in a few wrong answers will cause its test loss to rise but not its test misclassification error. In general, a lack of test error reporting makes this work incomparable to the literature."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "SoftTarget Regularization: An Effective Technique to Reduce Over-Fitting in Neural Networks", "abstract": "Deep neural networks are learning models with a very high capacity and therefore prone to over-fitting. Many regularization techniques such as Dropout, DropConnect, and weight decay all attempt to solve the problem of over-fitting by reducing the capacity of their respective models (Srivastava et al., 2014), (Wan et al., 2013), (Krogh & Hertz, 1992). In this paper we introduce a new form of regularization that guides the learning problem in a way that reduces over-fitting without sacrificing the capacity of the model. The mistakes that models make in early stages of training carry information about the learning problem. By adjusting the labels of the current epoch of training through a weighted average of the real labels, and an exponential average of the past soft-targets we achieved a regularization scheme as powerful as Dropout without necessarily reducing the capacity of the model, and simplified the complexity of the learning problem. SoftTarget regularization proved to be an effective tool in various neural network architectures.", "pdf": "/pdf/defec2079755d4334e3a2f4bb456b672e8cba34a.pdf", "paperhash": "aghajanyan|softtarget_regularization_an_effective_technique_to_reduce_overfitting_in_neural_networks", "author_emails": "armen.ag@live.com", "conflicts": ["dimensionalmechanics.com"], "authors": ["Armen Aghajanyan"], "authorids": ["armen.ag@live.com"], "keywords": ["Deep learning", "Optimization", "Computer vision"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959512455, "id": "ICLR.cc/2017/conference/-/paper3/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper3/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper3/AnonReviewer1", "ICLR.cc/2017/conference/paper3/AnonReviewer2"], "reply": {"forum": "BkCPyXm1l", "replyto": "BkCPyXm1l", "writers": {"values-regex": "ICLR.cc/2017/conference/paper3/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper3/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959512455}}}], "count": 21}