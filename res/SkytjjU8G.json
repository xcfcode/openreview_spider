{"notes": [{"tddate": null, "ddate": null, "original": null, "tmdate": 1521582807512, "tcdate": 1520619561070, "number": 1, "cdate": 1520619561070, "id": "By-VeLgYf", "invitation": "ICLR.cc/2018/Workshop/-/Paper23/Official_Review", "forum": "SkytjjU8G", "replyto": "SkytjjU8G", "signatures": ["ICLR.cc/2018/Workshop/Paper23/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper23/AnonReviewer1"], "content": {"title": "Nice idea, but it's been done before", "rating": "4: Ok but not good enough - rejection", "review": "This paper proposes dynamically increasing the minibatch size on a fixed schedule, since at early iterations noisy gradient estimates are fine, since the noise is smaller relative to the potential improvement, whereas in later iterations noise has a bigger impact. This makes the earlier iterations faster, potentially speeding up convergence, which they demonstrate experimentally. This is a nice, intuitive idea, and I like the simplicity of performing the increase on a fixed schedule.\n\nHowever, the KDD 2017 paper \"Small Batch or Large Batch? Gaussian Walk with Rebound can Teach\", by Yin, Luo and Nakamura, proposes the same idea, except that they dynamically increase the batch size based on a heuristic (for which they give some theoretical justification). It may be that a fixed schedule is better--it's certainly simpler--but I think that a comparison must be made to this previous work.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "AdaBatch: Adaptive Batch Sizes for Training Deep Neural Networks", "abstract": "We introduce a new deep learning training approach that adaptively increases the batch size during the training process. Our method delivers the convergence rate of small, fixed batch sizes while achieving performance similar to large, fixed batch sizes. We train the VGG and ResNet networks on the CIFAR-100 and ImageNet datasets. Our results show that learning with adaptive batch sizes can improve performance by factors of up to 6.25 on 4 NVIDIA Tesla P100 GPUs while attaining similar accuracies to small batch sizes. Using our technique, we are able to train ImageNet with batch sizes up to 524, 288.", "pdf": "/pdf/f14fb2a8d1f12e7b741664a3a42e8cc459248c92.pdf", "TL;DR": "The batch size during CNN training can be adaptively increased to yield better performance and obtain similar accuracies to fixed batch size training.", "paperhash": "devarakonda|adabatch_adaptive_batch_sizes_for_training_deep_neural_networks", "keywords": ["adaptive batch sizes", "convolutional neural networks"], "authors": ["Aditya Devarakonda", "Maxim Naumov", "Michael Garland"], "authorids": ["aditya@eecs.berkeley.edu", "mnaumov@nvidia.com", "mgarland@nvidia.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582807316, "id": "ICLR.cc/2018/Workshop/-/Paper23/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper23/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper23/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper23/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper23/AnonReviewer3"], "reply": {"forum": "SkytjjU8G", "replyto": "SkytjjU8G", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper23/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper23/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582807316}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582796284, "tcdate": 1520625738480, "number": 2, "cdate": 1520625738480, "id": "SkzI_DlYG", "invitation": "ICLR.cc/2018/Workshop/-/Paper23/Official_Review", "forum": "SkytjjU8G", "replyto": "SkytjjU8G", "signatures": ["ICLR.cc/2018/Workshop/Paper23/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper23/AnonReviewer2"], "content": {"title": "Good paper with presentational issues", "rating": "7: Good paper, accept", "review": "The authors present a clean method (double step size and batch size when otherwise decreasing step size) that works well. \n\nThere are some presentational / writing issues.\n\nIn figure 1, I'd like to see the test error values in a table, not as giant black circles. I can't figure out the test error of the baseline without holding a ruler to my screen. There's enough whitespace around those charts to fit in a table without increasing the paper length.\n\nI find your formulas in section 2 confusing / nonsensical. \n- What is the update matrix \\Delta W? Either define this matrix or write the update equation in terms of gradients directly.\n- Why does a single large batch iteration compute a sum of \\beta update matrices? The large-batch method doesn't know its batch size was recently increased by a factor \\beta, it only knows its current batch size. Hence, the update scheme for large-batch should not depend on \\beta.\n- I think the i' subscripts make no sense. In the original definition of W_{i+q}, the subscript of the update matrices i+j ranges from i to i+q. But in the definition of W_{i+\\tilde{q}}, the indices of the update matrices are independent of i and range from 0 to q? Something seems to be going wrong here.\n- The statement \\Delta W_i \\approx \\Delta W_{i'} also makes no sense to me. i is a fixed subscript indicating the epoch. i' is a running subscript indicating individual iterations. Why are you comparing the two?\n\nYour method can be summed up simply as follows: \n\n\"Whenever we would usually decay the learning rate by factor k, instead we decay it by k/c and increase the batch size by c.\"\n\nI think a sentence like this that lets the reader know what you are doing should appear in section 2. I had to re-read section 3 multiple times to realize that this is what you are doing. Your method is essentially only specified implicitly via your desciprtion of the experimental protocol in section 3.\n\n\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "AdaBatch: Adaptive Batch Sizes for Training Deep Neural Networks", "abstract": "We introduce a new deep learning training approach that adaptively increases the batch size during the training process. Our method delivers the convergence rate of small, fixed batch sizes while achieving performance similar to large, fixed batch sizes. We train the VGG and ResNet networks on the CIFAR-100 and ImageNet datasets. Our results show that learning with adaptive batch sizes can improve performance by factors of up to 6.25 on 4 NVIDIA Tesla P100 GPUs while attaining similar accuracies to small batch sizes. Using our technique, we are able to train ImageNet with batch sizes up to 524, 288.", "pdf": "/pdf/f14fb2a8d1f12e7b741664a3a42e8cc459248c92.pdf", "TL;DR": "The batch size during CNN training can be adaptively increased to yield better performance and obtain similar accuracies to fixed batch size training.", "paperhash": "devarakonda|adabatch_adaptive_batch_sizes_for_training_deep_neural_networks", "keywords": ["adaptive batch sizes", "convolutional neural networks"], "authors": ["Aditya Devarakonda", "Maxim Naumov", "Michael Garland"], "authorids": ["aditya@eecs.berkeley.edu", "mnaumov@nvidia.com", "mgarland@nvidia.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582807316, "id": "ICLR.cc/2018/Workshop/-/Paper23/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper23/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper23/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper23/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper23/AnonReviewer3"], "reply": {"forum": "SkytjjU8G", "replyto": "SkytjjU8G", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper23/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper23/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582807316}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582751337, "tcdate": 1520653126792, "number": 3, "cdate": 1520653126792, "id": "Bk18XRgtz", "invitation": "ICLR.cc/2018/Workshop/-/Paper23/Official_Review", "forum": "SkytjjU8G", "replyto": "SkytjjU8G", "signatures": ["ICLR.cc/2018/Workshop/Paper23/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper23/AnonReviewer3"], "content": {"title": "Not any new insights", "rating": "4: Ok but not good enough - rejection", "review": "The relationship between changing batch sizes and learning rate decay is well understood as pointed out by authors themselves. And the results shown in the plots show typical patterns. Although a valid study and correct idea, the paper does not offer new insights into the working of sgd or otherwise. ", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "AdaBatch: Adaptive Batch Sizes for Training Deep Neural Networks", "abstract": "We introduce a new deep learning training approach that adaptively increases the batch size during the training process. Our method delivers the convergence rate of small, fixed batch sizes while achieving performance similar to large, fixed batch sizes. We train the VGG and ResNet networks on the CIFAR-100 and ImageNet datasets. Our results show that learning with adaptive batch sizes can improve performance by factors of up to 6.25 on 4 NVIDIA Tesla P100 GPUs while attaining similar accuracies to small batch sizes. Using our technique, we are able to train ImageNet with batch sizes up to 524, 288.", "pdf": "/pdf/f14fb2a8d1f12e7b741664a3a42e8cc459248c92.pdf", "TL;DR": "The batch size during CNN training can be adaptively increased to yield better performance and obtain similar accuracies to fixed batch size training.", "paperhash": "devarakonda|adabatch_adaptive_batch_sizes_for_training_deep_neural_networks", "keywords": ["adaptive batch sizes", "convolutional neural networks"], "authors": ["Aditya Devarakonda", "Maxim Naumov", "Michael Garland"], "authorids": ["aditya@eecs.berkeley.edu", "mnaumov@nvidia.com", "mgarland@nvidia.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582807316, "id": "ICLR.cc/2018/Workshop/-/Paper23/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper23/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper23/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper23/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper23/AnonReviewer3"], "reply": {"forum": "SkytjjU8G", "replyto": "SkytjjU8G", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper23/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper23/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582807316}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521573594706, "tcdate": 1521573594706, "number": 221, "cdate": 1521573594368, "id": "HJQ111kqM", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "SkytjjU8G", "replyto": "SkytjjU8G", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Reject", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "Based on the reviews, this paper has not been accepted for presentation at the ICLR workshop. However, the conversation and updates can continue to appear here on OpenReview."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "AdaBatch: Adaptive Batch Sizes for Training Deep Neural Networks", "abstract": "We introduce a new deep learning training approach that adaptively increases the batch size during the training process. Our method delivers the convergence rate of small, fixed batch sizes while achieving performance similar to large, fixed batch sizes. We train the VGG and ResNet networks on the CIFAR-100 and ImageNet datasets. Our results show that learning with adaptive batch sizes can improve performance by factors of up to 6.25 on 4 NVIDIA Tesla P100 GPUs while attaining similar accuracies to small batch sizes. Using our technique, we are able to train ImageNet with batch sizes up to 524, 288.", "pdf": "/pdf/f14fb2a8d1f12e7b741664a3a42e8cc459248c92.pdf", "TL;DR": "The batch size during CNN training can be adaptively increased to yield better performance and obtain similar accuracies to fixed batch size training.", "paperhash": "devarakonda|adabatch_adaptive_batch_sizes_for_training_deep_neural_networks", "keywords": ["adaptive batch sizes", "convolutional neural networks"], "authors": ["Aditya Devarakonda", "Maxim Naumov", "Michael Garland"], "authorids": ["aditya@eecs.berkeley.edu", "mnaumov@nvidia.com", "mgarland@nvidia.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1517890422936, "tcdate": 1517890422936, "number": 23, "cdate": 1517890422936, "id": "SkytjjU8G", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "SkytjjU8G", "signatures": ["~Aditya_Devarakonda1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "AdaBatch: Adaptive Batch Sizes for Training Deep Neural Networks", "abstract": "We introduce a new deep learning training approach that adaptively increases the batch size during the training process. Our method delivers the convergence rate of small, fixed batch sizes while achieving performance similar to large, fixed batch sizes. We train the VGG and ResNet networks on the CIFAR-100 and ImageNet datasets. Our results show that learning with adaptive batch sizes can improve performance by factors of up to 6.25 on 4 NVIDIA Tesla P100 GPUs while attaining similar accuracies to small batch sizes. Using our technique, we are able to train ImageNet with batch sizes up to 524, 288.", "pdf": "/pdf/f14fb2a8d1f12e7b741664a3a42e8cc459248c92.pdf", "TL;DR": "The batch size during CNN training can be adaptively increased to yield better performance and obtain similar accuracies to fixed batch size training.", "paperhash": "devarakonda|adabatch_adaptive_batch_sizes_for_training_deep_neural_networks", "keywords": ["adaptive batch sizes", "convolutional neural networks"], "authors": ["Aditya Devarakonda", "Maxim Naumov", "Michael Garland"], "authorids": ["aditya@eecs.berkeley.edu", "mnaumov@nvidia.com", "mgarland@nvidia.com"]}, "nonreaders": [], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}}], "count": 5}