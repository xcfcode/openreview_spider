{"notes": [{"id": "Syg6jTNtDH", "original": "S1gpcYkOPr", "number": 762, "cdate": 1569439141360, "ddate": null, "tcdate": 1569439141360, "tmdate": 1577168242683, "tddate": null, "forum": "Syg6jTNtDH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Learning Numeral Embedding", "authors": ["Chengyue Jiang", "Zhonglin Nian", "Kaihao Guo", "Shanbo Chu", "Yinggong Zhao", "Libin Shen", "Kewei Tu"], "authorids": ["jiangchy@shanghaitech.edu.cn", "nianzhl@shanghaitech.edu.cn", "guokh@shanghaitech.edu.cn", "chushb@leyantech.com", "ygzhao@leyantech.com", "libin@leyantech.com", "tukw@shanghaitech.edu.cn"], "keywords": ["Natural Language Processing", "Numeral Embedding", "Word Embedding", "Out-of-vocabulary Problem"], "TL;DR": "We propose two methods for learning better numeral embeddings that solve the numeral out-of-vocabulary (OOV) problem and can be integrated into traditional word embedding training methods. ", "abstract": "Word embedding is an essential building block for deep learning methods for natural language processing. Although word embedding has been extensively studied over the years, the problem of how to effectively embed numerals, a special subset of words, is still underexplored. Existing word embedding methods do not learn numeral embeddings well because there are an infinite number of numerals and their individual appearances in training corpora are highly scarce.\nIn this paper, we propose two novel numeral embedding methods that can handle the out-of-vocabulary (OOV) problem for numerals. We first induce a finite set of prototype numerals using either a self-organizing map or a Gaussian mixture model. We then represent the embedding of a numeral as a weighted average of the prototype number embeddings. Numeral embeddings represented in this manner can be plugged into existing word embedding learning approaches such as skip-gram for training.\nWe evaluated our methods and showed its effectiveness on four intrinsic and extrinsic tasks: word similarity, embedding numeracy, numeral prediction, and sequence labeling. ", "pdf": "/pdf/4a076dd3baa83d984e8174e2e86ba83231e6705e.pdf", "paperhash": "jiang|learning_numeral_embedding", "original_pdf": "/attachment/29df16541d895beea2feac02a763249c1149b011.pdf", "_bibtex": "@misc{\njiang2020learning,\ntitle={Learning Numeral Embedding},\nauthor={Chengyue Jiang and Zhonglin Nian and Kaihao Guo and Shanbo Chu and Yinggong Zhao and Libin Shen and Kewei Tu},\nyear={2020},\nurl={https://openreview.net/forum?id=Syg6jTNtDH}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "KvC6pvYjWM", "original": null, "number": 1, "cdate": 1576798705363, "ddate": null, "tcdate": 1576798705363, "tmdate": 1576800930757, "tddate": null, "forum": "Syg6jTNtDH", "replyto": "Syg6jTNtDH", "invitation": "ICLR.cc/2020/Conference/Paper762/-/Decision", "content": {"decision": "Reject", "comment": "This paper proposes better methods to handle numerals within word embeddings.\n\nOverall, my impression is that this paper is solid, but not super-exciting. The scope is a little bit limited (to only numbers), and it is not by any means the first paper to handle understanding numbers within word embeddings. A more thorough theoretical and empirical comparison to other methods, e.g. Spithourakis & Riedel (2018) and Chen et al. (2019), could bring the paper a long way.\n\nI think this paper is somewhat borderline, but am recommending not to accept because I feel that the paper could be greatly improved by making the above-mentioned comparisons more complete, and thus this could find a better place as a better paper in a new venue.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Numeral Embedding", "authors": ["Chengyue Jiang", "Zhonglin Nian", "Kaihao Guo", "Shanbo Chu", "Yinggong Zhao", "Libin Shen", "Kewei Tu"], "authorids": ["jiangchy@shanghaitech.edu.cn", "nianzhl@shanghaitech.edu.cn", "guokh@shanghaitech.edu.cn", "chushb@leyantech.com", "ygzhao@leyantech.com", "libin@leyantech.com", "tukw@shanghaitech.edu.cn"], "keywords": ["Natural Language Processing", "Numeral Embedding", "Word Embedding", "Out-of-vocabulary Problem"], "TL;DR": "We propose two methods for learning better numeral embeddings that solve the numeral out-of-vocabulary (OOV) problem and can be integrated into traditional word embedding training methods. ", "abstract": "Word embedding is an essential building block for deep learning methods for natural language processing. Although word embedding has been extensively studied over the years, the problem of how to effectively embed numerals, a special subset of words, is still underexplored. Existing word embedding methods do not learn numeral embeddings well because there are an infinite number of numerals and their individual appearances in training corpora are highly scarce.\nIn this paper, we propose two novel numeral embedding methods that can handle the out-of-vocabulary (OOV) problem for numerals. We first induce a finite set of prototype numerals using either a self-organizing map or a Gaussian mixture model. We then represent the embedding of a numeral as a weighted average of the prototype number embeddings. Numeral embeddings represented in this manner can be plugged into existing word embedding learning approaches such as skip-gram for training.\nWe evaluated our methods and showed its effectiveness on four intrinsic and extrinsic tasks: word similarity, embedding numeracy, numeral prediction, and sequence labeling. ", "pdf": "/pdf/4a076dd3baa83d984e8174e2e86ba83231e6705e.pdf", "paperhash": "jiang|learning_numeral_embedding", "original_pdf": "/attachment/29df16541d895beea2feac02a763249c1149b011.pdf", "_bibtex": "@misc{\njiang2020learning,\ntitle={Learning Numeral Embedding},\nauthor={Chengyue Jiang and Zhonglin Nian and Kaihao Guo and Shanbo Chu and Yinggong Zhao and Libin Shen and Kewei Tu},\nyear={2020},\nurl={https://openreview.net/forum?id=Syg6jTNtDH}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "Syg6jTNtDH", "replyto": "Syg6jTNtDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795708329, "tmdate": 1576800256723, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper762/-/Decision"}}}, {"id": "B1lamhMaYr", "original": null, "number": 2, "cdate": 1571789860759, "ddate": null, "tcdate": 1571789860759, "tmdate": 1574198872783, "tddate": null, "forum": "Syg6jTNtDH", "replyto": "Syg6jTNtDH", "invitation": "ICLR.cc/2020/Conference/Paper762/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #1", "review": "I have read the author response.  Thank you for responding to my concerns.\n\nOriginal review:\nThis paper presents a word embedding approach for numbers.  The method is based on finding prototype numbers, and then representing numbers as a weighted average of the prototype embeddings, where the weights are based on numeric proximity.  The approach provides some gains over baselines on number similarity, number prediction, and sequence tagging tasks.  While modeling numbers is an interesting task, several aspects of the paper needed more clarification, and the paper\u2019s focus may be somewhat narrow for the ICLR audience.\n\nI think the paper could use a better motivation for the prototype-based approach.  In particular, the fact that the approach uses only quantity (and not the form of numbers) to represent similarity is contrary to my intuition.  For example, I would expect 1960 and 1960.1 to behave very differently in text, because one is a year and the other isn\u2019t.  But the proposed method, if I understand it correctly, would give them similar embeddings because they are very close numerically.\n\nI was not able to understand the SOM portion of the method, it is not self-contained within this paper.\n\nOn the Numeracy-600K data set, Chen et al. (2019) shows much higher F1 results than those shown here.  What explains this difference?\n\nThe proposed method seems relatively similar to that of (Spithourakis & Riedel, 2018), in that it exposes numeric quantity to the language/embedding model, and uses GMMs to represent numeral distributions.  More clarity about how this approach compares to that one (and others) would be helpful.  Also, how does the (Spithourakis & Riedel, 2018) approach fare on e.g. the number prediction tasks in the submission?  It is true that the submission's approach produces general-purpose embeddings that can be re-used, unlike (Spithourakis & Riedel, 2018).  But we would like to know whether that generality comes at the cost of performance on tasks, and if so how much of a cost.\n\nMinor:\nThe Lund and Burgess reference seems incorrect.  I don\u2019t see that those authors published a paper by that title in Brain and Cognition in 1996.\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper762/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper762/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Numeral Embedding", "authors": ["Chengyue Jiang", "Zhonglin Nian", "Kaihao Guo", "Shanbo Chu", "Yinggong Zhao", "Libin Shen", "Kewei Tu"], "authorids": ["jiangchy@shanghaitech.edu.cn", "nianzhl@shanghaitech.edu.cn", "guokh@shanghaitech.edu.cn", "chushb@leyantech.com", "ygzhao@leyantech.com", "libin@leyantech.com", "tukw@shanghaitech.edu.cn"], "keywords": ["Natural Language Processing", "Numeral Embedding", "Word Embedding", "Out-of-vocabulary Problem"], "TL;DR": "We propose two methods for learning better numeral embeddings that solve the numeral out-of-vocabulary (OOV) problem and can be integrated into traditional word embedding training methods. ", "abstract": "Word embedding is an essential building block for deep learning methods for natural language processing. Although word embedding has been extensively studied over the years, the problem of how to effectively embed numerals, a special subset of words, is still underexplored. Existing word embedding methods do not learn numeral embeddings well because there are an infinite number of numerals and their individual appearances in training corpora are highly scarce.\nIn this paper, we propose two novel numeral embedding methods that can handle the out-of-vocabulary (OOV) problem for numerals. We first induce a finite set of prototype numerals using either a self-organizing map or a Gaussian mixture model. We then represent the embedding of a numeral as a weighted average of the prototype number embeddings. Numeral embeddings represented in this manner can be plugged into existing word embedding learning approaches such as skip-gram for training.\nWe evaluated our methods and showed its effectiveness on four intrinsic and extrinsic tasks: word similarity, embedding numeracy, numeral prediction, and sequence labeling. ", "pdf": "/pdf/4a076dd3baa83d984e8174e2e86ba83231e6705e.pdf", "paperhash": "jiang|learning_numeral_embedding", "original_pdf": "/attachment/29df16541d895beea2feac02a763249c1149b011.pdf", "_bibtex": "@misc{\njiang2020learning,\ntitle={Learning Numeral Embedding},\nauthor={Chengyue Jiang and Zhonglin Nian and Kaihao Guo and Shanbo Chu and Yinggong Zhao and Libin Shen and Kewei Tu},\nyear={2020},\nurl={https://openreview.net/forum?id=Syg6jTNtDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Syg6jTNtDH", "replyto": "Syg6jTNtDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper762/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper762/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575413068247, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper762/Reviewers"], "noninvitees": [], "tcdate": 1570237747461, "tmdate": 1575413068260, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper762/-/Official_Review"}}}, {"id": "SyxgFyX2oH", "original": null, "number": 6, "cdate": 1573822328465, "ddate": null, "tcdate": 1573822328465, "tmdate": 1573822328465, "tddate": null, "forum": "Syg6jTNtDH", "replyto": "Syg6jTNtDH", "invitation": "ICLR.cc/2020/Conference/Paper762/-/Official_Comment", "content": {"title": "Short summary for paper revision ", "comment": "ICLR 2020 Conference Paper762 AuthorsChengyue Jiang(privately revealed to you)\n15 Nov 2019 (modified: 15 Nov 2019)ICLR 2020 Conference Paper762 Official CommentReaders:  Everyone\nComment: We have updated our paper in response to the reviews with the following revisions:\n1. We described a new experiment of probing tests using neural networks in Section 4.3 and Appendix D.\n2. We added why squashing is necessary in Section 3.1.\n3. We clarified the SOM method in Section 3.2.\n4. We discussed numeral polysemy (e.g., \"2019\") in Section 5. \n5. We added the results of sequence labeling with standard deviations in Appendix H.\n6. We changed the reference for Lund & Burgess (1996) and added the reference for t-SNE (Maaten & Hinton, 2008).\n\nIn addition, we are still in the process of applying the language model of Spithourakis & Riedel, 2018 to our numeral prediction task and will include the results in the final version of the paper."}, "signatures": ["ICLR.cc/2020/Conference/Paper762/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper762/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Numeral Embedding", "authors": ["Chengyue Jiang", "Zhonglin Nian", "Kaihao Guo", "Shanbo Chu", "Yinggong Zhao", "Libin Shen", "Kewei Tu"], "authorids": ["jiangchy@shanghaitech.edu.cn", "nianzhl@shanghaitech.edu.cn", "guokh@shanghaitech.edu.cn", "chushb@leyantech.com", "ygzhao@leyantech.com", "libin@leyantech.com", "tukw@shanghaitech.edu.cn"], "keywords": ["Natural Language Processing", "Numeral Embedding", "Word Embedding", "Out-of-vocabulary Problem"], "TL;DR": "We propose two methods for learning better numeral embeddings that solve the numeral out-of-vocabulary (OOV) problem and can be integrated into traditional word embedding training methods. ", "abstract": "Word embedding is an essential building block for deep learning methods for natural language processing. Although word embedding has been extensively studied over the years, the problem of how to effectively embed numerals, a special subset of words, is still underexplored. Existing word embedding methods do not learn numeral embeddings well because there are an infinite number of numerals and their individual appearances in training corpora are highly scarce.\nIn this paper, we propose two novel numeral embedding methods that can handle the out-of-vocabulary (OOV) problem for numerals. We first induce a finite set of prototype numerals using either a self-organizing map or a Gaussian mixture model. We then represent the embedding of a numeral as a weighted average of the prototype number embeddings. Numeral embeddings represented in this manner can be plugged into existing word embedding learning approaches such as skip-gram for training.\nWe evaluated our methods and showed its effectiveness on four intrinsic and extrinsic tasks: word similarity, embedding numeracy, numeral prediction, and sequence labeling. ", "pdf": "/pdf/4a076dd3baa83d984e8174e2e86ba83231e6705e.pdf", "paperhash": "jiang|learning_numeral_embedding", "original_pdf": "/attachment/29df16541d895beea2feac02a763249c1149b011.pdf", "_bibtex": "@misc{\njiang2020learning,\ntitle={Learning Numeral Embedding},\nauthor={Chengyue Jiang and Zhonglin Nian and Kaihao Guo and Shanbo Chu and Yinggong Zhao and Libin Shen and Kewei Tu},\nyear={2020},\nurl={https://openreview.net/forum?id=Syg6jTNtDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Syg6jTNtDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper762/Authors", "ICLR.cc/2020/Conference/Paper762/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper762/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper762/Reviewers", "ICLR.cc/2020/Conference/Paper762/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper762/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper762/Authors|ICLR.cc/2020/Conference/Paper762/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504166608, "tmdate": 1576860550123, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper762/Authors", "ICLR.cc/2020/Conference/Paper762/Reviewers", "ICLR.cc/2020/Conference/Paper762/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper762/-/Official_Comment"}}}, {"id": "Byxsm63YjB", "original": null, "number": 1, "cdate": 1573666083394, "ddate": null, "tcdate": 1573666083394, "tmdate": 1573818589462, "tddate": null, "forum": "Syg6jTNtDH", "replyto": "B1lHzT5atH", "invitation": "ICLR.cc/2020/Conference/Paper762/-/Official_Comment", "content": {"title": "Response to Reviewer #3", "comment": "We would like to thank the reviewer for the detailed review and helpful feedback.\n\n[ Not an end-to-end method and requires a regular expression ] \nWe use the standard regular expression for identifying numerals. It is ready-to-use and very fast and accurate, so we choose not to rely on learning for numeral identification. The prototype induction step can potentially be modified so that it can be trained jointly with the embedding training steps in an end-to-end manner. We leave this for future work.\n\n[\u201c2019\u201d is similar to \u201c19\u201d] Thank you for pointing out this problem. Our current similarity function is solely based on the intrinsic property of numbers that numbers close to each other are likely to convey similar semantic information. It is not perfect but serves as a solid start point for future extension. Regarding the 2019 vs. 19 problem, one potential solution is to treat each numeral as having multiple senses, each with a different embedding, so 2019 and 19 would each have one embedding for representing a year and another for representing an ordinary quantity; then we design the similarity function such that their year-representing embeddings have high similarity.\n\n[ Extend to language model training (using softmax)] It is possible to extend our method to language model training, but due to the infinite vocabulary of numerals, using softmax is intractable and the negative sampling technique would be preferred. \n\n[ A simple ablation experiment on the squashing function] If we do not squash the numbers, the gradient becomes NaN during training because of the occurrence of very large numbers like 10^15 in the corpus. This is exactly our motivation for using the squash function. We will clarify this in the paper.\n\n[ Guidelines on the selection of prototype numbers ] Determining the number of prototypes is very similar to determining the number of clusters in clustering. We show the tuned prototype numbers in our experiments in Table 6 in Appendix B. The optimal number of prototypes is around 200--500 for Wiki1B and is 10--25 for the much smaller sequence labeling dataset. So we suggest a rule of thumb that the prototype number is set to $(\\log(n))^2$, where $n$ is the number of distinct numerals in the training corpus.      \n\nMinor question:\n1.      We use negative sampling instead of softmax to deal with the infinite numeral vocabulary, so we have sigmoid in our objective. (See the equation at the top of page 4 in https://arxiv.org/pdf/1402.3722v1.pdf )\n2.      The reason is that the original work does not provide the dataset. We collect our own dataset following their procedure.\n3.      We do not have the performance for all the numerals. We could measure the performance if time permits, but we expect that the results would follow a similar pattern.\n4.      Yes."}, "signatures": ["ICLR.cc/2020/Conference/Paper762/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper762/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Numeral Embedding", "authors": ["Chengyue Jiang", "Zhonglin Nian", "Kaihao Guo", "Shanbo Chu", "Yinggong Zhao", "Libin Shen", "Kewei Tu"], "authorids": ["jiangchy@shanghaitech.edu.cn", "nianzhl@shanghaitech.edu.cn", "guokh@shanghaitech.edu.cn", "chushb@leyantech.com", "ygzhao@leyantech.com", "libin@leyantech.com", "tukw@shanghaitech.edu.cn"], "keywords": ["Natural Language Processing", "Numeral Embedding", "Word Embedding", "Out-of-vocabulary Problem"], "TL;DR": "We propose two methods for learning better numeral embeddings that solve the numeral out-of-vocabulary (OOV) problem and can be integrated into traditional word embedding training methods. ", "abstract": "Word embedding is an essential building block for deep learning methods for natural language processing. Although word embedding has been extensively studied over the years, the problem of how to effectively embed numerals, a special subset of words, is still underexplored. Existing word embedding methods do not learn numeral embeddings well because there are an infinite number of numerals and their individual appearances in training corpora are highly scarce.\nIn this paper, we propose two novel numeral embedding methods that can handle the out-of-vocabulary (OOV) problem for numerals. We first induce a finite set of prototype numerals using either a self-organizing map or a Gaussian mixture model. We then represent the embedding of a numeral as a weighted average of the prototype number embeddings. Numeral embeddings represented in this manner can be plugged into existing word embedding learning approaches such as skip-gram for training.\nWe evaluated our methods and showed its effectiveness on four intrinsic and extrinsic tasks: word similarity, embedding numeracy, numeral prediction, and sequence labeling. ", "pdf": "/pdf/4a076dd3baa83d984e8174e2e86ba83231e6705e.pdf", "paperhash": "jiang|learning_numeral_embedding", "original_pdf": "/attachment/29df16541d895beea2feac02a763249c1149b011.pdf", "_bibtex": "@misc{\njiang2020learning,\ntitle={Learning Numeral Embedding},\nauthor={Chengyue Jiang and Zhonglin Nian and Kaihao Guo and Shanbo Chu and Yinggong Zhao and Libin Shen and Kewei Tu},\nyear={2020},\nurl={https://openreview.net/forum?id=Syg6jTNtDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Syg6jTNtDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper762/Authors", "ICLR.cc/2020/Conference/Paper762/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper762/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper762/Reviewers", "ICLR.cc/2020/Conference/Paper762/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper762/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper762/Authors|ICLR.cc/2020/Conference/Paper762/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504166608, "tmdate": 1576860550123, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper762/Authors", "ICLR.cc/2020/Conference/Paper762/Reviewers", "ICLR.cc/2020/Conference/Paper762/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper762/-/Official_Comment"}}}, {"id": "HkeI9QB5oS", "original": null, "number": 2, "cdate": 1573700493790, "ddate": null, "tcdate": 1573700493790, "tmdate": 1573807963631, "tddate": null, "forum": "Syg6jTNtDH", "replyto": "B1lamhMaYr", "invitation": "ICLR.cc/2020/Conference/Paper762/-/Official_Comment", "content": {"title": "Response to Reviewer #1", "comment": "We would like to thank the reviewer for the detailed review and helpful feedback.\n\n[uses only quantity to represent similarity] Our current similarity function is solely based on quantity, which is not perfect but serves as a solid start point for future extension. Regarding the 1960 vs. 1960.1 problem, we observe that numerals like 1960.1 are rare in practice and hence our methods would work in most cases. A potential solution for this problem is to treat each numeral as having multiple senses, each with a different embedding. The numeral 1960 would have one embedding for representing a year and another for representing an ordinary quantity. \n\n[SOM portion of the method] Sorry for the unclearness of the SOM part.  SOM can be viewed as a clustering method except that its cluster centers are distributed more evenly on the number line. We will improve our description of SOM in section 3.2.\n\n[Numeracy-600K F1 scores] Chen et al. (2019) aim to test different neural models' ability to predict the numeral magnitude given the context. They rely on advanced neural architectures (capsule net, CNN, LSTM) to discover patterns in the text that indicate magnitude, and do not care about the embedding (they use random embedding for initialization). While we use their dataset, our goal is different in that we aim to evaluate embeddings instead of neural architectures. Hence, we only use the simple Skipgram model without any complex architecture and additional parameters.  This is why our scores are lower than those in Chen et al. (2019).\n  \n[Comparison to Spithourakis & Riedel, 2018] \n As correctly pointed out by the reviewer, Spithourakis & Riedel, 2018 is a language model that handles numerals, while our method learns general-purpose numeral embeddings that can be used in any neural model. For the number prediction task, we are currently trying to apply the system of Spithourakis & Riedel, 2018 to our dataset. However, because of the complexity in rewriting their preprocessing code for our dataset, as well as the time needed for training and tuning on the large Wikipedia-1B data, we do not expect to finish the experiments before the response deadline.\n\nMinor:\nThanks for pointing that out and sorry for the reference mistake. We have updated it."}, "signatures": ["ICLR.cc/2020/Conference/Paper762/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper762/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Numeral Embedding", "authors": ["Chengyue Jiang", "Zhonglin Nian", "Kaihao Guo", "Shanbo Chu", "Yinggong Zhao", "Libin Shen", "Kewei Tu"], "authorids": ["jiangchy@shanghaitech.edu.cn", "nianzhl@shanghaitech.edu.cn", "guokh@shanghaitech.edu.cn", "chushb@leyantech.com", "ygzhao@leyantech.com", "libin@leyantech.com", "tukw@shanghaitech.edu.cn"], "keywords": ["Natural Language Processing", "Numeral Embedding", "Word Embedding", "Out-of-vocabulary Problem"], "TL;DR": "We propose two methods for learning better numeral embeddings that solve the numeral out-of-vocabulary (OOV) problem and can be integrated into traditional word embedding training methods. ", "abstract": "Word embedding is an essential building block for deep learning methods for natural language processing. Although word embedding has been extensively studied over the years, the problem of how to effectively embed numerals, a special subset of words, is still underexplored. Existing word embedding methods do not learn numeral embeddings well because there are an infinite number of numerals and their individual appearances in training corpora are highly scarce.\nIn this paper, we propose two novel numeral embedding methods that can handle the out-of-vocabulary (OOV) problem for numerals. We first induce a finite set of prototype numerals using either a self-organizing map or a Gaussian mixture model. We then represent the embedding of a numeral as a weighted average of the prototype number embeddings. Numeral embeddings represented in this manner can be plugged into existing word embedding learning approaches such as skip-gram for training.\nWe evaluated our methods and showed its effectiveness on four intrinsic and extrinsic tasks: word similarity, embedding numeracy, numeral prediction, and sequence labeling. ", "pdf": "/pdf/4a076dd3baa83d984e8174e2e86ba83231e6705e.pdf", "paperhash": "jiang|learning_numeral_embedding", "original_pdf": "/attachment/29df16541d895beea2feac02a763249c1149b011.pdf", "_bibtex": "@misc{\njiang2020learning,\ntitle={Learning Numeral Embedding},\nauthor={Chengyue Jiang and Zhonglin Nian and Kaihao Guo and Shanbo Chu and Yinggong Zhao and Libin Shen and Kewei Tu},\nyear={2020},\nurl={https://openreview.net/forum?id=Syg6jTNtDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Syg6jTNtDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper762/Authors", "ICLR.cc/2020/Conference/Paper762/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper762/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper762/Reviewers", "ICLR.cc/2020/Conference/Paper762/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper762/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper762/Authors|ICLR.cc/2020/Conference/Paper762/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504166608, "tmdate": 1576860550123, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper762/Authors", "ICLR.cc/2020/Conference/Paper762/Reviewers", "ICLR.cc/2020/Conference/Paper762/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper762/-/Official_Comment"}}}, {"id": "HkewD9d5jB", "original": null, "number": 3, "cdate": 1573714527179, "ddate": null, "tcdate": 1573714527179, "tmdate": 1573784159995, "tddate": null, "forum": "Syg6jTNtDH", "replyto": "Hke6V2XEFB", "invitation": "ICLR.cc/2020/Conference/Paper762/-/Official_Comment", "content": {"title": "Response to Reviewer #2", "comment": "We would like to thank the reviewer for detailed review and helpful feedback.\n\n1.       We suppose [1] is \"Do NLP Models Know Numbers? Probing Numeracy in Embeddings [EMNLP19]\" and [2] is \"Exploring Numeracy in Word Embeddings [ACL19]\".\n\n[1] is a concurrent work that was made public right before we submitted our paper, so we were not able to consider its findings in our experimental setup. Following your suggestion, we are testing neural probing/decoding methods and applying them to the tasks in section 4.3.\n\nThe first test is Decoding (predicting the numeral value from its embedding using MLP). The second is Subtraction (predicting the difference between two numerals from their embeddings using MLP or BiLinear functions).\n\nThe preliminary result shows that, compared with the baselines, our method is much better at Decoding, and better at Subtraction using BiLinear, but slightly worse than NumAsTok (normal word embeddings) at Subtraction using MLP. We find that the probing test result is sensitive to the network structure. We will provide detailed probing results as well as the evaluation results on the tasks (OVA, SC, BC, AVGR) of Section 4.3 using these trained neural networks before the response deadline.\n\n2.       It is probably hard to say. While the objective of CBOW fits the numeral prediction task better, skip-gram is known to handle infrequent words (hence most of the numerals) better.\n\n3.      We will add the std of the experimental results in the appendix. We find that in most cases the std is quite small, so the result is stable."}, "signatures": ["ICLR.cc/2020/Conference/Paper762/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper762/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Numeral Embedding", "authors": ["Chengyue Jiang", "Zhonglin Nian", "Kaihao Guo", "Shanbo Chu", "Yinggong Zhao", "Libin Shen", "Kewei Tu"], "authorids": ["jiangchy@shanghaitech.edu.cn", "nianzhl@shanghaitech.edu.cn", "guokh@shanghaitech.edu.cn", "chushb@leyantech.com", "ygzhao@leyantech.com", "libin@leyantech.com", "tukw@shanghaitech.edu.cn"], "keywords": ["Natural Language Processing", "Numeral Embedding", "Word Embedding", "Out-of-vocabulary Problem"], "TL;DR": "We propose two methods for learning better numeral embeddings that solve the numeral out-of-vocabulary (OOV) problem and can be integrated into traditional word embedding training methods. ", "abstract": "Word embedding is an essential building block for deep learning methods for natural language processing. Although word embedding has been extensively studied over the years, the problem of how to effectively embed numerals, a special subset of words, is still underexplored. Existing word embedding methods do not learn numeral embeddings well because there are an infinite number of numerals and their individual appearances in training corpora are highly scarce.\nIn this paper, we propose two novel numeral embedding methods that can handle the out-of-vocabulary (OOV) problem for numerals. We first induce a finite set of prototype numerals using either a self-organizing map or a Gaussian mixture model. We then represent the embedding of a numeral as a weighted average of the prototype number embeddings. Numeral embeddings represented in this manner can be plugged into existing word embedding learning approaches such as skip-gram for training.\nWe evaluated our methods and showed its effectiveness on four intrinsic and extrinsic tasks: word similarity, embedding numeracy, numeral prediction, and sequence labeling. ", "pdf": "/pdf/4a076dd3baa83d984e8174e2e86ba83231e6705e.pdf", "paperhash": "jiang|learning_numeral_embedding", "original_pdf": "/attachment/29df16541d895beea2feac02a763249c1149b011.pdf", "_bibtex": "@misc{\njiang2020learning,\ntitle={Learning Numeral Embedding},\nauthor={Chengyue Jiang and Zhonglin Nian and Kaihao Guo and Shanbo Chu and Yinggong Zhao and Libin Shen and Kewei Tu},\nyear={2020},\nurl={https://openreview.net/forum?id=Syg6jTNtDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Syg6jTNtDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper762/Authors", "ICLR.cc/2020/Conference/Paper762/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper762/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper762/Reviewers", "ICLR.cc/2020/Conference/Paper762/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper762/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper762/Authors|ICLR.cc/2020/Conference/Paper762/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504166608, "tmdate": 1576860550123, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper762/Authors", "ICLR.cc/2020/Conference/Paper762/Reviewers", "ICLR.cc/2020/Conference/Paper762/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper762/-/Official_Comment"}}}, {"id": "Hke6V2XEFB", "original": null, "number": 1, "cdate": 1571204148615, "ddate": null, "tcdate": 1571204148615, "tmdate": 1572972555470, "tddate": null, "forum": "Syg6jTNtDH", "replyto": "Syg6jTNtDH", "invitation": "ICLR.cc/2020/Conference/Paper762/-/Official_Review", "content": {"rating": "6: Weak Accept", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "\nSummary:\nThe paper talks about a recently highlighted problem in word embeddings which is their incapability to represent numerals, especially the out-of-vocabulary numerals. For addressing the problem, they propose a method that induces a finite set of prototype numerals using either self-organizing map or Gaussian Mixture model. Then, each numeral is represented as a weighted average of prototype numeral embeddings. The method also involves squashing large quantities using log function. Finally, the training is performed similar to Skip-gram in word2vec but with the embedding of numerals computed using prototype numerals. \n\nQuestions:\n1. There are two basic motivations of the paper: (1) Most of the word embedding methods do not embed numerals correctly. (2) There is no mechanism of handing OOV numerals. The second one is well addressed but for the former one, it has been shown in recent work [1] that most of the embedding methods do have numerical reasoning capabilities. As stated in [1], the results of [2] demonstrate the opposite conclusion because their analysis is based on cosine distance and nearest neighbor which are not capable of capturing non-linear dependencies between embeddings. \n\nSo, it would be great if, for the results in Section 4.3 instead of using cosine distance, some neural models could be utilized for evaluation (3 layer MLP similar to [1]).\n\n2. In Section 4.4, the task is to predict the target numeral given its context words (similar to CBOW) while the embeddings are trained with a modified skip-gram model. Can one expect superior results if one uses modified CBOW for training embeddings rather than skip-gram?\n\n3. In Table 5, with 100% training data, the performance of all the methods is very close. It would be better if mean and variance across multiple runs are reported. "}, "signatures": ["ICLR.cc/2020/Conference/Paper762/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper762/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Numeral Embedding", "authors": ["Chengyue Jiang", "Zhonglin Nian", "Kaihao Guo", "Shanbo Chu", "Yinggong Zhao", "Libin Shen", "Kewei Tu"], "authorids": ["jiangchy@shanghaitech.edu.cn", "nianzhl@shanghaitech.edu.cn", "guokh@shanghaitech.edu.cn", "chushb@leyantech.com", "ygzhao@leyantech.com", "libin@leyantech.com", "tukw@shanghaitech.edu.cn"], "keywords": ["Natural Language Processing", "Numeral Embedding", "Word Embedding", "Out-of-vocabulary Problem"], "TL;DR": "We propose two methods for learning better numeral embeddings that solve the numeral out-of-vocabulary (OOV) problem and can be integrated into traditional word embedding training methods. ", "abstract": "Word embedding is an essential building block for deep learning methods for natural language processing. Although word embedding has been extensively studied over the years, the problem of how to effectively embed numerals, a special subset of words, is still underexplored. Existing word embedding methods do not learn numeral embeddings well because there are an infinite number of numerals and their individual appearances in training corpora are highly scarce.\nIn this paper, we propose two novel numeral embedding methods that can handle the out-of-vocabulary (OOV) problem for numerals. We first induce a finite set of prototype numerals using either a self-organizing map or a Gaussian mixture model. We then represent the embedding of a numeral as a weighted average of the prototype number embeddings. Numeral embeddings represented in this manner can be plugged into existing word embedding learning approaches such as skip-gram for training.\nWe evaluated our methods and showed its effectiveness on four intrinsic and extrinsic tasks: word similarity, embedding numeracy, numeral prediction, and sequence labeling. ", "pdf": "/pdf/4a076dd3baa83d984e8174e2e86ba83231e6705e.pdf", "paperhash": "jiang|learning_numeral_embedding", "original_pdf": "/attachment/29df16541d895beea2feac02a763249c1149b011.pdf", "_bibtex": "@misc{\njiang2020learning,\ntitle={Learning Numeral Embedding},\nauthor={Chengyue Jiang and Zhonglin Nian and Kaihao Guo and Shanbo Chu and Yinggong Zhao and Libin Shen and Kewei Tu},\nyear={2020},\nurl={https://openreview.net/forum?id=Syg6jTNtDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Syg6jTNtDH", "replyto": "Syg6jTNtDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper762/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper762/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575413068247, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper762/Reviewers"], "noninvitees": [], "tcdate": 1570237747461, "tmdate": 1575413068260, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper762/-/Official_Review"}}}, {"id": "B1lHzT5atH", "original": null, "number": 3, "cdate": 1571822860902, "ddate": null, "tcdate": 1571822860902, "tmdate": 1572972555392, "tddate": null, "forum": "Syg6jTNtDH", "replyto": "Syg6jTNtDH", "invitation": "ICLR.cc/2020/Conference/Paper762/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "\nThe paper proposes a novel method for embedding numerals which can be learned by using neural word embedding learning techniques. The paper motivates the work by reviewing the difficulty of embedding components to represent numerals: OOV in most cases. Their main contribution is the introduction of a method composes numeral embedding by a weighted average of prototype embeddings based on the similarities between the numeral and prototypes. There are two proposed prototypes: SOM and GMM and the similarity functions are an absolute difference and the density function respectively. During the training, the numerals have the proposed embeddings while the others have normal word embeddings. The paper slightly modifies the negative sampling to ensure numerals being sampled. A series of 4 empirical studies have been presented. First, the paper confirms that the proposed method does not negatively affect non-numeral embeddings. And then, the quality of the numeral embeddings are evaluated and compared. The experiments show that the proposed method has better performance on numerical property tests, numeral prediction, and a sequence labeling task.\n\nOverall, this paper has a novel contribution. The proposed method is well motivated and quite justified by the experiments abliet lacking comparison with previous published results. However, it has some weaknesses.\n\nFor the method part, one of the limitations is that it is not an end-to-end method and requires a regular expression to identify the numeral. Second, the weighted average of the prototypes is reasonable, but the similarity function only relies on the magnitude. I think there are other aspects of numerical tokens that it might not be able to capture (e.g. \u201c2019\u201d is similar to \u201c19\u201d in some context). In terms of training, I think it is not hard to extend the method to full language model training (using softmax). However, adding all numerals to the vocabulary would add significant overhead.\n\nFor the experiments, some design decisions are left unjustified. For example, a simple ablation experiment on the squashing function will be helpful. Furthermore, guidelines or empirical results on the effect of the number of prototypes can increase the impact of the paper. Finally, I think an analysis of the performance of numerical types will be helpful for future works (e.g., dates, phone numbers, currencies, etc, or discrete vs continuous). \n\nMinor comments and questions:\n1. The log sigmoid in equation 6 is a bit strange, isn\u2019t it log sum exp(). https://arxiv.org/pdf/1402.3722v1.pdf\n2. Why do you create a new dataset for the experiment in section 4.3?\n3. In section 4.4, you rank only numerals in the test set, but the scores are computed based on all numerals in the vocab. Do you have the performance of ranking all numerals?\n4. Just to confirm the \u201ctraining\u201d in section 4.4 refers to learning the embedding using the skip-gram model, right?\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper762/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper762/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Numeral Embedding", "authors": ["Chengyue Jiang", "Zhonglin Nian", "Kaihao Guo", "Shanbo Chu", "Yinggong Zhao", "Libin Shen", "Kewei Tu"], "authorids": ["jiangchy@shanghaitech.edu.cn", "nianzhl@shanghaitech.edu.cn", "guokh@shanghaitech.edu.cn", "chushb@leyantech.com", "ygzhao@leyantech.com", "libin@leyantech.com", "tukw@shanghaitech.edu.cn"], "keywords": ["Natural Language Processing", "Numeral Embedding", "Word Embedding", "Out-of-vocabulary Problem"], "TL;DR": "We propose two methods for learning better numeral embeddings that solve the numeral out-of-vocabulary (OOV) problem and can be integrated into traditional word embedding training methods. ", "abstract": "Word embedding is an essential building block for deep learning methods for natural language processing. Although word embedding has been extensively studied over the years, the problem of how to effectively embed numerals, a special subset of words, is still underexplored. Existing word embedding methods do not learn numeral embeddings well because there are an infinite number of numerals and their individual appearances in training corpora are highly scarce.\nIn this paper, we propose two novel numeral embedding methods that can handle the out-of-vocabulary (OOV) problem for numerals. We first induce a finite set of prototype numerals using either a self-organizing map or a Gaussian mixture model. We then represent the embedding of a numeral as a weighted average of the prototype number embeddings. Numeral embeddings represented in this manner can be plugged into existing word embedding learning approaches such as skip-gram for training.\nWe evaluated our methods and showed its effectiveness on four intrinsic and extrinsic tasks: word similarity, embedding numeracy, numeral prediction, and sequence labeling. ", "pdf": "/pdf/4a076dd3baa83d984e8174e2e86ba83231e6705e.pdf", "paperhash": "jiang|learning_numeral_embedding", "original_pdf": "/attachment/29df16541d895beea2feac02a763249c1149b011.pdf", "_bibtex": "@misc{\njiang2020learning,\ntitle={Learning Numeral Embedding},\nauthor={Chengyue Jiang and Zhonglin Nian and Kaihao Guo and Shanbo Chu and Yinggong Zhao and Libin Shen and Kewei Tu},\nyear={2020},\nurl={https://openreview.net/forum?id=Syg6jTNtDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Syg6jTNtDH", "replyto": "Syg6jTNtDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper762/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper762/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575413068247, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper762/Reviewers"], "noninvitees": [], "tcdate": 1570237747461, "tmdate": 1575413068260, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper762/-/Official_Review"}}}], "count": 9}