{"notes": [{"id": "Qm7R_SdqTpT", "original": "FM8J39BG3Env", "number": 1046, "cdate": 1601308117970, "ddate": null, "tcdate": 1601308117970, "tmdate": 1615924188160, "tddate": null, "forum": "Qm7R_SdqTpT", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Diverse Video Generation using a Gaussian Process Trigger", "authorids": ["~Gaurav_Shrivastava1", "~Abhinav_Shrivastava2"], "authors": ["Gaurav Shrivastava", "Abhinav Shrivastava"], "keywords": ["video synthesis", "future frame generation", "video generation", "gaussian process priors", "diverse video generation"], "abstract": "Generating future frames given a few context (or past) frames is a challenging task. It requires modeling the temporal coherence of videos as well as multi-modality in terms of diversity in the potential future states. Current variational approaches for video generation tend to marginalize over multi-modal future outcomes. Instead, we propose to explicitly model the multi-modality in the future outcomes and leverage it to sample diverse futures. Our approach, Diverse Video Generator, uses a GP to learn priors on future states given the past and maintains a probability distribution over possible futures given a particular sample. We leverage the changes in this distribution over time to control the sampling of diverse future states by estimating the end of on-going sequences. In particular, we use the variance of GP over the output function space to trigger a change in the action sequence. We achieve state-of-the-art results on diverse future frame generation in terms of reconstruction quality and diversity of the generated sequences.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shrivastava|diverse_video_generation_using_a_gaussian_process_trigger", "one-sentence_summary": "Diverse future frame synthesis by modeling the diversity of future states using a Gaussian Process, and using Bayesian inference to sample diverse future states.", "supplementary_material": "/attachment/d15b5d1b4c89a26709aa65f918bed1002209fff5.zip", "pdf": "/pdf/fa17e9ba0b11fb7c0b424feda300e370d82df3eb.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nshrivastava2021diverse,\ntitle={Diverse Video Generation using a Gaussian Process Trigger},\nauthor={Gaurav Shrivastava and Abhinav Shrivastava},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Qm7R_SdqTpT}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "zks_w885zL", "original": null, "number": 1, "cdate": 1610040511259, "ddate": null, "tcdate": 1610040511259, "tmdate": 1610474119069, "tddate": null, "forum": "Qm7R_SdqTpT", "replyto": "Qm7R_SdqTpT", "invitation": "ICLR.cc/2021/Conference/Paper1046/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Poster)", "comment": "All three reviewers agree on accepting the paper and think that the proposed approach will be of interest for those working in vdieo prediction.  The authors are asked to include the extra discussion with R3 as part of the paper and include the proposed changes by R2 to provide more thorough experimentation.  The paper is recommended as a poster presentation."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Diverse Video Generation using a Gaussian Process Trigger", "authorids": ["~Gaurav_Shrivastava1", "~Abhinav_Shrivastava2"], "authors": ["Gaurav Shrivastava", "Abhinav Shrivastava"], "keywords": ["video synthesis", "future frame generation", "video generation", "gaussian process priors", "diverse video generation"], "abstract": "Generating future frames given a few context (or past) frames is a challenging task. It requires modeling the temporal coherence of videos as well as multi-modality in terms of diversity in the potential future states. Current variational approaches for video generation tend to marginalize over multi-modal future outcomes. Instead, we propose to explicitly model the multi-modality in the future outcomes and leverage it to sample diverse futures. Our approach, Diverse Video Generator, uses a GP to learn priors on future states given the past and maintains a probability distribution over possible futures given a particular sample. We leverage the changes in this distribution over time to control the sampling of diverse future states by estimating the end of on-going sequences. In particular, we use the variance of GP over the output function space to trigger a change in the action sequence. We achieve state-of-the-art results on diverse future frame generation in terms of reconstruction quality and diversity of the generated sequences.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shrivastava|diverse_video_generation_using_a_gaussian_process_trigger", "one-sentence_summary": "Diverse future frame synthesis by modeling the diversity of future states using a Gaussian Process, and using Bayesian inference to sample diverse future states.", "supplementary_material": "/attachment/d15b5d1b4c89a26709aa65f918bed1002209fff5.zip", "pdf": "/pdf/fa17e9ba0b11fb7c0b424feda300e370d82df3eb.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nshrivastava2021diverse,\ntitle={Diverse Video Generation using a Gaussian Process Trigger},\nauthor={Gaurav Shrivastava and Abhinav Shrivastava},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Qm7R_SdqTpT}\n}"}, "tags": [], "invitation": {"reply": {"forum": "Qm7R_SdqTpT", "replyto": "Qm7R_SdqTpT", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040511246, "tmdate": 1610474119054, "id": "ICLR.cc/2021/Conference/Paper1046/-/Decision"}}}, {"id": "Zi0WDzjgDvZ", "original": null, "number": 2, "cdate": 1603856649018, "ddate": null, "tcdate": 1603856649018, "tmdate": 1606777588846, "tddate": null, "forum": "Qm7R_SdqTpT", "replyto": "Qm7R_SdqTpT", "invitation": "ICLR.cc/2021/Conference/Paper1046/-/Official_Review", "content": {"title": "Review for Diverse Video Generation using a Gaussian Process Trigger ", "review": "### SUMMARY\n\nThe authors propose to use a Gaussian Process (GP) to model the uncertainty of future frames in a video prediction setup. In particular, they employ a GP to model the uncertainty of the next step latent in a latent variable model. This allows them to use the GP variance to decide when to change an \"action sequence\", corresponding to a deterministic dynamics function implemented using an LSTM. \n\n### STRENGTHS AND WEAKNESSES\n\n[+] Empirical results\n\n[+] Well-motivated model\n\n[+] Clear presentation\n\n[-] Experimental section could be improved (missing baselines in some tables, results seem to differ from those in the literature)\n\n### DETAILED COMMENTS\n\nThe paper proposes a novel approach for video prediction. Following the standard latent variable model setup used by many VAE-based video prediction models, the authors propose to use a GP to model the uncertainty in the latent space while also learning a deterministic dynamics model (LSTM) on this latent space. Then the GP is used to decide when a future frame has high uncertainty, and in those cases multiple latents can be sampled from the GP. In general the paper is clear and well-written.\n\nThe experimental section could be improved. In particular, more details about how the comparison to some baselines was made would be appreciated. For example,  the results for the VRNN model in Figure 4 and 5 do not follow the results in the literature, where it outperforms SVG and SAVP, and its unclear whether its due to an architectural change, suboptimal hyperparameters, or a different reimplementation. Further this model is missing from some other comparisons such as Table 1. For SAVP the results for Figure 4 seem much worse than those reported in the original paper. On the other hand, the authors did some ablation experiments and included different metrics to analyze the performance of their method.\n\n### SCORE\nI vote for accepting the paper. The model formulation is clear, well-motivated and novel. The results are positive and overall it seems like a valid alternative to current approaches that will be of interest to the video prediction community. I would encourage the authors to provide a more thorough experimental section.\n\n### POST-REBUTTAL UPDATE\nAfter reading the other reviews and the authors' rebuttal, I stand by my rating of 6.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1046/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1046/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Diverse Video Generation using a Gaussian Process Trigger", "authorids": ["~Gaurav_Shrivastava1", "~Abhinav_Shrivastava2"], "authors": ["Gaurav Shrivastava", "Abhinav Shrivastava"], "keywords": ["video synthesis", "future frame generation", "video generation", "gaussian process priors", "diverse video generation"], "abstract": "Generating future frames given a few context (or past) frames is a challenging task. It requires modeling the temporal coherence of videos as well as multi-modality in terms of diversity in the potential future states. Current variational approaches for video generation tend to marginalize over multi-modal future outcomes. Instead, we propose to explicitly model the multi-modality in the future outcomes and leverage it to sample diverse futures. Our approach, Diverse Video Generator, uses a GP to learn priors on future states given the past and maintains a probability distribution over possible futures given a particular sample. We leverage the changes in this distribution over time to control the sampling of diverse future states by estimating the end of on-going sequences. In particular, we use the variance of GP over the output function space to trigger a change in the action sequence. We achieve state-of-the-art results on diverse future frame generation in terms of reconstruction quality and diversity of the generated sequences.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shrivastava|diverse_video_generation_using_a_gaussian_process_trigger", "one-sentence_summary": "Diverse future frame synthesis by modeling the diversity of future states using a Gaussian Process, and using Bayesian inference to sample diverse future states.", "supplementary_material": "/attachment/d15b5d1b4c89a26709aa65f918bed1002209fff5.zip", "pdf": "/pdf/fa17e9ba0b11fb7c0b424feda300e370d82df3eb.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nshrivastava2021diverse,\ntitle={Diverse Video Generation using a Gaussian Process Trigger},\nauthor={Gaurav Shrivastava and Abhinav Shrivastava},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Qm7R_SdqTpT}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Qm7R_SdqTpT", "replyto": "Qm7R_SdqTpT", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1046/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538128378, "tmdate": 1606915803302, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1046/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1046/-/Official_Review"}}}, {"id": "Fbzk6Pnb8r-", "original": null, "number": 6, "cdate": 1606071106704, "ddate": null, "tcdate": 1606071106704, "tmdate": 1606071137674, "tddate": null, "forum": "Qm7R_SdqTpT", "replyto": "Qm7R_SdqTpT", "invitation": "ICLR.cc/2021/Conference/Paper1046/-/Official_Comment", "content": {"title": "New Revision Uploaded", "comment": "Dear AC and Reviewers,\n\nThank you for your valuable suggestions. We have uploaded a new version of the paper with all of the changes we discuss in our individual responses below. In particular, we added new numbers in Table 1 of our updated paper. We also took the liberty to improve some of the figures based on the feedback. We are happy to answer any more of your concerns during this discussion period.\n\nThank you,\n\nThe Authors"}, "signatures": ["ICLR.cc/2021/Conference/Paper1046/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1046/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Diverse Video Generation using a Gaussian Process Trigger", "authorids": ["~Gaurav_Shrivastava1", "~Abhinav_Shrivastava2"], "authors": ["Gaurav Shrivastava", "Abhinav Shrivastava"], "keywords": ["video synthesis", "future frame generation", "video generation", "gaussian process priors", "diverse video generation"], "abstract": "Generating future frames given a few context (or past) frames is a challenging task. It requires modeling the temporal coherence of videos as well as multi-modality in terms of diversity in the potential future states. Current variational approaches for video generation tend to marginalize over multi-modal future outcomes. Instead, we propose to explicitly model the multi-modality in the future outcomes and leverage it to sample diverse futures. Our approach, Diverse Video Generator, uses a GP to learn priors on future states given the past and maintains a probability distribution over possible futures given a particular sample. We leverage the changes in this distribution over time to control the sampling of diverse future states by estimating the end of on-going sequences. In particular, we use the variance of GP over the output function space to trigger a change in the action sequence. We achieve state-of-the-art results on diverse future frame generation in terms of reconstruction quality and diversity of the generated sequences.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shrivastava|diverse_video_generation_using_a_gaussian_process_trigger", "one-sentence_summary": "Diverse future frame synthesis by modeling the diversity of future states using a Gaussian Process, and using Bayesian inference to sample diverse future states.", "supplementary_material": "/attachment/d15b5d1b4c89a26709aa65f918bed1002209fff5.zip", "pdf": "/pdf/fa17e9ba0b11fb7c0b424feda300e370d82df3eb.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nshrivastava2021diverse,\ntitle={Diverse Video Generation using a Gaussian Process Trigger},\nauthor={Gaurav Shrivastava and Abhinav Shrivastava},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Qm7R_SdqTpT}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Qm7R_SdqTpT", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1046/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1046/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1046/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1046/Authors|ICLR.cc/2021/Conference/Paper1046/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1046/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923864298, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1046/-/Official_Comment"}}}, {"id": "8NeC706mt60", "original": null, "number": 4, "cdate": 1605155864855, "ddate": null, "tcdate": 1605155864855, "tmdate": 1605157333051, "tddate": null, "forum": "Qm7R_SdqTpT", "replyto": "Zi0WDzjgDvZ", "invitation": "ICLR.cc/2021/Conference/Paper1046/-/Official_Comment", "content": {"title": "Addressed Concerns about experimental setup; Added additional results in the revised paper", "comment": "We want to thank the reviewer for all the valuable comments, especially for finding the paper well-motivated, good in terms of empirical results and presentation. \nWe would like to address the concerns regarding the comparison with baselines. Wherever available, we used either the official implementation or the pre-trained models for the baselines uploaded by their respective authors. We have added this clarification to the revised manuscript.\n\n**Results of VRNN** \nWe used the official implementation of VRNN to train the baseline models since pre-trained models were not released. We followed all the best practices and ensured that the results (qualitative and quantitative) match or are better than the original paper. Our LPIPS results on the BAIR dataset (Figure 4-center) are in-line with Figure 6 of [VRNN], where the performance of SAVP, SVG, and VRNN are close, and VRNN is slightly better. We have also updated the two missing numbers for VRNN in Table 1 in the revised manuscript.\n\n**SAVP results**\nWe used the pre-trained model released by the authors for the SAVP baseline. For both the KTH and BAIR datasets, we followed the testing protocol from SVG\u2019s official implementation (for each test video, sample 100 sequences, and evaluate the best matching sequence). The results from the original paper [SAVP] (Figure 8, row-1, col-3) show that SAVP is much worse compared to SVG (*with fixed prior*), especially for later frames. In our paper (Figure 4, KTH), we observe that SAVP is worse than SVG (*with learned prior*). A more considerable margin is explained by a stronger SVG model where the prior is learned. The magnitude of results are similar across Figure 4 (our paper) and Figure 8 ([SAVP]). \n\n**Updates to the paper:**\n* Added the VRNN diversity scores on the Human3.6M dataset to Table 1.\n* Added clarifications stating that we used official implementations/pre-trained models before the start of descriptions of the baselines.\n\n\n[SAVP] Lee, Zhang, Ebert, Abbeel, Finn, Levine. \u2018Stochastic Adversarial Video Prediction\u2019 \n\n[VRNN] Castrejon, Ballas, Courville. \u2018Improved Conditional VRNNs for Video Prediction\u2019\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1046/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1046/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Diverse Video Generation using a Gaussian Process Trigger", "authorids": ["~Gaurav_Shrivastava1", "~Abhinav_Shrivastava2"], "authors": ["Gaurav Shrivastava", "Abhinav Shrivastava"], "keywords": ["video synthesis", "future frame generation", "video generation", "gaussian process priors", "diverse video generation"], "abstract": "Generating future frames given a few context (or past) frames is a challenging task. It requires modeling the temporal coherence of videos as well as multi-modality in terms of diversity in the potential future states. Current variational approaches for video generation tend to marginalize over multi-modal future outcomes. Instead, we propose to explicitly model the multi-modality in the future outcomes and leverage it to sample diverse futures. Our approach, Diverse Video Generator, uses a GP to learn priors on future states given the past and maintains a probability distribution over possible futures given a particular sample. We leverage the changes in this distribution over time to control the sampling of diverse future states by estimating the end of on-going sequences. In particular, we use the variance of GP over the output function space to trigger a change in the action sequence. We achieve state-of-the-art results on diverse future frame generation in terms of reconstruction quality and diversity of the generated sequences.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shrivastava|diverse_video_generation_using_a_gaussian_process_trigger", "one-sentence_summary": "Diverse future frame synthesis by modeling the diversity of future states using a Gaussian Process, and using Bayesian inference to sample diverse future states.", "supplementary_material": "/attachment/d15b5d1b4c89a26709aa65f918bed1002209fff5.zip", "pdf": "/pdf/fa17e9ba0b11fb7c0b424feda300e370d82df3eb.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nshrivastava2021diverse,\ntitle={Diverse Video Generation using a Gaussian Process Trigger},\nauthor={Gaurav Shrivastava and Abhinav Shrivastava},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Qm7R_SdqTpT}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Qm7R_SdqTpT", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1046/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1046/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1046/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1046/Authors|ICLR.cc/2021/Conference/Paper1046/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1046/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923864298, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1046/-/Official_Comment"}}}, {"id": "y2r-ipmOqOf", "original": null, "number": 5, "cdate": 1605157232661, "ddate": null, "tcdate": 1605157232661, "tmdate": 1605157259467, "tddate": null, "forum": "Qm7R_SdqTpT", "replyto": "xt6RU6emoaT", "invitation": "ICLR.cc/2021/Conference/Paper1046/-/Official_Comment", "content": {"title": "Concerns addressed; Figure updated in the paper for clarification", "comment": "We thank R1 for taking the time to go through the paper thoroughly and understanding the gist of the paper, especially for acknowledging the elegance of our idea. \n\n**Using GP is straightforward**\nWe thank the reviewer for referring us to the previous work in GP literature, which closely resembles our intuition. We agree that modeling dynamical systems with GP is a vibrant community, and GP has been used in other communities (e.g., planning for robotics). However, to the best of our knowledge, no prior work has explored the use of GP in modeling the multimodal nature of video generation/prediction. Our work demonstrates that a simple GP model can beat the current state-of-the-art by a considerable margin in terms of both diversity and fidelity. We strongly believe that GP-based approaches are a better way to capture the multimodal nature of the video generation task and hope that the future line of work investigates such methods further. Therefore, we believe that our work deserves a wider audience of this conference.\n\n**Architecture clarification**\nLSTM frame generation and GP frame generation are not entirely different modules but are used as additional loss terms to train the decoder and encoder network. In Figure 3, they are depicted separately under \u201cTraining Dynamics Encoders\u201d for clarity. The joint framework is shown for inference in Figure 3, which depicts how they are used together. Since the decoder network maps the latent space to the image space, it is trained using three different frame generation loss functions given by the frame auto-encoder loss ($z_t$ ~ frame encoder), the LSTM frame generation loss ($\\hat{z}_t$ ~ LSTM temporal dynamics encoder), and the GP frame generation loss ($\\tilde{z}_t$~ GP temporal dynamics encoder). As for the concern, yes, we utilize the GP only in the latent space. \n\n**Datasets and High-level Research topics**\nThe datasets utilized for the experiments are standard in this community and were used to make a fair comparison with the baselines in terms of quantitative evaluations. We also demonstrate qualitative evaluation on a more representative dataset like UCF-101, which has videos in the wild setting. \n\nWe thank the reviewer for realizing that the proposed approach is a fundamental contribution that can be used to better understand other research areas in videos (e.g., video recognition). Though we agree with the comment, we emphasize that this paper only focuses on the task of diverse video generation. Adapting our approach to build better models in other applications is indeed an area of future research.\n\n**Updates to the paper:**\n* Added GPDM line of work in the related work section.\n* Updated figure 3\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1046/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1046/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Diverse Video Generation using a Gaussian Process Trigger", "authorids": ["~Gaurav_Shrivastava1", "~Abhinav_Shrivastava2"], "authors": ["Gaurav Shrivastava", "Abhinav Shrivastava"], "keywords": ["video synthesis", "future frame generation", "video generation", "gaussian process priors", "diverse video generation"], "abstract": "Generating future frames given a few context (or past) frames is a challenging task. It requires modeling the temporal coherence of videos as well as multi-modality in terms of diversity in the potential future states. Current variational approaches for video generation tend to marginalize over multi-modal future outcomes. Instead, we propose to explicitly model the multi-modality in the future outcomes and leverage it to sample diverse futures. Our approach, Diverse Video Generator, uses a GP to learn priors on future states given the past and maintains a probability distribution over possible futures given a particular sample. We leverage the changes in this distribution over time to control the sampling of diverse future states by estimating the end of on-going sequences. In particular, we use the variance of GP over the output function space to trigger a change in the action sequence. We achieve state-of-the-art results on diverse future frame generation in terms of reconstruction quality and diversity of the generated sequences.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shrivastava|diverse_video_generation_using_a_gaussian_process_trigger", "one-sentence_summary": "Diverse future frame synthesis by modeling the diversity of future states using a Gaussian Process, and using Bayesian inference to sample diverse future states.", "supplementary_material": "/attachment/d15b5d1b4c89a26709aa65f918bed1002209fff5.zip", "pdf": "/pdf/fa17e9ba0b11fb7c0b424feda300e370d82df3eb.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nshrivastava2021diverse,\ntitle={Diverse Video Generation using a Gaussian Process Trigger},\nauthor={Gaurav Shrivastava and Abhinav Shrivastava},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Qm7R_SdqTpT}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Qm7R_SdqTpT", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1046/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1046/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1046/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1046/Authors|ICLR.cc/2021/Conference/Paper1046/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1046/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923864298, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1046/-/Official_Comment"}}}, {"id": "6a8gOQ1MT-0", "original": null, "number": 3, "cdate": 1605154925816, "ddate": null, "tcdate": 1605154925816, "tmdate": 1605154925816, "tddate": null, "forum": "Qm7R_SdqTpT", "replyto": "_s6FkEfaiat", "invitation": "ICLR.cc/2021/Conference/Paper1046/-/Official_Comment", "content": {"title": "Clarifications on results; paper revised", "comment": "We want to thank the reviewer for thoughtful comments; especially for acknowledging the intuition behind using a GP based approach to solve an important problem of diverse video generation and the fact that sufficient empirical evidence was put forth to back our claims.  We are glad to incorporate suggestions put forth here.\n\n**GP Triggering (FVD)** \nFor finding the FVD scores, as mentioned in our paper, we take the best matching sample to the ground-truth. When we use GP variance as a trigger, the chances of getting the best matching are lower as there are multiple plausible (diverse) futures that can be picked after the trigger. In theory, we should be able to achieve the best ground-truth matching sample, if we are allowed to sample infinite times; however, for our evaluations, we keep the upper limit to 100 random samples per starting sequence. Hence, we observe better scores for the deterministic trigger DVG@15,35\n\n**Insights for GP Heuristics**\nWhile running our experiments, we found that the range of variance of the learned GP is different for different starting sequences and actions. For example, sequences from box action tend to have less spread in the variance values, whereas those from walk action have more spread. This makes picking a fixed threshold challenging, and hence our design choice of using greater than two standard deviations. Changing this to [1.5, 2.5]x standard deviations gives similar results. Lower than this range triggers changes in sequence before the action finishes, and higher values tend not to trigger diverse samples. Therefore, we opted for a simple design choice of using two-sigma, which gives us a sequence-dependent threshold.\n\n**Diversity Score for DVG@[15,35]**\nWe appreciate the thoroughness of the reviewer in raising this point -- `fixed number of frames performs better than GP triggering during the evaluation of the first clip of the generated sequence. We observe this phenomenon because our two-sigma heuristic makes conservative decisions on when to trigger the change from an ongoing action sequence. Hence, during the evaluation of the first clips, the number of videos triggered with the GP variance trigger is less than the number of videos triggered by the deterministic trigger as the deterministic trigger forces every video to trigger at the 15th frame. However, this is not the case when we perform our second clip evaluation as the number of GP triggers increases as the sequence progresses.\n\nLastly, we fixed the typos in our revised manuscript, as suggested. We again want to thank the reviewer for providing us with valuable feedback.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1046/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1046/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Diverse Video Generation using a Gaussian Process Trigger", "authorids": ["~Gaurav_Shrivastava1", "~Abhinav_Shrivastava2"], "authors": ["Gaurav Shrivastava", "Abhinav Shrivastava"], "keywords": ["video synthesis", "future frame generation", "video generation", "gaussian process priors", "diverse video generation"], "abstract": "Generating future frames given a few context (or past) frames is a challenging task. It requires modeling the temporal coherence of videos as well as multi-modality in terms of diversity in the potential future states. Current variational approaches for video generation tend to marginalize over multi-modal future outcomes. Instead, we propose to explicitly model the multi-modality in the future outcomes and leverage it to sample diverse futures. Our approach, Diverse Video Generator, uses a GP to learn priors on future states given the past and maintains a probability distribution over possible futures given a particular sample. We leverage the changes in this distribution over time to control the sampling of diverse future states by estimating the end of on-going sequences. In particular, we use the variance of GP over the output function space to trigger a change in the action sequence. We achieve state-of-the-art results on diverse future frame generation in terms of reconstruction quality and diversity of the generated sequences.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shrivastava|diverse_video_generation_using_a_gaussian_process_trigger", "one-sentence_summary": "Diverse future frame synthesis by modeling the diversity of future states using a Gaussian Process, and using Bayesian inference to sample diverse future states.", "supplementary_material": "/attachment/d15b5d1b4c89a26709aa65f918bed1002209fff5.zip", "pdf": "/pdf/fa17e9ba0b11fb7c0b424feda300e370d82df3eb.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nshrivastava2021diverse,\ntitle={Diverse Video Generation using a Gaussian Process Trigger},\nauthor={Gaurav Shrivastava and Abhinav Shrivastava},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Qm7R_SdqTpT}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Qm7R_SdqTpT", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1046/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1046/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1046/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1046/Authors|ICLR.cc/2021/Conference/Paper1046/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1046/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923864298, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1046/-/Official_Comment"}}}, {"id": "_s6FkEfaiat", "original": null, "number": 3, "cdate": 1603866427233, "ddate": null, "tcdate": 1603866427233, "tmdate": 1605024543707, "tddate": null, "forum": "Qm7R_SdqTpT", "replyto": "Qm7R_SdqTpT", "invitation": "ICLR.cc/2021/Conference/Paper1046/-/Official_Review", "content": {"title": "Official Blind Review #3", "review": "Summary:\nThis paper proposes a future frame prediction framework where the video generation can transition between different actions using a Gaussian process trigger. The framework consists of three components: an encoder which encodes the frame to a latent code, an LSTM which predicts the next latent code given the current one, and a Gaussian process which samples a new latent code. The framework can decide whether to switch to the next action by adopting the new latent code, depending on the number of frames passed or the variance of Gaussian.\n\nStrengths:\nThe paper is easy to follow overall. The usage of Gaussian process to trigger the transition to the next action is reasonable and intuitive. Quantitative evaluations show that the method outperforms existing works for both reconstruction and output diversity for various datasets.\n\nWeaknesses and comments:\nThere are quite a few typos in the writing, especially toward the latter part of the paper. I\u2019d encourage the authors to do a thorough check to ensure the paper is typo-free.\nIt seems switching actions at some fixed number of frames beats using the Gaussian variance for FVD, which is quite surprising. Can the authors provide some insights? Is it due to some inherent nature of FVD, or there\u2019s still some room for improvement for the choosing criteria?\nHow important is the heuristic of changing states when using GP? Currently it is triggered when the variance is larger than two standard deviations. How will it affect the performance if a different threshold is used?\nThere\u2019s a mistake in Table 1. The diversity score for DVG@15,35 is the best for KTH frames [10,25] (48.30), but DVG GP is bolded (47.71). This might also be an interesting point to discuss about why fixed number of frames performs better than GP.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1046/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1046/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Diverse Video Generation using a Gaussian Process Trigger", "authorids": ["~Gaurav_Shrivastava1", "~Abhinav_Shrivastava2"], "authors": ["Gaurav Shrivastava", "Abhinav Shrivastava"], "keywords": ["video synthesis", "future frame generation", "video generation", "gaussian process priors", "diverse video generation"], "abstract": "Generating future frames given a few context (or past) frames is a challenging task. It requires modeling the temporal coherence of videos as well as multi-modality in terms of diversity in the potential future states. Current variational approaches for video generation tend to marginalize over multi-modal future outcomes. Instead, we propose to explicitly model the multi-modality in the future outcomes and leverage it to sample diverse futures. Our approach, Diverse Video Generator, uses a GP to learn priors on future states given the past and maintains a probability distribution over possible futures given a particular sample. We leverage the changes in this distribution over time to control the sampling of diverse future states by estimating the end of on-going sequences. In particular, we use the variance of GP over the output function space to trigger a change in the action sequence. We achieve state-of-the-art results on diverse future frame generation in terms of reconstruction quality and diversity of the generated sequences.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shrivastava|diverse_video_generation_using_a_gaussian_process_trigger", "one-sentence_summary": "Diverse future frame synthesis by modeling the diversity of future states using a Gaussian Process, and using Bayesian inference to sample diverse future states.", "supplementary_material": "/attachment/d15b5d1b4c89a26709aa65f918bed1002209fff5.zip", "pdf": "/pdf/fa17e9ba0b11fb7c0b424feda300e370d82df3eb.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nshrivastava2021diverse,\ntitle={Diverse Video Generation using a Gaussian Process Trigger},\nauthor={Gaurav Shrivastava and Abhinav Shrivastava},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Qm7R_SdqTpT}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Qm7R_SdqTpT", "replyto": "Qm7R_SdqTpT", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1046/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538128378, "tmdate": 1606915803302, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1046/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1046/-/Official_Review"}}}, {"id": "xt6RU6emoaT", "original": null, "number": 1, "cdate": 1603178347461, "ddate": null, "tcdate": 1603178347461, "tmdate": 1605024543649, "tddate": null, "forum": "Qm7R_SdqTpT", "replyto": "Qm7R_SdqTpT", "invitation": "ICLR.cc/2021/Conference/Paper1046/-/Official_Review", "content": {"title": "GP for Video Generation", "review": "In this work, the authors propose to apply Gaussian Processes to generate future video frames with high diversity. Specifically, they use variance of GP prediction as a trigger to control when we should switch to a new action sequence.\n\nStrength \n\n1 The paper is written well, and the organization is OK\n\n2 The idea of using GP for video generation sounds interesting\n\nWeakness\n\n1 The way of using GP is kind of straightforward and naive. In the GP community, dynamical modeling has been widely investigated, from the start of Gaussian Process Dynamical Model in NIPs 2005. \n\n2 I do not quite get the modules of LSTM Frame Generation and GP Frame Generation in Eq (4). Where are these modules in Fig.3 ? The D in the Stage 3? Using GP to generate Images? Does it make sense? GP is more suitable to work in the latent space, is it? \n\n3 The datasets are not quite representative, due to the simple and experimental scenarios. Moreover, the proposed method is like a fundamental work. But is it useful for high-level research topics, e.g.,  large-scale action recognition, video caption, etc?", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1046/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1046/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Diverse Video Generation using a Gaussian Process Trigger", "authorids": ["~Gaurav_Shrivastava1", "~Abhinav_Shrivastava2"], "authors": ["Gaurav Shrivastava", "Abhinav Shrivastava"], "keywords": ["video synthesis", "future frame generation", "video generation", "gaussian process priors", "diverse video generation"], "abstract": "Generating future frames given a few context (or past) frames is a challenging task. It requires modeling the temporal coherence of videos as well as multi-modality in terms of diversity in the potential future states. Current variational approaches for video generation tend to marginalize over multi-modal future outcomes. Instead, we propose to explicitly model the multi-modality in the future outcomes and leverage it to sample diverse futures. Our approach, Diverse Video Generator, uses a GP to learn priors on future states given the past and maintains a probability distribution over possible futures given a particular sample. We leverage the changes in this distribution over time to control the sampling of diverse future states by estimating the end of on-going sequences. In particular, we use the variance of GP over the output function space to trigger a change in the action sequence. We achieve state-of-the-art results on diverse future frame generation in terms of reconstruction quality and diversity of the generated sequences.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shrivastava|diverse_video_generation_using_a_gaussian_process_trigger", "one-sentence_summary": "Diverse future frame synthesis by modeling the diversity of future states using a Gaussian Process, and using Bayesian inference to sample diverse future states.", "supplementary_material": "/attachment/d15b5d1b4c89a26709aa65f918bed1002209fff5.zip", "pdf": "/pdf/fa17e9ba0b11fb7c0b424feda300e370d82df3eb.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nshrivastava2021diverse,\ntitle={Diverse Video Generation using a Gaussian Process Trigger},\nauthor={Gaurav Shrivastava and Abhinav Shrivastava},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Qm7R_SdqTpT}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Qm7R_SdqTpT", "replyto": "Qm7R_SdqTpT", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1046/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538128378, "tmdate": 1606915803302, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1046/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1046/-/Official_Review"}}}], "count": 9}