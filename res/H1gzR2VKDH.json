{"notes": [{"id": "H1gzR2VKDH", "original": "HyxWEPIVwH", "number": 256, "cdate": 1569438922117, "ddate": null, "tcdate": 1569438922117, "tmdate": 1584729397026, "tddate": null, "forum": "H1gzR2VKDH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"abstract": "Video prediction models combined with planning algorithms have shown promise in enabling robots to learn to perform many vision-based tasks through only self-supervision, reaching novel goals in cluttered scenes with unseen objects. However, due to the compounding uncertainty in long horizon video prediction and poor scalability of sampling-based planning optimizers, one significant limitation of these approaches is the ability to plan over long horizons to reach distant goals. To that end, we propose a framework for subgoal generation and planning, hierarchical visual foresight (HVF), which generates subgoal images conditioned on a goal image, and uses them for planning. The subgoal images are directly optimized to decompose the task into easy to plan segments, and as a result, we observe that the method naturally identifies semantically meaningful states as subgoals. Across three out of four simulated vision-based manipulation tasks, we find that our method achieves more than 20% absolute performance improvement over planning without subgoals and model-free RL approaches. Further, our experiments illustrate that our approach extends to real, cluttered visual scenes.", "title": "Hierarchical Foresight: Self-Supervised Learning of Long-Horizon Tasks via Visual Subgoal Generation", "code": "https://github.com/suraj-nair-1/google-research/tree/master/hierarchical_foresight", "keywords": ["video prediction", "reinforcement learning", "planning"], "authors": ["Suraj Nair", "Chelsea Finn"], "TL;DR": "Hierarchical visual foresight learns to generate visual subgoals that break down long-horizon tasks into subtasks, using only self-supervision.", "authorids": ["surajn@stanford.edu", "chelseaf@google.com"], "pdf": "/pdf/aa5541d4bdbf7a204c65f956284a12d430af31bb.pdf", "paperhash": "nair|hierarchical_foresight_selfsupervised_learning_of_longhorizon_tasks_via_visual_subgoal_generation", "_bibtex": "@inproceedings{\nNair2020Hierarchical,\ntitle={Hierarchical Foresight: Self-Supervised Learning of Long-Horizon Tasks via Visual Subgoal Generation},\nauthor={Suraj Nair and Chelsea Finn},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gzR2VKDH}\n}", "original_pdf": "/attachment/a7794dcf2031fcb2a2154b4ff3be59427b1a4857.pdf"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 6, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "SYLj1sHTj", "original": null, "number": 1, "cdate": 1576798691614, "ddate": null, "tcdate": 1576798691614, "tmdate": 1576800943687, "tddate": null, "forum": "H1gzR2VKDH", "replyto": "H1gzR2VKDH", "invitation": "ICLR.cc/2020/Conference/Paper256/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "This paper proposes a method that uses subgoals for planning when using video prediction. The reviewers thought that the paper was clearly written and interesting. The reviewer questions and concerns were mostly addressed during the discussion phase, and the reviewers are in agreement that the paper should be accepted.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"abstract": "Video prediction models combined with planning algorithms have shown promise in enabling robots to learn to perform many vision-based tasks through only self-supervision, reaching novel goals in cluttered scenes with unseen objects. However, due to the compounding uncertainty in long horizon video prediction and poor scalability of sampling-based planning optimizers, one significant limitation of these approaches is the ability to plan over long horizons to reach distant goals. To that end, we propose a framework for subgoal generation and planning, hierarchical visual foresight (HVF), which generates subgoal images conditioned on a goal image, and uses them for planning. The subgoal images are directly optimized to decompose the task into easy to plan segments, and as a result, we observe that the method naturally identifies semantically meaningful states as subgoals. Across three out of four simulated vision-based manipulation tasks, we find that our method achieves more than 20% absolute performance improvement over planning without subgoals and model-free RL approaches. Further, our experiments illustrate that our approach extends to real, cluttered visual scenes.", "title": "Hierarchical Foresight: Self-Supervised Learning of Long-Horizon Tasks via Visual Subgoal Generation", "code": "https://github.com/suraj-nair-1/google-research/tree/master/hierarchical_foresight", "keywords": ["video prediction", "reinforcement learning", "planning"], "authors": ["Suraj Nair", "Chelsea Finn"], "TL;DR": "Hierarchical visual foresight learns to generate visual subgoals that break down long-horizon tasks into subtasks, using only self-supervision.", "authorids": ["surajn@stanford.edu", "chelseaf@google.com"], "pdf": "/pdf/aa5541d4bdbf7a204c65f956284a12d430af31bb.pdf", "paperhash": "nair|hierarchical_foresight_selfsupervised_learning_of_longhorizon_tasks_via_visual_subgoal_generation", "_bibtex": "@inproceedings{\nNair2020Hierarchical,\ntitle={Hierarchical Foresight: Self-Supervised Learning of Long-Horizon Tasks via Visual Subgoal Generation},\nauthor={Suraj Nair and Chelsea Finn},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gzR2VKDH}\n}", "original_pdf": "/attachment/a7794dcf2031fcb2a2154b4ff3be59427b1a4857.pdf"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "H1gzR2VKDH", "replyto": "H1gzR2VKDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795722938, "tmdate": 1576800274337, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper256/-/Decision"}}}, {"id": "BklFhY24jH", "original": null, "number": 3, "cdate": 1573337520609, "ddate": null, "tcdate": 1573337520609, "tmdate": 1573337520609, "tddate": null, "forum": "H1gzR2VKDH", "replyto": "Bygq9J7zKr", "invitation": "ICLR.cc/2020/Conference/Paper256/-/Official_Comment", "content": {"title": "Response to Review #1", "comment": "We thank reviewer 1 for their thoughtful comments. We address each comment below:\n\n(1): Thank you for pointing this out - we have switched to only using absolute percentages in the new revision.\n\n(2): We agree that the ability to explore the environment well is a critical assumption of this line of work, and this may not hold for more complex environments. The depth of literature in exploration [1,2,3] can likely be combined with HVF in a straightforward matter. We added a discussion of this to the limitations section in the new revision.\n\n(3): We modified the introduction of the paper to use these terms less frequently. Regardless of the semantics of the terminology \u201clong-horizon\u201d, we empirically find that existing methods, including both model-free and model-based RL methods, struggle to solve the tasks due to their time horizon and sparse reward signal, while HVF enables better performance by reducing the horizon. Regarding \u201cmanipulation\u201d, we agree that we are not doing dexterous/fine grained manipulation, but more coarse object pushing/door sliding. We are open to rephrasing to \u201crobotic control\u201d if the reviewer finds that term preferable.\n\n(4): Learned dynamics models and cost functions from images do have inherent limitations. We have added further discussion of this in the limitations section of the paper.  \n\n(5): This is a great point - and indicates a core challenge in using goal images to specify task. In reality there are only some parts of the image that matter, and the others can vary significantly. Prior work has studied how to combine visual foresight with classifiers that ignore irrelevant factors [4]. An approach such as [4] could be combined with HVF in a straight-forward manner, to measure a more semantic distance to an image of the goal.\n\n[1] Deepak Pathak, Dhiraj Gandhi, and Abhinav Gupta. Self-supervised exploration via disagreement. In Proceedings of the 36th International Conference on Machine Learning, 2019.\n[2] Yuri Burda, Harrison Edwards, Amos Storkey, and Oleg Klimov. Exploration by random network distillation. arXiv preprint arXiv:1810.12894, 2018.\n[3] A. Gupta, R. Mendonca, Y. Liu, P. Abbeel, and S. Levine. Meta-reinforcement learning of structured exploration strategies. arXiv preprint arXiv:1802.07245, 2018.\n[4] A. Xie, A. Singh, S. Levine, and C. Finn, \u201cFew-shot goal inference for visuomotor learning and planning,\u201d Conference on Robot Learning (CoRL), 2018.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper256/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper256/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"abstract": "Video prediction models combined with planning algorithms have shown promise in enabling robots to learn to perform many vision-based tasks through only self-supervision, reaching novel goals in cluttered scenes with unseen objects. However, due to the compounding uncertainty in long horizon video prediction and poor scalability of sampling-based planning optimizers, one significant limitation of these approaches is the ability to plan over long horizons to reach distant goals. To that end, we propose a framework for subgoal generation and planning, hierarchical visual foresight (HVF), which generates subgoal images conditioned on a goal image, and uses them for planning. The subgoal images are directly optimized to decompose the task into easy to plan segments, and as a result, we observe that the method naturally identifies semantically meaningful states as subgoals. Across three out of four simulated vision-based manipulation tasks, we find that our method achieves more than 20% absolute performance improvement over planning without subgoals and model-free RL approaches. Further, our experiments illustrate that our approach extends to real, cluttered visual scenes.", "title": "Hierarchical Foresight: Self-Supervised Learning of Long-Horizon Tasks via Visual Subgoal Generation", "code": "https://github.com/suraj-nair-1/google-research/tree/master/hierarchical_foresight", "keywords": ["video prediction", "reinforcement learning", "planning"], "authors": ["Suraj Nair", "Chelsea Finn"], "TL;DR": "Hierarchical visual foresight learns to generate visual subgoals that break down long-horizon tasks into subtasks, using only self-supervision.", "authorids": ["surajn@stanford.edu", "chelseaf@google.com"], "pdf": "/pdf/aa5541d4bdbf7a204c65f956284a12d430af31bb.pdf", "paperhash": "nair|hierarchical_foresight_selfsupervised_learning_of_longhorizon_tasks_via_visual_subgoal_generation", "_bibtex": "@inproceedings{\nNair2020Hierarchical,\ntitle={Hierarchical Foresight: Self-Supervised Learning of Long-Horizon Tasks via Visual Subgoal Generation},\nauthor={Suraj Nair and Chelsea Finn},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gzR2VKDH}\n}", "original_pdf": "/attachment/a7794dcf2031fcb2a2154b4ff3be59427b1a4857.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1gzR2VKDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper256/Authors", "ICLR.cc/2020/Conference/Paper256/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper256/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper256/Reviewers", "ICLR.cc/2020/Conference/Paper256/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper256/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper256/Authors|ICLR.cc/2020/Conference/Paper256/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504174084, "tmdate": 1576860547209, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper256/Authors", "ICLR.cc/2020/Conference/Paper256/Reviewers", "ICLR.cc/2020/Conference/Paper256/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper256/-/Official_Comment"}}}, {"id": "SkehQY24jr", "original": null, "number": 2, "cdate": 1573337380026, "ddate": null, "tcdate": 1573337380026, "tmdate": 1573337380026, "tddate": null, "forum": "H1gzR2VKDH", "replyto": "rylPuR2RKB", "invitation": "ICLR.cc/2020/Conference/Paper256/-/Official_Comment", "content": {"title": "Response to Review #2", "comment": "We thank reviewer 2 for their comments. We address each comment below:\n\n\u201cCould you provide us any failure cases and explain why this happens?\u201d: We added a discussion of this to Section 5.3 of the revision. The most common failure case is that the visual MPC cost focused too heavily on the arm, and as a result produced plans (and subgoals) which ignore the object changes which are a critical part of the task. One solution to this would be better cost functions which provide a denser, more object relevant cost.\n\nComparison of Computation Cost: We added a detailed discussion of computational cost to the last section of the paper, as follows: Assume that the horizon of the task is T, one iteration of visual MPC has cost C, and HVF is using K subgoals, and searching in a space of N subgoal sequences. Then normal VF/TAP would have cost T*C, while HVF would have cost (T*C) + (K*N*C). However note - the computation of (K*N*C) can be completely parallelized because it is done completely offline (without environment interaction). RIG is significantly more costly in terms of environment interaction, since it learns a model free policy, but evaluating the policy is cheap \u2014 a single feedforward model.\n\n\u201cHow important is the output quality of the video prediction model?\u201d: The quality of the short-term predictions is important for HVF to achieve good performance. For visual MPC *without* HVF to work well, the video predictions must be high quality much further into the future. The gap between oracle subgoals (<70%) and perfect performance (100%) is an indication of how much prediction quality is limiting the performance of HVF.\n\n\u201cAre the learned subgoals reasonable for the final task?\u201d: We qualitatively find that in our tasks the predicted subgoals do seem reasonable for the given tasks. For example in Figure 3, we see that the predicted subgoals map to close to the gaps in the walls, which are bottlenecks in the task. Similarly, in the bottom left of Figure 5, we see that when the task involves sliding the door shut and pushing a block off the table, the generated subgoal corresponds to first sliding the door shut. Additional examples on the BAIR dataset are also in Figure 8 in the appendix.\n\n\u201cCould you verify that subgoals are consistent across different initialization/runs?\u201d: Because there are multiple possible solutions of good subgoals we observe that the subgoals differ a small amount depending on the initialization, as in any stochastic optimization. However the general semantic subgoal is consistent (e.g. grasp the door handle), even if there is slight variation in the images. Regarding better than random performance, our results indicate stronger performance than Time-Agnostic Prediction, a state of the art subgoal prediction method, over 100 random initial states.\n\n\u201cIt is surprising that only 2 goals were enough for the BAIR robot pushing dataset\u201d: The optimal number of subgoals does depend on the complexity of the task, particularly the horizon (as opposed to visual complexity). So despite the visual complexity of the BAIR dataset, 2 subgoals still should effectively break at task into 3 sub segments as observed.\n\n\u201cHow can you find the right number of subgoals?\u201d: Extending the sampling-based optimization over subgoals to also optimize over the number of subgoals is a relatively straight-forward extension of HVF, which we leave for future work.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper256/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper256/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"abstract": "Video prediction models combined with planning algorithms have shown promise in enabling robots to learn to perform many vision-based tasks through only self-supervision, reaching novel goals in cluttered scenes with unseen objects. However, due to the compounding uncertainty in long horizon video prediction and poor scalability of sampling-based planning optimizers, one significant limitation of these approaches is the ability to plan over long horizons to reach distant goals. To that end, we propose a framework for subgoal generation and planning, hierarchical visual foresight (HVF), which generates subgoal images conditioned on a goal image, and uses them for planning. The subgoal images are directly optimized to decompose the task into easy to plan segments, and as a result, we observe that the method naturally identifies semantically meaningful states as subgoals. Across three out of four simulated vision-based manipulation tasks, we find that our method achieves more than 20% absolute performance improvement over planning without subgoals and model-free RL approaches. Further, our experiments illustrate that our approach extends to real, cluttered visual scenes.", "title": "Hierarchical Foresight: Self-Supervised Learning of Long-Horizon Tasks via Visual Subgoal Generation", "code": "https://github.com/suraj-nair-1/google-research/tree/master/hierarchical_foresight", "keywords": ["video prediction", "reinforcement learning", "planning"], "authors": ["Suraj Nair", "Chelsea Finn"], "TL;DR": "Hierarchical visual foresight learns to generate visual subgoals that break down long-horizon tasks into subtasks, using only self-supervision.", "authorids": ["surajn@stanford.edu", "chelseaf@google.com"], "pdf": "/pdf/aa5541d4bdbf7a204c65f956284a12d430af31bb.pdf", "paperhash": "nair|hierarchical_foresight_selfsupervised_learning_of_longhorizon_tasks_via_visual_subgoal_generation", "_bibtex": "@inproceedings{\nNair2020Hierarchical,\ntitle={Hierarchical Foresight: Self-Supervised Learning of Long-Horizon Tasks via Visual Subgoal Generation},\nauthor={Suraj Nair and Chelsea Finn},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gzR2VKDH}\n}", "original_pdf": "/attachment/a7794dcf2031fcb2a2154b4ff3be59427b1a4857.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1gzR2VKDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper256/Authors", "ICLR.cc/2020/Conference/Paper256/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper256/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper256/Reviewers", "ICLR.cc/2020/Conference/Paper256/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper256/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper256/Authors|ICLR.cc/2020/Conference/Paper256/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504174084, "tmdate": 1576860547209, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper256/Authors", "ICLR.cc/2020/Conference/Paper256/Reviewers", "ICLR.cc/2020/Conference/Paper256/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper256/-/Official_Comment"}}}, {"id": "Bygq9J7zKr", "original": null, "number": 1, "cdate": 1571069841841, "ddate": null, "tcdate": 1571069841841, "tmdate": 1572972618712, "tddate": null, "forum": "H1gzR2VKDH", "replyto": "H1gzR2VKDH", "invitation": "ICLR.cc/2020/Conference/Paper256/-/Official_Review", "content": {"rating": "6: Weak Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper introduces a hierarchical extension to existing work in vision-based model predictive control. Here, a hierarchical model is optimised to find sub-goals that minimise the planning cost (bottleneck states), so as to allow for improved planning to goal states expressed in higher dimensional state spaces. As expected, results show that this hierarchy improves tasks execution success rates. \n\nThe paper is clearly written, although many sub-components of the architecture are described in other articles. While I appreciate that the work is incremental, and that it relies on a number of previously modules, this brevity and the architectural complexity means that the work will be challenging to reproduce or adapt to different tasks. \n\nNevertheless, I believe the idea is interesting, and the paper does achieve its stated goals of allowing for entirely self-supervised learning without motion primitives, demonstrations and rewards, so makes a useful contribution here.  Compositional planning through sub-goals is a good idea, and this paper adds a useful mechanism to identify suitable sub-goals in a latent space and plan through these.\n\nHowever, I don't believe that this approach is in any way practically feasible at present, and the paper makes a number of hyperbolic claims that should be toned down to give a more balanced perspective (I appreciate that the authors attempted to do so in their limitations section, but this needs to be done throughout the paper).\n\nSpecifically:\n\n1.) Please avoid the use of relative improvement percentages (200% performance improvements in the abstract - the 20% absolute performance is strong enough to stand alone).\n\n2.) The paper oversimplifies the importance of exploration and data collection in the proposed approach. The paper states that a uniform random policy in the continuous action space of the agent works well, and offsets the data collection to \"any exploration policy\", but this is a key requirement if the proposed approach is to be useful. At present the method seems to be relying on typewriting monkeys writing Shakespeare as a precursor to control. An exploration policy using identified sub-goal of interest may be a particularly valuable extension in future work, but for now the paper glosses over this aspect.\n\n3.) The use of the term \"long-horizon tasks\" and \"manipulation\". In line with point 2, my personal feeling is that any task or state that can conceivably be explored and accessed by uniformly sampling from a continuous action space does not qualify as a long-horizon task. From the results and videos, it seems that all of the tasks could be solved by following a single trajectory in 3D space, ie. swing left to knock objects off the table, then right to close the door. Terming this \"long-horizon\" is a bit of a stretch, as is the notion that flopping about a table bashing into objects counts as \"manipulation\". Motion planning and trajectory following are not long-term horizon manipulation tasks, solving towers of Hanoi is. Moreover, I appreciate that experiments were conducted on real robot datasets, but this seems to be more of an exercise in latent space anthropomorphism than practical evidence of a feasible control policy.\n\n4.) Limitations regarding latent space expressiveness.  As mentioned in the limitations section of the paper, the proposed approach is heavily reliant on a latent space that fully captures the scene and can roll-out future states sensibly. This is an extremely challenging problem, and one which appears to affect the proposed approach, for example, in the accompanying video (3:.40 - 3:59), the desired goal state contains two objects standing on the table and a closed door, but the policy only closes the door, while objects are knocked over. This seems to indicate that the latent space and planning is unable to learn good object embeddings and spatial representations, and that the type of task that can be solved is along the lines of move forward and to the right when the black blob is on the left of the image, or move down to the left, when there are some coloured blobs on the table. \n\n5.) Limitations around goal state representations . Following on from 4, the identification of important aspects in a given image or latent space represents a major challenge to the proposed approach. Given an image of a desired state, how can the proposed approach identify which elements in the scene are required for success, and which are simply distractors? At present, I see no mechanism by which this could ever be learned in a self-play setting or through an image-based goal state. Eg. what if the image indicated that I want the two objects to be in a specific position, and I never cared about the state of the door?\n\nDespite these limitations, the granularity of the solution is fine for a proof of concept work like this, and is itself a commendable achievement. Unfortunately, the paper's choice of language, relying on terms like \"long term horizon\" and \"manipulation\" for simple reaching and pushing tasks exaggerates the state of robot learning, and is potentially misleading to those less familiar with the field.\n\nDespite my gripes, the paper definitely meets the ICLR threshold of \"accept if you'd share it with a colleague\", and I believe it is is a useful approach to sub-goal identification and a nice piece of work, so I recommend acceptance.\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper256/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper256/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"abstract": "Video prediction models combined with planning algorithms have shown promise in enabling robots to learn to perform many vision-based tasks through only self-supervision, reaching novel goals in cluttered scenes with unseen objects. However, due to the compounding uncertainty in long horizon video prediction and poor scalability of sampling-based planning optimizers, one significant limitation of these approaches is the ability to plan over long horizons to reach distant goals. To that end, we propose a framework for subgoal generation and planning, hierarchical visual foresight (HVF), which generates subgoal images conditioned on a goal image, and uses them for planning. The subgoal images are directly optimized to decompose the task into easy to plan segments, and as a result, we observe that the method naturally identifies semantically meaningful states as subgoals. Across three out of four simulated vision-based manipulation tasks, we find that our method achieves more than 20% absolute performance improvement over planning without subgoals and model-free RL approaches. Further, our experiments illustrate that our approach extends to real, cluttered visual scenes.", "title": "Hierarchical Foresight: Self-Supervised Learning of Long-Horizon Tasks via Visual Subgoal Generation", "code": "https://github.com/suraj-nair-1/google-research/tree/master/hierarchical_foresight", "keywords": ["video prediction", "reinforcement learning", "planning"], "authors": ["Suraj Nair", "Chelsea Finn"], "TL;DR": "Hierarchical visual foresight learns to generate visual subgoals that break down long-horizon tasks into subtasks, using only self-supervision.", "authorids": ["surajn@stanford.edu", "chelseaf@google.com"], "pdf": "/pdf/aa5541d4bdbf7a204c65f956284a12d430af31bb.pdf", "paperhash": "nair|hierarchical_foresight_selfsupervised_learning_of_longhorizon_tasks_via_visual_subgoal_generation", "_bibtex": "@inproceedings{\nNair2020Hierarchical,\ntitle={Hierarchical Foresight: Self-Supervised Learning of Long-Horizon Tasks via Visual Subgoal Generation},\nauthor={Suraj Nair and Chelsea Finn},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gzR2VKDH}\n}", "original_pdf": "/attachment/a7794dcf2031fcb2a2154b4ff3be59427b1a4857.pdf"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "H1gzR2VKDH", "replyto": "H1gzR2VKDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper256/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper256/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576378751489, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper256/Reviewers"], "noninvitees": [], "tcdate": 1570237754759, "tmdate": 1576378751504, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper256/-/Official_Review"}}}, {"id": "rylPuR2RKB", "original": null, "number": 2, "cdate": 1571896943371, "ddate": null, "tcdate": 1571896943371, "tmdate": 1572972618669, "tddate": null, "forum": "H1gzR2VKDH", "replyto": "H1gzR2VKDH", "invitation": "ICLR.cc/2020/Conference/Paper256/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "This paper proposes a method, hierarchical visual foresight (HVF) that learns to break down the long horizon tasks into short horizon segments. It first generates the subgoals conditioned on the main goal. These subgoals are optimized to have meaningful states and used for planning. The experiments on Maze navigation, simulated desk manipulation, and real robot manipulation show significant performance gain over the planning method without subgoals and model-free RL. \n\nThe paper tackles a novel and challenging problem for the long-horizon tasks. Also, the paper is well written and easy to understand.\n\nQuestions/Concerns:\n\n- How important is the output quality of the video prediction model? \n\n- Finding subgoals that split into the optimal subtasks seems not easy. Are the learned subgoals actually reasonable for the final task? I can see that Fig. 6 has some examples of subgoals but is hard to recognize what are the subtasks. \n- Also, is there a chance that subgoals are randomly selected? Could you verify that subgoals are consistent across different initialization/runs? \n- Could you provide us any failure cases if there's any and explain us why this happens? \n\n- The number of subgoals needs to be fixed. I assume this number depends on how complex of the task is. It is surprising that only 2 subgoals were enough for the BAIR Robot push dataset (Tab. 1). The authors commented that \"the sampling budget allocated for subgoal optimization is likely insufficient to find a large sequence of subgoals.\". Will it actually be solved by increasing the sampling? Or is it possible that the dataset is too simple or simply finding optimal subgoals is hard.  Could authors comment more about this issue?\n- Also, how can you find the right number of subgoals? \n\n- The computational requirement is one of the weaknesses. Please provide the comparison of how much is the computational cost of the proposed method compared to other ones such as VF (without subgaols), TAP, and RIG. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper256/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper256/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"abstract": "Video prediction models combined with planning algorithms have shown promise in enabling robots to learn to perform many vision-based tasks through only self-supervision, reaching novel goals in cluttered scenes with unseen objects. However, due to the compounding uncertainty in long horizon video prediction and poor scalability of sampling-based planning optimizers, one significant limitation of these approaches is the ability to plan over long horizons to reach distant goals. To that end, we propose a framework for subgoal generation and planning, hierarchical visual foresight (HVF), which generates subgoal images conditioned on a goal image, and uses them for planning. The subgoal images are directly optimized to decompose the task into easy to plan segments, and as a result, we observe that the method naturally identifies semantically meaningful states as subgoals. Across three out of four simulated vision-based manipulation tasks, we find that our method achieves more than 20% absolute performance improvement over planning without subgoals and model-free RL approaches. Further, our experiments illustrate that our approach extends to real, cluttered visual scenes.", "title": "Hierarchical Foresight: Self-Supervised Learning of Long-Horizon Tasks via Visual Subgoal Generation", "code": "https://github.com/suraj-nair-1/google-research/tree/master/hierarchical_foresight", "keywords": ["video prediction", "reinforcement learning", "planning"], "authors": ["Suraj Nair", "Chelsea Finn"], "TL;DR": "Hierarchical visual foresight learns to generate visual subgoals that break down long-horizon tasks into subtasks, using only self-supervision.", "authorids": ["surajn@stanford.edu", "chelseaf@google.com"], "pdf": "/pdf/aa5541d4bdbf7a204c65f956284a12d430af31bb.pdf", "paperhash": "nair|hierarchical_foresight_selfsupervised_learning_of_longhorizon_tasks_via_visual_subgoal_generation", "_bibtex": "@inproceedings{\nNair2020Hierarchical,\ntitle={Hierarchical Foresight: Self-Supervised Learning of Long-Horizon Tasks via Visual Subgoal Generation},\nauthor={Suraj Nair and Chelsea Finn},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gzR2VKDH}\n}", "original_pdf": "/attachment/a7794dcf2031fcb2a2154b4ff3be59427b1a4857.pdf"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "H1gzR2VKDH", "replyto": "H1gzR2VKDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper256/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper256/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576378751489, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper256/Reviewers"], "noninvitees": [], "tcdate": 1570237754759, "tmdate": 1576378751504, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper256/-/Official_Review"}}}, {"id": "rJgTs1NJuB", "original": null, "number": 1, "cdate": 1569828772742, "ddate": null, "tcdate": 1569828772742, "tmdate": 1569904707238, "tddate": null, "forum": "H1gzR2VKDH", "replyto": "H1gzR2VKDH", "invitation": "ICLR.cc/2020/Conference/Paper256/-/Official_Comment", "content": {"comment": "We realized that the original code link was not fully-anonymized. We have disabled sharing on that link. Here is a fully-anonymous link: https://drive.google.com/file/d/1cEQtdVmsYMRr3R0QFrClVNT-9BcgYBWF/view?usp=sharing", "title": "Anonymized Code Link"}, "signatures": ["ICLR.cc/2020/Conference/Paper256/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper256/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"abstract": "Video prediction models combined with planning algorithms have shown promise in enabling robots to learn to perform many vision-based tasks through only self-supervision, reaching novel goals in cluttered scenes with unseen objects. However, due to the compounding uncertainty in long horizon video prediction and poor scalability of sampling-based planning optimizers, one significant limitation of these approaches is the ability to plan over long horizons to reach distant goals. To that end, we propose a framework for subgoal generation and planning, hierarchical visual foresight (HVF), which generates subgoal images conditioned on a goal image, and uses them for planning. The subgoal images are directly optimized to decompose the task into easy to plan segments, and as a result, we observe that the method naturally identifies semantically meaningful states as subgoals. Across three out of four simulated vision-based manipulation tasks, we find that our method achieves more than 20% absolute performance improvement over planning without subgoals and model-free RL approaches. Further, our experiments illustrate that our approach extends to real, cluttered visual scenes.", "title": "Hierarchical Foresight: Self-Supervised Learning of Long-Horizon Tasks via Visual Subgoal Generation", "code": "https://github.com/suraj-nair-1/google-research/tree/master/hierarchical_foresight", "keywords": ["video prediction", "reinforcement learning", "planning"], "authors": ["Suraj Nair", "Chelsea Finn"], "TL;DR": "Hierarchical visual foresight learns to generate visual subgoals that break down long-horizon tasks into subtasks, using only self-supervision.", "authorids": ["surajn@stanford.edu", "chelseaf@google.com"], "pdf": "/pdf/aa5541d4bdbf7a204c65f956284a12d430af31bb.pdf", "paperhash": "nair|hierarchical_foresight_selfsupervised_learning_of_longhorizon_tasks_via_visual_subgoal_generation", "_bibtex": "@inproceedings{\nNair2020Hierarchical,\ntitle={Hierarchical Foresight: Self-Supervised Learning of Long-Horizon Tasks via Visual Subgoal Generation},\nauthor={Suraj Nair and Chelsea Finn},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gzR2VKDH}\n}", "original_pdf": "/attachment/a7794dcf2031fcb2a2154b4ff3be59427b1a4857.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1gzR2VKDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper256/Authors", "ICLR.cc/2020/Conference/Paper256/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper256/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper256/Reviewers", "ICLR.cc/2020/Conference/Paper256/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper256/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper256/Authors|ICLR.cc/2020/Conference/Paper256/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504174084, "tmdate": 1576860547209, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper256/Authors", "ICLR.cc/2020/Conference/Paper256/Reviewers", "ICLR.cc/2020/Conference/Paper256/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper256/-/Official_Comment"}}}], "count": 7}