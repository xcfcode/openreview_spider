{"notes": [{"id": "SyzVb3CcFX", "original": "S1lgKxsKtQ", "number": 1165, "cdate": 1538087932495, "ddate": null, "tcdate": 1538087932495, "tmdate": 1557615533490, "tddate": null, "forum": "SyzVb3CcFX", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Time-Agnostic Prediction: Predicting Predictable Video Frames", "abstract": "Prediction is arguably one of the most basic functions of an intelligent system. In general, the problem of predicting events in the future or between two waypoints is exceedingly difficult. However, most phenomena naturally pass through relatively predictable bottlenecks---while we cannot predict the precise trajectory of a robot arm between being at rest and holding an object up, we can be certain that it must have picked the object up. To exploit this, we decouple visual prediction from a rigid notion of time. While conventional approaches predict frames at regularly spaced temporal intervals, our time-agnostic predictors (TAP) are not tied to specific times so that they may instead discover predictable \"bottleneck\" frames no matter when they occur. We evaluate our approach for future and intermediate frame prediction across three robotic manipulation tasks. Our predictions are not only of higher visual quality, but also correspond to coherent semantic subgoals in temporally extended tasks.", "keywords": ["visual prediction", "subgoal generation", "bottleneck states", "time-agnostic"], "authorids": ["dineshjayaraman@berkeley.edu", "febert@berkeley.edu", "efros@eecs.berkeley.edu", "svlevine@eecs.berkeley.edu"], "authors": ["Dinesh Jayaraman", "Frederik Ebert", "Alexei Efros", "Sergey Levine"], "TL;DR": "In visual prediction tasks, letting your predictive model choose which times to predict does two things: (i) improves prediction quality, and (ii) leads to semantically coherent \"bottleneck state\" predictions, which are useful for planning.", "pdf": "/pdf/4a60fe530d6e791c663c8729169d0e471aa8e78f.pdf", "paperhash": "jayaraman|timeagnostic_prediction_predicting_predictable_video_frames", "_bibtex": "@inproceedings{\njayaraman2018timeagnostic,\ntitle={Time-Agnostic Prediction: Predicting Predictable Video Frames},\nauthor={Dinesh Jayaraman and Frederik Ebert and Alexei Efros and Sergey Levine},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SyzVb3CcFX},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "rylvZgZqxE", "original": null, "number": 9, "cdate": 1545371646996, "ddate": null, "tcdate": 1545371646996, "tmdate": 1545371646996, "tddate": null, "forum": "SyzVb3CcFX", "replyto": "B1x59ApR14", "invitation": "ICLR.cc/2019/Conference/-/Paper1165/Official_Comment", "content": {"title": "Clarifying contribution and real-data experiments", "comment": "Thank you, we are glad to have the opportunity to present our work at ICLR 2019!\n\nA couple of clarifications for interested readers:\n(i) This paper's contribution is not about maintaining prediction uncertainty through VAEs. Instead, the idea is to allow predictors to select which timesteps to make predictions about. We show that this not only improves prediction quality but also consistently predicts semantically coherent changepoints that can be used, for instance, as subgoals for planning.\n(ii) We do in fact have results for real videos (the BAIR pushing dataset) both in our paper and on the website.\n\nThank you,\n(On behalf of the authors)"}, "signatures": ["ICLR.cc/2019/Conference/Paper1165/Authors"], "readers": ["ICLR.cc/2019/Conference/Paper1165/Authors", "everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1165/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1165/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Time-Agnostic Prediction: Predicting Predictable Video Frames", "abstract": "Prediction is arguably one of the most basic functions of an intelligent system. In general, the problem of predicting events in the future or between two waypoints is exceedingly difficult. However, most phenomena naturally pass through relatively predictable bottlenecks---while we cannot predict the precise trajectory of a robot arm between being at rest and holding an object up, we can be certain that it must have picked the object up. To exploit this, we decouple visual prediction from a rigid notion of time. While conventional approaches predict frames at regularly spaced temporal intervals, our time-agnostic predictors (TAP) are not tied to specific times so that they may instead discover predictable \"bottleneck\" frames no matter when they occur. We evaluate our approach for future and intermediate frame prediction across three robotic manipulation tasks. Our predictions are not only of higher visual quality, but also correspond to coherent semantic subgoals in temporally extended tasks.", "keywords": ["visual prediction", "subgoal generation", "bottleneck states", "time-agnostic"], "authorids": ["dineshjayaraman@berkeley.edu", "febert@berkeley.edu", "efros@eecs.berkeley.edu", "svlevine@eecs.berkeley.edu"], "authors": ["Dinesh Jayaraman", "Frederik Ebert", "Alexei Efros", "Sergey Levine"], "TL;DR": "In visual prediction tasks, letting your predictive model choose which times to predict does two things: (i) improves prediction quality, and (ii) leads to semantically coherent \"bottleneck state\" predictions, which are useful for planning.", "pdf": "/pdf/4a60fe530d6e791c663c8729169d0e471aa8e78f.pdf", "paperhash": "jayaraman|timeagnostic_prediction_predicting_predictable_video_frames", "_bibtex": "@inproceedings{\njayaraman2018timeagnostic,\ntitle={Time-Agnostic Prediction: Predicting Predictable Video Frames},\nauthor={Dinesh Jayaraman and Frederik Ebert and Alexei Efros and Sergey Levine},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SyzVb3CcFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1165/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619161, "tddate": null, "super": null, "final": null, "reply": {"forum": "SyzVb3CcFX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1165/Authors", "ICLR.cc/2019/Conference/Paper1165/Reviewers", "ICLR.cc/2019/Conference/Paper1165/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1165/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1165/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1165/Authors|ICLR.cc/2019/Conference/Paper1165/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1165/Reviewers", "ICLR.cc/2019/Conference/Paper1165/Authors", "ICLR.cc/2019/Conference/Paper1165/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619161}}}, {"id": "B1x59ApR14", "original": null, "number": 1, "cdate": 1544638097860, "ddate": null, "tcdate": 1544638097860, "tmdate": 1545354502975, "tddate": null, "forum": "SyzVb3CcFX", "replyto": "SyzVb3CcFX", "invitation": "ICLR.cc/2019/Conference/-/Paper1165/Meta_Review", "content": {"metareview": "The paper introduces a new and convincing method for video frame prediction, by adding prediction uncertainty through VAEs.  The results are convincing, and the reviewers are convinced.\n\nIt's unfortunate however that the method is only evaluated on simulated data.  Letting it loose on real data would cement the results and merit oral representation; in the current form, poster presentation is recommended.", "confidence": "5: The area chair is absolutely certain", "recommendation": "Accept (Poster)", "title": "solid work, would merit from more experimentation"}, "signatures": ["ICLR.cc/2019/Conference/Paper1165/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper1165/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Time-Agnostic Prediction: Predicting Predictable Video Frames", "abstract": "Prediction is arguably one of the most basic functions of an intelligent system. In general, the problem of predicting events in the future or between two waypoints is exceedingly difficult. However, most phenomena naturally pass through relatively predictable bottlenecks---while we cannot predict the precise trajectory of a robot arm between being at rest and holding an object up, we can be certain that it must have picked the object up. To exploit this, we decouple visual prediction from a rigid notion of time. While conventional approaches predict frames at regularly spaced temporal intervals, our time-agnostic predictors (TAP) are not tied to specific times so that they may instead discover predictable \"bottleneck\" frames no matter when they occur. We evaluate our approach for future and intermediate frame prediction across three robotic manipulation tasks. Our predictions are not only of higher visual quality, but also correspond to coherent semantic subgoals in temporally extended tasks.", "keywords": ["visual prediction", "subgoal generation", "bottleneck states", "time-agnostic"], "authorids": ["dineshjayaraman@berkeley.edu", "febert@berkeley.edu", "efros@eecs.berkeley.edu", "svlevine@eecs.berkeley.edu"], "authors": ["Dinesh Jayaraman", "Frederik Ebert", "Alexei Efros", "Sergey Levine"], "TL;DR": "In visual prediction tasks, letting your predictive model choose which times to predict does two things: (i) improves prediction quality, and (ii) leads to semantically coherent \"bottleneck state\" predictions, which are useful for planning.", "pdf": "/pdf/4a60fe530d6e791c663c8729169d0e471aa8e78f.pdf", "paperhash": "jayaraman|timeagnostic_prediction_predicting_predictable_video_frames", "_bibtex": "@inproceedings{\njayaraman2018timeagnostic,\ntitle={Time-Agnostic Prediction: Predicting Predictable Video Frames},\nauthor={Dinesh Jayaraman and Frederik Ebert and Alexei Efros and Sergey Levine},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SyzVb3CcFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1165/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545352942139, "tddate": null, "super": null, "final": null, "reply": {"forum": "SyzVb3CcFX", "replyto": "SyzVb3CcFX", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1165/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper1165/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1165/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545352942139}}}, {"id": "H1eLJv-cn7", "original": null, "number": 2, "cdate": 1541179102290, "ddate": null, "tcdate": 1541179102290, "tmdate": 1542979603884, "tddate": null, "forum": "SyzVb3CcFX", "replyto": "SyzVb3CcFX", "invitation": "ICLR.cc/2019/Conference/-/Paper1165/Official_Review", "content": {"title": "Very interesting proposal and well-written method. Experiments section is though poorly structured", "review": "Revision\n----------\nThanks for taking the comments on board. I like the paper, before and after, and so do the other reviewers. Some video results might prove more valuable to follow than the tiny figures in the paper and supplementary. Adding notes on limitations is helpful to understand future extensions.\n\n-----------------------------------------------\nInitial Feedback:\n---------------------\nThis is a very exciting proposal that deviates from the typical assumption that all future frames can be predicted with the same certainty. Instead, motivated by the benefits of discovering bottlenecks for hierarchical RL, this work attempts to predict \u2018predictable video frames\u2019 \u2013 those that can be predicted with certainty, through minimising over all future frames (in forward prediction) or all the sequence (in bidirectional prediction). Additional, the paper tops this with a variational autoencoder to encode uncertainty, even within those predictable frames, as well as a GAN for pixel-level generation of future frames. \n\nThe first few pages of the paper are a joy to read and convincing by default without looking at experimental evidence. I do not work myself in video prediction, but having read in the area I believe the proposal is very novel and could make a significant shift in how prediction is currently perceived. It is a paper that is easy to recommend for publication based on the formulation novelty, topped with VAEs and GANs as/when needed.\n\nBeyond the method\u2019s explanation, I found the experiment section to be poorly structured. The figures are small and difficult to follow \u2013 looking at all the figures it felt that \u201cmore is actually less\u201d. Many of the evidence required to understand the method are only included in the appendices. However, having spent the time to go back and forth, I believe the experiments to be scientifically sound and convincing.\n\nI would have liked a discussion in the conclusion on the method\u2019s limitation. This reviewer believes that the approach will struggle to deal with cyclic motions. In this case the discovered bottlenecks might not be the most useful to predict, as these will correspond to future frames (not nearby though) that are visually similar to the start (in forward) or to the start/end (in bidirectional) frames. An additional loss to reward difficult-to-predict frames (though certain compared to other times) might be an interesting additional to conquer more realistic (rather than synthetic) video sequences.\n", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1165/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Time-Agnostic Prediction: Predicting Predictable Video Frames", "abstract": "Prediction is arguably one of the most basic functions of an intelligent system. In general, the problem of predicting events in the future or between two waypoints is exceedingly difficult. However, most phenomena naturally pass through relatively predictable bottlenecks---while we cannot predict the precise trajectory of a robot arm between being at rest and holding an object up, we can be certain that it must have picked the object up. To exploit this, we decouple visual prediction from a rigid notion of time. While conventional approaches predict frames at regularly spaced temporal intervals, our time-agnostic predictors (TAP) are not tied to specific times so that they may instead discover predictable \"bottleneck\" frames no matter when they occur. We evaluate our approach for future and intermediate frame prediction across three robotic manipulation tasks. Our predictions are not only of higher visual quality, but also correspond to coherent semantic subgoals in temporally extended tasks.", "keywords": ["visual prediction", "subgoal generation", "bottleneck states", "time-agnostic"], "authorids": ["dineshjayaraman@berkeley.edu", "febert@berkeley.edu", "efros@eecs.berkeley.edu", "svlevine@eecs.berkeley.edu"], "authors": ["Dinesh Jayaraman", "Frederik Ebert", "Alexei Efros", "Sergey Levine"], "TL;DR": "In visual prediction tasks, letting your predictive model choose which times to predict does two things: (i) improves prediction quality, and (ii) leads to semantically coherent \"bottleneck state\" predictions, which are useful for planning.", "pdf": "/pdf/4a60fe530d6e791c663c8729169d0e471aa8e78f.pdf", "paperhash": "jayaraman|timeagnostic_prediction_predicting_predictable_video_frames", "_bibtex": "@inproceedings{\njayaraman2018timeagnostic,\ntitle={Time-Agnostic Prediction: Predicting Predictable Video Frames},\nauthor={Dinesh Jayaraman and Frederik Ebert and Alexei Efros and Sergey Levine},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SyzVb3CcFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1165/Official_Review", "cdate": 1542234290768, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SyzVb3CcFX", "replyto": "SyzVb3CcFX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1165/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335886879, "tmdate": 1552335886879, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1165/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "BJlRH2cjp7", "original": null, "number": 4, "cdate": 1542331461626, "ddate": null, "tcdate": 1542331461626, "tmdate": 1542392423104, "tddate": null, "forum": "SyzVb3CcFX", "replyto": "H1eLJv-cn7", "invitation": "ICLR.cc/2019/Conference/-/Paper1165/Official_Comment", "content": {"title": "(R1 review response) Limitations para added, possible additions from appendices to main text?", "comment": "Thank you for your thoughtful feedback. We address your concerns below.\n\n* \u201cBeyond the method\u2019s explanation, I found the experiment section to be poorly structured. The figures are small and difficult to follow \u2013 looking at all the figures it felt that \u201cmore is actually less\u201d. Many of the evidence required to understand the method are only included in the appendices. However, having spent the time to go back and forth, I believe the experiments to be scientifically sound and convincing.\u201d\n\nThank you for taking the time to go through the appendices to understand our submission better, and apologies for having made this necessary. While we tried to keep the main manuscript concise, this was intended to aid readability and comprehension rather than hinder it. Is there any particular information you would suggest as particularly important to move up from appendices to the main text? We could also use some part of the remaining space to Figs 5, 6, 7, 8, 9 (showing prediction results from various methods) larger. Accounting for changes after other reviewer feedback, we now have about 1.2 pages left to reach the 10-page maximum. \n\n---------------------------------------------\n* \u201cI would have liked a discussion in the conclusion on the method\u2019s limitation. This reviewer believes that the approach will struggle to deal with cyclic motions. In this case the discovered bottlenecks might not be the most useful to predict, as these will correspond to future frames (not nearby though) that are visually similar to the start (in forward) or to the start/end (in bidirectional) frames. \u201d\n\nThank you for this suggestion. We have added a discussion of limitations to Sec 5, including this point. Yes, TAP in cyclic cases may not work in its current form. In particular, we expect that TAP will converge to the easy solution of repeating the input frame, since it will recur in the course of a cyclic motion. One solution is to use $\\mathcal{E}\u2019$ in the generalized minimum formulation of Sec 3.2 to express a preference for predicting frames that are different from the input frames. Another possibility is to preprocess states in some way to get TAP to work meaningfully. For instance, if the state is modified by appending the state visitation count, then the \u201ccyclicity\u201d of the trajectory would be destroyed so that bottlenecks are once again meaningful. \n\n---------------------------------------------\n* \u201cAn additional loss to reward difficult-to-predict frames (though certain compared to other times) might be an interesting additional to conquer more realistic (rather than synthetic) video sequences.\u201d\n\nOur generalized minimum formulation (Sec 3.2) already permits expressing additional objectives apart from ease of prediction. For instance, when we set $w(t)$ in future frame prediction to increase linearly with time as in Sec 4, we are indicating that while farther-away frames may be harder to predict, we would still prefer to predict those, so long as the prediction error is not too high. The idea of preferring predictions that are different from the input frames may also help address this point (cf. response to question about cyclicity above). Please also note that we already show results on BAIR pushing videos (non-synthetic video sequences), but we agree that results on videos from a broader domain would be interesting."}, "signatures": ["ICLR.cc/2019/Conference/Paper1165/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1165/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1165/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Time-Agnostic Prediction: Predicting Predictable Video Frames", "abstract": "Prediction is arguably one of the most basic functions of an intelligent system. In general, the problem of predicting events in the future or between two waypoints is exceedingly difficult. However, most phenomena naturally pass through relatively predictable bottlenecks---while we cannot predict the precise trajectory of a robot arm between being at rest and holding an object up, we can be certain that it must have picked the object up. To exploit this, we decouple visual prediction from a rigid notion of time. While conventional approaches predict frames at regularly spaced temporal intervals, our time-agnostic predictors (TAP) are not tied to specific times so that they may instead discover predictable \"bottleneck\" frames no matter when they occur. We evaluate our approach for future and intermediate frame prediction across three robotic manipulation tasks. Our predictions are not only of higher visual quality, but also correspond to coherent semantic subgoals in temporally extended tasks.", "keywords": ["visual prediction", "subgoal generation", "bottleneck states", "time-agnostic"], "authorids": ["dineshjayaraman@berkeley.edu", "febert@berkeley.edu", "efros@eecs.berkeley.edu", "svlevine@eecs.berkeley.edu"], "authors": ["Dinesh Jayaraman", "Frederik Ebert", "Alexei Efros", "Sergey Levine"], "TL;DR": "In visual prediction tasks, letting your predictive model choose which times to predict does two things: (i) improves prediction quality, and (ii) leads to semantically coherent \"bottleneck state\" predictions, which are useful for planning.", "pdf": "/pdf/4a60fe530d6e791c663c8729169d0e471aa8e78f.pdf", "paperhash": "jayaraman|timeagnostic_prediction_predicting_predictable_video_frames", "_bibtex": "@inproceedings{\njayaraman2018timeagnostic,\ntitle={Time-Agnostic Prediction: Predicting Predictable Video Frames},\nauthor={Dinesh Jayaraman and Frederik Ebert and Alexei Efros and Sergey Levine},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SyzVb3CcFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1165/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619161, "tddate": null, "super": null, "final": null, "reply": {"forum": "SyzVb3CcFX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1165/Authors", "ICLR.cc/2019/Conference/Paper1165/Reviewers", "ICLR.cc/2019/Conference/Paper1165/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1165/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1165/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1165/Authors|ICLR.cc/2019/Conference/Paper1165/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1165/Reviewers", "ICLR.cc/2019/Conference/Paper1165/Authors", "ICLR.cc/2019/Conference/Paper1165/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619161}}}, {"id": "SJgSfKuoTX", "original": null, "number": 2, "cdate": 1542322444958, "ddate": null, "tcdate": 1542322444958, "tmdate": 1542355597042, "tddate": null, "forum": "SyzVb3CcFX", "replyto": "BkevZnkq2Q", "invitation": "ICLR.cc/2019/Conference/-/Paper1165/Official_Comment", "content": {"title": "(R3 review response) Generality of planning subgoals provided by Time-Agnostic Prediction", "comment": "Thank you for your feedback and careful observations. \n\n-------------------------------------------------------------------------\n* \u201cThe hierarchical planning evaluation experiment seems like it would clearly favor TAP compared to a fixed model (why would the middle prediction in time of the fixed model correspond to reasonable planning goals?).\u201d\n\nOur hierarchical planning experiment is designed to evaluate the utility of a subgoal as follows: a planner spends half its time budget trying to move towards the subgoal, and the remaining half moving towards the end goal. Good subgoals are those that lead to better goal-reaching performance. Since the planner spends exactly half its time planning towards the subgoal, the middle frame is the most obvious choice for a subgoal---this is why FIX targets the middle frame. We will make this clearer in the text. \n\nHowever, if the above argument is not convincing, please let us know if the following experiment would help alleviate your concern. Rather than *predict* the middle frame, we could use the ground truth middle frame as a subgoal---let us call this baseline \u201dGT-MIDDLE\u201d.  More specifically, we would sample a trajectory and the task would be to get from its start state to its end state, using the true middle image in that trajectory as a subgoal. This would answer the question: \u201cif we were *perfectly* able to predict the middle frame, would that serve as a useful subgoal for planning?\u201d If GT-MIDDLE is shown to work well, then we could conclude that predicting the middle frame would therefore provide a reasonable baseline.\n\nPlease let us know if you believe that such an experiment would be informative and help to address your concern. Otherwise, do you perhaps have an alternative suggestion for a subgoal generation baseline?\n\n-------------------------------------------------------------------------\n* \u201cFurthermore, for certain tasks and environments it seems like the uncertain frames might be the ones that correspond to important subgoals. For example, for the BAIR Push Dataset, usually the harder frames to predict are the arm-object interactions, which probably would correspond to the relevant subgoals.\u201d\n\nAs shown in the paper, in all our experiments, the low-uncertainty predictions from TAP have corresponded to semantically coherent task decompositions and intuitive subgoals. We believe there are good reasons to expect this to hold generally. Suppose that we had a training dataset that consisted of all possible trajectories between every pair of start and goal states. For example, in the maze in Fig 1, where start and goal states are fixed, suppose our training dataset included *every* possible successful trajectory. Then the easiest-to-predict frames would correspond to frames that occur in *every* possible trajectory between start and goal, which are intuitively good subgoals to aim for. In the maze of Fig 1, these would be the asterisks. Note that other positions in the maze (which would be more uncertain because they do not occur in all trajectories) could also be valid subgoals. But while the asterisks are guaranteed to lie on the shortest path from start to goal, more uncertain subgoals could lead to detours from this shortest path.\n\nMore specifically for the BAIR pushing case in your comment, some example BAIR pushing videos are shown at https://sites.google.com/view/ta-pred/home#h.p_MyPmVZLyHypx. They consist of random arm motions, so that the arm is never continuously in contact with objects throughout the video. When an object is displaced in a video, our method usually produces images of the arm initiating or ending contact with that object, as shown in Fig 8. These correspond to low-uncertainty bottlenecks---the arm *must* have come into contact with that object, no matter how precisely the pushing motion occurred. We believe these states are also the most relevant subgoals in these cases, since they represent states that any successful trajectory has to pass through, as argued above.\n\nIt is possible to imagine other more difficult settings than BAIR pushing, where bottlenecks would correspond to images of an arm dynamically pushing an object, which, we agree, is a harder prediction task. Since TAP is designed to select *relatively* easier frames, this would not affect it adversely; it would continue to predict the easiest among those difficult frames. \n\n--------------------------------------------------------\nOverall, we broadly agree it is difficult to rigorously claim that TAP *always* discovers meaningful subgoals, since there is no agreed-upon notion of what constitutes a good subgoal. In our responses above, we argue that TAP naturally targets one reasonable notion of a good subgoal --- a state that would occur in a large fraction of trajectories between start and goal states. We will qualify the subgoals claim more clearly in this way in the text if R3 agrees that this would be appropriate."}, "signatures": ["ICLR.cc/2019/Conference/Paper1165/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1165/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1165/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Time-Agnostic Prediction: Predicting Predictable Video Frames", "abstract": "Prediction is arguably one of the most basic functions of an intelligent system. In general, the problem of predicting events in the future or between two waypoints is exceedingly difficult. However, most phenomena naturally pass through relatively predictable bottlenecks---while we cannot predict the precise trajectory of a robot arm between being at rest and holding an object up, we can be certain that it must have picked the object up. To exploit this, we decouple visual prediction from a rigid notion of time. While conventional approaches predict frames at regularly spaced temporal intervals, our time-agnostic predictors (TAP) are not tied to specific times so that they may instead discover predictable \"bottleneck\" frames no matter when they occur. We evaluate our approach for future and intermediate frame prediction across three robotic manipulation tasks. Our predictions are not only of higher visual quality, but also correspond to coherent semantic subgoals in temporally extended tasks.", "keywords": ["visual prediction", "subgoal generation", "bottleneck states", "time-agnostic"], "authorids": ["dineshjayaraman@berkeley.edu", "febert@berkeley.edu", "efros@eecs.berkeley.edu", "svlevine@eecs.berkeley.edu"], "authors": ["Dinesh Jayaraman", "Frederik Ebert", "Alexei Efros", "Sergey Levine"], "TL;DR": "In visual prediction tasks, letting your predictive model choose which times to predict does two things: (i) improves prediction quality, and (ii) leads to semantically coherent \"bottleneck state\" predictions, which are useful for planning.", "pdf": "/pdf/4a60fe530d6e791c663c8729169d0e471aa8e78f.pdf", "paperhash": "jayaraman|timeagnostic_prediction_predicting_predictable_video_frames", "_bibtex": "@inproceedings{\njayaraman2018timeagnostic,\ntitle={Time-Agnostic Prediction: Predicting Predictable Video Frames},\nauthor={Dinesh Jayaraman and Frederik Ebert and Alexei Efros and Sergey Levine},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SyzVb3CcFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1165/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619161, "tddate": null, "super": null, "final": null, "reply": {"forum": "SyzVb3CcFX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1165/Authors", "ICLR.cc/2019/Conference/Paper1165/Reviewers", "ICLR.cc/2019/Conference/Paper1165/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1165/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1165/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1165/Authors|ICLR.cc/2019/Conference/Paper1165/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1165/Reviewers", "ICLR.cc/2019/Conference/Paper1165/Authors", "ICLR.cc/2019/Conference/Paper1165/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619161}}}, {"id": "SJxRR70saQ", "original": null, "number": 5, "cdate": 1542345685629, "ddate": null, "tcdate": 1542345685629, "tmdate": 1542348013354, "tddate": null, "forum": "SyzVb3CcFX", "replyto": "Hkl7KkHj6X", "invitation": "ICLR.cc/2019/Conference/-/Paper1165/Official_Comment", "content": {"title": "Adding a para pointing out as concurrent work", "comment": "Thanks for bringing this to our notice, and congrats on the great paper! We have added a brief paragraph in related work pointing this out as concurrent work with similar ideas.\n> \"Concurrently with us, Neitz et al 2018 also propose a similar idea that allows a predictor to select when to predict, and their experiments demonstrate its advantages in specially constructed tasks with clear bottlenecks. In comparison, we propose not just the basic time-agnostic loss (Sec 3.1), but also improvements in Sec 3.2 to 3.4 that allow time-agnostic prediction to work in more general tasks such as synthetic and real videos of robotic object manipulation. Our experiments also test the quality of discovered bottlenecks in these scenarios and their usefulness as subgoals for hierarchical planning.\""}, "signatures": ["ICLR.cc/2019/Conference/Paper1165/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1165/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1165/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Time-Agnostic Prediction: Predicting Predictable Video Frames", "abstract": "Prediction is arguably one of the most basic functions of an intelligent system. In general, the problem of predicting events in the future or between two waypoints is exceedingly difficult. However, most phenomena naturally pass through relatively predictable bottlenecks---while we cannot predict the precise trajectory of a robot arm between being at rest and holding an object up, we can be certain that it must have picked the object up. To exploit this, we decouple visual prediction from a rigid notion of time. While conventional approaches predict frames at regularly spaced temporal intervals, our time-agnostic predictors (TAP) are not tied to specific times so that they may instead discover predictable \"bottleneck\" frames no matter when they occur. We evaluate our approach for future and intermediate frame prediction across three robotic manipulation tasks. Our predictions are not only of higher visual quality, but also correspond to coherent semantic subgoals in temporally extended tasks.", "keywords": ["visual prediction", "subgoal generation", "bottleneck states", "time-agnostic"], "authorids": ["dineshjayaraman@berkeley.edu", "febert@berkeley.edu", "efros@eecs.berkeley.edu", "svlevine@eecs.berkeley.edu"], "authors": ["Dinesh Jayaraman", "Frederik Ebert", "Alexei Efros", "Sergey Levine"], "TL;DR": "In visual prediction tasks, letting your predictive model choose which times to predict does two things: (i) improves prediction quality, and (ii) leads to semantically coherent \"bottleneck state\" predictions, which are useful for planning.", "pdf": "/pdf/4a60fe530d6e791c663c8729169d0e471aa8e78f.pdf", "paperhash": "jayaraman|timeagnostic_prediction_predicting_predictable_video_frames", "_bibtex": "@inproceedings{\njayaraman2018timeagnostic,\ntitle={Time-Agnostic Prediction: Predicting Predictable Video Frames},\nauthor={Dinesh Jayaraman and Frederik Ebert and Alexei Efros and Sergey Levine},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SyzVb3CcFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1165/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619161, "tddate": null, "super": null, "final": null, "reply": {"forum": "SyzVb3CcFX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1165/Authors", "ICLR.cc/2019/Conference/Paper1165/Reviewers", "ICLR.cc/2019/Conference/Paper1165/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1165/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1165/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1165/Authors|ICLR.cc/2019/Conference/Paper1165/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1165/Reviewers", "ICLR.cc/2019/Conference/Paper1165/Authors", "ICLR.cc/2019/Conference/Paper1165/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619161}}}, {"id": "Skl_eodiT7", "original": null, "number": 3, "cdate": 1542322927818, "ddate": null, "tcdate": 1542322927818, "tmdate": 1542330173556, "tddate": null, "forum": "SyzVb3CcFX", "replyto": "Byxjv_mHam", "invitation": "ICLR.cc/2019/Conference/-/Paper1165/Official_Comment", "content": {"title": "BAIR  pushing videos added", "comment": "Thank you for these suggestions. We have now added BAIR pushing videos to the website.\n\nKTH has some of the cyclic structure that R1 references in their review, so it may not be a good fit for TAP out of the box (we are adding a note on this limitation in our conclusions section). We will nevertheless attempt this soon, after prioritizing official review responses."}, "signatures": ["ICLR.cc/2019/Conference/Paper1165/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1165/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1165/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Time-Agnostic Prediction: Predicting Predictable Video Frames", "abstract": "Prediction is arguably one of the most basic functions of an intelligent system. In general, the problem of predicting events in the future or between two waypoints is exceedingly difficult. However, most phenomena naturally pass through relatively predictable bottlenecks---while we cannot predict the precise trajectory of a robot arm between being at rest and holding an object up, we can be certain that it must have picked the object up. To exploit this, we decouple visual prediction from a rigid notion of time. While conventional approaches predict frames at regularly spaced temporal intervals, our time-agnostic predictors (TAP) are not tied to specific times so that they may instead discover predictable \"bottleneck\" frames no matter when they occur. We evaluate our approach for future and intermediate frame prediction across three robotic manipulation tasks. Our predictions are not only of higher visual quality, but also correspond to coherent semantic subgoals in temporally extended tasks.", "keywords": ["visual prediction", "subgoal generation", "bottleneck states", "time-agnostic"], "authorids": ["dineshjayaraman@berkeley.edu", "febert@berkeley.edu", "efros@eecs.berkeley.edu", "svlevine@eecs.berkeley.edu"], "authors": ["Dinesh Jayaraman", "Frederik Ebert", "Alexei Efros", "Sergey Levine"], "TL;DR": "In visual prediction tasks, letting your predictive model choose which times to predict does two things: (i) improves prediction quality, and (ii) leads to semantically coherent \"bottleneck state\" predictions, which are useful for planning.", "pdf": "/pdf/4a60fe530d6e791c663c8729169d0e471aa8e78f.pdf", "paperhash": "jayaraman|timeagnostic_prediction_predicting_predictable_video_frames", "_bibtex": "@inproceedings{\njayaraman2018timeagnostic,\ntitle={Time-Agnostic Prediction: Predicting Predictable Video Frames},\nauthor={Dinesh Jayaraman and Frederik Ebert and Alexei Efros and Sergey Levine},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SyzVb3CcFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1165/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619161, "tddate": null, "super": null, "final": null, "reply": {"forum": "SyzVb3CcFX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1165/Authors", "ICLR.cc/2019/Conference/Paper1165/Reviewers", "ICLR.cc/2019/Conference/Paper1165/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1165/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1165/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1165/Authors|ICLR.cc/2019/Conference/Paper1165/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1165/Reviewers", "ICLR.cc/2019/Conference/Paper1165/Authors", "ICLR.cc/2019/Conference/Paper1165/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619161}}}, {"id": "BJxcsIMF6m", "original": null, "number": 1, "cdate": 1542166177947, "ddate": null, "tcdate": 1542166177947, "tmdate": 1542328875645, "tddate": null, "forum": "SyzVb3CcFX", "replyto": "B1g2eZUqnm", "invitation": "ICLR.cc/2019/Conference/-/Paper1165/Official_Comment", "content": {"title": "(R2 review response) Clarifications and pdf updates", "comment": "Thank you for your detailed questions and suggestions. We address your concerns below.\n-------------------------------------------------------------------------\n* \"What results does figure 4 present? Are they only for the grasping sequence? Please specify. \"\n\nYes, Fig 4 is only for the grasping sequence. As stated in the para under \u201cForward prediction\u201d on Pg 6, this is a scatter plot of minimum l1 error versus the closest-matching step for various models. We have changed the caption to present this information near the figure.\n\n-------------------------------------------------------------------------\n* \"In connection with the previous comment, I think the results would be more readable if the match-steps were normalized for each sequence (at least for Figure 4). There would be a clearer mapping between fixT methods and the normalized matching step (e.g., we would expect fix0.75 to achieve a matching step of 0.75 instead of 6 / ? ).\" \n\nThank you for pointing this out. We agree this does aid readability. We now present normalized match-step in both the figure as well as the table, for uniformity.\n\n-------------------------------------------------------------------------\n* \u201cThe statement \u201cthe genmin w(t) preference is bell-shaped and varies from 2/3 at the ends to 1 at the middle frame\u201d is vague.\u201d\n\nAgreed that this information should be more clearly presented. We omitted this in the submission to save space, but have included this in Appendix E now. In our experiments, w(t) was constructed as follows: the weight would rise linearly from baseval=0.66 at the first frame to 1.0 at the fifth frame, then stay at 1.0 for T-10 frames. From the (T-5)-th frame, it would drop linearly to baseval once more at the last frame. The only hyperparameter we tuned was baseval (search over 2/3 and 1/3).\n\n-------------------------------------------------------------------------\n* \u201cSection 4, Bottleneck discovery frequency. I am not entirely convinced by the measuring of bottleneck states. You say that a distance is computed between the predicted object position and the ground-truth object position. If a model were to output exactly the same frame as given in context, would the distance be zero? If so, doesn\u2019t that mean that a model who predicts a non-bottleneck state before or after the robotic arm moves the pieces is estimated to have a very good bottleneck prediction frequency? I found this part of the paper the hardest to follow and the least convincing.\u201d\n\nA model that output the same frame as given in context would actually incur a heavy error. The distance is computed between the predicted object positions and the ground truth object positions *when exactly one of the two objects has been moved*, which may be reasonably assumed  to be the \u201cbottleneck state\u201d in this task. This means that outputting the starting context frame or the ending context frame would both produce heavy distance errors: the entire displacement of the first object, or the entire displacement of the second object, respectively.\n\nUnfortunately, the details of this measurement are quite involved, so we were forced to relegate this to Appendix E (now Appendix F after revisions). We have now added the example suggested by your comment to the paragraph in Section 4, to serve as an intuitive representative of the behavior of our method. Please let us know if this helps make things clearer.\n\n----------------------------------------------------------------------------\n* \"I\u2019m curious as to why you called the method in section 3.2 the \u201cGeneralized minimum\u201d? It feels more like a weighted (or preference weighted) minimum to me and confused me a few times as I was reading the paper (GENerative? GENeralized? what\u2019s general about it?). Just a comment.\"\n\nThe generalization here has to do with the relationship between Eq 4 and 5 in the submission. To call it \u201cgeneralized minimum\u201d is indeed not the most precise since there may be many other ways to generalize the minimum operator, but we choose to call it this for want of a better, concise term.\n\nHere is the case for considering it a generalization of the minimum. A standard minimum over i of a function f(i) can be written as: min_i f(i) = f({argmin_i f(i)}), as in Eq 4 in the submission. \n\nNow note that there are two occurrences of f(i) in the RHS expression above. The \u201cgeneralized minimum\u201d of Eq 5 generalizes this by allowing those two functions to be different as long as they are defined over the same domain:  genmin_i f(i) = f({argmin_i g(i)}). \n\nRestating in words, the standard minimum value of a function may be defined as the value of the function at *its own argmin*. Instead, the *generalized* minimum of a function f(.) with respect to a function g(.), is the value of f(.) evaluated at *the argmin of g(.)*. This is how \u201cgenmin\u201d generalizes \u201cmin.\u201d\n\nPlease also take a look at the updated pdf portions and let us know if you have any further comments. Thank you."}, "signatures": ["ICLR.cc/2019/Conference/Paper1165/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1165/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1165/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Time-Agnostic Prediction: Predicting Predictable Video Frames", "abstract": "Prediction is arguably one of the most basic functions of an intelligent system. In general, the problem of predicting events in the future or between two waypoints is exceedingly difficult. However, most phenomena naturally pass through relatively predictable bottlenecks---while we cannot predict the precise trajectory of a robot arm between being at rest and holding an object up, we can be certain that it must have picked the object up. To exploit this, we decouple visual prediction from a rigid notion of time. While conventional approaches predict frames at regularly spaced temporal intervals, our time-agnostic predictors (TAP) are not tied to specific times so that they may instead discover predictable \"bottleneck\" frames no matter when they occur. We evaluate our approach for future and intermediate frame prediction across three robotic manipulation tasks. Our predictions are not only of higher visual quality, but also correspond to coherent semantic subgoals in temporally extended tasks.", "keywords": ["visual prediction", "subgoal generation", "bottleneck states", "time-agnostic"], "authorids": ["dineshjayaraman@berkeley.edu", "febert@berkeley.edu", "efros@eecs.berkeley.edu", "svlevine@eecs.berkeley.edu"], "authors": ["Dinesh Jayaraman", "Frederik Ebert", "Alexei Efros", "Sergey Levine"], "TL;DR": "In visual prediction tasks, letting your predictive model choose which times to predict does two things: (i) improves prediction quality, and (ii) leads to semantically coherent \"bottleneck state\" predictions, which are useful for planning.", "pdf": "/pdf/4a60fe530d6e791c663c8729169d0e471aa8e78f.pdf", "paperhash": "jayaraman|timeagnostic_prediction_predicting_predictable_video_frames", "_bibtex": "@inproceedings{\njayaraman2018timeagnostic,\ntitle={Time-Agnostic Prediction: Predicting Predictable Video Frames},\nauthor={Dinesh Jayaraman and Frederik Ebert and Alexei Efros and Sergey Levine},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SyzVb3CcFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1165/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619161, "tddate": null, "super": null, "final": null, "reply": {"forum": "SyzVb3CcFX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1165/Authors", "ICLR.cc/2019/Conference/Paper1165/Reviewers", "ICLR.cc/2019/Conference/Paper1165/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1165/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1165/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1165/Authors|ICLR.cc/2019/Conference/Paper1165/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1165/Reviewers", "ICLR.cc/2019/Conference/Paper1165/Authors", "ICLR.cc/2019/Conference/Paper1165/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619161}}}, {"id": "Hkl7KkHj6X", "original": null, "number": 2, "cdate": 1542307707296, "ddate": null, "tcdate": 1542307707296, "tmdate": 1542307707296, "tddate": null, "forum": "SyzVb3CcFX", "replyto": "SyzVb3CcFX", "invitation": "ICLR.cc/2019/Conference/-/Paper1165/Public_Comment", "content": {"comment": "Very exciting work! It would be great if you could include a brief comparison to the method proposed in the paper \"Adaptive Skip Intervals: Temporal Abstraction for Recurrent Dynamical Models\" (Neitz et. al, NIPS 2018; https://arxiv.org/abs/1808.04768 ).\n", "title": "Related work"}, "signatures": ["~Alexander_Neitz1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Alexander_Neitz1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Time-Agnostic Prediction: Predicting Predictable Video Frames", "abstract": "Prediction is arguably one of the most basic functions of an intelligent system. In general, the problem of predicting events in the future or between two waypoints is exceedingly difficult. However, most phenomena naturally pass through relatively predictable bottlenecks---while we cannot predict the precise trajectory of a robot arm between being at rest and holding an object up, we can be certain that it must have picked the object up. To exploit this, we decouple visual prediction from a rigid notion of time. While conventional approaches predict frames at regularly spaced temporal intervals, our time-agnostic predictors (TAP) are not tied to specific times so that they may instead discover predictable \"bottleneck\" frames no matter when they occur. We evaluate our approach for future and intermediate frame prediction across three robotic manipulation tasks. Our predictions are not only of higher visual quality, but also correspond to coherent semantic subgoals in temporally extended tasks.", "keywords": ["visual prediction", "subgoal generation", "bottleneck states", "time-agnostic"], "authorids": ["dineshjayaraman@berkeley.edu", "febert@berkeley.edu", "efros@eecs.berkeley.edu", "svlevine@eecs.berkeley.edu"], "authors": ["Dinesh Jayaraman", "Frederik Ebert", "Alexei Efros", "Sergey Levine"], "TL;DR": "In visual prediction tasks, letting your predictive model choose which times to predict does two things: (i) improves prediction quality, and (ii) leads to semantically coherent \"bottleneck state\" predictions, which are useful for planning.", "pdf": "/pdf/4a60fe530d6e791c663c8729169d0e471aa8e78f.pdf", "paperhash": "jayaraman|timeagnostic_prediction_predicting_predictable_video_frames", "_bibtex": "@inproceedings{\njayaraman2018timeagnostic,\ntitle={Time-Agnostic Prediction: Predicting Predictable Video Frames},\nauthor={Dinesh Jayaraman and Frederik Ebert and Alexei Efros and Sergey Levine},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SyzVb3CcFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1165/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311663482, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "SyzVb3CcFX", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1165/Authors", "ICLR.cc/2019/Conference/Paper1165/Reviewers", "ICLR.cc/2019/Conference/Paper1165/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1165/Authors", "ICLR.cc/2019/Conference/Paper1165/Reviewers", "ICLR.cc/2019/Conference/Paper1165/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311663482}}}, {"id": "Byxjv_mHam", "original": null, "number": 1, "cdate": 1541908579289, "ddate": null, "tcdate": 1541908579289, "tmdate": 1541909823837, "tddate": null, "forum": "SyzVb3CcFX", "replyto": "SyzVb3CcFX", "invitation": "ICLR.cc/2019/Conference/-/Paper1165/Public_Comment", "content": {"comment": "This approach seems simple as well as effective for me. However, I have two suggestions to improve the message of this paper.\n\nFirst of all, this paper uses few robot simulations and the BAIR pushing (robot arm) dataset, which are relatively easy to memorize/predict compared to the real-world videos. Thus, to reassure this concern, I recommend the author to add results on human action dataset, like Human 3.6M or KTH (if this model takes a long time to train)  for example. I believe such an experiment would strengthen this work.\n\nAlso, on the website ( https://sites.google.com/view/ta-pred ) shared in this paper, it only includes videos for the simulation task. So, I believe it would be helpful to understand the effectiveness of this work if the author shares the videos from the BAIR pushing (and the human action) dataset on the supplementary website also.", "title": "Two suggestions: (1) Add an experiment on real videos (e.g. human action) and (2) upload sample videos for the BAIR pushing dataset on the supplementary website."}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Time-Agnostic Prediction: Predicting Predictable Video Frames", "abstract": "Prediction is arguably one of the most basic functions of an intelligent system. In general, the problem of predicting events in the future or between two waypoints is exceedingly difficult. However, most phenomena naturally pass through relatively predictable bottlenecks---while we cannot predict the precise trajectory of a robot arm between being at rest and holding an object up, we can be certain that it must have picked the object up. To exploit this, we decouple visual prediction from a rigid notion of time. While conventional approaches predict frames at regularly spaced temporal intervals, our time-agnostic predictors (TAP) are not tied to specific times so that they may instead discover predictable \"bottleneck\" frames no matter when they occur. We evaluate our approach for future and intermediate frame prediction across three robotic manipulation tasks. Our predictions are not only of higher visual quality, but also correspond to coherent semantic subgoals in temporally extended tasks.", "keywords": ["visual prediction", "subgoal generation", "bottleneck states", "time-agnostic"], "authorids": ["dineshjayaraman@berkeley.edu", "febert@berkeley.edu", "efros@eecs.berkeley.edu", "svlevine@eecs.berkeley.edu"], "authors": ["Dinesh Jayaraman", "Frederik Ebert", "Alexei Efros", "Sergey Levine"], "TL;DR": "In visual prediction tasks, letting your predictive model choose which times to predict does two things: (i) improves prediction quality, and (ii) leads to semantically coherent \"bottleneck state\" predictions, which are useful for planning.", "pdf": "/pdf/4a60fe530d6e791c663c8729169d0e471aa8e78f.pdf", "paperhash": "jayaraman|timeagnostic_prediction_predicting_predictable_video_frames", "_bibtex": "@inproceedings{\njayaraman2018timeagnostic,\ntitle={Time-Agnostic Prediction: Predicting Predictable Video Frames},\nauthor={Dinesh Jayaraman and Frederik Ebert and Alexei Efros and Sergey Levine},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SyzVb3CcFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1165/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311663482, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "SyzVb3CcFX", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1165/Authors", "ICLR.cc/2019/Conference/Paper1165/Reviewers", "ICLR.cc/2019/Conference/Paper1165/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1165/Authors", "ICLR.cc/2019/Conference/Paper1165/Reviewers", "ICLR.cc/2019/Conference/Paper1165/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311663482}}}, {"id": "B1g2eZUqnm", "original": null, "number": 3, "cdate": 1541198068062, "ddate": null, "tcdate": 1541198068062, "tmdate": 1541533368066, "tddate": null, "forum": "SyzVb3CcFX", "replyto": "SyzVb3CcFX", "invitation": "ICLR.cc/2019/Conference/-/Paper1165/Official_Review", "content": {"title": "simple yet effective", "review": "The authors present a method on prediction of frames in a video, with the key contribution being that the target prediction is floating, resolved by a minimum on the error of prediction. The authors show the merits of the approach on a synthetic benchmark of object manipulation with a robotic arm. \n\nQuality: this paper appears to contain a lot of work, and in general is of high quality. \n\nClarity: some sections of the paper were harder to digest, but overall the quality of the writing is good and the authors have made efforts to present examples and diagrams where appropriate. Fig 1, especially helps one get a quick understanding of the concept of a `bottleneck` state. \n\nOriginality: To the extent of my knowledge, this work is novel. It proposes a new loss function, which is an interesting direction to explore.\n\nSignificance: I would say this work is significant. There appears to be a significant improvement in the visual quality of predictions. In most cases, the L1 error metric does not show such a huge improvement, but the visual difference is remarkable, so this goes to show that the L1 metric is perhaps not good enough at this point. \n\nOverall, I think this work is significant and I would recommend its acceptance for publication at ICLR. There are some drawbacks, but I don\u2019t think they are major or would justify rejection (see comments below). \n\n\nI\u2019m curious as to why you called the method in section 3.2 the \u201cGeneralized minimum\u201d? It feels more like a weighted (or preference weighted) minimum to me and confused me a few times as I was reading the paper (GENerative? GENeralized? what\u2019s general about it?). Just a comment.\n\nWhat results does figure 4 present? Are they only for the grasping sequence? Please specify.  \n\nIn connection with the previous comment, I think the results would be more readable if the match-steps were normalized for each sequence (at least for Figure 4). There would be a clearer mapping between fixT methods and the normalized matching step (e.g., we would expect fix0.75 to achieve a matching step of 0.75 instead of 6 / ? ).\n\nSection 4, Intermediate prediction. The statement \u201cthe genmin w(t) preference is bell-shaped\u201d is vague. Do you mean a Gaussian? If so, you should say \u201ca Gaussian centered at T/2 and tuned so that \u2026\u201d\n\nSection 4, Bottleneck discovery frequency. I am not entirely convinced by the measuring of bottleneck states. You say that a distance is computed between the predicted object position and the ground-truth object position. If a model were to output exactly the same frame as given in context, would the distance be zero? If so, doesn\u2019t that mean that a model who predicts a non-bottleneck state before or after the robotic arm moves the pieces is estimated to have a very good bottleneck prediction frequency? I found this part of the paper the hardest to follow and the least convincing. Perhaps some intermediate results could help prospective readers understand better and be convinced of the protocol\u2019s merits.\n\n\n\nTypos:\n\nAppendix E, 2nd paragraph, first sentence: \u201c... generate an bidirectional state\u201d --> \u201cgenerate A bidirectional state\u201d \n", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1165/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Time-Agnostic Prediction: Predicting Predictable Video Frames", "abstract": "Prediction is arguably one of the most basic functions of an intelligent system. In general, the problem of predicting events in the future or between two waypoints is exceedingly difficult. However, most phenomena naturally pass through relatively predictable bottlenecks---while we cannot predict the precise trajectory of a robot arm between being at rest and holding an object up, we can be certain that it must have picked the object up. To exploit this, we decouple visual prediction from a rigid notion of time. While conventional approaches predict frames at regularly spaced temporal intervals, our time-agnostic predictors (TAP) are not tied to specific times so that they may instead discover predictable \"bottleneck\" frames no matter when they occur. We evaluate our approach for future and intermediate frame prediction across three robotic manipulation tasks. Our predictions are not only of higher visual quality, but also correspond to coherent semantic subgoals in temporally extended tasks.", "keywords": ["visual prediction", "subgoal generation", "bottleneck states", "time-agnostic"], "authorids": ["dineshjayaraman@berkeley.edu", "febert@berkeley.edu", "efros@eecs.berkeley.edu", "svlevine@eecs.berkeley.edu"], "authors": ["Dinesh Jayaraman", "Frederik Ebert", "Alexei Efros", "Sergey Levine"], "TL;DR": "In visual prediction tasks, letting your predictive model choose which times to predict does two things: (i) improves prediction quality, and (ii) leads to semantically coherent \"bottleneck state\" predictions, which are useful for planning.", "pdf": "/pdf/4a60fe530d6e791c663c8729169d0e471aa8e78f.pdf", "paperhash": "jayaraman|timeagnostic_prediction_predicting_predictable_video_frames", "_bibtex": "@inproceedings{\njayaraman2018timeagnostic,\ntitle={Time-Agnostic Prediction: Predicting Predictable Video Frames},\nauthor={Dinesh Jayaraman and Frederik Ebert and Alexei Efros and Sergey Levine},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SyzVb3CcFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1165/Official_Review", "cdate": 1542234290768, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SyzVb3CcFX", "replyto": "SyzVb3CcFX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1165/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335886879, "tmdate": 1552335886879, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1165/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "BkevZnkq2Q", "original": null, "number": 1, "cdate": 1541172223343, "ddate": null, "tcdate": 1541172223343, "tmdate": 1541533367613, "tddate": null, "forum": "SyzVb3CcFX", "replyto": "SyzVb3CcFX", "invitation": "ICLR.cc/2019/Conference/-/Paper1165/Official_Review", "content": {"title": "Good novel contribution", "review": "Summary:\nThe paper reformulates the task of video prediction/interpolation so that a predictor is not forced to generate frames at fixed time intervals, but instead it is trained to generate frames that happen at any point in the future. The motivation for such approach is that there might be future states that are highly uncertain \u2013 and thus, difficult to predict \u2013 that might not be useful for other tasks involving video prediction such as planning. The authors derive different loss functions for such Time-Agnostic Prediction (TAP), including extensions to the Variational AutoEncoders (VAE) and Generative Adversarial Networks (GAN) frameworks, and conduct experiments that suggest that the frames predicted by TAP models correspond to \u2018subgoal\u2019 states useful for planning.\n\nStrenghts:\n[+] The idea of TAP is novel and intuitively makes sense. \nIt is clear that there are frames in video prediction that might not be interesting/useful yet are difficult to predict, TAP allows to skip such frames.\n[+] The formulation of the TAP losses is clear and well justified. \nThe authors do a good job at showing a first version of a TAP loss, generalizing it to express preferences, and its extension to VAE and GAN models, showing that \n\nWeaknesses:\n[-] The claim that the model discovers meaningful planning subgoals might be overstated. \nThe hierarchical planning evaluation experiment seems like it would clearly favor TAP compared to a fixed model (why would the middle prediction in time of the fixed model correspond to reasonable planning goals?). Furthermore, for certain tasks and environments it seems like the uncertain frames might be the ones that correspond to important subgoals. For example, for the BAIR Push Dataset, usually the harder frames to predict are the arm-object interactions, which probably would correspond to the relevant subgoals.\n\nOverall I believe that the idea in this paper is a meaningful novel contribution. The paper is well-written and the experiments support the fact that TAP might be a better choice for training frame predictors for certain tasks.", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper1165/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Time-Agnostic Prediction: Predicting Predictable Video Frames", "abstract": "Prediction is arguably one of the most basic functions of an intelligent system. In general, the problem of predicting events in the future or between two waypoints is exceedingly difficult. However, most phenomena naturally pass through relatively predictable bottlenecks---while we cannot predict the precise trajectory of a robot arm between being at rest and holding an object up, we can be certain that it must have picked the object up. To exploit this, we decouple visual prediction from a rigid notion of time. While conventional approaches predict frames at regularly spaced temporal intervals, our time-agnostic predictors (TAP) are not tied to specific times so that they may instead discover predictable \"bottleneck\" frames no matter when they occur. We evaluate our approach for future and intermediate frame prediction across three robotic manipulation tasks. Our predictions are not only of higher visual quality, but also correspond to coherent semantic subgoals in temporally extended tasks.", "keywords": ["visual prediction", "subgoal generation", "bottleneck states", "time-agnostic"], "authorids": ["dineshjayaraman@berkeley.edu", "febert@berkeley.edu", "efros@eecs.berkeley.edu", "svlevine@eecs.berkeley.edu"], "authors": ["Dinesh Jayaraman", "Frederik Ebert", "Alexei Efros", "Sergey Levine"], "TL;DR": "In visual prediction tasks, letting your predictive model choose which times to predict does two things: (i) improves prediction quality, and (ii) leads to semantically coherent \"bottleneck state\" predictions, which are useful for planning.", "pdf": "/pdf/4a60fe530d6e791c663c8729169d0e471aa8e78f.pdf", "paperhash": "jayaraman|timeagnostic_prediction_predicting_predictable_video_frames", "_bibtex": "@inproceedings{\njayaraman2018timeagnostic,\ntitle={Time-Agnostic Prediction: Predicting Predictable Video Frames},\nauthor={Dinesh Jayaraman and Frederik Ebert and Alexei Efros and Sergey Levine},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SyzVb3CcFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1165/Official_Review", "cdate": 1542234290768, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SyzVb3CcFX", "replyto": "SyzVb3CcFX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1165/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335886879, "tmdate": 1552335886879, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1165/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 13}