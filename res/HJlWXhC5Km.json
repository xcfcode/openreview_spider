{"notes": [{"id": "HJlWXhC5Km", "original": "SyxTCZAct7", "number": 1330, "cdate": 1538087960710, "ddate": null, "tcdate": 1538087960710, "tmdate": 1545355420608, "tddate": null, "forum": "HJlWXhC5Km", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Learning to Control Visual Abstractions for Structured Exploration in Deep Reinforcement Learning", "abstract": "Exploration in environments with sparse rewards is a key challenge for reinforcement learning. How do we design agents with generic inductive biases so that they can explore in a consistent manner instead of just using local exploration schemes like epsilon-greedy? We propose an unsupervised reinforcement learning agent which learns a discrete pixel grouping model that preserves spatial geometry of the sensors and implicitly of the environment as well. We use this representation to derive geometric intrinsic reward functions, like centroid coordinates and area, and learn policies to control each one of them with off-policy learning. These policies form a basis set of behaviors (options) which allows us explore in a consistent way and use them in a hierarchical reinforcement learning setup to solve for extrinsically defined rewards. We show that our approach can scale to a variety of domains with competitive performance, including navigation in 3D environments and Atari games with sparse rewards.", "keywords": ["exploration", "deep reinforcement learning", "intrinsic motivation", "unsupervised learning"], "authorids": ["cdi@google.com", "tkulkarni@google.com", "avdnoord@google.com", "amnih@google.com", "vmnih@google.com"], "authors": ["catalin ionescu", "tejas kulkarni", "aaron van de oord", "andriy mnih", "vlad mnih"], "TL;DR": "structured exploration in deep reinforcement learning via unsupervised visual abstraction discovery and control", "pdf": "/pdf/748c4242f1ffa85472f907e610157d4664387154.pdf", "paperhash": "ionescu|learning_to_control_visual_abstractions_for_structured_exploration_in_deep_reinforcement_learning", "_bibtex": "@misc{\nionescu2019learning,\ntitle={Learning to Control Visual Abstractions for Structured Exploration in Deep Reinforcement Learning},\nauthor={catalin ionescu and tejas kulkarni and aaron van de oord and andriy mnih and vlad mnih},\nyear={2019},\nurl={https://openreview.net/forum?id=HJlWXhC5Km},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "HJxYaNqflV", "original": null, "number": 1, "cdate": 1544885441495, "ddate": null, "tcdate": 1544885441495, "tmdate": 1545354494632, "tddate": null, "forum": "HJlWXhC5Km", "replyto": "HJlWXhC5Km", "invitation": "ICLR.cc/2019/Conference/-/Paper1330/Meta_Review", "content": {"metareview": "The paper presents an unsupervised visual abstraction model, used for reinforcement learning tasks. It is trained through intrinsic rewards, generated from temporal differences of inputs. This is similar to \"learning to control pixels\". The method is tested in DM Lab (3D environment, 2D navigation tasks) and Atari (Montezuma's Revenge).\n\nThe paper is at times hard to follow, and it seems the improvements accompanying the rebuttals did not convince reviewers to change their notes significantly. The experiments do not contain enough comparisons to other models, baselines, nor ablations, to sustain the claims.\n\nIn its current form, this is not acceptable for publication at ICLR.", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Reject", "title": "Interesting structured exploration idea, not clear nor detailed enough"}, "signatures": ["ICLR.cc/2019/Conference/Paper1330/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper1330/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Control Visual Abstractions for Structured Exploration in Deep Reinforcement Learning", "abstract": "Exploration in environments with sparse rewards is a key challenge for reinforcement learning. How do we design agents with generic inductive biases so that they can explore in a consistent manner instead of just using local exploration schemes like epsilon-greedy? We propose an unsupervised reinforcement learning agent which learns a discrete pixel grouping model that preserves spatial geometry of the sensors and implicitly of the environment as well. We use this representation to derive geometric intrinsic reward functions, like centroid coordinates and area, and learn policies to control each one of them with off-policy learning. These policies form a basis set of behaviors (options) which allows us explore in a consistent way and use them in a hierarchical reinforcement learning setup to solve for extrinsically defined rewards. We show that our approach can scale to a variety of domains with competitive performance, including navigation in 3D environments and Atari games with sparse rewards.", "keywords": ["exploration", "deep reinforcement learning", "intrinsic motivation", "unsupervised learning"], "authorids": ["cdi@google.com", "tkulkarni@google.com", "avdnoord@google.com", "amnih@google.com", "vmnih@google.com"], "authors": ["catalin ionescu", "tejas kulkarni", "aaron van de oord", "andriy mnih", "vlad mnih"], "TL;DR": "structured exploration in deep reinforcement learning via unsupervised visual abstraction discovery and control", "pdf": "/pdf/748c4242f1ffa85472f907e610157d4664387154.pdf", "paperhash": "ionescu|learning_to_control_visual_abstractions_for_structured_exploration_in_deep_reinforcement_learning", "_bibtex": "@misc{\nionescu2019learning,\ntitle={Learning to Control Visual Abstractions for Structured Exploration in Deep Reinforcement Learning},\nauthor={catalin ionescu and tejas kulkarni and aaron van de oord and andriy mnih and vlad mnih},\nyear={2019},\nurl={https://openreview.net/forum?id=HJlWXhC5Km},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1330/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545352877796, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJlWXhC5Km", "replyto": "HJlWXhC5Km", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1330/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper1330/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1330/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545352877796}}}, {"id": "r1g7AgzD0X", "original": null, "number": 6, "cdate": 1543082186761, "ddate": null, "tcdate": 1543082186761, "tmdate": 1543082186761, "tddate": null, "forum": "HJlWXhC5Km", "replyto": "ByePRcoH07", "invitation": "ICLR.cc/2019/Conference/-/Paper1330/Official_Comment", "content": {"title": "termination condition", "comment": "Thanks for this question. The options in our setting maximize or minimize entity attributes, so there isn't a natural goal success criteria (e.g. sometimes there could be obstacles in an entity's path or none at all). In some cases it might be possible to make statements about goal achievement, for instance if the agent can learn to reason about immovable obstacles. This could be an interesting direction to explore in the future. Also in other papers which have considered adaptive T in the setting of relational goals (e.g. Kulkarni et al.), an internal goal critic could clearly measure goal success contrary to our goal space. In practice, a single T (=20) works well across all our experiments and domains."}, "signatures": ["ICLR.cc/2019/Conference/Paper1330/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1330/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1330/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Control Visual Abstractions for Structured Exploration in Deep Reinforcement Learning", "abstract": "Exploration in environments with sparse rewards is a key challenge for reinforcement learning. How do we design agents with generic inductive biases so that they can explore in a consistent manner instead of just using local exploration schemes like epsilon-greedy? We propose an unsupervised reinforcement learning agent which learns a discrete pixel grouping model that preserves spatial geometry of the sensors and implicitly of the environment as well. We use this representation to derive geometric intrinsic reward functions, like centroid coordinates and area, and learn policies to control each one of them with off-policy learning. These policies form a basis set of behaviors (options) which allows us explore in a consistent way and use them in a hierarchical reinforcement learning setup to solve for extrinsically defined rewards. We show that our approach can scale to a variety of domains with competitive performance, including navigation in 3D environments and Atari games with sparse rewards.", "keywords": ["exploration", "deep reinforcement learning", "intrinsic motivation", "unsupervised learning"], "authorids": ["cdi@google.com", "tkulkarni@google.com", "avdnoord@google.com", "amnih@google.com", "vmnih@google.com"], "authors": ["catalin ionescu", "tejas kulkarni", "aaron van de oord", "andriy mnih", "vlad mnih"], "TL;DR": "structured exploration in deep reinforcement learning via unsupervised visual abstraction discovery and control", "pdf": "/pdf/748c4242f1ffa85472f907e610157d4664387154.pdf", "paperhash": "ionescu|learning_to_control_visual_abstractions_for_structured_exploration_in_deep_reinforcement_learning", "_bibtex": "@misc{\nionescu2019learning,\ntitle={Learning to Control Visual Abstractions for Structured Exploration in Deep Reinforcement Learning},\nauthor={catalin ionescu and tejas kulkarni and aaron van de oord and andriy mnih and vlad mnih},\nyear={2019},\nurl={https://openreview.net/forum?id=HJlWXhC5Km},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1330/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621615403, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJlWXhC5Km", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1330/Authors", "ICLR.cc/2019/Conference/Paper1330/Reviewers", "ICLR.cc/2019/Conference/Paper1330/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1330/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1330/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1330/Authors|ICLR.cc/2019/Conference/Paper1330/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1330/Reviewers", "ICLR.cc/2019/Conference/Paper1330/Authors", "ICLR.cc/2019/Conference/Paper1330/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621615403}}}, {"id": "rkg-OWpO2Q", "original": null, "number": 1, "cdate": 1541095784952, "ddate": null, "tcdate": 1541095784952, "tmdate": 1543028655847, "tddate": null, "forum": "HJlWXhC5Km", "replyto": "HJlWXhC5Km", "invitation": "ICLR.cc/2019/Conference/-/Paper1330/Official_Review", "content": {"title": "Insufficient clarity", "review": "REVISION: thanks for the clarification. I have slightly increased my rating (to 4).\n\nThis paper tackles a very interesting subject but lacks sufficient clarity of presentation to allow me to do a proper review.\n\nFirst, there are many sentences which are not well-formed or are ambiguous (in pretty much all the sections). Then there are terms which are introduced without being first clearly explained or defined. Finally, there are issues with the mathematical clarity as well, with many notations which are used without being explained or defined. Sometimes one can figure out the missing information later (e.g., fig 1 talks about mutual information objectives without stating if we want to maximize or minimize it, but later in the text we figure that out) but it makes reading very difficult.\n\nWhat is a 'transformed one' (on page 2)\nWhat is a 'geometric intrinsic reward'?\nWhere are the intrinsic rewards defined?\nWhat is a 'non-parametric classifier'? A neural net? an kernel SVM?\n\nThere are also some mathematical problems:\n- if f (page 3) has a discrete output, then it will probably lose information, so it cannot be inverted (contrary to the stated assumption that f(x)!=f(y) for x!=y)\n- what are the differences between the different Q functions being defined? do the correspond to different action spaces? What is Q_task? What is pi_meta?\n- in eqn 2, I do not think that the log q_c term maximizes the mutual information between actions and (G(t),G(t+1)), i.e. it would be missing an entropy term\n- what is Z_c in eqn 2?\n\n", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1330/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Learning to Control Visual Abstractions for Structured Exploration in Deep Reinforcement Learning", "abstract": "Exploration in environments with sparse rewards is a key challenge for reinforcement learning. How do we design agents with generic inductive biases so that they can explore in a consistent manner instead of just using local exploration schemes like epsilon-greedy? We propose an unsupervised reinforcement learning agent which learns a discrete pixel grouping model that preserves spatial geometry of the sensors and implicitly of the environment as well. We use this representation to derive geometric intrinsic reward functions, like centroid coordinates and area, and learn policies to control each one of them with off-policy learning. These policies form a basis set of behaviors (options) which allows us explore in a consistent way and use them in a hierarchical reinforcement learning setup to solve for extrinsically defined rewards. We show that our approach can scale to a variety of domains with competitive performance, including navigation in 3D environments and Atari games with sparse rewards.", "keywords": ["exploration", "deep reinforcement learning", "intrinsic motivation", "unsupervised learning"], "authorids": ["cdi@google.com", "tkulkarni@google.com", "avdnoord@google.com", "amnih@google.com", "vmnih@google.com"], "authors": ["catalin ionescu", "tejas kulkarni", "aaron van de oord", "andriy mnih", "vlad mnih"], "TL;DR": "structured exploration in deep reinforcement learning via unsupervised visual abstraction discovery and control", "pdf": "/pdf/748c4242f1ffa85472f907e610157d4664387154.pdf", "paperhash": "ionescu|learning_to_control_visual_abstractions_for_structured_exploration_in_deep_reinforcement_learning", "_bibtex": "@misc{\nionescu2019learning,\ntitle={Learning to Control Visual Abstractions for Structured Exploration in Deep Reinforcement Learning},\nauthor={catalin ionescu and tejas kulkarni and aaron van de oord and andriy mnih and vlad mnih},\nyear={2019},\nurl={https://openreview.net/forum?id=HJlWXhC5Km},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1330/Official_Review", "cdate": 1542234253437, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HJlWXhC5Km", "replyto": "HJlWXhC5Km", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1330/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335923417, "tmdate": 1552335923417, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1330/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "ByePRcoH07", "original": null, "number": 5, "cdate": 1542990543069, "ddate": null, "tcdate": 1542990543069, "tmdate": 1542990543069, "tddate": null, "forum": "HJlWXhC5Km", "replyto": "r1x_6y94RX", "invitation": "ICLR.cc/2019/Conference/-/Paper1330/Official_Comment", "content": {"title": "Option termination condition", "comment": "Thanks for the rebuttal. Can you explain why the option termination condition is that Q_{meta} picks an action for every fixed number of steps? Why the termination condition is independent with the task, the environment and the state? Why this is a good choice?"}, "signatures": ["ICLR.cc/2019/Conference/Paper1330/AnonReviewer1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1330/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1330/AnonReviewer1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Control Visual Abstractions for Structured Exploration in Deep Reinforcement Learning", "abstract": "Exploration in environments with sparse rewards is a key challenge for reinforcement learning. How do we design agents with generic inductive biases so that they can explore in a consistent manner instead of just using local exploration schemes like epsilon-greedy? We propose an unsupervised reinforcement learning agent which learns a discrete pixel grouping model that preserves spatial geometry of the sensors and implicitly of the environment as well. We use this representation to derive geometric intrinsic reward functions, like centroid coordinates and area, and learn policies to control each one of them with off-policy learning. These policies form a basis set of behaviors (options) which allows us explore in a consistent way and use them in a hierarchical reinforcement learning setup to solve for extrinsically defined rewards. We show that our approach can scale to a variety of domains with competitive performance, including navigation in 3D environments and Atari games with sparse rewards.", "keywords": ["exploration", "deep reinforcement learning", "intrinsic motivation", "unsupervised learning"], "authorids": ["cdi@google.com", "tkulkarni@google.com", "avdnoord@google.com", "amnih@google.com", "vmnih@google.com"], "authors": ["catalin ionescu", "tejas kulkarni", "aaron van de oord", "andriy mnih", "vlad mnih"], "TL;DR": "structured exploration in deep reinforcement learning via unsupervised visual abstraction discovery and control", "pdf": "/pdf/748c4242f1ffa85472f907e610157d4664387154.pdf", "paperhash": "ionescu|learning_to_control_visual_abstractions_for_structured_exploration_in_deep_reinforcement_learning", "_bibtex": "@misc{\nionescu2019learning,\ntitle={Learning to Control Visual Abstractions for Structured Exploration in Deep Reinforcement Learning},\nauthor={catalin ionescu and tejas kulkarni and aaron van de oord and andriy mnih and vlad mnih},\nyear={2019},\nurl={https://openreview.net/forum?id=HJlWXhC5Km},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1330/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621615403, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJlWXhC5Km", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1330/Authors", "ICLR.cc/2019/Conference/Paper1330/Reviewers", "ICLR.cc/2019/Conference/Paper1330/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1330/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1330/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1330/Authors|ICLR.cc/2019/Conference/Paper1330/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1330/Reviewers", "ICLR.cc/2019/Conference/Paper1330/Authors", "ICLR.cc/2019/Conference/Paper1330/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621615403}}}, {"id": "HJgpllc4RX", "original": null, "number": 4, "cdate": 1542918132851, "ddate": null, "tcdate": 1542918132851, "tmdate": 1542918132851, "tddate": null, "forum": "HJlWXhC5Km", "replyto": "rkg-OWpO2Q", "invitation": "ICLR.cc/2019/Conference/-/Paper1330/Official_Comment", "content": {"title": "Significantly improved the clarity of our writing in the revision", "comment": "Thanks a lot for the critical feedback. We have improved the clarity of our writing and made the contributions clearer. We urge you to read the revised paper and hopefully it will convince you of our contributions. \n\nQ: \u201cWhat is a 'transformed one' (on page 2)\u201d \nThe original image is transformed by applying the following operators: additive color changes in HSV space, horizontal flips and spatial shifts. \n\nQ: \u201cWhat is a 'geometric intrinsic reward?\u201d\nWe compute geometric measurements as position_x, position_y, area of each segment. Temporal changes in these measurements gives our agent a notion of how it moves relative to extracted entities. If an entity\u2019s area becomes larger/smaller it means it is getting closer/farther away from it. Because both getting closer and farther may be useful we train to maximize both the measurements and its negative. Because these are over segments trained in an unsupervised way it makes them intrinsic.\n\nQ: \u201cWhat is a 'non-parametric classifier'? A neural net? an kernel SVM?\u201d\nWe use a K-NN classifier with a distance metric induced by a neural network. We apologize for not explaining this clearly and have updated Sec 3.1 to reflect it more clearly. \n\nQ: \u201cif f (page 3) has a discrete output, then it will probably lose information, so it cannot be inverted (contrary to the stated assumption that f(x)!=f(y) for x!=y)\u201d\nSorry for the confusing explanation. We have updated Sec 3.1. The VQ layer will only keep information that is important for distinguishing states as per space-time consistency and controllability constraints, while discarding other information. \n\nQ: \u201cwhat are the differences between the different Q functions being defined? do the correspond to different action spaces? What is Q_task? What is pi_meta?\u201d\nQ_meta and Q_task optimize the task reward while Q^{e,m} optimized the m-th reward of the e-th entity. Q_task and all Q^{e,m} are defined over the environment actions but Q_meta\u2019s action space is ExM+1 as described at the end of section 3. Pi meta is the policy derived from the corresponding Q, in our case this is epsilon-greedy.\n\nQ: \u201cwhat is Z_c in eqn 2?\u201d\nThis is a new random variable that we have to introduce in order to have a well defined objective. This has been explained more clearly in the updated text.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1330/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1330/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1330/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Control Visual Abstractions for Structured Exploration in Deep Reinforcement Learning", "abstract": "Exploration in environments with sparse rewards is a key challenge for reinforcement learning. How do we design agents with generic inductive biases so that they can explore in a consistent manner instead of just using local exploration schemes like epsilon-greedy? We propose an unsupervised reinforcement learning agent which learns a discrete pixel grouping model that preserves spatial geometry of the sensors and implicitly of the environment as well. We use this representation to derive geometric intrinsic reward functions, like centroid coordinates and area, and learn policies to control each one of them with off-policy learning. These policies form a basis set of behaviors (options) which allows us explore in a consistent way and use them in a hierarchical reinforcement learning setup to solve for extrinsically defined rewards. We show that our approach can scale to a variety of domains with competitive performance, including navigation in 3D environments and Atari games with sparse rewards.", "keywords": ["exploration", "deep reinforcement learning", "intrinsic motivation", "unsupervised learning"], "authorids": ["cdi@google.com", "tkulkarni@google.com", "avdnoord@google.com", "amnih@google.com", "vmnih@google.com"], "authors": ["catalin ionescu", "tejas kulkarni", "aaron van de oord", "andriy mnih", "vlad mnih"], "TL;DR": "structured exploration in deep reinforcement learning via unsupervised visual abstraction discovery and control", "pdf": "/pdf/748c4242f1ffa85472f907e610157d4664387154.pdf", "paperhash": "ionescu|learning_to_control_visual_abstractions_for_structured_exploration_in_deep_reinforcement_learning", "_bibtex": "@misc{\nionescu2019learning,\ntitle={Learning to Control Visual Abstractions for Structured Exploration in Deep Reinforcement Learning},\nauthor={catalin ionescu and tejas kulkarni and aaron van de oord and andriy mnih and vlad mnih},\nyear={2019},\nurl={https://openreview.net/forum?id=HJlWXhC5Km},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1330/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621615403, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJlWXhC5Km", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1330/Authors", "ICLR.cc/2019/Conference/Paper1330/Reviewers", "ICLR.cc/2019/Conference/Paper1330/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1330/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1330/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1330/Authors|ICLR.cc/2019/Conference/Paper1330/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1330/Reviewers", "ICLR.cc/2019/Conference/Paper1330/Authors", "ICLR.cc/2019/Conference/Paper1330/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621615403}}}, {"id": "r1x_6y94RX", "original": null, "number": 3, "cdate": 1542918079618, "ddate": null, "tcdate": 1542918079618, "tmdate": 1542918079618, "tddate": null, "forum": "HJlWXhC5Km", "replyto": "S1eCQpbq3X", "invitation": "ICLR.cc/2019/Conference/-/Paper1330/Official_Comment", "content": {"title": "Addressed concerns about the experimental setup/experiments and clarity of the ideas/contributions", "comment": "Thanks a lot for all the feedback and suggestions. This has helped us improve the clarity of our writing. \n\nQ: \u201cHowever, I am concerned if the proposed method solved the problem with the need of hand-crafted instance segmentation since, as shown in the Algorithm 1 and the caption of Figure 2, Q_{meta} acts every T steps\u201d\n\nWe do not require hand-crafted instance segmentation, such as in Kulkarni et al. We use an information theoretic loss to learn visual abstractions, which are further used to learn temporal abstractions or options. Q_meta picks an internal action, i.e. indexes into either the options bank or Q_task, and is optimized to maximize the environment task reward. This choice is fixed for T steps (option termination condition) and the chosen sub-controller executes real actions in the environment. \n\nQ: \u201cHowever, the authors did not show in the experiments if having too many intrinsic reward functions helps a lot. It will be better if the authors can show that, larger values for E or M can make the performances better.\u201d\nWe experimented with large E and found that the model discovers similar number of entities while often predicting empty segments. In terms of M, we have reported the simplest set of measurements that capture the 2D and 3D temporal structure of different environments. There could be many more functions which should be explored building upon this work.\n\nQ: \u201cAnother concern I have is on some of the experiment results \u2026 \u201c\nWe are using the Espeholt et al. training setup but with Q(lambda) to make it comparable with our agent. We have clarified this in the revised supplemental. Our method either outperforms or is in the same ballpark as the baseline. In tasks with sparse rewards, our method is especially beneficial as the options bank aids temporally extended exploration. Our main claim is that prior DRL agents have not been able to  object based structured exploration from pixels. Scaling this approach and making it more robust is an open question but we believe we have shown a promising avenue along these lines. \n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1330/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1330/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1330/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Control Visual Abstractions for Structured Exploration in Deep Reinforcement Learning", "abstract": "Exploration in environments with sparse rewards is a key challenge for reinforcement learning. How do we design agents with generic inductive biases so that they can explore in a consistent manner instead of just using local exploration schemes like epsilon-greedy? We propose an unsupervised reinforcement learning agent which learns a discrete pixel grouping model that preserves spatial geometry of the sensors and implicitly of the environment as well. We use this representation to derive geometric intrinsic reward functions, like centroid coordinates and area, and learn policies to control each one of them with off-policy learning. These policies form a basis set of behaviors (options) which allows us explore in a consistent way and use them in a hierarchical reinforcement learning setup to solve for extrinsically defined rewards. We show that our approach can scale to a variety of domains with competitive performance, including navigation in 3D environments and Atari games with sparse rewards.", "keywords": ["exploration", "deep reinforcement learning", "intrinsic motivation", "unsupervised learning"], "authorids": ["cdi@google.com", "tkulkarni@google.com", "avdnoord@google.com", "amnih@google.com", "vmnih@google.com"], "authors": ["catalin ionescu", "tejas kulkarni", "aaron van de oord", "andriy mnih", "vlad mnih"], "TL;DR": "structured exploration in deep reinforcement learning via unsupervised visual abstraction discovery and control", "pdf": "/pdf/748c4242f1ffa85472f907e610157d4664387154.pdf", "paperhash": "ionescu|learning_to_control_visual_abstractions_for_structured_exploration_in_deep_reinforcement_learning", "_bibtex": "@misc{\nionescu2019learning,\ntitle={Learning to Control Visual Abstractions for Structured Exploration in Deep Reinforcement Learning},\nauthor={catalin ionescu and tejas kulkarni and aaron van de oord and andriy mnih and vlad mnih},\nyear={2019},\nurl={https://openreview.net/forum?id=HJlWXhC5Km},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1330/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621615403, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJlWXhC5Km", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1330/Authors", "ICLR.cc/2019/Conference/Paper1330/Reviewers", "ICLR.cc/2019/Conference/Paper1330/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1330/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1330/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1330/Authors|ICLR.cc/2019/Conference/Paper1330/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1330/Reviewers", "ICLR.cc/2019/Conference/Paper1330/Authors", "ICLR.cc/2019/Conference/Paper1330/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621615403}}}, {"id": "Bkl0LJ9N0X", "original": null, "number": 2, "cdate": 1542917974327, "ddate": null, "tcdate": 1542917974327, "tmdate": 1542917974327, "tddate": null, "forum": "HJlWXhC5Km", "replyto": "rkg7LLJ6h7", "invitation": "ICLR.cc/2019/Conference/-/Paper1330/Official_Comment", "content": {"title": "Improved clarity of the writing, clearly wrote down our contributions and made revisions to the paper", "comment": "We want to thank the reviewer for valuable feedback in improving the clarity of the paper. \nQ: \u201cIt is unclear how the intrinsic reward is defined (which is critical to understand the approach).\u201d\nWe apologize for the confusion. We mentioned this in fig 2 and sec 3 but need to make it clear. The intrinsic rewards are geometric properties of the learnt segmentations (min/max of area, centroid x, centroid y for each learnt segment). The segments are obtained directly from the spatial VQ layer. We have updated the write up to make this clear in TODO. \n\nQ: \u201cIt is unclear what the M different measurements are or for what they are used for.\u201d\nControlling the geometric features of learnt segments is a principled way to learn skills to control different object attributes (relative distance to observer, relative position of objects) in 2D/3D scenes. The M different measurements are the affine variables (e.g. position_x, position_y, area) of each segment. Controlling such geometric features can enable higher levels of behaviors such as reaching towards an object (max area), avoiding certain objects (min area), moving an object away towards the left (min position x), controlling the avatar\u2019s position on the screen etc. We have reflected this in Sec 3 and the introduction. \n\nQ: \u201cIt is unclear why equation 1 defines a classification loss. Distribution q is not defined in Eq (1)\u201d.\nWe apologize for not clearly explaining this. This is a classification loss due to reasons and derivations explained in prior work, namely -- MINE and CPC. We now state this in Sec 3.2. We have also defined q in eq (3) in the revised draft. \n\nQ: \u201c Please clarify what Qmeta and Qtask do in the text right in the beginning.\u201c\nQ_meta picks an internal action, i.e. indexes into either the options bank or Q_task (gets extrinsic reward and operates over low level actions), and is optimized to maximize the environment task reward. This choice is fixed for T steps and the chosen sub-controller executes real actions in the environment. We added a clear explanation in the introduction as well as Sec 3. \n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1330/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1330/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1330/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Control Visual Abstractions for Structured Exploration in Deep Reinforcement Learning", "abstract": "Exploration in environments with sparse rewards is a key challenge for reinforcement learning. How do we design agents with generic inductive biases so that they can explore in a consistent manner instead of just using local exploration schemes like epsilon-greedy? We propose an unsupervised reinforcement learning agent which learns a discrete pixel grouping model that preserves spatial geometry of the sensors and implicitly of the environment as well. We use this representation to derive geometric intrinsic reward functions, like centroid coordinates and area, and learn policies to control each one of them with off-policy learning. These policies form a basis set of behaviors (options) which allows us explore in a consistent way and use them in a hierarchical reinforcement learning setup to solve for extrinsically defined rewards. We show that our approach can scale to a variety of domains with competitive performance, including navigation in 3D environments and Atari games with sparse rewards.", "keywords": ["exploration", "deep reinforcement learning", "intrinsic motivation", "unsupervised learning"], "authorids": ["cdi@google.com", "tkulkarni@google.com", "avdnoord@google.com", "amnih@google.com", "vmnih@google.com"], "authors": ["catalin ionescu", "tejas kulkarni", "aaron van de oord", "andriy mnih", "vlad mnih"], "TL;DR": "structured exploration in deep reinforcement learning via unsupervised visual abstraction discovery and control", "pdf": "/pdf/748c4242f1ffa85472f907e610157d4664387154.pdf", "paperhash": "ionescu|learning_to_control_visual_abstractions_for_structured_exploration_in_deep_reinforcement_learning", "_bibtex": "@misc{\nionescu2019learning,\ntitle={Learning to Control Visual Abstractions for Structured Exploration in Deep Reinforcement Learning},\nauthor={catalin ionescu and tejas kulkarni and aaron van de oord and andriy mnih and vlad mnih},\nyear={2019},\nurl={https://openreview.net/forum?id=HJlWXhC5Km},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1330/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621615403, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJlWXhC5Km", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1330/Authors", "ICLR.cc/2019/Conference/Paper1330/Reviewers", "ICLR.cc/2019/Conference/Paper1330/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1330/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1330/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1330/Authors|ICLR.cc/2019/Conference/Paper1330/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1330/Reviewers", "ICLR.cc/2019/Conference/Paper1330/Authors", "ICLR.cc/2019/Conference/Paper1330/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621615403}}}, {"id": "rklGJycVCm", "original": null, "number": 1, "cdate": 1542917849529, "ddate": null, "tcdate": 1542917849529, "tmdate": 1542917849529, "tddate": null, "forum": "HJlWXhC5Km", "replyto": "HJlWXhC5Km", "invitation": "ICLR.cc/2019/Conference/-/Paper1330/Official_Comment", "content": {"title": "Rebuttal summary", "comment": "We want to thank all reviewers for their critical feedback and suggestions, which has already helped us improve the paper\u2019s clarity and presentation. All the reviewers agree that the paper tackles an interesting and important problem of discovering spatial and temporal abstractions given raw observations and actions. The main concerns were about the clarity of the writing, making it hard to clearly assess the underlying contributions. \n\nWe have significantly improved the presentation of our ideas considering all the feedback and explicitly made our contributions clearer. Our two key contributions are: (1) An information theoretic loss and architecture to learn spatio-temporal visual abstractions given raw pixels and actions, (2) a new agent architecture which learns temporal abstractions grounded in the geometry of the discovered visual abstractions. There have been several agent architectures in the past that make use of object-oriented information for constructing states and to aid exploration. However, this is the first agent architecture that simultaneously learns visual and temporal abstractions, while demonstrating clear improvements over baselines on hard 3D navigation and Atari games.\n\nWe urge all reviewers to read the updated version of the paper, as we have carefully addressed and incorporated all critical feedback and suggestions. \n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1330/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1330/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1330/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Control Visual Abstractions for Structured Exploration in Deep Reinforcement Learning", "abstract": "Exploration in environments with sparse rewards is a key challenge for reinforcement learning. How do we design agents with generic inductive biases so that they can explore in a consistent manner instead of just using local exploration schemes like epsilon-greedy? We propose an unsupervised reinforcement learning agent which learns a discrete pixel grouping model that preserves spatial geometry of the sensors and implicitly of the environment as well. We use this representation to derive geometric intrinsic reward functions, like centroid coordinates and area, and learn policies to control each one of them with off-policy learning. These policies form a basis set of behaviors (options) which allows us explore in a consistent way and use them in a hierarchical reinforcement learning setup to solve for extrinsically defined rewards. We show that our approach can scale to a variety of domains with competitive performance, including navigation in 3D environments and Atari games with sparse rewards.", "keywords": ["exploration", "deep reinforcement learning", "intrinsic motivation", "unsupervised learning"], "authorids": ["cdi@google.com", "tkulkarni@google.com", "avdnoord@google.com", "amnih@google.com", "vmnih@google.com"], "authors": ["catalin ionescu", "tejas kulkarni", "aaron van de oord", "andriy mnih", "vlad mnih"], "TL;DR": "structured exploration in deep reinforcement learning via unsupervised visual abstraction discovery and control", "pdf": "/pdf/748c4242f1ffa85472f907e610157d4664387154.pdf", "paperhash": "ionescu|learning_to_control_visual_abstractions_for_structured_exploration_in_deep_reinforcement_learning", "_bibtex": "@misc{\nionescu2019learning,\ntitle={Learning to Control Visual Abstractions for Structured Exploration in Deep Reinforcement Learning},\nauthor={catalin ionescu and tejas kulkarni and aaron van de oord and andriy mnih and vlad mnih},\nyear={2019},\nurl={https://openreview.net/forum?id=HJlWXhC5Km},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1330/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621615403, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJlWXhC5Km", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1330/Authors", "ICLR.cc/2019/Conference/Paper1330/Reviewers", "ICLR.cc/2019/Conference/Paper1330/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1330/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1330/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1330/Authors|ICLR.cc/2019/Conference/Paper1330/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1330/Reviewers", "ICLR.cc/2019/Conference/Paper1330/Authors", "ICLR.cc/2019/Conference/Paper1330/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621615403}}}, {"id": "rkg7LLJ6h7", "original": null, "number": 3, "cdate": 1541367370823, "ddate": null, "tcdate": 1541367370823, "tmdate": 1541533228518, "tddate": null, "forum": "HJlWXhC5Km", "replyto": "HJlWXhC5Km", "invitation": "ICLR.cc/2019/Conference/-/Paper1330/Official_Review", "content": {"title": "The paper is unfortunately written quite confusingly such that it is hard to evaluate the contribution of the potentially interesting ideas.", "review": "The appproach introduces visual abstractions that are used for reinforcement learning. The abstractions are learned using a lower bound on the mutual information and options are created to generate different measurements for each abstraction. The algorithm hence learns to \"control\" each abstraction as well as to select the options to achieve the overall task. The algorithm is tested on a 3D navigation task and a few Atari tasks which are known for difficult exploration.\n\nThe paper might contain some interesting ideas, however, I am quite confused about the paper due to lack of clarity in writing. The approach is not properly motivated, many equations are not really eplained and important information is missing, so it is really hard to evaluate the contribution of the approach. Please see below for more comments:\n- It is unclear how the intrinsic reward is defined (which is critical to understand the approach).\n- It is unclear what the M different measurements are or for what they are used for. \n- It is unclear qhy equation 1 defines a classification loss. Distribution q is not defined in Eq (1).\n- I do not understand the description of Q-meta in caption of Figure 2, \"Qmeta acts every T steps, which is the fixed temporal\ncommitment window, and outputs an action to select and execute either: (1) composition over Q\nfunction from the option bank indexed by a particular entity and an intrinsic reward function or (2)\nthe Qtask policy which outputs raw actions.\" How can an action be a composition over Q-function and a intrinisic reward function? Please clarify what Qmeta and Qtask do in the text right in the beginning. \n\nI have to say that the paper confused me too much that it is likely I missed the point of the paper. On the positive side, I think the learning of the abstractions using lower bounds of the mutual information is very interesting. The authors should work on their presentation and this could be a very nice paper.  \n\n", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1330/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Control Visual Abstractions for Structured Exploration in Deep Reinforcement Learning", "abstract": "Exploration in environments with sparse rewards is a key challenge for reinforcement learning. How do we design agents with generic inductive biases so that they can explore in a consistent manner instead of just using local exploration schemes like epsilon-greedy? We propose an unsupervised reinforcement learning agent which learns a discrete pixel grouping model that preserves spatial geometry of the sensors and implicitly of the environment as well. We use this representation to derive geometric intrinsic reward functions, like centroid coordinates and area, and learn policies to control each one of them with off-policy learning. These policies form a basis set of behaviors (options) which allows us explore in a consistent way and use them in a hierarchical reinforcement learning setup to solve for extrinsically defined rewards. We show that our approach can scale to a variety of domains with competitive performance, including navigation in 3D environments and Atari games with sparse rewards.", "keywords": ["exploration", "deep reinforcement learning", "intrinsic motivation", "unsupervised learning"], "authorids": ["cdi@google.com", "tkulkarni@google.com", "avdnoord@google.com", "amnih@google.com", "vmnih@google.com"], "authors": ["catalin ionescu", "tejas kulkarni", "aaron van de oord", "andriy mnih", "vlad mnih"], "TL;DR": "structured exploration in deep reinforcement learning via unsupervised visual abstraction discovery and control", "pdf": "/pdf/748c4242f1ffa85472f907e610157d4664387154.pdf", "paperhash": "ionescu|learning_to_control_visual_abstractions_for_structured_exploration_in_deep_reinforcement_learning", "_bibtex": "@misc{\nionescu2019learning,\ntitle={Learning to Control Visual Abstractions for Structured Exploration in Deep Reinforcement Learning},\nauthor={catalin ionescu and tejas kulkarni and aaron van de oord and andriy mnih and vlad mnih},\nyear={2019},\nurl={https://openreview.net/forum?id=HJlWXhC5Km},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1330/Official_Review", "cdate": 1542234253437, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HJlWXhC5Km", "replyto": "HJlWXhC5Km", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1330/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335923417, "tmdate": 1552335923417, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1330/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "S1eCQpbq3X", "original": null, "number": 2, "cdate": 1541180710444, "ddate": null, "tcdate": 1541180710444, "tmdate": 1541533228250, "tddate": null, "forum": "HJlWXhC5Km", "replyto": "HJlWXhC5Km", "invitation": "ICLR.cc/2019/Conference/-/Paper1330/Official_Review", "content": {"title": "A  Structured Exploration Algorithm with Visual Abstractions", "review": "This paper proposed an algorithm for structured exploration in deep reinforcement learning via learning the visual abstractions from pixels. The proposed method learns discrete visual abstractions and derives intrinsic reward functions from them so as to help the agent to optimize the policy. \n\nThe proposed method is interesting in that learning the visual abstractions together with the policy may assist in computing an optimal policy. The method is learning a meta Q function and (E * M+1) other Q functions. The authors mentioned that their work is most similar to hierarchical-DQN (Kulkarni et al., 2016) but this work required hand-crafted instance segmentation and the agent architecture do not learn about many intrinsic rewards learners. However, I am concerned if the proposed method solved the problem with the need of hand-crafted instance segmentation since, as shown in the Algorithm 1 and the caption of Figure 2, Q_{meta} acts every T steps. I do not understand why the meta Q function is used to propose actions for every fixed number of steps. Besides that, though the proposed method does have many intrinsic reward functions (in fact, there are E * M additional intrinsic reward functions). However, the authors did not show in the experiments if having too many intrinsic reward functions helps a lot. It will be better if the authors can show that, larger values for E or M can make the performances better.\n\nAnother concern I have is on some of the experiment results. For the experiment results in Figure 5 and 6, only in the left figures can the results of the proposed methods outperform the baselines. Besides that, the authors may need to describe the baseline methods in the experiments in more details.\n\nAlso, it will be better if the authors can improve the paper a little bit with the writing. For example, it will be better if the authors can explain the variables X, Y and the distribution q when mentioning Equation 1 so that it is easier to understand the paper. Also, there are some typos, such as the section reference on line 10 of Algorithm 1, the definition of the g function on the last line of page 3 (I guess the authors want to write \"{0...E}\" instead of \"{0, E}\") and the second sentence of the experiment section (at least I did not see the supplementary sections, but the authors mentioned that). It is better if these typos can be fixed. \n \n\nReferences:\n\nTejas D Kulkarni, Karthik Narasimhan, Ardavan Saeedi, and Josh Tenenbaum. Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation. In Advances in neural information processing systems, pp. 3675\u20133683, 2016.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1330/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Control Visual Abstractions for Structured Exploration in Deep Reinforcement Learning", "abstract": "Exploration in environments with sparse rewards is a key challenge for reinforcement learning. How do we design agents with generic inductive biases so that they can explore in a consistent manner instead of just using local exploration schemes like epsilon-greedy? We propose an unsupervised reinforcement learning agent which learns a discrete pixel grouping model that preserves spatial geometry of the sensors and implicitly of the environment as well. We use this representation to derive geometric intrinsic reward functions, like centroid coordinates and area, and learn policies to control each one of them with off-policy learning. These policies form a basis set of behaviors (options) which allows us explore in a consistent way and use them in a hierarchical reinforcement learning setup to solve for extrinsically defined rewards. We show that our approach can scale to a variety of domains with competitive performance, including navigation in 3D environments and Atari games with sparse rewards.", "keywords": ["exploration", "deep reinforcement learning", "intrinsic motivation", "unsupervised learning"], "authorids": ["cdi@google.com", "tkulkarni@google.com", "avdnoord@google.com", "amnih@google.com", "vmnih@google.com"], "authors": ["catalin ionescu", "tejas kulkarni", "aaron van de oord", "andriy mnih", "vlad mnih"], "TL;DR": "structured exploration in deep reinforcement learning via unsupervised visual abstraction discovery and control", "pdf": "/pdf/748c4242f1ffa85472f907e610157d4664387154.pdf", "paperhash": "ionescu|learning_to_control_visual_abstractions_for_structured_exploration_in_deep_reinforcement_learning", "_bibtex": "@misc{\nionescu2019learning,\ntitle={Learning to Control Visual Abstractions for Structured Exploration in Deep Reinforcement Learning},\nauthor={catalin ionescu and tejas kulkarni and aaron van de oord and andriy mnih and vlad mnih},\nyear={2019},\nurl={https://openreview.net/forum?id=HJlWXhC5Km},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1330/Official_Review", "cdate": 1542234253437, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HJlWXhC5Km", "replyto": "HJlWXhC5Km", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1330/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335923417, "tmdate": 1552335923417, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1330/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 11}