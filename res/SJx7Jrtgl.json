{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396352138, "tcdate": 1486396352138, "number": 1, "id": "HydpiMIdx", "invitation": "ICLR.cc/2017/conference/-/paper93/acceptance", "forum": "SJx7Jrtgl", "replyto": "SJx7Jrtgl", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "The reviewers have looked through both the responses, updates, and had much discussion. We agree that the paper is well executed and exposes ideas that are of value and interest. At the same same, the extent to which the methods can be applied in practice and scaled to different types of problems remains unclear, especially in high-dimensional settings. For this reason, we feel that the paper is not yet ready for acceptance in this years conference."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Unsupervised Clustering with Gaussian Mixture Variational Autoencoders", "abstract": "We study a variant of the variational autoencoder model (VAE) with a Gaussian mixture as a prior distribution, with the goal of performing unsupervised clustering through deep generative models. We observe that the known problem of over-regularisation that has been shown to arise in regular VAEs also manifests itself in our model and leads to cluster degeneracy. We show that a heuristic called minimum information constraint that has been shown to mitigate this effect in VAEs can also be applied to improve unsupervised clustering performance with our model. Furthermore we analyse the effect of this heuristic and provide an intuition of the various processes with the help of visualizations. Finally, we demonstrate the performance of our model on synthetic data, MNIST and SVHN, showing that the obtained clusters are distinct, interpretable and result in achieving competitive performance on unsupervised clustering to the state-of-the-art results.", "pdf": "/pdf/8ab4a1509cc93eedf19c7ef4f404209b0aebfe7e.pdf", "TL;DR": "We study a variant of the variational autoencoder model with a Gaussian mixture as prior distribution and discuss its optimization difficulties and capabilities for unsupervised clustering.", "paperhash": "dilokthanakul|deep_unsupervised_clustering_with_gaussian_mixture_variational_autoencoders", "conflicts": ["imperial.ac.uk"], "keywords": ["Unsupervised Learning", "Deep learning"], "authors": ["Nat Dilokthanakul", "Pedro A. M. Mediano", "Marta Garnelo", "Matthew C.H. Lee", "Hugh Salimbeni", "Kai Arulkumaran", "Murray Shanahan"], "authorids": ["n.dilokthanakul14@imperial.ac.uk", "pmediano@imperial.ac.uk", "m.garnelo-abellanas13@imperial.ac.uk", "matthew.lee13@imperial.ac.uk", "h.salimbeni15@imperial.ac.uk", "kailash.arulkumaran13@imperial.ac.uk", "m.shanahan@imperial.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396352650, "id": "ICLR.cc/2017/conference/-/paper93/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "SJx7Jrtgl", "replyto": "SJx7Jrtgl", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396352650}}}, {"tddate": null, "tmdate": 1484312005621, "tcdate": 1484312005621, "number": 14, "id": "HyCppHL8l", "invitation": "ICLR.cc/2017/conference/-/paper93/public/comment", "forum": "SJx7Jrtgl", "replyto": "rySYcM6mg", "signatures": ["~Nat_Dilokthanakul1"], "readers": ["everyone"], "writers": ["~Nat_Dilokthanakul1"], "content": {"title": "reply to comment", "comment": "Sorry for the late reply. This interesting blog post http://ruishu.io/2016/12/25/gmvae/ has carried out an interesting analysis of model M2 and shown that it is not well suited for clustering tasks."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Unsupervised Clustering with Gaussian Mixture Variational Autoencoders", "abstract": "We study a variant of the variational autoencoder model (VAE) with a Gaussian mixture as a prior distribution, with the goal of performing unsupervised clustering through deep generative models. We observe that the known problem of over-regularisation that has been shown to arise in regular VAEs also manifests itself in our model and leads to cluster degeneracy. We show that a heuristic called minimum information constraint that has been shown to mitigate this effect in VAEs can also be applied to improve unsupervised clustering performance with our model. Furthermore we analyse the effect of this heuristic and provide an intuition of the various processes with the help of visualizations. Finally, we demonstrate the performance of our model on synthetic data, MNIST and SVHN, showing that the obtained clusters are distinct, interpretable and result in achieving competitive performance on unsupervised clustering to the state-of-the-art results.", "pdf": "/pdf/8ab4a1509cc93eedf19c7ef4f404209b0aebfe7e.pdf", "TL;DR": "We study a variant of the variational autoencoder model with a Gaussian mixture as prior distribution and discuss its optimization difficulties and capabilities for unsupervised clustering.", "paperhash": "dilokthanakul|deep_unsupervised_clustering_with_gaussian_mixture_variational_autoencoders", "conflicts": ["imperial.ac.uk"], "keywords": ["Unsupervised Learning", "Deep learning"], "authors": ["Nat Dilokthanakul", "Pedro A. M. Mediano", "Marta Garnelo", "Matthew C.H. Lee", "Hugh Salimbeni", "Kai Arulkumaran", "Murray Shanahan"], "authorids": ["n.dilokthanakul14@imperial.ac.uk", "pmediano@imperial.ac.uk", "m.garnelo-abellanas13@imperial.ac.uk", "matthew.lee13@imperial.ac.uk", "h.salimbeni15@imperial.ac.uk", "kailash.arulkumaran13@imperial.ac.uk", "m.shanahan@imperial.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287731533, "id": "ICLR.cc/2017/conference/-/paper93/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SJx7Jrtgl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper93/reviewers", "ICLR.cc/2017/conference/paper93/areachairs"], "cdate": 1485287731533}}}, {"tddate": null, "tmdate": 1484310866088, "tcdate": 1484310866088, "number": 13, "id": "Sk5LtSL8e", "invitation": "ICLR.cc/2017/conference/-/paper93/public/comment", "forum": "SJx7Jrtgl", "replyto": "BJC3xsWEe", "signatures": ["~Nat_Dilokthanakul1"], "readers": ["everyone"], "writers": ["~Nat_Dilokthanakul1"], "content": {"title": "Reply to AnonReviewer2", "comment": "We would like to thank you for your review and the very useful suggestion with regards to the inference model. We addressed a justification of the additional complexity by comparing with GMM in section 4.1. We do find that our model performs worse than adversarial autoencoders and we have included our hypothesis as to why this is the case in section 4.2. With regards to the concern about the results, we confirm that the results we report are testing results. Please also refer to our summary of the paper revision for a more comprehensive overview of the changes we have made in response to all of the reviewers."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Unsupervised Clustering with Gaussian Mixture Variational Autoencoders", "abstract": "We study a variant of the variational autoencoder model (VAE) with a Gaussian mixture as a prior distribution, with the goal of performing unsupervised clustering through deep generative models. We observe that the known problem of over-regularisation that has been shown to arise in regular VAEs also manifests itself in our model and leads to cluster degeneracy. We show that a heuristic called minimum information constraint that has been shown to mitigate this effect in VAEs can also be applied to improve unsupervised clustering performance with our model. Furthermore we analyse the effect of this heuristic and provide an intuition of the various processes with the help of visualizations. Finally, we demonstrate the performance of our model on synthetic data, MNIST and SVHN, showing that the obtained clusters are distinct, interpretable and result in achieving competitive performance on unsupervised clustering to the state-of-the-art results.", "pdf": "/pdf/8ab4a1509cc93eedf19c7ef4f404209b0aebfe7e.pdf", "TL;DR": "We study a variant of the variational autoencoder model with a Gaussian mixture as prior distribution and discuss its optimization difficulties and capabilities for unsupervised clustering.", "paperhash": "dilokthanakul|deep_unsupervised_clustering_with_gaussian_mixture_variational_autoencoders", "conflicts": ["imperial.ac.uk"], "keywords": ["Unsupervised Learning", "Deep learning"], "authors": ["Nat Dilokthanakul", "Pedro A. M. Mediano", "Marta Garnelo", "Matthew C.H. Lee", "Hugh Salimbeni", "Kai Arulkumaran", "Murray Shanahan"], "authorids": ["n.dilokthanakul14@imperial.ac.uk", "pmediano@imperial.ac.uk", "m.garnelo-abellanas13@imperial.ac.uk", "matthew.lee13@imperial.ac.uk", "h.salimbeni15@imperial.ac.uk", "kailash.arulkumaran13@imperial.ac.uk", "m.shanahan@imperial.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287731533, "id": "ICLR.cc/2017/conference/-/paper93/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SJx7Jrtgl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper93/reviewers", "ICLR.cc/2017/conference/paper93/areachairs"], "cdate": 1485287731533}}}, {"tddate": null, "tmdate": 1484310690783, "tcdate": 1484310690783, "number": 12, "id": "rJoiuSUUx", "invitation": "ICLR.cc/2017/conference/-/paper93/public/comment", "forum": "SJx7Jrtgl", "replyto": "SkxLJRyMVl", "signatures": ["~Nat_Dilokthanakul1"], "readers": ["everyone"], "writers": ["~Nat_Dilokthanakul1"], "content": {"title": "Reply to AnonReviewer3", "comment": "Thank you for suggesting an interesting related paper. We have included it in section 3, where we acknowledge previous work on backpropagation through stochastic variables. We emphasise that our choice was made to make the inference model as simple as possible. While we agree that scaling up sideways might not be the best idea as the model scales up linearly with the number of clusters, we believe that stacking GMVAEs on top of each other could in theory increases the effective number of clusters much better. \n\nWe do agree that our old argument about mean-field is not very well explained and not necessarily correct as you have mentioned. As a result, we have approached this problem from a new angle and explained our rationale in section 3.3, 3.4 and in the experiments on the synthetic dataset. Please also refer to our summary of the paper revision for a more comprehensive overview of the changes we have made in response to all of the reviewers."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Unsupervised Clustering with Gaussian Mixture Variational Autoencoders", "abstract": "We study a variant of the variational autoencoder model (VAE) with a Gaussian mixture as a prior distribution, with the goal of performing unsupervised clustering through deep generative models. We observe that the known problem of over-regularisation that has been shown to arise in regular VAEs also manifests itself in our model and leads to cluster degeneracy. We show that a heuristic called minimum information constraint that has been shown to mitigate this effect in VAEs can also be applied to improve unsupervised clustering performance with our model. Furthermore we analyse the effect of this heuristic and provide an intuition of the various processes with the help of visualizations. Finally, we demonstrate the performance of our model on synthetic data, MNIST and SVHN, showing that the obtained clusters are distinct, interpretable and result in achieving competitive performance on unsupervised clustering to the state-of-the-art results.", "pdf": "/pdf/8ab4a1509cc93eedf19c7ef4f404209b0aebfe7e.pdf", "TL;DR": "We study a variant of the variational autoencoder model with a Gaussian mixture as prior distribution and discuss its optimization difficulties and capabilities for unsupervised clustering.", "paperhash": "dilokthanakul|deep_unsupervised_clustering_with_gaussian_mixture_variational_autoencoders", "conflicts": ["imperial.ac.uk"], "keywords": ["Unsupervised Learning", "Deep learning"], "authors": ["Nat Dilokthanakul", "Pedro A. M. Mediano", "Marta Garnelo", "Matthew C.H. Lee", "Hugh Salimbeni", "Kai Arulkumaran", "Murray Shanahan"], "authorids": ["n.dilokthanakul14@imperial.ac.uk", "pmediano@imperial.ac.uk", "m.garnelo-abellanas13@imperial.ac.uk", "matthew.lee13@imperial.ac.uk", "h.salimbeni15@imperial.ac.uk", "kailash.arulkumaran13@imperial.ac.uk", "m.shanahan@imperial.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287731533, "id": "ICLR.cc/2017/conference/-/paper93/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SJx7Jrtgl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper93/reviewers", "ICLR.cc/2017/conference/paper93/areachairs"], "cdate": 1485287731533}}}, {"tddate": null, "tmdate": 1484310575744, "tcdate": 1484310575744, "number": 11, "id": "ry_4uHI8g", "invitation": "ICLR.cc/2017/conference/-/paper93/public/comment", "forum": "SJx7Jrtgl", "replyto": "BkE1E-fNx", "signatures": ["~Nat_Dilokthanakul1"], "readers": ["everyone"], "writers": ["~Nat_Dilokthanakul1"], "content": {"title": "Reply to AnonReviewer1", "comment": "We would like to thank you for the constructive and encouraging review. We hope that our new model without ad-hoc parameters is more attractive from a practical point of view. Please also refer to our summary of the paper revision for a more comprehensive overview of the changes we have made in response to all of the reviewers."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Unsupervised Clustering with Gaussian Mixture Variational Autoencoders", "abstract": "We study a variant of the variational autoencoder model (VAE) with a Gaussian mixture as a prior distribution, with the goal of performing unsupervised clustering through deep generative models. We observe that the known problem of over-regularisation that has been shown to arise in regular VAEs also manifests itself in our model and leads to cluster degeneracy. We show that a heuristic called minimum information constraint that has been shown to mitigate this effect in VAEs can also be applied to improve unsupervised clustering performance with our model. Furthermore we analyse the effect of this heuristic and provide an intuition of the various processes with the help of visualizations. Finally, we demonstrate the performance of our model on synthetic data, MNIST and SVHN, showing that the obtained clusters are distinct, interpretable and result in achieving competitive performance on unsupervised clustering to the state-of-the-art results.", "pdf": "/pdf/8ab4a1509cc93eedf19c7ef4f404209b0aebfe7e.pdf", "TL;DR": "We study a variant of the variational autoencoder model with a Gaussian mixture as prior distribution and discuss its optimization difficulties and capabilities for unsupervised clustering.", "paperhash": "dilokthanakul|deep_unsupervised_clustering_with_gaussian_mixture_variational_autoencoders", "conflicts": ["imperial.ac.uk"], "keywords": ["Unsupervised Learning", "Deep learning"], "authors": ["Nat Dilokthanakul", "Pedro A. M. Mediano", "Marta Garnelo", "Matthew C.H. Lee", "Hugh Salimbeni", "Kai Arulkumaran", "Murray Shanahan"], "authorids": ["n.dilokthanakul14@imperial.ac.uk", "pmediano@imperial.ac.uk", "m.garnelo-abellanas13@imperial.ac.uk", "matthew.lee13@imperial.ac.uk", "h.salimbeni15@imperial.ac.uk", "kailash.arulkumaran13@imperial.ac.uk", "m.shanahan@imperial.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287731533, "id": "ICLR.cc/2017/conference/-/paper93/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SJx7Jrtgl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper93/reviewers", "ICLR.cc/2017/conference/paper93/areachairs"], "cdate": 1485287731533}}}, {"tddate": null, "tmdate": 1484310507751, "tcdate": 1484310507751, "number": 10, "id": "H14edSLIe", "invitation": "ICLR.cc/2017/conference/-/paper93/public/comment", "forum": "SJx7Jrtgl", "replyto": "SJx7Jrtgl", "signatures": ["~Nat_Dilokthanakul1"], "readers": ["everyone"], "writers": ["~Nat_Dilokthanakul1"], "content": {"title": "Changes to the paper", "comment": "We would like to thank the reviewers for the constructive reviews which have, indeed, been very useful. We apologize for the late reply as we had to redo the majority of the experiments and rewrite a big part of the paper accordingly. The following is an outline of the major modifications to the paper:\n\n1.) We have changed our inference model and use the approximate posterior suggested by AnonReviewer2. This means the model we now call GMVAE corresponds to what we called GMVAE+ in the old version of the paper. (See section 3.2).\n\n2.) The entire analysis is now specific to this model. We have removed the argument about the mean-field problem which, as argued by AnonReviewer3, might not be strictly correct. Instead, we approach the problem from a different angle and address the problem of over-regularisation (over-pruning) in VAEs. We have high confidence that this argument is both better suited and more grounded. (See section 3.3 and section 3.4).\n\n3.) Following the analysis in section 3.4, we solve the problem with a heuristic called \"minimum information constraint (free-bits)\" that was introduced by Kingma et al. (2016). As a result we do not resort to ad-hoc modifications like the eta-term in the previous version, but introduce a different tuning parameter, lambda, instead. We study this parameter, lambda, on the synthetic dataset and visualize its behaviour. We found that the GMVAE has a high tolerance region for the possible values of lambda and moreover does not require this heuristic for the MNIST experiments at all. Therefore, from a practical point of view we have improved our model and alleviated the concerns raised by AnonReviewer1. (see section 4.1 and 4.2)\n\n4.) The MNIST and SVHN experiments work successfully without any regularisation using the new inference model suggested by AnonReviewer2. As a result, we decided to take out the section on consistency violation that introduced an extra tuning parameter with unintuitive behaviour.\n\n5.) Crucially, we study the problem of over-regularisation in GMVAEs which manifests itself differently from regular VAEs. We explicitly show the behaviour of the \"minimum information constraint\" heuristic that we use to address the problem.\n\nIn conclusion, we have redone all experiments using the new inference model which led to a considerable increase in performance and allowed us to remove all ad-hoc tuning parameters. We show that, although we can train GMVAE using the normal ELBO on the MNIST dataset, the over-regularisation problem persists when trained on our synthetic dataset. Finally, we show that a heuristic introduced to solve the over-regularisation problem in standard VAEs can be used to improve our model as well.\n\nWe hope the reviewers see the intuitions gained from the analysis and the experiments as useful contributions. Although simple, we believe that GMVAEs or deep GMVAEs (stacked GMVAEs) are a very interesting class of model which has the potential to become very powerful as we gain more understanding about the underlying inference process."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Unsupervised Clustering with Gaussian Mixture Variational Autoencoders", "abstract": "We study a variant of the variational autoencoder model (VAE) with a Gaussian mixture as a prior distribution, with the goal of performing unsupervised clustering through deep generative models. We observe that the known problem of over-regularisation that has been shown to arise in regular VAEs also manifests itself in our model and leads to cluster degeneracy. We show that a heuristic called minimum information constraint that has been shown to mitigate this effect in VAEs can also be applied to improve unsupervised clustering performance with our model. Furthermore we analyse the effect of this heuristic and provide an intuition of the various processes with the help of visualizations. Finally, we demonstrate the performance of our model on synthetic data, MNIST and SVHN, showing that the obtained clusters are distinct, interpretable and result in achieving competitive performance on unsupervised clustering to the state-of-the-art results.", "pdf": "/pdf/8ab4a1509cc93eedf19c7ef4f404209b0aebfe7e.pdf", "TL;DR": "We study a variant of the variational autoencoder model with a Gaussian mixture as prior distribution and discuss its optimization difficulties and capabilities for unsupervised clustering.", "paperhash": "dilokthanakul|deep_unsupervised_clustering_with_gaussian_mixture_variational_autoencoders", "conflicts": ["imperial.ac.uk"], "keywords": ["Unsupervised Learning", "Deep learning"], "authors": ["Nat Dilokthanakul", "Pedro A. M. Mediano", "Marta Garnelo", "Matthew C.H. Lee", "Hugh Salimbeni", "Kai Arulkumaran", "Murray Shanahan"], "authorids": ["n.dilokthanakul14@imperial.ac.uk", "pmediano@imperial.ac.uk", "m.garnelo-abellanas13@imperial.ac.uk", "matthew.lee13@imperial.ac.uk", "h.salimbeni15@imperial.ac.uk", "kailash.arulkumaran13@imperial.ac.uk", "m.shanahan@imperial.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287731533, "id": "ICLR.cc/2017/conference/-/paper93/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SJx7Jrtgl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper93/reviewers", "ICLR.cc/2017/conference/paper93/areachairs"], "cdate": 1485287731533}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1484310320779, "tcdate": 1478213400310, "number": 93, "id": "SJx7Jrtgl", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "SJx7Jrtgl", "signatures": ["~Nat_Dilokthanakul1"], "readers": ["everyone"], "content": {"title": "Deep Unsupervised Clustering with Gaussian Mixture Variational Autoencoders", "abstract": "We study a variant of the variational autoencoder model (VAE) with a Gaussian mixture as a prior distribution, with the goal of performing unsupervised clustering through deep generative models. We observe that the known problem of over-regularisation that has been shown to arise in regular VAEs also manifests itself in our model and leads to cluster degeneracy. We show that a heuristic called minimum information constraint that has been shown to mitigate this effect in VAEs can also be applied to improve unsupervised clustering performance with our model. Furthermore we analyse the effect of this heuristic and provide an intuition of the various processes with the help of visualizations. Finally, we demonstrate the performance of our model on synthetic data, MNIST and SVHN, showing that the obtained clusters are distinct, interpretable and result in achieving competitive performance on unsupervised clustering to the state-of-the-art results.", "pdf": "/pdf/8ab4a1509cc93eedf19c7ef4f404209b0aebfe7e.pdf", "TL;DR": "We study a variant of the variational autoencoder model with a Gaussian mixture as prior distribution and discuss its optimization difficulties and capabilities for unsupervised clustering.", "paperhash": "dilokthanakul|deep_unsupervised_clustering_with_gaussian_mixture_variational_autoencoders", "conflicts": ["imperial.ac.uk"], "keywords": ["Unsupervised Learning", "Deep learning"], "authors": ["Nat Dilokthanakul", "Pedro A. M. Mediano", "Marta Garnelo", "Matthew C.H. Lee", "Hugh Salimbeni", "Kai Arulkumaran", "Murray Shanahan"], "authorids": ["n.dilokthanakul14@imperial.ac.uk", "pmediano@imperial.ac.uk", "m.garnelo-abellanas13@imperial.ac.uk", "matthew.lee13@imperial.ac.uk", "h.salimbeni15@imperial.ac.uk", "kailash.arulkumaran13@imperial.ac.uk", "m.shanahan@imperial.ac.uk"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 17, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "tmdate": 1481933788318, "tcdate": 1481933788318, "number": 3, "id": "BkE1E-fNx", "invitation": "ICLR.cc/2017/conference/-/paper93/official/review", "forum": "SJx7Jrtgl", "replyto": "SJx7Jrtgl", "signatures": ["ICLR.cc/2017/conference/paper93/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper93/AnonReviewer1"], "content": {"title": "", "rating": "8: Top 50% of accepted papers, clear accept", "review": "This submission proposes an approach to adapting the variational auto-encoder framework (VAE) to the clustering scenario. First the model has to be adapted (with a Gaussian mixture as a prior) and then the inference has to become consistent (by introducing a regularization term). \n\nA general positive point about this paper is that the model construction is kept simple. Indeed, the assumption about the mixture prior is simple but reasonable, and the inference follows the VAE framework where appropriate with only changing parts that do not conform with the clustering task. These changes are also motivated by some analysis. The presentation is also kept simple: the linking to VAE and related methods is made in an clear and honest way, so that it's easy to follow the paper and understand how everything fits together. \n\nAlso, the regularization term is a well motivated and reasonable addition. Given the VAE context in this paper, I'd be interested in seeing a discussion on the variance of the samples in (6).\n\nA negative issue of this paper is that all crucial regularizations rely upon ad-hoc parameters that control their strength, namely \\eta (eq. (3)) and \\alpha (eq. 7). According to the authors adjusting these parameters is crucial, and there seems to be no principled way of adjusting them. It also seems that these two parameters interact, since they both regularize z in different ways. This makes the search space over them grow multiplicatively, since the tuning problem now becomes combinatorial. The authors mention that they tune the trade-off between these two regularizers, but I'd be interested in a comment concerning how this is done (what's the space of parameters to search on). In practical clustering applications, high sensitivity to tuned parameters is undesirable, since one also needs to cross-validate values of K at the same time.\n\nI really liked the experiments section. It is not very exhaustive in terms of comparison, but it is very exploratory in terms of demonstrating the model components, strengths and weaknesses. This is much more useful than reporting unintuitive percentage improvements relative to arbitrarily selected baselines and datasets.\n\n\nOverall, I am a little concerned about the practicality of this approach, given the tuning it requires. However, I am in favor of accepting this paper because it makes its strong and weak points very clear through good explanation and demonstration. Therefore, I expect further research to be built on top of this paper, so that the aforementioned issues will hopefully be alleviated in the future. Finally, the theoretical intuitions given in the paper (and author comments) improve its usefulness as a scientific manuscript.\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Unsupervised Clustering with Gaussian Mixture Variational Autoencoders", "abstract": "We study a variant of the variational autoencoder model (VAE) with a Gaussian mixture as a prior distribution, with the goal of performing unsupervised clustering through deep generative models. We observe that the known problem of over-regularisation that has been shown to arise in regular VAEs also manifests itself in our model and leads to cluster degeneracy. We show that a heuristic called minimum information constraint that has been shown to mitigate this effect in VAEs can also be applied to improve unsupervised clustering performance with our model. Furthermore we analyse the effect of this heuristic and provide an intuition of the various processes with the help of visualizations. Finally, we demonstrate the performance of our model on synthetic data, MNIST and SVHN, showing that the obtained clusters are distinct, interpretable and result in achieving competitive performance on unsupervised clustering to the state-of-the-art results.", "pdf": "/pdf/8ab4a1509cc93eedf19c7ef4f404209b0aebfe7e.pdf", "TL;DR": "We study a variant of the variational autoencoder model with a Gaussian mixture as prior distribution and discuss its optimization difficulties and capabilities for unsupervised clustering.", "paperhash": "dilokthanakul|deep_unsupervised_clustering_with_gaussian_mixture_variational_autoencoders", "conflicts": ["imperial.ac.uk"], "keywords": ["Unsupervised Learning", "Deep learning"], "authors": ["Nat Dilokthanakul", "Pedro A. M. Mediano", "Marta Garnelo", "Matthew C.H. Lee", "Hugh Salimbeni", "Kai Arulkumaran", "Murray Shanahan"], "authorids": ["n.dilokthanakul14@imperial.ac.uk", "pmediano@imperial.ac.uk", "m.garnelo-abellanas13@imperial.ac.uk", "matthew.lee13@imperial.ac.uk", "h.salimbeni15@imperial.ac.uk", "kailash.arulkumaran13@imperial.ac.uk", "m.shanahan@imperial.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512701144, "id": "ICLR.cc/2017/conference/-/paper93/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper93/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper93/AnonReviewer2", "ICLR.cc/2017/conference/paper93/AnonReviewer3", "ICLR.cc/2017/conference/paper93/AnonReviewer1"], "reply": {"forum": "SJx7Jrtgl", "replyto": "SJx7Jrtgl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper93/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper93/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512701144}}}, {"tddate": null, "tmdate": 1481928158426, "tcdate": 1481928158426, "number": 2, "id": "SkxLJRyMVl", "invitation": "ICLR.cc/2017/conference/-/paper93/official/review", "forum": "SJx7Jrtgl", "replyto": "SJx7Jrtgl", "signatures": ["ICLR.cc/2017/conference/paper93/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper93/AnonReviewer3"], "content": {"title": "Review: Deep Unsupervised Clustering with Gaussian Mixture Variational Autoencoders", "rating": "4: Ok but not good enough - rejection", "review": "The authors posit a mixture of Gaussian prior for variational\nauto-encoders. They also consider a regularization term motivated\nfrom information theory.\n\nThe modeling extension is simple and the inference follows\nmechanically from what's already standard in the literature. Instead\nof using discrete latent variable samples they collapse the expected\nKL; this works for few mixture components and has been considered\nbefore in more general contexts, e.g., Titsias and Lazaro-Gredilla\n(2015). It will not scale to many mixture components.\n\nI find the discussion in Section 3.2.2 difficult to parse and, if I\nunderstood it correctly, not necessarily correct. Many arguments are\nintroduced and few fleshed out. First, there is a claim that a\nmultinomial prior with equal class probabilities assigns the same\nnumber of data points to each class on average; this is true a priori\nbut certainly not true given data. Second, they claim the KL\nregularizer forces the approximate posterior to be close to this\nuniform; this is only true for small data, certainly the energy term\nin the ELBO (expected log-likelihood) will overpower the regularizer;\nis this not the case in a mean-field approximation to a mixture of\nGaussians model? Third, there is a claim that \"under the mean-field\napproximation, this constraint is enforced on each sample\"; how does\nthe mean-field approximation enforce a constraint on the effect of\nMonte Carlo sampling? Fourth, they argue Johnson et al. (2016) can\novercome this issue partly due to SVI; how does data subsampling\naffect this behavior? Fifth, they derive the exact posterior in\nEquation 6; so to what extent are these arguments relevant?\n\nThe experiments are limited on toy data and only a few mixture\ncomponents are considered (not enough where the collapsed approach\nwill not scale).\n\n+ Titsias, M. K., & L\u00e1zaro-Gredilla, M. (2015). Local Expectation Gradients for Black Box Variational Inference. In Neural Information Processing Systems.\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Unsupervised Clustering with Gaussian Mixture Variational Autoencoders", "abstract": "We study a variant of the variational autoencoder model (VAE) with a Gaussian mixture as a prior distribution, with the goal of performing unsupervised clustering through deep generative models. We observe that the known problem of over-regularisation that has been shown to arise in regular VAEs also manifests itself in our model and leads to cluster degeneracy. We show that a heuristic called minimum information constraint that has been shown to mitigate this effect in VAEs can also be applied to improve unsupervised clustering performance with our model. Furthermore we analyse the effect of this heuristic and provide an intuition of the various processes with the help of visualizations. Finally, we demonstrate the performance of our model on synthetic data, MNIST and SVHN, showing that the obtained clusters are distinct, interpretable and result in achieving competitive performance on unsupervised clustering to the state-of-the-art results.", "pdf": "/pdf/8ab4a1509cc93eedf19c7ef4f404209b0aebfe7e.pdf", "TL;DR": "We study a variant of the variational autoencoder model with a Gaussian mixture as prior distribution and discuss its optimization difficulties and capabilities for unsupervised clustering.", "paperhash": "dilokthanakul|deep_unsupervised_clustering_with_gaussian_mixture_variational_autoencoders", "conflicts": ["imperial.ac.uk"], "keywords": ["Unsupervised Learning", "Deep learning"], "authors": ["Nat Dilokthanakul", "Pedro A. M. Mediano", "Marta Garnelo", "Matthew C.H. Lee", "Hugh Salimbeni", "Kai Arulkumaran", "Murray Shanahan"], "authorids": ["n.dilokthanakul14@imperial.ac.uk", "pmediano@imperial.ac.uk", "m.garnelo-abellanas13@imperial.ac.uk", "matthew.lee13@imperial.ac.uk", "h.salimbeni15@imperial.ac.uk", "kailash.arulkumaran13@imperial.ac.uk", "m.shanahan@imperial.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512701144, "id": "ICLR.cc/2017/conference/-/paper93/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper93/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper93/AnonReviewer2", "ICLR.cc/2017/conference/paper93/AnonReviewer3", "ICLR.cc/2017/conference/paper93/AnonReviewer1"], "reply": {"forum": "SJx7Jrtgl", "replyto": "SJx7Jrtgl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper93/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper93/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512701144}}}, {"tddate": null, "tmdate": 1481908405758, "tcdate": 1481908405758, "number": 1, "id": "BJC3xsWEe", "invitation": "ICLR.cc/2017/conference/-/paper93/official/review", "forum": "SJx7Jrtgl", "replyto": "SJx7Jrtgl", "signatures": ["ICLR.cc/2017/conference/paper93/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper93/AnonReviewer2"], "content": {"title": "", "rating": "4: Ok but not good enough - rejection", "review": "The authors proposes to a variant of Variational Auto-Encoders using a mixture distribution to enable unsupervised clustering that they combine with an information-theoretical regularization. To demonstrate the merit of such approach, they perform experiments on a synthetic dataset, MNIST and SVHN.\nThe use of a mixture of VAE is an incremental idea if novel.\nI would like to see the comparison with the more straightforward use of a mixture of gaussians prior. This model is more complex and I would like to see a justification of this additional complexity.\nThe results in Table 1 are questionable. First of all, GMVAE+ seems to outperform other methods with M=1, there should be a run of GMVAE+ with M=10 for proper comparison. But what I find more disturbing in this table is the variance of the results, especially since you are taking the \"Best Run\". Was the best run maximizing the validation performance or the test performance ? Moreover, the average performance (higher) and standard deviation (lower) of Adversarial Autoencoder makes me question the claim that \"we have advanced the state of the art in deep unsupervised clustering both in theory and practice\".\nThe consistency violation regularization might be interesting. But the cluster degeneracy problem is, as far as I know, also a problem in plain mixture of gaussians models. So making an experiment on this simpler model on a synthetic dataset should also be done.\nIn general, I would recommend running more experiments as to solidify your claims.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Unsupervised Clustering with Gaussian Mixture Variational Autoencoders", "abstract": "We study a variant of the variational autoencoder model (VAE) with a Gaussian mixture as a prior distribution, with the goal of performing unsupervised clustering through deep generative models. We observe that the known problem of over-regularisation that has been shown to arise in regular VAEs also manifests itself in our model and leads to cluster degeneracy. We show that a heuristic called minimum information constraint that has been shown to mitigate this effect in VAEs can also be applied to improve unsupervised clustering performance with our model. Furthermore we analyse the effect of this heuristic and provide an intuition of the various processes with the help of visualizations. Finally, we demonstrate the performance of our model on synthetic data, MNIST and SVHN, showing that the obtained clusters are distinct, interpretable and result in achieving competitive performance on unsupervised clustering to the state-of-the-art results.", "pdf": "/pdf/8ab4a1509cc93eedf19c7ef4f404209b0aebfe7e.pdf", "TL;DR": "We study a variant of the variational autoencoder model with a Gaussian mixture as prior distribution and discuss its optimization difficulties and capabilities for unsupervised clustering.", "paperhash": "dilokthanakul|deep_unsupervised_clustering_with_gaussian_mixture_variational_autoencoders", "conflicts": ["imperial.ac.uk"], "keywords": ["Unsupervised Learning", "Deep learning"], "authors": ["Nat Dilokthanakul", "Pedro A. M. Mediano", "Marta Garnelo", "Matthew C.H. Lee", "Hugh Salimbeni", "Kai Arulkumaran", "Murray Shanahan"], "authorids": ["n.dilokthanakul14@imperial.ac.uk", "pmediano@imperial.ac.uk", "m.garnelo-abellanas13@imperial.ac.uk", "matthew.lee13@imperial.ac.uk", "h.salimbeni15@imperial.ac.uk", "kailash.arulkumaran13@imperial.ac.uk", "m.shanahan@imperial.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512701144, "id": "ICLR.cc/2017/conference/-/paper93/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper93/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper93/AnonReviewer2", "ICLR.cc/2017/conference/paper93/AnonReviewer3", "ICLR.cc/2017/conference/paper93/AnonReviewer1"], "reply": {"forum": "SJx7Jrtgl", "replyto": "SJx7Jrtgl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper93/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper93/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512701144}}}, {"tddate": null, "tmdate": 1481052545317, "tcdate": 1480981099626, "number": 7, "id": "rJ4d5O77x", "invitation": "ICLR.cc/2017/conference/-/paper93/public/comment", "forum": "SJx7Jrtgl", "replyto": "HkHPwiJQg", "signatures": ["~Nat_Dilokthanakul1"], "readers": ["everyone"], "writers": ["~Nat_Dilokthanakul1"], "content": {"title": "The ad-hoc parameter", "comment": "Thank you for your comment. We agree that our method could be extended with a hierarchical prior on z; however, as you mentioned, this does introduce more difficulties. In our work we use eta to illustrate the effect of the z-prior term on clustering \u2013 as shown in Figure 2, turning off the effect of z-prior term improves the model\u2019s ability to form separate clusters. With CV regularization, the z-prior term no longer needs to be turned off, thereby highlighting the benefit of information-theoretic regularization. Although we are considering hierarchical priors or alternative priors for future work, we believe that we have shown that CV regularization is beneficial, and empirically it removes the need for the ad-hoc eta.\n\nAs a side note, CV regularisation could be interpreted as imposing an assumption that the Gaussians are not supposed to be overlapping. It would be interesting to make more connection to a sparse prior assumption on z. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Unsupervised Clustering with Gaussian Mixture Variational Autoencoders", "abstract": "We study a variant of the variational autoencoder model (VAE) with a Gaussian mixture as a prior distribution, with the goal of performing unsupervised clustering through deep generative models. We observe that the known problem of over-regularisation that has been shown to arise in regular VAEs also manifests itself in our model and leads to cluster degeneracy. We show that a heuristic called minimum information constraint that has been shown to mitigate this effect in VAEs can also be applied to improve unsupervised clustering performance with our model. Furthermore we analyse the effect of this heuristic and provide an intuition of the various processes with the help of visualizations. Finally, we demonstrate the performance of our model on synthetic data, MNIST and SVHN, showing that the obtained clusters are distinct, interpretable and result in achieving competitive performance on unsupervised clustering to the state-of-the-art results.", "pdf": "/pdf/8ab4a1509cc93eedf19c7ef4f404209b0aebfe7e.pdf", "TL;DR": "We study a variant of the variational autoencoder model with a Gaussian mixture as prior distribution and discuss its optimization difficulties and capabilities for unsupervised clustering.", "paperhash": "dilokthanakul|deep_unsupervised_clustering_with_gaussian_mixture_variational_autoencoders", "conflicts": ["imperial.ac.uk"], "keywords": ["Unsupervised Learning", "Deep learning"], "authors": ["Nat Dilokthanakul", "Pedro A. M. Mediano", "Marta Garnelo", "Matthew C.H. Lee", "Hugh Salimbeni", "Kai Arulkumaran", "Murray Shanahan"], "authorids": ["n.dilokthanakul14@imperial.ac.uk", "pmediano@imperial.ac.uk", "m.garnelo-abellanas13@imperial.ac.uk", "matthew.lee13@imperial.ac.uk", "h.salimbeni15@imperial.ac.uk", "kailash.arulkumaran13@imperial.ac.uk", "m.shanahan@imperial.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287731533, "id": "ICLR.cc/2017/conference/-/paper93/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SJx7Jrtgl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper93/reviewers", "ICLR.cc/2017/conference/paper93/areachairs"], "cdate": 1485287731533}}}, {"tddate": null, "tmdate": 1480730460720, "tcdate": 1480730460715, "number": 1, "id": "HkHPwiJQg", "invitation": "ICLR.cc/2017/conference/-/paper93/pre-review/question", "forum": "SJx7Jrtgl", "replyto": "SJx7Jrtgl", "signatures": ["ICLR.cc/2017/conference/paper93/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper93/AnonReviewer1"], "content": {"title": "The ad-hoc parameter", "question": "Instead of adding the ad-hoc \\eta, wouldn't it be more principled to get a similar effect by putting the assumptions/constraints in the prior? For example, a hierarchical prior on z. I understand that that could introduce additional difficulties, but I was curious to get a comment from the authors.\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Unsupervised Clustering with Gaussian Mixture Variational Autoencoders", "abstract": "We study a variant of the variational autoencoder model (VAE) with a Gaussian mixture as a prior distribution, with the goal of performing unsupervised clustering through deep generative models. We observe that the known problem of over-regularisation that has been shown to arise in regular VAEs also manifests itself in our model and leads to cluster degeneracy. We show that a heuristic called minimum information constraint that has been shown to mitigate this effect in VAEs can also be applied to improve unsupervised clustering performance with our model. Furthermore we analyse the effect of this heuristic and provide an intuition of the various processes with the help of visualizations. Finally, we demonstrate the performance of our model on synthetic data, MNIST and SVHN, showing that the obtained clusters are distinct, interpretable and result in achieving competitive performance on unsupervised clustering to the state-of-the-art results.", "pdf": "/pdf/8ab4a1509cc93eedf19c7ef4f404209b0aebfe7e.pdf", "TL;DR": "We study a variant of the variational autoencoder model with a Gaussian mixture as prior distribution and discuss its optimization difficulties and capabilities for unsupervised clustering.", "paperhash": "dilokthanakul|deep_unsupervised_clustering_with_gaussian_mixture_variational_autoencoders", "conflicts": ["imperial.ac.uk"], "keywords": ["Unsupervised Learning", "Deep learning"], "authors": ["Nat Dilokthanakul", "Pedro A. M. Mediano", "Marta Garnelo", "Matthew C.H. Lee", "Hugh Salimbeni", "Kai Arulkumaran", "Murray Shanahan"], "authorids": ["n.dilokthanakul14@imperial.ac.uk", "pmediano@imperial.ac.uk", "m.garnelo-abellanas13@imperial.ac.uk", "matthew.lee13@imperial.ac.uk", "h.salimbeni15@imperial.ac.uk", "kailash.arulkumaran13@imperial.ac.uk", "m.shanahan@imperial.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959467540, "id": "ICLR.cc/2017/conference/-/paper93/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper93/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper93/AnonReviewer1"], "reply": {"forum": "SJx7Jrtgl", "replyto": "SJx7Jrtgl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper93/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper93/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959467540}}}, {"tddate": null, "tmdate": 1480088511212, "tcdate": 1480088511207, "number": 6, "id": "B1PpiRrfe", "invitation": "ICLR.cc/2017/conference/-/paper93/public/comment", "forum": "SJx7Jrtgl", "replyto": "HyoWIt7zg", "signatures": ["~Nat_Dilokthanakul1"], "readers": ["everyone"], "writers": ["~Nat_Dilokthanakul1"], "content": {"title": "Advantage of q_{\\phi_z}(z|y) ?", "comment": "Hi,\n\nThank you for your insightful suggestion. We absolutely agree that using p(z|x,w) instead of q(z|y) is going to give us a better approximate posterior. In fact, we just finished coded this up and ran some experiments. One benefit of using q(z|y) would be, q(z|y) would give us a convenient/cheap way to infer the classification labels. But, not using an approximation would surely out weight this.\n\nFor the synthetic dataset, the modified posterior doesn't change our main results where adjusting eta and alpha is crucial for successful clustering. For MNIST with 1 mc-sample, it improves the best classification result for K = 16 by 4 per cent. No conclusive improvement for the case of K = 10 without extensive experiment. More experiments will be run and we will discuss this in the next version of the paper."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Unsupervised Clustering with Gaussian Mixture Variational Autoencoders", "abstract": "We study a variant of the variational autoencoder model (VAE) with a Gaussian mixture as a prior distribution, with the goal of performing unsupervised clustering through deep generative models. We observe that the known problem of over-regularisation that has been shown to arise in regular VAEs also manifests itself in our model and leads to cluster degeneracy. We show that a heuristic called minimum information constraint that has been shown to mitigate this effect in VAEs can also be applied to improve unsupervised clustering performance with our model. Furthermore we analyse the effect of this heuristic and provide an intuition of the various processes with the help of visualizations. Finally, we demonstrate the performance of our model on synthetic data, MNIST and SVHN, showing that the obtained clusters are distinct, interpretable and result in achieving competitive performance on unsupervised clustering to the state-of-the-art results.", "pdf": "/pdf/8ab4a1509cc93eedf19c7ef4f404209b0aebfe7e.pdf", "TL;DR": "We study a variant of the variational autoencoder model with a Gaussian mixture as prior distribution and discuss its optimization difficulties and capabilities for unsupervised clustering.", "paperhash": "dilokthanakul|deep_unsupervised_clustering_with_gaussian_mixture_variational_autoencoders", "conflicts": ["imperial.ac.uk"], "keywords": ["Unsupervised Learning", "Deep learning"], "authors": ["Nat Dilokthanakul", "Pedro A. M. Mediano", "Marta Garnelo", "Matthew C.H. Lee", "Hugh Salimbeni", "Kai Arulkumaran", "Murray Shanahan"], "authorids": ["n.dilokthanakul14@imperial.ac.uk", "pmediano@imperial.ac.uk", "m.garnelo-abellanas13@imperial.ac.uk", "matthew.lee13@imperial.ac.uk", "h.salimbeni15@imperial.ac.uk", "kailash.arulkumaran13@imperial.ac.uk", "m.shanahan@imperial.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287731533, "id": "ICLR.cc/2017/conference/-/paper93/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SJx7Jrtgl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper93/reviewers", "ICLR.cc/2017/conference/paper93/areachairs"], "cdate": 1485287731533}}}, {"tddate": null, "tmdate": 1479935678654, "tcdate": 1479935490628, "number": 1, "id": "HyoWIt7zg", "invitation": "ICLR.cc/2017/conference/-/paper93/official/comment", "forum": "SJx7Jrtgl", "replyto": "SJx7Jrtgl", "signatures": ["ICLR.cc/2017/conference/paper93/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper93/AnonReviewer2"], "content": {"title": "Advantage of q_{\\phi_z}(z|y) ?", "comment": "I fail to understand what is the advantage of using a learned approximate posterior q_{\\phi_z}(z|y) when in equation (6), you derive the *exact* form of p(z|x,w). Why don't you use the approximate posterior q_{\\phi_w}(w|y)q_{\\phi_x}(x|y)p(z|x,w) instead of q_{\\phi_w}(w|y)q_{\\phi_x}(x|y)q_{\\phi_z}(z|y) ? Both are valid approximate posteriors, you have less parameters to train and your approximation would be less biased. I don't seem to see that experiment in the paper and I don't think it corresponds to any value of (\\eta, \\alpha).\nThe only justification I could see is that the integrals are not closed form anymore, but that have hardly been a major issue (if at all) when training variational auto-encoders with full differentiability. Is there something that I am not understanding ?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Unsupervised Clustering with Gaussian Mixture Variational Autoencoders", "abstract": "We study a variant of the variational autoencoder model (VAE) with a Gaussian mixture as a prior distribution, with the goal of performing unsupervised clustering through deep generative models. We observe that the known problem of over-regularisation that has been shown to arise in regular VAEs also manifests itself in our model and leads to cluster degeneracy. We show that a heuristic called minimum information constraint that has been shown to mitigate this effect in VAEs can also be applied to improve unsupervised clustering performance with our model. Furthermore we analyse the effect of this heuristic and provide an intuition of the various processes with the help of visualizations. Finally, we demonstrate the performance of our model on synthetic data, MNIST and SVHN, showing that the obtained clusters are distinct, interpretable and result in achieving competitive performance on unsupervised clustering to the state-of-the-art results.", "pdf": "/pdf/8ab4a1509cc93eedf19c7ef4f404209b0aebfe7e.pdf", "TL;DR": "We study a variant of the variational autoencoder model with a Gaussian mixture as prior distribution and discuss its optimization difficulties and capabilities for unsupervised clustering.", "paperhash": "dilokthanakul|deep_unsupervised_clustering_with_gaussian_mixture_variational_autoencoders", "conflicts": ["imperial.ac.uk"], "keywords": ["Unsupervised Learning", "Deep learning"], "authors": ["Nat Dilokthanakul", "Pedro A. M. Mediano", "Marta Garnelo", "Matthew C.H. Lee", "Hugh Salimbeni", "Kai Arulkumaran", "Murray Shanahan"], "authorids": ["n.dilokthanakul14@imperial.ac.uk", "pmediano@imperial.ac.uk", "m.garnelo-abellanas13@imperial.ac.uk", "matthew.lee13@imperial.ac.uk", "h.salimbeni15@imperial.ac.uk", "kailash.arulkumaran13@imperial.ac.uk", "m.shanahan@imperial.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287731406, "id": "ICLR.cc/2017/conference/-/paper93/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "SJx7Jrtgl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper93/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper93/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper93/reviewers", "ICLR.cc/2017/conference/paper93/areachairs"], "cdate": 1485287731406}}}, {"tddate": null, "tmdate": 1479402112105, "tcdate": 1479400694445, "number": 5, "id": "Bk0laLobl", "invitation": "ICLR.cc/2017/conference/-/paper93/public/comment", "forum": "SJx7Jrtgl", "replyto": "H1m3Op_-x", "signatures": ["~Nat_Dilokthanakul1"], "readers": ["everyone"], "writers": ["~Nat_Dilokthanakul1"], "content": {"title": "Backprop Through Mixture Samples", "comment": "Hi Eric, \n\nThank you for your interest! Unfortunately, we haven\u2019t had time to tried Grave's algorithm yet. Although it is not obvious how to calculate the CV term within his framework, it is a possible extension to our work. \n\nWe would like to take this opportunity to emphasise that a different more expressive posterior would not necessarily solve the problem of the ``anti-clustering term'' discussed in section 3.2.2. In our case, the source of problem is not that our posterior is not a GMM but rather the way that the variational posterior is factorised among observations -- i.e. q(Z|X) = \\prod_i q(z_i|x_i) where i indexes observations, z_i is a 'local' latent variable and x_i is an observation. Ideally, we would want Z to be 'global' variable that generates the whole X dataset -- but that would require passing the whole dataset through the recognition network to output just one encoding!\n\nMore obvious solutions would be:\n1.) Structured inference [1]\n2.) Passing the whole dataset through recognition network [2]\n\n[1] M.J. Johnson et al. Composing graphical model with neural networks for structured representations and fast inference (https://arxiv.org/abs/1603.06277)\n[2] H. Edwards, A. Storkey. Towards a Neural Statistician (https://arxiv.org/abs/1606.02185)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Unsupervised Clustering with Gaussian Mixture Variational Autoencoders", "abstract": "We study a variant of the variational autoencoder model (VAE) with a Gaussian mixture as a prior distribution, with the goal of performing unsupervised clustering through deep generative models. We observe that the known problem of over-regularisation that has been shown to arise in regular VAEs also manifests itself in our model and leads to cluster degeneracy. We show that a heuristic called minimum information constraint that has been shown to mitigate this effect in VAEs can also be applied to improve unsupervised clustering performance with our model. Furthermore we analyse the effect of this heuristic and provide an intuition of the various processes with the help of visualizations. Finally, we demonstrate the performance of our model on synthetic data, MNIST and SVHN, showing that the obtained clusters are distinct, interpretable and result in achieving competitive performance on unsupervised clustering to the state-of-the-art results.", "pdf": "/pdf/8ab4a1509cc93eedf19c7ef4f404209b0aebfe7e.pdf", "TL;DR": "We study a variant of the variational autoencoder model with a Gaussian mixture as prior distribution and discuss its optimization difficulties and capabilities for unsupervised clustering.", "paperhash": "dilokthanakul|deep_unsupervised_clustering_with_gaussian_mixture_variational_autoencoders", "conflicts": ["imperial.ac.uk"], "keywords": ["Unsupervised Learning", "Deep learning"], "authors": ["Nat Dilokthanakul", "Pedro A. M. Mediano", "Marta Garnelo", "Matthew C.H. Lee", "Hugh Salimbeni", "Kai Arulkumaran", "Murray Shanahan"], "authorids": ["n.dilokthanakul14@imperial.ac.uk", "pmediano@imperial.ac.uk", "m.garnelo-abellanas13@imperial.ac.uk", "matthew.lee13@imperial.ac.uk", "h.salimbeni15@imperial.ac.uk", "kailash.arulkumaran13@imperial.ac.uk", "m.shanahan@imperial.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287731533, "id": "ICLR.cc/2017/conference/-/paper93/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SJx7Jrtgl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper93/reviewers", "ICLR.cc/2017/conference/paper93/areachairs"], "cdate": 1485287731533}}}, {"tddate": null, "tmdate": 1479231855967, "tcdate": 1479231774465, "number": 3, "id": "H1m3Op_-x", "invitation": "ICLR.cc/2017/conference/-/paper93/public/comment", "forum": "SJx7Jrtgl", "replyto": "SJx7Jrtgl", "signatures": ["~Eric_Nalisnick1"], "readers": ["everyone"], "writers": ["~Eric_Nalisnick1"], "content": {"title": "Backprop Through Mixture Samples", "comment": "Hi, interesting work.  Just curious: did you try implementing Alex Graves' method for backprop through mixture samples (https://arxiv.org/abs/1607.05690)?  \nBest,\nEric"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Unsupervised Clustering with Gaussian Mixture Variational Autoencoders", "abstract": "We study a variant of the variational autoencoder model (VAE) with a Gaussian mixture as a prior distribution, with the goal of performing unsupervised clustering through deep generative models. We observe that the known problem of over-regularisation that has been shown to arise in regular VAEs also manifests itself in our model and leads to cluster degeneracy. We show that a heuristic called minimum information constraint that has been shown to mitigate this effect in VAEs can also be applied to improve unsupervised clustering performance with our model. Furthermore we analyse the effect of this heuristic and provide an intuition of the various processes with the help of visualizations. Finally, we demonstrate the performance of our model on synthetic data, MNIST and SVHN, showing that the obtained clusters are distinct, interpretable and result in achieving competitive performance on unsupervised clustering to the state-of-the-art results.", "pdf": "/pdf/8ab4a1509cc93eedf19c7ef4f404209b0aebfe7e.pdf", "TL;DR": "We study a variant of the variational autoencoder model with a Gaussian mixture as prior distribution and discuss its optimization difficulties and capabilities for unsupervised clustering.", "paperhash": "dilokthanakul|deep_unsupervised_clustering_with_gaussian_mixture_variational_autoencoders", "conflicts": ["imperial.ac.uk"], "keywords": ["Unsupervised Learning", "Deep learning"], "authors": ["Nat Dilokthanakul", "Pedro A. M. Mediano", "Marta Garnelo", "Matthew C.H. Lee", "Hugh Salimbeni", "Kai Arulkumaran", "Murray Shanahan"], "authorids": ["n.dilokthanakul14@imperial.ac.uk", "pmediano@imperial.ac.uk", "m.garnelo-abellanas13@imperial.ac.uk", "matthew.lee13@imperial.ac.uk", "h.salimbeni15@imperial.ac.uk", "kailash.arulkumaran13@imperial.ac.uk", "m.shanahan@imperial.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287731533, "id": "ICLR.cc/2017/conference/-/paper93/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SJx7Jrtgl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper93/reviewers", "ICLR.cc/2017/conference/paper93/areachairs"], "cdate": 1485287731533}}}, {"tddate": null, "tmdate": 1478786392935, "tcdate": 1478786392930, "number": 2, "id": "Hk-P6gGbe", "invitation": "ICLR.cc/2017/conference/-/paper93/public/comment", "forum": "SJx7Jrtgl", "replyto": "SyUPRhgWx", "signatures": ["~Nat_Dilokthanakul1"], "readers": ["everyone"], "writers": ["~Nat_Dilokthanakul1"], "content": {"title": "ELBO cost prefers seperating the arcs, as it gives tighter bound.", "comment": "Thank you for your interest! In our work the random seed effects weight initialisations. We use Glorot weight initialisation [1] - a standard heuristic - and find this can sometimes be sensitive to seed values; when this is the case the issue can be easily alleviated by using batch normalisation [2] in the recognition model. When deviating from Glorot initialisation we experimentally found that our method is more sensitive to seed values, however, this can be compensated for by batch normalisation. CV regularisation does not address the specific problem that you mention, but it is addressed by the maximisation of the ELBO, as having one cluster containing 2 arcs will have a lower ELBO.\n\n[1] Glorot, X., & Bengio, Y. (2010). Understanding the difficulty of training deep feedforward neural networks. In International Conference on Artificial Intelligence and Statistics.\n[2] Ioffe, S. and Szegedy, C., 2015. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Unsupervised Clustering with Gaussian Mixture Variational Autoencoders", "abstract": "We study a variant of the variational autoencoder model (VAE) with a Gaussian mixture as a prior distribution, with the goal of performing unsupervised clustering through deep generative models. We observe that the known problem of over-regularisation that has been shown to arise in regular VAEs also manifests itself in our model and leads to cluster degeneracy. We show that a heuristic called minimum information constraint that has been shown to mitigate this effect in VAEs can also be applied to improve unsupervised clustering performance with our model. Furthermore we analyse the effect of this heuristic and provide an intuition of the various processes with the help of visualizations. Finally, we demonstrate the performance of our model on synthetic data, MNIST and SVHN, showing that the obtained clusters are distinct, interpretable and result in achieving competitive performance on unsupervised clustering to the state-of-the-art results.", "pdf": "/pdf/8ab4a1509cc93eedf19c7ef4f404209b0aebfe7e.pdf", "TL;DR": "We study a variant of the variational autoencoder model with a Gaussian mixture as prior distribution and discuss its optimization difficulties and capabilities for unsupervised clustering.", "paperhash": "dilokthanakul|deep_unsupervised_clustering_with_gaussian_mixture_variational_autoencoders", "conflicts": ["imperial.ac.uk"], "keywords": ["Unsupervised Learning", "Deep learning"], "authors": ["Nat Dilokthanakul", "Pedro A. M. Mediano", "Marta Garnelo", "Matthew C.H. Lee", "Hugh Salimbeni", "Kai Arulkumaran", "Murray Shanahan"], "authorids": ["n.dilokthanakul14@imperial.ac.uk", "pmediano@imperial.ac.uk", "m.garnelo-abellanas13@imperial.ac.uk", "matthew.lee13@imperial.ac.uk", "h.salimbeni15@imperial.ac.uk", "kailash.arulkumaran13@imperial.ac.uk", "m.shanahan@imperial.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287731533, "id": "ICLR.cc/2017/conference/-/paper93/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SJx7Jrtgl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper93/reviewers", "ICLR.cc/2017/conference/paper93/areachairs"], "cdate": 1485287731533}}}, {"tddate": null, "tmdate": 1478704734221, "tcdate": 1478704734213, "number": 1, "id": "SyUPRhgWx", "invitation": "ICLR.cc/2017/conference/-/paper93/public/comment", "forum": "SJx7Jrtgl", "replyto": "SJx7Jrtgl", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "stability of the algorithm?", "comment": "Thank you for the interesting paper! I like the idea of adding regularization for improving the interpretability of the model. But I am a bit concerned about the stability of the algorithm. For example, in your synthetic data example (section 4.1), how sensitive is the algorithm w.r.t. the initialization or random seeds etc.? Seems to me that it is likely to learn a cluster containing, say two 2 arcs instead of one. And somehow I don't see why the CV regularization can help avoid that. Thanks a lot! (In fact I am also not sure why this is not a problem for Johnson et. al 2016. I don't see why the model would prefer separating the arcs instead of merging them as a group, maybe mostly due to the regularization caused by finite layers of neural nets?)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Unsupervised Clustering with Gaussian Mixture Variational Autoencoders", "abstract": "We study a variant of the variational autoencoder model (VAE) with a Gaussian mixture as a prior distribution, with the goal of performing unsupervised clustering through deep generative models. We observe that the known problem of over-regularisation that has been shown to arise in regular VAEs also manifests itself in our model and leads to cluster degeneracy. We show that a heuristic called minimum information constraint that has been shown to mitigate this effect in VAEs can also be applied to improve unsupervised clustering performance with our model. Furthermore we analyse the effect of this heuristic and provide an intuition of the various processes with the help of visualizations. Finally, we demonstrate the performance of our model on synthetic data, MNIST and SVHN, showing that the obtained clusters are distinct, interpretable and result in achieving competitive performance on unsupervised clustering to the state-of-the-art results.", "pdf": "/pdf/8ab4a1509cc93eedf19c7ef4f404209b0aebfe7e.pdf", "TL;DR": "We study a variant of the variational autoencoder model with a Gaussian mixture as prior distribution and discuss its optimization difficulties and capabilities for unsupervised clustering.", "paperhash": "dilokthanakul|deep_unsupervised_clustering_with_gaussian_mixture_variational_autoencoders", "conflicts": ["imperial.ac.uk"], "keywords": ["Unsupervised Learning", "Deep learning"], "authors": ["Nat Dilokthanakul", "Pedro A. M. Mediano", "Marta Garnelo", "Matthew C.H. Lee", "Hugh Salimbeni", "Kai Arulkumaran", "Murray Shanahan"], "authorids": ["n.dilokthanakul14@imperial.ac.uk", "pmediano@imperial.ac.uk", "m.garnelo-abellanas13@imperial.ac.uk", "matthew.lee13@imperial.ac.uk", "h.salimbeni15@imperial.ac.uk", "kailash.arulkumaran13@imperial.ac.uk", "m.shanahan@imperial.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287731533, "id": "ICLR.cc/2017/conference/-/paper93/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SJx7Jrtgl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper93/reviewers", "ICLR.cc/2017/conference/paper93/areachairs"], "cdate": 1485287731533}}}], "count": 18}