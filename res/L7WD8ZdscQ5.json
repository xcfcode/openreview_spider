{"notes": [{"id": "L7WD8ZdscQ5", "original": "5AcpKg-spIB", "number": 1139, "cdate": 1601308128047, "ddate": null, "tcdate": 1601308128047, "tmdate": 1615968227790, "tddate": null, "forum": "L7WD8ZdscQ5", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "The Role of Momentum Parameters in the Optimal Convergence of Adaptive Polyak's Heavy-ball Methods", "authorids": ["~Wei_Tao3", "ls15186322349@163.com", "gaowei.wu@ia.ac.cn", "qing.tao@ia.ac.cn"], "authors": ["Wei Tao", "Sheng Long", "Gaowei Wu", "Qing Tao"], "keywords": ["Deep learning", "convex optimization", "momentum methods", "adaptive heavy-ball methods", "optimal convergence"], "abstract": "The adaptive stochastic gradient descent (SGD) with momentum has been widely adopted in deep learning as well as convex optimization. In practice, the last iterate is commonly used as the final solution. However, the available regret analysis and the setting of constant momentum parameters only guarantee the optimal convergence of the averaged solution. In this paper, we fill this theory-practice gap by investigating the convergence of the last iterate (referred to as {\\it individual convergence}), which is a more difficult task than convergence analysis of the averaged solution. Specifically, in the constrained convex cases, we prove that the adaptive Polyak's Heavy-ball (HB) method, in which the step size is only updated using the exponential moving average strategy, attains an individual convergence rate of $O(\\frac{1}{\\sqrt{t}})$, as opposed to that of $O(\\frac{\\log t}{\\sqrt {t}})$ of SGD, where $t$ is the number of iterations. Our new analysis not only shows how the HB momentum and its time-varying weight help us to achieve the acceleration in convex optimization but also gives valuable hints how the momentum parameters should be scheduled in deep learning. Empirical results validate the correctness of our convergence analysis in optimizing convex functions and demonstrate the improved performance of the adaptive HB methods in training deep networks.", "one-sentence_summary": "A theory-practice gap in convex optimization and deep learning is bridged by giving a novel convergence analysis of the last Iterate of adaptive Heavy-ball methods.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "tao|the_role_of_momentum_parameters_in_the_optimal_convergence_of_adaptive_polyaks_heavyball_methods", "supplementary_material": "/attachment/d6b8197575da8df8b53edccb93c71a89d4c0b9b0.zip", "pdf": "/pdf/88887f8c02852ef3a9f82b1ae8a010345d221175.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ntao2021the,\ntitle={The Role of Momentum Parameters in the Optimal Convergence of Adaptive Polyak's Heavy-ball Methods},\nauthor={Wei Tao and Sheng Long and Gaowei Wu and Qing Tao},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=L7WD8ZdscQ5}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "OCGeLE5ZE68", "original": null, "number": 1, "cdate": 1610040475303, "ddate": null, "tcdate": 1610040475303, "tmdate": 1610474079719, "tddate": null, "forum": "L7WD8ZdscQ5", "replyto": "L7WD8ZdscQ5", "invitation": "ICLR.cc/2021/Conference/Paper1139/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Poster)", "comment": "This paper studies the *last iterate* convergence of the projected Heavy-ball method (and an adaptive variant) for convex problems, and propose a specific coefficient schedule. All reviewers thought that looking at the last-iterate convergence of the HB method was interesting and that the proofs, while simple, were interestingly novel. Several concerns were raised by the quality of the writing. Several were addressed in a revision and the rebuttal. While R1 did not update their score, the AC thinks that the rebuttal has addressed appropriately their initial concerns. The AC recommends the paper for acceptance, *but* it is important that the authors make an appropriate careful pass over their paper for the camera ready version.\n\n### comments about the write-up\n\n- The paper still contains many typos (e.g. missing $1/t$ term in the average after equation (2); many misspelled words, etc.), please carefully proofread your paper again.\n- The AC agrees with R1 that the quality of presentation still needs improvement. $\\beta_{1t}$ is still used in the introduction without being defined -- please define it properly first e.g. \n- The word \"optimal\" and \"optimality\" is usually misused in the manuscript. To refer to the convergence rate of an optimization algorithm, the standard terminology is to talk about the \"suboptimality\" or the \"error\" (e.g. see the terminology used by the cited [Harvey et al. 2019, Jain et al. 2019] papers). For example, one would say that the error or suboptimality of SGD has a $O(1/\\sqrt{t})$ convergence rate. Saying \"optimality of\" or \"optimal individual convergence rate\" is quite confusing, and should be corrected. The adjective \"optimal\" (when talking about a convergence rate) should be restricted to when a matching lower bound exists.\n- Finally, the text introducing the experimental section should be fixed to clarify the actual results and motivation. Specifically, the \"validate the correctness of our convergence analysis\" only applies in the convex setting. I recommend that a high level description of the convex experiment and the main message of the results is moved from the appendix to the main paper there (there is space). And then, the deep learning experiments can be introduced as just investigating the practical performance of the suggested coefficient schedule for HB."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Role of Momentum Parameters in the Optimal Convergence of Adaptive Polyak's Heavy-ball Methods", "authorids": ["~Wei_Tao3", "ls15186322349@163.com", "gaowei.wu@ia.ac.cn", "qing.tao@ia.ac.cn"], "authors": ["Wei Tao", "Sheng Long", "Gaowei Wu", "Qing Tao"], "keywords": ["Deep learning", "convex optimization", "momentum methods", "adaptive heavy-ball methods", "optimal convergence"], "abstract": "The adaptive stochastic gradient descent (SGD) with momentum has been widely adopted in deep learning as well as convex optimization. In practice, the last iterate is commonly used as the final solution. However, the available regret analysis and the setting of constant momentum parameters only guarantee the optimal convergence of the averaged solution. In this paper, we fill this theory-practice gap by investigating the convergence of the last iterate (referred to as {\\it individual convergence}), which is a more difficult task than convergence analysis of the averaged solution. Specifically, in the constrained convex cases, we prove that the adaptive Polyak's Heavy-ball (HB) method, in which the step size is only updated using the exponential moving average strategy, attains an individual convergence rate of $O(\\frac{1}{\\sqrt{t}})$, as opposed to that of $O(\\frac{\\log t}{\\sqrt {t}})$ of SGD, where $t$ is the number of iterations. Our new analysis not only shows how the HB momentum and its time-varying weight help us to achieve the acceleration in convex optimization but also gives valuable hints how the momentum parameters should be scheduled in deep learning. Empirical results validate the correctness of our convergence analysis in optimizing convex functions and demonstrate the improved performance of the adaptive HB methods in training deep networks.", "one-sentence_summary": "A theory-practice gap in convex optimization and deep learning is bridged by giving a novel convergence analysis of the last Iterate of adaptive Heavy-ball methods.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "tao|the_role_of_momentum_parameters_in_the_optimal_convergence_of_adaptive_polyaks_heavyball_methods", "supplementary_material": "/attachment/d6b8197575da8df8b53edccb93c71a89d4c0b9b0.zip", "pdf": "/pdf/88887f8c02852ef3a9f82b1ae8a010345d221175.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ntao2021the,\ntitle={The Role of Momentum Parameters in the Optimal Convergence of Adaptive Polyak's Heavy-ball Methods},\nauthor={Wei Tao and Sheng Long and Gaowei Wu and Qing Tao},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=L7WD8ZdscQ5}\n}"}, "tags": [], "invitation": {"reply": {"forum": "L7WD8ZdscQ5", "replyto": "L7WD8ZdscQ5", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040475290, "tmdate": 1610474079704, "id": "ICLR.cc/2021/Conference/Paper1139/-/Decision"}}}, {"id": "dIYYegEuAi", "original": null, "number": 3, "cdate": 1603909130330, "ddate": null, "tcdate": 1603909130330, "tmdate": 1606246841145, "tddate": null, "forum": "L7WD8ZdscQ5", "replyto": "L7WD8ZdscQ5", "invitation": "ICLR.cc/2021/Conference/Paper1139/-/Official_Review", "content": {"title": "An interesting read that still requires a bit of work", "review": "The authors investigate the convergence of the projected Heavy-ball method (and an adaptive variant) for convex problems with convex constraints. The authors prove 4 results: 2 individual (last iterate) convergence rates and 2 rates using averaging. Notably, in their proofs they require an increasing (from 1/2 to 1) momentum parameter and a decreasing stepsize. Finally, the authors present some experimental results.\n\nThe paper is overall a nice and pleasant read, with no major typos and a nice introduction. Also, to the best of my knowledge, the individual convergence rates proved by the authors are novel.\nHowever, I do not think the paper is ready for publication, for the following reasons (in order of importance, the last points are easy-to-fix)\n\n1) The paper is short \u2013 the main section is only 2 pages and the proof (in the appendix) does not exceed one page. I am not particularly blown away by the results or the proof technique \u2013 I think the authors should try to extend their study. For instance, an easy way to make the result more general is to extend it to momentum parameters of the form t/(t+r) for r>2 (it should work the same).\n\n2) Reading the paper, it seems the authors are the first to provide an individual convergence rate for HB. This is not true, indeed Ghadimi 2015 also has a rate of O(1/t) for the last iterate of HB under momentum t/(t+2)  and stepsize 1/t. Here, the authors add a projection, hence have to also reduce the stepsize to 1/t^3/2.. @authors is this the only novelty? Also, an increasing momentum is considered in Orvieto et al. 2019 (Role of memory in stochastic optimization).\n\n3) Even though the authors claim to have convex empirical results in the abstract, the experiments are for non convex problems (a CNN model). This in my view does not make much sense since (a) the authors only study convex problems with convex constraints (where are the constraints here?), (b) no experiment has a decreasing stepsize (c) a momentum of k/(k+2) was shown already in Sutskever et al. 2013 not to be optimal in the non convex case. In essence, I think the experiments do not back up the theory: the authors should at least compare with a fixed-momentum method.\n\n4) I am seriously not convinced that O(1/t) compared to O(log(t)/t ) motivates the so-called \u201cacceleration\u201d of Heavy ball. I think the authors should rephrase that. Btw @authors can you actually see this log difference in experiments on convex objectives? Also, it would be great to cite and compare your result/proof technique with Defossez et al. 2019 (On the convergence of Adam and Adagrad).\n\n5) Many results are presented without proof, such as the ones for the stochastic case.\n\nI think the authors should take some time to work on their interesting direction and resubmit to ICML. I think the results are interesting, just they need to be discussed/complemented/verified a bit better.\n\n------------------------------------------\n\nScore updated after rebuttal, please see comment below", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1139/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1139/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Role of Momentum Parameters in the Optimal Convergence of Adaptive Polyak's Heavy-ball Methods", "authorids": ["~Wei_Tao3", "ls15186322349@163.com", "gaowei.wu@ia.ac.cn", "qing.tao@ia.ac.cn"], "authors": ["Wei Tao", "Sheng Long", "Gaowei Wu", "Qing Tao"], "keywords": ["Deep learning", "convex optimization", "momentum methods", "adaptive heavy-ball methods", "optimal convergence"], "abstract": "The adaptive stochastic gradient descent (SGD) with momentum has been widely adopted in deep learning as well as convex optimization. In practice, the last iterate is commonly used as the final solution. However, the available regret analysis and the setting of constant momentum parameters only guarantee the optimal convergence of the averaged solution. In this paper, we fill this theory-practice gap by investigating the convergence of the last iterate (referred to as {\\it individual convergence}), which is a more difficult task than convergence analysis of the averaged solution. Specifically, in the constrained convex cases, we prove that the adaptive Polyak's Heavy-ball (HB) method, in which the step size is only updated using the exponential moving average strategy, attains an individual convergence rate of $O(\\frac{1}{\\sqrt{t}})$, as opposed to that of $O(\\frac{\\log t}{\\sqrt {t}})$ of SGD, where $t$ is the number of iterations. Our new analysis not only shows how the HB momentum and its time-varying weight help us to achieve the acceleration in convex optimization but also gives valuable hints how the momentum parameters should be scheduled in deep learning. Empirical results validate the correctness of our convergence analysis in optimizing convex functions and demonstrate the improved performance of the adaptive HB methods in training deep networks.", "one-sentence_summary": "A theory-practice gap in convex optimization and deep learning is bridged by giving a novel convergence analysis of the last Iterate of adaptive Heavy-ball methods.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "tao|the_role_of_momentum_parameters_in_the_optimal_convergence_of_adaptive_polyaks_heavyball_methods", "supplementary_material": "/attachment/d6b8197575da8df8b53edccb93c71a89d4c0b9b0.zip", "pdf": "/pdf/88887f8c02852ef3a9f82b1ae8a010345d221175.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ntao2021the,\ntitle={The Role of Momentum Parameters in the Optimal Convergence of Adaptive Polyak's Heavy-ball Methods},\nauthor={Wei Tao and Sheng Long and Gaowei Wu and Qing Tao},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=L7WD8ZdscQ5}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "L7WD8ZdscQ5", "replyto": "L7WD8ZdscQ5", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1139/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538125898, "tmdate": 1606915793452, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1139/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1139/-/Official_Review"}}}, {"id": "RQ1qRKHr2m", "original": null, "number": 11, "cdate": 1606246778409, "ddate": null, "tcdate": 1606246778409, "tmdate": 1606246778409, "tddate": null, "forum": "L7WD8ZdscQ5", "replyto": "dMuWteVPYZB", "invitation": "ICLR.cc/2021/Conference/Paper1139/-/Official_Comment", "content": {"title": "Nice effort, and nice reply", "comment": "Dear Authors, thanks a lot for your reply. \n\nI will raise the score to a weak accept because I saw some effort in improving the paper quality (such as adding the Harvey experiment). The paper changed a bit from the original one, and I think it's way more mature now. \n\nJust one thing: I still think going from log(t)/t to 1/t (asymptotically) does not \"fill the theory-practice gap\" (said in the abstract). This is misleading for a potential reader, please consider removing it: the paper is interesting but I am again not convinced the analysis and the results are strong enough for such a claim. This also because momentum-based methods are also very competitive in the smooth case compared to SGD.\n\nAll in all, I think the results can be enough for a decent publication in optimization, but the abstract should be modified a bit to match the impact of the results.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1139/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1139/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Role of Momentum Parameters in the Optimal Convergence of Adaptive Polyak's Heavy-ball Methods", "authorids": ["~Wei_Tao3", "ls15186322349@163.com", "gaowei.wu@ia.ac.cn", "qing.tao@ia.ac.cn"], "authors": ["Wei Tao", "Sheng Long", "Gaowei Wu", "Qing Tao"], "keywords": ["Deep learning", "convex optimization", "momentum methods", "adaptive heavy-ball methods", "optimal convergence"], "abstract": "The adaptive stochastic gradient descent (SGD) with momentum has been widely adopted in deep learning as well as convex optimization. In practice, the last iterate is commonly used as the final solution. However, the available regret analysis and the setting of constant momentum parameters only guarantee the optimal convergence of the averaged solution. In this paper, we fill this theory-practice gap by investigating the convergence of the last iterate (referred to as {\\it individual convergence}), which is a more difficult task than convergence analysis of the averaged solution. Specifically, in the constrained convex cases, we prove that the adaptive Polyak's Heavy-ball (HB) method, in which the step size is only updated using the exponential moving average strategy, attains an individual convergence rate of $O(\\frac{1}{\\sqrt{t}})$, as opposed to that of $O(\\frac{\\log t}{\\sqrt {t}})$ of SGD, where $t$ is the number of iterations. Our new analysis not only shows how the HB momentum and its time-varying weight help us to achieve the acceleration in convex optimization but also gives valuable hints how the momentum parameters should be scheduled in deep learning. Empirical results validate the correctness of our convergence analysis in optimizing convex functions and demonstrate the improved performance of the adaptive HB methods in training deep networks.", "one-sentence_summary": "A theory-practice gap in convex optimization and deep learning is bridged by giving a novel convergence analysis of the last Iterate of adaptive Heavy-ball methods.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "tao|the_role_of_momentum_parameters_in_the_optimal_convergence_of_adaptive_polyaks_heavyball_methods", "supplementary_material": "/attachment/d6b8197575da8df8b53edccb93c71a89d4c0b9b0.zip", "pdf": "/pdf/88887f8c02852ef3a9f82b1ae8a010345d221175.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ntao2021the,\ntitle={The Role of Momentum Parameters in the Optimal Convergence of Adaptive Polyak's Heavy-ball Methods},\nauthor={Wei Tao and Sheng Long and Gaowei Wu and Qing Tao},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=L7WD8ZdscQ5}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "L7WD8ZdscQ5", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1139/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1139/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1139/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1139/Authors|ICLR.cc/2021/Conference/Paper1139/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1139/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923863199, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1139/-/Official_Comment"}}}, {"id": "ygRDPbItk5o", "original": null, "number": 10, "cdate": 1605600865041, "ddate": null, "tcdate": 1605600865041, "tmdate": 1605673850208, "tddate": null, "forum": "L7WD8ZdscQ5", "replyto": "L7WD8ZdscQ5", "invitation": "ICLR.cc/2021/Conference/Paper1139/-/Official_Comment", "content": {"title": "Paper Revision", "comment": "Dear reviewers, \n\nWe would like to thank you for all your comments and suggestions. We have uploaded a revised version. The main changes are summarized as follows,\n\n1. A toy example was added in the supplementary material to illustrate the acceleration of HB and AdaHB. \n\n2. The original first appendix has been deleted and its content was moved into Section 3.\n\nWe will update this paper again in a few days with further revisions (e.g. add more details about the individual convergence)."}, "signatures": ["ICLR.cc/2021/Conference/Paper1139/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1139/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Role of Momentum Parameters in the Optimal Convergence of Adaptive Polyak's Heavy-ball Methods", "authorids": ["~Wei_Tao3", "ls15186322349@163.com", "gaowei.wu@ia.ac.cn", "qing.tao@ia.ac.cn"], "authors": ["Wei Tao", "Sheng Long", "Gaowei Wu", "Qing Tao"], "keywords": ["Deep learning", "convex optimization", "momentum methods", "adaptive heavy-ball methods", "optimal convergence"], "abstract": "The adaptive stochastic gradient descent (SGD) with momentum has been widely adopted in deep learning as well as convex optimization. In practice, the last iterate is commonly used as the final solution. However, the available regret analysis and the setting of constant momentum parameters only guarantee the optimal convergence of the averaged solution. In this paper, we fill this theory-practice gap by investigating the convergence of the last iterate (referred to as {\\it individual convergence}), which is a more difficult task than convergence analysis of the averaged solution. Specifically, in the constrained convex cases, we prove that the adaptive Polyak's Heavy-ball (HB) method, in which the step size is only updated using the exponential moving average strategy, attains an individual convergence rate of $O(\\frac{1}{\\sqrt{t}})$, as opposed to that of $O(\\frac{\\log t}{\\sqrt {t}})$ of SGD, where $t$ is the number of iterations. Our new analysis not only shows how the HB momentum and its time-varying weight help us to achieve the acceleration in convex optimization but also gives valuable hints how the momentum parameters should be scheduled in deep learning. Empirical results validate the correctness of our convergence analysis in optimizing convex functions and demonstrate the improved performance of the adaptive HB methods in training deep networks.", "one-sentence_summary": "A theory-practice gap in convex optimization and deep learning is bridged by giving a novel convergence analysis of the last Iterate of adaptive Heavy-ball methods.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "tao|the_role_of_momentum_parameters_in_the_optimal_convergence_of_adaptive_polyaks_heavyball_methods", "supplementary_material": "/attachment/d6b8197575da8df8b53edccb93c71a89d4c0b9b0.zip", "pdf": "/pdf/88887f8c02852ef3a9f82b1ae8a010345d221175.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ntao2021the,\ntitle={The Role of Momentum Parameters in the Optimal Convergence of Adaptive Polyak's Heavy-ball Methods},\nauthor={Wei Tao and Sheng Long and Gaowei Wu and Qing Tao},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=L7WD8ZdscQ5}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "L7WD8ZdscQ5", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1139/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1139/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1139/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1139/Authors|ICLR.cc/2021/Conference/Paper1139/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1139/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923863199, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1139/-/Official_Comment"}}}, {"id": "R8eXAJr0cpp", "original": null, "number": 6, "cdate": 1605597508127, "ddate": null, "tcdate": 1605597508127, "tmdate": 1605668074100, "tddate": null, "forum": "L7WD8ZdscQ5", "replyto": "XWUCRxcKD60", "invitation": "ICLR.cc/2021/Conference/Paper1139/-/Official_Comment", "content": {"title": "Response to R1", "comment": "Thank you for your efforts reviewing our paper and providing helpful comments and suggestions. We are looking forward to hearing your response.\n\nA1:\n\nThanks for your suggestions. We will do our best to revise the manuscript to make it ready to be published. Specifically, we have used momentum parameter to take the place of its specific value.\n\na)  We have given the full name of SGD.\n\nb) Sorry for the typos, and we have corrected them in the revised version.\n\nc) The content of the first appendix has been moved into Section 3, i.e., the simple but key proof in this paper is included in Section 3, and other details are given in the supplementary material.\n\nd) They restrict the convex problems to be smooth while we focus on the nonsmooth convex problems. What is more, we derive the optimal convergence. Of course, we would like to cite these works in the revised version. \n\n\nA2:\n\na) A concrete example was constructed in (Harvey et al., 2019), in which the error of the final iterate of deterministic GD is exactly $\\Omega (log(T)/T)$. Thus, the effect of acceleration can be clearly illustrated on this specific example. We have included this toy example in the revised experimental section.\n\nWe don\u2019t agree that log(t) term appears due to some technical difficulties in the analysis. In fact, it has been proved that the log(t) term is indeed necessary in the convergence rate of GD (Harvey et al., 2019). Thus, how to drop the log(t) term remains a challenging problem in machine learning community.\n\nb) Thanks for suggestions. To make the assumptions relatively simple and our proof easy to understand, we only focus on the deterministic cases. The general stochastic and adaptive settings have been provided in the original supplementary material.   \n\nc) Our deep learning experiments are conducted by following a recent ICLR paper (Wang et al., 2020), where only CIFAR10, CIFAR100, MNIST are considered to demonstrate the effectiveness of their methods. Of course, we also would like to see the results on bigger dataset. Unfortunately, our poor hardware conditions prevent us to finish this work in a short time. \n\nWe respectfully disagree with the reviewer about the convergence of SGD. In fact, for SGD, even if we suitably choose the time-varying step size $\\alpha_t$, the convergence of SGD is still not guaranteed due to the nonconvexity in this situation. \n\nDecreasing $\\alpha_t$ for SGD with an appropriate rate is surely a good idea. Unfortunately, it is almost impossible to get this appropriate rate in deep learning tasks. As far as we know, constant $\\alpha$ for SGD has been used in almost the deep learning experiments such as (Wang et al., 2020). We have also added the error bar to our graphs."}, "signatures": ["ICLR.cc/2021/Conference/Paper1139/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1139/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Role of Momentum Parameters in the Optimal Convergence of Adaptive Polyak's Heavy-ball Methods", "authorids": ["~Wei_Tao3", "ls15186322349@163.com", "gaowei.wu@ia.ac.cn", "qing.tao@ia.ac.cn"], "authors": ["Wei Tao", "Sheng Long", "Gaowei Wu", "Qing Tao"], "keywords": ["Deep learning", "convex optimization", "momentum methods", "adaptive heavy-ball methods", "optimal convergence"], "abstract": "The adaptive stochastic gradient descent (SGD) with momentum has been widely adopted in deep learning as well as convex optimization. In practice, the last iterate is commonly used as the final solution. However, the available regret analysis and the setting of constant momentum parameters only guarantee the optimal convergence of the averaged solution. In this paper, we fill this theory-practice gap by investigating the convergence of the last iterate (referred to as {\\it individual convergence}), which is a more difficult task than convergence analysis of the averaged solution. Specifically, in the constrained convex cases, we prove that the adaptive Polyak's Heavy-ball (HB) method, in which the step size is only updated using the exponential moving average strategy, attains an individual convergence rate of $O(\\frac{1}{\\sqrt{t}})$, as opposed to that of $O(\\frac{\\log t}{\\sqrt {t}})$ of SGD, where $t$ is the number of iterations. Our new analysis not only shows how the HB momentum and its time-varying weight help us to achieve the acceleration in convex optimization but also gives valuable hints how the momentum parameters should be scheduled in deep learning. Empirical results validate the correctness of our convergence analysis in optimizing convex functions and demonstrate the improved performance of the adaptive HB methods in training deep networks.", "one-sentence_summary": "A theory-practice gap in convex optimization and deep learning is bridged by giving a novel convergence analysis of the last Iterate of adaptive Heavy-ball methods.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "tao|the_role_of_momentum_parameters_in_the_optimal_convergence_of_adaptive_polyaks_heavyball_methods", "supplementary_material": "/attachment/d6b8197575da8df8b53edccb93c71a89d4c0b9b0.zip", "pdf": "/pdf/88887f8c02852ef3a9f82b1ae8a010345d221175.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ntao2021the,\ntitle={The Role of Momentum Parameters in the Optimal Convergence of Adaptive Polyak's Heavy-ball Methods},\nauthor={Wei Tao and Sheng Long and Gaowei Wu and Qing Tao},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=L7WD8ZdscQ5}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "L7WD8ZdscQ5", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1139/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1139/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1139/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1139/Authors|ICLR.cc/2021/Conference/Paper1139/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1139/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923863199, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1139/-/Official_Comment"}}}, {"id": "ZHSMs3fVufn", "original": null, "number": 8, "cdate": 1605599457012, "ddate": null, "tcdate": 1605599457012, "tmdate": 1605615987343, "tddate": null, "forum": "L7WD8ZdscQ5", "replyto": "x2cHXCgddCd", "invitation": "ICLR.cc/2021/Conference/Paper1139/-/Official_Comment", "content": {"title": "Response to R4", "comment": "Thank you for your efforts reviewing our paper and providing helpful comments and suggestions.\n\nCons:\n\nA1: Like many papers, we only focus on the theoretical analysis for convex problems. \n\nBTW: (1) Almost all the state-of-art optimizers including SGD and Adam also suffer from this problem. (2) Even if the nonconvex problems are considered, their convergence analysis inevitably needs some assumptions that actually does not hold in deep learning.\n\nA2: GD is a specific example of HB with $\\beta_{1t}=0$, and it has been proved that GD cannot attain the optimal individual convergence rate of O(1/t) (Harvey et al., 2019). Based on this, we said that the constant momentum cannot guarantee the optimal individual convergence. The statement has been rephrased as \u201cTo guarantee the optimal individual convergence, Theorem 5 suggests that time-varying $\\beta_{1t}$ can be adopted.\u201d\n\nIn fact, establishing averaging convergence for constant momentum is easier than analyzing the individual convergence due to no needing to select the time-varying parameters.\n\nA3: Following many papers, such as (Wang et al., 2020), we only focus on the theoretical analysis for convex problems. Indeed, the large gap exists for almost all the optimizers in deep learning. So far, there have been some papers considering nonconvex problems. However, their analysis still needs many additional assumptions that actually does not hold in deep learning.\n\nThe theoretical results about general stochastic and adaptive settings have been provided in the original supplementary material.\n\nIn experiments, we set $\\beta_{1t}=\\frac{t}{t+2}$ in Algorithm 1, which is motivated by the convergence analysis for convex problems and different from the existing methods. \n\nMinor Comments:\n\nA1: It has been included in the original supplementary material. We have also updated our paper with a revision to extend the Appendix.\n\nA2: Thanks for your advice. To help the reader understand the proof, only concise part is given in the paper. The complicated settings are provided in the supplementary material."}, "signatures": ["ICLR.cc/2021/Conference/Paper1139/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1139/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Role of Momentum Parameters in the Optimal Convergence of Adaptive Polyak's Heavy-ball Methods", "authorids": ["~Wei_Tao3", "ls15186322349@163.com", "gaowei.wu@ia.ac.cn", "qing.tao@ia.ac.cn"], "authors": ["Wei Tao", "Sheng Long", "Gaowei Wu", "Qing Tao"], "keywords": ["Deep learning", "convex optimization", "momentum methods", "adaptive heavy-ball methods", "optimal convergence"], "abstract": "The adaptive stochastic gradient descent (SGD) with momentum has been widely adopted in deep learning as well as convex optimization. In practice, the last iterate is commonly used as the final solution. However, the available regret analysis and the setting of constant momentum parameters only guarantee the optimal convergence of the averaged solution. In this paper, we fill this theory-practice gap by investigating the convergence of the last iterate (referred to as {\\it individual convergence}), which is a more difficult task than convergence analysis of the averaged solution. Specifically, in the constrained convex cases, we prove that the adaptive Polyak's Heavy-ball (HB) method, in which the step size is only updated using the exponential moving average strategy, attains an individual convergence rate of $O(\\frac{1}{\\sqrt{t}})$, as opposed to that of $O(\\frac{\\log t}{\\sqrt {t}})$ of SGD, where $t$ is the number of iterations. Our new analysis not only shows how the HB momentum and its time-varying weight help us to achieve the acceleration in convex optimization but also gives valuable hints how the momentum parameters should be scheduled in deep learning. Empirical results validate the correctness of our convergence analysis in optimizing convex functions and demonstrate the improved performance of the adaptive HB methods in training deep networks.", "one-sentence_summary": "A theory-practice gap in convex optimization and deep learning is bridged by giving a novel convergence analysis of the last Iterate of adaptive Heavy-ball methods.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "tao|the_role_of_momentum_parameters_in_the_optimal_convergence_of_adaptive_polyaks_heavyball_methods", "supplementary_material": "/attachment/d6b8197575da8df8b53edccb93c71a89d4c0b9b0.zip", "pdf": "/pdf/88887f8c02852ef3a9f82b1ae8a010345d221175.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ntao2021the,\ntitle={The Role of Momentum Parameters in the Optimal Convergence of Adaptive Polyak's Heavy-ball Methods},\nauthor={Wei Tao and Sheng Long and Gaowei Wu and Qing Tao},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=L7WD8ZdscQ5}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "L7WD8ZdscQ5", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1139/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1139/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1139/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1139/Authors|ICLR.cc/2021/Conference/Paper1139/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1139/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923863199, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1139/-/Official_Comment"}}}, {"id": "zT_dO_17uR", "original": null, "number": 7, "cdate": 1605599059398, "ddate": null, "tcdate": 1605599059398, "tmdate": 1605615924749, "tddate": null, "forum": "L7WD8ZdscQ5", "replyto": "0E4RZoVI_cl", "invitation": "ICLR.cc/2021/Conference/Paper1139/-/Official_Comment", "content": {"title": "Response to R2", "comment": "Thank you for your efforts reviewing our paper and providing helpful comments and suggestions.\n\nPros:\n\nA: Thanks for your positive evaluation.\n\nCons: \n\nA: The main novelty of our individual convergence analysis is that we employ the momentum term to establish a recursion between $f(w_t) - f(w)$ and $f(w_{t-1}) - f(w)$, in which the time-varying momentum parameters are suitably selected. The existing proof mainly concerned about the bound of regret, in which only $f(w_t) - f(w)$ is required to consider.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1139/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1139/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Role of Momentum Parameters in the Optimal Convergence of Adaptive Polyak's Heavy-ball Methods", "authorids": ["~Wei_Tao3", "ls15186322349@163.com", "gaowei.wu@ia.ac.cn", "qing.tao@ia.ac.cn"], "authors": ["Wei Tao", "Sheng Long", "Gaowei Wu", "Qing Tao"], "keywords": ["Deep learning", "convex optimization", "momentum methods", "adaptive heavy-ball methods", "optimal convergence"], "abstract": "The adaptive stochastic gradient descent (SGD) with momentum has been widely adopted in deep learning as well as convex optimization. In practice, the last iterate is commonly used as the final solution. However, the available regret analysis and the setting of constant momentum parameters only guarantee the optimal convergence of the averaged solution. In this paper, we fill this theory-practice gap by investigating the convergence of the last iterate (referred to as {\\it individual convergence}), which is a more difficult task than convergence analysis of the averaged solution. Specifically, in the constrained convex cases, we prove that the adaptive Polyak's Heavy-ball (HB) method, in which the step size is only updated using the exponential moving average strategy, attains an individual convergence rate of $O(\\frac{1}{\\sqrt{t}})$, as opposed to that of $O(\\frac{\\log t}{\\sqrt {t}})$ of SGD, where $t$ is the number of iterations. Our new analysis not only shows how the HB momentum and its time-varying weight help us to achieve the acceleration in convex optimization but also gives valuable hints how the momentum parameters should be scheduled in deep learning. Empirical results validate the correctness of our convergence analysis in optimizing convex functions and demonstrate the improved performance of the adaptive HB methods in training deep networks.", "one-sentence_summary": "A theory-practice gap in convex optimization and deep learning is bridged by giving a novel convergence analysis of the last Iterate of adaptive Heavy-ball methods.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "tao|the_role_of_momentum_parameters_in_the_optimal_convergence_of_adaptive_polyaks_heavyball_methods", "supplementary_material": "/attachment/d6b8197575da8df8b53edccb93c71a89d4c0b9b0.zip", "pdf": "/pdf/88887f8c02852ef3a9f82b1ae8a010345d221175.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ntao2021the,\ntitle={The Role of Momentum Parameters in the Optimal Convergence of Adaptive Polyak's Heavy-ball Methods},\nauthor={Wei Tao and Sheng Long and Gaowei Wu and Qing Tao},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=L7WD8ZdscQ5}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "L7WD8ZdscQ5", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1139/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1139/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1139/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1139/Authors|ICLR.cc/2021/Conference/Paper1139/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1139/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923863199, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1139/-/Official_Comment"}}}, {"id": "dMuWteVPYZB", "original": null, "number": 9, "cdate": 1605600223902, "ddate": null, "tcdate": 1605600223902, "tmdate": 1605603529390, "tddate": null, "forum": "L7WD8ZdscQ5", "replyto": "dIYYegEuAi", "invitation": "ICLR.cc/2021/Conference/Paper1139/-/Official_Comment", "content": {"title": "Response to R3", "comment": "Thank you for your efforts reviewing our paper and providing helpful comments and suggestions.\n\nA1: To make the assumptions relatively simple and our proof easy to understand, only the concise proof is given in the paper, and other details have already been provided in the original supplementary material.\n\nThanks for your advice. The momentum parameters can be extended to take the form t/(t+r) for r>2. However, the parameter $\\alpha_t$ should be changed accordingly and the derived convergence bound will depend on the parameter r.\n\nA2: (Ghadimi 2015) only consider smooth convex problems and their O(1/t) convergence rate is obviously not optimal (the optimal rate is O(1/t^2) ). As far as we know, we are the first to provide an individual convergence rate for HB which is optimal for nonsmooth convex problems.  \n\nFrom the formulation (7) in the paper, our actual stepsize is still $1/\\sqrt t$. Similar to PSG, the projection operation does not affect the stepsize at all.\n\nIn nonsmooth convex cases, even for the basic GD, its optimal individual convergence was posed as an open problem on COLT2012 and this open problem remains unsolved until 2019. In our manuscript, we in fact solve the optimal individual convergence of HB. Although there have been some papers discussing the convergence of HB in nonsmooth convex cases, to the best of our knowledge, only averaging convergence is concerned. We believe that it is novel and theoretically shows the power of HB.\n\nWe would like to cite (Orvieto et al. 2019) in the revised version. Indeed, there are some other papers considering the increasing momentum strategy. However, as far as know, the relationship between the increasing momentum strategy and optimal individual convergence is not concerned so far.\n\nA3: Yes. We indeed give convergence results in convex cases but conduct the experiments for nonconvex problems. We don\u2019t agree that this does not make sense. In fact, almost all the state-of-art optimizers including the well-known SGD and Adam also suffer from this problem. However, it does not prevent the convex optimization scheme from motivating great empirical successes in deep learning tasks. On the other hand, although the nonconvex problems are recently considered by many papers, their convergence analysis still needs many additional assumptions (such as smoothness) that actually does not hold in deep learning (such as using the activation function ReLU).\n\n(a) We have already considered the hinge loss optimization problem with $l_1$-ball constraints in the original experimental section, which was included the supplementary material. \n\n(b) We follow (Mukkamala & Hein, 2017) and (Wang et al., 2020) to conduct the deep learning experiments. Indeed, decreasing stepsize are also not used there. \n\n(c) In the nonconvex case, there is generally no way to get the expected optimality. In fact, we only prove that a momentum of k/(k+2) is optimal in terms of the individual convergence for nonsmooth convex problems. Similar to SGD, we only provide some motivating hints how the momentum parameters can be scheduled in deep learning. We have compared with a fixed-momentum method, which is denoted as SGD-momentum in our experiment.\n\nA4: Different people have different viewpoints on the understanding of acceleration. From the perspective of theoretical analysis, O(1/t) can naturally be regarded as an acceleration over O(log(t)/t). When comparing with different algorithms with the same order of convergence rate, even when a factor is decreased, it is sometimes called an acceleration in the optimization community.\n\nA concrete example was constructed in (Harvey et al., 2019), where the error of the final iterate of deterministic GD is exactly $\\Omega (log(T)/T)$. Thus, we can clearly see this log difference in experiments on this specific example. We have included this example in the experimental section.\n\nWe would like to cite (Defossez et al. 2019) in the revised version. It should be indicated they limit the considered nonconvex problem to be smooth while we focus on the nonsmooth convex problem. BTW: for the nonconvex problem, even if the objective function is smooth, we still don\u2019t know how to describe the individual convergence by using the norm of gradient oracle until now. \n\nA5: To make the assumptions relatively simple and our proof easy to understand, we only focus on the deterministic case. At the end of the original introduction, it has already been indicated that the general stochastic and adaptive settings are provided in the supplementary material.\n\nFinally, thanks for your positive evaluation about our work. The only thing we can do now is to try our best to make our manuscript satisfy the level of a ICLR paper, and we are looking forward to your further comments."}, "signatures": ["ICLR.cc/2021/Conference/Paper1139/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1139/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Role of Momentum Parameters in the Optimal Convergence of Adaptive Polyak's Heavy-ball Methods", "authorids": ["~Wei_Tao3", "ls15186322349@163.com", "gaowei.wu@ia.ac.cn", "qing.tao@ia.ac.cn"], "authors": ["Wei Tao", "Sheng Long", "Gaowei Wu", "Qing Tao"], "keywords": ["Deep learning", "convex optimization", "momentum methods", "adaptive heavy-ball methods", "optimal convergence"], "abstract": "The adaptive stochastic gradient descent (SGD) with momentum has been widely adopted in deep learning as well as convex optimization. In practice, the last iterate is commonly used as the final solution. However, the available regret analysis and the setting of constant momentum parameters only guarantee the optimal convergence of the averaged solution. In this paper, we fill this theory-practice gap by investigating the convergence of the last iterate (referred to as {\\it individual convergence}), which is a more difficult task than convergence analysis of the averaged solution. Specifically, in the constrained convex cases, we prove that the adaptive Polyak's Heavy-ball (HB) method, in which the step size is only updated using the exponential moving average strategy, attains an individual convergence rate of $O(\\frac{1}{\\sqrt{t}})$, as opposed to that of $O(\\frac{\\log t}{\\sqrt {t}})$ of SGD, where $t$ is the number of iterations. Our new analysis not only shows how the HB momentum and its time-varying weight help us to achieve the acceleration in convex optimization but also gives valuable hints how the momentum parameters should be scheduled in deep learning. Empirical results validate the correctness of our convergence analysis in optimizing convex functions and demonstrate the improved performance of the adaptive HB methods in training deep networks.", "one-sentence_summary": "A theory-practice gap in convex optimization and deep learning is bridged by giving a novel convergence analysis of the last Iterate of adaptive Heavy-ball methods.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "tao|the_role_of_momentum_parameters_in_the_optimal_convergence_of_adaptive_polyaks_heavyball_methods", "supplementary_material": "/attachment/d6b8197575da8df8b53edccb93c71a89d4c0b9b0.zip", "pdf": "/pdf/88887f8c02852ef3a9f82b1ae8a010345d221175.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ntao2021the,\ntitle={The Role of Momentum Parameters in the Optimal Convergence of Adaptive Polyak's Heavy-ball Methods},\nauthor={Wei Tao and Sheng Long and Gaowei Wu and Qing Tao},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=L7WD8ZdscQ5}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "L7WD8ZdscQ5", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1139/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1139/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1139/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1139/Authors|ICLR.cc/2021/Conference/Paper1139/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1139/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923863199, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1139/-/Official_Comment"}}}, {"id": "x2cHXCgddCd", "original": null, "number": 2, "cdate": 1603894551723, "ddate": null, "tcdate": 1603894551723, "tmdate": 1605024520763, "tddate": null, "forum": "L7WD8ZdscQ5", "replyto": "L7WD8ZdscQ5", "invitation": "ICLR.cc/2021/Conference/Paper1139/-/Official_Review", "content": {"title": "This work study the the individual convergence of Polyak momentum method in solving non-convex problems.", "review": "Summary:\n\nThe convergence of Momentum methods has been widely studied but most of existing works consider the average convergence. In this work the author considers the individual convergence of the last iteration. The accelerated convergence rate O(1/sqrt(t)) is established.\n\nPros:\n1. The individual convergence of momentum methods is pretty interesting. It is very valuable to established the accelerated convergence rate, which is missing in the existing litterature.\n\n2. The paper overall is well written but need to reword some statement for rigorousness.\n\nCons:\n1. This paper only consider convex optimization. This limits its effectiveness to explain the good performance of momentum method in training deep neural networks.\n\n2. I suggest the authors to reword their statement of the third contribution on page 3 \"However, in order to get the optimal individual convergence, \u03b21t has to be time-varying\". Theorem 4 only shows that adaptive momentum can achieve individual convergence. Theorem 5 shows that constant momentum can achieve average convergence. These two results cannot lead to the conclusion that constant moment does not have individual convergence. In fact, we have observed it in practice. Moreover, the author should add more discussion on the difficulty of establish constant convergence for constant momentum\n\n3. There is a large gap between the numerical and theoretical results. In the theoretical result, the authors consider convex optimization which is quite different from training deep neural networks. Moreover, when training DNN, the algorithm used is SGD with momentum but not GD with momentum. Note that M-SGD and M-GD has significantly difference, I am not sure how the theoretical results can provide useful hints to practice.\n\nMinor Comments:\n1. The abstract mentions that the numerical result on convex optimization is included. However, I don't find this result. Please add it.\n2. I hope the authors can provide more proof sketch to help the reader understand the proof.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1139/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1139/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Role of Momentum Parameters in the Optimal Convergence of Adaptive Polyak's Heavy-ball Methods", "authorids": ["~Wei_Tao3", "ls15186322349@163.com", "gaowei.wu@ia.ac.cn", "qing.tao@ia.ac.cn"], "authors": ["Wei Tao", "Sheng Long", "Gaowei Wu", "Qing Tao"], "keywords": ["Deep learning", "convex optimization", "momentum methods", "adaptive heavy-ball methods", "optimal convergence"], "abstract": "The adaptive stochastic gradient descent (SGD) with momentum has been widely adopted in deep learning as well as convex optimization. In practice, the last iterate is commonly used as the final solution. However, the available regret analysis and the setting of constant momentum parameters only guarantee the optimal convergence of the averaged solution. In this paper, we fill this theory-practice gap by investigating the convergence of the last iterate (referred to as {\\it individual convergence}), which is a more difficult task than convergence analysis of the averaged solution. Specifically, in the constrained convex cases, we prove that the adaptive Polyak's Heavy-ball (HB) method, in which the step size is only updated using the exponential moving average strategy, attains an individual convergence rate of $O(\\frac{1}{\\sqrt{t}})$, as opposed to that of $O(\\frac{\\log t}{\\sqrt {t}})$ of SGD, where $t$ is the number of iterations. Our new analysis not only shows how the HB momentum and its time-varying weight help us to achieve the acceleration in convex optimization but also gives valuable hints how the momentum parameters should be scheduled in deep learning. Empirical results validate the correctness of our convergence analysis in optimizing convex functions and demonstrate the improved performance of the adaptive HB methods in training deep networks.", "one-sentence_summary": "A theory-practice gap in convex optimization and deep learning is bridged by giving a novel convergence analysis of the last Iterate of adaptive Heavy-ball methods.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "tao|the_role_of_momentum_parameters_in_the_optimal_convergence_of_adaptive_polyaks_heavyball_methods", "supplementary_material": "/attachment/d6b8197575da8df8b53edccb93c71a89d4c0b9b0.zip", "pdf": "/pdf/88887f8c02852ef3a9f82b1ae8a010345d221175.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ntao2021the,\ntitle={The Role of Momentum Parameters in the Optimal Convergence of Adaptive Polyak's Heavy-ball Methods},\nauthor={Wei Tao and Sheng Long and Gaowei Wu and Qing Tao},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=L7WD8ZdscQ5}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "L7WD8ZdscQ5", "replyto": "L7WD8ZdscQ5", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1139/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538125898, "tmdate": 1606915793452, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1139/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1139/-/Official_Review"}}}, {"id": "XWUCRxcKD60", "original": null, "number": 4, "cdate": 1603920552194, "ddate": null, "tcdate": 1603920552194, "tmdate": 1605024520636, "tddate": null, "forum": "L7WD8ZdscQ5", "replyto": "L7WD8ZdscQ5", "invitation": "ICLR.cc/2021/Conference/Paper1139/-/Official_Review", "content": {"title": "The paper needs major rewriting. ", "review": "This paper's major contribution is analyzing the HB method for non-smooth objectives functions and showing the last iterate convergence for it. Comparing to existing results, they show a tighter upper bound by dropping a log(t) term from the\nsuboptimality's upper bound. It also analysis an adaptive variant of the HB and show similar results. All the proofs are clear and straightforward.\n\nComments: \n\n1- This paper needs major rewriting and restructuring and in its current format is not ready to be published. \nThe abstract talks about a specific parameter and its value, which the reader has no idea about it. \n\n a)- When an acronym is used, its full name should appear once before.\n\n b)- Typos exist in their proof. For example, in the proof of Lemma 1, and in the last equation, there is no u, but the line above reads for all u in Q. Also, in the paragraph for the paper's 3rd contribution, it says decaying \\beta_1,t goes to 1. It is not a decaying but increasing parameter. \n\nc)- There are two appendices, one at the end of the main part, and the other part is in a separate file. \n\nd)- Two related works which haven\u2019t been considered in related work \n      Sebbouh O., et.al. On the convergence of the Stochastic Heavy Ball Method\n      Sun, T. et. la., Non-Ergodic Convergence Analysis of Heavy-Ball Algorithms\n\n2- In terms of contributions: \n\na)- I don\u2019t think dropping a log(t) terms in an analysis of a method means acceleration. Usually, Log(t) term appears due to some technical difficulties in the analysis and also in practice when training a model is not a large value. \n\nb)- Since you compare the HB method with SGD, I suggest putting the proof for stochastic HB instead of the deterministic case. \n\nc)- In the experimental section, it would be nice to see the results on bigger datasets like Imagenet and also different tasks such as NLP models. Moreover, for SGD you set the step size \\alpha_t to be fixed, which theoretically SGD won\u2019t converge in this situation. To be fair, similar to your sep-size, which is decreasing \\alpha_t for SGD should be decreasing with an appropriate rate. Finally, since you run the experiments for 5 runs, it would be useful to add the error bar to your graphs. ", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1139/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1139/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Role of Momentum Parameters in the Optimal Convergence of Adaptive Polyak's Heavy-ball Methods", "authorids": ["~Wei_Tao3", "ls15186322349@163.com", "gaowei.wu@ia.ac.cn", "qing.tao@ia.ac.cn"], "authors": ["Wei Tao", "Sheng Long", "Gaowei Wu", "Qing Tao"], "keywords": ["Deep learning", "convex optimization", "momentum methods", "adaptive heavy-ball methods", "optimal convergence"], "abstract": "The adaptive stochastic gradient descent (SGD) with momentum has been widely adopted in deep learning as well as convex optimization. In practice, the last iterate is commonly used as the final solution. However, the available regret analysis and the setting of constant momentum parameters only guarantee the optimal convergence of the averaged solution. In this paper, we fill this theory-practice gap by investigating the convergence of the last iterate (referred to as {\\it individual convergence}), which is a more difficult task than convergence analysis of the averaged solution. Specifically, in the constrained convex cases, we prove that the adaptive Polyak's Heavy-ball (HB) method, in which the step size is only updated using the exponential moving average strategy, attains an individual convergence rate of $O(\\frac{1}{\\sqrt{t}})$, as opposed to that of $O(\\frac{\\log t}{\\sqrt {t}})$ of SGD, where $t$ is the number of iterations. Our new analysis not only shows how the HB momentum and its time-varying weight help us to achieve the acceleration in convex optimization but also gives valuable hints how the momentum parameters should be scheduled in deep learning. Empirical results validate the correctness of our convergence analysis in optimizing convex functions and demonstrate the improved performance of the adaptive HB methods in training deep networks.", "one-sentence_summary": "A theory-practice gap in convex optimization and deep learning is bridged by giving a novel convergence analysis of the last Iterate of adaptive Heavy-ball methods.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "tao|the_role_of_momentum_parameters_in_the_optimal_convergence_of_adaptive_polyaks_heavyball_methods", "supplementary_material": "/attachment/d6b8197575da8df8b53edccb93c71a89d4c0b9b0.zip", "pdf": "/pdf/88887f8c02852ef3a9f82b1ae8a010345d221175.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ntao2021the,\ntitle={The Role of Momentum Parameters in the Optimal Convergence of Adaptive Polyak's Heavy-ball Methods},\nauthor={Wei Tao and Sheng Long and Gaowei Wu and Qing Tao},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=L7WD8ZdscQ5}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "L7WD8ZdscQ5", "replyto": "L7WD8ZdscQ5", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1139/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538125898, "tmdate": 1606915793452, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1139/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1139/-/Official_Review"}}}, {"id": "0E4RZoVI_cl", "original": null, "number": 1, "cdate": 1603809049431, "ddate": null, "tcdate": 1603809049431, "tmdate": 1605024520572, "tddate": null, "forum": "L7WD8ZdscQ5", "replyto": "L7WD8ZdscQ5", "invitation": "ICLR.cc/2021/Conference/Paper1139/-/Official_Review", "content": {"title": "Review", "review": "Summary:\n\nThis paper provides the convergence analysis of the Heavy-ball method with the individual convergence and provides the convergence of its adaptive version. \n\nPros: \n\n1. The motivation is interesting, most of time we use the last iteration while most of theorem can only provide convergence result for the average output. I am glad to see that the theoretical proof of the individual convergence for stochastic Heavy-ball method is studied in this paper,  \n\n2. The convergence result of HB methods achieved in this paper is 1/\\sqrt(t) which is better the optimal result of SGD. This demonstrate the advantage of momentum-based methods.\n\n3. Their proof is different form all the existing analysis of averaging convergence.\n\nCons:\n\n1. It will be much better if the author can explain more on the difference between the current proof and the existing proof. \n\n\nOverall, I tend to accept this paper. However I am not an expert on this area and I will not be sad if this paper is rejected by others.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1139/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1139/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Role of Momentum Parameters in the Optimal Convergence of Adaptive Polyak's Heavy-ball Methods", "authorids": ["~Wei_Tao3", "ls15186322349@163.com", "gaowei.wu@ia.ac.cn", "qing.tao@ia.ac.cn"], "authors": ["Wei Tao", "Sheng Long", "Gaowei Wu", "Qing Tao"], "keywords": ["Deep learning", "convex optimization", "momentum methods", "adaptive heavy-ball methods", "optimal convergence"], "abstract": "The adaptive stochastic gradient descent (SGD) with momentum has been widely adopted in deep learning as well as convex optimization. In practice, the last iterate is commonly used as the final solution. However, the available regret analysis and the setting of constant momentum parameters only guarantee the optimal convergence of the averaged solution. In this paper, we fill this theory-practice gap by investigating the convergence of the last iterate (referred to as {\\it individual convergence}), which is a more difficult task than convergence analysis of the averaged solution. Specifically, in the constrained convex cases, we prove that the adaptive Polyak's Heavy-ball (HB) method, in which the step size is only updated using the exponential moving average strategy, attains an individual convergence rate of $O(\\frac{1}{\\sqrt{t}})$, as opposed to that of $O(\\frac{\\log t}{\\sqrt {t}})$ of SGD, where $t$ is the number of iterations. Our new analysis not only shows how the HB momentum and its time-varying weight help us to achieve the acceleration in convex optimization but also gives valuable hints how the momentum parameters should be scheduled in deep learning. Empirical results validate the correctness of our convergence analysis in optimizing convex functions and demonstrate the improved performance of the adaptive HB methods in training deep networks.", "one-sentence_summary": "A theory-practice gap in convex optimization and deep learning is bridged by giving a novel convergence analysis of the last Iterate of adaptive Heavy-ball methods.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "tao|the_role_of_momentum_parameters_in_the_optimal_convergence_of_adaptive_polyaks_heavyball_methods", "supplementary_material": "/attachment/d6b8197575da8df8b53edccb93c71a89d4c0b9b0.zip", "pdf": "/pdf/88887f8c02852ef3a9f82b1ae8a010345d221175.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ntao2021the,\ntitle={The Role of Momentum Parameters in the Optimal Convergence of Adaptive Polyak's Heavy-ball Methods},\nauthor={Wei Tao and Sheng Long and Gaowei Wu and Qing Tao},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=L7WD8ZdscQ5}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "L7WD8ZdscQ5", "replyto": "L7WD8ZdscQ5", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1139/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538125898, "tmdate": 1606915793452, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1139/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1139/-/Official_Review"}}}], "count": 12}