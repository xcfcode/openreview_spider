{"notes": [{"id": "H1gdF34FvS", "original": "H1gJo8d3Lr", "number": 84, "cdate": 1569438848068, "ddate": null, "tcdate": 1569438848068, "tmdate": 1577168229418, "tddate": null, "forum": "H1gdF34FvS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Advantage Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning", "authors": ["Xue Bin Peng", "Aviral Kumar", "Grace Zhang", "Sergey Levine"], "authorids": ["xbpeng@berkeley.edu", "aviralkumar2907@gmail.com", "grace.zhang@berkeley.edu", "svlevine@eecs.berkeley.edu"], "keywords": ["reinforcement learning", "policy search", "control"], "TL;DR": "We represent a simple off-policy reinforcement learning algorithm that uses standard supervised learning methods as subroutines.", "abstract": "In this paper, we aim to develop a simple and scalable reinforcement learning algorithm that uses standard supervised learning methods as subroutines. Our goal is an algorithm that utilizes only simple and convergent maximum likelihood loss functions, while also being able to leverage off-policy data. Our proposed approach, which we refer to as advantage-weighted regression (AWR), consists of two standard supervised learning steps: one to regress onto target values for a value function, and another to regress onto weighted target actions for the policy. The method is simple and general, can accommodate continuous and discrete actions, and can be implemented in just a few lines of code on top of standard supervised learning methods. We provide a theoretical motivation for AWR and analyze its properties when incorporating off-policy data from experience replay. We evaluate AWR on a suite of standard OpenAI Gym benchmark tasks, and show that it achieves competitive performance compared to a number of well-established state-of-the-art RL algorithms. AWR is also able to acquire more effective policies than most off-policy algorithms when learning from purely static datasets with no additional environmental interactions. Furthermore, we demonstrate our algorithm on challenging continuous control tasks with highly complex simulated characters.", "pdf": "/pdf/eed265837f73af8aab0ac60b47f1622bbdfc7d95.pdf", "code": "https://sites.google.com/view/awr-supp", "paperhash": "peng|advantage_weighted_regression_simple_and_scalable_offpolicy_reinforcement_learning", "original_pdf": "/attachment/15f6cbf979852a7aa318898dac52647ab0ee5939.pdf", "_bibtex": "@misc{\npeng2020advantage,\ntitle={Advantage Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning},\nauthor={Xue Bin Peng and Aviral Kumar and Grace Zhang and Sergey Levine},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gdF34FvS}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 42, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "MyMfDOV9eq", "original": null, "number": 1, "cdate": 1576798687014, "ddate": null, "tcdate": 1576798687014, "tmdate": 1576800948059, "tddate": null, "forum": "H1gdF34FvS", "replyto": "H1gdF34FvS", "invitation": "ICLR.cc/2020/Conference/Paper84/-/Decision", "content": {"decision": "Reject", "comment": "This paper caused a lot of discussions before and after the rebuttal. The concerns are related to the novelty of this paper, which seems to be relatively limited. Since we do not have a champion among positive reviewers, and the overall score is not high enough, I cannot recommend its acceptance at this stage.\n", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Advantage Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning", "authors": ["Xue Bin Peng", "Aviral Kumar", "Grace Zhang", "Sergey Levine"], "authorids": ["xbpeng@berkeley.edu", "aviralkumar2907@gmail.com", "grace.zhang@berkeley.edu", "svlevine@eecs.berkeley.edu"], "keywords": ["reinforcement learning", "policy search", "control"], "TL;DR": "We represent a simple off-policy reinforcement learning algorithm that uses standard supervised learning methods as subroutines.", "abstract": "In this paper, we aim to develop a simple and scalable reinforcement learning algorithm that uses standard supervised learning methods as subroutines. Our goal is an algorithm that utilizes only simple and convergent maximum likelihood loss functions, while also being able to leverage off-policy data. Our proposed approach, which we refer to as advantage-weighted regression (AWR), consists of two standard supervised learning steps: one to regress onto target values for a value function, and another to regress onto weighted target actions for the policy. The method is simple and general, can accommodate continuous and discrete actions, and can be implemented in just a few lines of code on top of standard supervised learning methods. We provide a theoretical motivation for AWR and analyze its properties when incorporating off-policy data from experience replay. We evaluate AWR on a suite of standard OpenAI Gym benchmark tasks, and show that it achieves competitive performance compared to a number of well-established state-of-the-art RL algorithms. AWR is also able to acquire more effective policies than most off-policy algorithms when learning from purely static datasets with no additional environmental interactions. Furthermore, we demonstrate our algorithm on challenging continuous control tasks with highly complex simulated characters.", "pdf": "/pdf/eed265837f73af8aab0ac60b47f1622bbdfc7d95.pdf", "code": "https://sites.google.com/view/awr-supp", "paperhash": "peng|advantage_weighted_regression_simple_and_scalable_offpolicy_reinforcement_learning", "original_pdf": "/attachment/15f6cbf979852a7aa318898dac52647ab0ee5939.pdf", "_bibtex": "@misc{\npeng2020advantage,\ntitle={Advantage Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning},\nauthor={Xue Bin Peng and Aviral Kumar and Grace Zhang and Sergey Levine},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gdF34FvS}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "H1gdF34FvS", "replyto": "H1gdF34FvS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795720692, "tmdate": 1576800271573, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper84/-/Decision"}}}, {"id": "Byl2SGsuur", "original": null, "number": 1, "cdate": 1570447939798, "ddate": null, "tcdate": 1570447939798, "tmdate": 1574251176652, "tddate": null, "forum": "H1gdF34FvS", "replyto": "H1gdF34FvS", "invitation": "ICLR.cc/2020/Conference/Paper84/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #3", "review": "[Note: I wrote this review after John Schulman's first comment, before any reply, and before Gehrard Neumann's comment]\n\nThe authors propose an actor-critic algorithm based mostly on regression. Being off-policy, the algorithm can learn from multiple policies. It can also be applied to continuous as well as to discrete actions, and it can be trained in a batch RL setting. They compare it to a set of state-of-the-art algorithms in standard openAI gym continuous action benchmarks and show competitive performance despite a much simpler implementation of the algorithm.\n\nI basically subscribe to John Schulman's comment below, both about empirical results and about citing Self-Imitation Learning, but I have a stronger point against the paper, which is insufficient positionning with respect to the relevant literature.\n\nThe paper does not cite or discuss the one below, though it looks VERY close:\n\n@inproceedings{neumann2009fitted,\n  title={Fitted Q-iteration by advantage weighted regression},\n  author={Neumann, Gerhard and Peters, Jan R},\n  booktitle={Advances in neural information processing systems},\n  pages={1177--1184},\n  year={2009}\n}\n\nThis paper also starts from RWR and performs weighted regression based on the advantage rather than on the return. So to me it is exactly the same idea, and it is mandatory that the authors clearly establish what is the novelty of their work with respect to this previous paper.\n\nLess importantly, the authors may also want to have a look at :\n\n@article{zimmer2019exploiting,\n  title={Exploiting the sign of the advantage function to learn deterministic policies in continuous domains},\n  author={Zimmer, Matthieu and Weng, Paul},\n  journal={arXiv preprint arXiv:1906.04556},\n  year={2019}\n}\n\nwhich also uses ideas along the same line.\n\nTo me, a good way to improve the novelty of this work would be to perform a detailed empirical study of the inner mechanisms of the algorithm on very simple benchmarks where the value function and policy could be visualized. In particular, how stable is the estimation of the value function? This is known to be an issue, as most algorithms avoid approximating it and prefer estimating the Q-function.\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper84/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper84/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Advantage Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning", "authors": ["Xue Bin Peng", "Aviral Kumar", "Grace Zhang", "Sergey Levine"], "authorids": ["xbpeng@berkeley.edu", "aviralkumar2907@gmail.com", "grace.zhang@berkeley.edu", "svlevine@eecs.berkeley.edu"], "keywords": ["reinforcement learning", "policy search", "control"], "TL;DR": "We represent a simple off-policy reinforcement learning algorithm that uses standard supervised learning methods as subroutines.", "abstract": "In this paper, we aim to develop a simple and scalable reinforcement learning algorithm that uses standard supervised learning methods as subroutines. Our goal is an algorithm that utilizes only simple and convergent maximum likelihood loss functions, while also being able to leverage off-policy data. Our proposed approach, which we refer to as advantage-weighted regression (AWR), consists of two standard supervised learning steps: one to regress onto target values for a value function, and another to regress onto weighted target actions for the policy. The method is simple and general, can accommodate continuous and discrete actions, and can be implemented in just a few lines of code on top of standard supervised learning methods. We provide a theoretical motivation for AWR and analyze its properties when incorporating off-policy data from experience replay. We evaluate AWR on a suite of standard OpenAI Gym benchmark tasks, and show that it achieves competitive performance compared to a number of well-established state-of-the-art RL algorithms. AWR is also able to acquire more effective policies than most off-policy algorithms when learning from purely static datasets with no additional environmental interactions. Furthermore, we demonstrate our algorithm on challenging continuous control tasks with highly complex simulated characters.", "pdf": "/pdf/eed265837f73af8aab0ac60b47f1622bbdfc7d95.pdf", "code": "https://sites.google.com/view/awr-supp", "paperhash": "peng|advantage_weighted_regression_simple_and_scalable_offpolicy_reinforcement_learning", "original_pdf": "/attachment/15f6cbf979852a7aa318898dac52647ab0ee5939.pdf", "_bibtex": "@misc{\npeng2020advantage,\ntitle={Advantage Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning},\nauthor={Xue Bin Peng and Aviral Kumar and Grace Zhang and Sergey Levine},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gdF34FvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "H1gdF34FvS", "replyto": "H1gdF34FvS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper84/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper84/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575878245175, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper84/Reviewers"], "noninvitees": [], "tcdate": 1570237757312, "tmdate": 1575878245188, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper84/-/Official_Review"}}}, {"id": "Bkx4jhqnjH", "original": null, "number": 34, "cdate": 1573854363957, "ddate": null, "tcdate": 1573854363957, "tmdate": 1573854363957, "tddate": null, "forum": "H1gdF34FvS", "replyto": "H1gdF34FvS", "invitation": "ICLR.cc/2020/Conference/Paper84/-/Official_Comment", "content": {"title": "Author Response: Update Summary", "comment": "We thank the reviewers for their constructive feedback. We have revised the paper to include additional discussion of prior work and quantitative results. We summarize the main updates below:\n\n1) comparisons with Neumann & Peters (https://imgur.com/a/YjfMQS9)\n\n2) additional discussion of prior work in Section 4, including FQI by AWR, SIL, and MARWIL\n\n3) a more detailed review RWR and EM methods in Section 2\n\n4) additional ablation experiments in Appendix E\n\n5) additional PPO experiments based on John Schulman\u2019s updated PPO implementation (https://imgur.com/a/ny2rqFd) \n\n6) discussion of differences between policy gradient and AWR update in Appendix D \n\n7) experiments with 10 random seeds instead of 5 (https://imgur.com/a/yLntOzf)\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper84/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper84/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Advantage Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning", "authors": ["Xue Bin Peng", "Aviral Kumar", "Grace Zhang", "Sergey Levine"], "authorids": ["xbpeng@berkeley.edu", "aviralkumar2907@gmail.com", "grace.zhang@berkeley.edu", "svlevine@eecs.berkeley.edu"], "keywords": ["reinforcement learning", "policy search", "control"], "TL;DR": "We represent a simple off-policy reinforcement learning algorithm that uses standard supervised learning methods as subroutines.", "abstract": "In this paper, we aim to develop a simple and scalable reinforcement learning algorithm that uses standard supervised learning methods as subroutines. Our goal is an algorithm that utilizes only simple and convergent maximum likelihood loss functions, while also being able to leverage off-policy data. Our proposed approach, which we refer to as advantage-weighted regression (AWR), consists of two standard supervised learning steps: one to regress onto target values for a value function, and another to regress onto weighted target actions for the policy. The method is simple and general, can accommodate continuous and discrete actions, and can be implemented in just a few lines of code on top of standard supervised learning methods. We provide a theoretical motivation for AWR and analyze its properties when incorporating off-policy data from experience replay. We evaluate AWR on a suite of standard OpenAI Gym benchmark tasks, and show that it achieves competitive performance compared to a number of well-established state-of-the-art RL algorithms. AWR is also able to acquire more effective policies than most off-policy algorithms when learning from purely static datasets with no additional environmental interactions. Furthermore, we demonstrate our algorithm on challenging continuous control tasks with highly complex simulated characters.", "pdf": "/pdf/eed265837f73af8aab0ac60b47f1622bbdfc7d95.pdf", "code": "https://sites.google.com/view/awr-supp", "paperhash": "peng|advantage_weighted_regression_simple_and_scalable_offpolicy_reinforcement_learning", "original_pdf": "/attachment/15f6cbf979852a7aa318898dac52647ab0ee5939.pdf", "_bibtex": "@misc{\npeng2020advantage,\ntitle={Advantage Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning},\nauthor={Xue Bin Peng and Aviral Kumar and Grace Zhang and Sergey Levine},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gdF34FvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1gdF34FvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper84/Authors", "ICLR.cc/2020/Conference/Paper84/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper84/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper84/Reviewers", "ICLR.cc/2020/Conference/Paper84/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper84/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper84/Authors|ICLR.cc/2020/Conference/Paper84/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176634, "tmdate": 1576860555463, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper84/Authors", "ICLR.cc/2020/Conference/Paper84/Reviewers", "ICLR.cc/2020/Conference/Paper84/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper84/-/Official_Comment"}}}, {"id": "SJlNOIGiir", "original": null, "number": 32, "cdate": 1573754475973, "ddate": null, "tcdate": 1573754475973, "tmdate": 1573756846082, "tddate": null, "forum": "H1gdF34FvS", "replyto": "Hkxkf1xssS", "invitation": "ICLR.cc/2020/Conference/Paper84/-/Official_Comment", "content": {"title": "TD3", "comment": "We used the public implementation of TD3 available in RLKit: https://github.com/vitchyr/rlkit. There is a large variation in the performance of TD3 across different runs. As we have discussed in the other posts, methods that directly use a Q-function to update the policy are more susceptible to bias in the learned Q-function, which could be a factor in the performance of TD3 that we observe. Please note that the performance of TD3 does not collapse on Walker2D. In fact, it achieves a respectable score of 4212.\n\nSome of the TD3 runs were terminated earlier than others due to the slow wall-clock time of the TD3 code. We will update TD3 learning curves with longer runs once we have the time to train the policies for more time.\n\nWe would like to emphasize again, that the goal of our experiments is not to show whether one algorithm is necessarily better than another. It is to show that a simple off-policy RL algorithm that uses supervised learning as subroutines can be competitive with current state-of-the-art techniques."}, "signatures": ["ICLR.cc/2020/Conference/Paper84/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper84/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Advantage Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning", "authors": ["Xue Bin Peng", "Aviral Kumar", "Grace Zhang", "Sergey Levine"], "authorids": ["xbpeng@berkeley.edu", "aviralkumar2907@gmail.com", "grace.zhang@berkeley.edu", "svlevine@eecs.berkeley.edu"], "keywords": ["reinforcement learning", "policy search", "control"], "TL;DR": "We represent a simple off-policy reinforcement learning algorithm that uses standard supervised learning methods as subroutines.", "abstract": "In this paper, we aim to develop a simple and scalable reinforcement learning algorithm that uses standard supervised learning methods as subroutines. Our goal is an algorithm that utilizes only simple and convergent maximum likelihood loss functions, while also being able to leverage off-policy data. Our proposed approach, which we refer to as advantage-weighted regression (AWR), consists of two standard supervised learning steps: one to regress onto target values for a value function, and another to regress onto weighted target actions for the policy. The method is simple and general, can accommodate continuous and discrete actions, and can be implemented in just a few lines of code on top of standard supervised learning methods. We provide a theoretical motivation for AWR and analyze its properties when incorporating off-policy data from experience replay. We evaluate AWR on a suite of standard OpenAI Gym benchmark tasks, and show that it achieves competitive performance compared to a number of well-established state-of-the-art RL algorithms. AWR is also able to acquire more effective policies than most off-policy algorithms when learning from purely static datasets with no additional environmental interactions. Furthermore, we demonstrate our algorithm on challenging continuous control tasks with highly complex simulated characters.", "pdf": "/pdf/eed265837f73af8aab0ac60b47f1622bbdfc7d95.pdf", "code": "https://sites.google.com/view/awr-supp", "paperhash": "peng|advantage_weighted_regression_simple_and_scalable_offpolicy_reinforcement_learning", "original_pdf": "/attachment/15f6cbf979852a7aa318898dac52647ab0ee5939.pdf", "_bibtex": "@misc{\npeng2020advantage,\ntitle={Advantage Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning},\nauthor={Xue Bin Peng and Aviral Kumar and Grace Zhang and Sergey Levine},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gdF34FvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1gdF34FvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper84/Authors", "ICLR.cc/2020/Conference/Paper84/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper84/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper84/Reviewers", "ICLR.cc/2020/Conference/Paper84/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper84/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper84/Authors|ICLR.cc/2020/Conference/Paper84/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176634, "tmdate": 1576860555463, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper84/Authors", "ICLR.cc/2020/Conference/Paper84/Reviewers", "ICLR.cc/2020/Conference/Paper84/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper84/-/Official_Comment"}}}, {"id": "Hkxkf1xssS", "original": null, "number": 31, "cdate": 1573744391328, "ddate": null, "tcdate": 1573744391328, "tmdate": 1573744391328, "tddate": null, "forum": "H1gdF34FvS", "replyto": "r1gTvqFziS", "invitation": "ICLR.cc/2020/Conference/Paper84/-/Official_Comment", "content": {"title": "Performance of TD3 in Fig. 3", "comment": "A late comment (sorry) which is also minor: I have seen papers where TD3 reaches 12000 on HalfCheetah-v2. Any idea why yours does not perform well? Why did you stop this run earlier than the other ones? Given my doubts on Half-Cheetah, it also raises doubts about the performance of TD3 on Walker-2D, which seems to collapse."}, "signatures": ["ICLR.cc/2020/Conference/Paper84/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper84/AnonReviewer3", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Advantage Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning", "authors": ["Xue Bin Peng", "Aviral Kumar", "Grace Zhang", "Sergey Levine"], "authorids": ["xbpeng@berkeley.edu", "aviralkumar2907@gmail.com", "grace.zhang@berkeley.edu", "svlevine@eecs.berkeley.edu"], "keywords": ["reinforcement learning", "policy search", "control"], "TL;DR": "We represent a simple off-policy reinforcement learning algorithm that uses standard supervised learning methods as subroutines.", "abstract": "In this paper, we aim to develop a simple and scalable reinforcement learning algorithm that uses standard supervised learning methods as subroutines. Our goal is an algorithm that utilizes only simple and convergent maximum likelihood loss functions, while also being able to leverage off-policy data. Our proposed approach, which we refer to as advantage-weighted regression (AWR), consists of two standard supervised learning steps: one to regress onto target values for a value function, and another to regress onto weighted target actions for the policy. The method is simple and general, can accommodate continuous and discrete actions, and can be implemented in just a few lines of code on top of standard supervised learning methods. We provide a theoretical motivation for AWR and analyze its properties when incorporating off-policy data from experience replay. We evaluate AWR on a suite of standard OpenAI Gym benchmark tasks, and show that it achieves competitive performance compared to a number of well-established state-of-the-art RL algorithms. AWR is also able to acquire more effective policies than most off-policy algorithms when learning from purely static datasets with no additional environmental interactions. Furthermore, we demonstrate our algorithm on challenging continuous control tasks with highly complex simulated characters.", "pdf": "/pdf/eed265837f73af8aab0ac60b47f1622bbdfc7d95.pdf", "code": "https://sites.google.com/view/awr-supp", "paperhash": "peng|advantage_weighted_regression_simple_and_scalable_offpolicy_reinforcement_learning", "original_pdf": "/attachment/15f6cbf979852a7aa318898dac52647ab0ee5939.pdf", "_bibtex": "@misc{\npeng2020advantage,\ntitle={Advantage Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning},\nauthor={Xue Bin Peng and Aviral Kumar and Grace Zhang and Sergey Levine},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gdF34FvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1gdF34FvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper84/Authors", "ICLR.cc/2020/Conference/Paper84/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper84/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper84/Reviewers", "ICLR.cc/2020/Conference/Paper84/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper84/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper84/Authors|ICLR.cc/2020/Conference/Paper84/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176634, "tmdate": 1576860555463, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper84/Authors", "ICLR.cc/2020/Conference/Paper84/Reviewers", "ICLR.cc/2020/Conference/Paper84/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper84/-/Official_Comment"}}}, {"id": "H1e5oakisr", "original": null, "number": 30, "cdate": 1573744033745, "ddate": null, "tcdate": 1573744033745, "tmdate": 1573744033745, "tddate": null, "forum": "H1gdF34FvS", "replyto": "BklEPf_dsH", "invitation": "ICLR.cc/2020/Conference/Paper84/-/Official_Comment", "content": {"title": "OK, no mix-up", "comment": "Thank you, I'm now convinced, I have been too fast when looking at these equations."}, "signatures": ["ICLR.cc/2020/Conference/Paper84/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper84/AnonReviewer3", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Advantage Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning", "authors": ["Xue Bin Peng", "Aviral Kumar", "Grace Zhang", "Sergey Levine"], "authorids": ["xbpeng@berkeley.edu", "aviralkumar2907@gmail.com", "grace.zhang@berkeley.edu", "svlevine@eecs.berkeley.edu"], "keywords": ["reinforcement learning", "policy search", "control"], "TL;DR": "We represent a simple off-policy reinforcement learning algorithm that uses standard supervised learning methods as subroutines.", "abstract": "In this paper, we aim to develop a simple and scalable reinforcement learning algorithm that uses standard supervised learning methods as subroutines. Our goal is an algorithm that utilizes only simple and convergent maximum likelihood loss functions, while also being able to leverage off-policy data. Our proposed approach, which we refer to as advantage-weighted regression (AWR), consists of two standard supervised learning steps: one to regress onto target values for a value function, and another to regress onto weighted target actions for the policy. The method is simple and general, can accommodate continuous and discrete actions, and can be implemented in just a few lines of code on top of standard supervised learning methods. We provide a theoretical motivation for AWR and analyze its properties when incorporating off-policy data from experience replay. We evaluate AWR on a suite of standard OpenAI Gym benchmark tasks, and show that it achieves competitive performance compared to a number of well-established state-of-the-art RL algorithms. AWR is also able to acquire more effective policies than most off-policy algorithms when learning from purely static datasets with no additional environmental interactions. Furthermore, we demonstrate our algorithm on challenging continuous control tasks with highly complex simulated characters.", "pdf": "/pdf/eed265837f73af8aab0ac60b47f1622bbdfc7d95.pdf", "code": "https://sites.google.com/view/awr-supp", "paperhash": "peng|advantage_weighted_regression_simple_and_scalable_offpolicy_reinforcement_learning", "original_pdf": "/attachment/15f6cbf979852a7aa318898dac52647ab0ee5939.pdf", "_bibtex": "@misc{\npeng2020advantage,\ntitle={Advantage Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning},\nauthor={Xue Bin Peng and Aviral Kumar and Grace Zhang and Sergey Levine},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gdF34FvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1gdF34FvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper84/Authors", "ICLR.cc/2020/Conference/Paper84/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper84/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper84/Reviewers", "ICLR.cc/2020/Conference/Paper84/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper84/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper84/Authors|ICLR.cc/2020/Conference/Paper84/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176634, "tmdate": 1576860555463, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper84/Authors", "ICLR.cc/2020/Conference/Paper84/Reviewers", "ICLR.cc/2020/Conference/Paper84/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper84/-/Official_Comment"}}}, {"id": "H1l6B0MKiB", "original": null, "number": 28, "cdate": 1573625413138, "ddate": null, "tcdate": 1573625413138, "tmdate": 1573625413138, "tddate": null, "forum": "H1gdF34FvS", "replyto": "H1gdF34FvS", "invitation": "ICLR.cc/2020/Conference/Paper84/-/Official_Comment", "content": {"title": "Regarding novelty", "comment": "In regard to novelty, one thing we would like to note is that the combination of principled theoretical backing and solid empirical results that show a generally known recipe (in this case RWR), with a few careful design choices not present in prior work, can result in an effective deep RL algorithm is very much in line with widely recognized previously published papers. The same criticism in regard to novelty could have been applied to TRPO \u2014 which extended the theoretical backing of natural policy gradient and showed that it worked in the deep RL setting. Also PPO, which implementation wise is a small and simple modification on the very well known importance sampling policy gradient. The same for DDPG, which is a small but critical modification on NFQCA. By the novelty standard in the reviews, none of these prior papers \u2014 now viewed as landmark papers in deep RL \u2014 would have passed the bar. We believe that demonstrating, for the first time, that an RWR-like recipe produces a competitive deep RL algorithm with simple subroutines for value and policy fitting, together with theoretical motivation, is a contribution of similar scope in the context of prior work"}, "signatures": ["ICLR.cc/2020/Conference/Paper84/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper84/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Advantage Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning", "authors": ["Xue Bin Peng", "Aviral Kumar", "Grace Zhang", "Sergey Levine"], "authorids": ["xbpeng@berkeley.edu", "aviralkumar2907@gmail.com", "grace.zhang@berkeley.edu", "svlevine@eecs.berkeley.edu"], "keywords": ["reinforcement learning", "policy search", "control"], "TL;DR": "We represent a simple off-policy reinforcement learning algorithm that uses standard supervised learning methods as subroutines.", "abstract": "In this paper, we aim to develop a simple and scalable reinforcement learning algorithm that uses standard supervised learning methods as subroutines. Our goal is an algorithm that utilizes only simple and convergent maximum likelihood loss functions, while also being able to leverage off-policy data. Our proposed approach, which we refer to as advantage-weighted regression (AWR), consists of two standard supervised learning steps: one to regress onto target values for a value function, and another to regress onto weighted target actions for the policy. The method is simple and general, can accommodate continuous and discrete actions, and can be implemented in just a few lines of code on top of standard supervised learning methods. We provide a theoretical motivation for AWR and analyze its properties when incorporating off-policy data from experience replay. We evaluate AWR on a suite of standard OpenAI Gym benchmark tasks, and show that it achieves competitive performance compared to a number of well-established state-of-the-art RL algorithms. AWR is also able to acquire more effective policies than most off-policy algorithms when learning from purely static datasets with no additional environmental interactions. Furthermore, we demonstrate our algorithm on challenging continuous control tasks with highly complex simulated characters.", "pdf": "/pdf/eed265837f73af8aab0ac60b47f1622bbdfc7d95.pdf", "code": "https://sites.google.com/view/awr-supp", "paperhash": "peng|advantage_weighted_regression_simple_and_scalable_offpolicy_reinforcement_learning", "original_pdf": "/attachment/15f6cbf979852a7aa318898dac52647ab0ee5939.pdf", "_bibtex": "@misc{\npeng2020advantage,\ntitle={Advantage Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning},\nauthor={Xue Bin Peng and Aviral Kumar and Grace Zhang and Sergey Levine},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gdF34FvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1gdF34FvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper84/Authors", "ICLR.cc/2020/Conference/Paper84/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper84/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper84/Reviewers", "ICLR.cc/2020/Conference/Paper84/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper84/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper84/Authors|ICLR.cc/2020/Conference/Paper84/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176634, "tmdate": 1576860555463, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper84/Authors", "ICLR.cc/2020/Conference/Paper84/Reviewers", "ICLR.cc/2020/Conference/Paper84/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper84/-/Official_Comment"}}}, {"id": "BklEPf_dsH", "original": null, "number": 27, "cdate": 1573581404368, "ddate": null, "tcdate": 1573581404368, "tmdate": 1573581514331, "tddate": null, "forum": "H1gdF34FvS", "replyto": "ByeI-RU_sS", "invitation": "ICLR.cc/2020/Conference/Paper84/-/Official_Comment", "content": {"title": "The equation should be correct", "comment": "Ok, seems like we are talking about the same equation. The equation in 3.1 should be correct. We adjusted the notation to be consistent with the notation we use in this paper, but it is saying the same thing as the equation in Neumann & Peters. For your convenience, we have included a side-by-side comparison of the three equations here:\nhttps://imgur.com/a/oEAqBhP\nNote that the new equation is different from our Equation 8. It doesn't not include the likelihood of an action under \\mu. It is however semantically the same as equation 11 from Neumann & Peters, with only some notation differences. We replaced the normalization constant in the denominator with Z(s), the temperature was changed from \\tau to \\beta, and we also expanded the definition of the advantage A(s, a) = R_{s, a} - V(s)."}, "signatures": ["ICLR.cc/2020/Conference/Paper84/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper84/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Advantage Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning", "authors": ["Xue Bin Peng", "Aviral Kumar", "Grace Zhang", "Sergey Levine"], "authorids": ["xbpeng@berkeley.edu", "aviralkumar2907@gmail.com", "grace.zhang@berkeley.edu", "svlevine@eecs.berkeley.edu"], "keywords": ["reinforcement learning", "policy search", "control"], "TL;DR": "We represent a simple off-policy reinforcement learning algorithm that uses standard supervised learning methods as subroutines.", "abstract": "In this paper, we aim to develop a simple and scalable reinforcement learning algorithm that uses standard supervised learning methods as subroutines. Our goal is an algorithm that utilizes only simple and convergent maximum likelihood loss functions, while also being able to leverage off-policy data. Our proposed approach, which we refer to as advantage-weighted regression (AWR), consists of two standard supervised learning steps: one to regress onto target values for a value function, and another to regress onto weighted target actions for the policy. The method is simple and general, can accommodate continuous and discrete actions, and can be implemented in just a few lines of code on top of standard supervised learning methods. We provide a theoretical motivation for AWR and analyze its properties when incorporating off-policy data from experience replay. We evaluate AWR on a suite of standard OpenAI Gym benchmark tasks, and show that it achieves competitive performance compared to a number of well-established state-of-the-art RL algorithms. AWR is also able to acquire more effective policies than most off-policy algorithms when learning from purely static datasets with no additional environmental interactions. Furthermore, we demonstrate our algorithm on challenging continuous control tasks with highly complex simulated characters.", "pdf": "/pdf/eed265837f73af8aab0ac60b47f1622bbdfc7d95.pdf", "code": "https://sites.google.com/view/awr-supp", "paperhash": "peng|advantage_weighted_regression_simple_and_scalable_offpolicy_reinforcement_learning", "original_pdf": "/attachment/15f6cbf979852a7aa318898dac52647ab0ee5939.pdf", "_bibtex": "@misc{\npeng2020advantage,\ntitle={Advantage Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning},\nauthor={Xue Bin Peng and Aviral Kumar and Grace Zhang and Sergey Levine},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gdF34FvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1gdF34FvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper84/Authors", "ICLR.cc/2020/Conference/Paper84/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper84/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper84/Reviewers", "ICLR.cc/2020/Conference/Paper84/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper84/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper84/Authors|ICLR.cc/2020/Conference/Paper84/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176634, "tmdate": 1576860555463, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper84/Authors", "ICLR.cc/2020/Conference/Paper84/Reviewers", "ICLR.cc/2020/Conference/Paper84/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper84/-/Official_Comment"}}}, {"id": "HJgGMnwujr", "original": null, "number": 26, "cdate": 1573579786162, "ddate": null, "tcdate": 1573579786162, "tmdate": 1573579786162, "tddate": null, "forum": "H1gdF34FvS", "replyto": "ByeI-RU_sS", "invitation": "ICLR.cc/2020/Conference/Paper84/-/Official_Comment", "content": {"title": "Neumann & Peters equation", "comment": "Let me be more explicit about the mix-up.\n\nI'm refering to the sentence in red, page 4, which starts with \"A similar advantage-weighting scheme has been previously used for fitted Q-iteration(Neumann & Peters, 2009), where the policy is given by *EQ*\", where, if I understood correctly, *EQ* is supposed to be Neumann & Peters equation. But as a *EQ* I see exactly your Eq. (8) and not the Equation you were giving for Neumann & Peters equation there:\nhttps://imgur.com/a/0DuRH7o\n\nDo you agree that there is a mix-up now?"}, "signatures": ["ICLR.cc/2020/Conference/Paper84/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper84/AnonReviewer3", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Advantage Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning", "authors": ["Xue Bin Peng", "Aviral Kumar", "Grace Zhang", "Sergey Levine"], "authorids": ["xbpeng@berkeley.edu", "aviralkumar2907@gmail.com", "grace.zhang@berkeley.edu", "svlevine@eecs.berkeley.edu"], "keywords": ["reinforcement learning", "policy search", "control"], "TL;DR": "We represent a simple off-policy reinforcement learning algorithm that uses standard supervised learning methods as subroutines.", "abstract": "In this paper, we aim to develop a simple and scalable reinforcement learning algorithm that uses standard supervised learning methods as subroutines. Our goal is an algorithm that utilizes only simple and convergent maximum likelihood loss functions, while also being able to leverage off-policy data. Our proposed approach, which we refer to as advantage-weighted regression (AWR), consists of two standard supervised learning steps: one to regress onto target values for a value function, and another to regress onto weighted target actions for the policy. The method is simple and general, can accommodate continuous and discrete actions, and can be implemented in just a few lines of code on top of standard supervised learning methods. We provide a theoretical motivation for AWR and analyze its properties when incorporating off-policy data from experience replay. We evaluate AWR on a suite of standard OpenAI Gym benchmark tasks, and show that it achieves competitive performance compared to a number of well-established state-of-the-art RL algorithms. AWR is also able to acquire more effective policies than most off-policy algorithms when learning from purely static datasets with no additional environmental interactions. Furthermore, we demonstrate our algorithm on challenging continuous control tasks with highly complex simulated characters.", "pdf": "/pdf/eed265837f73af8aab0ac60b47f1622bbdfc7d95.pdf", "code": "https://sites.google.com/view/awr-supp", "paperhash": "peng|advantage_weighted_regression_simple_and_scalable_offpolicy_reinforcement_learning", "original_pdf": "/attachment/15f6cbf979852a7aa318898dac52647ab0ee5939.pdf", "_bibtex": "@misc{\npeng2020advantage,\ntitle={Advantage Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning},\nauthor={Xue Bin Peng and Aviral Kumar and Grace Zhang and Sergey Levine},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gdF34FvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1gdF34FvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper84/Authors", "ICLR.cc/2020/Conference/Paper84/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper84/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper84/Reviewers", "ICLR.cc/2020/Conference/Paper84/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper84/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper84/Authors|ICLR.cc/2020/Conference/Paper84/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176634, "tmdate": 1576860555463, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper84/Authors", "ICLR.cc/2020/Conference/Paper84/Reviewers", "ICLR.cc/2020/Conference/Paper84/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper84/-/Official_Comment"}}}, {"id": "ByeI-RU_sS", "original": null, "number": 25, "cdate": 1573576189727, "ddate": null, "tcdate": 1573576189727, "tmdate": 1573576189727, "tddate": null, "forum": "H1gdF34FvS", "replyto": "S1xbcZkOsr", "invitation": "ICLR.cc/2020/Conference/Paper84/-/Official_Comment", "content": {"title": "Neumann & Peters equation", "comment": "The equation in 3.1 for the definition of the policy from Neumann & Peters should be correct. Notice that the likelihood of an action depends only on its advantage and the likelihood under the sampling distribution is not present. Are you referring to some other mix-up?"}, "signatures": ["ICLR.cc/2020/Conference/Paper84/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper84/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Advantage Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning", "authors": ["Xue Bin Peng", "Aviral Kumar", "Grace Zhang", "Sergey Levine"], "authorids": ["xbpeng@berkeley.edu", "aviralkumar2907@gmail.com", "grace.zhang@berkeley.edu", "svlevine@eecs.berkeley.edu"], "keywords": ["reinforcement learning", "policy search", "control"], "TL;DR": "We represent a simple off-policy reinforcement learning algorithm that uses standard supervised learning methods as subroutines.", "abstract": "In this paper, we aim to develop a simple and scalable reinforcement learning algorithm that uses standard supervised learning methods as subroutines. Our goal is an algorithm that utilizes only simple and convergent maximum likelihood loss functions, while also being able to leverage off-policy data. Our proposed approach, which we refer to as advantage-weighted regression (AWR), consists of two standard supervised learning steps: one to regress onto target values for a value function, and another to regress onto weighted target actions for the policy. The method is simple and general, can accommodate continuous and discrete actions, and can be implemented in just a few lines of code on top of standard supervised learning methods. We provide a theoretical motivation for AWR and analyze its properties when incorporating off-policy data from experience replay. We evaluate AWR on a suite of standard OpenAI Gym benchmark tasks, and show that it achieves competitive performance compared to a number of well-established state-of-the-art RL algorithms. AWR is also able to acquire more effective policies than most off-policy algorithms when learning from purely static datasets with no additional environmental interactions. Furthermore, we demonstrate our algorithm on challenging continuous control tasks with highly complex simulated characters.", "pdf": "/pdf/eed265837f73af8aab0ac60b47f1622bbdfc7d95.pdf", "code": "https://sites.google.com/view/awr-supp", "paperhash": "peng|advantage_weighted_regression_simple_and_scalable_offpolicy_reinforcement_learning", "original_pdf": "/attachment/15f6cbf979852a7aa318898dac52647ab0ee5939.pdf", "_bibtex": "@misc{\npeng2020advantage,\ntitle={Advantage Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning},\nauthor={Xue Bin Peng and Aviral Kumar and Grace Zhang and Sergey Levine},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gdF34FvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1gdF34FvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper84/Authors", "ICLR.cc/2020/Conference/Paper84/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper84/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper84/Reviewers", "ICLR.cc/2020/Conference/Paper84/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper84/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper84/Authors|ICLR.cc/2020/Conference/Paper84/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176634, "tmdate": 1576860555463, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper84/Authors", "ICLR.cc/2020/Conference/Paper84/Reviewers", "ICLR.cc/2020/Conference/Paper84/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper84/-/Official_Comment"}}}, {"id": "S1xbcZkOsr", "original": null, "number": 24, "cdate": 1573544328640, "ddate": null, "tcdate": 1573544328640, "tmdate": 1573544328640, "tddate": null, "forum": "H1gdF34FvS", "replyto": "HygmpBpPoB", "invitation": "ICLR.cc/2020/Conference/Paper84/-/Official_Comment", "content": {"title": "Equations mixed up", "comment": "In the addition to p. 4, unless I got it wrong you repeated your \\pi equation instead of stating Neumann's.\n\nAsking me to update my review is the AC's job, not yours. ;)\nI'll do so after the discussion with the other reviewers, but I can already say that I appreciated your efforts in clarifying your paper."}, "signatures": ["ICLR.cc/2020/Conference/Paper84/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper84/AnonReviewer3", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Advantage Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning", "authors": ["Xue Bin Peng", "Aviral Kumar", "Grace Zhang", "Sergey Levine"], "authorids": ["xbpeng@berkeley.edu", "aviralkumar2907@gmail.com", "grace.zhang@berkeley.edu", "svlevine@eecs.berkeley.edu"], "keywords": ["reinforcement learning", "policy search", "control"], "TL;DR": "We represent a simple off-policy reinforcement learning algorithm that uses standard supervised learning methods as subroutines.", "abstract": "In this paper, we aim to develop a simple and scalable reinforcement learning algorithm that uses standard supervised learning methods as subroutines. Our goal is an algorithm that utilizes only simple and convergent maximum likelihood loss functions, while also being able to leverage off-policy data. Our proposed approach, which we refer to as advantage-weighted regression (AWR), consists of two standard supervised learning steps: one to regress onto target values for a value function, and another to regress onto weighted target actions for the policy. The method is simple and general, can accommodate continuous and discrete actions, and can be implemented in just a few lines of code on top of standard supervised learning methods. We provide a theoretical motivation for AWR and analyze its properties when incorporating off-policy data from experience replay. We evaluate AWR on a suite of standard OpenAI Gym benchmark tasks, and show that it achieves competitive performance compared to a number of well-established state-of-the-art RL algorithms. AWR is also able to acquire more effective policies than most off-policy algorithms when learning from purely static datasets with no additional environmental interactions. Furthermore, we demonstrate our algorithm on challenging continuous control tasks with highly complex simulated characters.", "pdf": "/pdf/eed265837f73af8aab0ac60b47f1622bbdfc7d95.pdf", "code": "https://sites.google.com/view/awr-supp", "paperhash": "peng|advantage_weighted_regression_simple_and_scalable_offpolicy_reinforcement_learning", "original_pdf": "/attachment/15f6cbf979852a7aa318898dac52647ab0ee5939.pdf", "_bibtex": "@misc{\npeng2020advantage,\ntitle={Advantage Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning},\nauthor={Xue Bin Peng and Aviral Kumar and Grace Zhang and Sergey Levine},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gdF34FvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1gdF34FvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper84/Authors", "ICLR.cc/2020/Conference/Paper84/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper84/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper84/Reviewers", "ICLR.cc/2020/Conference/Paper84/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper84/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper84/Authors|ICLR.cc/2020/Conference/Paper84/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176634, "tmdate": 1576860555463, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper84/Authors", "ICLR.cc/2020/Conference/Paper84/Reviewers", "ICLR.cc/2020/Conference/Paper84/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper84/-/Official_Comment"}}}, {"id": "HygmpBpPoB", "original": null, "number": 23, "cdate": 1573537210966, "ddate": null, "tcdate": 1573537210966, "tmdate": 1573537210966, "tddate": null, "forum": "H1gdF34FvS", "replyto": "Byl2SGsuur", "invitation": "ICLR.cc/2020/Conference/Paper84/-/Official_Comment", "content": {"title": "Updates to the submission", "comment": "Thank you again for the detailed feedback. Here is a summary of the changes we have made to the latest draft of the submission:\n\n1) We have performed additional experiments comparing our methods to FQI-by-AWR and results are available here https://openreview.net/forum?id=H1gdF34FvS&noteId=H1xClC-Lsr. These experiments will be added to the paper once we have more time to run additional random seeds.\n\n2) A more thorough discussion of additional prior work has been added to Section 4. \n\n3) We have expanded the preliminary section (Section 2) to include a more detailed review of RWR and EM algorithms, as well as a summary of their differences as compared to policy gradient algorithms.\n\n4) We have added a discussion of the differences between our definition of the policy and the policy in FQI-by-AWR to Section 3.1.\n\n5) We have included a more in-depth discussion of the differences between the AWR update and the standard policy gradient update to the supplementary material (Section D).\n\n6) The additional ablation experiments for weight clipping that you requested for have been added to Section E.1 in the supplementary material.\n\nWe hope these changes and experiments have helped to address your concerns. When you get a chance to, please update your review to reflect these changes, and let us know if you have any additional feedback."}, "signatures": ["ICLR.cc/2020/Conference/Paper84/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper84/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Advantage Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning", "authors": ["Xue Bin Peng", "Aviral Kumar", "Grace Zhang", "Sergey Levine"], "authorids": ["xbpeng@berkeley.edu", "aviralkumar2907@gmail.com", "grace.zhang@berkeley.edu", "svlevine@eecs.berkeley.edu"], "keywords": ["reinforcement learning", "policy search", "control"], "TL;DR": "We represent a simple off-policy reinforcement learning algorithm that uses standard supervised learning methods as subroutines.", "abstract": "In this paper, we aim to develop a simple and scalable reinforcement learning algorithm that uses standard supervised learning methods as subroutines. Our goal is an algorithm that utilizes only simple and convergent maximum likelihood loss functions, while also being able to leverage off-policy data. Our proposed approach, which we refer to as advantage-weighted regression (AWR), consists of two standard supervised learning steps: one to regress onto target values for a value function, and another to regress onto weighted target actions for the policy. The method is simple and general, can accommodate continuous and discrete actions, and can be implemented in just a few lines of code on top of standard supervised learning methods. We provide a theoretical motivation for AWR and analyze its properties when incorporating off-policy data from experience replay. We evaluate AWR on a suite of standard OpenAI Gym benchmark tasks, and show that it achieves competitive performance compared to a number of well-established state-of-the-art RL algorithms. AWR is also able to acquire more effective policies than most off-policy algorithms when learning from purely static datasets with no additional environmental interactions. Furthermore, we demonstrate our algorithm on challenging continuous control tasks with highly complex simulated characters.", "pdf": "/pdf/eed265837f73af8aab0ac60b47f1622bbdfc7d95.pdf", "code": "https://sites.google.com/view/awr-supp", "paperhash": "peng|advantage_weighted_regression_simple_and_scalable_offpolicy_reinforcement_learning", "original_pdf": "/attachment/15f6cbf979852a7aa318898dac52647ab0ee5939.pdf", "_bibtex": "@misc{\npeng2020advantage,\ntitle={Advantage Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning},\nauthor={Xue Bin Peng and Aviral Kumar and Grace Zhang and Sergey Levine},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gdF34FvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1gdF34FvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper84/Authors", "ICLR.cc/2020/Conference/Paper84/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper84/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper84/Reviewers", "ICLR.cc/2020/Conference/Paper84/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper84/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper84/Authors|ICLR.cc/2020/Conference/Paper84/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176634, "tmdate": 1576860555463, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper84/Authors", "ICLR.cc/2020/Conference/Paper84/Reviewers", "ICLR.cc/2020/Conference/Paper84/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper84/-/Official_Comment"}}}, {"id": "H1xClC-Lsr", "original": null, "number": 19, "cdate": 1573424629793, "ddate": null, "tcdate": 1573424629793, "tmdate": 1573494490591, "tddate": null, "forum": "H1gdF34FvS", "replyto": "H1gdF34FvS", "invitation": "ICLR.cc/2020/Conference/Paper84/-/Official_Comment", "content": {"title": "Neumann & Peters  Comparison", "comment": "We have implemented the FQI-by-AWR algorithm as described in Algorithm 1 from Neumann & Peters (https://papers.nips.cc/paper/3501-fitted-q-iteration-by-advantage-weighted-regression.pdf), but instead of kernel-based function approximators, we used neural networks, like in our method, to ensure a fair comparison -- we believe that this likely makes the method stronger. A link to the code is available here:\nhttps://drive.google.com/file/d/1vZdYSm84tdqMy1etJ7rtPGdgho3Zn1Jq/view?usp=sharing\n\nLearning curves comparing the two methods are available here:\nhttps://imgur.com/a/YjfMQS9\nSo far, our AWR algorithm performs substantially better than FQI-by-AWR on the tasks considered, despite also been much simpler. To the best of our knowledge, we are not aware of any prior work that shows the algorithm from Neumann & Peters to be effective for deep RL on this suite of tasks. \n\nHere is a highlight of the differences between their methods and ours:\n1) FQI-by-AWR performs fitted Q-iteration by learning both a Q-function and a value function. Our method requires only learning a value function.\n\n2) FQI-by-AWR updates the policy by computing advantages using the learned Q-function and value function, whereas AWR simply uses monte-carlo returns (+ TD lambda) and a learned value function to update the policy. FQI-by-AWR\u2019s use of the Q-function for policy updates could explain some of the instability observed in our experiments, since it is more susceptible to bias in the learned Q-function. This instability is commonly observed in other Q-learning based algorithms and often requires a myriad of stabilization techniques (https://arxiv.org/pdf/1710.02298.pdf, https://arxiv.org/pdf/1802.09477.pdf), none of which are needed for AWR.\n\n3) In addition to fitting a Q-function and a value function, FQI-by-AWR also requires fitting estimators for the mean and standard deviation of the advantage at each state, which is not needed for our method.\n\n4) FQI-by-AWR fits the value function using a td update with 1-step bootstrapping, while AWR uses either simple Monte Carlo returns or an off-the-shelf TD-lambda estimator.\n\nWe will continue tuning the parameters of our implementation of FQI-by-AWR and run the algorithm with more random seeds. The paper will be updated with the additional comparisons. We emphasize that we made a best-faith effort to implement this method, and were able to get it to achieve non-trivial performance. We hope the insights in our work for producing a simple but effective off-policy RL algorithm will be of interest to the community.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper84/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper84/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Advantage Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning", "authors": ["Xue Bin Peng", "Aviral Kumar", "Grace Zhang", "Sergey Levine"], "authorids": ["xbpeng@berkeley.edu", "aviralkumar2907@gmail.com", "grace.zhang@berkeley.edu", "svlevine@eecs.berkeley.edu"], "keywords": ["reinforcement learning", "policy search", "control"], "TL;DR": "We represent a simple off-policy reinforcement learning algorithm that uses standard supervised learning methods as subroutines.", "abstract": "In this paper, we aim to develop a simple and scalable reinforcement learning algorithm that uses standard supervised learning methods as subroutines. Our goal is an algorithm that utilizes only simple and convergent maximum likelihood loss functions, while also being able to leverage off-policy data. Our proposed approach, which we refer to as advantage-weighted regression (AWR), consists of two standard supervised learning steps: one to regress onto target values for a value function, and another to regress onto weighted target actions for the policy. The method is simple and general, can accommodate continuous and discrete actions, and can be implemented in just a few lines of code on top of standard supervised learning methods. We provide a theoretical motivation for AWR and analyze its properties when incorporating off-policy data from experience replay. We evaluate AWR on a suite of standard OpenAI Gym benchmark tasks, and show that it achieves competitive performance compared to a number of well-established state-of-the-art RL algorithms. AWR is also able to acquire more effective policies than most off-policy algorithms when learning from purely static datasets with no additional environmental interactions. Furthermore, we demonstrate our algorithm on challenging continuous control tasks with highly complex simulated characters.", "pdf": "/pdf/eed265837f73af8aab0ac60b47f1622bbdfc7d95.pdf", "code": "https://sites.google.com/view/awr-supp", "paperhash": "peng|advantage_weighted_regression_simple_and_scalable_offpolicy_reinforcement_learning", "original_pdf": "/attachment/15f6cbf979852a7aa318898dac52647ab0ee5939.pdf", "_bibtex": "@misc{\npeng2020advantage,\ntitle={Advantage Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning},\nauthor={Xue Bin Peng and Aviral Kumar and Grace Zhang and Sergey Levine},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gdF34FvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1gdF34FvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper84/Authors", "ICLR.cc/2020/Conference/Paper84/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper84/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper84/Reviewers", "ICLR.cc/2020/Conference/Paper84/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper84/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper84/Authors|ICLR.cc/2020/Conference/Paper84/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176634, "tmdate": 1576860555463, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper84/Authors", "ICLR.cc/2020/Conference/Paper84/Reviewers", "ICLR.cc/2020/Conference/Paper84/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper84/-/Official_Comment"}}}, {"id": "ByxfG-fUoS", "original": null, "number": 22, "cdate": 1573425417979, "ddate": null, "tcdate": 1573425417979, "tmdate": 1573425417979, "tddate": null, "forum": "H1gdF34FvS", "replyto": "HyxTjQo6FH", "invitation": "ICLR.cc/2020/Conference/Paper84/-/Official_Comment", "content": {"title": "Neumann & Peters comparison", "comment": "We have conducted additional experiments comparing AWR to the method from Neumann & Peters. Please refer to the general post for details:\nhttps://openreview.net/forum?id=H1gdF34FvS&noteId=H1xClC-Lsr"}, "signatures": ["ICLR.cc/2020/Conference/Paper84/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper84/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Advantage Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning", "authors": ["Xue Bin Peng", "Aviral Kumar", "Grace Zhang", "Sergey Levine"], "authorids": ["xbpeng@berkeley.edu", "aviralkumar2907@gmail.com", "grace.zhang@berkeley.edu", "svlevine@eecs.berkeley.edu"], "keywords": ["reinforcement learning", "policy search", "control"], "TL;DR": "We represent a simple off-policy reinforcement learning algorithm that uses standard supervised learning methods as subroutines.", "abstract": "In this paper, we aim to develop a simple and scalable reinforcement learning algorithm that uses standard supervised learning methods as subroutines. Our goal is an algorithm that utilizes only simple and convergent maximum likelihood loss functions, while also being able to leverage off-policy data. Our proposed approach, which we refer to as advantage-weighted regression (AWR), consists of two standard supervised learning steps: one to regress onto target values for a value function, and another to regress onto weighted target actions for the policy. The method is simple and general, can accommodate continuous and discrete actions, and can be implemented in just a few lines of code on top of standard supervised learning methods. We provide a theoretical motivation for AWR and analyze its properties when incorporating off-policy data from experience replay. We evaluate AWR on a suite of standard OpenAI Gym benchmark tasks, and show that it achieves competitive performance compared to a number of well-established state-of-the-art RL algorithms. AWR is also able to acquire more effective policies than most off-policy algorithms when learning from purely static datasets with no additional environmental interactions. Furthermore, we demonstrate our algorithm on challenging continuous control tasks with highly complex simulated characters.", "pdf": "/pdf/eed265837f73af8aab0ac60b47f1622bbdfc7d95.pdf", "code": "https://sites.google.com/view/awr-supp", "paperhash": "peng|advantage_weighted_regression_simple_and_scalable_offpolicy_reinforcement_learning", "original_pdf": "/attachment/15f6cbf979852a7aa318898dac52647ab0ee5939.pdf", "_bibtex": "@misc{\npeng2020advantage,\ntitle={Advantage Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning},\nauthor={Xue Bin Peng and Aviral Kumar and Grace Zhang and Sergey Levine},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gdF34FvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1gdF34FvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper84/Authors", "ICLR.cc/2020/Conference/Paper84/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper84/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper84/Reviewers", "ICLR.cc/2020/Conference/Paper84/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper84/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper84/Authors|ICLR.cc/2020/Conference/Paper84/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176634, "tmdate": 1576860555463, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper84/Authors", "ICLR.cc/2020/Conference/Paper84/Reviewers", "ICLR.cc/2020/Conference/Paper84/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper84/-/Official_Comment"}}}, {"id": "B1ely-G8sH", "original": null, "number": 21, "cdate": 1573425367665, "ddate": null, "tcdate": 1573425367665, "tmdate": 1573425367665, "tddate": null, "forum": "H1gdF34FvS", "replyto": "rylVAPJCFH", "invitation": "ICLR.cc/2020/Conference/Paper84/-/Official_Comment", "content": {"title": "Additional experiments", "comment": "We have conducted additional experiments comparing AWR to the method from Neumann & Peters. Please refer to the general post for details:\nhttps://openreview.net/forum?id=H1gdF34FvS&noteId=H1xClC-Lsr"}, "signatures": ["ICLR.cc/2020/Conference/Paper84/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper84/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Advantage Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning", "authors": ["Xue Bin Peng", "Aviral Kumar", "Grace Zhang", "Sergey Levine"], "authorids": ["xbpeng@berkeley.edu", "aviralkumar2907@gmail.com", "grace.zhang@berkeley.edu", "svlevine@eecs.berkeley.edu"], "keywords": ["reinforcement learning", "policy search", "control"], "TL;DR": "We represent a simple off-policy reinforcement learning algorithm that uses standard supervised learning methods as subroutines.", "abstract": "In this paper, we aim to develop a simple and scalable reinforcement learning algorithm that uses standard supervised learning methods as subroutines. Our goal is an algorithm that utilizes only simple and convergent maximum likelihood loss functions, while also being able to leverage off-policy data. Our proposed approach, which we refer to as advantage-weighted regression (AWR), consists of two standard supervised learning steps: one to regress onto target values for a value function, and another to regress onto weighted target actions for the policy. The method is simple and general, can accommodate continuous and discrete actions, and can be implemented in just a few lines of code on top of standard supervised learning methods. We provide a theoretical motivation for AWR and analyze its properties when incorporating off-policy data from experience replay. We evaluate AWR on a suite of standard OpenAI Gym benchmark tasks, and show that it achieves competitive performance compared to a number of well-established state-of-the-art RL algorithms. AWR is also able to acquire more effective policies than most off-policy algorithms when learning from purely static datasets with no additional environmental interactions. Furthermore, we demonstrate our algorithm on challenging continuous control tasks with highly complex simulated characters.", "pdf": "/pdf/eed265837f73af8aab0ac60b47f1622bbdfc7d95.pdf", "code": "https://sites.google.com/view/awr-supp", "paperhash": "peng|advantage_weighted_regression_simple_and_scalable_offpolicy_reinforcement_learning", "original_pdf": "/attachment/15f6cbf979852a7aa318898dac52647ab0ee5939.pdf", "_bibtex": "@misc{\npeng2020advantage,\ntitle={Advantage Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning},\nauthor={Xue Bin Peng and Aviral Kumar and Grace Zhang and Sergey Levine},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gdF34FvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1gdF34FvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper84/Authors", "ICLR.cc/2020/Conference/Paper84/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper84/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper84/Reviewers", "ICLR.cc/2020/Conference/Paper84/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper84/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper84/Authors|ICLR.cc/2020/Conference/Paper84/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176634, "tmdate": 1576860555463, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper84/Authors", "ICLR.cc/2020/Conference/Paper84/Reviewers", "ICLR.cc/2020/Conference/Paper84/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper84/-/Official_Comment"}}}, {"id": "rJxlseGLsr", "original": null, "number": 20, "cdate": 1573425303922, "ddate": null, "tcdate": 1573425303922, "tmdate": 1573425303922, "tddate": null, "forum": "H1gdF34FvS", "replyto": "Byl2SGsuur", "invitation": "ICLR.cc/2020/Conference/Paper84/-/Official_Comment", "content": {"title": "Neumann & Peters comparison", "comment": "We have conducted additional experiments comparing AWR to the method from Neumann & Peters. Please refer to the general post for details:\nhttps://openreview.net/forum?id=H1gdF34FvS&noteId=H1xClC-Lsr"}, "signatures": ["ICLR.cc/2020/Conference/Paper84/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper84/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Advantage Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning", "authors": ["Xue Bin Peng", "Aviral Kumar", "Grace Zhang", "Sergey Levine"], "authorids": ["xbpeng@berkeley.edu", "aviralkumar2907@gmail.com", "grace.zhang@berkeley.edu", "svlevine@eecs.berkeley.edu"], "keywords": ["reinforcement learning", "policy search", "control"], "TL;DR": "We represent a simple off-policy reinforcement learning algorithm that uses standard supervised learning methods as subroutines.", "abstract": "In this paper, we aim to develop a simple and scalable reinforcement learning algorithm that uses standard supervised learning methods as subroutines. Our goal is an algorithm that utilizes only simple and convergent maximum likelihood loss functions, while also being able to leverage off-policy data. Our proposed approach, which we refer to as advantage-weighted regression (AWR), consists of two standard supervised learning steps: one to regress onto target values for a value function, and another to regress onto weighted target actions for the policy. The method is simple and general, can accommodate continuous and discrete actions, and can be implemented in just a few lines of code on top of standard supervised learning methods. We provide a theoretical motivation for AWR and analyze its properties when incorporating off-policy data from experience replay. We evaluate AWR on a suite of standard OpenAI Gym benchmark tasks, and show that it achieves competitive performance compared to a number of well-established state-of-the-art RL algorithms. AWR is also able to acquire more effective policies than most off-policy algorithms when learning from purely static datasets with no additional environmental interactions. Furthermore, we demonstrate our algorithm on challenging continuous control tasks with highly complex simulated characters.", "pdf": "/pdf/eed265837f73af8aab0ac60b47f1622bbdfc7d95.pdf", "code": "https://sites.google.com/view/awr-supp", "paperhash": "peng|advantage_weighted_regression_simple_and_scalable_offpolicy_reinforcement_learning", "original_pdf": "/attachment/15f6cbf979852a7aa318898dac52647ab0ee5939.pdf", "_bibtex": "@misc{\npeng2020advantage,\ntitle={Advantage Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning},\nauthor={Xue Bin Peng and Aviral Kumar and Grace Zhang and Sergey Levine},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gdF34FvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1gdF34FvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper84/Authors", "ICLR.cc/2020/Conference/Paper84/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper84/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper84/Reviewers", "ICLR.cc/2020/Conference/Paper84/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper84/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper84/Authors|ICLR.cc/2020/Conference/Paper84/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176634, "tmdate": 1576860555463, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper84/Authors", "ICLR.cc/2020/Conference/Paper84/Reviewers", "ICLR.cc/2020/Conference/Paper84/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper84/-/Official_Comment"}}}, {"id": "Ske7cbjNsH", "original": null, "number": 18, "cdate": 1573331339252, "ddate": null, "tcdate": 1573331339252, "tmdate": 1573331339252, "tddate": null, "forum": "H1gdF34FvS", "replyto": "rkxEYyY7sS", "invitation": "ICLR.cc/2020/Conference/Paper84/-/Official_Comment", "content": {"title": "Thank you for the suggestion", "comment": "We really appreciate the detailed feedback. We will adjust the writing to incorporate your suggestions. We will do our best to improve the clarity of our presentation and let you know once the new revision is ready."}, "signatures": ["ICLR.cc/2020/Conference/Paper84/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper84/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Advantage Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning", "authors": ["Xue Bin Peng", "Aviral Kumar", "Grace Zhang", "Sergey Levine"], "authorids": ["xbpeng@berkeley.edu", "aviralkumar2907@gmail.com", "grace.zhang@berkeley.edu", "svlevine@eecs.berkeley.edu"], "keywords": ["reinforcement learning", "policy search", "control"], "TL;DR": "We represent a simple off-policy reinforcement learning algorithm that uses standard supervised learning methods as subroutines.", "abstract": "In this paper, we aim to develop a simple and scalable reinforcement learning algorithm that uses standard supervised learning methods as subroutines. Our goal is an algorithm that utilizes only simple and convergent maximum likelihood loss functions, while also being able to leverage off-policy data. Our proposed approach, which we refer to as advantage-weighted regression (AWR), consists of two standard supervised learning steps: one to regress onto target values for a value function, and another to regress onto weighted target actions for the policy. The method is simple and general, can accommodate continuous and discrete actions, and can be implemented in just a few lines of code on top of standard supervised learning methods. We provide a theoretical motivation for AWR and analyze its properties when incorporating off-policy data from experience replay. We evaluate AWR on a suite of standard OpenAI Gym benchmark tasks, and show that it achieves competitive performance compared to a number of well-established state-of-the-art RL algorithms. AWR is also able to acquire more effective policies than most off-policy algorithms when learning from purely static datasets with no additional environmental interactions. Furthermore, we demonstrate our algorithm on challenging continuous control tasks with highly complex simulated characters.", "pdf": "/pdf/eed265837f73af8aab0ac60b47f1622bbdfc7d95.pdf", "code": "https://sites.google.com/view/awr-supp", "paperhash": "peng|advantage_weighted_regression_simple_and_scalable_offpolicy_reinforcement_learning", "original_pdf": "/attachment/15f6cbf979852a7aa318898dac52647ab0ee5939.pdf", "_bibtex": "@misc{\npeng2020advantage,\ntitle={Advantage Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning},\nauthor={Xue Bin Peng and Aviral Kumar and Grace Zhang and Sergey Levine},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gdF34FvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1gdF34FvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper84/Authors", "ICLR.cc/2020/Conference/Paper84/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper84/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper84/Reviewers", "ICLR.cc/2020/Conference/Paper84/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper84/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper84/Authors|ICLR.cc/2020/Conference/Paper84/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176634, "tmdate": 1576860555463, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper84/Authors", "ICLR.cc/2020/Conference/Paper84/Reviewers", "ICLR.cc/2020/Conference/Paper84/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper84/-/Official_Comment"}}}, {"id": "rkgHb-o4jB", "original": null, "number": 17, "cdate": 1573331197268, "ddate": null, "tcdate": 1573331197268, "tmdate": 1573331197268, "tddate": null, "forum": "H1gdF34FvS", "replyto": "SJgoLpzbsS", "invitation": "ICLR.cc/2020/Conference/Paper84/-/Official_Comment", "content": {"title": "Weight Clipping", "comment": "We are added additional ablation experiments comparing AWR with and without weight clipping. Results are available in the supplementary section D.1. To summarize, here are learning curves comparing the different policies:\nhttps://imgur.com/a/FOsC0Cg\nPolicies trained without weight clipping are substantially more unstable, exhibiting drastic fluctuations in performance as a result of exploding gradients from excessively\nlarge weights. Some training runs without clipping are terminated early due to exploding\ngradients causing the networks to output NaNs. These experiments suggest that weight clipping is vital for ensuring stable training with AWR."}, "signatures": ["ICLR.cc/2020/Conference/Paper84/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper84/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Advantage Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning", "authors": ["Xue Bin Peng", "Aviral Kumar", "Grace Zhang", "Sergey Levine"], "authorids": ["xbpeng@berkeley.edu", "aviralkumar2907@gmail.com", "grace.zhang@berkeley.edu", "svlevine@eecs.berkeley.edu"], "keywords": ["reinforcement learning", "policy search", "control"], "TL;DR": "We represent a simple off-policy reinforcement learning algorithm that uses standard supervised learning methods as subroutines.", "abstract": "In this paper, we aim to develop a simple and scalable reinforcement learning algorithm that uses standard supervised learning methods as subroutines. Our goal is an algorithm that utilizes only simple and convergent maximum likelihood loss functions, while also being able to leverage off-policy data. Our proposed approach, which we refer to as advantage-weighted regression (AWR), consists of two standard supervised learning steps: one to regress onto target values for a value function, and another to regress onto weighted target actions for the policy. The method is simple and general, can accommodate continuous and discrete actions, and can be implemented in just a few lines of code on top of standard supervised learning methods. We provide a theoretical motivation for AWR and analyze its properties when incorporating off-policy data from experience replay. We evaluate AWR on a suite of standard OpenAI Gym benchmark tasks, and show that it achieves competitive performance compared to a number of well-established state-of-the-art RL algorithms. AWR is also able to acquire more effective policies than most off-policy algorithms when learning from purely static datasets with no additional environmental interactions. Furthermore, we demonstrate our algorithm on challenging continuous control tasks with highly complex simulated characters.", "pdf": "/pdf/eed265837f73af8aab0ac60b47f1622bbdfc7d95.pdf", "code": "https://sites.google.com/view/awr-supp", "paperhash": "peng|advantage_weighted_regression_simple_and_scalable_offpolicy_reinforcement_learning", "original_pdf": "/attachment/15f6cbf979852a7aa318898dac52647ab0ee5939.pdf", "_bibtex": "@misc{\npeng2020advantage,\ntitle={Advantage Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning},\nauthor={Xue Bin Peng and Aviral Kumar and Grace Zhang and Sergey Levine},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gdF34FvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1gdF34FvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper84/Authors", "ICLR.cc/2020/Conference/Paper84/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper84/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper84/Reviewers", "ICLR.cc/2020/Conference/Paper84/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper84/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper84/Authors|ICLR.cc/2020/Conference/Paper84/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176634, "tmdate": 1576860555463, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper84/Authors", "ICLR.cc/2020/Conference/Paper84/Reviewers", "ICLR.cc/2020/Conference/Paper84/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper84/-/Official_Comment"}}}, {"id": "rylJoJQViS", "original": null, "number": 16, "cdate": 1573298071299, "ddate": null, "tcdate": 1573298071299, "tmdate": 1573298071299, "tddate": null, "forum": "H1gdF34FvS", "replyto": "rkxEYyY7sS", "invitation": "ICLR.cc/2020/Conference/Paper84/-/Official_Comment", "content": {"title": "Improving exposition", "comment": "Thanks again for the explanation of the differences to PG, it provides very useful clarifications (that would deserve to be in the paper somehow, together with the explanation of better stability than actor-critic approaches).\n\nBy the way, I were you, here is how I would improve exposition (I'm not asking, I'm suggesting).\n\nFirst, I would add a \"Background\" section where I would explain everything that is common to RWR, FQI by AWR (LAWER), MARWIL: the reward weighted approach, the EM approach, and the features of this approach (see just above).\n\nThen in the main section I would expose AWR by explaining how it is different from these related algorithms and what additional features it gains from these differences.\nFor instance, the formulas of https://imgur.com/a/0DuRH7o should be given explicitly together with what you said in the related work section about the differences between LAWER and AWR. As is, the non expert reader has to do a lot of work to understand what you say in this related work section.\nHope it helps."}, "signatures": ["ICLR.cc/2020/Conference/Paper84/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper84/AnonReviewer3", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Advantage Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning", "authors": ["Xue Bin Peng", "Aviral Kumar", "Grace Zhang", "Sergey Levine"], "authorids": ["xbpeng@berkeley.edu", "aviralkumar2907@gmail.com", "grace.zhang@berkeley.edu", "svlevine@eecs.berkeley.edu"], "keywords": ["reinforcement learning", "policy search", "control"], "TL;DR": "We represent a simple off-policy reinforcement learning algorithm that uses standard supervised learning methods as subroutines.", "abstract": "In this paper, we aim to develop a simple and scalable reinforcement learning algorithm that uses standard supervised learning methods as subroutines. Our goal is an algorithm that utilizes only simple and convergent maximum likelihood loss functions, while also being able to leverage off-policy data. Our proposed approach, which we refer to as advantage-weighted regression (AWR), consists of two standard supervised learning steps: one to regress onto target values for a value function, and another to regress onto weighted target actions for the policy. The method is simple and general, can accommodate continuous and discrete actions, and can be implemented in just a few lines of code on top of standard supervised learning methods. We provide a theoretical motivation for AWR and analyze its properties when incorporating off-policy data from experience replay. We evaluate AWR on a suite of standard OpenAI Gym benchmark tasks, and show that it achieves competitive performance compared to a number of well-established state-of-the-art RL algorithms. AWR is also able to acquire more effective policies than most off-policy algorithms when learning from purely static datasets with no additional environmental interactions. Furthermore, we demonstrate our algorithm on challenging continuous control tasks with highly complex simulated characters.", "pdf": "/pdf/eed265837f73af8aab0ac60b47f1622bbdfc7d95.pdf", "code": "https://sites.google.com/view/awr-supp", "paperhash": "peng|advantage_weighted_regression_simple_and_scalable_offpolicy_reinforcement_learning", "original_pdf": "/attachment/15f6cbf979852a7aa318898dac52647ab0ee5939.pdf", "_bibtex": "@misc{\npeng2020advantage,\ntitle={Advantage Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning},\nauthor={Xue Bin Peng and Aviral Kumar and Grace Zhang and Sergey Levine},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gdF34FvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1gdF34FvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper84/Authors", "ICLR.cc/2020/Conference/Paper84/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper84/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper84/Reviewers", "ICLR.cc/2020/Conference/Paper84/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper84/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper84/Authors|ICLR.cc/2020/Conference/Paper84/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176634, "tmdate": 1576860555463, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper84/Authors", "ICLR.cc/2020/Conference/Paper84/Reviewers", "ICLR.cc/2020/Conference/Paper84/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper84/-/Official_Comment"}}}, {"id": "rkxEYyY7sS", "original": null, "number": 15, "cdate": 1573257083771, "ddate": null, "tcdate": 1573257083771, "tmdate": 1573257083771, "tddate": null, "forum": "H1gdF34FvS", "replyto": "SJgoLpzbsS", "invitation": "ICLR.cc/2020/Conference/Paper84/-/Official_Comment", "content": {"title": "Response to R3", "comment": "Re: Prior work\nWe appreciate the pointers to MARWIL and FQI by AWR, and we have included a discussion of this work in the latest revision of the paper (Section 4). We will also perform additional experiments that compare AWR directly with FQI by AWR. We would like to emphasize that FQI by AWR as a method uses a kernel-based Q-function estimator to perform fitted Q-iteration. MARWIL is arguably more related to our method in terms of the form of the update, but addresses a different problem setting -- imitation learning rather than reinforcement learning from scratch.\n\nRe: Improving exposition\nThank you for the suggestions, we would be happy to revise the presentation to be more didactic. For clarification, could you elaborate on what precisely you have in mind for improving the presentation of the method? We believe that our exposition is already more formal and rigorous than prior work in terms of justifying why we should expect off-policy AWR to perform well. We describe how the method implements a type of trust region (Section 3.1) and how the baseline that give rise to advantages emerges as a natural consequence of the conservative policy improvement objective in Equation 8. We believe that this derivation, which is not presented in Neumann & Peters, contributes substantially in terms of elucidating the reason such a method should work well as an RL algorithm.\n\nRe: difference to PG\nWhile the AWR update appears similar to a PG update, there are subtle but important differences between the two. In terms of derivation, AWR is not a policy gradient algorithm, it is an EM algorithm (http://is.tuebingen.mpg.de/fileadmin/user_upload/files/publications/ICML2007-Peters_4493[0].pdf). PG methods update a policy by directly estimating the gradient of the expected return with respect to the policy parameters. But with EM algorithms, they first construct an estimate of the optimal action distribution at each state, and then project that action distribution onto the space of parameterized policies. Algorithmically, the two vital differences are: \n1) in AWR the log probability of an action is weighted by \u201cexp(adv)\u201c, while in PG the log probability is weighted by just \u201cadv\u201d without the exponential. Since exp(adv) is non-negative, the objective used in the AWR update is a maximum likelihood objective that tries to maximize the likelihood of all actions, but to varying amounts depending on exp(adv). In the case of PG, the advantage can be both positive and negative, therefore PG updates will try to decrease the likelihood of actions with a negative advantage and thus it is not a conventional maximum likelihood objective. In practice, negative TD updates are often the source of instability when applying PG to off-policy data.\n2) policy gradient methods differentiate through the sampling distribution, while AWR can in principle use any off-policy data without the need to differentiate through the sampling distribution (as shown in equation 10).\n\nre: weight clipping\nYes, we will perform additional ablation experiments to show the impact of weight clipping. We will update you with the results once those are ready."}, "signatures": ["ICLR.cc/2020/Conference/Paper84/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper84/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Advantage Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning", "authors": ["Xue Bin Peng", "Aviral Kumar", "Grace Zhang", "Sergey Levine"], "authorids": ["xbpeng@berkeley.edu", "aviralkumar2907@gmail.com", "grace.zhang@berkeley.edu", "svlevine@eecs.berkeley.edu"], "keywords": ["reinforcement learning", "policy search", "control"], "TL;DR": "We represent a simple off-policy reinforcement learning algorithm that uses standard supervised learning methods as subroutines.", "abstract": "In this paper, we aim to develop a simple and scalable reinforcement learning algorithm that uses standard supervised learning methods as subroutines. Our goal is an algorithm that utilizes only simple and convergent maximum likelihood loss functions, while also being able to leverage off-policy data. Our proposed approach, which we refer to as advantage-weighted regression (AWR), consists of two standard supervised learning steps: one to regress onto target values for a value function, and another to regress onto weighted target actions for the policy. The method is simple and general, can accommodate continuous and discrete actions, and can be implemented in just a few lines of code on top of standard supervised learning methods. We provide a theoretical motivation for AWR and analyze its properties when incorporating off-policy data from experience replay. We evaluate AWR on a suite of standard OpenAI Gym benchmark tasks, and show that it achieves competitive performance compared to a number of well-established state-of-the-art RL algorithms. AWR is also able to acquire more effective policies than most off-policy algorithms when learning from purely static datasets with no additional environmental interactions. Furthermore, we demonstrate our algorithm on challenging continuous control tasks with highly complex simulated characters.", "pdf": "/pdf/eed265837f73af8aab0ac60b47f1622bbdfc7d95.pdf", "code": "https://sites.google.com/view/awr-supp", "paperhash": "peng|advantage_weighted_regression_simple_and_scalable_offpolicy_reinforcement_learning", "original_pdf": "/attachment/15f6cbf979852a7aa318898dac52647ab0ee5939.pdf", "_bibtex": "@misc{\npeng2020advantage,\ntitle={Advantage Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning},\nauthor={Xue Bin Peng and Aviral Kumar and Grace Zhang and Sergey Levine},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gdF34FvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1gdF34FvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper84/Authors", "ICLR.cc/2020/Conference/Paper84/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper84/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper84/Reviewers", "ICLR.cc/2020/Conference/Paper84/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper84/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper84/Authors|ICLR.cc/2020/Conference/Paper84/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176634, "tmdate": 1576860555463, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper84/Authors", "ICLR.cc/2020/Conference/Paper84/Reviewers", "ICLR.cc/2020/Conference/Paper84/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper84/-/Official_Comment"}}}, {"id": "Bkx4V19Mor", "original": null, "number": 13, "cdate": 1573195564390, "ddate": null, "tcdate": 1573195564390, "tmdate": 1573195564390, "tddate": null, "forum": "H1gdF34FvS", "replyto": "SJgoLpzbsS", "invitation": "ICLR.cc/2020/Conference/Paper84/-/Official_Comment", "content": {"title": "Difference to REINFORCE-like approaches + weight clipping", "comment": "Thank you, the reason why AWR is less sensitive to critic approximation errors is now clear.\nBy the way, the above simplified description of AWR as a Monte-Carlo-with-baseline method makes it look very close to a REINFORCE-like approach based on the Policy Gradient Theorem (PGT). This led me to the following questions.\n\nREINFORCE-like Monte Carlo approaches using the PG are more stable than bootstrap methods, but they are generally on-policy. My main question is: what is the fundamental point in a weighted regression approach that makes it off-policy? To be even more specific: given that regression is implemented with gradient descent on the residuals and that the loss looks the same as given by the PGT, what is the key difference between the AWR approach and REINFORCE that makes it off-policy? Please forgive me if I'm missing something obvious, I can be quite slow on these matters.\n\nGiven that the main weakness of your paper will be lack of novelty (with respect to MARWIL and FQI by AWR), to me the best way to increase the value of the Methods part is to give a much more didactic presentation of the approach than the previous papers.\n\nAnother unrelated point: I would be glad to see the impact of weight clipping. An ablation about this implementation detail is missing. Does it drive the algorithm far away from what the derivation suggests?\n\nNote: I wrote this comment before reading your responses to all reviewers.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper84/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper84/AnonReviewer3", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Advantage Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning", "authors": ["Xue Bin Peng", "Aviral Kumar", "Grace Zhang", "Sergey Levine"], "authorids": ["xbpeng@berkeley.edu", "aviralkumar2907@gmail.com", "grace.zhang@berkeley.edu", "svlevine@eecs.berkeley.edu"], "keywords": ["reinforcement learning", "policy search", "control"], "TL;DR": "We represent a simple off-policy reinforcement learning algorithm that uses standard supervised learning methods as subroutines.", "abstract": "In this paper, we aim to develop a simple and scalable reinforcement learning algorithm that uses standard supervised learning methods as subroutines. Our goal is an algorithm that utilizes only simple and convergent maximum likelihood loss functions, while also being able to leverage off-policy data. Our proposed approach, which we refer to as advantage-weighted regression (AWR), consists of two standard supervised learning steps: one to regress onto target values for a value function, and another to regress onto weighted target actions for the policy. The method is simple and general, can accommodate continuous and discrete actions, and can be implemented in just a few lines of code on top of standard supervised learning methods. We provide a theoretical motivation for AWR and analyze its properties when incorporating off-policy data from experience replay. We evaluate AWR on a suite of standard OpenAI Gym benchmark tasks, and show that it achieves competitive performance compared to a number of well-established state-of-the-art RL algorithms. AWR is also able to acquire more effective policies than most off-policy algorithms when learning from purely static datasets with no additional environmental interactions. Furthermore, we demonstrate our algorithm on challenging continuous control tasks with highly complex simulated characters.", "pdf": "/pdf/eed265837f73af8aab0ac60b47f1622bbdfc7d95.pdf", "code": "https://sites.google.com/view/awr-supp", "paperhash": "peng|advantage_weighted_regression_simple_and_scalable_offpolicy_reinforcement_learning", "original_pdf": "/attachment/15f6cbf979852a7aa318898dac52647ab0ee5939.pdf", "_bibtex": "@misc{\npeng2020advantage,\ntitle={Advantage Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning},\nauthor={Xue Bin Peng and Aviral Kumar and Grace Zhang and Sergey Levine},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gdF34FvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1gdF34FvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper84/Authors", "ICLR.cc/2020/Conference/Paper84/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper84/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper84/Reviewers", "ICLR.cc/2020/Conference/Paper84/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper84/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper84/Authors|ICLR.cc/2020/Conference/Paper84/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176634, "tmdate": 1576860555463, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper84/Authors", "ICLR.cc/2020/Conference/Paper84/Reviewers", "ICLR.cc/2020/Conference/Paper84/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper84/-/Official_Comment"}}}, {"id": "r1gTvqFziS", "original": null, "number": 12, "cdate": 1573194340906, "ddate": null, "tcdate": 1573194340906, "tmdate": 1573194340906, "tddate": null, "forum": "H1gdF34FvS", "replyto": "Byl2SGsuur", "invitation": "ICLR.cc/2020/Conference/Paper84/-/Official_Comment", "content": {"title": "Initial response to R3", "comment": "Re: Fitted Q-iteration by Advantage Weighted Regression\nThank you for the pointer, We have revised the paper to include a discussion of \u201cFitted Q-iteration by Advantage Weighted Regression\u201d [Neumann & Peters] at the end of Section 4 (highlighted in red). Neumann & Peters proposed a kernel-based fitted Q-iteration (FQI) algorithm. Though their method also uses exponentiated advantages as weights, their definition of the policy is different from our formulation:\nhttps://imgur.com/a/0DuRH7o\nThe key difference is that in our method, the likelihood of an action is determined by both the likelihood of the sampling policy \\mu and the exponentiated advantage, while the policy in Neumann & Peters depends only on the advantage. Therefore, the policy update from Neumann & Peters does not enforce a trust region penalty that ensures the new policy is similar to the sampling policy, which is crucial for obtaining a good estimate of the objective using off-policy data collected from the sampling policy. Our method is simpler, does not perform fitted Q-iteration, and incorporates experience for off-policy learning. Furthermore, we provide a principled derivation of the advantage weights from a conservative policy improvement perspective. We further extend this analysis to AWR with experience replay and demonstrate its effectiveness for batch RL, both of which were not presented in Neumann & Peters.\n\nRe: Self-Imitation\nSIL as described in https://arxiv.org/abs/1806.05635, augments policy gradient algorithms with an auxiliary behaviour cloning loss to reuse samples from past experiences. In contrast to SIL, AWR is a standalone algorithm, and does not need to be combined with an auxiliary RL algorithm. Also note that the weights in the self-imitation loss is given by max(adv, 0) rather than exp(adv).\n\nRe: PPO results\nWe will update the PPO results with the data from John\u2019s modified PPO implementation. We would like to point out that the original PPO results in our paper uses the standard implementation from OpenAI baselines, and the performance matches those reported in the original PPO paper and subsequent work that compares to PPO. As shown in our experiments, the difference between the deterministic and stochastic policies is fairly minor:\nhttps://imgur.com/a/mTOOZyc\nand most of the performance improvements are due to the other modifications (action squashing, reward scaling, etc\u2026), which are not included in the standard PPO implementation:\nhttps://imgur.com/a/ny2rqFd"}, "signatures": ["ICLR.cc/2020/Conference/Paper84/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper84/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Advantage Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning", "authors": ["Xue Bin Peng", "Aviral Kumar", "Grace Zhang", "Sergey Levine"], "authorids": ["xbpeng@berkeley.edu", "aviralkumar2907@gmail.com", "grace.zhang@berkeley.edu", "svlevine@eecs.berkeley.edu"], "keywords": ["reinforcement learning", "policy search", "control"], "TL;DR": "We represent a simple off-policy reinforcement learning algorithm that uses standard supervised learning methods as subroutines.", "abstract": "In this paper, we aim to develop a simple and scalable reinforcement learning algorithm that uses standard supervised learning methods as subroutines. Our goal is an algorithm that utilizes only simple and convergent maximum likelihood loss functions, while also being able to leverage off-policy data. Our proposed approach, which we refer to as advantage-weighted regression (AWR), consists of two standard supervised learning steps: one to regress onto target values for a value function, and another to regress onto weighted target actions for the policy. The method is simple and general, can accommodate continuous and discrete actions, and can be implemented in just a few lines of code on top of standard supervised learning methods. We provide a theoretical motivation for AWR and analyze its properties when incorporating off-policy data from experience replay. We evaluate AWR on a suite of standard OpenAI Gym benchmark tasks, and show that it achieves competitive performance compared to a number of well-established state-of-the-art RL algorithms. AWR is also able to acquire more effective policies than most off-policy algorithms when learning from purely static datasets with no additional environmental interactions. Furthermore, we demonstrate our algorithm on challenging continuous control tasks with highly complex simulated characters.", "pdf": "/pdf/eed265837f73af8aab0ac60b47f1622bbdfc7d95.pdf", "code": "https://sites.google.com/view/awr-supp", "paperhash": "peng|advantage_weighted_regression_simple_and_scalable_offpolicy_reinforcement_learning", "original_pdf": "/attachment/15f6cbf979852a7aa318898dac52647ab0ee5939.pdf", "_bibtex": "@misc{\npeng2020advantage,\ntitle={Advantage Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning},\nauthor={Xue Bin Peng and Aviral Kumar and Grace Zhang and Sergey Levine},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gdF34FvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1gdF34FvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper84/Authors", "ICLR.cc/2020/Conference/Paper84/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper84/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper84/Reviewers", "ICLR.cc/2020/Conference/Paper84/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper84/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper84/Authors|ICLR.cc/2020/Conference/Paper84/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176634, "tmdate": 1576860555463, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper84/Authors", "ICLR.cc/2020/Conference/Paper84/Reviewers", "ICLR.cc/2020/Conference/Paper84/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper84/-/Official_Comment"}}}, {"id": "SylWvtKMsB", "original": null, "number": 11, "cdate": 1573194072610, "ddate": null, "tcdate": 1573194072610, "tmdate": 1573194072610, "tddate": null, "forum": "H1gdF34FvS", "replyto": "rylVAPJCFH", "invitation": "ICLR.cc/2020/Conference/Paper84/-/Official_Comment", "content": {"title": "Initial response to R2", "comment": "Thank you for the feedback, we will improve the writing to more clearly articulate the contribution of this work and include a more thorough discussion to contrast our method with related techniques. We have revised the paper to include a discussion of these prior works at the end of Section 4 (highlighted in red). We will also perform experiments that directly compare AWR with REPS and FQI by AWR [Neumann & Peters].\n\nWhile exponentiated-advantage weights have been used in a number of prior work, we present a principled derivation of AWR from a conservative policy improvement perspective, and also provide an analysis of AWR when combined with experience replay. We show that a number of simple design decisions, such as the use of TD-lambda and experience replay, enables AWR to achieve competitive performance with state-of-the-art algorithms both for RL and batch RL settings."}, "signatures": ["ICLR.cc/2020/Conference/Paper84/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper84/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Advantage Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning", "authors": ["Xue Bin Peng", "Aviral Kumar", "Grace Zhang", "Sergey Levine"], "authorids": ["xbpeng@berkeley.edu", "aviralkumar2907@gmail.com", "grace.zhang@berkeley.edu", "svlevine@eecs.berkeley.edu"], "keywords": ["reinforcement learning", "policy search", "control"], "TL;DR": "We represent a simple off-policy reinforcement learning algorithm that uses standard supervised learning methods as subroutines.", "abstract": "In this paper, we aim to develop a simple and scalable reinforcement learning algorithm that uses standard supervised learning methods as subroutines. Our goal is an algorithm that utilizes only simple and convergent maximum likelihood loss functions, while also being able to leverage off-policy data. Our proposed approach, which we refer to as advantage-weighted regression (AWR), consists of two standard supervised learning steps: one to regress onto target values for a value function, and another to regress onto weighted target actions for the policy. The method is simple and general, can accommodate continuous and discrete actions, and can be implemented in just a few lines of code on top of standard supervised learning methods. We provide a theoretical motivation for AWR and analyze its properties when incorporating off-policy data from experience replay. We evaluate AWR on a suite of standard OpenAI Gym benchmark tasks, and show that it achieves competitive performance compared to a number of well-established state-of-the-art RL algorithms. AWR is also able to acquire more effective policies than most off-policy algorithms when learning from purely static datasets with no additional environmental interactions. Furthermore, we demonstrate our algorithm on challenging continuous control tasks with highly complex simulated characters.", "pdf": "/pdf/eed265837f73af8aab0ac60b47f1622bbdfc7d95.pdf", "code": "https://sites.google.com/view/awr-supp", "paperhash": "peng|advantage_weighted_regression_simple_and_scalable_offpolicy_reinforcement_learning", "original_pdf": "/attachment/15f6cbf979852a7aa318898dac52647ab0ee5939.pdf", "_bibtex": "@misc{\npeng2020advantage,\ntitle={Advantage Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning},\nauthor={Xue Bin Peng and Aviral Kumar and Grace Zhang and Sergey Levine},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gdF34FvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1gdF34FvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper84/Authors", "ICLR.cc/2020/Conference/Paper84/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper84/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper84/Reviewers", "ICLR.cc/2020/Conference/Paper84/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper84/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper84/Authors|ICLR.cc/2020/Conference/Paper84/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176634, "tmdate": 1576860555463, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper84/Authors", "ICLR.cc/2020/Conference/Paper84/Reviewers", "ICLR.cc/2020/Conference/Paper84/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper84/-/Official_Comment"}}}, {"id": "SkePE_FzjS", "original": null, "number": 10, "cdate": 1573193774628, "ddate": null, "tcdate": 1573193774628, "tmdate": 1573193943362, "tddate": null, "forum": "H1gdF34FvS", "replyto": "HyxTjQo6FH", "invitation": "ICLR.cc/2020/Conference/Paper84/-/Official_Comment", "content": {"title": "Initial response to R1", "comment": "Initial response to R1\nThank you for your feedback, we will aim to run the following additional experiments to address your questions:\n1) We will perform experiments that directly compare AWR with REPS and FQI by AWR [Neumann & Peters].\n2) We will include additional tasks with discrete action spaces such as Cartpole-v1, which is more commonly used.\nPlease let us know if there are any additional experiments that you would find helpful.\n\nRe: Fitted Q-iteration by Advantage Weighted Regression\nThank you for the pointer, We have revised the paper to include a discussion of \u201cFitted Q-iteration by Advantage Weighted Regression\u201d [Neumann & Peters] in Section 4 (highlighted in red). Neumann & Peters proposed a kernel-based fitted Q-iteration algorithm. Though their method also uses exponentiated advantages as weights, their definition of the policy is different from ours:\nhttps://imgur.com/a/0DuRH7o\nThe key difference is that in our method, the likelihood of an action is determined by both the likelihood of the sampling policy \\mu and the exponentiated advantage, while the policy in Neumann & Peters depends only on the advantage. Therefore, the policy update from Neumann & Peters does not enforce a trust region penalty that ensures the new policy is similar to the sampling policy, which is crucial for obtaining a good estimate of the objective using off-policy data collected from the sampling policy. Our method is simpler, does not perform fitted Q-iteration, and incorporates experience for off-policy learning. Furthermore, we provide a principled derivation of the advantage weights from a conservative policy improvement perspective. We further extend this analysis to AWR with experience replay and demonstrate its effectiveness for batch RL, both of which were not presented in these prior works. \"Model-Free Preference-Based Reinforcement Learning\" uses REPS as its learning algorithm, and a discussion of REPS is available in Section 4. We will perform additional experiments to directly compare AWR with REPS and FQI by AWR.\n\nRe: Not outperforming previous methods\nWhile AWR does not outperform previous methods on all tasks, which we acknowledge in the paper, we would like to emphasize that our goal is to show that a simple off-policy method that uses supervised regression as subroutines can in fact be competitive with a number of current state-of-the-art algorithms. We believe that simple and effective RL algorithms are of interest to the ICLR community, and beating all state-of-the-art methods is not a prerequisite. We also demonstrate AWR\u2019s effectiveness in the fully off-policy batch RL setting, where previous methods such DDPG and SAC perform poorly. Furthermore, AWR does not require the additional complexity of batch RL methods such as BCQ and BEAR.\n\nRe: Benchmarks\nWe will also include additional tasks with discrete action spaces such as Cartpole-v1, which is a more commonly used benchmark. Note, our focus is primarily on continuous control tasks, and the addition of the discrete tasks is mainly to show that AWR can also be easily applied to discrete actions. The motivation for the tasks in section 5.3 is to show that AWR is also effective for controlling complex agents with larger numbers of degrees-of-freedom. Since the agents in standard benchmark tasks are relatively simple, we opt to use new environments to better demonstrate this capability.\n\nRe: additional random seeds\nWe have ran additional experiments for AWR with 10 different seeds. Here are learning curves comparing the results with 5 seeds and 10 seeds. The results are similar. We will update the paper to include results with 10 seeds.\nhttps://imgur.com/a/yLntOzf\n\nRe: Q1 Importance sampling in Equation 5\nImportance sampling (IS) is not required in Equation 5. The objective itself does not require IS. For policy gradient methods like TRPO and PPO, IS is used to estimate the policy gradient from samples. In the AWR formulation, the solution of the Lagrangian in Equation 8 yields an update that does not require importance sampling, but it does require that \\mu and \\pi are similar, which is enforced by the trust region constraint (Equation 6).\n\nRe: Q2 Optimize w and uniform d(s)\nThe algorithm does not optimize with respect to w. w_i represents the probability of selecting samples from a particular policy \\pi_i from the replay buffer, which is a constant. d(s) is also not a uniform distribution over states, it represents the state-marginal of the sampling distribution. In our implementation, we sample uniformly from the replay buffer, which does not result in a uniform d(s), since some states might be visited more frequently than others.\n\nRe: code\nThis is the same code used in the experiments. We do not manually assign random seeds to each run, instead we use python\u2019s default random seed initialization, which assigns a different random seed to each execution of the problem. \u201caction_std\u201d specifies the standard deviation of the gaussian action distribution."}, "signatures": ["ICLR.cc/2020/Conference/Paper84/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper84/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Advantage Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning", "authors": ["Xue Bin Peng", "Aviral Kumar", "Grace Zhang", "Sergey Levine"], "authorids": ["xbpeng@berkeley.edu", "aviralkumar2907@gmail.com", "grace.zhang@berkeley.edu", "svlevine@eecs.berkeley.edu"], "keywords": ["reinforcement learning", "policy search", "control"], "TL;DR": "We represent a simple off-policy reinforcement learning algorithm that uses standard supervised learning methods as subroutines.", "abstract": "In this paper, we aim to develop a simple and scalable reinforcement learning algorithm that uses standard supervised learning methods as subroutines. Our goal is an algorithm that utilizes only simple and convergent maximum likelihood loss functions, while also being able to leverage off-policy data. Our proposed approach, which we refer to as advantage-weighted regression (AWR), consists of two standard supervised learning steps: one to regress onto target values for a value function, and another to regress onto weighted target actions for the policy. The method is simple and general, can accommodate continuous and discrete actions, and can be implemented in just a few lines of code on top of standard supervised learning methods. We provide a theoretical motivation for AWR and analyze its properties when incorporating off-policy data from experience replay. We evaluate AWR on a suite of standard OpenAI Gym benchmark tasks, and show that it achieves competitive performance compared to a number of well-established state-of-the-art RL algorithms. AWR is also able to acquire more effective policies than most off-policy algorithms when learning from purely static datasets with no additional environmental interactions. Furthermore, we demonstrate our algorithm on challenging continuous control tasks with highly complex simulated characters.", "pdf": "/pdf/eed265837f73af8aab0ac60b47f1622bbdfc7d95.pdf", "code": "https://sites.google.com/view/awr-supp", "paperhash": "peng|advantage_weighted_regression_simple_and_scalable_offpolicy_reinforcement_learning", "original_pdf": "/attachment/15f6cbf979852a7aa318898dac52647ab0ee5939.pdf", "_bibtex": "@misc{\npeng2020advantage,\ntitle={Advantage Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning},\nauthor={Xue Bin Peng and Aviral Kumar and Grace Zhang and Sergey Levine},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gdF34FvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1gdF34FvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper84/Authors", "ICLR.cc/2020/Conference/Paper84/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper84/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper84/Reviewers", "ICLR.cc/2020/Conference/Paper84/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper84/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper84/Authors|ICLR.cc/2020/Conference/Paper84/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176634, "tmdate": 1576860555463, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper84/Authors", "ICLR.cc/2020/Conference/Paper84/Reviewers", "ICLR.cc/2020/Conference/Paper84/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper84/-/Official_Comment"}}}, {"id": "S1x-voXMiS", "original": null, "number": 9, "cdate": 1573170008862, "ddate": null, "tcdate": 1573170008862, "tmdate": 1573174474143, "tddate": null, "forum": "H1gdF34FvS", "replyto": "B1eG4Bzlir", "invitation": "ICLR.cc/2020/Conference/Paper84/-/Official_Comment", "content": {"title": "MARWIL", "comment": "re: tuning temperature\nThe temperature does require some tuning, but more automated methods can be used, such as dual gradient descent. However, this strategy would require manually specifying an upperbound on the trust region constraint, which becomes another hyperparameter that requires tuning, which can also vary across different tasks. Therefore, we opt for a simpler strategy and used a fixed temperature.\n\nre: MARWIL\nThank you for the pointer to MARWIL, indeed the method does appear similar to AWR, but their method was demonstrated only for imitation learning, while we include both RL and batch RL tasks, and compare our method to a number of state-of-the-art RL and batch RL algorithms. We also provide an analysis of AWR when combined with experience replay, where the sampling distribution is modeled by a trajectory-level mixture of rollouts collected from different policies, which was not presented in MARWIL. Though our design decisions such as TD-lambda and experience replay are fairly simple, we show that they are important components for an effective RL algorithm.  These consideration may not be as important when dealing only with imitation learning."}, "signatures": ["ICLR.cc/2020/Conference/Paper84/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper84/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Advantage Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning", "authors": ["Xue Bin Peng", "Aviral Kumar", "Grace Zhang", "Sergey Levine"], "authorids": ["xbpeng@berkeley.edu", "aviralkumar2907@gmail.com", "grace.zhang@berkeley.edu", "svlevine@eecs.berkeley.edu"], "keywords": ["reinforcement learning", "policy search", "control"], "TL;DR": "We represent a simple off-policy reinforcement learning algorithm that uses standard supervised learning methods as subroutines.", "abstract": "In this paper, we aim to develop a simple and scalable reinforcement learning algorithm that uses standard supervised learning methods as subroutines. Our goal is an algorithm that utilizes only simple and convergent maximum likelihood loss functions, while also being able to leverage off-policy data. Our proposed approach, which we refer to as advantage-weighted regression (AWR), consists of two standard supervised learning steps: one to regress onto target values for a value function, and another to regress onto weighted target actions for the policy. The method is simple and general, can accommodate continuous and discrete actions, and can be implemented in just a few lines of code on top of standard supervised learning methods. We provide a theoretical motivation for AWR and analyze its properties when incorporating off-policy data from experience replay. We evaluate AWR on a suite of standard OpenAI Gym benchmark tasks, and show that it achieves competitive performance compared to a number of well-established state-of-the-art RL algorithms. AWR is also able to acquire more effective policies than most off-policy algorithms when learning from purely static datasets with no additional environmental interactions. Furthermore, we demonstrate our algorithm on challenging continuous control tasks with highly complex simulated characters.", "pdf": "/pdf/eed265837f73af8aab0ac60b47f1622bbdfc7d95.pdf", "code": "https://sites.google.com/view/awr-supp", "paperhash": "peng|advantage_weighted_regression_simple_and_scalable_offpolicy_reinforcement_learning", "original_pdf": "/attachment/15f6cbf979852a7aa318898dac52647ab0ee5939.pdf", "_bibtex": "@misc{\npeng2020advantage,\ntitle={Advantage Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning},\nauthor={Xue Bin Peng and Aviral Kumar and Grace Zhang and Sergey Levine},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gdF34FvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1gdF34FvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper84/Authors", "ICLR.cc/2020/Conference/Paper84/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper84/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper84/Reviewers", "ICLR.cc/2020/Conference/Paper84/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper84/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper84/Authors|ICLR.cc/2020/Conference/Paper84/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176634, "tmdate": 1576860555463, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper84/Authors", "ICLR.cc/2020/Conference/Paper84/Reviewers", "ICLR.cc/2020/Conference/Paper84/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper84/-/Official_Comment"}}}, {"id": "SJgoLpzbsS", "original": null, "number": 8, "cdate": 1573100882532, "ddate": null, "tcdate": 1573100882532, "tmdate": 1573100882532, "tddate": null, "forum": "H1gdF34FvS", "replyto": "Syl_SLxxsr", "invitation": "ICLR.cc/2020/Conference/Paper84/-/Official_Comment", "content": {"title": "value function", "comment": "Thank you for the clarification. Yes, off-policy methods that use only a Q-function to update the policy (e.g. DDPG and SAC) can be more susceptible to model bias in the Q-function, which can lead to instability (https://papers.nips.cc/paper/3964-double-q-learning, https://arxiv.org/abs/1802.09477). For AWR, the policy is  updated using a combination of monte-carlo returns and a value function, where the value function is used primarily as a baseline. This enables AWR to be less susceptible to inaccurate value estimates, compared to methods that use only a value/Q function for policy updates. This can be seen in the experiments from Figure 4. The \"No Baseline\" policies can be interpreted as using a highly inaccurate value function that just returns zeros for all states. This does lead to some deterioration in performance, but AWR is still able to learn reasonably effective policies. If such an inaccurate value function is used for methods such as DDPG or SAC, the policy will fail to learn anything. Therefore, these results do seem to suggest that AWR can be less susceptible to inaccuracies in the value function."}, "signatures": ["ICLR.cc/2020/Conference/Paper84/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper84/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Advantage Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning", "authors": ["Xue Bin Peng", "Aviral Kumar", "Grace Zhang", "Sergey Levine"], "authorids": ["xbpeng@berkeley.edu", "aviralkumar2907@gmail.com", "grace.zhang@berkeley.edu", "svlevine@eecs.berkeley.edu"], "keywords": ["reinforcement learning", "policy search", "control"], "TL;DR": "We represent a simple off-policy reinforcement learning algorithm that uses standard supervised learning methods as subroutines.", "abstract": "In this paper, we aim to develop a simple and scalable reinforcement learning algorithm that uses standard supervised learning methods as subroutines. Our goal is an algorithm that utilizes only simple and convergent maximum likelihood loss functions, while also being able to leverage off-policy data. Our proposed approach, which we refer to as advantage-weighted regression (AWR), consists of two standard supervised learning steps: one to regress onto target values for a value function, and another to regress onto weighted target actions for the policy. The method is simple and general, can accommodate continuous and discrete actions, and can be implemented in just a few lines of code on top of standard supervised learning methods. We provide a theoretical motivation for AWR and analyze its properties when incorporating off-policy data from experience replay. We evaluate AWR on a suite of standard OpenAI Gym benchmark tasks, and show that it achieves competitive performance compared to a number of well-established state-of-the-art RL algorithms. AWR is also able to acquire more effective policies than most off-policy algorithms when learning from purely static datasets with no additional environmental interactions. Furthermore, we demonstrate our algorithm on challenging continuous control tasks with highly complex simulated characters.", "pdf": "/pdf/eed265837f73af8aab0ac60b47f1622bbdfc7d95.pdf", "code": "https://sites.google.com/view/awr-supp", "paperhash": "peng|advantage_weighted_regression_simple_and_scalable_offpolicy_reinforcement_learning", "original_pdf": "/attachment/15f6cbf979852a7aa318898dac52647ab0ee5939.pdf", "_bibtex": "@misc{\npeng2020advantage,\ntitle={Advantage Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning},\nauthor={Xue Bin Peng and Aviral Kumar and Grace Zhang and Sergey Levine},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gdF34FvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1gdF34FvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper84/Authors", "ICLR.cc/2020/Conference/Paper84/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper84/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper84/Reviewers", "ICLR.cc/2020/Conference/Paper84/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper84/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper84/Authors|ICLR.cc/2020/Conference/Paper84/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176634, "tmdate": 1576860555463, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper84/Authors", "ICLR.cc/2020/Conference/Paper84/Reviewers", "ICLR.cc/2020/Conference/Paper84/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper84/-/Official_Comment"}}}, {"id": "B1eG4Bzlir", "original": null, "number": 8, "cdate": 1573033258046, "ddate": null, "tcdate": 1573033258046, "tmdate": 1573033258046, "tddate": null, "forum": "H1gdF34FvS", "replyto": "H1gdF34FvS", "invitation": "ICLR.cc/2020/Conference/Paper84/-/Public_Comment", "content": {"title": "An interesting paper but perhaps even closer prior work is missing ?", "comment": "I finally had the time to read through this paper in detail and it seems to be a very nice empirical evaluation of one of the simplest ways to perform a form of reward weighted (or in this case advantage weighted) regression in combination with modern best practices for RL.\nI applaud that the authors went this route and resisted the temptation to make the algorithm more complicated than necessary. Among the myriad of papers recently proposed this is a refreshingly simple and well written paper. The only part that would be great to clarify is that I suspect beta to be a quite hard to choose parameter if the reward scales become different (i.e. in benchmarks that are less homogeneous than the ones considered in the paper).\n\nHowever, after reading the paper two times I am scratching my head about how this is different from MARWIL ( https://papers.nips.cc/paper/7866-exponentially-weighted-imitation-learning-for-batched-historical-data ) - which already used learned value functions for calculating the advantage and a replay mechanism. The only difference that I can see is that this paper not only considers the offline/imitation learning setting (where data is generated from some expert or behavior policy) but the full RL setting (which was hinted as a logical next application in the MARWIL paper). Perhaps I am missing something ? "}, "signatures": ["~Jost_Tobias_Springenberg2"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Jost_Tobias_Springenberg2", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Advantage Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning", "authors": ["Xue Bin Peng", "Aviral Kumar", "Grace Zhang", "Sergey Levine"], "authorids": ["xbpeng@berkeley.edu", "aviralkumar2907@gmail.com", "grace.zhang@berkeley.edu", "svlevine@eecs.berkeley.edu"], "keywords": ["reinforcement learning", "policy search", "control"], "TL;DR": "We represent a simple off-policy reinforcement learning algorithm that uses standard supervised learning methods as subroutines.", "abstract": "In this paper, we aim to develop a simple and scalable reinforcement learning algorithm that uses standard supervised learning methods as subroutines. Our goal is an algorithm that utilizes only simple and convergent maximum likelihood loss functions, while also being able to leverage off-policy data. Our proposed approach, which we refer to as advantage-weighted regression (AWR), consists of two standard supervised learning steps: one to regress onto target values for a value function, and another to regress onto weighted target actions for the policy. The method is simple and general, can accommodate continuous and discrete actions, and can be implemented in just a few lines of code on top of standard supervised learning methods. We provide a theoretical motivation for AWR and analyze its properties when incorporating off-policy data from experience replay. We evaluate AWR on a suite of standard OpenAI Gym benchmark tasks, and show that it achieves competitive performance compared to a number of well-established state-of-the-art RL algorithms. AWR is also able to acquire more effective policies than most off-policy algorithms when learning from purely static datasets with no additional environmental interactions. Furthermore, we demonstrate our algorithm on challenging continuous control tasks with highly complex simulated characters.", "pdf": "/pdf/eed265837f73af8aab0ac60b47f1622bbdfc7d95.pdf", "code": "https://sites.google.com/view/awr-supp", "paperhash": "peng|advantage_weighted_regression_simple_and_scalable_offpolicy_reinforcement_learning", "original_pdf": "/attachment/15f6cbf979852a7aa318898dac52647ab0ee5939.pdf", "_bibtex": "@misc{\npeng2020advantage,\ntitle={Advantage Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning},\nauthor={Xue Bin Peng and Aviral Kumar and Grace Zhang and Sergey Levine},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gdF34FvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1gdF34FvS", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504214177, "tmdate": 1576860588610, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper84/Authors", "ICLR.cc/2020/Conference/Paper84/Reviewers", "ICLR.cc/2020/Conference/Paper84/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper84/-/Public_Comment"}}}, {"id": "Syl_SLxxsr", "original": null, "number": 7, "cdate": 1573025343794, "ddate": null, "tcdate": 1573025343794, "tmdate": 1573025343794, "tddate": null, "forum": "H1gdF34FvS", "replyto": "B1lG-o6yiB", "invitation": "ICLR.cc/2020/Conference/Paper84/-/Official_Comment", "content": {"title": "Value function and off-policyness", "comment": "You are right. I have a strong bias towards off-policy methods, which more often use a Q-function, thus my intuition that estimating a value function could be harder than a Q function might be wrong. One recent clue about this intuition was the fact that the value function estimator was removed from the latest version of SAC, another one is that the V function implictly contains a maximization (over Q values) and the presence of this implicit maximization may complicate training (despite a lower dimensionality of V wrt to Q), but I must admit that I have not seen a paper comparing the stability of learning V versus learning Q. Such a paper would be welcome.\n\nAnyways, your AWR algorithm pretends to be off-policy, and instability of the critic is a major issue in off-policy algorithms using function approximators, so I'm wondering how your algorithm is behaving in this respect compared to those which use a Q-function.\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper84/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper84/AnonReviewer3", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Advantage Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning", "authors": ["Xue Bin Peng", "Aviral Kumar", "Grace Zhang", "Sergey Levine"], "authorids": ["xbpeng@berkeley.edu", "aviralkumar2907@gmail.com", "grace.zhang@berkeley.edu", "svlevine@eecs.berkeley.edu"], "keywords": ["reinforcement learning", "policy search", "control"], "TL;DR": "We represent a simple off-policy reinforcement learning algorithm that uses standard supervised learning methods as subroutines.", "abstract": "In this paper, we aim to develop a simple and scalable reinforcement learning algorithm that uses standard supervised learning methods as subroutines. Our goal is an algorithm that utilizes only simple and convergent maximum likelihood loss functions, while also being able to leverage off-policy data. Our proposed approach, which we refer to as advantage-weighted regression (AWR), consists of two standard supervised learning steps: one to regress onto target values for a value function, and another to regress onto weighted target actions for the policy. The method is simple and general, can accommodate continuous and discrete actions, and can be implemented in just a few lines of code on top of standard supervised learning methods. We provide a theoretical motivation for AWR and analyze its properties when incorporating off-policy data from experience replay. We evaluate AWR on a suite of standard OpenAI Gym benchmark tasks, and show that it achieves competitive performance compared to a number of well-established state-of-the-art RL algorithms. AWR is also able to acquire more effective policies than most off-policy algorithms when learning from purely static datasets with no additional environmental interactions. Furthermore, we demonstrate our algorithm on challenging continuous control tasks with highly complex simulated characters.", "pdf": "/pdf/eed265837f73af8aab0ac60b47f1622bbdfc7d95.pdf", "code": "https://sites.google.com/view/awr-supp", "paperhash": "peng|advantage_weighted_regression_simple_and_scalable_offpolicy_reinforcement_learning", "original_pdf": "/attachment/15f6cbf979852a7aa318898dac52647ab0ee5939.pdf", "_bibtex": "@misc{\npeng2020advantage,\ntitle={Advantage Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning},\nauthor={Xue Bin Peng and Aviral Kumar and Grace Zhang and Sergey Levine},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gdF34FvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1gdF34FvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper84/Authors", "ICLR.cc/2020/Conference/Paper84/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper84/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper84/Reviewers", "ICLR.cc/2020/Conference/Paper84/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper84/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper84/Authors|ICLR.cc/2020/Conference/Paper84/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176634, "tmdate": 1576860555463, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper84/Authors", "ICLR.cc/2020/Conference/Paper84/Reviewers", "ICLR.cc/2020/Conference/Paper84/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper84/-/Official_Comment"}}}, {"id": "B1lG-o6yiB", "original": null, "number": 6, "cdate": 1573014266389, "ddate": null, "tcdate": 1573014266389, "tmdate": 1573014266389, "tddate": null, "forum": "H1gdF34FvS", "replyto": "Byl2SGsuur", "invitation": "ICLR.cc/2020/Conference/Paper84/-/Official_Comment", "content": {"title": "clarification", "comment": "Thank you for the feedback, we will aim to conduct additional experiments to address your questions. But first, could you clarify what you mean by the stability of the value function? Value functions are very commonly used for actor-critic algorithms, such as policy gradient methods, while Q-functions are more commonly used for off-policy methods. To the best of our knowledge, neither one is necessarily easier to learn than the other."}, "signatures": ["ICLR.cc/2020/Conference/Paper84/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper84/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Advantage Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning", "authors": ["Xue Bin Peng", "Aviral Kumar", "Grace Zhang", "Sergey Levine"], "authorids": ["xbpeng@berkeley.edu", "aviralkumar2907@gmail.com", "grace.zhang@berkeley.edu", "svlevine@eecs.berkeley.edu"], "keywords": ["reinforcement learning", "policy search", "control"], "TL;DR": "We represent a simple off-policy reinforcement learning algorithm that uses standard supervised learning methods as subroutines.", "abstract": "In this paper, we aim to develop a simple and scalable reinforcement learning algorithm that uses standard supervised learning methods as subroutines. Our goal is an algorithm that utilizes only simple and convergent maximum likelihood loss functions, while also being able to leverage off-policy data. Our proposed approach, which we refer to as advantage-weighted regression (AWR), consists of two standard supervised learning steps: one to regress onto target values for a value function, and another to regress onto weighted target actions for the policy. The method is simple and general, can accommodate continuous and discrete actions, and can be implemented in just a few lines of code on top of standard supervised learning methods. We provide a theoretical motivation for AWR and analyze its properties when incorporating off-policy data from experience replay. We evaluate AWR on a suite of standard OpenAI Gym benchmark tasks, and show that it achieves competitive performance compared to a number of well-established state-of-the-art RL algorithms. AWR is also able to acquire more effective policies than most off-policy algorithms when learning from purely static datasets with no additional environmental interactions. Furthermore, we demonstrate our algorithm on challenging continuous control tasks with highly complex simulated characters.", "pdf": "/pdf/eed265837f73af8aab0ac60b47f1622bbdfc7d95.pdf", "code": "https://sites.google.com/view/awr-supp", "paperhash": "peng|advantage_weighted_regression_simple_and_scalable_offpolicy_reinforcement_learning", "original_pdf": "/attachment/15f6cbf979852a7aa318898dac52647ab0ee5939.pdf", "_bibtex": "@misc{\npeng2020advantage,\ntitle={Advantage Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning},\nauthor={Xue Bin Peng and Aviral Kumar and Grace Zhang and Sergey Levine},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gdF34FvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1gdF34FvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper84/Authors", "ICLR.cc/2020/Conference/Paper84/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper84/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper84/Reviewers", "ICLR.cc/2020/Conference/Paper84/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper84/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper84/Authors|ICLR.cc/2020/Conference/Paper84/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176634, "tmdate": 1576860555463, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper84/Authors", "ICLR.cc/2020/Conference/Paper84/Reviewers", "ICLR.cc/2020/Conference/Paper84/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper84/-/Official_Comment"}}}, {"id": "HyxTjQo6FH", "original": null, "number": 2, "cdate": 1571824549147, "ddate": null, "tcdate": 1571824549147, "tmdate": 1572972640630, "tddate": null, "forum": "H1gdF34FvS", "replyto": "H1gdF34FvS", "invitation": "ICLR.cc/2020/Conference/Paper84/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proposes an off-policy reinforcement learning method in which the model parameters have been updated using the regression style loss function. Specifically, this method uses two regression update steps: one update value function and another one update policy using weighted regression. To compare the proposed method with others [main comprasion], 6 MuJoCo tasks are used for continuous control and LunarLander-v2 for discrete space. \n\n-- Even though this paper has done a good job in terms of running different experiments, the selection of some of the benchmarks seems arbitrary. For example, for discrete action space, this paper uses LunarLander which is rarely used in any papers so it makes very difficult to draw a conclusion based on these results. Common 49 Atari-2600 games should have been used for comparison. The same thing about experiments in section 5.3 is true too as those tasks are not that well-known. \n\n-- The proposed method doesn't outperform previous off-policy methods on Mujoco task (Table 1). Since the main claim of this paper is a new off-policy method, outperforming the previous off-policy methods is a fair game. The current results are not convincing enough.\n\n-- There are significant overlaps between this paper, \"Fitted Q-iteration by Advantage Weighted Regression\", \"Model-Free Preference-Based Reinforcement Learning \", and \"Reinforcement learning by reward-weighted regression for operational space control\" which makes the contribution of this paper very incremental.\n\n-- The authors used only 5 seeds to run Mujoco experiments. Given the sensitivity of Mujoco for different starting points, the experiments should have been run at least with 10 different seeds.\n\nQuestions:\n1) Shouldn't be an importance sampling ratio between \\pi and \\mu in the equations? starting from eq.5. \n2) Does the algorithm optimize the respect to $w$ as well? (eq. 15) if yes, why it is not mentioned in algorithm 1? Plus, since $d(s)$ is uniform dist. (at least this is assumed for implementation), eq. 14,15,and 16 (wherever there is d(s)), those can be simplified, e.g. \\hat{V} = \\sum(w_i V_i), wouldn't be better just introduced simplified version rather than current ones? (referring to the only equation above section 3.3)\n3) Is this the same code used to report results in this paper? if yes, I didn't see any seed assignment in the code?! and what is \"action_std\" in the code?\n\nThere are a couple of recent works in merging on-policy with off-policy updates which you might want to cite them. \n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper84/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper84/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Advantage Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning", "authors": ["Xue Bin Peng", "Aviral Kumar", "Grace Zhang", "Sergey Levine"], "authorids": ["xbpeng@berkeley.edu", "aviralkumar2907@gmail.com", "grace.zhang@berkeley.edu", "svlevine@eecs.berkeley.edu"], "keywords": ["reinforcement learning", "policy search", "control"], "TL;DR": "We represent a simple off-policy reinforcement learning algorithm that uses standard supervised learning methods as subroutines.", "abstract": "In this paper, we aim to develop a simple and scalable reinforcement learning algorithm that uses standard supervised learning methods as subroutines. Our goal is an algorithm that utilizes only simple and convergent maximum likelihood loss functions, while also being able to leverage off-policy data. Our proposed approach, which we refer to as advantage-weighted regression (AWR), consists of two standard supervised learning steps: one to regress onto target values for a value function, and another to regress onto weighted target actions for the policy. The method is simple and general, can accommodate continuous and discrete actions, and can be implemented in just a few lines of code on top of standard supervised learning methods. We provide a theoretical motivation for AWR and analyze its properties when incorporating off-policy data from experience replay. We evaluate AWR on a suite of standard OpenAI Gym benchmark tasks, and show that it achieves competitive performance compared to a number of well-established state-of-the-art RL algorithms. AWR is also able to acquire more effective policies than most off-policy algorithms when learning from purely static datasets with no additional environmental interactions. Furthermore, we demonstrate our algorithm on challenging continuous control tasks with highly complex simulated characters.", "pdf": "/pdf/eed265837f73af8aab0ac60b47f1622bbdfc7d95.pdf", "code": "https://sites.google.com/view/awr-supp", "paperhash": "peng|advantage_weighted_regression_simple_and_scalable_offpolicy_reinforcement_learning", "original_pdf": "/attachment/15f6cbf979852a7aa318898dac52647ab0ee5939.pdf", "_bibtex": "@misc{\npeng2020advantage,\ntitle={Advantage Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning},\nauthor={Xue Bin Peng and Aviral Kumar and Grace Zhang and Sergey Levine},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gdF34FvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "H1gdF34FvS", "replyto": "H1gdF34FvS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper84/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper84/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575878245175, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper84/Reviewers"], "noninvitees": [], "tcdate": 1570237757312, "tmdate": 1575878245188, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper84/-/Official_Review"}}}, {"id": "rylVAPJCFH", "original": null, "number": 3, "cdate": 1571841995953, "ddate": null, "tcdate": 1571841995953, "tmdate": 1572972640585, "tddate": null, "forum": "H1gdF34FvS", "replyto": "H1gdF34FvS", "invitation": "ICLR.cc/2020/Conference/Paper84/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "\n# Summary\nThe paper shows that good old reward weighted regression (RWR) with value-function baseline is still state-of-the-art algorithm.\n\n# Decision\nThe paper is well-written and provides many evaluations. The contribution should be articulated more carefully, though, taking into account that most algorithmic ideas are present in prior work (https://openreview.net/forum?id=H1gdF34FvS&noteId=Bkxi11nsdr). Perhaps, the experience replay part is somewhat novel. It seems emphasizing more that the aim is to show that simple methods are competitive rather than focusing on novelty could be a good idea.\n\nProvided that the authors incorporate the feedback of the other reviewers and update the paper accordingly, it will make a good contribution.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper84/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper84/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Advantage Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning", "authors": ["Xue Bin Peng", "Aviral Kumar", "Grace Zhang", "Sergey Levine"], "authorids": ["xbpeng@berkeley.edu", "aviralkumar2907@gmail.com", "grace.zhang@berkeley.edu", "svlevine@eecs.berkeley.edu"], "keywords": ["reinforcement learning", "policy search", "control"], "TL;DR": "We represent a simple off-policy reinforcement learning algorithm that uses standard supervised learning methods as subroutines.", "abstract": "In this paper, we aim to develop a simple and scalable reinforcement learning algorithm that uses standard supervised learning methods as subroutines. Our goal is an algorithm that utilizes only simple and convergent maximum likelihood loss functions, while also being able to leverage off-policy data. Our proposed approach, which we refer to as advantage-weighted regression (AWR), consists of two standard supervised learning steps: one to regress onto target values for a value function, and another to regress onto weighted target actions for the policy. The method is simple and general, can accommodate continuous and discrete actions, and can be implemented in just a few lines of code on top of standard supervised learning methods. We provide a theoretical motivation for AWR and analyze its properties when incorporating off-policy data from experience replay. We evaluate AWR on a suite of standard OpenAI Gym benchmark tasks, and show that it achieves competitive performance compared to a number of well-established state-of-the-art RL algorithms. AWR is also able to acquire more effective policies than most off-policy algorithms when learning from purely static datasets with no additional environmental interactions. Furthermore, we demonstrate our algorithm on challenging continuous control tasks with highly complex simulated characters.", "pdf": "/pdf/eed265837f73af8aab0ac60b47f1622bbdfc7d95.pdf", "code": "https://sites.google.com/view/awr-supp", "paperhash": "peng|advantage_weighted_regression_simple_and_scalable_offpolicy_reinforcement_learning", "original_pdf": "/attachment/15f6cbf979852a7aa318898dac52647ab0ee5939.pdf", "_bibtex": "@misc{\npeng2020advantage,\ntitle={Advantage Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning},\nauthor={Xue Bin Peng and Aviral Kumar and Grace Zhang and Sergey Levine},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gdF34FvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "H1gdF34FvS", "replyto": "H1gdF34FvS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper84/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper84/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575878245175, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper84/Reviewers"], "noninvitees": [], "tcdate": 1570237757312, "tmdate": 1575878245188, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper84/-/Official_Review"}}}, {"id": "SJxeIEnPtS", "original": null, "number": 7, "cdate": 1571435591686, "ddate": null, "tcdate": 1571435591686, "tmdate": 1571435591686, "tddate": null, "forum": "H1gdF34FvS", "replyto": "H1gdF34FvS", "invitation": "ICLR.cc/2020/Conference/Paper84/-/Public_Comment", "content": {"comment": "Great, thanks for putting in the time to do these thorough experiments!", "title": "looks good"}, "signatures": ["~John_Schulman1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~John_Schulman1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Advantage Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning", "authors": ["Xue Bin Peng", "Aviral Kumar", "Grace Zhang", "Sergey Levine"], "authorids": ["xbpeng@berkeley.edu", "aviralkumar2907@gmail.com", "grace.zhang@berkeley.edu", "svlevine@eecs.berkeley.edu"], "keywords": ["reinforcement learning", "policy search", "control"], "TL;DR": "We represent a simple off-policy reinforcement learning algorithm that uses standard supervised learning methods as subroutines.", "abstract": "In this paper, we aim to develop a simple and scalable reinforcement learning algorithm that uses standard supervised learning methods as subroutines. Our goal is an algorithm that utilizes only simple and convergent maximum likelihood loss functions, while also being able to leverage off-policy data. Our proposed approach, which we refer to as advantage-weighted regression (AWR), consists of two standard supervised learning steps: one to regress onto target values for a value function, and another to regress onto weighted target actions for the policy. The method is simple and general, can accommodate continuous and discrete actions, and can be implemented in just a few lines of code on top of standard supervised learning methods. We provide a theoretical motivation for AWR and analyze its properties when incorporating off-policy data from experience replay. We evaluate AWR on a suite of standard OpenAI Gym benchmark tasks, and show that it achieves competitive performance compared to a number of well-established state-of-the-art RL algorithms. AWR is also able to acquire more effective policies than most off-policy algorithms when learning from purely static datasets with no additional environmental interactions. Furthermore, we demonstrate our algorithm on challenging continuous control tasks with highly complex simulated characters.", "pdf": "/pdf/eed265837f73af8aab0ac60b47f1622bbdfc7d95.pdf", "code": "https://sites.google.com/view/awr-supp", "paperhash": "peng|advantage_weighted_regression_simple_and_scalable_offpolicy_reinforcement_learning", "original_pdf": "/attachment/15f6cbf979852a7aa318898dac52647ab0ee5939.pdf", "_bibtex": "@misc{\npeng2020advantage,\ntitle={Advantage Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning},\nauthor={Xue Bin Peng and Aviral Kumar and Grace Zhang and Sergey Levine},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gdF34FvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1gdF34FvS", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504214177, "tmdate": 1576860588610, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper84/Authors", "ICLR.cc/2020/Conference/Paper84/Reviewers", "ICLR.cc/2020/Conference/Paper84/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper84/-/Public_Comment"}}}, {"id": "H1lFeZYDKB", "original": null, "number": 5, "cdate": 1571422448586, "ddate": null, "tcdate": 1571422448586, "tmdate": 1571422448586, "tddate": null, "forum": "H1gdF34FvS", "replyto": "SJx0Wp89uH", "invitation": "ICLR.cc/2020/Conference/Paper84/-/Official_Comment", "content": {"comment": "Thanks again for the suggestion. We have tried applying observation normalization to TD3 for the humanoid. Here's a performance comparison when training with and without normalization:\nhttps://imgur.com/a/ss7baug\nFor this task, it doesn't seem to lead to a noticeable improvement in performance.", "title": "observation normalization"}, "signatures": ["ICLR.cc/2020/Conference/Paper84/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper84/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Advantage Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning", "authors": ["Xue Bin Peng", "Aviral Kumar", "Grace Zhang", "Sergey Levine"], "authorids": ["xbpeng@berkeley.edu", "aviralkumar2907@gmail.com", "grace.zhang@berkeley.edu", "svlevine@eecs.berkeley.edu"], "keywords": ["reinforcement learning", "policy search", "control"], "TL;DR": "We represent a simple off-policy reinforcement learning algorithm that uses standard supervised learning methods as subroutines.", "abstract": "In this paper, we aim to develop a simple and scalable reinforcement learning algorithm that uses standard supervised learning methods as subroutines. Our goal is an algorithm that utilizes only simple and convergent maximum likelihood loss functions, while also being able to leverage off-policy data. Our proposed approach, which we refer to as advantage-weighted regression (AWR), consists of two standard supervised learning steps: one to regress onto target values for a value function, and another to regress onto weighted target actions for the policy. The method is simple and general, can accommodate continuous and discrete actions, and can be implemented in just a few lines of code on top of standard supervised learning methods. We provide a theoretical motivation for AWR and analyze its properties when incorporating off-policy data from experience replay. We evaluate AWR on a suite of standard OpenAI Gym benchmark tasks, and show that it achieves competitive performance compared to a number of well-established state-of-the-art RL algorithms. AWR is also able to acquire more effective policies than most off-policy algorithms when learning from purely static datasets with no additional environmental interactions. Furthermore, we demonstrate our algorithm on challenging continuous control tasks with highly complex simulated characters.", "pdf": "/pdf/eed265837f73af8aab0ac60b47f1622bbdfc7d95.pdf", "code": "https://sites.google.com/view/awr-supp", "paperhash": "peng|advantage_weighted_regression_simple_and_scalable_offpolicy_reinforcement_learning", "original_pdf": "/attachment/15f6cbf979852a7aa318898dac52647ab0ee5939.pdf", "_bibtex": "@misc{\npeng2020advantage,\ntitle={Advantage Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning},\nauthor={Xue Bin Peng and Aviral Kumar and Grace Zhang and Sergey Levine},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gdF34FvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1gdF34FvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper84/Authors", "ICLR.cc/2020/Conference/Paper84/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper84/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper84/Reviewers", "ICLR.cc/2020/Conference/Paper84/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper84/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper84/Authors|ICLR.cc/2020/Conference/Paper84/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176634, "tmdate": 1576860555463, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper84/Authors", "ICLR.cc/2020/Conference/Paper84/Reviewers", "ICLR.cc/2020/Conference/Paper84/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper84/-/Official_Comment"}}}, {"id": "rkluHDAXYS", "original": null, "number": 4, "cdate": 1571182399603, "ddate": null, "tcdate": 1571182399603, "tmdate": 1571182399603, "tddate": null, "forum": "H1gdF34FvS", "replyto": "rkxEQg_9uB", "invitation": "ICLR.cc/2020/Conference/Paper84/-/Official_Comment", "content": {"comment": "After scaling the returns, they seem to be similar to the plots you posted. We have conducted some ablation experiments on the modifications that were made to PPO. The overall conclusion appears to be that the difference between deterministic vs stochastic evaluation is fairly small for these tasks. Most of the performance improvements are a result of the other modifications. Here're some comparisons of the different modifications:\nhttps://imgur.com/a/ny2rqFd\n\n\"Baselines\": the original PPO code from OpenAI Baselines that we included in the paper (ppo2), which matches the performance in the original PPO paper.\n\"Modified\": uses the modified hyperparameters from your script (e.g. larger batch size, as well as using ppo1 instead of ppo2).\n\"R Scale\": applying reward scaling.\n\"A Squash\": applying action squashing.\n\nReward scaling and action squashing seems to be responsible for most of the improvements. AWR still shows comparable performance on most of the tasks. We will update the paper to include these new results with the modified PPO implementation.", "title": "new experiments"}, "signatures": ["ICLR.cc/2020/Conference/Paper84/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper84/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Advantage Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning", "authors": ["Xue Bin Peng", "Aviral Kumar", "Grace Zhang", "Sergey Levine"], "authorids": ["xbpeng@berkeley.edu", "aviralkumar2907@gmail.com", "grace.zhang@berkeley.edu", "svlevine@eecs.berkeley.edu"], "keywords": ["reinforcement learning", "policy search", "control"], "TL;DR": "We represent a simple off-policy reinforcement learning algorithm that uses standard supervised learning methods as subroutines.", "abstract": "In this paper, we aim to develop a simple and scalable reinforcement learning algorithm that uses standard supervised learning methods as subroutines. Our goal is an algorithm that utilizes only simple and convergent maximum likelihood loss functions, while also being able to leverage off-policy data. Our proposed approach, which we refer to as advantage-weighted regression (AWR), consists of two standard supervised learning steps: one to regress onto target values for a value function, and another to regress onto weighted target actions for the policy. The method is simple and general, can accommodate continuous and discrete actions, and can be implemented in just a few lines of code on top of standard supervised learning methods. We provide a theoretical motivation for AWR and analyze its properties when incorporating off-policy data from experience replay. We evaluate AWR on a suite of standard OpenAI Gym benchmark tasks, and show that it achieves competitive performance compared to a number of well-established state-of-the-art RL algorithms. AWR is also able to acquire more effective policies than most off-policy algorithms when learning from purely static datasets with no additional environmental interactions. Furthermore, we demonstrate our algorithm on challenging continuous control tasks with highly complex simulated characters.", "pdf": "/pdf/eed265837f73af8aab0ac60b47f1622bbdfc7d95.pdf", "code": "https://sites.google.com/view/awr-supp", "paperhash": "peng|advantage_weighted_regression_simple_and_scalable_offpolicy_reinforcement_learning", "original_pdf": "/attachment/15f6cbf979852a7aa318898dac52647ab0ee5939.pdf", "_bibtex": "@misc{\npeng2020advantage,\ntitle={Advantage Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning},\nauthor={Xue Bin Peng and Aviral Kumar and Grace Zhang and Sergey Levine},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gdF34FvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1gdF34FvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper84/Authors", "ICLR.cc/2020/Conference/Paper84/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper84/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper84/Reviewers", "ICLR.cc/2020/Conference/Paper84/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper84/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper84/Authors|ICLR.cc/2020/Conference/Paper84/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176634, "tmdate": 1576860555463, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper84/Authors", "ICLR.cc/2020/Conference/Paper84/Reviewers", "ICLR.cc/2020/Conference/Paper84/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper84/-/Official_Comment"}}}, {"id": "BkeyCI73_H", "original": null, "number": 3, "cdate": 1570678470627, "ddate": null, "tcdate": 1570678470627, "tmdate": 1570678470627, "tddate": null, "forum": "H1gdF34FvS", "replyto": "Bkxi11nsdr", "invitation": "ICLR.cc/2020/Conference/Paper84/-/Official_Comment", "content": {"comment": "Thank you for the references! We indeed missed this paper when we were preparing our submission, and we agree that this is a very relevant reference. We've added this reference to the paper, along with a discussion. We cannot update the openreview submission at this time, but will include an updated related work section in the final.\n\nWe definitely agree that this prior paper also uses an advantage weighting formulation, though in contrast with this work, our paper does not perform fitted Q-iteration, instead opting for the simplest possible approach for policy search with off-policy data.", "title": "references"}, "signatures": ["ICLR.cc/2020/Conference/Paper84/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper84/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Advantage Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning", "authors": ["Xue Bin Peng", "Aviral Kumar", "Grace Zhang", "Sergey Levine"], "authorids": ["xbpeng@berkeley.edu", "aviralkumar2907@gmail.com", "grace.zhang@berkeley.edu", "svlevine@eecs.berkeley.edu"], "keywords": ["reinforcement learning", "policy search", "control"], "TL;DR": "We represent a simple off-policy reinforcement learning algorithm that uses standard supervised learning methods as subroutines.", "abstract": "In this paper, we aim to develop a simple and scalable reinforcement learning algorithm that uses standard supervised learning methods as subroutines. Our goal is an algorithm that utilizes only simple and convergent maximum likelihood loss functions, while also being able to leverage off-policy data. Our proposed approach, which we refer to as advantage-weighted regression (AWR), consists of two standard supervised learning steps: one to regress onto target values for a value function, and another to regress onto weighted target actions for the policy. The method is simple and general, can accommodate continuous and discrete actions, and can be implemented in just a few lines of code on top of standard supervised learning methods. We provide a theoretical motivation for AWR and analyze its properties when incorporating off-policy data from experience replay. We evaluate AWR on a suite of standard OpenAI Gym benchmark tasks, and show that it achieves competitive performance compared to a number of well-established state-of-the-art RL algorithms. AWR is also able to acquire more effective policies than most off-policy algorithms when learning from purely static datasets with no additional environmental interactions. Furthermore, we demonstrate our algorithm on challenging continuous control tasks with highly complex simulated characters.", "pdf": "/pdf/eed265837f73af8aab0ac60b47f1622bbdfc7d95.pdf", "code": "https://sites.google.com/view/awr-supp", "paperhash": "peng|advantage_weighted_regression_simple_and_scalable_offpolicy_reinforcement_learning", "original_pdf": "/attachment/15f6cbf979852a7aa318898dac52647ab0ee5939.pdf", "_bibtex": "@misc{\npeng2020advantage,\ntitle={Advantage Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning},\nauthor={Xue Bin Peng and Aviral Kumar and Grace Zhang and Sergey Levine},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gdF34FvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1gdF34FvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper84/Authors", "ICLR.cc/2020/Conference/Paper84/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper84/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper84/Reviewers", "ICLR.cc/2020/Conference/Paper84/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper84/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper84/Authors|ICLR.cc/2020/Conference/Paper84/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176634, "tmdate": 1576860555463, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper84/Authors", "ICLR.cc/2020/Conference/Paper84/Reviewers", "ICLR.cc/2020/Conference/Paper84/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper84/-/Official_Comment"}}}, {"id": "Bkxi11nsdr", "original": null, "number": 6, "cdate": 1570647778650, "ddate": null, "tcdate": 1570647778650, "tmdate": 1570647778650, "tddate": null, "forum": "H1gdF34FvS", "replyto": "H1gdF34FvS", "invitation": "ICLR.cc/2020/Conference/Paper84/-/Public_Comment", "content": {"comment": "Its a nice paper and the results look promising, but it should be put in a proper context. We introduced Advantage weighted regression already 10 years ago, although in a different algorithmic context (for fitted Q-iteration), see\n\nhttps://papers.nips.cc/paper/3501-fitted-q-iteration-by-advantage-weighted-regression\n\nI was a bit surprised that this paper is not cited. We also used already baselines in REPS like formulations, for example in the actor critic REPS algorithm \n\nhttps://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/download/12247/11865\n\nAgain the setup is a bit different (its a kernel based algorithm and the baseline emerges from a constraint), but I think it would be fair to cite it and discuss the similarities. \n", "title": "A few missing references..."}, "signatures": ["~Gerhard_Neumann1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Gerhard_Neumann1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Advantage Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning", "authors": ["Xue Bin Peng", "Aviral Kumar", "Grace Zhang", "Sergey Levine"], "authorids": ["xbpeng@berkeley.edu", "aviralkumar2907@gmail.com", "grace.zhang@berkeley.edu", "svlevine@eecs.berkeley.edu"], "keywords": ["reinforcement learning", "policy search", "control"], "TL;DR": "We represent a simple off-policy reinforcement learning algorithm that uses standard supervised learning methods as subroutines.", "abstract": "In this paper, we aim to develop a simple and scalable reinforcement learning algorithm that uses standard supervised learning methods as subroutines. Our goal is an algorithm that utilizes only simple and convergent maximum likelihood loss functions, while also being able to leverage off-policy data. Our proposed approach, which we refer to as advantage-weighted regression (AWR), consists of two standard supervised learning steps: one to regress onto target values for a value function, and another to regress onto weighted target actions for the policy. The method is simple and general, can accommodate continuous and discrete actions, and can be implemented in just a few lines of code on top of standard supervised learning methods. We provide a theoretical motivation for AWR and analyze its properties when incorporating off-policy data from experience replay. We evaluate AWR on a suite of standard OpenAI Gym benchmark tasks, and show that it achieves competitive performance compared to a number of well-established state-of-the-art RL algorithms. AWR is also able to acquire more effective policies than most off-policy algorithms when learning from purely static datasets with no additional environmental interactions. Furthermore, we demonstrate our algorithm on challenging continuous control tasks with highly complex simulated characters.", "pdf": "/pdf/eed265837f73af8aab0ac60b47f1622bbdfc7d95.pdf", "code": "https://sites.google.com/view/awr-supp", "paperhash": "peng|advantage_weighted_regression_simple_and_scalable_offpolicy_reinforcement_learning", "original_pdf": "/attachment/15f6cbf979852a7aa318898dac52647ab0ee5939.pdf", "_bibtex": "@misc{\npeng2020advantage,\ntitle={Advantage Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning},\nauthor={Xue Bin Peng and Aviral Kumar and Grace Zhang and Sergey Levine},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gdF34FvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1gdF34FvS", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504214177, "tmdate": 1576860588610, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper84/Authors", "ICLR.cc/2020/Conference/Paper84/Reviewers", "ICLR.cc/2020/Conference/Paper84/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper84/-/Public_Comment"}}}, {"id": "rkxEQg_9uB", "original": null, "number": 5, "cdate": 1570566171956, "ddate": null, "tcdate": 1570566171956, "tmdate": 1570631435936, "tddate": null, "forum": "H1gdF34FvS", "replyto": "rJgY9sVcuS", "invitation": "ICLR.cc/2020/Conference/Paper84/-/Public_Comment", "content": {"comment": "You need to multiply the return by 10 in your plot, because the RewScale wrapper divided it by 10. (Sorry, I should've mentioned that.) If you multiply by 10, the result looks like the plot I provided.\n\nOn this task, the returns are poorly scaled, so the performance mostly depends on the details of normalization. I see that your code uses a separate stepsize for the policy and value function, with a much larger stepsize on a value function -- that seems like a way to achieve the same result.\n\n[EDIT] deleted previous comment because they appear in reverse order\n[EDIT2] Modified the gist so now eval.csv gives the correctly scaled return.", "title": "followup"}, "signatures": ["~John_Schulman1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~John_Schulman1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Advantage Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning", "authors": ["Xue Bin Peng", "Aviral Kumar", "Grace Zhang", "Sergey Levine"], "authorids": ["xbpeng@berkeley.edu", "aviralkumar2907@gmail.com", "grace.zhang@berkeley.edu", "svlevine@eecs.berkeley.edu"], "keywords": ["reinforcement learning", "policy search", "control"], "TL;DR": "We represent a simple off-policy reinforcement learning algorithm that uses standard supervised learning methods as subroutines.", "abstract": "In this paper, we aim to develop a simple and scalable reinforcement learning algorithm that uses standard supervised learning methods as subroutines. Our goal is an algorithm that utilizes only simple and convergent maximum likelihood loss functions, while also being able to leverage off-policy data. Our proposed approach, which we refer to as advantage-weighted regression (AWR), consists of two standard supervised learning steps: one to regress onto target values for a value function, and another to regress onto weighted target actions for the policy. The method is simple and general, can accommodate continuous and discrete actions, and can be implemented in just a few lines of code on top of standard supervised learning methods. We provide a theoretical motivation for AWR and analyze its properties when incorporating off-policy data from experience replay. We evaluate AWR on a suite of standard OpenAI Gym benchmark tasks, and show that it achieves competitive performance compared to a number of well-established state-of-the-art RL algorithms. AWR is also able to acquire more effective policies than most off-policy algorithms when learning from purely static datasets with no additional environmental interactions. Furthermore, we demonstrate our algorithm on challenging continuous control tasks with highly complex simulated characters.", "pdf": "/pdf/eed265837f73af8aab0ac60b47f1622bbdfc7d95.pdf", "code": "https://sites.google.com/view/awr-supp", "paperhash": "peng|advantage_weighted_regression_simple_and_scalable_offpolicy_reinforcement_learning", "original_pdf": "/attachment/15f6cbf979852a7aa318898dac52647ab0ee5939.pdf", "_bibtex": "@misc{\npeng2020advantage,\ntitle={Advantage Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning},\nauthor={Xue Bin Peng and Aviral Kumar and Grace Zhang and Sergey Levine},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gdF34FvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1gdF34FvS", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504214177, "tmdate": 1576860588610, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper84/Authors", "ICLR.cc/2020/Conference/Paper84/Reviewers", "ICLR.cc/2020/Conference/Paper84/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper84/-/Public_Comment"}}}, {"id": "rJgY9sVcuS", "original": null, "number": 1, "cdate": 1570552721438, "ddate": null, "tcdate": 1570552721438, "tmdate": 1570563092447, "tddate": null, "forum": "H1gdF34FvS", "replyto": "S1xI3dbuuH", "invitation": "ICLR.cc/2020/Conference/Paper84/-/Official_Comment", "content": {"comment": "Hi John,\n\nThank you for the feedback, and the code. We ran the code that you linked, but our results do not match the ones you posted. In fact, our original PPO comparison (which was included in the paper) performed substantially better. We provide learning curves for both versions here:\nhttps://imgur.com/a/UWamd6N\n\"Original\" refers to using the original baselines PPO code, and \"Modified\" refers to the script you provided. We have also asked other people run your script and they observe similar results to ours. We have ran your script without any modifications, but were unable to  reproduce the results you posted, and those results do not appear to match the figures reported in the original PPO paper. We are not aware of any other previously published papers or publicly available code that generate results similar to the ones that you posted.\n\nThe PPO performance figures in our comparisons match those reported in the original PPO paper. We also find that  the performance of the stochastic and deterministic PPO policies from the baselines implementation are fairly similar:\nhttps://imgur.com/a/mTOOZyc\nHowever, we will update the performance statistics with those from the deterministic policies. The performance of the stochastic policy in your plots also appears to be substantially better than those from the baselines implementation. Perhaps there are other important differences besides using a deterministic vs stochastic policy?\n\nIn the paper, we were using the ppo2 implementation from the baselines, since it seems that ppo1 might be deprecated:\nhttps://github.com/openai/baselines/issues/485\nThe PPO results from the original paper more closely align with the ones reported in our comparisons.\n\nThat said, we would like to emphasize that the purpose of our comparisons is to illustrate that AWR attains results that are comparable to current state-of-the-art methods. Our goal is not to show that a particular algorithm is necessarily better than another. It is well known that the particular details of the implementation can make a significant difference, especially on widely studied tasks like the gym benchmarks. So in the interest of reproducible, we prefer to use more standard and publicly available implementations of these algorithms.\n\nWe would also like to point out that most hyperparameters are the same across the various environments. There are some parameters that varies a bit from environment to environment, such as the actor stepsizes for the humanoid and lunarlander, and the standard deviation of the action distribution.", "title": "PPO Results"}, "signatures": ["ICLR.cc/2020/Conference/Paper84/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper84/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Advantage Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning", "authors": ["Xue Bin Peng", "Aviral Kumar", "Grace Zhang", "Sergey Levine"], "authorids": ["xbpeng@berkeley.edu", "aviralkumar2907@gmail.com", "grace.zhang@berkeley.edu", "svlevine@eecs.berkeley.edu"], "keywords": ["reinforcement learning", "policy search", "control"], "TL;DR": "We represent a simple off-policy reinforcement learning algorithm that uses standard supervised learning methods as subroutines.", "abstract": "In this paper, we aim to develop a simple and scalable reinforcement learning algorithm that uses standard supervised learning methods as subroutines. Our goal is an algorithm that utilizes only simple and convergent maximum likelihood loss functions, while also being able to leverage off-policy data. Our proposed approach, which we refer to as advantage-weighted regression (AWR), consists of two standard supervised learning steps: one to regress onto target values for a value function, and another to regress onto weighted target actions for the policy. The method is simple and general, can accommodate continuous and discrete actions, and can be implemented in just a few lines of code on top of standard supervised learning methods. We provide a theoretical motivation for AWR and analyze its properties when incorporating off-policy data from experience replay. We evaluate AWR on a suite of standard OpenAI Gym benchmark tasks, and show that it achieves competitive performance compared to a number of well-established state-of-the-art RL algorithms. AWR is also able to acquire more effective policies than most off-policy algorithms when learning from purely static datasets with no additional environmental interactions. Furthermore, we demonstrate our algorithm on challenging continuous control tasks with highly complex simulated characters.", "pdf": "/pdf/eed265837f73af8aab0ac60b47f1622bbdfc7d95.pdf", "code": "https://sites.google.com/view/awr-supp", "paperhash": "peng|advantage_weighted_regression_simple_and_scalable_offpolicy_reinforcement_learning", "original_pdf": "/attachment/15f6cbf979852a7aa318898dac52647ab0ee5939.pdf", "_bibtex": "@misc{\npeng2020advantage,\ntitle={Advantage Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning},\nauthor={Xue Bin Peng and Aviral Kumar and Grace Zhang and Sergey Levine},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gdF34FvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1gdF34FvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper84/Authors", "ICLR.cc/2020/Conference/Paper84/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper84/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper84/Reviewers", "ICLR.cc/2020/Conference/Paper84/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper84/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper84/Authors|ICLR.cc/2020/Conference/Paper84/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176634, "tmdate": 1576860555463, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper84/Authors", "ICLR.cc/2020/Conference/Paper84/Reviewers", "ICLR.cc/2020/Conference/Paper84/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper84/-/Official_Comment"}}}, {"id": "SJx0Wp89uH", "original": null, "number": 3, "cdate": 1570561286459, "ddate": null, "tcdate": 1570561286459, "tmdate": 1570561286459, "tddate": null, "forum": "H1gdF34FvS", "replyto": "HkeJshE9OS", "invitation": "ICLR.cc/2020/Conference/Paper84/-/Public_Comment", "content": {"comment": "Thanks for the update. I think it will be beneficial to the community to compare each method in a fair way, so that people will have the insight as to which algorithm to use for their own problems. \n\nBut I completely understand that this is out of the scope of this paper.", "title": "TD3"}, "signatures": ["~Zhaoming_Xie1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Zhaoming_Xie1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Advantage Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning", "authors": ["Xue Bin Peng", "Aviral Kumar", "Grace Zhang", "Sergey Levine"], "authorids": ["xbpeng@berkeley.edu", "aviralkumar2907@gmail.com", "grace.zhang@berkeley.edu", "svlevine@eecs.berkeley.edu"], "keywords": ["reinforcement learning", "policy search", "control"], "TL;DR": "We represent a simple off-policy reinforcement learning algorithm that uses standard supervised learning methods as subroutines.", "abstract": "In this paper, we aim to develop a simple and scalable reinforcement learning algorithm that uses standard supervised learning methods as subroutines. Our goal is an algorithm that utilizes only simple and convergent maximum likelihood loss functions, while also being able to leverage off-policy data. Our proposed approach, which we refer to as advantage-weighted regression (AWR), consists of two standard supervised learning steps: one to regress onto target values for a value function, and another to regress onto weighted target actions for the policy. The method is simple and general, can accommodate continuous and discrete actions, and can be implemented in just a few lines of code on top of standard supervised learning methods. We provide a theoretical motivation for AWR and analyze its properties when incorporating off-policy data from experience replay. We evaluate AWR on a suite of standard OpenAI Gym benchmark tasks, and show that it achieves competitive performance compared to a number of well-established state-of-the-art RL algorithms. AWR is also able to acquire more effective policies than most off-policy algorithms when learning from purely static datasets with no additional environmental interactions. Furthermore, we demonstrate our algorithm on challenging continuous control tasks with highly complex simulated characters.", "pdf": "/pdf/eed265837f73af8aab0ac60b47f1622bbdfc7d95.pdf", "code": "https://sites.google.com/view/awr-supp", "paperhash": "peng|advantage_weighted_regression_simple_and_scalable_offpolicy_reinforcement_learning", "original_pdf": "/attachment/15f6cbf979852a7aa318898dac52647ab0ee5939.pdf", "_bibtex": "@misc{\npeng2020advantage,\ntitle={Advantage Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning},\nauthor={Xue Bin Peng and Aviral Kumar and Grace Zhang and Sergey Levine},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gdF34FvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1gdF34FvS", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504214177, "tmdate": 1576860588610, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper84/Authors", "ICLR.cc/2020/Conference/Paper84/Reviewers", "ICLR.cc/2020/Conference/Paper84/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper84/-/Public_Comment"}}}, {"id": "HkeJshE9OS", "original": null, "number": 2, "cdate": 1570552983476, "ddate": null, "tcdate": 1570552983476, "tmdate": 1570552983476, "tddate": null, "forum": "H1gdF34FvS", "replyto": "rJgU1TfYur", "invitation": "ICLR.cc/2020/Conference/Paper84/-/Official_Comment", "content": {"comment": "Hi Zhaoming,\n\nThank you for the suggestion and insight. We will look to try out input normalization for TD3. We would like to point out that it is generally standard practice to use publicly available code when comparing to prior algorithms. Since implementation details can have a significant effect on the performance of RL algorithms, in the interest of reproducibility, we prefer to use more standard and publicly available implementations of these algorithms in  our experiments. We would like to emphasize that the goal of our comparisons is to show that AWR can achieve comparable performance to current state-of-the-art methods, it is not to show whether one algorithm is necessarily better than another.", "title": "TD3"}, "signatures": ["ICLR.cc/2020/Conference/Paper84/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper84/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Advantage Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning", "authors": ["Xue Bin Peng", "Aviral Kumar", "Grace Zhang", "Sergey Levine"], "authorids": ["xbpeng@berkeley.edu", "aviralkumar2907@gmail.com", "grace.zhang@berkeley.edu", "svlevine@eecs.berkeley.edu"], "keywords": ["reinforcement learning", "policy search", "control"], "TL;DR": "We represent a simple off-policy reinforcement learning algorithm that uses standard supervised learning methods as subroutines.", "abstract": "In this paper, we aim to develop a simple and scalable reinforcement learning algorithm that uses standard supervised learning methods as subroutines. Our goal is an algorithm that utilizes only simple and convergent maximum likelihood loss functions, while also being able to leverage off-policy data. Our proposed approach, which we refer to as advantage-weighted regression (AWR), consists of two standard supervised learning steps: one to regress onto target values for a value function, and another to regress onto weighted target actions for the policy. The method is simple and general, can accommodate continuous and discrete actions, and can be implemented in just a few lines of code on top of standard supervised learning methods. We provide a theoretical motivation for AWR and analyze its properties when incorporating off-policy data from experience replay. We evaluate AWR on a suite of standard OpenAI Gym benchmark tasks, and show that it achieves competitive performance compared to a number of well-established state-of-the-art RL algorithms. AWR is also able to acquire more effective policies than most off-policy algorithms when learning from purely static datasets with no additional environmental interactions. Furthermore, we demonstrate our algorithm on challenging continuous control tasks with highly complex simulated characters.", "pdf": "/pdf/eed265837f73af8aab0ac60b47f1622bbdfc7d95.pdf", "code": "https://sites.google.com/view/awr-supp", "paperhash": "peng|advantage_weighted_regression_simple_and_scalable_offpolicy_reinforcement_learning", "original_pdf": "/attachment/15f6cbf979852a7aa318898dac52647ab0ee5939.pdf", "_bibtex": "@misc{\npeng2020advantage,\ntitle={Advantage Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning},\nauthor={Xue Bin Peng and Aviral Kumar and Grace Zhang and Sergey Levine},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gdF34FvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1gdF34FvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper84/Authors", "ICLR.cc/2020/Conference/Paper84/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper84/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper84/Reviewers", "ICLR.cc/2020/Conference/Paper84/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper84/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper84/Authors|ICLR.cc/2020/Conference/Paper84/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176634, "tmdate": 1576860555463, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper84/Authors", "ICLR.cc/2020/Conference/Paper84/Reviewers", "ICLR.cc/2020/Conference/Paper84/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper84/-/Official_Comment"}}}, {"id": "rJgU1TfYur", "original": null, "number": 2, "cdate": 1570479326135, "ddate": null, "tcdate": 1570479326135, "tmdate": 1570479326135, "tddate": null, "forum": "H1gdF34FvS", "replyto": "H1gdF34FvS", "invitation": "ICLR.cc/2020/Conference/Paper84/-/Public_Comment", "content": {"comment": "Just want to mention that in the original TD3 implementation, there is no normalization for the input to the network. That is probably why it performs so bad on the Humanoid task. The humanoid task include contact force to the state for some unknown reason and can have magnitude up to 1000. This is really bad for neural network without normalization. I think for fair comparison, similar normalization schemes should be employed for TD3.  ", "title": "TD3 humanoid comparison"}, "signatures": ["~Zhaoming_Xie1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Zhaoming_Xie1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Advantage Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning", "authors": ["Xue Bin Peng", "Aviral Kumar", "Grace Zhang", "Sergey Levine"], "authorids": ["xbpeng@berkeley.edu", "aviralkumar2907@gmail.com", "grace.zhang@berkeley.edu", "svlevine@eecs.berkeley.edu"], "keywords": ["reinforcement learning", "policy search", "control"], "TL;DR": "We represent a simple off-policy reinforcement learning algorithm that uses standard supervised learning methods as subroutines.", "abstract": "In this paper, we aim to develop a simple and scalable reinforcement learning algorithm that uses standard supervised learning methods as subroutines. Our goal is an algorithm that utilizes only simple and convergent maximum likelihood loss functions, while also being able to leverage off-policy data. Our proposed approach, which we refer to as advantage-weighted regression (AWR), consists of two standard supervised learning steps: one to regress onto target values for a value function, and another to regress onto weighted target actions for the policy. The method is simple and general, can accommodate continuous and discrete actions, and can be implemented in just a few lines of code on top of standard supervised learning methods. We provide a theoretical motivation for AWR and analyze its properties when incorporating off-policy data from experience replay. We evaluate AWR on a suite of standard OpenAI Gym benchmark tasks, and show that it achieves competitive performance compared to a number of well-established state-of-the-art RL algorithms. AWR is also able to acquire more effective policies than most off-policy algorithms when learning from purely static datasets with no additional environmental interactions. Furthermore, we demonstrate our algorithm on challenging continuous control tasks with highly complex simulated characters.", "pdf": "/pdf/eed265837f73af8aab0ac60b47f1622bbdfc7d95.pdf", "code": "https://sites.google.com/view/awr-supp", "paperhash": "peng|advantage_weighted_regression_simple_and_scalable_offpolicy_reinforcement_learning", "original_pdf": "/attachment/15f6cbf979852a7aa318898dac52647ab0ee5939.pdf", "_bibtex": "@misc{\npeng2020advantage,\ntitle={Advantage Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning},\nauthor={Xue Bin Peng and Aviral Kumar and Grace Zhang and Sergey Levine},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gdF34FvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1gdF34FvS", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504214177, "tmdate": 1576860588610, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper84/Authors", "ICLR.cc/2020/Conference/Paper84/Reviewers", "ICLR.cc/2020/Conference/Paper84/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper84/-/Public_Comment"}}}, {"id": "S1xI3dbuuH", "original": null, "number": 1, "cdate": 1570408622282, "ddate": null, "tcdate": 1570408622282, "tmdate": 1570408916717, "tddate": null, "forum": "H1gdF34FvS", "replyto": "H1gdF34FvS", "invitation": "ICLR.cc/2020/Conference/Paper84/-/Public_Comment", "content": {"comment": "Hi authors,\n\nI enjoyed this paper; I think it's the cleanest version of a cluster of ideas for weighted maximum likelihood objectives. (MAP-PO and self imitation learning are also in this cluster, and you might want to cite SIL.)\n\nBut the comparisons against PPO are not meaningful. You're comparing PPO's stochastic policy against the zero-noise deterministic policies from the other algorithms. By evaluating PPO in the same way, I get learning curves similar to your SAC learning curves: https://imgur.com/fTxLGWi (x axis units = 10^6 steps). Also, you're using different hyperparameters per environment, which seems improper.\n\nHere's the code that produced this result, which is a minor modification of ppo1/run_mujoco.py in baselines: https://gist.github.com/joschu/852b04e985fe6bd74ed7557e83e6538a . (Also squashing the action space to enforce limits, as SAC implementations usually do, along with reward scaling.) If you want, I am happy to run the code myself and provide csv files for inclusion in your plots.\n\nIn addition, your humanoid learning curve shows PPO failing to learn anything. If you plan to include this comparison, I suggest that you run train_humanoid.py from https://github.com/openai/baselines/tree/master/baselines/ppo1 as described in the readme, and add the deterministic eval that I've provided in the gist.\n\nI think the comparisons would be more meaningful if you used the same codebase for both algorithms but swapped only the loss functions (and separately optimized the hyperparams for each algorithm).", "title": "Nice paper, but PPO comparisons are flawed"}, "signatures": ["~John_Schulman1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~John_Schulman1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Advantage Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning", "authors": ["Xue Bin Peng", "Aviral Kumar", "Grace Zhang", "Sergey Levine"], "authorids": ["xbpeng@berkeley.edu", "aviralkumar2907@gmail.com", "grace.zhang@berkeley.edu", "svlevine@eecs.berkeley.edu"], "keywords": ["reinforcement learning", "policy search", "control"], "TL;DR": "We represent a simple off-policy reinforcement learning algorithm that uses standard supervised learning methods as subroutines.", "abstract": "In this paper, we aim to develop a simple and scalable reinforcement learning algorithm that uses standard supervised learning methods as subroutines. Our goal is an algorithm that utilizes only simple and convergent maximum likelihood loss functions, while also being able to leverage off-policy data. Our proposed approach, which we refer to as advantage-weighted regression (AWR), consists of two standard supervised learning steps: one to regress onto target values for a value function, and another to regress onto weighted target actions for the policy. The method is simple and general, can accommodate continuous and discrete actions, and can be implemented in just a few lines of code on top of standard supervised learning methods. We provide a theoretical motivation for AWR and analyze its properties when incorporating off-policy data from experience replay. We evaluate AWR on a suite of standard OpenAI Gym benchmark tasks, and show that it achieves competitive performance compared to a number of well-established state-of-the-art RL algorithms. AWR is also able to acquire more effective policies than most off-policy algorithms when learning from purely static datasets with no additional environmental interactions. Furthermore, we demonstrate our algorithm on challenging continuous control tasks with highly complex simulated characters.", "pdf": "/pdf/eed265837f73af8aab0ac60b47f1622bbdfc7d95.pdf", "code": "https://sites.google.com/view/awr-supp", "paperhash": "peng|advantage_weighted_regression_simple_and_scalable_offpolicy_reinforcement_learning", "original_pdf": "/attachment/15f6cbf979852a7aa318898dac52647ab0ee5939.pdf", "_bibtex": "@misc{\npeng2020advantage,\ntitle={Advantage Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning},\nauthor={Xue Bin Peng and Aviral Kumar and Grace Zhang and Sergey Levine},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gdF34FvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1gdF34FvS", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504214177, "tmdate": 1576860588610, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper84/Authors", "ICLR.cc/2020/Conference/Paper84/Reviewers", "ICLR.cc/2020/Conference/Paper84/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper84/-/Public_Comment"}}}], "count": 43}