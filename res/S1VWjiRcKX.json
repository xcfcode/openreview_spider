{"notes": [{"id": "S1VWjiRcKX", "original": "SyglADn9tQ", "number": 599, "cdate": 1538087833304, "ddate": null, "tcdate": 1538087833304, "tmdate": 1550778039371, "tddate": null, "forum": "S1VWjiRcKX", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Universal Successor Features Approximators", "abstract": "The ability of a reinforcement learning (RL) agent to learn about many reward functions at the same time has many potential benefits, such as the decomposition of complex tasks into simpler ones, the exchange of information between tasks, and the reuse of skills. We focus on one aspect in particular, namely the ability to generalise to unseen tasks. Parametric generalisation relies on the interpolation power of a function approximator that is given the task description as input; one of its most common form are universal value function approximators (UVFAs). Another way to generalise to new tasks is to exploit structure in the RL problem itself. Generalised policy improvement (GPI) combines solutions of previous tasks into a policy for the unseen task; this relies on instantaneous policy evaluation of old policies under the new reward function, which is made possible through successor features (SFs). Our proposed \\emph{universal successor features approximators} (USFAs) combine the advantages of all of these, namely the scalability of UVFAs, the instant inference of SFs, and the strong generalisation of GPI. We discuss the challenges involved in training a USFA, its generalisation properties and demonstrate its practical benefits and transfer abilities on a large-scale domain in which the agent has to navigate in a first-person perspective three-dimensional environment. ", "keywords": ["reinforcement learning", "zero-shot transfer", "successor features", "universal value functions", "general value functions"], "authorids": ["borsa@google.com", "andrebarreto@google.com", "johnquan@google.com", "dmankowitz@google.com", "hado@google.com", "munos@google.com", "davidsilver@google.com", "schaul@google.com"], "authors": ["Diana Borsa", "Andre Barreto", "John Quan", "Daniel J. Mankowitz", "Hado van Hasselt", "Remi Munos", "David Silver", "Tom Schaul"], "pdf": "/pdf/70175fb6b53961f2a1af03e5550716938c3a5158.pdf", "paperhash": "borsa|universal_successor_features_approximators", "_bibtex": "@inproceedings{\nborsa2018universal,\ntitle={Universal Successor Features Approximators},\nauthor={Diana Borsa and Andre Barreto and John Quan and Daniel J. Mankowitz and Hado van Hasselt and Remi Munos and David Silver and Tom Schaul},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1VWjiRcKX},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "BylYlVZ_gE", "original": null, "number": 1, "cdate": 1545241585220, "ddate": null, "tcdate": 1545241585220, "tmdate": 1545354475057, "tddate": null, "forum": "S1VWjiRcKX", "replyto": "S1VWjiRcKX", "invitation": "ICLR.cc/2019/Conference/-/Paper599/Meta_Review", "content": {"metareview": "This paper addresses an importnant and more realistic setting of multi-task RL where the reward function changes; the approach is elegant, and empirical results are convincing. The paper presents an importnant contribution to the challenging multi-task RL problem.", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Accept (Poster)", "title": "Nice contribution to multi-task (multiple reward functions) setting for RL"}, "signatures": ["ICLR.cc/2019/Conference/Paper599/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper599/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Universal Successor Features Approximators", "abstract": "The ability of a reinforcement learning (RL) agent to learn about many reward functions at the same time has many potential benefits, such as the decomposition of complex tasks into simpler ones, the exchange of information between tasks, and the reuse of skills. We focus on one aspect in particular, namely the ability to generalise to unseen tasks. Parametric generalisation relies on the interpolation power of a function approximator that is given the task description as input; one of its most common form are universal value function approximators (UVFAs). Another way to generalise to new tasks is to exploit structure in the RL problem itself. Generalised policy improvement (GPI) combines solutions of previous tasks into a policy for the unseen task; this relies on instantaneous policy evaluation of old policies under the new reward function, which is made possible through successor features (SFs). Our proposed \\emph{universal successor features approximators} (USFAs) combine the advantages of all of these, namely the scalability of UVFAs, the instant inference of SFs, and the strong generalisation of GPI. We discuss the challenges involved in training a USFA, its generalisation properties and demonstrate its practical benefits and transfer abilities on a large-scale domain in which the agent has to navigate in a first-person perspective three-dimensional environment. ", "keywords": ["reinforcement learning", "zero-shot transfer", "successor features", "universal value functions", "general value functions"], "authorids": ["borsa@google.com", "andrebarreto@google.com", "johnquan@google.com", "dmankowitz@google.com", "hado@google.com", "munos@google.com", "davidsilver@google.com", "schaul@google.com"], "authors": ["Diana Borsa", "Andre Barreto", "John Quan", "Daniel J. Mankowitz", "Hado van Hasselt", "Remi Munos", "David Silver", "Tom Schaul"], "pdf": "/pdf/70175fb6b53961f2a1af03e5550716938c3a5158.pdf", "paperhash": "borsa|universal_successor_features_approximators", "_bibtex": "@inproceedings{\nborsa2018universal,\ntitle={Universal Successor Features Approximators},\nauthor={Diana Borsa and Andre Barreto and John Quan and Daniel J. Mankowitz and Hado van Hasselt and Remi Munos and David Silver and Tom Schaul},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1VWjiRcKX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper599/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353159901, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1VWjiRcKX", "replyto": "S1VWjiRcKX", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper599/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper599/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper599/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353159901}}}, {"id": "HJllwrS8aQ", "original": null, "number": 3, "cdate": 1541981527764, "ddate": null, "tcdate": 1541981527764, "tmdate": 1541981527764, "tddate": null, "forum": "S1VWjiRcKX", "replyto": "S1VWjiRcKX", "invitation": "ICLR.cc/2019/Conference/-/Paper599/Official_Review", "content": {"title": "Scheme to generalize Q values across policies and tasks, by combining universal value function approximation and successor features+generalized policy improvement", "review": "The goal here is multi-task learning and generalization, assuming that the expected one-step reward for any member of the task family can be written as $\\phi(s,a,s')^T w$. The authors propose universal successor features (USF) $\\psi$s, such that the action-value functions Q can be written as $Q(s,a,w,z)=\\psi(s,a,z)^T w$, generalizing over mutiple tasks each denoted by $w$, and multiple policies each denoted by $z$. Here, $z$ represents the optimal policy induced by a reward specified by $z$ (from the same set as $w$). Using USFs $\\psi$-s, the Q values can be interpolated across policies and tasks. Due to the disentangling of reward and policy generalizations, the training sets for $w$ and $z$ can be independently sampled. The authors further generalize a temporal difference error in these USFs $\\psi$s, using the TD error to learn to approximate the $\\psi$s by a network (USF Approximator i.e USFA). They then test the generalization capabilities of these USFAs on families of a simple task and a DeepMind Lab based task.\n\nI find this paper a good fit for ICLR as the paper significantly advances learning representations for Q values that generalize across policies and tasks.\n\nSome issues to consider:\n1. Given a policy, I would think that the reward function that induces this policy is not unique. This non-uniqueness probably doesn't matter for the USF development, since the policies are restricted to those induced by z-s (from the same set as w-s), but the authors should clarify this point.\n\n2. I suppose there are no convergence guarantees on the $\\psi$-learning?\n\n3. I do believe that this work goes reasonably beyond the Ma et al 2018 paper, and the authors do clarify their advance especially in incorporating generalized policy improvement. However, the authors way of writing makes it appear as if their work only differs in some details. I recommend to remove this unexplanatory sentence:\n\"Although this work is superficially similar to ours, it differs a lot in the details.\"\n\nMinor:\npage 3: last but one line: \"more clear\" --> \"clearer\"\npage 3: \"In contrast\" --> \"By contrast\" -- but this is not a hard rule\n", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper599/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Universal Successor Features Approximators", "abstract": "The ability of a reinforcement learning (RL) agent to learn about many reward functions at the same time has many potential benefits, such as the decomposition of complex tasks into simpler ones, the exchange of information between tasks, and the reuse of skills. We focus on one aspect in particular, namely the ability to generalise to unseen tasks. Parametric generalisation relies on the interpolation power of a function approximator that is given the task description as input; one of its most common form are universal value function approximators (UVFAs). Another way to generalise to new tasks is to exploit structure in the RL problem itself. Generalised policy improvement (GPI) combines solutions of previous tasks into a policy for the unseen task; this relies on instantaneous policy evaluation of old policies under the new reward function, which is made possible through successor features (SFs). Our proposed \\emph{universal successor features approximators} (USFAs) combine the advantages of all of these, namely the scalability of UVFAs, the instant inference of SFs, and the strong generalisation of GPI. We discuss the challenges involved in training a USFA, its generalisation properties and demonstrate its practical benefits and transfer abilities on a large-scale domain in which the agent has to navigate in a first-person perspective three-dimensional environment. ", "keywords": ["reinforcement learning", "zero-shot transfer", "successor features", "universal value functions", "general value functions"], "authorids": ["borsa@google.com", "andrebarreto@google.com", "johnquan@google.com", "dmankowitz@google.com", "hado@google.com", "munos@google.com", "davidsilver@google.com", "schaul@google.com"], "authors": ["Diana Borsa", "Andre Barreto", "John Quan", "Daniel J. Mankowitz", "Hado van Hasselt", "Remi Munos", "David Silver", "Tom Schaul"], "pdf": "/pdf/70175fb6b53961f2a1af03e5550716938c3a5158.pdf", "paperhash": "borsa|universal_successor_features_approximators", "_bibtex": "@inproceedings{\nborsa2018universal,\ntitle={Universal Successor Features Approximators},\nauthor={Diana Borsa and Andre Barreto and John Quan and Daniel J. Mankowitz and Hado van Hasselt and Remi Munos and David Silver and Tom Schaul},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1VWjiRcKX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper599/Official_Review", "cdate": 1542234422971, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "S1VWjiRcKX", "replyto": "S1VWjiRcKX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper599/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335760677, "tmdate": 1552335760677, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper599/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SJe_R952hm", "original": null, "number": 2, "cdate": 1541348048123, "ddate": null, "tcdate": 1541348048123, "tmdate": 1541533854289, "tddate": null, "forum": "S1VWjiRcKX", "replyto": "S1VWjiRcKX", "invitation": "ICLR.cc/2019/Conference/-/Paper599/Official_Review", "content": {"title": "Universal Successor Feature Approximators", "review": "This paper proposes new ideas in the context of deep multi-task learning for RL. Ideas seem to me to be a rather small (epsiilon) improvement over the cited works.\n\nThe main problem - to me - with described approach is that the Q* value now lives in a much higher dimensional space, levelling any advantage a subsequent heuristic might give. \n\nStatements as 'Although this work is superficially similar to ours, it differs a lot in the details' makes clear that this work is only of potential interest for a rather small audience, a tenet also supported by the density of presentation. I leave it to the AC to decide on relevance. \n\n", "rating": "5: Marginally below acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2019/Conference/Paper599/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Universal Successor Features Approximators", "abstract": "The ability of a reinforcement learning (RL) agent to learn about many reward functions at the same time has many potential benefits, such as the decomposition of complex tasks into simpler ones, the exchange of information between tasks, and the reuse of skills. We focus on one aspect in particular, namely the ability to generalise to unseen tasks. Parametric generalisation relies on the interpolation power of a function approximator that is given the task description as input; one of its most common form are universal value function approximators (UVFAs). Another way to generalise to new tasks is to exploit structure in the RL problem itself. Generalised policy improvement (GPI) combines solutions of previous tasks into a policy for the unseen task; this relies on instantaneous policy evaluation of old policies under the new reward function, which is made possible through successor features (SFs). Our proposed \\emph{universal successor features approximators} (USFAs) combine the advantages of all of these, namely the scalability of UVFAs, the instant inference of SFs, and the strong generalisation of GPI. We discuss the challenges involved in training a USFA, its generalisation properties and demonstrate its practical benefits and transfer abilities on a large-scale domain in which the agent has to navigate in a first-person perspective three-dimensional environment. ", "keywords": ["reinforcement learning", "zero-shot transfer", "successor features", "universal value functions", "general value functions"], "authorids": ["borsa@google.com", "andrebarreto@google.com", "johnquan@google.com", "dmankowitz@google.com", "hado@google.com", "munos@google.com", "davidsilver@google.com", "schaul@google.com"], "authors": ["Diana Borsa", "Andre Barreto", "John Quan", "Daniel J. Mankowitz", "Hado van Hasselt", "Remi Munos", "David Silver", "Tom Schaul"], "pdf": "/pdf/70175fb6b53961f2a1af03e5550716938c3a5158.pdf", "paperhash": "borsa|universal_successor_features_approximators", "_bibtex": "@inproceedings{\nborsa2018universal,\ntitle={Universal Successor Features Approximators},\nauthor={Diana Borsa and Andre Barreto and John Quan and Daniel J. Mankowitz and Hado van Hasselt and Remi Munos and David Silver and Tom Schaul},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1VWjiRcKX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper599/Official_Review", "cdate": 1542234422971, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "S1VWjiRcKX", "replyto": "S1VWjiRcKX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper599/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335760677, "tmdate": 1552335760677, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper599/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "rJxFXMyF2X", "original": null, "number": 1, "cdate": 1541104160556, "ddate": null, "tcdate": 1541104160556, "tmdate": 1541533854083, "tddate": null, "forum": "S1VWjiRcKX", "replyto": "S1VWjiRcKX", "invitation": "ICLR.cc/2019/Conference/-/Paper599/Official_Review", "content": {"title": "New method for RL generalization to new tasks", "review": "Paper\u2019s contributions:\nThis paper considers the challenging problem of generalizing well to new RL tasks, based on having learned on a set of previous related RL tasks.  It considers tasks that differ only in their reward function (assume the dynamics are identical), and where the reward functions are constrained to be linear combinations over a set of given features.  The main approach, Universal Successor Features Approximators (USFAs) is a combination of two recent approaches:  Universal Value Function Approximators (UVFAs) and Generalized Policy Improvement (GPI).  The main claim is that while each of these methods leverages different types of regularity when generalizing to new tasks, USFAs are able to jointly leverage both types (and elegantly have both other methods as special cases).\n\nSummary of evaluation:\nOverall the paper tackles an important problem, and provides careful explanation and reasonably extensive results showing the ability of USFA to leverage structure.  I\u2019m on the fence because I really wish the combination of generalization properties could be understood in a more intuitive way.  There are some more minor issues, such as lack of complexity analysis and a few notation details, that can be easily fixed.\n\nPros:\n-\tThe problem of generalizing to new tasks in RL is an important open problem.\n-\tThe paper is carefully written and provides clear explanation of most of the methods & results.\n\nCons:\n-\tThe authors are diligent about trying to explain what type of regularities are exploited by each of UVFAs and GPI, and how this can be combined in USFAs.  However despite reading these parts carefully, I could not get a really good intuition, either in the methods or in the results, for the nature of the regularities exploited, and how it really differs.  Top of p.4 says that GPI generalizes well when the policy \\pi(s) does well on task w\u2019.  Can you give a specific MDP where Q is not smooth, but the policy does well?\n-\tThere is no complexity analysis.  I would like to know the computational complexity of each of the key steps in Algorithm 1 (with comparison to simple UVFA and GPI).\n-\tIt would be useful to see the empirical comparison with the approach of Ma et al. (2018), which also combines SFs and UFVAs. I understand there are differences in the details, but I would like to see confirmation of whether the claims about USFA\u2019s superior ability to exploit structure is supported by results.\n\nMinor comments:\n-\tThe limitation to linear rewards is a reasonably strong assumption.  It would be good to support this, e.g. by references to domain that meet this assumption.\n-\tIt seems the mathematical properties in Sec.3.1 could be further developed.\n-\tP.4: \u201cGiven a deterministic policy \\pi, one can easily define a reward function r_\\pi\u201d.  I did not think this mapping was unique (see the literature on IRL, e.g. Ross et al.).  Can you provide a proof or reference to support this statement?\n-\tThe definition of Q(s,a,w,z) is interesting. Can this be seen as a kernel between w and z?\n-\t\\theta suddenly shows up in Algorithm 1. I presume these are the parameters of Q?  Should be defined.\n-\tThe distribution used to sample policies seems to be a key step of this approach, yet not much guidance is given on how to do this in general.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper599/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Universal Successor Features Approximators", "abstract": "The ability of a reinforcement learning (RL) agent to learn about many reward functions at the same time has many potential benefits, such as the decomposition of complex tasks into simpler ones, the exchange of information between tasks, and the reuse of skills. We focus on one aspect in particular, namely the ability to generalise to unseen tasks. Parametric generalisation relies on the interpolation power of a function approximator that is given the task description as input; one of its most common form are universal value function approximators (UVFAs). Another way to generalise to new tasks is to exploit structure in the RL problem itself. Generalised policy improvement (GPI) combines solutions of previous tasks into a policy for the unseen task; this relies on instantaneous policy evaluation of old policies under the new reward function, which is made possible through successor features (SFs). Our proposed \\emph{universal successor features approximators} (USFAs) combine the advantages of all of these, namely the scalability of UVFAs, the instant inference of SFs, and the strong generalisation of GPI. We discuss the challenges involved in training a USFA, its generalisation properties and demonstrate its practical benefits and transfer abilities on a large-scale domain in which the agent has to navigate in a first-person perspective three-dimensional environment. ", "keywords": ["reinforcement learning", "zero-shot transfer", "successor features", "universal value functions", "general value functions"], "authorids": ["borsa@google.com", "andrebarreto@google.com", "johnquan@google.com", "dmankowitz@google.com", "hado@google.com", "munos@google.com", "davidsilver@google.com", "schaul@google.com"], "authors": ["Diana Borsa", "Andre Barreto", "John Quan", "Daniel J. Mankowitz", "Hado van Hasselt", "Remi Munos", "David Silver", "Tom Schaul"], "pdf": "/pdf/70175fb6b53961f2a1af03e5550716938c3a5158.pdf", "paperhash": "borsa|universal_successor_features_approximators", "_bibtex": "@inproceedings{\nborsa2018universal,\ntitle={Universal Successor Features Approximators},\nauthor={Diana Borsa and Andre Barreto and John Quan and Daniel J. Mankowitz and Hado van Hasselt and Remi Munos and David Silver and Tom Schaul},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1VWjiRcKX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper599/Official_Review", "cdate": 1542234422971, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "S1VWjiRcKX", "replyto": "S1VWjiRcKX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper599/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335760677, "tmdate": 1552335760677, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper599/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 5}