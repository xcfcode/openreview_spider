{"notes": [{"id": "rJg3zxBYwH", "original": "Skg0uQgtDH", "number": 2189, "cdate": 1569439764420, "ddate": null, "tcdate": 1569439764420, "tmdate": 1577168250008, "tddate": null, "forum": "rJg3zxBYwH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["christina.winkler.94@gmail.com", "d.e.worrall@uva.nl", "e.hoogeboom@uva.nl", "m.welling@uva.nl"], "title": "Learning Likelihoods with Conditional Normalizing Flows ", "authors": ["Christina Winkler", "Daniel Worrall", "Emiel Hoogeboom", "Max Welling"], "pdf": "/pdf/c5c80e5f52349ab77e4cc93b2f766836d0664bc1.pdf", "abstract": "Normalizing Flows (NFs) are able to model complicated distributions p(y) with strong inter-dimensional correlations and high multimodality by transforming a simple base density p(z) through an invertible neural network under the change of variables formula. Such behavior is desirable in multivariate structured prediction tasks, where handcrafted per-pixel loss-based methods inadequately capture strong correlations between output dimensions. We present a study of conditional normalizing flows (CNFs), a class of NFs where the base density to output space mapping is conditioned on an input x, to model conditional densities p(y|x). CNFs are efficient in sampling and inference, they can be trained with a likelihood-based objective, and CNFs, being generative flows, do not suffer from mode collapse or training instabilities. We provide an effective method to train continuous CNFs for binary problems and in particular, we apply these CNFs to super-resolution and vessel segmentation tasks demonstrating competitive performance on standard benchmark datasets in terms of likelihood and conventional metrics.", "keywords": ["Likelihood learning", "conditional normalizing flows", "generative modelling", "super-resolution", "vessel segmentation"], "paperhash": "winkler|learning_likelihoods_with_conditional_normalizing_flows", "original_pdf": "/attachment/1ba61e405382141129c54f53172139fd411e157e.pdf", "_bibtex": "@misc{\nwinkler2020learning,\ntitle={Learning Likelihoods with Conditional Normalizing Flows },\nauthor={Christina Winkler and Daniel Worrall and Emiel Hoogeboom and Max Welling},\nyear={2020},\nurl={https://openreview.net/forum?id=rJg3zxBYwH}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 16, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "97nrRkPW3R", "original": null, "number": 1, "cdate": 1576798742780, "ddate": null, "tcdate": 1576798742780, "tmdate": 1576800893444, "tddate": null, "forum": "rJg3zxBYwH", "replyto": "rJg3zxBYwH", "invitation": "ICLR.cc/2020/Conference/Paper2189/-/Decision", "content": {"decision": "Reject", "comment": "The authors propose a conditional normalizing flow approach to learning likelihoods. While reviewers appreciated the paper, in its present form it lacked a clear champion, and there were still some remaining concerns about novelty and clarity of presentation. The authors are encouraged to continue with this work and to account for reviewer comments in future revisions. Following up on the author response, a reviewer adds:\n\"Thanks for your clarification. I still disagree that the conditional flow architecture proposed should be considered as a novel contribution. The reason why I mentioned [1] or [2] was not because they follow the exact setting (coupling based conditional flow model) discussed in this paper. I wanted to highlight that the idea to use conditioning variables as an input to the transforming network (whether it is an autoregressive density function, autoregressive transforming network, or coupling layers) is quite universal (as we all know many of the existing codes implementing flow-based models includes additional keyword arguments 'context' to model conditioning). I'm not sure why the fact that the proposed framework is conditioning on high-dimensional variables makes a contribution. There seems to be no particular challenge in doing that and novel design choices to circumvent that (i.e., we can just use existing architectures with minor modifications).\n\nI agree that the binary dequantization should be considered as a contribution, but as significant as to change my decision to accept. Thanks for the clarification on experiments. Considering this, I raise my rating to weak reject...\n\nAnother previous work I forgot to mention in the initial review is \"Structured output learning with the conditional generative flow\", Lu and Huang 2019, ICML 2019 invertible neural network workshop. This paper discusses the conditional flow based on a similar idea, and attacks high-dimensional structured output prediction. I think this should be cited in the paper.\"\n", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["christina.winkler.94@gmail.com", "d.e.worrall@uva.nl", "e.hoogeboom@uva.nl", "m.welling@uva.nl"], "title": "Learning Likelihoods with Conditional Normalizing Flows ", "authors": ["Christina Winkler", "Daniel Worrall", "Emiel Hoogeboom", "Max Welling"], "pdf": "/pdf/c5c80e5f52349ab77e4cc93b2f766836d0664bc1.pdf", "abstract": "Normalizing Flows (NFs) are able to model complicated distributions p(y) with strong inter-dimensional correlations and high multimodality by transforming a simple base density p(z) through an invertible neural network under the change of variables formula. Such behavior is desirable in multivariate structured prediction tasks, where handcrafted per-pixel loss-based methods inadequately capture strong correlations between output dimensions. We present a study of conditional normalizing flows (CNFs), a class of NFs where the base density to output space mapping is conditioned on an input x, to model conditional densities p(y|x). CNFs are efficient in sampling and inference, they can be trained with a likelihood-based objective, and CNFs, being generative flows, do not suffer from mode collapse or training instabilities. We provide an effective method to train continuous CNFs for binary problems and in particular, we apply these CNFs to super-resolution and vessel segmentation tasks demonstrating competitive performance on standard benchmark datasets in terms of likelihood and conventional metrics.", "keywords": ["Likelihood learning", "conditional normalizing flows", "generative modelling", "super-resolution", "vessel segmentation"], "paperhash": "winkler|learning_likelihoods_with_conditional_normalizing_flows", "original_pdf": "/attachment/1ba61e405382141129c54f53172139fd411e157e.pdf", "_bibtex": "@misc{\nwinkler2020learning,\ntitle={Learning Likelihoods with Conditional Normalizing Flows },\nauthor={Christina Winkler and Daniel Worrall and Emiel Hoogeboom and Max Welling},\nyear={2020},\nurl={https://openreview.net/forum?id=rJg3zxBYwH}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "rJg3zxBYwH", "replyto": "rJg3zxBYwH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795714585, "tmdate": 1576800264321, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2189/-/Decision"}}}, {"id": "rJeN7RdTYr", "original": null, "number": 1, "cdate": 1571814939912, "ddate": null, "tcdate": 1571814939912, "tmdate": 1574390794517, "tddate": null, "forum": "rJg3zxBYwH", "replyto": "rJg3zxBYwH", "invitation": "ICLR.cc/2020/Conference/Paper2189/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "N/A", "review_assessment:_thoroughness_in_paper_reading": "N/A", "title": "Official Blind Review #1", "review": "The paper proposes the conditional normalizing flow for structured prediction. The idea is to use conditioning variables as additional inputs to the flow parameter forming networks.  The model was demonstrated on image superresolution and vessel segmentation.\n\nI find the contribution of this paper minimal. The idea of conditioning has extensively been used during recent years because it is the most natural thing to do (e.g., [1], [2] and numerous other papers). Their's nothing new about the flows used in this paper. The results in table 2 are not convincing; I see no benefit of using the proposed flow model for image super-resolution instead of the SOTA super-resolution methods. This also applies to other experiments.\n\n[1] van den Oord et al., Conditional Image Generation with PixelCNN Decoders, 2016.\n[2] Papamakarios et al., Masked Autoregressive Flow for Density Estimation, 2017.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper2189/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2189/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["christina.winkler.94@gmail.com", "d.e.worrall@uva.nl", "e.hoogeboom@uva.nl", "m.welling@uva.nl"], "title": "Learning Likelihoods with Conditional Normalizing Flows ", "authors": ["Christina Winkler", "Daniel Worrall", "Emiel Hoogeboom", "Max Welling"], "pdf": "/pdf/c5c80e5f52349ab77e4cc93b2f766836d0664bc1.pdf", "abstract": "Normalizing Flows (NFs) are able to model complicated distributions p(y) with strong inter-dimensional correlations and high multimodality by transforming a simple base density p(z) through an invertible neural network under the change of variables formula. Such behavior is desirable in multivariate structured prediction tasks, where handcrafted per-pixel loss-based methods inadequately capture strong correlations between output dimensions. We present a study of conditional normalizing flows (CNFs), a class of NFs where the base density to output space mapping is conditioned on an input x, to model conditional densities p(y|x). CNFs are efficient in sampling and inference, they can be trained with a likelihood-based objective, and CNFs, being generative flows, do not suffer from mode collapse or training instabilities. We provide an effective method to train continuous CNFs for binary problems and in particular, we apply these CNFs to super-resolution and vessel segmentation tasks demonstrating competitive performance on standard benchmark datasets in terms of likelihood and conventional metrics.", "keywords": ["Likelihood learning", "conditional normalizing flows", "generative modelling", "super-resolution", "vessel segmentation"], "paperhash": "winkler|learning_likelihoods_with_conditional_normalizing_flows", "original_pdf": "/attachment/1ba61e405382141129c54f53172139fd411e157e.pdf", "_bibtex": "@misc{\nwinkler2020learning,\ntitle={Learning Likelihoods with Conditional Normalizing Flows },\nauthor={Christina Winkler and Daniel Worrall and Emiel Hoogeboom and Max Welling},\nyear={2020},\nurl={https://openreview.net/forum?id=rJg3zxBYwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rJg3zxBYwH", "replyto": "rJg3zxBYwH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2189/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2189/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576141185231, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2189/Reviewers"], "noninvitees": [], "tcdate": 1570237726426, "tmdate": 1576141185244, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2189/-/Official_Review"}}}, {"id": "HylwT6ZjoS", "original": null, "number": 9, "cdate": 1573752255301, "ddate": null, "tcdate": 1573752255301, "tmdate": 1573752255301, "tddate": null, "forum": "rJg3zxBYwH", "replyto": "BJxfq8jg5S", "invitation": "ICLR.cc/2020/Conference/Paper2189/-/Official_Comment", "content": {"title": "Response to Reviewer 3", "comment": "Thank you for your comments\n\nIn Figure 2, we will include examples of the low resolution input, for an easier comparison of the results. In this figure in particular, we did not use a temperature for sampling of the baseline, since we are displaying the mode of the distribution. Since the distribution is factorized, sampling would add uncorrelated noise, meaning this comparison is actually skewed in favour of the baseline model.\n\nConcerning the DRIVE database, it indeed has very few images. Since we are training a likelihood based model, it is very easy for us to check for overfitting and early stop accordingly. In practice, we found that the standard data augmentation implied that we do not overfit. Furthermore, since the task is a per-pixel reconstruction task, the effective number of labels is much higher than the number of training images.\n\nIn the DRIVE experiments, we dropped the scaling modules since they did not appear to add much benefit to the results.\n\nWith regards to the exact architectures we have now placed network architecture tables in the appendix to clear up any confusion. Furthermore, we are adding a diagram of the conditional coupling layers in the appendix, which show the invertibility property clearly.\n\nWe have extended our related work on non-flow-based competing methods from the literature. and we have added some extra references on (conditional) normalizing flows as well.\n\nWe have already cleaned up the bibliography and any formatting issues, which we had at submission time. Thank you also for the sharp observation regarding the missing ^{-1} in Figure 1."}, "signatures": ["ICLR.cc/2020/Conference/Paper2189/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2189/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["christina.winkler.94@gmail.com", "d.e.worrall@uva.nl", "e.hoogeboom@uva.nl", "m.welling@uva.nl"], "title": "Learning Likelihoods with Conditional Normalizing Flows ", "authors": ["Christina Winkler", "Daniel Worrall", "Emiel Hoogeboom", "Max Welling"], "pdf": "/pdf/c5c80e5f52349ab77e4cc93b2f766836d0664bc1.pdf", "abstract": "Normalizing Flows (NFs) are able to model complicated distributions p(y) with strong inter-dimensional correlations and high multimodality by transforming a simple base density p(z) through an invertible neural network under the change of variables formula. Such behavior is desirable in multivariate structured prediction tasks, where handcrafted per-pixel loss-based methods inadequately capture strong correlations between output dimensions. We present a study of conditional normalizing flows (CNFs), a class of NFs where the base density to output space mapping is conditioned on an input x, to model conditional densities p(y|x). CNFs are efficient in sampling and inference, they can be trained with a likelihood-based objective, and CNFs, being generative flows, do not suffer from mode collapse or training instabilities. We provide an effective method to train continuous CNFs for binary problems and in particular, we apply these CNFs to super-resolution and vessel segmentation tasks demonstrating competitive performance on standard benchmark datasets in terms of likelihood and conventional metrics.", "keywords": ["Likelihood learning", "conditional normalizing flows", "generative modelling", "super-resolution", "vessel segmentation"], "paperhash": "winkler|learning_likelihoods_with_conditional_normalizing_flows", "original_pdf": "/attachment/1ba61e405382141129c54f53172139fd411e157e.pdf", "_bibtex": "@misc{\nwinkler2020learning,\ntitle={Learning Likelihoods with Conditional Normalizing Flows },\nauthor={Christina Winkler and Daniel Worrall and Emiel Hoogeboom and Max Welling},\nyear={2020},\nurl={https://openreview.net/forum?id=rJg3zxBYwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJg3zxBYwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2189/Authors", "ICLR.cc/2020/Conference/Paper2189/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2189/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2189/Reviewers", "ICLR.cc/2020/Conference/Paper2189/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2189/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2189/Authors|ICLR.cc/2020/Conference/Paper2189/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504145030, "tmdate": 1576860547165, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2189/Authors", "ICLR.cc/2020/Conference/Paper2189/Reviewers", "ICLR.cc/2020/Conference/Paper2189/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2189/-/Official_Comment"}}}, {"id": "Skgiqp-ooB", "original": null, "number": 8, "cdate": 1573752210963, "ddate": null, "tcdate": 1573752210963, "tmdate": 1573752210963, "tddate": null, "forum": "rJg3zxBYwH", "replyto": "BklyFUW-9r", "invitation": "ICLR.cc/2020/Conference/Paper2189/-/Official_Comment", "content": {"title": "Response to Reviewer 2", "comment": "Thank you for your comments.\n\nTo clarify your concerns, the design is covered in the section 3.1 Conditional modules. In particular the main invertible module is the conditional coupling layer. This takes in a conditioning input x and a latent variable z, which is transformed deterministically into a latent variable y. The transformation y <-> z conditioned on x is invertible. This transformation is similar to the coupling layer of RealNVP, but where every subnetwork in the layer takes and additional x as input. For clarity, we can add a diagram detailing this in the appendix.\n\nWith regards to your comments about mode collapse and training instability, it has been noted in the literature that normalizing flows do not suffer so much from mode collapse in the same way that GANs do, for instance. And on the topic of training instability, we did not notice any instabilities in the training of our flow models.\n\nThank you for your suggestions on follow up experiments. We agree that a text to image scenario would be interesting, since the conditioning argument in this case is structured. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2189/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2189/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["christina.winkler.94@gmail.com", "d.e.worrall@uva.nl", "e.hoogeboom@uva.nl", "m.welling@uva.nl"], "title": "Learning Likelihoods with Conditional Normalizing Flows ", "authors": ["Christina Winkler", "Daniel Worrall", "Emiel Hoogeboom", "Max Welling"], "pdf": "/pdf/c5c80e5f52349ab77e4cc93b2f766836d0664bc1.pdf", "abstract": "Normalizing Flows (NFs) are able to model complicated distributions p(y) with strong inter-dimensional correlations and high multimodality by transforming a simple base density p(z) through an invertible neural network under the change of variables formula. Such behavior is desirable in multivariate structured prediction tasks, where handcrafted per-pixel loss-based methods inadequately capture strong correlations between output dimensions. We present a study of conditional normalizing flows (CNFs), a class of NFs where the base density to output space mapping is conditioned on an input x, to model conditional densities p(y|x). CNFs are efficient in sampling and inference, they can be trained with a likelihood-based objective, and CNFs, being generative flows, do not suffer from mode collapse or training instabilities. We provide an effective method to train continuous CNFs for binary problems and in particular, we apply these CNFs to super-resolution and vessel segmentation tasks demonstrating competitive performance on standard benchmark datasets in terms of likelihood and conventional metrics.", "keywords": ["Likelihood learning", "conditional normalizing flows", "generative modelling", "super-resolution", "vessel segmentation"], "paperhash": "winkler|learning_likelihoods_with_conditional_normalizing_flows", "original_pdf": "/attachment/1ba61e405382141129c54f53172139fd411e157e.pdf", "_bibtex": "@misc{\nwinkler2020learning,\ntitle={Learning Likelihoods with Conditional Normalizing Flows },\nauthor={Christina Winkler and Daniel Worrall and Emiel Hoogeboom and Max Welling},\nyear={2020},\nurl={https://openreview.net/forum?id=rJg3zxBYwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJg3zxBYwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2189/Authors", "ICLR.cc/2020/Conference/Paper2189/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2189/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2189/Reviewers", "ICLR.cc/2020/Conference/Paper2189/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2189/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2189/Authors|ICLR.cc/2020/Conference/Paper2189/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504145030, "tmdate": 1576860547165, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2189/Authors", "ICLR.cc/2020/Conference/Paper2189/Reviewers", "ICLR.cc/2020/Conference/Paper2189/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2189/-/Official_Comment"}}}, {"id": "rkgaw6Wijr", "original": null, "number": 7, "cdate": 1573752164744, "ddate": null, "tcdate": 1573752164744, "tmdate": 1573752164744, "tddate": null, "forum": "rJg3zxBYwH", "replyto": "rJeN7RdTYr", "invitation": "ICLR.cc/2020/Conference/Paper2189/-/Official_Comment", "content": {"title": "Response to Reviewer 1", "comment": "Thank you for your comments.\n\nWe would like to disagree on the topic on novelty of our contribution. In particular, reference [1] is not a flow. And indeed as you state, class-conditional flow models are not new in the literature. Where we would like to draw our distinction, however, is that we are in fact not considering class-conditional flow models. Instead our generative flow uses high-dimensional images as the conditioning argument. This warrants the use of a different kind of conditional coupling layer, unlike the ones in the papers that you cite. These papers also happen to be autoregressive, which makes sampling computationally expensive. \n\nAnother contribution we would like to highlight, which was also recognized by reviewers 2 and 3, is the link we draw between variational dequantization and existing variational inference methods. This new viewpoint allows us to derive a form of variational dequantization adapted to binary random variables in a consistent probabilistic framework. This innovation is important when it comes to finding a good lower bound on the likelihood. For instance, in the retinal vessel segmentation experiments, the log-likelihood scores for uniform dequantization versus our method is about 0.35 BPD versus 0.025 BPD (2.s.f). In an updated version of the paper, we are going to include this results to stress this improvement.\n\nWith respect to Table 2, the specific metrics that are important depend on what task you are willing to solve. In terms of fitting distributions, we outperform our baselines. As stated in our introduction, we want to learn distributions over the data, because they can be easily evaluated in terms of likelihood, they are very interpretable compared to other generative methods such as GANs, there is no mode dropping behavior, and there exists easy tests for overfitting. "}, "signatures": ["ICLR.cc/2020/Conference/Paper2189/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2189/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["christina.winkler.94@gmail.com", "d.e.worrall@uva.nl", "e.hoogeboom@uva.nl", "m.welling@uva.nl"], "title": "Learning Likelihoods with Conditional Normalizing Flows ", "authors": ["Christina Winkler", "Daniel Worrall", "Emiel Hoogeboom", "Max Welling"], "pdf": "/pdf/c5c80e5f52349ab77e4cc93b2f766836d0664bc1.pdf", "abstract": "Normalizing Flows (NFs) are able to model complicated distributions p(y) with strong inter-dimensional correlations and high multimodality by transforming a simple base density p(z) through an invertible neural network under the change of variables formula. Such behavior is desirable in multivariate structured prediction tasks, where handcrafted per-pixel loss-based methods inadequately capture strong correlations between output dimensions. We present a study of conditional normalizing flows (CNFs), a class of NFs where the base density to output space mapping is conditioned on an input x, to model conditional densities p(y|x). CNFs are efficient in sampling and inference, they can be trained with a likelihood-based objective, and CNFs, being generative flows, do not suffer from mode collapse or training instabilities. We provide an effective method to train continuous CNFs for binary problems and in particular, we apply these CNFs to super-resolution and vessel segmentation tasks demonstrating competitive performance on standard benchmark datasets in terms of likelihood and conventional metrics.", "keywords": ["Likelihood learning", "conditional normalizing flows", "generative modelling", "super-resolution", "vessel segmentation"], "paperhash": "winkler|learning_likelihoods_with_conditional_normalizing_flows", "original_pdf": "/attachment/1ba61e405382141129c54f53172139fd411e157e.pdf", "_bibtex": "@misc{\nwinkler2020learning,\ntitle={Learning Likelihoods with Conditional Normalizing Flows },\nauthor={Christina Winkler and Daniel Worrall and Emiel Hoogeboom and Max Welling},\nyear={2020},\nurl={https://openreview.net/forum?id=rJg3zxBYwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJg3zxBYwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2189/Authors", "ICLR.cc/2020/Conference/Paper2189/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2189/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2189/Reviewers", "ICLR.cc/2020/Conference/Paper2189/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2189/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2189/Authors|ICLR.cc/2020/Conference/Paper2189/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504145030, "tmdate": 1576860547165, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2189/Authors", "ICLR.cc/2020/Conference/Paper2189/Reviewers", "ICLR.cc/2020/Conference/Paper2189/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2189/-/Official_Comment"}}}, {"id": "BJxfq8jg5S", "original": null, "number": 2, "cdate": 1572021898443, "ddate": null, "tcdate": 1572021898443, "tmdate": 1572972371413, "tddate": null, "forum": "rJg3zxBYwH", "replyto": "rJg3zxBYwH", "invitation": "ICLR.cc/2020/Conference/Paper2189/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Summary of the paper: \n\nThe paper proposes an extension of Normalizing flows to conditional distributions. The paper is well written overall and easy to follow. Basically the conditional prior z|x = z=f_{\\phi}(y,x), where x is the conditioning random variable, and we apply the change of variable formula to get the density of y|x . For example in super resolution y is the high res image and  x is the low res. image.  To sample from the models authors propose to use f^{-1}_{\\phi}(z;x). \n\nThe conditional modules are natural extensions of invertible blocks used in the literature (coupling layers, split priors, conditional coupling, 1x1 conv), where the conditioning is done on some hidden representations of the conditioning variable x (i.e one or multiple layers of NN).\n\nAuthors propose a dequantization for binary random variables (useful for segmentation applications), where they give an implicit model for the dequantizer (obtain a continuous variable from a discrete binary variable).\n\nAuthor apply the method in two applications super-resolution and vessel segmentation. the method is compared to supervised learning of the corresponds between x and y and to others competitive methods in the literature and shows some advantage. \n\nMinor comments : \n\n- Formatting the bibliography is messed up and needs some cleaning , Figure 5 is also making formatting issues of the paper. \n- Figure 1 for sampling it should be f^-1_{\\phi } and not f_{\\phi}\n\nReview: \n\n- Figure 2 is hard to get any idea of the sample quality would be good also to put the low resolution input to the algorithm . Also did you use a temperature sampling for the baseline ? otherwise the comparison is not fair.\n\n- The Drive database is too small 20 training samples and 20 testing only? can the model be just overfitting?\n\n- In the vessel implementation why do you drop the scaling modules? \n\n- The conditioning for the vessel implementation on x is on two layers , would be great to put all architectures of the models in details , and to show both sampling and training paths \n\n- It would be great to add the details of the skip connection used from the network processing x, and how ensure that the flow remains invertible.  \n\nOverall this is a well written paper and a good addition to normalizing flows methods , some discussion of related works on conditional normalizing flows and more baselines with other competitive methods based on GANs for example would be helpful but not necessary. \n\nIt would be great to add details of the architectures and on skip connections and how to ensure invertibility for this part in the model . \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2189/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2189/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["christina.winkler.94@gmail.com", "d.e.worrall@uva.nl", "e.hoogeboom@uva.nl", "m.welling@uva.nl"], "title": "Learning Likelihoods with Conditional Normalizing Flows ", "authors": ["Christina Winkler", "Daniel Worrall", "Emiel Hoogeboom", "Max Welling"], "pdf": "/pdf/c5c80e5f52349ab77e4cc93b2f766836d0664bc1.pdf", "abstract": "Normalizing Flows (NFs) are able to model complicated distributions p(y) with strong inter-dimensional correlations and high multimodality by transforming a simple base density p(z) through an invertible neural network under the change of variables formula. Such behavior is desirable in multivariate structured prediction tasks, where handcrafted per-pixel loss-based methods inadequately capture strong correlations between output dimensions. We present a study of conditional normalizing flows (CNFs), a class of NFs where the base density to output space mapping is conditioned on an input x, to model conditional densities p(y|x). CNFs are efficient in sampling and inference, they can be trained with a likelihood-based objective, and CNFs, being generative flows, do not suffer from mode collapse or training instabilities. We provide an effective method to train continuous CNFs for binary problems and in particular, we apply these CNFs to super-resolution and vessel segmentation tasks demonstrating competitive performance on standard benchmark datasets in terms of likelihood and conventional metrics.", "keywords": ["Likelihood learning", "conditional normalizing flows", "generative modelling", "super-resolution", "vessel segmentation"], "paperhash": "winkler|learning_likelihoods_with_conditional_normalizing_flows", "original_pdf": "/attachment/1ba61e405382141129c54f53172139fd411e157e.pdf", "_bibtex": "@misc{\nwinkler2020learning,\ntitle={Learning Likelihoods with Conditional Normalizing Flows },\nauthor={Christina Winkler and Daniel Worrall and Emiel Hoogeboom and Max Welling},\nyear={2020},\nurl={https://openreview.net/forum?id=rJg3zxBYwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rJg3zxBYwH", "replyto": "rJg3zxBYwH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2189/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2189/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576141185231, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2189/Reviewers"], "noninvitees": [], "tcdate": 1570237726426, "tmdate": 1576141185244, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2189/-/Official_Review"}}}, {"id": "BklyFUW-9r", "original": null, "number": 3, "cdate": 1572046454895, "ddate": null, "tcdate": 1572046454895, "tmdate": 1572972371367, "tddate": null, "forum": "rJg3zxBYwH", "replyto": "rJg3zxBYwH", "invitation": "ICLR.cc/2020/Conference/Paper2189/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "This paper presented the conditional normalizing flows (CNFs) as a new kind of likelihood-based learning objective.  There are two keys in CNFs. One is the parametric mapping function f_{\\phi} and the other is the conditional prior. This paper assumed the conditional prior as Gaussian distribution of x. The mapping function is invertible with x as a parameter. The prior parameter and \\phi are updated by stochastic gradient descent. The latent variable z is then sampled from conditional prior. The output targe y is obtained with dependency on x and f_{\\phi}. \n\nStrength:\n1. This study adopted the flow-based model to estimate the conditional flow without using any generative model or adversarial method.\n2. This method obtained the advanced results on DRIU dataset without the requirement of pretraining.\n3. This paper proposed an useful solution to train continuous CNFs for binary problems.\n\nWeakness:\n1. It is required to address how to design the function f_{\\phi} which depends on x. In particular, the property invertibility should be clarified.\n2. Why the issues of mode collapse or training instability in flow are considerable in the experiments?\n3. It will be meaningful to evaluate this method by performing the tasks on text to image or label to image."}, "signatures": ["ICLR.cc/2020/Conference/Paper2189/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2189/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["christina.winkler.94@gmail.com", "d.e.worrall@uva.nl", "e.hoogeboom@uva.nl", "m.welling@uva.nl"], "title": "Learning Likelihoods with Conditional Normalizing Flows ", "authors": ["Christina Winkler", "Daniel Worrall", "Emiel Hoogeboom", "Max Welling"], "pdf": "/pdf/c5c80e5f52349ab77e4cc93b2f766836d0664bc1.pdf", "abstract": "Normalizing Flows (NFs) are able to model complicated distributions p(y) with strong inter-dimensional correlations and high multimodality by transforming a simple base density p(z) through an invertible neural network under the change of variables formula. Such behavior is desirable in multivariate structured prediction tasks, where handcrafted per-pixel loss-based methods inadequately capture strong correlations between output dimensions. We present a study of conditional normalizing flows (CNFs), a class of NFs where the base density to output space mapping is conditioned on an input x, to model conditional densities p(y|x). CNFs are efficient in sampling and inference, they can be trained with a likelihood-based objective, and CNFs, being generative flows, do not suffer from mode collapse or training instabilities. We provide an effective method to train continuous CNFs for binary problems and in particular, we apply these CNFs to super-resolution and vessel segmentation tasks demonstrating competitive performance on standard benchmark datasets in terms of likelihood and conventional metrics.", "keywords": ["Likelihood learning", "conditional normalizing flows", "generative modelling", "super-resolution", "vessel segmentation"], "paperhash": "winkler|learning_likelihoods_with_conditional_normalizing_flows", "original_pdf": "/attachment/1ba61e405382141129c54f53172139fd411e157e.pdf", "_bibtex": "@misc{\nwinkler2020learning,\ntitle={Learning Likelihoods with Conditional Normalizing Flows },\nauthor={Christina Winkler and Daniel Worrall and Emiel Hoogeboom and Max Welling},\nyear={2020},\nurl={https://openreview.net/forum?id=rJg3zxBYwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rJg3zxBYwH", "replyto": "rJg3zxBYwH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2189/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2189/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576141185231, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2189/Reviewers"], "noninvitees": [], "tcdate": 1570237726426, "tmdate": 1576141185244, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2189/-/Official_Review"}}}, {"id": "H1l055o6qB", "original": null, "number": 5, "cdate": 1572874901795, "ddate": null, "tcdate": 1572874901795, "tmdate": 1572874901795, "tddate": null, "forum": "rJg3zxBYwH", "replyto": "SJxUsi1j9H", "invitation": "ICLR.cc/2020/Conference/Paper2189/-/Public_Comment", "content": {"title": "MAP vs MLE", "comment": "Hello,\n\nI apologize, I misunderstood what was meant by MAP (I took it to mean the MAP w.r.t. x of p(x|y), as would be learned by a standard feed-forward regression model).\nIn this case, I agree with the distinction you make.\n\nSo as I understand it, the practical difference between the two training procedures (MAP/MLE) is whether L2 weight regularization is applied to the network weights or not."}, "signatures": ["~Lynton_Ardizzone1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Lynton_Ardizzone1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["christina.winkler.94@gmail.com", "d.e.worrall@uva.nl", "e.hoogeboom@uva.nl", "m.welling@uva.nl"], "title": "Learning Likelihoods with Conditional Normalizing Flows ", "authors": ["Christina Winkler", "Daniel Worrall", "Emiel Hoogeboom", "Max Welling"], "pdf": "/pdf/c5c80e5f52349ab77e4cc93b2f766836d0664bc1.pdf", "abstract": "Normalizing Flows (NFs) are able to model complicated distributions p(y) with strong inter-dimensional correlations and high multimodality by transforming a simple base density p(z) through an invertible neural network under the change of variables formula. Such behavior is desirable in multivariate structured prediction tasks, where handcrafted per-pixel loss-based methods inadequately capture strong correlations between output dimensions. We present a study of conditional normalizing flows (CNFs), a class of NFs where the base density to output space mapping is conditioned on an input x, to model conditional densities p(y|x). CNFs are efficient in sampling and inference, they can be trained with a likelihood-based objective, and CNFs, being generative flows, do not suffer from mode collapse or training instabilities. We provide an effective method to train continuous CNFs for binary problems and in particular, we apply these CNFs to super-resolution and vessel segmentation tasks demonstrating competitive performance on standard benchmark datasets in terms of likelihood and conventional metrics.", "keywords": ["Likelihood learning", "conditional normalizing flows", "generative modelling", "super-resolution", "vessel segmentation"], "paperhash": "winkler|learning_likelihoods_with_conditional_normalizing_flows", "original_pdf": "/attachment/1ba61e405382141129c54f53172139fd411e157e.pdf", "_bibtex": "@misc{\nwinkler2020learning,\ntitle={Learning Likelihoods with Conditional Normalizing Flows },\nauthor={Christina Winkler and Daniel Worrall and Emiel Hoogeboom and Max Welling},\nyear={2020},\nurl={https://openreview.net/forum?id=rJg3zxBYwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJg3zxBYwH", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504183859, "tmdate": 1576860580518, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper2189/Authors", "ICLR.cc/2020/Conference/Paper2189/Reviewers", "ICLR.cc/2020/Conference/Paper2189/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2189/-/Public_Comment"}}}, {"id": "SJxUsi1j9H", "original": null, "number": 6, "cdate": 1572694941541, "ddate": null, "tcdate": 1572694941541, "tmdate": 1572694941541, "tddate": null, "forum": "rJg3zxBYwH", "replyto": "S1eCD0Vv9H", "invitation": "ICLR.cc/2020/Conference/Paper2189/-/Official_Comment", "content": {"title": "MAP vs MLE", "comment": "Hi Lynton, \n\nThank you for your reply. We agree with you that eq. 4 is the maximum likelihood.\n\nHowever in your paper you say that you minimize the loss as the negative logarithm of:\np(theta | x, c) proportional to p(x | c, theta) p(theta). (Eq. 5 & 6 in your paper)\n\nThe paper refers to this as the \"posterior over model parameters\". Perhaps you could explain what you think is the difference between MLE and MAP? \n\nBest,\nThe authors"}, "signatures": ["ICLR.cc/2020/Conference/Paper2189/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2189/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["christina.winkler.94@gmail.com", "d.e.worrall@uva.nl", "e.hoogeboom@uva.nl", "m.welling@uva.nl"], "title": "Learning Likelihoods with Conditional Normalizing Flows ", "authors": ["Christina Winkler", "Daniel Worrall", "Emiel Hoogeboom", "Max Welling"], "pdf": "/pdf/c5c80e5f52349ab77e4cc93b2f766836d0664bc1.pdf", "abstract": "Normalizing Flows (NFs) are able to model complicated distributions p(y) with strong inter-dimensional correlations and high multimodality by transforming a simple base density p(z) through an invertible neural network under the change of variables formula. Such behavior is desirable in multivariate structured prediction tasks, where handcrafted per-pixel loss-based methods inadequately capture strong correlations between output dimensions. We present a study of conditional normalizing flows (CNFs), a class of NFs where the base density to output space mapping is conditioned on an input x, to model conditional densities p(y|x). CNFs are efficient in sampling and inference, they can be trained with a likelihood-based objective, and CNFs, being generative flows, do not suffer from mode collapse or training instabilities. We provide an effective method to train continuous CNFs for binary problems and in particular, we apply these CNFs to super-resolution and vessel segmentation tasks demonstrating competitive performance on standard benchmark datasets in terms of likelihood and conventional metrics.", "keywords": ["Likelihood learning", "conditional normalizing flows", "generative modelling", "super-resolution", "vessel segmentation"], "paperhash": "winkler|learning_likelihoods_with_conditional_normalizing_flows", "original_pdf": "/attachment/1ba61e405382141129c54f53172139fd411e157e.pdf", "_bibtex": "@misc{\nwinkler2020learning,\ntitle={Learning Likelihoods with Conditional Normalizing Flows },\nauthor={Christina Winkler and Daniel Worrall and Emiel Hoogeboom and Max Welling},\nyear={2020},\nurl={https://openreview.net/forum?id=rJg3zxBYwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJg3zxBYwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2189/Authors", "ICLR.cc/2020/Conference/Paper2189/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2189/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2189/Reviewers", "ICLR.cc/2020/Conference/Paper2189/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2189/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2189/Authors|ICLR.cc/2020/Conference/Paper2189/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504145030, "tmdate": 1576860547165, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2189/Authors", "ICLR.cc/2020/Conference/Paper2189/Reviewers", "ICLR.cc/2020/Conference/Paper2189/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2189/-/Official_Comment"}}}, {"id": "S1eCD0Vv9H", "original": null, "number": 4, "cdate": 1572453989623, "ddate": null, "tcdate": 1572453989623, "tmdate": 1572454117581, "tddate": null, "forum": "rJg3zxBYwH", "replyto": "Skx3ofIe5r", "invitation": "ICLR.cc/2020/Conference/Paper2189/-/Public_Comment", "content": {"title": "Response to prior work", "comment": "Hi,\n\nI am sorry in case our notation is confusing, we will change it in a future revision in this case. But I am confident we use the same loss, because we also use the standard maximum likelihood loss used to train normalizing flows. Put simply,\n\nL = 0.5 * z^2 - log(det(J))\n\nWith latent vector z and Jacobian J.\nThe loss is only applied in z-space, not on the actual images (therefore not MAP).\n\nYour eq. 7 is the same as our eq. 4 (without the conditional split prior), and we directly optimize the negative logarithm of this, in the same way as you.\n\nSo I feel you have misunderstood our training procedure. (Perhaps it is also the case, because we use the term 'backwards' and 'fowards' with regards to the flow in the opposite way, calling X -> Z 'forward')"}, "signatures": ["~Lynton_Ardizzone1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Lynton_Ardizzone1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["christina.winkler.94@gmail.com", "d.e.worrall@uva.nl", "e.hoogeboom@uva.nl", "m.welling@uva.nl"], "title": "Learning Likelihoods with Conditional Normalizing Flows ", "authors": ["Christina Winkler", "Daniel Worrall", "Emiel Hoogeboom", "Max Welling"], "pdf": "/pdf/c5c80e5f52349ab77e4cc93b2f766836d0664bc1.pdf", "abstract": "Normalizing Flows (NFs) are able to model complicated distributions p(y) with strong inter-dimensional correlations and high multimodality by transforming a simple base density p(z) through an invertible neural network under the change of variables formula. Such behavior is desirable in multivariate structured prediction tasks, where handcrafted per-pixel loss-based methods inadequately capture strong correlations between output dimensions. We present a study of conditional normalizing flows (CNFs), a class of NFs where the base density to output space mapping is conditioned on an input x, to model conditional densities p(y|x). CNFs are efficient in sampling and inference, they can be trained with a likelihood-based objective, and CNFs, being generative flows, do not suffer from mode collapse or training instabilities. We provide an effective method to train continuous CNFs for binary problems and in particular, we apply these CNFs to super-resolution and vessel segmentation tasks demonstrating competitive performance on standard benchmark datasets in terms of likelihood and conventional metrics.", "keywords": ["Likelihood learning", "conditional normalizing flows", "generative modelling", "super-resolution", "vessel segmentation"], "paperhash": "winkler|learning_likelihoods_with_conditional_normalizing_flows", "original_pdf": "/attachment/1ba61e405382141129c54f53172139fd411e157e.pdf", "_bibtex": "@misc{\nwinkler2020learning,\ntitle={Learning Likelihoods with Conditional Normalizing Flows },\nauthor={Christina Winkler and Daniel Worrall and Emiel Hoogeboom and Max Welling},\nyear={2020},\nurl={https://openreview.net/forum?id=rJg3zxBYwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJg3zxBYwH", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504183859, "tmdate": 1576860580518, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper2189/Authors", "ICLR.cc/2020/Conference/Paper2189/Reviewers", "ICLR.cc/2020/Conference/Paper2189/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2189/-/Public_Comment"}}}, {"id": "Skx3ofIe5r", "original": null, "number": 5, "cdate": 1572000419770, "ddate": null, "tcdate": 1572000419770, "tmdate": 1572000419770, "tddate": null, "forum": "rJg3zxBYwH", "replyto": "r1lfgEbJ9r", "invitation": "ICLR.cc/2020/Conference/Paper2189/-/Official_Comment", "content": {"title": "Response to prior work", "comment": "Thanks for acknowledging the differences in the architecture and dequantization.\n\nWe do have one disagreement though, which we would like to flag about the training objective. In Sec 3.2 of your paper, equations 5 and 6 denote the (unnormalized) log-posterior distribution of the weights of your flow given the data. Therefore, it seems from the paper that you are performing maximum a posteriori (MAP) model fitting of your weights. \n\nBest,\nThe authors"}, "signatures": ["ICLR.cc/2020/Conference/Paper2189/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2189/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["christina.winkler.94@gmail.com", "d.e.worrall@uva.nl", "e.hoogeboom@uva.nl", "m.welling@uva.nl"], "title": "Learning Likelihoods with Conditional Normalizing Flows ", "authors": ["Christina Winkler", "Daniel Worrall", "Emiel Hoogeboom", "Max Welling"], "pdf": "/pdf/c5c80e5f52349ab77e4cc93b2f766836d0664bc1.pdf", "abstract": "Normalizing Flows (NFs) are able to model complicated distributions p(y) with strong inter-dimensional correlations and high multimodality by transforming a simple base density p(z) through an invertible neural network under the change of variables formula. Such behavior is desirable in multivariate structured prediction tasks, where handcrafted per-pixel loss-based methods inadequately capture strong correlations between output dimensions. We present a study of conditional normalizing flows (CNFs), a class of NFs where the base density to output space mapping is conditioned on an input x, to model conditional densities p(y|x). CNFs are efficient in sampling and inference, they can be trained with a likelihood-based objective, and CNFs, being generative flows, do not suffer from mode collapse or training instabilities. We provide an effective method to train continuous CNFs for binary problems and in particular, we apply these CNFs to super-resolution and vessel segmentation tasks demonstrating competitive performance on standard benchmark datasets in terms of likelihood and conventional metrics.", "keywords": ["Likelihood learning", "conditional normalizing flows", "generative modelling", "super-resolution", "vessel segmentation"], "paperhash": "winkler|learning_likelihoods_with_conditional_normalizing_flows", "original_pdf": "/attachment/1ba61e405382141129c54f53172139fd411e157e.pdf", "_bibtex": "@misc{\nwinkler2020learning,\ntitle={Learning Likelihoods with Conditional Normalizing Flows },\nauthor={Christina Winkler and Daniel Worrall and Emiel Hoogeboom and Max Welling},\nyear={2020},\nurl={https://openreview.net/forum?id=rJg3zxBYwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJg3zxBYwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2189/Authors", "ICLR.cc/2020/Conference/Paper2189/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2189/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2189/Reviewers", "ICLR.cc/2020/Conference/Paper2189/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2189/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2189/Authors|ICLR.cc/2020/Conference/Paper2189/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504145030, "tmdate": 1576860547165, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2189/Authors", "ICLR.cc/2020/Conference/Paper2189/Reviewers", "ICLR.cc/2020/Conference/Paper2189/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2189/-/Official_Comment"}}}, {"id": "r1lfgEbJ9r", "original": null, "number": 3, "cdate": 1571914730136, "ddate": null, "tcdate": 1571914730136, "tmdate": 1571914730136, "tddate": null, "forum": "rJg3zxBYwH", "replyto": "B1ggTdXDKS", "invitation": "ICLR.cc/2020/Conference/Paper2189/-/Public_Comment", "content": {"title": "Response to prior work", "comment": "Hello,\n\nthank you for the response!\n\nWe agree with the differences in architecture and dequantization.\n\nConcerning the training:\n- We also train as a normalizing flow using maximum likelihood training (see our Sec. 3.2).\n- The main difference we see, is that you use a conditional split prior in addition to the conditional Flow, whereas we only have a conditional flow, with a fixed unconditional prior.\n- It may be informative performing an ablation, to demonstrate the improvement produced by the more flexible conditional prior.\n\nBest,\nLynton"}, "signatures": ["~Lynton_Ardizzone1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Lynton_Ardizzone1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["christina.winkler.94@gmail.com", "d.e.worrall@uva.nl", "e.hoogeboom@uva.nl", "m.welling@uva.nl"], "title": "Learning Likelihoods with Conditional Normalizing Flows ", "authors": ["Christina Winkler", "Daniel Worrall", "Emiel Hoogeboom", "Max Welling"], "pdf": "/pdf/c5c80e5f52349ab77e4cc93b2f766836d0664bc1.pdf", "abstract": "Normalizing Flows (NFs) are able to model complicated distributions p(y) with strong inter-dimensional correlations and high multimodality by transforming a simple base density p(z) through an invertible neural network under the change of variables formula. Such behavior is desirable in multivariate structured prediction tasks, where handcrafted per-pixel loss-based methods inadequately capture strong correlations between output dimensions. We present a study of conditional normalizing flows (CNFs), a class of NFs where the base density to output space mapping is conditioned on an input x, to model conditional densities p(y|x). CNFs are efficient in sampling and inference, they can be trained with a likelihood-based objective, and CNFs, being generative flows, do not suffer from mode collapse or training instabilities. We provide an effective method to train continuous CNFs for binary problems and in particular, we apply these CNFs to super-resolution and vessel segmentation tasks demonstrating competitive performance on standard benchmark datasets in terms of likelihood and conventional metrics.", "keywords": ["Likelihood learning", "conditional normalizing flows", "generative modelling", "super-resolution", "vessel segmentation"], "paperhash": "winkler|learning_likelihoods_with_conditional_normalizing_flows", "original_pdf": "/attachment/1ba61e405382141129c54f53172139fd411e157e.pdf", "_bibtex": "@misc{\nwinkler2020learning,\ntitle={Learning Likelihoods with Conditional Normalizing Flows },\nauthor={Christina Winkler and Daniel Worrall and Emiel Hoogeboom and Max Welling},\nyear={2020},\nurl={https://openreview.net/forum?id=rJg3zxBYwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJg3zxBYwH", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504183859, "tmdate": 1576860580518, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper2189/Authors", "ICLR.cc/2020/Conference/Paper2189/Reviewers", "ICLR.cc/2020/Conference/Paper2189/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2189/-/Public_Comment"}}}, {"id": "B1ggTdXDKS", "original": null, "number": 2, "cdate": 1571399864101, "ddate": null, "tcdate": 1571399864101, "tmdate": 1571399864101, "tddate": null, "forum": "rJg3zxBYwH", "replyto": "S1xLH2ZGYr", "invitation": "ICLR.cc/2020/Conference/Paper2189/-/Official_Comment", "content": {"comment": "Hi Lynton, \n\nThanks for the reference. It looks like a very nice paper. It certainly is relevant and we shall of course include it in our related work.\n\nTo answer your question about differences and similarities, here is a brief list:\n\n- Architecture: To some degree our architectures are similar. We do indeed both use conditional affine coupling layers (cACL). Perhaps the largest difference is that you couple two cACLs together; whereas, we use a single cACL followed by a learnable, (nonconditional) 1x1 convolution (you refer to this as a soft channel permutation). Furthermore, we deploy a dequantization network (more below).\n\n- Dequantization: We introduce a new variational dequantization scheme, which builds on the work of Flow++, (Ho et al., 2019). This works for binary data spaces. Furthermore, we make a connection between variational dequantization and variational inference, which allows us to generalize the binning scheme of Flow++.\n\n- Per-pixel loss interpretation: We make explicit the disadvantages of previous per-pixel reconstruction losses, which forms the motivation for why we would wish to use a flow.\n\n- ML versus MAP: We do maximum likelihood, which is well known to be parameterization invariant instead of, say, MAP inference.\n\nWe hope this answers your questions. If you have more, do feel free to let us know.\n\nBest,\nThe authors", "title": "Response to prior work"}, "signatures": ["ICLR.cc/2020/Conference/Paper2189/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2189/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["christina.winkler.94@gmail.com", "d.e.worrall@uva.nl", "e.hoogeboom@uva.nl", "m.welling@uva.nl"], "title": "Learning Likelihoods with Conditional Normalizing Flows ", "authors": ["Christina Winkler", "Daniel Worrall", "Emiel Hoogeboom", "Max Welling"], "pdf": "/pdf/c5c80e5f52349ab77e4cc93b2f766836d0664bc1.pdf", "abstract": "Normalizing Flows (NFs) are able to model complicated distributions p(y) with strong inter-dimensional correlations and high multimodality by transforming a simple base density p(z) through an invertible neural network under the change of variables formula. Such behavior is desirable in multivariate structured prediction tasks, where handcrafted per-pixel loss-based methods inadequately capture strong correlations between output dimensions. We present a study of conditional normalizing flows (CNFs), a class of NFs where the base density to output space mapping is conditioned on an input x, to model conditional densities p(y|x). CNFs are efficient in sampling and inference, they can be trained with a likelihood-based objective, and CNFs, being generative flows, do not suffer from mode collapse or training instabilities. We provide an effective method to train continuous CNFs for binary problems and in particular, we apply these CNFs to super-resolution and vessel segmentation tasks demonstrating competitive performance on standard benchmark datasets in terms of likelihood and conventional metrics.", "keywords": ["Likelihood learning", "conditional normalizing flows", "generative modelling", "super-resolution", "vessel segmentation"], "paperhash": "winkler|learning_likelihoods_with_conditional_normalizing_flows", "original_pdf": "/attachment/1ba61e405382141129c54f53172139fd411e157e.pdf", "_bibtex": "@misc{\nwinkler2020learning,\ntitle={Learning Likelihoods with Conditional Normalizing Flows },\nauthor={Christina Winkler and Daniel Worrall and Emiel Hoogeboom and Max Welling},\nyear={2020},\nurl={https://openreview.net/forum?id=rJg3zxBYwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJg3zxBYwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2189/Authors", "ICLR.cc/2020/Conference/Paper2189/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2189/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2189/Reviewers", "ICLR.cc/2020/Conference/Paper2189/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2189/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2189/Authors|ICLR.cc/2020/Conference/Paper2189/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504145030, "tmdate": 1576860547165, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2189/Authors", "ICLR.cc/2020/Conference/Paper2189/Reviewers", "ICLR.cc/2020/Conference/Paper2189/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2189/-/Official_Comment"}}}, {"id": "S1xLH2ZGYr", "original": null, "number": 2, "cdate": 1571064893540, "ddate": null, "tcdate": 1571064893540, "tmdate": 1571066804681, "tddate": null, "forum": "rJg3zxBYwH", "replyto": "rJg3zxBYwH", "invitation": "ICLR.cc/2020/Conference/Paper2189/-/Public_Comment", "content": {"comment": "Hello,\n\nwe would like to point out our work, which was published on arxiv on July 4th this year:\n\"Guided Image Generation with Conditional Invertible Neural Networks\" (https://arxiv.org/abs/1907.02392 ),\nwhich is very similar in the approach, and is also applied to an inverse problem in computer vision (colorization).\n\nWe feel this should be included in the related work, and differentiated from your own contributions.\n\nBest regards,\nLynton Ardizzone\nVisual Learning Lab Heidelberg", "title": "Prior Work"}, "signatures": ["~Lynton_Ardizzone1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Lynton_Ardizzone1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["christina.winkler.94@gmail.com", "d.e.worrall@uva.nl", "e.hoogeboom@uva.nl", "m.welling@uva.nl"], "title": "Learning Likelihoods with Conditional Normalizing Flows ", "authors": ["Christina Winkler", "Daniel Worrall", "Emiel Hoogeboom", "Max Welling"], "pdf": "/pdf/c5c80e5f52349ab77e4cc93b2f766836d0664bc1.pdf", "abstract": "Normalizing Flows (NFs) are able to model complicated distributions p(y) with strong inter-dimensional correlations and high multimodality by transforming a simple base density p(z) through an invertible neural network under the change of variables formula. Such behavior is desirable in multivariate structured prediction tasks, where handcrafted per-pixel loss-based methods inadequately capture strong correlations between output dimensions. We present a study of conditional normalizing flows (CNFs), a class of NFs where the base density to output space mapping is conditioned on an input x, to model conditional densities p(y|x). CNFs are efficient in sampling and inference, they can be trained with a likelihood-based objective, and CNFs, being generative flows, do not suffer from mode collapse or training instabilities. We provide an effective method to train continuous CNFs for binary problems and in particular, we apply these CNFs to super-resolution and vessel segmentation tasks demonstrating competitive performance on standard benchmark datasets in terms of likelihood and conventional metrics.", "keywords": ["Likelihood learning", "conditional normalizing flows", "generative modelling", "super-resolution", "vessel segmentation"], "paperhash": "winkler|learning_likelihoods_with_conditional_normalizing_flows", "original_pdf": "/attachment/1ba61e405382141129c54f53172139fd411e157e.pdf", "_bibtex": "@misc{\nwinkler2020learning,\ntitle={Learning Likelihoods with Conditional Normalizing Flows },\nauthor={Christina Winkler and Daniel Worrall and Emiel Hoogeboom and Max Welling},\nyear={2020},\nurl={https://openreview.net/forum?id=rJg3zxBYwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJg3zxBYwH", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504183859, "tmdate": 1576860580518, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper2189/Authors", "ICLR.cc/2020/Conference/Paper2189/Reviewers", "ICLR.cc/2020/Conference/Paper2189/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2189/-/Public_Comment"}}}, {"id": "BkegD4JfKS", "original": null, "number": 1, "cdate": 1571054680286, "ddate": null, "tcdate": 1571054680286, "tmdate": 1571054680286, "tddate": null, "forum": "rJg3zxBYwH", "replyto": "Hklnft3zdS", "invitation": "ICLR.cc/2020/Conference/Paper2189/-/Official_Comment", "content": {"comment": "Thank you for your comment and the reference. We will include it in our paper.", "title": "Thank you "}, "signatures": ["ICLR.cc/2020/Conference/Paper2189/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2189/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["christina.winkler.94@gmail.com", "d.e.worrall@uva.nl", "e.hoogeboom@uva.nl", "m.welling@uva.nl"], "title": "Learning Likelihoods with Conditional Normalizing Flows ", "authors": ["Christina Winkler", "Daniel Worrall", "Emiel Hoogeboom", "Max Welling"], "pdf": "/pdf/c5c80e5f52349ab77e4cc93b2f766836d0664bc1.pdf", "abstract": "Normalizing Flows (NFs) are able to model complicated distributions p(y) with strong inter-dimensional correlations and high multimodality by transforming a simple base density p(z) through an invertible neural network under the change of variables formula. Such behavior is desirable in multivariate structured prediction tasks, where handcrafted per-pixel loss-based methods inadequately capture strong correlations between output dimensions. We present a study of conditional normalizing flows (CNFs), a class of NFs where the base density to output space mapping is conditioned on an input x, to model conditional densities p(y|x). CNFs are efficient in sampling and inference, they can be trained with a likelihood-based objective, and CNFs, being generative flows, do not suffer from mode collapse or training instabilities. We provide an effective method to train continuous CNFs for binary problems and in particular, we apply these CNFs to super-resolution and vessel segmentation tasks demonstrating competitive performance on standard benchmark datasets in terms of likelihood and conventional metrics.", "keywords": ["Likelihood learning", "conditional normalizing flows", "generative modelling", "super-resolution", "vessel segmentation"], "paperhash": "winkler|learning_likelihoods_with_conditional_normalizing_flows", "original_pdf": "/attachment/1ba61e405382141129c54f53172139fd411e157e.pdf", "_bibtex": "@misc{\nwinkler2020learning,\ntitle={Learning Likelihoods with Conditional Normalizing Flows },\nauthor={Christina Winkler and Daniel Worrall and Emiel Hoogeboom and Max Welling},\nyear={2020},\nurl={https://openreview.net/forum?id=rJg3zxBYwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJg3zxBYwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2189/Authors", "ICLR.cc/2020/Conference/Paper2189/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2189/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2189/Reviewers", "ICLR.cc/2020/Conference/Paper2189/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2189/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2189/Authors|ICLR.cc/2020/Conference/Paper2189/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504145030, "tmdate": 1576860547165, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2189/Authors", "ICLR.cc/2020/Conference/Paper2189/Reviewers", "ICLR.cc/2020/Conference/Paper2189/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2189/-/Official_Comment"}}}, {"id": "Hklnft3zdS", "original": null, "number": 1, "cdate": 1570060564353, "ddate": null, "tcdate": 1570060564353, "tmdate": 1570060564353, "tddate": null, "forum": "rJg3zxBYwH", "replyto": "rJg3zxBYwH", "invitation": "ICLR.cc/2020/Conference/Paper2189/-/Public_Comment", "content": {"comment": "I'm a strong advocate of moving beyond the limitations of parametric distributions by using normalizing flows, so I'm happy to see the nice set of experiments in this paper. Best of luck! You might consider including the following, somewhat obscure, reference:\n\nDeep Variational Inference Without Pixel-Wise Reconstruction, Agrawal & Dukkipati, 2016, (https://arxiv.org/abs/1611.05209)\n\nThey parameterize the conditional likelihood in a variational autoencoder using normalizing flows.", "title": "Great idea, nice set of experiments, and an additional reference"}, "signatures": ["~Joseph_Marino1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Joseph_Marino1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["christina.winkler.94@gmail.com", "d.e.worrall@uva.nl", "e.hoogeboom@uva.nl", "m.welling@uva.nl"], "title": "Learning Likelihoods with Conditional Normalizing Flows ", "authors": ["Christina Winkler", "Daniel Worrall", "Emiel Hoogeboom", "Max Welling"], "pdf": "/pdf/c5c80e5f52349ab77e4cc93b2f766836d0664bc1.pdf", "abstract": "Normalizing Flows (NFs) are able to model complicated distributions p(y) with strong inter-dimensional correlations and high multimodality by transforming a simple base density p(z) through an invertible neural network under the change of variables formula. Such behavior is desirable in multivariate structured prediction tasks, where handcrafted per-pixel loss-based methods inadequately capture strong correlations between output dimensions. We present a study of conditional normalizing flows (CNFs), a class of NFs where the base density to output space mapping is conditioned on an input x, to model conditional densities p(y|x). CNFs are efficient in sampling and inference, they can be trained with a likelihood-based objective, and CNFs, being generative flows, do not suffer from mode collapse or training instabilities. We provide an effective method to train continuous CNFs for binary problems and in particular, we apply these CNFs to super-resolution and vessel segmentation tasks demonstrating competitive performance on standard benchmark datasets in terms of likelihood and conventional metrics.", "keywords": ["Likelihood learning", "conditional normalizing flows", "generative modelling", "super-resolution", "vessel segmentation"], "paperhash": "winkler|learning_likelihoods_with_conditional_normalizing_flows", "original_pdf": "/attachment/1ba61e405382141129c54f53172139fd411e157e.pdf", "_bibtex": "@misc{\nwinkler2020learning,\ntitle={Learning Likelihoods with Conditional Normalizing Flows },\nauthor={Christina Winkler and Daniel Worrall and Emiel Hoogeboom and Max Welling},\nyear={2020},\nurl={https://openreview.net/forum?id=rJg3zxBYwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJg3zxBYwH", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504183859, "tmdate": 1576860580518, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper2189/Authors", "ICLR.cc/2020/Conference/Paper2189/Reviewers", "ICLR.cc/2020/Conference/Paper2189/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2189/-/Public_Comment"}}}], "count": 17}