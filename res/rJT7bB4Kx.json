{"notes": [{"tddate": null, "ddate": null, "cdate": null, "original": null, "tmdate": 1490028583877, "tcdate": 1490028583877, "number": 1, "id": "r1xVdF6je", "invitation": "ICLR.cc/2017/workshop/-/paper75/acceptance", "forum": "rJT7bB4Kx", "replyto": "rJT7bB4Kx", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Multimodal Compact Bilinear Pooling for Multimodal Neural Machine Translation", "abstract": "In state-of-the-art Neural Machine Translation, an attention mechanism is used during decoding to enhance the translation. At every step, the decoder uses this mechanism to focus on different parts of the source sentence to gather the most useful information before outputting its target word. Recently, the effectiveness of the attention mechanism has also been explored for multimodal tasks, where it becomes possible to focus both on sentence parts and image regions. Approaches to pool two modalities usually include element-wise product, sum or concatenation.\nIn this paper, we evaluate the more advanced Multimodal Compact Bilinear pooling method, which takes the outer product of two vectors to combine the attention features for the two modalities. This has been previously investigated for visual question answering. We try out this approach for multimodal image caption translation and show improvements compared to basic combination methods.", "pdf": "/pdf/145fd5bb7d7c3f72e070dede27aafb646a8cf9fe.pdf", "paperhash": "delbrouck|multimodal_compact_bilinear_pooling_for_multimodal_neural_machine_translation", "keywords": ["Natural language processing"], "conflicts": ["umons.ac.be"], "authors": ["Jean-Benoit Delbrouck", "Stephane Dupont"], "authorids": ["Jean-Benoit.DELBROUCK@umons.ac.be", "Stephane.DUPONT@umons.ac.be"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1490028584415, "id": "ICLR.cc/2017/workshop/-/paper75/acceptance", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "rJT7bB4Kx", "replyto": "rJT7bB4Kx", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept", "Reject"]}}}, "nonreaders": [], "cdate": 1490028584415}}}, {"tddate": null, "tmdate": 1489750476056, "tcdate": 1489750476056, "number": 3, "id": "SJN0KBKox", "invitation": "ICLR.cc/2017/workshop/-/paper75/public/comment", "forum": "rJT7bB4Kx", "replyto": "B17g-Uesg", "signatures": ["~Jean-Benoit_Delbrouck1"], "readers": ["everyone"], "writers": ["~Jean-Benoit_Delbrouck1"], "content": {"title": "No title", "comment": "Dear reviewer,\n\nThank you for your comment.\n\nWe agree that our statement you quote is incorrect and has been taken out of our draft. The \"previous work\" has also been updated according to your comments.\n\nAs stated in our first review's response (see below), our main focus wasn't to compare our work to any monomodal or multimodal baseline but rather to show that more complex combination techniques help a system to translate better. Yet, we agree that to compare our work to others, and therefore to give it a more significant impact, we should have used state of the art models like [Iacer 2016].\n\nAlso, we'll try to enhance the explanation of the proposed attention models in this paper by reducing the basic model section, which could be easily shortened.\n\nBest,"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Multimodal Compact Bilinear Pooling for Multimodal Neural Machine Translation", "abstract": "In state-of-the-art Neural Machine Translation, an attention mechanism is used during decoding to enhance the translation. At every step, the decoder uses this mechanism to focus on different parts of the source sentence to gather the most useful information before outputting its target word. Recently, the effectiveness of the attention mechanism has also been explored for multimodal tasks, where it becomes possible to focus both on sentence parts and image regions. Approaches to pool two modalities usually include element-wise product, sum or concatenation.\nIn this paper, we evaluate the more advanced Multimodal Compact Bilinear pooling method, which takes the outer product of two vectors to combine the attention features for the two modalities. This has been previously investigated for visual question answering. We try out this approach for multimodal image caption translation and show improvements compared to basic combination methods.", "pdf": "/pdf/145fd5bb7d7c3f72e070dede27aafb646a8cf9fe.pdf", "paperhash": "delbrouck|multimodal_compact_bilinear_pooling_for_multimodal_neural_machine_translation", "keywords": ["Natural language processing"], "conflicts": ["umons.ac.be"], "authors": ["Jean-Benoit Delbrouck", "Stephane Dupont"], "authorids": ["Jean-Benoit.DELBROUCK@umons.ac.be", "Stephane.DUPONT@umons.ac.be"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487323429938, "tcdate": 1487323429938, "id": "ICLR.cc/2017/workshop/-/paper75/public/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2017/workshop/paper75/reviewers"], "reply": {"forum": "rJT7bB4Kx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1487323429938}}}, {"tddate": null, "nonreaders": null, "tmdate": 1489750018251, "tcdate": 1489749360098, "number": 2, "id": "SJu_HBtox", "invitation": "ICLR.cc/2017/workshop/-/paper75/public/comment", "forum": "rJT7bB4Kx", "replyto": "ByJvq0kcx", "signatures": ["~Jean-Benoit_Delbrouck1"], "readers": ["everyone"], "writers": ["~Jean-Benoit_Delbrouck1"], "content": {"title": "No title", "comment": "Dear reviewer, \n\nThank you for your helpful comment. \n\nThe missing references you pointed out has been added to the paper. \n\nI understand that a weakness is the low performance (Bleu scores) reported in our work.  The main difference is that we dont use dropout which seems to significantly improve the translations. \nOriginally, our main focus wasn't to propose a state of the art model, but rather to show that combining multimodal attention vectors with more complex techniques actually improves the scores, wether or not they are state of the art. Yet, we agree that to compare to previous work, a similar model like [Iacer 2016] should have been used.\n\nAll your further comments, like the lack of precision whilst citing previous work has been taken into account. Our workshop draft has been updated to address these points. "}, "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Multimodal Compact Bilinear Pooling for Multimodal Neural Machine Translation", "abstract": "In state-of-the-art Neural Machine Translation, an attention mechanism is used during decoding to enhance the translation. At every step, the decoder uses this mechanism to focus on different parts of the source sentence to gather the most useful information before outputting its target word. Recently, the effectiveness of the attention mechanism has also been explored for multimodal tasks, where it becomes possible to focus both on sentence parts and image regions. Approaches to pool two modalities usually include element-wise product, sum or concatenation.\nIn this paper, we evaluate the more advanced Multimodal Compact Bilinear pooling method, which takes the outer product of two vectors to combine the attention features for the two modalities. This has been previously investigated for visual question answering. We try out this approach for multimodal image caption translation and show improvements compared to basic combination methods.", "pdf": "/pdf/145fd5bb7d7c3f72e070dede27aafb646a8cf9fe.pdf", "paperhash": "delbrouck|multimodal_compact_bilinear_pooling_for_multimodal_neural_machine_translation", "keywords": ["Natural language processing"], "conflicts": ["umons.ac.be"], "authors": ["Jean-Benoit Delbrouck", "Stephane Dupont"], "authorids": ["Jean-Benoit.DELBROUCK@umons.ac.be", "Stephane.DUPONT@umons.ac.be"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487323429938, "tcdate": 1487323429938, "id": "ICLR.cc/2017/workshop/-/paper75/public/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2017/workshop/paper75/reviewers"], "reply": {"forum": "rJT7bB4Kx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1487323429938}}}, {"tddate": null, "tmdate": 1489423643650, "tcdate": 1489423643650, "number": 1, "id": "rkNm6SEsl", "invitation": "ICLR.cc/2017/workshop/-/paper75/public/comment", "forum": "rJT7bB4Kx", "replyto": "rJT7bB4Kx", "signatures": ["~Desmond_Elliott1"], "readers": ["everyone"], "writers": ["~Desmond_Elliott1"], "content": {"title": "Experimental protocol and a suggestion", "comment": "I like this approach to training a multimodal translation model but the results are difficult to interpret, given the details in the paper. \n\nI encourage you to follow the Shared Task evaluation procedure for measuring the BLEU scores on the test data. This procedure is described on the Shared Task web page (http://www.statmt.org/wmt17/multimodal-task.html) with hyperlinks to the processing scripts. If you follow this procedure, it will make it easier to compare your against other papers.\n\nI also have a suggestion: you may want to use a decompounder on the German vocabulary. 19,000 types is quite high for the German dataset, and this could be reduced to ~ 15,000 by following the exact preprocessing steps described in Caglayan et al. (WMT 2016). A reduced German vocabulary should give you better BLEU scores because the model will be easier to train. You could also think about using the Moses compounder (Koehn et al. (2007)), Byte Pair Encodings (Sennrich et al. (2016)), or a pretrained neural decompounder (Daiber et al. (2016)).\n\nDaiber et al. (2015) http://jodaiber.github.io/doc/compound_analogy.pdf\nKoehn et al .(2007) http://www.aclweb.org/anthology/P07-2045\nCaglayan et al. (2016) https://arxiv.org/abs/1605.09186\nSennrich et al. (2016) https://arxiv.org/abs/1508.07909\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Multimodal Compact Bilinear Pooling for Multimodal Neural Machine Translation", "abstract": "In state-of-the-art Neural Machine Translation, an attention mechanism is used during decoding to enhance the translation. At every step, the decoder uses this mechanism to focus on different parts of the source sentence to gather the most useful information before outputting its target word. Recently, the effectiveness of the attention mechanism has also been explored for multimodal tasks, where it becomes possible to focus both on sentence parts and image regions. Approaches to pool two modalities usually include element-wise product, sum or concatenation.\nIn this paper, we evaluate the more advanced Multimodal Compact Bilinear pooling method, which takes the outer product of two vectors to combine the attention features for the two modalities. This has been previously investigated for visual question answering. We try out this approach for multimodal image caption translation and show improvements compared to basic combination methods.", "pdf": "/pdf/145fd5bb7d7c3f72e070dede27aafb646a8cf9fe.pdf", "paperhash": "delbrouck|multimodal_compact_bilinear_pooling_for_multimodal_neural_machine_translation", "keywords": ["Natural language processing"], "conflicts": ["umons.ac.be"], "authors": ["Jean-Benoit Delbrouck", "Stephane Dupont"], "authorids": ["Jean-Benoit.DELBROUCK@umons.ac.be", "Stephane.DUPONT@umons.ac.be"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487323429938, "tcdate": 1487323429938, "id": "ICLR.cc/2017/workshop/-/paper75/public/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2017/workshop/paper75/reviewers"], "reply": {"forum": "rJT7bB4Kx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1487323429938}}}, {"tddate": null, "tmdate": 1489162474784, "tcdate": 1489162474784, "number": 2, "id": "B17g-Uesg", "invitation": "ICLR.cc/2017/workshop/-/paper75/official/review", "forum": "rJT7bB4Kx", "replyto": "rJT7bB4Kx", "signatures": ["ICLR.cc/2017/workshop/paper75/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper75/AnonReviewer1"], "content": {"title": "results are not consistent with prior work", "rating": "5: Marginally below acceptance threshold", "review": "The paper investigates the problem of combining variable-length information from two different modalities. The specific method considered in the paper is compact bilinear pooling, which is compared against simpler methods in the context of multimodal machine translation. Two versions of the algorithm are considered, which differ in whether the information extracted from text influences the computation of attention weights for the elements of the representation of the image.\n\nAs mentioned by another review, a major issue of the paper is that that the prior work by [Calixto 2016] is not mentioned. Besides the statement \"To our knowledge, there is currently no multimodal translation architecture that convincingly surpass [sic] a monomodal attention baseline\" contradicts the results reported in [Calixto 2016]. They do report an improvement over the text-only NMT. This makes it hard to trust the results of this paper.\n\nThe writing of the paper could be improved. A lot of space is used to explain the basic model, but the proposed methods are explained extremely briefly. A few sentences explaining the compact bilinear pooling could help. The Algorithm 1 is not very helpful without any explanation. Most importantly, the explanation of the pre-attention mechanism, which is perhaps is the main novelty, is very vague.\n\nTypos and minor writing issues:\n- bottom of page 2: c_t^t is rather confusing notation\n- bottom of page 2: \"in a multiplicative but\" - a word is missing\n- beginning of Section 3: I believe it should be \\alpha instead of \\epsilon, and it makes sense to say \"learning rate \\alpha\" to prevent confusion\n\nPros: the idea of pre-attention seems novel\nCons: results are not consistent with the prior work (which has not been mentioned), writing is not clear\n\n[Calixto 2016] Calixto, Iacer, Desmond Elliott, and Stella Frank. \"Dcu-uva multimodal mt system report.\" Proceedings of the First Conference on Machine Translation, Berlin, Germany. 2016.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Multimodal Compact Bilinear Pooling for Multimodal Neural Machine Translation", "abstract": "In state-of-the-art Neural Machine Translation, an attention mechanism is used during decoding to enhance the translation. At every step, the decoder uses this mechanism to focus on different parts of the source sentence to gather the most useful information before outputting its target word. Recently, the effectiveness of the attention mechanism has also been explored for multimodal tasks, where it becomes possible to focus both on sentence parts and image regions. Approaches to pool two modalities usually include element-wise product, sum or concatenation.\nIn this paper, we evaluate the more advanced Multimodal Compact Bilinear pooling method, which takes the outer product of two vectors to combine the attention features for the two modalities. This has been previously investigated for visual question answering. We try out this approach for multimodal image caption translation and show improvements compared to basic combination methods.", "pdf": "/pdf/145fd5bb7d7c3f72e070dede27aafb646a8cf9fe.pdf", "paperhash": "delbrouck|multimodal_compact_bilinear_pooling_for_multimodal_neural_machine_translation", "keywords": ["Natural language processing"], "conflicts": ["umons.ac.be"], "authors": ["Jean-Benoit Delbrouck", "Stephane Dupont"], "authorids": ["Jean-Benoit.DELBROUCK@umons.ac.be", "Stephane.DUPONT@umons.ac.be"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489162475470, "id": "ICLR.cc/2017/workshop/-/paper75/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper75/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper75/AnonReviewer2", "ICLR.cc/2017/workshop/paper75/AnonReviewer1"], "reply": {"forum": "rJT7bB4Kx", "replyto": "rJT7bB4Kx", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper75/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper75/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489162475470}}}, {"tddate": null, "tmdate": 1488083542614, "tcdate": 1488083542614, "number": 1, "id": "ByJvq0kcx", "invitation": "ICLR.cc/2017/workshop/-/paper75/official/review", "forum": "rJT7bB4Kx", "replyto": "rJT7bB4Kx", "signatures": ["ICLR.cc/2017/workshop/paper75/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper75/AnonReviewer2"], "content": {"title": "Sensible idea, but comparison to related work insufficient", "rating": "4: Ok but not good enough - rejection", "review": "For the task of translating a sentence describing an image from one language to another language with the image as additional input for the translation task, the paper uses multimodal attention. For the multimodal attention, the paper explores to use Multimodal Compact Bilinear Pooling (MCB) [Fukui 2016].\n\nStrength:\n-\tUsing MCB for this task seems to makes sense has not previously explored to my knowledge and slightly improves performances.\n-\tThe paper evaluates the task and ablations on the Multi30k dataset.\n\nMain Weaknesses:\nDiscussion and comparison to related work: \n1.\tThere has been a large number of works looking at the multimodal translation problem, e.g. [Elliott 2015], [Iacer 2016], but the paper reads like, it is the first work looking at this problem. Specifically, the model from [Iacer 2016] is very similar to this work, apart from MCB.\n2.\tPlease cite prior work more precisely: The work misses the citation for tensor sketch algorithm from [Pham and Pagh 2013]; specifically also in Figure 1, where the visualization and algorithm seems to be based on Fukui  2016.\n3.\t [Iacer 2016] also reports the results of using Moses, a statistical machine translation pipeline, which does not use the image an achieves 52. Meteor, higher than any result reported in this work.\n4.\tSee for https://staff.fnwi.uva.nl/s.c.frank/mmt_wmt_slides.pdf for many more results on the same dataset and task, many approaches achieving > 50 METEOR.\n\n\nFurther Weaknesses:\n1.\tPlease cite the actual publications not the arXives, whenever available.\n\nWhile the paper integrates MCB [Fukui] in multimodal translation, which has not been done before to my knowledge, the paper significantly lacks coverage and comparison to related work, making it not acceptable in this form. Most notably, the approach is very similar to [Iacer 2016], apart from using MCB, but the paper does not cite [Iacer 2016].\n\n\nReferences:\n[Pham and Pagh 2013] Ninh Pham and Rasmus Pagh. 2013. Fast and scalable polynomial kernels via explicit feature maps. In Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD \u201913, pages 239\u2013247, New York, NY, USA. ACM. \n[Elliott 2015] Elliott, Desmond, Stella Frank, and Eva Hasler. \"Multilingual Image Description with Neural Sequence Models.\"\u00a0arXiv preprint arXiv:1510.04709\u00a0(2015).\n\n[Iacer 2016] Calixto, Iacer, Desmond Elliott, and Stella Frank. \"Dcu-uva multimodal mt system report.\"\u00a0Proceedings of the First Conference on Machine Translation, Berlin, Germany. 2016.\n", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Multimodal Compact Bilinear Pooling for Multimodal Neural Machine Translation", "abstract": "In state-of-the-art Neural Machine Translation, an attention mechanism is used during decoding to enhance the translation. At every step, the decoder uses this mechanism to focus on different parts of the source sentence to gather the most useful information before outputting its target word. Recently, the effectiveness of the attention mechanism has also been explored for multimodal tasks, where it becomes possible to focus both on sentence parts and image regions. Approaches to pool two modalities usually include element-wise product, sum or concatenation.\nIn this paper, we evaluate the more advanced Multimodal Compact Bilinear pooling method, which takes the outer product of two vectors to combine the attention features for the two modalities. This has been previously investigated for visual question answering. We try out this approach for multimodal image caption translation and show improvements compared to basic combination methods.", "pdf": "/pdf/145fd5bb7d7c3f72e070dede27aafb646a8cf9fe.pdf", "paperhash": "delbrouck|multimodal_compact_bilinear_pooling_for_multimodal_neural_machine_translation", "keywords": ["Natural language processing"], "conflicts": ["umons.ac.be"], "authors": ["Jean-Benoit Delbrouck", "Stephane Dupont"], "authorids": ["Jean-Benoit.DELBROUCK@umons.ac.be", "Stephane.DUPONT@umons.ac.be"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489162475470, "id": "ICLR.cc/2017/workshop/-/paper75/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper75/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper75/AnonReviewer2", "ICLR.cc/2017/workshop/paper75/AnonReviewer1"], "reply": {"forum": "rJT7bB4Kx", "replyto": "rJT7bB4Kx", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper75/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper75/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489162475470}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1487323429270, "tcdate": 1487323429270, "number": 75, "id": "rJT7bB4Kx", "invitation": "ICLR.cc/2017/workshop/-/submission", "forum": "rJT7bB4Kx", "signatures": ["~Jean-Benoit_Delbrouck1"], "readers": ["everyone"], "content": {"TL;DR": "", "title": "Multimodal Compact Bilinear Pooling for Multimodal Neural Machine Translation", "abstract": "In state-of-the-art Neural Machine Translation, an attention mechanism is used during decoding to enhance the translation. At every step, the decoder uses this mechanism to focus on different parts of the source sentence to gather the most useful information before outputting its target word. Recently, the effectiveness of the attention mechanism has also been explored for multimodal tasks, where it becomes possible to focus both on sentence parts and image regions. Approaches to pool two modalities usually include element-wise product, sum or concatenation.\nIn this paper, we evaluate the more advanced Multimodal Compact Bilinear pooling method, which takes the outer product of two vectors to combine the attention features for the two modalities. This has been previously investigated for visual question answering. We try out this approach for multimodal image caption translation and show improvements compared to basic combination methods.", "pdf": "/pdf/145fd5bb7d7c3f72e070dede27aafb646a8cf9fe.pdf", "paperhash": "delbrouck|multimodal_compact_bilinear_pooling_for_multimodal_neural_machine_translation", "keywords": ["Natural language processing"], "conflicts": ["umons.ac.be"], "authors": ["Jean-Benoit Delbrouck", "Stephane Dupont"], "authorids": ["Jean-Benoit.DELBROUCK@umons.ac.be", "Stephane.DUPONT@umons.ac.be"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 6, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1487690420000, "tmdate": 1484242559574, "id": "ICLR.cc/2017/workshop/-/submission", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1495466420000, "cdate": 1484242559574}}}], "count": 7}