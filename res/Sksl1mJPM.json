{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1528124449749, "tcdate": 1518444275026, "number": 119, "cdate": 1518444275026, "id": "Sksl1mJPM", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "Sksl1mJPM", "signatures": ["~Xue_Geng1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "Hardware-aware Exponential Approximation for Deep Neural Network", "abstract": "In this paper, we address the problem of cost-efficient inference for non-linear operations in deep neural networks (DNNs), in particular, the exponential function exin softmax layer of DNNs for object detection. The goal is to minimize the hardware cost in terms of energy and area, while maintaining the application accuracy. To this end, we introduce Piecewise Linear Function (PLF) for approximating ex. First, we derive a theoretical upper bound of the number of pieces required for retaining the detection accuracy. Moreover, we constrain PLF to bounded domain in order to minimize bitwidths of the lookup table of pieces, resulting in lower energy and area cost. The non-differentiable bounded PLF layer can be optimized via the straight-through estimator. ASIC synthesis demonstrates that the hardware-oriented softmax costs 4x less energy and area than the direct lookup table of ex, while with comparable detection accuracy on benchmark datasets.", "paperhash": "geng|hardwareaware_exponential_approximation_for_deep_neural_network", "keywords": ["Hardware", "Softmax", "Exponential", "Deep Learning"], "_bibtex": "@misc{\n  geng2018hardware-aware,\n  title={Hardware-aware Exponential Approximation for Deep Neural Network},\n  author={Xue Geng and Jie Lin and Bin Zhao and Zhe Wang and Mohamed M. Sabry Aly and Vijay Chandrasekhar},\n  year={2018},\n  url={https://openreview.net/forum?id=Sksl1mJPM}\n}", "authorids": ["geng_xue@i2r.a-star.edu.sg", "lin-j@i2r.a-star.edu.sg", "wang_zhe@i2r.a-star.edu.sg", "zhaobin@ime.a-star.edu.sg", "msabry@ntu.edu.sg", "vijay@i2r.a-star.edu.sg"], "authors": ["Xue Geng", "Jie Lin", "Bin Zhao", "Zhe Wang", "Mohamed M. Sabry Aly", "Vijay Chandrasekhar"], "TL;DR": "Hardware-aware approximation for exponential function.", "pdf": "/pdf/dd5e792538198ef2912fbafbcc2baa8fa45f40cb.pdf"}, "nonreaders": [], "details": {"replyCount": 3, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582972702, "tcdate": 1520001518143, "number": 1, "cdate": 1520001518143, "id": "HyUlfJvuG", "invitation": "ICLR.cc/2018/Workshop/-/Paper119/Official_Review", "forum": "Sksl1mJPM", "replyto": "Sksl1mJPM", "signatures": ["ICLR.cc/2018/Workshop/Paper119/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper119/AnonReviewer3"], "content": {"title": "Interesting work, but it is not clear which are the adavantages", "rating": "5: Marginally below acceptance threshold", "review": "This paper proposes a piecewise linear function (PLF) to quickly approximate the exponential e^x used in the softmax layer of a neural net. In particular, for object detection, softmax is applied to every boundingbox, and therefore it may be important to reduce its computational cost. Compared to an approaximation based on constant values (instead of linear approximation), PLF can reduce the lookup table size by a factor 4, without reducing the detection performance. The table size can be further reduced using a bounded approximation of e^x and by retraining the detector with the approximate exponential.\n\nPros:\n- The paper is well presented\n\nCons:\n- My main critic to the paper is the very small practical effect of the contribution. I have to say that I am not an expert in hardware, but, in my understanding, even in detection, the real cost of softmax is really marginal compared to convolutions. Thus, optimizing a part that takes only a small fraction of the full computation, to me does not change much in terms of speed or hardware cost. The authors do not enter in this discussion, they just mentioned that in R-CNN for the case of classifying 10k classes, (which is not the case of COCO, 80 classes or VOC 20 classes), the softmax will be used 3M times. However, they do not mention, which is the percentage of cost of this operation compared with the rest. I do not have numbers, but, in my understanding it should be below 1% of the total cost.\n- The idea of using a PLF for e^x is a very simple contribution.\n- It is not clear how much is the advantage of PLF compared with standard approaches of computing e^x. Even if the approximation of e^x with Taylor series is slower, it would have been interesting to see results with that too.\n\n\n\nAdditional comments:\n- What about using a continuos linear approximation of e^x so that you do not need to use the straight through estimator?\n- The section about the estimation of the lookup table size is not very clear to me\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hardware-aware Exponential Approximation for Deep Neural Network", "abstract": "In this paper, we address the problem of cost-efficient inference for non-linear operations in deep neural networks (DNNs), in particular, the exponential function exin softmax layer of DNNs for object detection. The goal is to minimize the hardware cost in terms of energy and area, while maintaining the application accuracy. To this end, we introduce Piecewise Linear Function (PLF) for approximating ex. First, we derive a theoretical upper bound of the number of pieces required for retaining the detection accuracy. Moreover, we constrain PLF to bounded domain in order to minimize bitwidths of the lookup table of pieces, resulting in lower energy and area cost. The non-differentiable bounded PLF layer can be optimized via the straight-through estimator. ASIC synthesis demonstrates that the hardware-oriented softmax costs 4x less energy and area than the direct lookup table of ex, while with comparable detection accuracy on benchmark datasets.", "paperhash": "geng|hardwareaware_exponential_approximation_for_deep_neural_network", "keywords": ["Hardware", "Softmax", "Exponential", "Deep Learning"], "_bibtex": "@misc{\n  geng2018hardware-aware,\n  title={Hardware-aware Exponential Approximation for Deep Neural Network},\n  author={Xue Geng and Jie Lin and Bin Zhao and Zhe Wang and Mohamed M. Sabry Aly and Vijay Chandrasekhar},\n  year={2018},\n  url={https://openreview.net/forum?id=Sksl1mJPM}\n}", "authorids": ["geng_xue@i2r.a-star.edu.sg", "lin-j@i2r.a-star.edu.sg", "wang_zhe@i2r.a-star.edu.sg", "zhaobin@ime.a-star.edu.sg", "msabry@ntu.edu.sg", "vijay@i2r.a-star.edu.sg"], "authors": ["Xue Geng", "Jie Lin", "Bin Zhao", "Zhe Wang", "Mohamed M. Sabry Aly", "Vijay Chandrasekhar"], "TL;DR": "Hardware-aware approximation for exponential function.", "pdf": "/pdf/dd5e792538198ef2912fbafbcc2baa8fa45f40cb.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582972471, "id": "ICLR.cc/2018/Workshop/-/Paper119/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper119/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper119/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper119/AnonReviewer1"], "reply": {"forum": "Sksl1mJPM", "replyto": "Sksl1mJPM", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper119/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper119/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582972471}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582840139, "tcdate": 1520594084791, "number": 2, "cdate": 1520594084791, "id": "HyTi3JgFf", "invitation": "ICLR.cc/2018/Workshop/-/Paper119/Official_Review", "forum": "Sksl1mJPM", "replyto": "Sksl1mJPM", "signatures": ["ICLR.cc/2018/Workshop/Paper119/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper119/AnonReviewer1"], "content": {"title": "Insufficient technical depth and breadth of applicability", "rating": "4: Ok but not good enough - rejection", "review": "The authors propose approximating the exponential e^x in softmax layers of DNNs for object detection by a bounded piece-wise linear function (BPLF). They show a theoretical upper bound on the number of pieces for a required accuracy. They train DNNs with softmax and replace e^x with BPLF while doing inference. They analyze how lookup table size affects detection accuracy and also save on the bitwidths of lookup table values using BPLFs. They observe 2x- 4x energy savings on detection benchmark datasets.\n\nI do not see enough technical depth in their approach. I am also unsure about the breadth of this line of work. The authors do mention that the same idea can be applied to various non-linear operations. However, the current experiments are not enough to draw any general conclusion about energy/area-savings along with faster inference. ", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hardware-aware Exponential Approximation for Deep Neural Network", "abstract": "In this paper, we address the problem of cost-efficient inference for non-linear operations in deep neural networks (DNNs), in particular, the exponential function exin softmax layer of DNNs for object detection. The goal is to minimize the hardware cost in terms of energy and area, while maintaining the application accuracy. To this end, we introduce Piecewise Linear Function (PLF) for approximating ex. First, we derive a theoretical upper bound of the number of pieces required for retaining the detection accuracy. Moreover, we constrain PLF to bounded domain in order to minimize bitwidths of the lookup table of pieces, resulting in lower energy and area cost. The non-differentiable bounded PLF layer can be optimized via the straight-through estimator. ASIC synthesis demonstrates that the hardware-oriented softmax costs 4x less energy and area than the direct lookup table of ex, while with comparable detection accuracy on benchmark datasets.", "paperhash": "geng|hardwareaware_exponential_approximation_for_deep_neural_network", "keywords": ["Hardware", "Softmax", "Exponential", "Deep Learning"], "_bibtex": "@misc{\n  geng2018hardware-aware,\n  title={Hardware-aware Exponential Approximation for Deep Neural Network},\n  author={Xue Geng and Jie Lin and Bin Zhao and Zhe Wang and Mohamed M. Sabry Aly and Vijay Chandrasekhar},\n  year={2018},\n  url={https://openreview.net/forum?id=Sksl1mJPM}\n}", "authorids": ["geng_xue@i2r.a-star.edu.sg", "lin-j@i2r.a-star.edu.sg", "wang_zhe@i2r.a-star.edu.sg", "zhaobin@ime.a-star.edu.sg", "msabry@ntu.edu.sg", "vijay@i2r.a-star.edu.sg"], "authors": ["Xue Geng", "Jie Lin", "Bin Zhao", "Zhe Wang", "Mohamed M. Sabry Aly", "Vijay Chandrasekhar"], "TL;DR": "Hardware-aware approximation for exponential function.", "pdf": "/pdf/dd5e792538198ef2912fbafbcc2baa8fa45f40cb.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582972471, "id": "ICLR.cc/2018/Workshop/-/Paper119/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper119/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper119/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper119/AnonReviewer1"], "reply": {"forum": "Sksl1mJPM", "replyto": "Sksl1mJPM", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper119/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper119/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582972471}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521573593161, "tcdate": 1521573593161, "number": 214, "cdate": 1521573592815, "id": "HJb1kk19G", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "Sksl1mJPM", "replyto": "Sksl1mJPM", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Reject", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "Based on the reviews, this paper has not been accepted for presentation at the ICLR workshop. However, the conversation and updates can continue to appear here on OpenReview."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hardware-aware Exponential Approximation for Deep Neural Network", "abstract": "In this paper, we address the problem of cost-efficient inference for non-linear operations in deep neural networks (DNNs), in particular, the exponential function exin softmax layer of DNNs for object detection. The goal is to minimize the hardware cost in terms of energy and area, while maintaining the application accuracy. To this end, we introduce Piecewise Linear Function (PLF) for approximating ex. First, we derive a theoretical upper bound of the number of pieces required for retaining the detection accuracy. Moreover, we constrain PLF to bounded domain in order to minimize bitwidths of the lookup table of pieces, resulting in lower energy and area cost. The non-differentiable bounded PLF layer can be optimized via the straight-through estimator. ASIC synthesis demonstrates that the hardware-oriented softmax costs 4x less energy and area than the direct lookup table of ex, while with comparable detection accuracy on benchmark datasets.", "paperhash": "geng|hardwareaware_exponential_approximation_for_deep_neural_network", "keywords": ["Hardware", "Softmax", "Exponential", "Deep Learning"], "_bibtex": "@misc{\n  geng2018hardware-aware,\n  title={Hardware-aware Exponential Approximation for Deep Neural Network},\n  author={Xue Geng and Jie Lin and Bin Zhao and Zhe Wang and Mohamed M. Sabry Aly and Vijay Chandrasekhar},\n  year={2018},\n  url={https://openreview.net/forum?id=Sksl1mJPM}\n}", "authorids": ["geng_xue@i2r.a-star.edu.sg", "lin-j@i2r.a-star.edu.sg", "wang_zhe@i2r.a-star.edu.sg", "zhaobin@ime.a-star.edu.sg", "msabry@ntu.edu.sg", "vijay@i2r.a-star.edu.sg"], "authors": ["Xue Geng", "Jie Lin", "Bin Zhao", "Zhe Wang", "Mohamed M. Sabry Aly", "Vijay Chandrasekhar"], "TL;DR": "Hardware-aware approximation for exponential function.", "pdf": "/pdf/dd5e792538198ef2912fbafbcc2baa8fa45f40cb.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}], "count": 4}