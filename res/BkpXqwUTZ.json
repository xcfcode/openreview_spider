{"notes": [{"tddate": null, "ddate": null, "tmdate": 1518730191648, "tcdate": 1508436516724, "number": 18, "cdate": 1518730191638, "id": "BkpXqwUTZ", "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "forum": "BkpXqwUTZ", "original": "HJh79DLab", "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference"], "content": {"title": "Iterative temporal differencing with fixed random feedback alignment support spike-time dependent plasticity in vanilla backpropagation for deep learning", "abstract": "In vanilla backpropagation (VBP), activation function matters considerably in terms of non-linearity and differentiability.\nVanishing gradient has been an important problem related to the bad choice of activation function in deep learning (DL).\nThis work shows that a differentiable activation function is not necessary any more for error backpropagation. \nThe derivative of the activation function can be replaced by an iterative temporal differencing (ITD) using fixed random feedback weight alignment (FBA).\nUsing FBA with ITD, we can transform the VBP into a more biologically plausible approach for learning deep neural network architectures.\nWe don't claim that ITD works completely the same as the spike-time dependent plasticity (STDP) in our brain but this work can be a step toward the integration of STDP-based error backpropagation in deep learning.", "pdf": "/pdf/775ad0b287f599d4744d54c1a7e2189ee66e40be.pdf", "TL;DR": "Iterative temporal differencing with fixed random feedback alignment support spike-time dependent plasticity in vanilla backpropagation for deep learning.", "paperhash": "dargazany|iterative_temporal_differencing_with_fixed_random_feedback_alignment_support_spiketime_dependent_plasticity_in_vanilla_backpropagation_for_deep_learning", "_bibtex": "@misc{\ndargazany2018iterative,\ntitle={Iterative temporal differencing with fixed random feedback alignment support spike-time dependent plasticity in vanilla backpropagation for deep learning},\nauthor={Aras Dargazany and Kunal Mankodiya},\nyear={2018},\nurl={https://openreview.net/forum?id=BkpXqwUTZ},\n}", "authorids": ["arasdar@uri.edu"], "keywords": ["Iterative temporal differencing", "feedback alignment", "spike-time dependent plasticity", "vanilla backpropagation", "deep learning"], "authors": ["Aras Dargazany", "Kunal Mankodiya"]}, "nonreaders": [], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1506717071958, "id": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Conference"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference"]}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"authors": {"required": false, "order": 1, "values-regex": ".*", "description": "Comma separated list of author names, as they appear in the paper."}, "authorids": {"required": false, "order": 2, "values-regex": ".*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "cdate": 1506717071958}}, "tauthor": "ICLR.cc/2018/Conference"}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1517260082697, "tcdate": 1517249985372, "number": 671, "cdate": 1517249985359, "id": "SkF6SJarM", "invitation": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "forum": "BkpXqwUTZ", "replyto": "BkpXqwUTZ", "signatures": ["ICLR.cc/2018/Conference/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Program_Chairs"], "content": {"decision": "Reject", "title": "ICLR 2018 Conference Acceptance Decision", "comment": "This paper is nowhere near standards for publication anywhere."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Iterative temporal differencing with fixed random feedback alignment support spike-time dependent plasticity in vanilla backpropagation for deep learning", "abstract": "In vanilla backpropagation (VBP), activation function matters considerably in terms of non-linearity and differentiability.\nVanishing gradient has been an important problem related to the bad choice of activation function in deep learning (DL).\nThis work shows that a differentiable activation function is not necessary any more for error backpropagation. \nThe derivative of the activation function can be replaced by an iterative temporal differencing (ITD) using fixed random feedback weight alignment (FBA).\nUsing FBA with ITD, we can transform the VBP into a more biologically plausible approach for learning deep neural network architectures.\nWe don't claim that ITD works completely the same as the spike-time dependent plasticity (STDP) in our brain but this work can be a step toward the integration of STDP-based error backpropagation in deep learning.", "pdf": "/pdf/775ad0b287f599d4744d54c1a7e2189ee66e40be.pdf", "TL;DR": "Iterative temporal differencing with fixed random feedback alignment support spike-time dependent plasticity in vanilla backpropagation for deep learning.", "paperhash": "dargazany|iterative_temporal_differencing_with_fixed_random_feedback_alignment_support_spiketime_dependent_plasticity_in_vanilla_backpropagation_for_deep_learning", "_bibtex": "@misc{\ndargazany2018iterative,\ntitle={Iterative temporal differencing with fixed random feedback alignment support spike-time dependent plasticity in vanilla backpropagation for deep learning},\nauthor={Aras Dargazany and Kunal Mankodiya},\nyear={2018},\nurl={https://openreview.net/forum?id=BkpXqwUTZ},\n}", "authorids": ["arasdar@uri.edu"], "keywords": ["Iterative temporal differencing", "feedback alignment", "spike-time dependent plasticity", "vanilla backpropagation", "deep learning"], "authors": ["Aras Dargazany", "Kunal Mankodiya"]}, "tags": [], "invitation": {"id": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "rdate": null, "ddate": null, "expdate": 1541175629000, "duedate": null, "tmdate": 1541177635767, "tddate": null, "super": null, "final": null, "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": {"values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Conference/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Conference Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": [], "noninvitees": [], "writers": ["ICLR.cc/2018/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1541177635767}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642404904, "tcdate": 1511630254199, "number": 1, "cdate": 1511630254199, "id": "rJUnrQDlG", "invitation": "ICLR.cc/2018/Conference/-/Paper18/Official_Review", "forum": "BkpXqwUTZ", "replyto": "BkpXqwUTZ", "signatures": ["ICLR.cc/2018/Conference/Paper18/AnonReviewer2"], "readers": ["everyone"], "content": {"title": "Not clear what exactly the authors want to achieve", "rating": "3: Clear rejection", "review": "- This paper is not well written and incomplete. There is no clear explanation of what exactly the authors want to achieve in the paper, what exactly is their approach/contribution, experimental setup, and analysis of their results. \n\n- The paper is hard to read due to many abbreviations, e.g., the last paragraph in page 2. \n\n- The format is inconsistent. Section 1 is numbered, but not the other sections. \n\n- in page 2, what do the numbers mean at the end of each sentence? Probably the figures? \n\n- in page 2, \"in this figure\": which figure is this referring to?\n\n\nComments on prior work:\n\np 1: authors write: \"vanilla backpropagation (VBP)\" \"was proposed around 1987 Rumelhart et al. (1985).\" \n\nNot true. A main problem with the 1985 paper is that it does not cite the inventors of backpropagation. The VBP that everybody is using now is the one published by  Linnainmaa in 1970, extending Kelley's work of 1960. The first to publish the application of VBP to NNs was Werbos in 1982. Please correct. \n\np 1: authors write: \"Almost at the same time, biologically inspired convolutional networks was also introduced as well using VBP LeCun et al. (1989).\"\n\nHere one must cite the person who really invented this biologically inspired convolutional architecture (but did not apply backprop to it): Fukushima (1979). He is cited later, but in a misleading way. Please correct.\n\np 1: authors write: \"Deep learning (DL) was introduced as an approach to learn deep neural network architecture using VBP LeCun et al. (1989; 2015); Krizhevsky et al. (2012).\" \n\nNot true. Deep Learning was introduced by Ivakhnenko and Lapa in 1965: the first working method for learning in multilayer perceptrons of arbitrary depth. Please correct. (The term \"deep learning\" was introduced to ML in 1986 by Dechter for something else.)\n\np1: authors write: \"Extremely deep networks learning reached 152 layers of representation with residual and highway networks He et al. (2016); Srivastava et al. (2015).\" \n\nHighway networks were published half a year earlier than resnets, and reached many hundreds of layers before resnets. Please correct.\n\n\nGeneral recommendation: Clear rejection for now. But perhaps the author want to resubmit this to another conference, taking into account the reviewer comments.\n\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Iterative temporal differencing with fixed random feedback alignment support spike-time dependent plasticity in vanilla backpropagation for deep learning", "abstract": "In vanilla backpropagation (VBP), activation function matters considerably in terms of non-linearity and differentiability.\nVanishing gradient has been an important problem related to the bad choice of activation function in deep learning (DL).\nThis work shows that a differentiable activation function is not necessary any more for error backpropagation. \nThe derivative of the activation function can be replaced by an iterative temporal differencing (ITD) using fixed random feedback weight alignment (FBA).\nUsing FBA with ITD, we can transform the VBP into a more biologically plausible approach for learning deep neural network architectures.\nWe don't claim that ITD works completely the same as the spike-time dependent plasticity (STDP) in our brain but this work can be a step toward the integration of STDP-based error backpropagation in deep learning.", "pdf": "/pdf/775ad0b287f599d4744d54c1a7e2189ee66e40be.pdf", "TL;DR": "Iterative temporal differencing with fixed random feedback alignment support spike-time dependent plasticity in vanilla backpropagation for deep learning.", "paperhash": "dargazany|iterative_temporal_differencing_with_fixed_random_feedback_alignment_support_spiketime_dependent_plasticity_in_vanilla_backpropagation_for_deep_learning", "_bibtex": "@misc{\ndargazany2018iterative,\ntitle={Iterative temporal differencing with fixed random feedback alignment support spike-time dependent plasticity in vanilla backpropagation for deep learning},\nauthor={Aras Dargazany and Kunal Mankodiya},\nyear={2018},\nurl={https://openreview.net/forum?id=BkpXqwUTZ},\n}", "authorids": ["arasdar@uri.edu"], "keywords": ["Iterative temporal differencing", "feedback alignment", "spike-time dependent plasticity", "vanilla backpropagation", "deep learning"], "authors": ["Aras Dargazany", "Kunal Mankodiya"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642404808, "id": "ICLR.cc/2018/Conference/-/Paper18/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper18/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper18/AnonReviewer2", "ICLR.cc/2018/Conference/Paper18/AnonReviewer1", "ICLR.cc/2018/Conference/Paper18/AnonReviewer3"], "reply": {"forum": "BkpXqwUTZ", "replyto": "BkpXqwUTZ", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper18/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642404808}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642404866, "tcdate": 1511729536932, "number": 2, "cdate": 1511729536932, "id": "rkFKYjdlz", "invitation": "ICLR.cc/2018/Conference/-/Paper18/Official_Review", "forum": "BkpXqwUTZ", "replyto": "BkpXqwUTZ", "signatures": ["ICLR.cc/2018/Conference/Paper18/AnonReviewer1"], "readers": ["everyone"], "content": {"title": "Not up to a professional standard.", "rating": "2: Strong rejection", "review": "The paper falls far short of the standard expected of an ICLR submission. \n\nThe paper has little to no content. There are large sections of blank page throughout. The algorithm, iterative temporal differencing, is introduced in a figure -- there is no formal description. The experiments are only performed on MNIST. The subfigures are not labeled. The paper over-uses acronyms; sentences like \u201cIn this figure, VBP, VBP with FBA, and ITD using FBA for VBP\u2026\u201d are painful to read. \n\n\n", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Iterative temporal differencing with fixed random feedback alignment support spike-time dependent plasticity in vanilla backpropagation for deep learning", "abstract": "In vanilla backpropagation (VBP), activation function matters considerably in terms of non-linearity and differentiability.\nVanishing gradient has been an important problem related to the bad choice of activation function in deep learning (DL).\nThis work shows that a differentiable activation function is not necessary any more for error backpropagation. \nThe derivative of the activation function can be replaced by an iterative temporal differencing (ITD) using fixed random feedback weight alignment (FBA).\nUsing FBA with ITD, we can transform the VBP into a more biologically plausible approach for learning deep neural network architectures.\nWe don't claim that ITD works completely the same as the spike-time dependent plasticity (STDP) in our brain but this work can be a step toward the integration of STDP-based error backpropagation in deep learning.", "pdf": "/pdf/775ad0b287f599d4744d54c1a7e2189ee66e40be.pdf", "TL;DR": "Iterative temporal differencing with fixed random feedback alignment support spike-time dependent plasticity in vanilla backpropagation for deep learning.", "paperhash": "dargazany|iterative_temporal_differencing_with_fixed_random_feedback_alignment_support_spiketime_dependent_plasticity_in_vanilla_backpropagation_for_deep_learning", "_bibtex": "@misc{\ndargazany2018iterative,\ntitle={Iterative temporal differencing with fixed random feedback alignment support spike-time dependent plasticity in vanilla backpropagation for deep learning},\nauthor={Aras Dargazany and Kunal Mankodiya},\nyear={2018},\nurl={https://openreview.net/forum?id=BkpXqwUTZ},\n}", "authorids": ["arasdar@uri.edu"], "keywords": ["Iterative temporal differencing", "feedback alignment", "spike-time dependent plasticity", "vanilla backpropagation", "deep learning"], "authors": ["Aras Dargazany", "Kunal Mankodiya"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642404808, "id": "ICLR.cc/2018/Conference/-/Paper18/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper18/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper18/AnonReviewer2", "ICLR.cc/2018/Conference/Paper18/AnonReviewer1", "ICLR.cc/2018/Conference/Paper18/AnonReviewer3"], "reply": {"forum": "BkpXqwUTZ", "replyto": "BkpXqwUTZ", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper18/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642404808}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642404827, "tcdate": 1511768765687, "number": 3, "cdate": 1511768765687, "id": "r18pMHFgM", "invitation": "ICLR.cc/2018/Conference/-/Paper18/Official_Review", "forum": "BkpXqwUTZ", "replyto": "BkpXqwUTZ", "signatures": ["ICLR.cc/2018/Conference/Paper18/AnonReviewer3"], "readers": ["everyone"], "content": {"title": "The paper claims to work towards a more biological version of error-backpropagation.", "rating": "2: Strong rejection", "review": "The paper is incomplete and nowhere near finished, it should have been withdrawn. \n\nThe theoretical results are presented in a bitmap figure and only referred to in the text (not explained), and  the results on datasets are not explained either (and pretty bad). A waste of my time.", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Iterative temporal differencing with fixed random feedback alignment support spike-time dependent plasticity in vanilla backpropagation for deep learning", "abstract": "In vanilla backpropagation (VBP), activation function matters considerably in terms of non-linearity and differentiability.\nVanishing gradient has been an important problem related to the bad choice of activation function in deep learning (DL).\nThis work shows that a differentiable activation function is not necessary any more for error backpropagation. \nThe derivative of the activation function can be replaced by an iterative temporal differencing (ITD) using fixed random feedback weight alignment (FBA).\nUsing FBA with ITD, we can transform the VBP into a more biologically plausible approach for learning deep neural network architectures.\nWe don't claim that ITD works completely the same as the spike-time dependent plasticity (STDP) in our brain but this work can be a step toward the integration of STDP-based error backpropagation in deep learning.", "pdf": "/pdf/775ad0b287f599d4744d54c1a7e2189ee66e40be.pdf", "TL;DR": "Iterative temporal differencing with fixed random feedback alignment support spike-time dependent plasticity in vanilla backpropagation for deep learning.", "paperhash": "dargazany|iterative_temporal_differencing_with_fixed_random_feedback_alignment_support_spiketime_dependent_plasticity_in_vanilla_backpropagation_for_deep_learning", "_bibtex": "@misc{\ndargazany2018iterative,\ntitle={Iterative temporal differencing with fixed random feedback alignment support spike-time dependent plasticity in vanilla backpropagation for deep learning},\nauthor={Aras Dargazany and Kunal Mankodiya},\nyear={2018},\nurl={https://openreview.net/forum?id=BkpXqwUTZ},\n}", "authorids": ["arasdar@uri.edu"], "keywords": ["Iterative temporal differencing", "feedback alignment", "spike-time dependent plasticity", "vanilla backpropagation", "deep learning"], "authors": ["Aras Dargazany", "Kunal Mankodiya"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642404808, "id": "ICLR.cc/2018/Conference/-/Paper18/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper18/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper18/AnonReviewer2", "ICLR.cc/2018/Conference/Paper18/AnonReviewer1", "ICLR.cc/2018/Conference/Paper18/AnonReviewer3"], "reply": {"forum": "BkpXqwUTZ", "replyto": "BkpXqwUTZ", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper18/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642404808}}}], "count": 5}