{"notes": [{"id": "LzhEvTWpzH", "original": "6L9OHDx85OZ", "number": 1514, "cdate": 1601308168183, "ddate": null, "tcdate": 1601308168183, "tmdate": 1614985665171, "tddate": null, "forum": "LzhEvTWpzH", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Switching-Aligned-Words Data Augmentation for Neural Machine Translation", "authorids": ["~Fengshun_Xiao1", "~Zuchao_Li1", "~hai_zhao1"], "authors": ["Fengshun Xiao", "Zuchao Li", "hai zhao"], "keywords": ["Machine Translation", "Data augmentation"], "abstract": "In neural machine translation (NMT), data augmentation methods such as back-translation make it possible to use extra monolingual data to help improve translation performance, while it needs extra training data and the in-domain monolingual data is not always available. In this paper, we present a novel data augmentation method for neural machine translation by using only the original training data without extra data. More accurately, we randomly replace words or mixup with their aligned alternatives in another language when training neural machine translation models. Since aligned word pairs appear in the same position of each other during training, it is helpful to form bilingual embeddings which are proved useful to provide a performance boost \\citep{liu2019shared}. Experiments on both small and large scale datasets show that our method significantly outperforms the baseline models.", "one-sentence_summary": "In this paper, we present a novel data augmentation method for neural machine translation by using only the original training data without extra data. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xiao|switchingalignedwords_data_augmentation_for_neural_machine_translation", "pdf": "/pdf/2dda90597d761c6c4eb3ec487711fbe704bbcd21.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=wZY52rhUn8", "_bibtex": "@misc{\nxiao2021switchingalignedwords,\ntitle={Switching-Aligned-Words Data Augmentation for Neural Machine Translation},\nauthor={Fengshun Xiao and Zuchao Li and hai zhao},\nyear={2021},\nurl={https://openreview.net/forum?id=LzhEvTWpzH}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "4eFSS9u-aIo", "original": null, "number": 1, "cdate": 1610040496040, "ddate": null, "tcdate": 1610040496040, "tmdate": 1610474102362, "tddate": null, "forum": "LzhEvTWpzH", "replyto": "LzhEvTWpzH", "invitation": "ICLR.cc/2021/Conference/Paper1514/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "All reviewers agreed to reject."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Switching-Aligned-Words Data Augmentation for Neural Machine Translation", "authorids": ["~Fengshun_Xiao1", "~Zuchao_Li1", "~hai_zhao1"], "authors": ["Fengshun Xiao", "Zuchao Li", "hai zhao"], "keywords": ["Machine Translation", "Data augmentation"], "abstract": "In neural machine translation (NMT), data augmentation methods such as back-translation make it possible to use extra monolingual data to help improve translation performance, while it needs extra training data and the in-domain monolingual data is not always available. In this paper, we present a novel data augmentation method for neural machine translation by using only the original training data without extra data. More accurately, we randomly replace words or mixup with their aligned alternatives in another language when training neural machine translation models. Since aligned word pairs appear in the same position of each other during training, it is helpful to form bilingual embeddings which are proved useful to provide a performance boost \\citep{liu2019shared}. Experiments on both small and large scale datasets show that our method significantly outperforms the baseline models.", "one-sentence_summary": "In this paper, we present a novel data augmentation method for neural machine translation by using only the original training data without extra data. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xiao|switchingalignedwords_data_augmentation_for_neural_machine_translation", "pdf": "/pdf/2dda90597d761c6c4eb3ec487711fbe704bbcd21.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=wZY52rhUn8", "_bibtex": "@misc{\nxiao2021switchingalignedwords,\ntitle={Switching-Aligned-Words Data Augmentation for Neural Machine Translation},\nauthor={Fengshun Xiao and Zuchao Li and hai zhao},\nyear={2021},\nurl={https://openreview.net/forum?id=LzhEvTWpzH}\n}"}, "tags": [], "invitation": {"reply": {"forum": "LzhEvTWpzH", "replyto": "LzhEvTWpzH", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040496027, "tmdate": 1610474102346, "id": "ICLR.cc/2021/Conference/Paper1514/-/Decision"}}}, {"id": "KwKLEfBICBW", "original": null, "number": 7, "cdate": 1605609454862, "ddate": null, "tcdate": 1605609454862, "tmdate": 1605609454862, "tddate": null, "forum": "LzhEvTWpzH", "replyto": "j5JWWZU-rHB", "invitation": "ICLR.cc/2021/Conference/Paper1514/-/Official_Comment", "content": {"title": "Response to Reviewer4", "comment": "Thanks for your insightful comments.\n\n1) For all experiments, hyperparameters are optimized on a development set and then tested using only a single hyperparameter. We are sorry that we did not clear it explicitly and we will add it to the revised version of the paper.\n\n2) We have tied word embeddings between the source, target and output softmax embeddings as it is a normal setting. Even the Transformer would already be able to place words with similar meanings close together, our experiments show that our methods bring the embeddings of them closer compared to the Transformer baseline.\n\n3) We will test the cosine similarity between word pairs in different frequencies."}, "signatures": ["ICLR.cc/2021/Conference/Paper1514/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1514/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Switching-Aligned-Words Data Augmentation for Neural Machine Translation", "authorids": ["~Fengshun_Xiao1", "~Zuchao_Li1", "~hai_zhao1"], "authors": ["Fengshun Xiao", "Zuchao Li", "hai zhao"], "keywords": ["Machine Translation", "Data augmentation"], "abstract": "In neural machine translation (NMT), data augmentation methods such as back-translation make it possible to use extra monolingual data to help improve translation performance, while it needs extra training data and the in-domain monolingual data is not always available. In this paper, we present a novel data augmentation method for neural machine translation by using only the original training data without extra data. More accurately, we randomly replace words or mixup with their aligned alternatives in another language when training neural machine translation models. Since aligned word pairs appear in the same position of each other during training, it is helpful to form bilingual embeddings which are proved useful to provide a performance boost \\citep{liu2019shared}. Experiments on both small and large scale datasets show that our method significantly outperforms the baseline models.", "one-sentence_summary": "In this paper, we present a novel data augmentation method for neural machine translation by using only the original training data without extra data. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xiao|switchingalignedwords_data_augmentation_for_neural_machine_translation", "pdf": "/pdf/2dda90597d761c6c4eb3ec487711fbe704bbcd21.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=wZY52rhUn8", "_bibtex": "@misc{\nxiao2021switchingalignedwords,\ntitle={Switching-Aligned-Words Data Augmentation for Neural Machine Translation},\nauthor={Fengshun Xiao and Zuchao Li and hai zhao},\nyear={2021},\nurl={https://openreview.net/forum?id=LzhEvTWpzH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "LzhEvTWpzH", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1514/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1514/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1514/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1514/Authors|ICLR.cc/2021/Conference/Paper1514/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1514/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923858827, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1514/-/Official_Comment"}}}, {"id": "YnqIFp4JWNJ", "original": null, "number": 6, "cdate": 1605609360180, "ddate": null, "tcdate": 1605609360180, "tmdate": 1605609360180, "tddate": null, "forum": "LzhEvTWpzH", "replyto": "EwH-tvor1Ut", "invitation": "ICLR.cc/2021/Conference/Paper1514/-/Official_Comment", "content": {"title": "Response to Reviewer3", "comment": "Thanks for your insightful comments.\n\n1) The intuition behind the paper is that since the word and its corresponding word in the other language have similar embeddings, we can replace words with their aligned counterparts to add noise to source sentences but maintain semantic close. Our experiments prove it a good intuition for data augmentation.\n\n2) Our settings are on any scale datasets and without using any monolingual data of source language, while [1] uses extra monolingual source data and [2] targets low-resource settings.\n\n3) We are currently running replications for each experiment.\n\n4) We will add more experiments on different language pairs. \n\n5) Typos have been corrected."}, "signatures": ["ICLR.cc/2021/Conference/Paper1514/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1514/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Switching-Aligned-Words Data Augmentation for Neural Machine Translation", "authorids": ["~Fengshun_Xiao1", "~Zuchao_Li1", "~hai_zhao1"], "authors": ["Fengshun Xiao", "Zuchao Li", "hai zhao"], "keywords": ["Machine Translation", "Data augmentation"], "abstract": "In neural machine translation (NMT), data augmentation methods such as back-translation make it possible to use extra monolingual data to help improve translation performance, while it needs extra training data and the in-domain monolingual data is not always available. In this paper, we present a novel data augmentation method for neural machine translation by using only the original training data without extra data. More accurately, we randomly replace words or mixup with their aligned alternatives in another language when training neural machine translation models. Since aligned word pairs appear in the same position of each other during training, it is helpful to form bilingual embeddings which are proved useful to provide a performance boost \\citep{liu2019shared}. Experiments on both small and large scale datasets show that our method significantly outperforms the baseline models.", "one-sentence_summary": "In this paper, we present a novel data augmentation method for neural machine translation by using only the original training data without extra data. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xiao|switchingalignedwords_data_augmentation_for_neural_machine_translation", "pdf": "/pdf/2dda90597d761c6c4eb3ec487711fbe704bbcd21.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=wZY52rhUn8", "_bibtex": "@misc{\nxiao2021switchingalignedwords,\ntitle={Switching-Aligned-Words Data Augmentation for Neural Machine Translation},\nauthor={Fengshun Xiao and Zuchao Li and hai zhao},\nyear={2021},\nurl={https://openreview.net/forum?id=LzhEvTWpzH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "LzhEvTWpzH", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1514/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1514/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1514/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1514/Authors|ICLR.cc/2021/Conference/Paper1514/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1514/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923858827, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1514/-/Official_Comment"}}}, {"id": "i7ZJ44rIP_n", "original": null, "number": 5, "cdate": 1605609243919, "ddate": null, "tcdate": 1605609243919, "tmdate": 1605609243919, "tddate": null, "forum": "LzhEvTWpzH", "replyto": "rdApDRkuuiY", "invitation": "ICLR.cc/2021/Conference/Paper1514/-/Official_Comment", "content": {"title": "Response to Reviewer2 ", "comment": "Thanks for your insightful comments.\n\n1) We have modified our expression from \"Specially, we find that our method works better on low resource settings\" to \"Specially, we find that our method works better on relatively small scale datasets\".\n\n2) We are running more experiments on different language pairs and smaller scale datasets."}, "signatures": ["ICLR.cc/2021/Conference/Paper1514/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1514/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Switching-Aligned-Words Data Augmentation for Neural Machine Translation", "authorids": ["~Fengshun_Xiao1", "~Zuchao_Li1", "~hai_zhao1"], "authors": ["Fengshun Xiao", "Zuchao Li", "hai zhao"], "keywords": ["Machine Translation", "Data augmentation"], "abstract": "In neural machine translation (NMT), data augmentation methods such as back-translation make it possible to use extra monolingual data to help improve translation performance, while it needs extra training data and the in-domain monolingual data is not always available. In this paper, we present a novel data augmentation method for neural machine translation by using only the original training data without extra data. More accurately, we randomly replace words or mixup with their aligned alternatives in another language when training neural machine translation models. Since aligned word pairs appear in the same position of each other during training, it is helpful to form bilingual embeddings which are proved useful to provide a performance boost \\citep{liu2019shared}. Experiments on both small and large scale datasets show that our method significantly outperforms the baseline models.", "one-sentence_summary": "In this paper, we present a novel data augmentation method for neural machine translation by using only the original training data without extra data. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xiao|switchingalignedwords_data_augmentation_for_neural_machine_translation", "pdf": "/pdf/2dda90597d761c6c4eb3ec487711fbe704bbcd21.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=wZY52rhUn8", "_bibtex": "@misc{\nxiao2021switchingalignedwords,\ntitle={Switching-Aligned-Words Data Augmentation for Neural Machine Translation},\nauthor={Fengshun Xiao and Zuchao Li and hai zhao},\nyear={2021},\nurl={https://openreview.net/forum?id=LzhEvTWpzH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "LzhEvTWpzH", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1514/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1514/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1514/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1514/Authors|ICLR.cc/2021/Conference/Paper1514/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1514/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923858827, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1514/-/Official_Comment"}}}, {"id": "IF9_gDeTRku", "original": null, "number": 4, "cdate": 1605608733859, "ddate": null, "tcdate": 1605608733859, "tmdate": 1605609100867, "tddate": null, "forum": "LzhEvTWpzH", "replyto": "owRICe6FYYc", "invitation": "ICLR.cc/2021/Conference/Paper1514/-/Official_Comment", "content": {"title": "Response to Reviewer1", "comment": "Thanks for your insightful comments. \n\n1) We are currently running replications for each experiment.\n\n2) Our swapping decision is made at runtime allowing different swaps for the same sentence pair. We have tied word embeddings between the source, target and output softmax embeddings as it is a normal setting. We have added these details to the paper according to your advice.\n\n3) Thank you for your advice, we have added discussions about [2].  [3] introduces a new subword regularization method called BPE-Dropout to overcome the problem that BPE splits words into unique subword sequences. [4] uses doubly adversarial inputs to improve the robustness of the NMT systems. These two works are orthogonal to our work but not very closely related to ours.\n\n4) Since we switch words randomly in the training phase, the NMT training is like a GAN process. The noise generator produces noise where several words in the source sentence are randomly replaced by its translation in the target sentence, and the NMT model is trained as a discriminator. Because the word embeddings of these word pairs are close, it can be a good noise generator to fool the discriminator.\n\n5) Spelling errors have been corrected."}, "signatures": ["ICLR.cc/2021/Conference/Paper1514/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1514/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Switching-Aligned-Words Data Augmentation for Neural Machine Translation", "authorids": ["~Fengshun_Xiao1", "~Zuchao_Li1", "~hai_zhao1"], "authors": ["Fengshun Xiao", "Zuchao Li", "hai zhao"], "keywords": ["Machine Translation", "Data augmentation"], "abstract": "In neural machine translation (NMT), data augmentation methods such as back-translation make it possible to use extra monolingual data to help improve translation performance, while it needs extra training data and the in-domain monolingual data is not always available. In this paper, we present a novel data augmentation method for neural machine translation by using only the original training data without extra data. More accurately, we randomly replace words or mixup with their aligned alternatives in another language when training neural machine translation models. Since aligned word pairs appear in the same position of each other during training, it is helpful to form bilingual embeddings which are proved useful to provide a performance boost \\citep{liu2019shared}. Experiments on both small and large scale datasets show that our method significantly outperforms the baseline models.", "one-sentence_summary": "In this paper, we present a novel data augmentation method for neural machine translation by using only the original training data without extra data. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xiao|switchingalignedwords_data_augmentation_for_neural_machine_translation", "pdf": "/pdf/2dda90597d761c6c4eb3ec487711fbe704bbcd21.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=wZY52rhUn8", "_bibtex": "@misc{\nxiao2021switchingalignedwords,\ntitle={Switching-Aligned-Words Data Augmentation for Neural Machine Translation},\nauthor={Fengshun Xiao and Zuchao Li and hai zhao},\nyear={2021},\nurl={https://openreview.net/forum?id=LzhEvTWpzH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "LzhEvTWpzH", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1514/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1514/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1514/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1514/Authors|ICLR.cc/2021/Conference/Paper1514/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1514/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923858827, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1514/-/Official_Comment"}}}, {"id": "EwH-tvor1Ut", "original": null, "number": 1, "cdate": 1603890442351, "ddate": null, "tcdate": 1603890442351, "tmdate": 1605024424764, "tddate": null, "forum": "LzhEvTWpzH", "replyto": "LzhEvTWpzH", "invitation": "ICLR.cc/2021/Conference/Paper1514/-/Official_Review", "content": {"title": "Interesting idea, but the experiments are not sufficient to provide a good picture of the effectiveness of the proposed approach.", "review": "Summary:\n\nThe paper proposes a GAN process for training neural machine translation models. The noise generator in this approach uses a switching-aligned-words technique where they randomly switch a word in the source sentence with its translation in the target sentence. They use fast-align to get alignments between source and target sentences. The experiments show that the noisy sentence pair generator performs best with the proposed switch and align approach in comparison with other (more random) methods. \n\n\n##########################################################################\n\nPros:\n\nUsing GANs for training an NMT model is an interesting idea. I like the idea of using word translations as replacements for data augmentation. Since the embeddings of these words are close, these can be good candidates for the noise generator to fool the discriminator.  \n\nThe paper covers the literature quite well. \n\n##########################################################################\n\nCons:\n\nThe intuition of the paper is not clearly defined. \n\nThe experiments do not cover other augmentation methods. In this paper, the authors compare their approach to the following baselines: SWAP, DROPOUT, BLANK, SMOOTH. However, these are baseline approaches for the noise generator component. It would be valuable to know how this approach performs in comparison with other augmentation approaches such as [1] and [2].\n\nAll baseline methods (SWAP, DROPOUT, BLANK, SMOOTH) have an element of randomness in them and are not strong baselines. This provides little insight for understanding the impact of the bilingual switching. \n\nThe lack of comparison with the literature makes it difficult to evaluate the reported results. \n\nIt would be insightful to cover at least another language pair where the relation between the source and the target language is different. For instance, two languages that are not similar at all (structurally, morphologically, or semantically). It's interesting to see how this model performs when the assumption that word embeddings of the same word in two different languages are close to each other. \n\n#########################################################################\n\nSome typos:\n\n(1) Typo on page 1: additional monolingual corpus is uesd -> additional monolingual corpus is used\n\n(2) Typo in Figure 1: SRT -> SRC\n\n(3) Typo on page 4:  words with similar meandings  ->  words with similar meanings  \n\n(4) Typo on page 7: we have aligned words pairs -> we have aligned word pairs\n\n(5) Typo on page 7: switching aligned words is helpeful -> switching aligned words is helpful \n\n#########################################################################\n\nReferences:\n\n[1] Rico Sennrich, Barry Haddow, and Alexandra Birch. Improving neural machine translation models with monolingual data. In ACL, 2016.\n\n[2] Marzieh Fadaee, Arianna Bisazza, and Christof Monz. Data augmentation for low-resource neural machine translation. In ACL, 2017.\n\n\n\n\n", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper1514/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1514/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Switching-Aligned-Words Data Augmentation for Neural Machine Translation", "authorids": ["~Fengshun_Xiao1", "~Zuchao_Li1", "~hai_zhao1"], "authors": ["Fengshun Xiao", "Zuchao Li", "hai zhao"], "keywords": ["Machine Translation", "Data augmentation"], "abstract": "In neural machine translation (NMT), data augmentation methods such as back-translation make it possible to use extra monolingual data to help improve translation performance, while it needs extra training data and the in-domain monolingual data is not always available. In this paper, we present a novel data augmentation method for neural machine translation by using only the original training data without extra data. More accurately, we randomly replace words or mixup with their aligned alternatives in another language when training neural machine translation models. Since aligned word pairs appear in the same position of each other during training, it is helpful to form bilingual embeddings which are proved useful to provide a performance boost \\citep{liu2019shared}. Experiments on both small and large scale datasets show that our method significantly outperforms the baseline models.", "one-sentence_summary": "In this paper, we present a novel data augmentation method for neural machine translation by using only the original training data without extra data. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xiao|switchingalignedwords_data_augmentation_for_neural_machine_translation", "pdf": "/pdf/2dda90597d761c6c4eb3ec487711fbe704bbcd21.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=wZY52rhUn8", "_bibtex": "@misc{\nxiao2021switchingalignedwords,\ntitle={Switching-Aligned-Words Data Augmentation for Neural Machine Translation},\nauthor={Fengshun Xiao and Zuchao Li and hai zhao},\nyear={2021},\nurl={https://openreview.net/forum?id=LzhEvTWpzH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "LzhEvTWpzH", "replyto": "LzhEvTWpzH", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1514/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538116887, "tmdate": 1606915799079, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1514/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1514/-/Official_Review"}}}, {"id": "owRICe6FYYc", "original": null, "number": 2, "cdate": 1603929709259, "ddate": null, "tcdate": 1603929709259, "tmdate": 1605024424701, "tddate": null, "forum": "LzhEvTWpzH", "replyto": "LzhEvTWpzH", "invitation": "ICLR.cc/2021/Conference/Paper1514/-/Official_Review", "content": {"title": "Unclear whether this idea represents a significant improvement over existing techniques.", "review": "This paper describes a method for data augmentation and/or regularization for machine translation that works by running a word aligner on the parallel data, and then with some probability \\gamma, replacing a source token with its corresponding target token or vice versa. A proposed variant also mixes the embeddings of the two words. Small improvements are shown over simpler noising strategies such as replacing words with placeholder tokens or with random words from the vocabulary.\n\nThis is a neat idea, I like the motivation that the corresponding word in the other language is likely to maintain semantic coherence (though at the cost of linguistic fluency). However, the results are simply not strong enough to warrant a strong recommendation. Table 1 shows that the proposed method performs about the same as the stronger baselines (+0.2 or +0.24 BLEU). Furthermore, with both the method and the baselines having a substantial random component, I strongly urge the authors to carry out several random replications of each experiment, so we can get error bars around these results, and perhaps carry out a replication-aware significance test, such as [1]. As it is, no attempt to do significance testing is made.\n\nI also think this work is missing some details: I assume that the swapping decision is made at runtime (and not at data construction time), allowing different swaps for the same sentence pair depending on the epoch, but it would be nice to be clear about this. I also assume that the authors have implemented shared source, target and output softmax embeddings, so as to strengthen the argument that equivalent words in different languages should have similar embeddings, but again, this should be made clear.\n\nI also think the authors missed some relevant baselines -- these are perhaps equivalent to some of the other baselines mentioned, but they should be discussed [2] [3] [4].\n\nI did not find the analogy to GANs or Figure 1 to be particularly useful for understanding this method.\n\nFinally, there are some spelling errors that indicate that a spell-checker wasn\u2019t run:\n\nGarman \u2192 German\n\nnegrator \u2192 generator\n\n[1] Jonathan Clark, Chris Dyer, Alon Lavie, and Noah Smith, \"Better Hypothesis Testing for Statistical Machine Translation: Controlling for Optimizer Instability\", Proceedings of the Association for Computational Lingustics, 2011. https://github.com/jhclark/multeval\n\n[2] SwitchOut: an Efficient Data Augmentation Algorithm for Neural Machine Translation\nXinyi Wang, Hieu Pham, Zihang Dai, Graham Neubig, EMNLP 2018, https://arxiv.org/abs/1808.07512\n\n[3] Provilkov I, Emelianenko D, Voita E. BPE-dropout: Simple and effective subword regularization. ACL 2020. https://arxiv.org/abs/1910.13267\n\n[4] Robust Neural Machine Translation with Doubly Adversarial Inputs\nYong Cheng, Lu Jiang, Wolfgang Macherey. ACL 2019. https://arxiv.org/abs/1906.02443", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1514/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1514/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Switching-Aligned-Words Data Augmentation for Neural Machine Translation", "authorids": ["~Fengshun_Xiao1", "~Zuchao_Li1", "~hai_zhao1"], "authors": ["Fengshun Xiao", "Zuchao Li", "hai zhao"], "keywords": ["Machine Translation", "Data augmentation"], "abstract": "In neural machine translation (NMT), data augmentation methods such as back-translation make it possible to use extra monolingual data to help improve translation performance, while it needs extra training data and the in-domain monolingual data is not always available. In this paper, we present a novel data augmentation method for neural machine translation by using only the original training data without extra data. More accurately, we randomly replace words or mixup with their aligned alternatives in another language when training neural machine translation models. Since aligned word pairs appear in the same position of each other during training, it is helpful to form bilingual embeddings which are proved useful to provide a performance boost \\citep{liu2019shared}. Experiments on both small and large scale datasets show that our method significantly outperforms the baseline models.", "one-sentence_summary": "In this paper, we present a novel data augmentation method for neural machine translation by using only the original training data without extra data. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xiao|switchingalignedwords_data_augmentation_for_neural_machine_translation", "pdf": "/pdf/2dda90597d761c6c4eb3ec487711fbe704bbcd21.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=wZY52rhUn8", "_bibtex": "@misc{\nxiao2021switchingalignedwords,\ntitle={Switching-Aligned-Words Data Augmentation for Neural Machine Translation},\nauthor={Fengshun Xiao and Zuchao Li and hai zhao},\nyear={2021},\nurl={https://openreview.net/forum?id=LzhEvTWpzH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "LzhEvTWpzH", "replyto": "LzhEvTWpzH", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1514/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538116887, "tmdate": 1606915799079, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1514/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1514/-/Official_Review"}}}, {"id": "j5JWWZU-rHB", "original": null, "number": 3, "cdate": 1603943070219, "ddate": null, "tcdate": 1603943070219, "tmdate": 1605024424637, "tddate": null, "forum": "LzhEvTWpzH", "replyto": "LzhEvTWpzH", "invitation": "ICLR.cc/2021/Conference/Paper1514/-/Official_Review", "content": {"title": "Interesting trick for training NMT systems", "review": "This paper shows that aligning parallel text with fastalign and then randomly replacing source words with their aligned target words, or interpolating their embeddings, improves machine translation.\n\nThis method is different from other data-augmentation methods that try to alter the source sentence without changing its meaning; here, the source sentence is altered into a mixture of the source and target. That\u2019s interesting, but not very strongly motivated.\n\nThe paper doesn\u2019t make clear whether the noise probability / coefficient is optimized on a development set or the test set. Based on Figures 3 and 4, it looks as though these hyperparameters may have been optimized on the test set, which is concerning. For both the baseline systems and your system, hyperparameters should be optimized on a development set and then tested using only a single hyperparameter setting on the test set. If this is what you did, please explicitly state this to reassure the reader.\n\nNot much attempt is made to explain why this method helps; the only analysis is a measurement of cosine similarity between five German-English word pairs. Do you tie word embeddings between the source and target languages (Press and Wolf, 2017)?\n\n- If so, one would expect that the transformer would already be able to place words with similar meanings close together, so the fact that your method improves this is interesting; do you know whether it helps more, e.g., for rare words, proper names, technical terms? Why is fastalign able to align some words better than the transformer? Would an even simpler method help, e.g., if (and only if) word f and word e both occur <= k times in the training data and they occur in exactly the same sentence pairs, then allow f to be switched to e?\n\n- If not, I'd suggest doing so and rerunning the experiments to see if you still get an improvement.\n\nOverall, this seems like a good trick for training NMT systems, but I would hope to see more insight either into why the proposed method works, or how NMT works or doesn\u2019t work.\n", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1514/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1514/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Switching-Aligned-Words Data Augmentation for Neural Machine Translation", "authorids": ["~Fengshun_Xiao1", "~Zuchao_Li1", "~hai_zhao1"], "authors": ["Fengshun Xiao", "Zuchao Li", "hai zhao"], "keywords": ["Machine Translation", "Data augmentation"], "abstract": "In neural machine translation (NMT), data augmentation methods such as back-translation make it possible to use extra monolingual data to help improve translation performance, while it needs extra training data and the in-domain monolingual data is not always available. In this paper, we present a novel data augmentation method for neural machine translation by using only the original training data without extra data. More accurately, we randomly replace words or mixup with their aligned alternatives in another language when training neural machine translation models. Since aligned word pairs appear in the same position of each other during training, it is helpful to form bilingual embeddings which are proved useful to provide a performance boost \\citep{liu2019shared}. Experiments on both small and large scale datasets show that our method significantly outperforms the baseline models.", "one-sentence_summary": "In this paper, we present a novel data augmentation method for neural machine translation by using only the original training data without extra data. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xiao|switchingalignedwords_data_augmentation_for_neural_machine_translation", "pdf": "/pdf/2dda90597d761c6c4eb3ec487711fbe704bbcd21.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=wZY52rhUn8", "_bibtex": "@misc{\nxiao2021switchingalignedwords,\ntitle={Switching-Aligned-Words Data Augmentation for Neural Machine Translation},\nauthor={Fengshun Xiao and Zuchao Li and hai zhao},\nyear={2021},\nurl={https://openreview.net/forum?id=LzhEvTWpzH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "LzhEvTWpzH", "replyto": "LzhEvTWpzH", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1514/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538116887, "tmdate": 1606915799079, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1514/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1514/-/Official_Review"}}}, {"id": "rdApDRkuuiY", "original": null, "number": 4, "cdate": 1604267790969, "ddate": null, "tcdate": 1604267790969, "tmdate": 1605024424472, "tddate": null, "forum": "LzhEvTWpzH", "replyto": "LzhEvTWpzH", "invitation": "ICLR.cc/2021/Conference/Paper1514/-/Official_Review", "content": {"title": "The paper proposes a data augmentation technique where source sentences are perturbed to include a few aligned words from the target language.", "review": "The paper proposes a data augmentation technique where source sentences are perturbed by replacing (or mixing) source words with their aligned counterparts from the target language (while the target sentences remain as is). Alignments can be either obtained from an unsupervised aligner like fast-align or from the attention distribution of an NMT model. Perturbations are aimed to be semantically invariant to preserve the meaning of the source sentence. In addition to simply replacing the source word with the aligned word, authors also try out inputting a weighted combination of both the source word and the target word and refer to this method as \u201cmixing\u201d. Empirical observations suggest that simply replacing the source word with the aligned target yields better results.\n\nI believe the paper in the current form has a lot of scope for improvement. The experimental section needs to be strengthened and more thought needs to go into improving the proposed method.\n\nI have the following suggestions for improving the paper:\nMore experiments: The experiments are based on a reasonably large translation dataset, while the paper \"claims\" to improve the performance for low-resource NMT in section 4.5\n\n> Specially, we find that our method works better on a low resource settings\n", "rating": "2: Strong rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper1514/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1514/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Switching-Aligned-Words Data Augmentation for Neural Machine Translation", "authorids": ["~Fengshun_Xiao1", "~Zuchao_Li1", "~hai_zhao1"], "authors": ["Fengshun Xiao", "Zuchao Li", "hai zhao"], "keywords": ["Machine Translation", "Data augmentation"], "abstract": "In neural machine translation (NMT), data augmentation methods such as back-translation make it possible to use extra monolingual data to help improve translation performance, while it needs extra training data and the in-domain monolingual data is not always available. In this paper, we present a novel data augmentation method for neural machine translation by using only the original training data without extra data. More accurately, we randomly replace words or mixup with their aligned alternatives in another language when training neural machine translation models. Since aligned word pairs appear in the same position of each other during training, it is helpful to form bilingual embeddings which are proved useful to provide a performance boost \\citep{liu2019shared}. Experiments on both small and large scale datasets show that our method significantly outperforms the baseline models.", "one-sentence_summary": "In this paper, we present a novel data augmentation method for neural machine translation by using only the original training data without extra data. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xiao|switchingalignedwords_data_augmentation_for_neural_machine_translation", "pdf": "/pdf/2dda90597d761c6c4eb3ec487711fbe704bbcd21.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=wZY52rhUn8", "_bibtex": "@misc{\nxiao2021switchingalignedwords,\ntitle={Switching-Aligned-Words Data Augmentation for Neural Machine Translation},\nauthor={Fengshun Xiao and Zuchao Li and hai zhao},\nyear={2021},\nurl={https://openreview.net/forum?id=LzhEvTWpzH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "LzhEvTWpzH", "replyto": "LzhEvTWpzH", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1514/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538116887, "tmdate": 1606915799079, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1514/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1514/-/Official_Review"}}}], "count": 10}