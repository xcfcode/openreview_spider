{"notes": [{"id": "HJlTpCEKvS", "original": "HklnD4cOPB", "number": 1410, "cdate": 1569439428672, "ddate": null, "tcdate": 1569439428672, "tmdate": 1577168280273, "tddate": null, "forum": "HJlTpCEKvS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["tstand@cs.stanford.edu", "zamir@cs.stanford.edu", "sdawnchen@gmail.com", "guibas@cs.stanford.edu", "malik@eecs.berkeley.edu", "ssilvio@stanford.edu"], "title": "Which Tasks Should Be Learned Together in Multi-task Learning?", "authors": ["Trevor Standley", "Amir R. Zamir", "Dawn Chen", "Leonidas Guibas", "Jitendra Malik", "Silvio Savarese"], "pdf": "/pdf/2149d3a2992bff379985b93ec6eb341b8059fc6e.pdf", "TL;DR": "We analyze what tasks are best learned together in one network, and which are best to learn separately. ", "abstract": "Many computer vision applications require solving multiple tasks in real-time. A neural network can be trained to solve multiple tasks simultaneously using 'multi-task learning'. This saves computation at inference time as only a single network needs to be evaluated. Unfortunately, this often leads to inferior overall performance as task objectives compete, which consequently poses the question: which tasks should and should not be learned together in one network when employing multi-task learning? We systematically study task cooperation and competition and propose a framework for assigning tasks to a few neural networks such that cooperating tasks are computed by the same neural network, while competing tasks are computed by different networks. Our framework offers a time-accuracy trade-off and can produce better accuracy using less inference time than not only a single large multi-task neural network but also many single-task networks.\n", "code": "https://anonymous.4open.science/r/6cd16de7-0d82-454f-86ef-b540591cd782/", "keywords": ["multi-task learning", "Computer Vision"], "paperhash": "standley|which_tasks_should_be_learned_together_in_multitask_learning", "original_pdf": "/attachment/43205402489be1a1b200a0dda4518a1660b64fd2.pdf", "_bibtex": "@misc{\nstandley2020which,\ntitle={Which Tasks Should Be Learned Together in Multi-task Learning?},\nauthor={Trevor Standley and Amir R. Zamir and Dawn Chen and Leonidas Guibas and Jitendra Malik and Silvio Savarese},\nyear={2020},\nurl={https://openreview.net/forum?id=HJlTpCEKvS}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "T2kvFLucPj", "original": null, "number": 1, "cdate": 1576798722559, "ddate": null, "tcdate": 1576798722559, "tmdate": 1576800914007, "tddate": null, "forum": "HJlTpCEKvS", "replyto": "HJlTpCEKvS", "invitation": "ICLR.cc/2020/Conference/Paper1410/-/Decision", "content": {"decision": "Reject", "comment": "An approach to make multi-task learning is presented, based on the idea of assigning tasks through the concepts of cooperation and competition. \n\nThe main idea is well-motivated and explained well. The experiments demonstrate that the method is promising. However, there are a few  concerns regarding fundamental aspects, such as: how are the decisions affected by the number of parameters? Could ad-hoc algorithms with human in the loop provide the same benefit, when the task-set is small? More importantly, identifying task groups for multi-task learning is an idea presented in prior work, e.g. [1,2,3]. This important body of prior work is not discussed at all in this paper.\n\n[1] Han and Zhang. \"Learning multi-level task groups in multi-task learning\"\n[2] Bonilla et al. \"Multi-task Gaussian process prediction\"\n[3] Zhang and Yang. \"A Survey on Multi-Task Learning\"\n", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["tstand@cs.stanford.edu", "zamir@cs.stanford.edu", "sdawnchen@gmail.com", "guibas@cs.stanford.edu", "malik@eecs.berkeley.edu", "ssilvio@stanford.edu"], "title": "Which Tasks Should Be Learned Together in Multi-task Learning?", "authors": ["Trevor Standley", "Amir R. Zamir", "Dawn Chen", "Leonidas Guibas", "Jitendra Malik", "Silvio Savarese"], "pdf": "/pdf/2149d3a2992bff379985b93ec6eb341b8059fc6e.pdf", "TL;DR": "We analyze what tasks are best learned together in one network, and which are best to learn separately. ", "abstract": "Many computer vision applications require solving multiple tasks in real-time. A neural network can be trained to solve multiple tasks simultaneously using 'multi-task learning'. This saves computation at inference time as only a single network needs to be evaluated. Unfortunately, this often leads to inferior overall performance as task objectives compete, which consequently poses the question: which tasks should and should not be learned together in one network when employing multi-task learning? We systematically study task cooperation and competition and propose a framework for assigning tasks to a few neural networks such that cooperating tasks are computed by the same neural network, while competing tasks are computed by different networks. Our framework offers a time-accuracy trade-off and can produce better accuracy using less inference time than not only a single large multi-task neural network but also many single-task networks.\n", "code": "https://anonymous.4open.science/r/6cd16de7-0d82-454f-86ef-b540591cd782/", "keywords": ["multi-task learning", "Computer Vision"], "paperhash": "standley|which_tasks_should_be_learned_together_in_multitask_learning", "original_pdf": "/attachment/43205402489be1a1b200a0dda4518a1660b64fd2.pdf", "_bibtex": "@misc{\nstandley2020which,\ntitle={Which Tasks Should Be Learned Together in Multi-task Learning?},\nauthor={Trevor Standley and Amir R. Zamir and Dawn Chen and Leonidas Guibas and Jitendra Malik and Silvio Savarese},\nyear={2020},\nurl={https://openreview.net/forum?id=HJlTpCEKvS}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "HJlTpCEKvS", "replyto": "HJlTpCEKvS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795705502, "tmdate": 1576800253303, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1410/-/Decision"}}}, {"id": "HJgJ83u0KS", "original": null, "number": 2, "cdate": 1571880007074, "ddate": null, "tcdate": 1571880007074, "tmdate": 1574793830352, "tddate": null, "forum": "HJlTpCEKvS", "replyto": "HJlTpCEKvS", "invitation": "ICLR.cc/2020/Conference/Paper1410/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #1", "review": "This paper focuses on how to partition a bunch of tasks in several groups and then it use multi-task learning to improve the performance.  The paper makes an observation that multi-task relationships are not entirely correlated to transfer relationships and proposes a computational framework to optimize the assignment of tasks to network under a given computational budget constraint. It experiments on different combinations of the tasks and uses two heuristics to reduce the training overheads, early stopping approximation and higher order approximation. \n\nPlease see the detained comments as follows:\n1. The experiments are based on the assumptions that the network structures (how parameters are shared across tasks) are fixed. From my perspective, understanding how to optimize the parameters sharing across two tasks should be the first step to study how to optimally combine the training of tasks. Otherwise, different parameter sharing structures across tasks may lead to different conclusions.\n\n2. It requires optimization of the article structure. E.g., algorithm 1 is important and should be in the main context.\n\n3. It is also related to neural architecture search and it requires some discussions.\n\n4. The paper is over-length. \n\n5. A lot of typos. \n\nNits:\nPage 1: vide versa -> vice versa\n\nPage 3: two networsk -> two networks\n\nPage 6: budge ?? 1.5\n\nPage 7: overlap between lines (under figure 3)\n\nPage 8: half segmented sentence in section 6.\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1410/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1410/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["tstand@cs.stanford.edu", "zamir@cs.stanford.edu", "sdawnchen@gmail.com", "guibas@cs.stanford.edu", "malik@eecs.berkeley.edu", "ssilvio@stanford.edu"], "title": "Which Tasks Should Be Learned Together in Multi-task Learning?", "authors": ["Trevor Standley", "Amir R. Zamir", "Dawn Chen", "Leonidas Guibas", "Jitendra Malik", "Silvio Savarese"], "pdf": "/pdf/2149d3a2992bff379985b93ec6eb341b8059fc6e.pdf", "TL;DR": "We analyze what tasks are best learned together in one network, and which are best to learn separately. ", "abstract": "Many computer vision applications require solving multiple tasks in real-time. A neural network can be trained to solve multiple tasks simultaneously using 'multi-task learning'. This saves computation at inference time as only a single network needs to be evaluated. Unfortunately, this often leads to inferior overall performance as task objectives compete, which consequently poses the question: which tasks should and should not be learned together in one network when employing multi-task learning? We systematically study task cooperation and competition and propose a framework for assigning tasks to a few neural networks such that cooperating tasks are computed by the same neural network, while competing tasks are computed by different networks. Our framework offers a time-accuracy trade-off and can produce better accuracy using less inference time than not only a single large multi-task neural network but also many single-task networks.\n", "code": "https://anonymous.4open.science/r/6cd16de7-0d82-454f-86ef-b540591cd782/", "keywords": ["multi-task learning", "Computer Vision"], "paperhash": "standley|which_tasks_should_be_learned_together_in_multitask_learning", "original_pdf": "/attachment/43205402489be1a1b200a0dda4518a1660b64fd2.pdf", "_bibtex": "@misc{\nstandley2020which,\ntitle={Which Tasks Should Be Learned Together in Multi-task Learning?},\nauthor={Trevor Standley and Amir R. Zamir and Dawn Chen and Leonidas Guibas and Jitendra Malik and Silvio Savarese},\nyear={2020},\nurl={https://openreview.net/forum?id=HJlTpCEKvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJlTpCEKvS", "replyto": "HJlTpCEKvS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1410/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1410/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575736288253, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1410/Reviewers"], "noninvitees": [], "tcdate": 1570237737787, "tmdate": 1575736288265, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1410/-/Official_Review"}}}, {"id": "S1lIQQ92jB", "original": null, "number": 7, "cdate": 1573851933791, "ddate": null, "tcdate": 1573851933791, "tmdate": 1573851933791, "tddate": null, "forum": "HJlTpCEKvS", "replyto": "HkxHDaSaKS", "invitation": "ICLR.cc/2020/Conference/Paper1410/-/Official_Comment", "content": {"title": "Response to Review #2", "comment": "Thank you for your review and your appreciation of the section comparing transfer learning and multi-task learning. We value your comments and will be updating the final version according to the discussion below.\n\nThe data in Figure 3 shows the performance of all-in-one networks and individual networks as the number of parameters grows. The trend seems to be that as the number of parameters increases, training individually actually beats all-in-one networks by wider and wider margins. Thus, competition actually seems to increase as the number of parameters grows. Nevertheless, a distillation approach seems interesting and MTL performance might be improved by distilling from very large networks. We are not aware of any work exploring this technique, but it might be an interesting topic for a separate study of its own.\n\nIf distillation does work for MTL, it could be combined with our technique, possibly leading to even better performance.\n\nWe've edited the paper to exactly 8 pages by removing unnecessary section headings and making minor wording changes.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1410/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1410/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["tstand@cs.stanford.edu", "zamir@cs.stanford.edu", "sdawnchen@gmail.com", "guibas@cs.stanford.edu", "malik@eecs.berkeley.edu", "ssilvio@stanford.edu"], "title": "Which Tasks Should Be Learned Together in Multi-task Learning?", "authors": ["Trevor Standley", "Amir R. Zamir", "Dawn Chen", "Leonidas Guibas", "Jitendra Malik", "Silvio Savarese"], "pdf": "/pdf/2149d3a2992bff379985b93ec6eb341b8059fc6e.pdf", "TL;DR": "We analyze what tasks are best learned together in one network, and which are best to learn separately. ", "abstract": "Many computer vision applications require solving multiple tasks in real-time. A neural network can be trained to solve multiple tasks simultaneously using 'multi-task learning'. This saves computation at inference time as only a single network needs to be evaluated. Unfortunately, this often leads to inferior overall performance as task objectives compete, which consequently poses the question: which tasks should and should not be learned together in one network when employing multi-task learning? We systematically study task cooperation and competition and propose a framework for assigning tasks to a few neural networks such that cooperating tasks are computed by the same neural network, while competing tasks are computed by different networks. Our framework offers a time-accuracy trade-off and can produce better accuracy using less inference time than not only a single large multi-task neural network but also many single-task networks.\n", "code": "https://anonymous.4open.science/r/6cd16de7-0d82-454f-86ef-b540591cd782/", "keywords": ["multi-task learning", "Computer Vision"], "paperhash": "standley|which_tasks_should_be_learned_together_in_multitask_learning", "original_pdf": "/attachment/43205402489be1a1b200a0dda4518a1660b64fd2.pdf", "_bibtex": "@misc{\nstandley2020which,\ntitle={Which Tasks Should Be Learned Together in Multi-task Learning?},\nauthor={Trevor Standley and Amir R. Zamir and Dawn Chen and Leonidas Guibas and Jitendra Malik and Silvio Savarese},\nyear={2020},\nurl={https://openreview.net/forum?id=HJlTpCEKvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJlTpCEKvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1410/Authors", "ICLR.cc/2020/Conference/Paper1410/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1410/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1410/Reviewers", "ICLR.cc/2020/Conference/Paper1410/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1410/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1410/Authors|ICLR.cc/2020/Conference/Paper1410/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504156437, "tmdate": 1576860535352, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1410/Authors", "ICLR.cc/2020/Conference/Paper1410/Reviewers", "ICLR.cc/2020/Conference/Paper1410/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1410/-/Official_Comment"}}}, {"id": "BkeoAM9hor", "original": null, "number": 6, "cdate": 1573851858606, "ddate": null, "tcdate": 1573851858606, "tmdate": 1573851858606, "tddate": null, "forum": "HJlTpCEKvS", "replyto": "ByeKMG-W5H", "invitation": "ICLR.cc/2020/Conference/Paper1410/-/Official_Comment", "content": {"title": "Response to Review #3", "comment": "Thank you for your review and finding the submission \u201ca good paper to open interesting research direction with solid baselines\u201d. We appreciate your comments and will be updating the final version according to the discussion below.\n\nRegarding manually designed groupings, Figure 5 shows that there seems to be a negative correlation between multi-task affinity and the more intuitive transfer learning affinity of Taskonomy. This makes it seem unlikely that a human would be able to do a good job of engineering task groupings without extensive experience. Furthermore, humans are unlikely to leverage the full potential of auxiliary tasks (i.e., tasks that help other tasks when trained with them but have better performance when trained separately).\n\nAs an example, using the data in Table 9, we can see how well the grouping you suggest performs. We can also evaluate an alternative grouping supported by intuition (e.g., group the two 3D tasks together and the two 2D tasks together). The table below shows how each of those groupings perform versus the computationally found optimal grouping:\n\n        Grouping                                           Total Loss (lower is better) \noptimal 3 grouping (SDn,N,nKE)                       0.44235\t\nYour suggested grouping (S, NE, DK)              0.45866\t\n2D/3D grouping (S, DN, KE)                               0.46368\t\n\nWe see that the two human designed groupings underperform the optimal grouping found computationally. In addition, the optimal task grouping is likely architecture/data-dependent at least to a certain extent, which supports developing computational methods for finding them, in contrast to fixed human intuitions.\n\nAs for the comparison to Sener and Koltun (and single traditional network), the reason we chose to make the network wider rather than deeper is that wider networks can emulate multiple networks, but it is not clear that deeper ones can. Nevertheless it would be interesting to see how well deeper (or some combination of deeper and wider) networks perform.  We will have this comparison for the camera ready, but we expect they will perform similarly to the wider networks we use.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1410/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1410/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["tstand@cs.stanford.edu", "zamir@cs.stanford.edu", "sdawnchen@gmail.com", "guibas@cs.stanford.edu", "malik@eecs.berkeley.edu", "ssilvio@stanford.edu"], "title": "Which Tasks Should Be Learned Together in Multi-task Learning?", "authors": ["Trevor Standley", "Amir R. Zamir", "Dawn Chen", "Leonidas Guibas", "Jitendra Malik", "Silvio Savarese"], "pdf": "/pdf/2149d3a2992bff379985b93ec6eb341b8059fc6e.pdf", "TL;DR": "We analyze what tasks are best learned together in one network, and which are best to learn separately. ", "abstract": "Many computer vision applications require solving multiple tasks in real-time. A neural network can be trained to solve multiple tasks simultaneously using 'multi-task learning'. This saves computation at inference time as only a single network needs to be evaluated. Unfortunately, this often leads to inferior overall performance as task objectives compete, which consequently poses the question: which tasks should and should not be learned together in one network when employing multi-task learning? We systematically study task cooperation and competition and propose a framework for assigning tasks to a few neural networks such that cooperating tasks are computed by the same neural network, while competing tasks are computed by different networks. Our framework offers a time-accuracy trade-off and can produce better accuracy using less inference time than not only a single large multi-task neural network but also many single-task networks.\n", "code": "https://anonymous.4open.science/r/6cd16de7-0d82-454f-86ef-b540591cd782/", "keywords": ["multi-task learning", "Computer Vision"], "paperhash": "standley|which_tasks_should_be_learned_together_in_multitask_learning", "original_pdf": "/attachment/43205402489be1a1b200a0dda4518a1660b64fd2.pdf", "_bibtex": "@misc{\nstandley2020which,\ntitle={Which Tasks Should Be Learned Together in Multi-task Learning?},\nauthor={Trevor Standley and Amir R. Zamir and Dawn Chen and Leonidas Guibas and Jitendra Malik and Silvio Savarese},\nyear={2020},\nurl={https://openreview.net/forum?id=HJlTpCEKvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJlTpCEKvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1410/Authors", "ICLR.cc/2020/Conference/Paper1410/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1410/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1410/Reviewers", "ICLR.cc/2020/Conference/Paper1410/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1410/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1410/Authors|ICLR.cc/2020/Conference/Paper1410/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504156437, "tmdate": 1576860535352, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1410/Authors", "ICLR.cc/2020/Conference/Paper1410/Reviewers", "ICLR.cc/2020/Conference/Paper1410/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1410/-/Official_Comment"}}}, {"id": "S1gehxK3sB", "original": null, "number": 5, "cdate": 1573847208236, "ddate": null, "tcdate": 1573847208236, "tmdate": 1573847271185, "tddate": null, "forum": "HJlTpCEKvS", "replyto": "HJgJ83u0KS", "invitation": "ICLR.cc/2020/Conference/Paper1410/-/Official_Comment", "content": {"title": "Response to Review #1", "comment": "Thank you for your review. We appreciate your comments. \n\n1. How to best share parameters:\nWe agree that finding the best way to share parameters is an open problem and a promising direction for future work. However, we believe that a good technique for determining which parameters to share could be used in conjunction with our techniques to yield further performance improvements.\n\nIn our work, we chose the [encoder->multiple decoder] structure because a number of recent and high profile MTL works use it (Kokkinos 2016 (Ubernet), Chen et al 2018 (Gradnorm), Sener et al 2018, Kendall et al. (2018) and many more). Furthermore, we considered two additional sharing schemes, [U-Net with multiple output channels] and [encoder->single decoder with multiple output channels]. Both were found to be inferior to [encoder->multiple decoder]. Experimental numbers below for 1-SNT networks trained on all tasks jointly:\n\n                                                          total_loss (lower is better)\nU-Net                                                               0.414                               14.05% Worse\nencoder->single decoder                             0.377                               3.85% Worse\nencoder->multiple decoders (ours)           0.363\n*note that these numbers cannot be compared with those in the paper due to differences in the loss function.\n\nThis is far from a comprehensive search of parameter sharing possibilities, but out of the techniques commonly used in the literature, the one we chose performs the best.\n\n2. Algorithm 1.\nThanks for the suggestion. We believe that the particular algorithm chosen for our optimization problem is a detail because many algorithms would work equally well, but we can move Algorithm 1 to the main text if other reviewers agree.\n\n3. Neural Architecture Search section.\nWe've added a neural architecture search section in the related work.\n\n4. Paper length. \nWe've edited the paper to exactly 8 pages by removing unnecessary section headings and making minor wording changes.\n\n5. Typos. \nThank you for these edits. We've gone through the entire paper and fixed typos and grammatical errors. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1410/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1410/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["tstand@cs.stanford.edu", "zamir@cs.stanford.edu", "sdawnchen@gmail.com", "guibas@cs.stanford.edu", "malik@eecs.berkeley.edu", "ssilvio@stanford.edu"], "title": "Which Tasks Should Be Learned Together in Multi-task Learning?", "authors": ["Trevor Standley", "Amir R. Zamir", "Dawn Chen", "Leonidas Guibas", "Jitendra Malik", "Silvio Savarese"], "pdf": "/pdf/2149d3a2992bff379985b93ec6eb341b8059fc6e.pdf", "TL;DR": "We analyze what tasks are best learned together in one network, and which are best to learn separately. ", "abstract": "Many computer vision applications require solving multiple tasks in real-time. A neural network can be trained to solve multiple tasks simultaneously using 'multi-task learning'. This saves computation at inference time as only a single network needs to be evaluated. Unfortunately, this often leads to inferior overall performance as task objectives compete, which consequently poses the question: which tasks should and should not be learned together in one network when employing multi-task learning? We systematically study task cooperation and competition and propose a framework for assigning tasks to a few neural networks such that cooperating tasks are computed by the same neural network, while competing tasks are computed by different networks. Our framework offers a time-accuracy trade-off and can produce better accuracy using less inference time than not only a single large multi-task neural network but also many single-task networks.\n", "code": "https://anonymous.4open.science/r/6cd16de7-0d82-454f-86ef-b540591cd782/", "keywords": ["multi-task learning", "Computer Vision"], "paperhash": "standley|which_tasks_should_be_learned_together_in_multitask_learning", "original_pdf": "/attachment/43205402489be1a1b200a0dda4518a1660b64fd2.pdf", "_bibtex": "@misc{\nstandley2020which,\ntitle={Which Tasks Should Be Learned Together in Multi-task Learning?},\nauthor={Trevor Standley and Amir R. Zamir and Dawn Chen and Leonidas Guibas and Jitendra Malik and Silvio Savarese},\nyear={2020},\nurl={https://openreview.net/forum?id=HJlTpCEKvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJlTpCEKvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1410/Authors", "ICLR.cc/2020/Conference/Paper1410/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1410/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1410/Reviewers", "ICLR.cc/2020/Conference/Paper1410/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1410/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1410/Authors|ICLR.cc/2020/Conference/Paper1410/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504156437, "tmdate": 1576860535352, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1410/Authors", "ICLR.cc/2020/Conference/Paper1410/Reviewers", "ICLR.cc/2020/Conference/Paper1410/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1410/-/Official_Comment"}}}, {"id": "r1e4KROnjH", "original": null, "number": 4, "cdate": 1573846652427, "ddate": null, "tcdate": 1573846652427, "tmdate": 1573846652427, "tddate": null, "forum": "HJlTpCEKvS", "replyto": "HJlTpCEKvS", "invitation": "ICLR.cc/2020/Conference/Paper1410/-/Official_Comment", "content": {"title": "Thanks", "comment": "We thank the reviewers and meta-reviewers for their consideration. \n\nWe've edited the paper to exactly 8 pages by removing unnecessary section headings and making minor wording changes.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1410/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1410/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["tstand@cs.stanford.edu", "zamir@cs.stanford.edu", "sdawnchen@gmail.com", "guibas@cs.stanford.edu", "malik@eecs.berkeley.edu", "ssilvio@stanford.edu"], "title": "Which Tasks Should Be Learned Together in Multi-task Learning?", "authors": ["Trevor Standley", "Amir R. Zamir", "Dawn Chen", "Leonidas Guibas", "Jitendra Malik", "Silvio Savarese"], "pdf": "/pdf/2149d3a2992bff379985b93ec6eb341b8059fc6e.pdf", "TL;DR": "We analyze what tasks are best learned together in one network, and which are best to learn separately. ", "abstract": "Many computer vision applications require solving multiple tasks in real-time. A neural network can be trained to solve multiple tasks simultaneously using 'multi-task learning'. This saves computation at inference time as only a single network needs to be evaluated. Unfortunately, this often leads to inferior overall performance as task objectives compete, which consequently poses the question: which tasks should and should not be learned together in one network when employing multi-task learning? We systematically study task cooperation and competition and propose a framework for assigning tasks to a few neural networks such that cooperating tasks are computed by the same neural network, while competing tasks are computed by different networks. Our framework offers a time-accuracy trade-off and can produce better accuracy using less inference time than not only a single large multi-task neural network but also many single-task networks.\n", "code": "https://anonymous.4open.science/r/6cd16de7-0d82-454f-86ef-b540591cd782/", "keywords": ["multi-task learning", "Computer Vision"], "paperhash": "standley|which_tasks_should_be_learned_together_in_multitask_learning", "original_pdf": "/attachment/43205402489be1a1b200a0dda4518a1660b64fd2.pdf", "_bibtex": "@misc{\nstandley2020which,\ntitle={Which Tasks Should Be Learned Together in Multi-task Learning?},\nauthor={Trevor Standley and Amir R. Zamir and Dawn Chen and Leonidas Guibas and Jitendra Malik and Silvio Savarese},\nyear={2020},\nurl={https://openreview.net/forum?id=HJlTpCEKvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJlTpCEKvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1410/Authors", "ICLR.cc/2020/Conference/Paper1410/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1410/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1410/Reviewers", "ICLR.cc/2020/Conference/Paper1410/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1410/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1410/Authors|ICLR.cc/2020/Conference/Paper1410/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504156437, "tmdate": 1576860535352, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1410/Authors", "ICLR.cc/2020/Conference/Paper1410/Reviewers", "ICLR.cc/2020/Conference/Paper1410/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1410/-/Official_Comment"}}}, {"id": "HkxHDaSaKS", "original": null, "number": 1, "cdate": 1571802460930, "ddate": null, "tcdate": 1571802460930, "tmdate": 1572972472493, "tddate": null, "forum": "HJlTpCEKvS", "replyto": "HJlTpCEKvS", "invitation": "ICLR.cc/2020/Conference/Paper1410/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This submission studies how to group tasks to train together to find a better runtime-accuracy trade-off with a single but large multi-task neural model. The authors perform an extensive empirical study in the Taskonomy dataset. I like Section 6.1 in particular, which shows the difference between multi-task learning and transfer learning. \n\nMy main concern is that the competition between different tasks may stem from the limited capacity of the model during training. It might be possible that with enough parameters, the competing tasks become compatible. If I were the authors, I would first train a bigger model on multiple tasks and then distill it into a smaller one, which does not increase inference time. \n\nFurthermore, the paper has more than 8 pages. "}, "signatures": ["ICLR.cc/2020/Conference/Paper1410/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1410/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["tstand@cs.stanford.edu", "zamir@cs.stanford.edu", "sdawnchen@gmail.com", "guibas@cs.stanford.edu", "malik@eecs.berkeley.edu", "ssilvio@stanford.edu"], "title": "Which Tasks Should Be Learned Together in Multi-task Learning?", "authors": ["Trevor Standley", "Amir R. Zamir", "Dawn Chen", "Leonidas Guibas", "Jitendra Malik", "Silvio Savarese"], "pdf": "/pdf/2149d3a2992bff379985b93ec6eb341b8059fc6e.pdf", "TL;DR": "We analyze what tasks are best learned together in one network, and which are best to learn separately. ", "abstract": "Many computer vision applications require solving multiple tasks in real-time. A neural network can be trained to solve multiple tasks simultaneously using 'multi-task learning'. This saves computation at inference time as only a single network needs to be evaluated. Unfortunately, this often leads to inferior overall performance as task objectives compete, which consequently poses the question: which tasks should and should not be learned together in one network when employing multi-task learning? We systematically study task cooperation and competition and propose a framework for assigning tasks to a few neural networks such that cooperating tasks are computed by the same neural network, while competing tasks are computed by different networks. Our framework offers a time-accuracy trade-off and can produce better accuracy using less inference time than not only a single large multi-task neural network but also many single-task networks.\n", "code": "https://anonymous.4open.science/r/6cd16de7-0d82-454f-86ef-b540591cd782/", "keywords": ["multi-task learning", "Computer Vision"], "paperhash": "standley|which_tasks_should_be_learned_together_in_multitask_learning", "original_pdf": "/attachment/43205402489be1a1b200a0dda4518a1660b64fd2.pdf", "_bibtex": "@misc{\nstandley2020which,\ntitle={Which Tasks Should Be Learned Together in Multi-task Learning?},\nauthor={Trevor Standley and Amir R. Zamir and Dawn Chen and Leonidas Guibas and Jitendra Malik and Silvio Savarese},\nyear={2020},\nurl={https://openreview.net/forum?id=HJlTpCEKvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJlTpCEKvS", "replyto": "HJlTpCEKvS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1410/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1410/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575736288253, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1410/Reviewers"], "noninvitees": [], "tcdate": 1570237737787, "tmdate": 1575736288265, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1410/-/Official_Review"}}}, {"id": "ByeKMG-W5H", "original": null, "number": 3, "cdate": 1572045329382, "ddate": null, "tcdate": 1572045329382, "tmdate": 1572972472403, "tddate": null, "forum": "HJlTpCEKvS", "replyto": "HJlTpCEKvS", "invitation": "ICLR.cc/2020/Conference/Paper1410/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper works on the problem if training a set of networks to solve a set of tasks. The authors try to discover an optimal task split into the networks so that the test performances are maximized given a fixed testing resource budget. By default, this requires searching over the entire task combination space and is too slow. The authors propose two strategies for fast approximating the enumerative search. Experiments show their searched combinations give better performance in the fixed-budget testing setting than several alternatives.\n\n+ This paper works on a new and interesting problem. Training more than one networks for a few tasks is definitely a valid idea in real applications and related to broad research fields.\n+ The baseline setup is comprehensive. The difference between optimal, random, and worst clearly shows this problem worths effort for research.\n- I believe this problem setup requires a larger task set. In the paper the authors manually picked 5 tasks. It seems straightforward for a human to manually group them together: segmentation and edge/ surface + depth/ keypoint for 3 networks. It is unclear how better a network can do than a human in one minute or should we expect learning the task split is better than manual design.\n- Both technical contributions in Section 3.3 look straightforward. Given the good performance in Figure3, it is fine.\n- I am confused by the comparison to Sener and Koltun. How do you change the inference budget for them? If it is changing the number of channels for a single network, I believe it can be improved more.\n\nOverall I believe this is a good paper to open interesting research direction with solid baselines. I am happy to accept this paper and see more exciting future works in this direction."}, "signatures": ["ICLR.cc/2020/Conference/Paper1410/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1410/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["tstand@cs.stanford.edu", "zamir@cs.stanford.edu", "sdawnchen@gmail.com", "guibas@cs.stanford.edu", "malik@eecs.berkeley.edu", "ssilvio@stanford.edu"], "title": "Which Tasks Should Be Learned Together in Multi-task Learning?", "authors": ["Trevor Standley", "Amir R. Zamir", "Dawn Chen", "Leonidas Guibas", "Jitendra Malik", "Silvio Savarese"], "pdf": "/pdf/2149d3a2992bff379985b93ec6eb341b8059fc6e.pdf", "TL;DR": "We analyze what tasks are best learned together in one network, and which are best to learn separately. ", "abstract": "Many computer vision applications require solving multiple tasks in real-time. A neural network can be trained to solve multiple tasks simultaneously using 'multi-task learning'. This saves computation at inference time as only a single network needs to be evaluated. Unfortunately, this often leads to inferior overall performance as task objectives compete, which consequently poses the question: which tasks should and should not be learned together in one network when employing multi-task learning? We systematically study task cooperation and competition and propose a framework for assigning tasks to a few neural networks such that cooperating tasks are computed by the same neural network, while competing tasks are computed by different networks. Our framework offers a time-accuracy trade-off and can produce better accuracy using less inference time than not only a single large multi-task neural network but also many single-task networks.\n", "code": "https://anonymous.4open.science/r/6cd16de7-0d82-454f-86ef-b540591cd782/", "keywords": ["multi-task learning", "Computer Vision"], "paperhash": "standley|which_tasks_should_be_learned_together_in_multitask_learning", "original_pdf": "/attachment/43205402489be1a1b200a0dda4518a1660b64fd2.pdf", "_bibtex": "@misc{\nstandley2020which,\ntitle={Which Tasks Should Be Learned Together in Multi-task Learning?},\nauthor={Trevor Standley and Amir R. Zamir and Dawn Chen and Leonidas Guibas and Jitendra Malik and Silvio Savarese},\nyear={2020},\nurl={https://openreview.net/forum?id=HJlTpCEKvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJlTpCEKvS", "replyto": "HJlTpCEKvS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1410/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1410/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575736288253, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1410/Reviewers"], "noninvitees": [], "tcdate": 1570237737787, "tmdate": 1575736288265, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1410/-/Official_Review"}}}], "count": 9}