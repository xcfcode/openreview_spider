{"notes": [{"id": "SJeLIgBKPS", "original": "HklQm9ltwB", "number": 2324, "cdate": 1569439822121, "ddate": null, "tcdate": 1569439822121, "tmdate": 1583912034150, "tddate": null, "forum": "SJeLIgBKPS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["vfleaking@gmail.com", "lijian83@mail.tsinghua.edu.cn"], "title": "Gradient Descent Maximizes the Margin of Homogeneous Neural Networks", "authors": ["Kaifeng Lyu", "Jian Li"], "pdf": "/pdf/1961d0a01f9b41951a88793dcc0e818a80d108fe.pdf", "TL;DR": "We study the implicit bias of gradient descent and prove under a minimal set of assumptions that the parameter direction of homogeneous models converges to KKT points of a natural margin maximization problem.", "abstract": "In this paper, we study the implicit regularization of the gradient descent algorithm in homogeneous neural networks, including fully-connected and convolutional neural networks with ReLU or LeakyReLU activations. In particular, we study the gradient descent or gradient flow (i.e., gradient descent with infinitesimal step size) optimizing the logistic loss or cross-entropy loss of any homogeneous model (possibly non-smooth), and show that if the training loss decreases below a certain threshold, then we can define a smoothed version of the normalized margin which increases over time. We also formulate a natural constrained optimization problem related to margin maximization, and prove that both the normalized margin and its smoothed version converge to the objective value at a KKT point of the optimization problem. Our results generalize the previous results for logistic regression with one-layer or multi-layer linear networks, and provide more quantitative convergence results with weaker assumptions than previous results for homogeneous smooth neural networks. We conduct several experiments to justify our theoretical finding on MNIST and CIFAR-10 datasets. Finally, as margin is closely related to robustness, we discuss potential benefits of training longer for improving the robustness of the model.", "keywords": ["margin", "homogeneous", "gradient descent"], "paperhash": "lyu|gradient_descent_maximizes_the_margin_of_homogeneous_neural_networks", "code": "https://github.com/vfleaking/max-margin", "_bibtex": "@inproceedings{\nLyu2020Gradient,\ntitle={Gradient Descent Maximizes the Margin of Homogeneous Neural Networks},\nauthor={Kaifeng Lyu and Jian Li},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJeLIgBKPS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/d1a24a7d706bfb9a441f0228882c153ae05937d9.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "SLyRt7Anr", "original": null, "number": 1, "cdate": 1576798746205, "ddate": null, "tcdate": 1576798746205, "tmdate": 1576800889917, "tddate": null, "forum": "SJeLIgBKPS", "replyto": "SJeLIgBKPS", "invitation": "ICLR.cc/2020/Conference/Paper2324/-/Decision", "content": {"decision": "Accept (Talk)", "comment": "This paper studies the implicit regularization of the gradient descent in homogeneous and shows that when the training loss falls below a threshold, then the smoothed. This study generalizes some of the earlier related works by relying on weaker assumptions. Experiments on MNIST and CIFAR-10 are provided to backup the theoretical findings of the paper. \nR2 had some concern about one of the assumptions in this work (A4). While authors admitted that (A4) may not hold for all neural networks and all datasets, they stressed that this assumptions is reasonable when the network is overparameterized and can perfectly fit the training data. Overall, all reviewers are very positive about this submission and find a valuable step toward understanding implicit regularization.\n", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["vfleaking@gmail.com", "lijian83@mail.tsinghua.edu.cn"], "title": "Gradient Descent Maximizes the Margin of Homogeneous Neural Networks", "authors": ["Kaifeng Lyu", "Jian Li"], "pdf": "/pdf/1961d0a01f9b41951a88793dcc0e818a80d108fe.pdf", "TL;DR": "We study the implicit bias of gradient descent and prove under a minimal set of assumptions that the parameter direction of homogeneous models converges to KKT points of a natural margin maximization problem.", "abstract": "In this paper, we study the implicit regularization of the gradient descent algorithm in homogeneous neural networks, including fully-connected and convolutional neural networks with ReLU or LeakyReLU activations. In particular, we study the gradient descent or gradient flow (i.e., gradient descent with infinitesimal step size) optimizing the logistic loss or cross-entropy loss of any homogeneous model (possibly non-smooth), and show that if the training loss decreases below a certain threshold, then we can define a smoothed version of the normalized margin which increases over time. We also formulate a natural constrained optimization problem related to margin maximization, and prove that both the normalized margin and its smoothed version converge to the objective value at a KKT point of the optimization problem. Our results generalize the previous results for logistic regression with one-layer or multi-layer linear networks, and provide more quantitative convergence results with weaker assumptions than previous results for homogeneous smooth neural networks. We conduct several experiments to justify our theoretical finding on MNIST and CIFAR-10 datasets. Finally, as margin is closely related to robustness, we discuss potential benefits of training longer for improving the robustness of the model.", "keywords": ["margin", "homogeneous", "gradient descent"], "paperhash": "lyu|gradient_descent_maximizes_the_margin_of_homogeneous_neural_networks", "code": "https://github.com/vfleaking/max-margin", "_bibtex": "@inproceedings{\nLyu2020Gradient,\ntitle={Gradient Descent Maximizes the Margin of Homogeneous Neural Networks},\nauthor={Kaifeng Lyu and Jian Li},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJeLIgBKPS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/d1a24a7d706bfb9a441f0228882c153ae05937d9.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "SJeLIgBKPS", "replyto": "SJeLIgBKPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795725052, "tmdate": 1576800276805, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2324/-/Decision"}}}, {"id": "rJegO1tfKB", "original": null, "number": 1, "cdate": 1571094376157, "ddate": null, "tcdate": 1571094376157, "tmdate": 1574723690400, "tddate": null, "forum": "SJeLIgBKPS", "replyto": "SJeLIgBKPS", "invitation": "ICLR.cc/2020/Conference/Paper2324/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "N/A", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #2", "review": "This paper studies the implicit regularization phenomenon. More precisely, given separable data the authors ask whether homogenous functions (including neural networks) trained by gradient flow/descent converge to the max-margin solution.  The authors show that the limit points of gradient descent are KKT points of a constrained optimization problem.\n\n-I think that the topic is important and the authors clearly made some interesting insights.\n-The main results of this paper (Theorem 4.1 and Theorem 4.4) require that assumption (A4) is satisfied. Assumption (A4) essentially means, that gradient flow/descent is able to reach weights, such that every data x_n is classified correctly. To me this seems to be a quit restrictive assumption as due to the nonconvexity of the neural net there is a priori no reason to assume that such a point is reached. In this sense, the paper only studies the latter part of the training process. \n\nI feel that Assumption (A4) clearly weakens the strength of the main results. However, because the topic studied by the paper is interesting and the authors have obtained some interesting insights, I decided to rate the paper as a weak accept.\n\nTypos:\n-p. 4: \"Very Recently\"\n-p. 7 and p. 9: \"homogenuous\" (instead of \"homogeneous\")\n\n----------\n\nI want to thank the authors for their response. However, I will stand by me evaluation and will not change it.\nI agree though that assumption (A4) is indeed reasonable, although of course very strong.  \n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper2324/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2324/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["vfleaking@gmail.com", "lijian83@mail.tsinghua.edu.cn"], "title": "Gradient Descent Maximizes the Margin of Homogeneous Neural Networks", "authors": ["Kaifeng Lyu", "Jian Li"], "pdf": "/pdf/1961d0a01f9b41951a88793dcc0e818a80d108fe.pdf", "TL;DR": "We study the implicit bias of gradient descent and prove under a minimal set of assumptions that the parameter direction of homogeneous models converges to KKT points of a natural margin maximization problem.", "abstract": "In this paper, we study the implicit regularization of the gradient descent algorithm in homogeneous neural networks, including fully-connected and convolutional neural networks with ReLU or LeakyReLU activations. In particular, we study the gradient descent or gradient flow (i.e., gradient descent with infinitesimal step size) optimizing the logistic loss or cross-entropy loss of any homogeneous model (possibly non-smooth), and show that if the training loss decreases below a certain threshold, then we can define a smoothed version of the normalized margin which increases over time. We also formulate a natural constrained optimization problem related to margin maximization, and prove that both the normalized margin and its smoothed version converge to the objective value at a KKT point of the optimization problem. Our results generalize the previous results for logistic regression with one-layer or multi-layer linear networks, and provide more quantitative convergence results with weaker assumptions than previous results for homogeneous smooth neural networks. We conduct several experiments to justify our theoretical finding on MNIST and CIFAR-10 datasets. Finally, as margin is closely related to robustness, we discuss potential benefits of training longer for improving the robustness of the model.", "keywords": ["margin", "homogeneous", "gradient descent"], "paperhash": "lyu|gradient_descent_maximizes_the_margin_of_homogeneous_neural_networks", "code": "https://github.com/vfleaking/max-margin", "_bibtex": "@inproceedings{\nLyu2020Gradient,\ntitle={Gradient Descent Maximizes the Margin of Homogeneous Neural Networks},\nauthor={Kaifeng Lyu and Jian Li},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJeLIgBKPS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/d1a24a7d706bfb9a441f0228882c153ae05937d9.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SJeLIgBKPS", "replyto": "SJeLIgBKPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2324/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2324/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576137988388, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2324/Reviewers"], "noninvitees": [], "tcdate": 1570237724452, "tmdate": 1576137988402, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2324/-/Official_Review"}}}, {"id": "BylrCxVAFB", "original": null, "number": 3, "cdate": 1571860684661, "ddate": null, "tcdate": 1571860684661, "tmdate": 1574225791359, "tddate": null, "forum": "SJeLIgBKPS", "replyto": "SJeLIgBKPS", "invitation": "ICLR.cc/2020/Conference/Paper2324/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "8: Accept", "review_assessment:_checking_correctness_of_experiments": "N/A", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #1", "review": "This is a strong deep learning theory paper, and I recommend to accept.\n\nThis paper studies the trajectory induced by applying gradient descent/gradient flow for optimizing a homogeneous model with exponential tail loss functions, including logistic and cross-entropy loss in particular. This is an important direction in recent theoretical studies on deep learning as we need to understand which global minimizer the training algorithm picks to analyze the generalization behavior. \n\nThis paper makes a significant contribution to this direction. This paper rigorously proves gradient descent / gradient flow can maximize the L2 margin of homogeneous models. Existing works mostly focus on linear models or deep linear networks, and comparing with Nascon et al., 2019a, the assumptions in this paper are significantly weaker. Furthermore, this paper provides convergence rates, which seem to be the first work of this kind for non-linear models.\n\nI really like Lemma 5.1. This is not only a technical lemma for proving the main theorem. Lemma 5.1 itself has a nice geometric interpretation. It naturally decomposes the dynamics of the smoothed version into a radial component and a tangential velocity component. I believe this lemma can be useful in other settings as well.\n\n\nComments:\nThe bibliography should be fixed. Some papers are already published, so they should not be cited as the arXiv version, and author lists in some papers have \"et al.\"\n\n-----------------------------------------------------\nI have read the rebuttal and I maintain my score.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper2324/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2324/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["vfleaking@gmail.com", "lijian83@mail.tsinghua.edu.cn"], "title": "Gradient Descent Maximizes the Margin of Homogeneous Neural Networks", "authors": ["Kaifeng Lyu", "Jian Li"], "pdf": "/pdf/1961d0a01f9b41951a88793dcc0e818a80d108fe.pdf", "TL;DR": "We study the implicit bias of gradient descent and prove under a minimal set of assumptions that the parameter direction of homogeneous models converges to KKT points of a natural margin maximization problem.", "abstract": "In this paper, we study the implicit regularization of the gradient descent algorithm in homogeneous neural networks, including fully-connected and convolutional neural networks with ReLU or LeakyReLU activations. In particular, we study the gradient descent or gradient flow (i.e., gradient descent with infinitesimal step size) optimizing the logistic loss or cross-entropy loss of any homogeneous model (possibly non-smooth), and show that if the training loss decreases below a certain threshold, then we can define a smoothed version of the normalized margin which increases over time. We also formulate a natural constrained optimization problem related to margin maximization, and prove that both the normalized margin and its smoothed version converge to the objective value at a KKT point of the optimization problem. Our results generalize the previous results for logistic regression with one-layer or multi-layer linear networks, and provide more quantitative convergence results with weaker assumptions than previous results for homogeneous smooth neural networks. We conduct several experiments to justify our theoretical finding on MNIST and CIFAR-10 datasets. Finally, as margin is closely related to robustness, we discuss potential benefits of training longer for improving the robustness of the model.", "keywords": ["margin", "homogeneous", "gradient descent"], "paperhash": "lyu|gradient_descent_maximizes_the_margin_of_homogeneous_neural_networks", "code": "https://github.com/vfleaking/max-margin", "_bibtex": "@inproceedings{\nLyu2020Gradient,\ntitle={Gradient Descent Maximizes the Margin of Homogeneous Neural Networks},\nauthor={Kaifeng Lyu and Jian Li},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJeLIgBKPS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/d1a24a7d706bfb9a441f0228882c153ae05937d9.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SJeLIgBKPS", "replyto": "SJeLIgBKPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2324/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2324/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576137988388, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2324/Reviewers"], "noninvitees": [], "tcdate": 1570237724452, "tmdate": 1576137988402, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2324/-/Official_Review"}}}, {"id": "ByeXHx5KoS", "original": null, "number": 3, "cdate": 1573654587051, "ddate": null, "tcdate": 1573654587051, "tmdate": 1573654587051, "tddate": null, "forum": "SJeLIgBKPS", "replyto": "BylrCxVAFB", "invitation": "ICLR.cc/2020/Conference/Paper2324/-/Official_Comment", "content": {"title": "Response to Reviewer #1", "comment": "Thanks for your appreciation! We will fix the errors in the bibliography."}, "signatures": ["ICLR.cc/2020/Conference/Paper2324/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2324/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["vfleaking@gmail.com", "lijian83@mail.tsinghua.edu.cn"], "title": "Gradient Descent Maximizes the Margin of Homogeneous Neural Networks", "authors": ["Kaifeng Lyu", "Jian Li"], "pdf": "/pdf/1961d0a01f9b41951a88793dcc0e818a80d108fe.pdf", "TL;DR": "We study the implicit bias of gradient descent and prove under a minimal set of assumptions that the parameter direction of homogeneous models converges to KKT points of a natural margin maximization problem.", "abstract": "In this paper, we study the implicit regularization of the gradient descent algorithm in homogeneous neural networks, including fully-connected and convolutional neural networks with ReLU or LeakyReLU activations. In particular, we study the gradient descent or gradient flow (i.e., gradient descent with infinitesimal step size) optimizing the logistic loss or cross-entropy loss of any homogeneous model (possibly non-smooth), and show that if the training loss decreases below a certain threshold, then we can define a smoothed version of the normalized margin which increases over time. We also formulate a natural constrained optimization problem related to margin maximization, and prove that both the normalized margin and its smoothed version converge to the objective value at a KKT point of the optimization problem. Our results generalize the previous results for logistic regression with one-layer or multi-layer linear networks, and provide more quantitative convergence results with weaker assumptions than previous results for homogeneous smooth neural networks. We conduct several experiments to justify our theoretical finding on MNIST and CIFAR-10 datasets. Finally, as margin is closely related to robustness, we discuss potential benefits of training longer for improving the robustness of the model.", "keywords": ["margin", "homogeneous", "gradient descent"], "paperhash": "lyu|gradient_descent_maximizes_the_margin_of_homogeneous_neural_networks", "code": "https://github.com/vfleaking/max-margin", "_bibtex": "@inproceedings{\nLyu2020Gradient,\ntitle={Gradient Descent Maximizes the Margin of Homogeneous Neural Networks},\nauthor={Kaifeng Lyu and Jian Li},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJeLIgBKPS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/d1a24a7d706bfb9a441f0228882c153ae05937d9.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJeLIgBKPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2324/Authors", "ICLR.cc/2020/Conference/Paper2324/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2324/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2324/Reviewers", "ICLR.cc/2020/Conference/Paper2324/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2324/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2324/Authors|ICLR.cc/2020/Conference/Paper2324/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504143054, "tmdate": 1576860530390, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2324/Authors", "ICLR.cc/2020/Conference/Paper2324/Reviewers", "ICLR.cc/2020/Conference/Paper2324/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2324/-/Official_Comment"}}}, {"id": "BklZZeqtjB", "original": null, "number": 2, "cdate": 1573654521470, "ddate": null, "tcdate": 1573654521470, "tmdate": 1573654521470, "tddate": null, "forum": "SJeLIgBKPS", "replyto": "rJegO1tfKB", "invitation": "ICLR.cc/2020/Conference/Paper2324/-/Official_Comment", "content": {"title": "Response to Reviewer #2", "comment": "Thanks for your reviews and for pointing out the typos!\n\nWe admit that (A4) may not hold for all neural networks and all datasets. Indeed, the loss of a neural network is highly non-convex and (A4) seems to be a quite strong assumption. However, it is known that sufficiently overparameterized neural networks can fit the training set through (stochastic) gradient descent. As we discussed in the introduction of our paper, state-of-the-art neural networks are typically overparameterized, and they can perfectly fit not only normal data but also randomly labeled data easily in image classification tasks (Zhang et al., 2017). Theoretically, (Allen-Zhu et al., 2019; Du et al. 2018; Zou et al., 2018) showed that gradient descent can achieve 100% training accuracy if the width is large enough. Given the evidence from both theory and practice, we believe (A4) is a reasonable assumption (at least for many DL tasks)."}, "signatures": ["ICLR.cc/2020/Conference/Paper2324/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2324/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["vfleaking@gmail.com", "lijian83@mail.tsinghua.edu.cn"], "title": "Gradient Descent Maximizes the Margin of Homogeneous Neural Networks", "authors": ["Kaifeng Lyu", "Jian Li"], "pdf": "/pdf/1961d0a01f9b41951a88793dcc0e818a80d108fe.pdf", "TL;DR": "We study the implicit bias of gradient descent and prove under a minimal set of assumptions that the parameter direction of homogeneous models converges to KKT points of a natural margin maximization problem.", "abstract": "In this paper, we study the implicit regularization of the gradient descent algorithm in homogeneous neural networks, including fully-connected and convolutional neural networks with ReLU or LeakyReLU activations. In particular, we study the gradient descent or gradient flow (i.e., gradient descent with infinitesimal step size) optimizing the logistic loss or cross-entropy loss of any homogeneous model (possibly non-smooth), and show that if the training loss decreases below a certain threshold, then we can define a smoothed version of the normalized margin which increases over time. We also formulate a natural constrained optimization problem related to margin maximization, and prove that both the normalized margin and its smoothed version converge to the objective value at a KKT point of the optimization problem. Our results generalize the previous results for logistic regression with one-layer or multi-layer linear networks, and provide more quantitative convergence results with weaker assumptions than previous results for homogeneous smooth neural networks. We conduct several experiments to justify our theoretical finding on MNIST and CIFAR-10 datasets. Finally, as margin is closely related to robustness, we discuss potential benefits of training longer for improving the robustness of the model.", "keywords": ["margin", "homogeneous", "gradient descent"], "paperhash": "lyu|gradient_descent_maximizes_the_margin_of_homogeneous_neural_networks", "code": "https://github.com/vfleaking/max-margin", "_bibtex": "@inproceedings{\nLyu2020Gradient,\ntitle={Gradient Descent Maximizes the Margin of Homogeneous Neural Networks},\nauthor={Kaifeng Lyu and Jian Li},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJeLIgBKPS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/d1a24a7d706bfb9a441f0228882c153ae05937d9.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJeLIgBKPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2324/Authors", "ICLR.cc/2020/Conference/Paper2324/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2324/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2324/Reviewers", "ICLR.cc/2020/Conference/Paper2324/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2324/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2324/Authors|ICLR.cc/2020/Conference/Paper2324/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504143054, "tmdate": 1576860530390, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2324/Authors", "ICLR.cc/2020/Conference/Paper2324/Reviewers", "ICLR.cc/2020/Conference/Paper2324/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2324/-/Official_Comment"}}}, {"id": "BygiThKFsr", "original": null, "number": 1, "cdate": 1573653698682, "ddate": null, "tcdate": 1573653698682, "tmdate": 1573653698682, "tddate": null, "forum": "SJeLIgBKPS", "replyto": "SJgkPWhoYH", "invitation": "ICLR.cc/2020/Conference/Paper2324/-/Official_Comment", "content": {"title": "Response to Reviewer #3", "comment": "Thanks for your comments!\n\nThe L2-normalization is due to the use of gradient descent (GD is the steepest descent algorithm w.r.t. L2). If we change the optimization algorithm, the normalized margin being optimized should be also changed. Note that this has been studied in the linear case (Gunasekar et al., 2018a): if we run steepest descent with respect to a generic norm $\\|\\cdot\\|$, then the $\\|\\cdot\\|$-normalized margin is maximized. When $\\|\\cdot\\|$ is the L2 norm, it is just the case of gradient descent; when $\\|\\cdot\\|$ is the L1 norm, the corresponding optimization problem is coordinate descent, and it maximizes the L1-normalized margin. Right now, our results only hold for gradient descent and L2 norm. Extending it to more general norm and optimization problems is an interesting future direction.\n\nFor robustness, we want to emphasize that the Lipschitz constant is evaluated after normalizing the weight norm. As the weight norm is always $1$ during training, we can expect that the Lipschitz constant does not get extremely large. In our experiments, we admit that the normalized margin and robustness do not grow in the same speed, so the Lipschitz constant may change; however, the normalized margin and robustness do have quite positive correlations, and we think improving robustness by maximizing normalized margin is relevant (it may be able to provide certified robustness). So we will discuss this phenomenon in the next version of our paper to encourage further discussions."}, "signatures": ["ICLR.cc/2020/Conference/Paper2324/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2324/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["vfleaking@gmail.com", "lijian83@mail.tsinghua.edu.cn"], "title": "Gradient Descent Maximizes the Margin of Homogeneous Neural Networks", "authors": ["Kaifeng Lyu", "Jian Li"], "pdf": "/pdf/1961d0a01f9b41951a88793dcc0e818a80d108fe.pdf", "TL;DR": "We study the implicit bias of gradient descent and prove under a minimal set of assumptions that the parameter direction of homogeneous models converges to KKT points of a natural margin maximization problem.", "abstract": "In this paper, we study the implicit regularization of the gradient descent algorithm in homogeneous neural networks, including fully-connected and convolutional neural networks with ReLU or LeakyReLU activations. In particular, we study the gradient descent or gradient flow (i.e., gradient descent with infinitesimal step size) optimizing the logistic loss or cross-entropy loss of any homogeneous model (possibly non-smooth), and show that if the training loss decreases below a certain threshold, then we can define a smoothed version of the normalized margin which increases over time. We also formulate a natural constrained optimization problem related to margin maximization, and prove that both the normalized margin and its smoothed version converge to the objective value at a KKT point of the optimization problem. Our results generalize the previous results for logistic regression with one-layer or multi-layer linear networks, and provide more quantitative convergence results with weaker assumptions than previous results for homogeneous smooth neural networks. We conduct several experiments to justify our theoretical finding on MNIST and CIFAR-10 datasets. Finally, as margin is closely related to robustness, we discuss potential benefits of training longer for improving the robustness of the model.", "keywords": ["margin", "homogeneous", "gradient descent"], "paperhash": "lyu|gradient_descent_maximizes_the_margin_of_homogeneous_neural_networks", "code": "https://github.com/vfleaking/max-margin", "_bibtex": "@inproceedings{\nLyu2020Gradient,\ntitle={Gradient Descent Maximizes the Margin of Homogeneous Neural Networks},\nauthor={Kaifeng Lyu and Jian Li},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJeLIgBKPS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/d1a24a7d706bfb9a441f0228882c153ae05937d9.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJeLIgBKPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2324/Authors", "ICLR.cc/2020/Conference/Paper2324/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2324/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2324/Reviewers", "ICLR.cc/2020/Conference/Paper2324/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2324/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2324/Authors|ICLR.cc/2020/Conference/Paper2324/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504143054, "tmdate": 1576860530390, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2324/Authors", "ICLR.cc/2020/Conference/Paper2324/Reviewers", "ICLR.cc/2020/Conference/Paper2324/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2324/-/Official_Comment"}}}, {"id": "SJgkPWhoYH", "original": null, "number": 2, "cdate": 1571696983452, "ddate": null, "tcdate": 1571696983452, "tmdate": 1572972353478, "tddate": null, "forum": "SJeLIgBKPS", "replyto": "SJeLIgBKPS", "invitation": "ICLR.cc/2020/Conference/Paper2324/-/Official_Review", "content": {"rating": "8: Accept", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The goal of the paper is to formally prove that gradient flow / gradient descent performed on homogeneous neural network models maximizes the margin of the learnt function; assuming gradient flow/descent manages to separate the data. This is proved in two steps:\n  1. Assuming that gradient descent manages to find a set of network parameters that separate the data, thereafter gradient flow/descent monotonically increases the normalized margin (rather an approximation of it).\n  2. The limit points of optimization are KKT points of the margin maximization optimization problem.\nWhile the main body of the paper presents a restricted set of results, the appendix generalizes this much further applying it to various kinds of loss functions (logistic/cross-entropy, exponential), to multi-class classification and to multi-homogeneous models. There seem to be many subtleties in the proofs and the paper seems to be quite thorough. (I must say that I'm not expert enough to assess the technical novelty of this paper over prior works.)\n\nRecommendation:\nI recommend \"acceptance\". The paper takes a significant step by unifying existing results on margin maximization and going beyond them. \n\nTechnical comments:\n- It is clear that in order to define margin meaningfully, some form of normalization is necessary. But a priori, $\\|\\theta\\|_2^L$ is not the *only* choice; $\\|\\theta\\|^L$ could also work for any norm $\\|\\cdot\\|$. But perhaps the choice of $\\|\\cdot\\|_2$ is special (as Thm 4.4 suggests). It will be nice to have some insights/comments on why this choice of $\\|\\cdot\\|_2$ based normalization is the right one.\n- The paper argues that having a larger margin helps in obtaining better robustness to adversarial perturbations (within $\\|\\cdot\\|$ balls for some choice of $\\|\\cdot\\|$). However note that the notion of \"margin\" is not just a function of the decision boundary, but instead depends on the specific function computed by the neural network --- this is unlike margin maximization in linear models, where \"margin\" in determined entirely by the decision boundary. As the paper argues, if we have an upper bound on the Lipschitz constant w.r.t. $\\|\\cdot\\|$ norm, then we get a lower bound on required adversarial perturbations for any training point. However, this does not mean that training longer is necessarily better because by doing so, we might end up with a larger Lipschitz constant (even after normalizing). So even if the \"margin\" is larger, the actual adversarial perturbations (in $\\|\\cdot\\|$ norm) allowed might get smaller. So I'm not sure how relevant this result is for adversarial robustness.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2324/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2324/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["vfleaking@gmail.com", "lijian83@mail.tsinghua.edu.cn"], "title": "Gradient Descent Maximizes the Margin of Homogeneous Neural Networks", "authors": ["Kaifeng Lyu", "Jian Li"], "pdf": "/pdf/1961d0a01f9b41951a88793dcc0e818a80d108fe.pdf", "TL;DR": "We study the implicit bias of gradient descent and prove under a minimal set of assumptions that the parameter direction of homogeneous models converges to KKT points of a natural margin maximization problem.", "abstract": "In this paper, we study the implicit regularization of the gradient descent algorithm in homogeneous neural networks, including fully-connected and convolutional neural networks with ReLU or LeakyReLU activations. In particular, we study the gradient descent or gradient flow (i.e., gradient descent with infinitesimal step size) optimizing the logistic loss or cross-entropy loss of any homogeneous model (possibly non-smooth), and show that if the training loss decreases below a certain threshold, then we can define a smoothed version of the normalized margin which increases over time. We also formulate a natural constrained optimization problem related to margin maximization, and prove that both the normalized margin and its smoothed version converge to the objective value at a KKT point of the optimization problem. Our results generalize the previous results for logistic regression with one-layer or multi-layer linear networks, and provide more quantitative convergence results with weaker assumptions than previous results for homogeneous smooth neural networks. We conduct several experiments to justify our theoretical finding on MNIST and CIFAR-10 datasets. Finally, as margin is closely related to robustness, we discuss potential benefits of training longer for improving the robustness of the model.", "keywords": ["margin", "homogeneous", "gradient descent"], "paperhash": "lyu|gradient_descent_maximizes_the_margin_of_homogeneous_neural_networks", "code": "https://github.com/vfleaking/max-margin", "_bibtex": "@inproceedings{\nLyu2020Gradient,\ntitle={Gradient Descent Maximizes the Margin of Homogeneous Neural Networks},\nauthor={Kaifeng Lyu and Jian Li},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJeLIgBKPS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/d1a24a7d706bfb9a441f0228882c153ae05937d9.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SJeLIgBKPS", "replyto": "SJeLIgBKPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2324/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2324/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576137988388, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2324/Reviewers"], "noninvitees": [], "tcdate": 1570237724452, "tmdate": 1576137988402, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2324/-/Official_Review"}}}], "count": 8}