{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1524145327806, "tcdate": 1518460615795, "number": 198, "cdate": 1518460615795, "id": "rylRCUJDG", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "rylRCUJDG", "original": "Hk2MHt-3-", "signatures": ["~Anuvabh_Dutt1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "Coupled Ensembles of Neural Networks", "abstract": "We present coupled ensembles of neural networks, which is a reconfiguration of existing neural network models into parallel branches. We empirically show that this modification leads to results on CIFAR and SVHN that are competitive to state of the art, with a greatly reduced parameter count. Additionally, for a fixed parameter, or a training time budget coupled ensembles are significantly better than single branch models. Preliminary results on ImageNet are also promising.", "pdf": "/pdf/6ee5f35f683eea109ab843fb98fd795f792fdd7b.pdf", "TL;DR": "We show that splitting a neural network into parallel branches improves performance for a parameter budget.", "paperhash": "dutt|coupled_ensembles_of_neural_networks", "_bibtex": "@misc{\ndutt2018coupled,\ntitle={Coupled Ensembles of Neural Networks},\nauthor={Anuvabh Dutt and Denis Pellerin and Georges Qu\u00e9not},\nyear={2018},\nurl={https://openreview.net/forum?id=Hk2MHt-3-},\n}", "keywords": ["neural networks", "ensemble learning"], "authors": ["Anuvabh Dutt", "Denis Pellerin", "Georges Qu\u00e9not"], "authorids": ["anuvabhdutt@gmail.com", "denis.pellerin@gipsa-lab.grenoble-inp.fr", "georges.quenot@imag.fr"]}, "nonreaders": [], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": true, "tags": [], "original": {"tddate": null, "ddate": null, "tmdate": 1518730192104, "tcdate": 1507067156473, "number": 3, "cdate": 1518730192095, "id": "Hk2MHt-3-", "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "forum": "Hk2MHt-3-", "original": "rynzSYW2W", "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference"], "content": {"title": "Coupled Ensembles of Neural Networks", "abstract": "We investigate in this paper the architecture of deep convolutional networks. Building on existing state of the art models, we propose a reconfiguration of the model parameters into several parallel branches at the global network level, with each branch being a standalone CNN. We show that this arrangement is an efficient way to significantly reduce the number of parameters while at the same time improving the performance. The use of branches brings an additional form of regularization. In addition to splitting the parameters into parallel branches, we propose a tighter coupling of these branches by averaging their log-probabilities. The tighter coupling favours the learning of better representations, even at the level of the individual branches, as compared to when each branch is trained independently. We refer to this branched architecture as \"coupled ensembles\". The approach is very generic and can be applied with almost any neural network architecture. With coupled ensembles of DenseNet-BC and parameter budget of 25M, we obtain error rates of 2.92%, 15.68% and 1.50% respectively on CIFAR-10, CIFAR-100 and SVHN tasks. For the same parameter budget, DenseNet-BC has an error rate of 3.46%, 17.18%, and 1.8% respectively.  With ensembles of coupled ensembles, of DenseNet-BC networks, with 50M total parameters, we obtain error rates of 2.72%, 15.13% and 1.42% respectively on these tasks.", "pdf": "/pdf/136817040064c14bffc0c514c4cfb078e02cfed4.pdf", "TL;DR": "We show that splitting a neural network into parallel branches improves performance and that proper coupling of the branches improves performance even further.", "paperhash": "dutt|coupled_ensembles_of_neural_networks", "_bibtex": "@misc{\ndutt2018coupled,\ntitle={Coupled Ensembles of Neural Networks},\nauthor={Anuvabh Dutt and Denis Pellerin and Georges Qu\u00e9not},\nyear={2018},\nurl={https://openreview.net/forum?id=Hk2MHt-3-},\n}", "keywords": ["Ensemble learning", "neural networks"], "authors": ["Anuvabh Dutt", "Denis Pellerin", "Georges Qu\u00e9not"], "authorids": ["anuvabh.dutt@univ-grenoble-alpes.fr", "denis.pellerin@gipsa-lab.grenoble-inp.fr", "georges.quenot@imag.fr"]}, "nonreaders": []}, "originalWritable": false, "originalInvitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1506717071958, "id": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Conference"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference"]}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"authors": {"required": false, "order": 1, "values-regex": ".*", "description": "Comma separated list of author names, as they appear in the paper."}, "authorids": {"required": false, "order": 2, "values-regex": ".*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "cdate": 1506717071958}, "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582991575, "tcdate": 1519503600523, "number": 1, "cdate": 1519503600523, "id": "HJOxKBkuz", "invitation": "ICLR.cc/2018/Workshop/-/Paper198/Official_Review", "forum": "rylRCUJDG", "replyto": "rylRCUJDG", "signatures": ["ICLR.cc/2018/Workshop/Paper198/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper198/AnonReviewer1"], "content": {"title": "Interesting observation for neural network ensemble", "rating": "6: Marginally above acceptance threshold", "review": "This paper introduces a meta neural network structure with multiple branches. Each of the branches outputs a softmax probability, which is them averaged over the branches as then final output the whole network. The authors empirically show that jointly training these branches yields better results than training the branches independently.\n\nOverall, I think this is an interesting finding, as usually we would expect that independantly trained models may have more diversity, and thus lead to better ensemble performance. The results shown here seem to suggest that couple training is actually more effective. I'm curious to see how the model diversity obtained by the proposed method compared to training models independently, by perform an analytical experiments similar to that in ref [1] given below. I would also expect that more comparisons against indepent training on multple datasets.\n\nFinally, the deeply fused network and fractalnet are two related network architectures, which should be discussed in the related work.\n\n\n[1] Snapshot Ensemble: Train 1, Get M for Free. Huang et al, ICLR 2017\n[2] FractalNet: Ultra-Deep Neural Networks without Residual, Larsson et al, ICLR 2017\n[3] Deeply-Fused Nets. Wang et al, arXiv 2016", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Coupled Ensembles of Neural Networks", "abstract": "We present coupled ensembles of neural networks, which is a reconfiguration of existing neural network models into parallel branches. We empirically show that this modification leads to results on CIFAR and SVHN that are competitive to state of the art, with a greatly reduced parameter count. Additionally, for a fixed parameter, or a training time budget coupled ensembles are significantly better than single branch models. Preliminary results on ImageNet are also promising.", "pdf": "/pdf/6ee5f35f683eea109ab843fb98fd795f792fdd7b.pdf", "TL;DR": "We show that splitting a neural network into parallel branches improves performance for a parameter budget.", "paperhash": "dutt|coupled_ensembles_of_neural_networks", "_bibtex": "@misc{\ndutt2018coupled,\ntitle={Coupled Ensembles of Neural Networks},\nauthor={Anuvabh Dutt and Denis Pellerin and Georges Qu\u00e9not},\nyear={2018},\nurl={https://openreview.net/forum?id=Hk2MHt-3-},\n}", "keywords": ["neural networks", "ensemble learning"], "authors": ["Anuvabh Dutt", "Denis Pellerin", "Georges Qu\u00e9not"], "authorids": ["anuvabhdutt@gmail.com", "denis.pellerin@gipsa-lab.grenoble-inp.fr", "georges.quenot@imag.fr"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582991385, "id": "ICLR.cc/2018/Workshop/-/Paper198/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper198/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper198/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper198/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper198/AnonReviewer2"], "reply": {"forum": "rylRCUJDG", "replyto": "rylRCUJDG", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper198/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper198/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582991385}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582916662, "tcdate": 1520393287041, "number": 2, "cdate": 1520393287041, "id": "B1JI3Rhuz", "invitation": "ICLR.cc/2018/Workshop/-/Paper198/Official_Review", "forum": "rylRCUJDG", "replyto": "rylRCUJDG", "signatures": ["ICLR.cc/2018/Workshop/Paper198/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper198/AnonReviewer3"], "content": {"title": "Deep model with coupled branches ", "rating": "5: Marginally below acceptance threshold", "review": "In this work, the authors propose to design a light-weight and accurate deep model, by coupling several branches at the softmax layer. Here are some comments:\n1 The idea is not quite novel, since MobileNet, ShuffleNet are the similar kind of model type via group convolutions. \n2 The literature review of light-weight model design is missing.\n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Coupled Ensembles of Neural Networks", "abstract": "We present coupled ensembles of neural networks, which is a reconfiguration of existing neural network models into parallel branches. We empirically show that this modification leads to results on CIFAR and SVHN that are competitive to state of the art, with a greatly reduced parameter count. Additionally, for a fixed parameter, or a training time budget coupled ensembles are significantly better than single branch models. Preliminary results on ImageNet are also promising.", "pdf": "/pdf/6ee5f35f683eea109ab843fb98fd795f792fdd7b.pdf", "TL;DR": "We show that splitting a neural network into parallel branches improves performance for a parameter budget.", "paperhash": "dutt|coupled_ensembles_of_neural_networks", "_bibtex": "@misc{\ndutt2018coupled,\ntitle={Coupled Ensembles of Neural Networks},\nauthor={Anuvabh Dutt and Denis Pellerin and Georges Qu\u00e9not},\nyear={2018},\nurl={https://openreview.net/forum?id=Hk2MHt-3-},\n}", "keywords": ["neural networks", "ensemble learning"], "authors": ["Anuvabh Dutt", "Denis Pellerin", "Georges Qu\u00e9not"], "authorids": ["anuvabhdutt@gmail.com", "denis.pellerin@gipsa-lab.grenoble-inp.fr", "georges.quenot@imag.fr"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582991385, "id": "ICLR.cc/2018/Workshop/-/Paper198/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper198/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper198/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper198/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper198/AnonReviewer2"], "reply": {"forum": "rylRCUJDG", "replyto": "rylRCUJDG", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper198/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper198/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582991385}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582913557, "tcdate": 1520434468357, "number": 3, "cdate": 1520434468357, "id": "Hy27auTuz", "invitation": "ICLR.cc/2018/Workshop/-/Paper198/Official_Review", "forum": "rylRCUJDG", "replyto": "rylRCUJDG", "signatures": ["ICLR.cc/2018/Workshop/Paper198/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper198/AnonReviewer2"], "content": {"title": "Paper is at a test of concept stage and shows promising experimental results. It does not provide much discussion on why it works well.", "rating": "6: Marginally above acceptance threshold", "review": "Quality:\n\nThe paper proposes a neural network architecture that fuses multiple parallel neural network architectures into one architecture. This is achieved by branching of standard NN architectures, each taking an image as input and producing outputs to be combined in the last layer. The paper shows promising results on improvement in accuracy compared to architectures without branching even when ensemble uses smaller number of parameters and it also performs better compared to to the ensemble of independent NN outputs via averaging.\n\nThe paper does not provide much discussion regarding why it works well compared to the baseline. And the experiments on ImageNet data seems incomplete due to computing resource constraints. The paper does not touch up on ease of implementing this architecture using existing NN libraries. However, the paper is at a test-of-concept stage and provides good experimental results on non-trivial dataset.\n\nClarity: \n\nThe paper is written in simple terms and hence, easy to follow. There seems to be small typos: \n\n- In section 3.2, \"Row 4 of table 2 shows the results obtained by an ensemble 4 DenseNet-L100-k12 models, each of which where trained seperately.\" I think it should be row 2 of table 2 not row 4.\n\n- \"All yyperparameters\" -> \"All hyperparameters\"\n\nSignificance: It can become a useful in various NN applications.\n\nOriginality: N/A. This reviewer is not knowledgeable enough to comment on the originality of the work.\n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Coupled Ensembles of Neural Networks", "abstract": "We present coupled ensembles of neural networks, which is a reconfiguration of existing neural network models into parallel branches. We empirically show that this modification leads to results on CIFAR and SVHN that are competitive to state of the art, with a greatly reduced parameter count. Additionally, for a fixed parameter, or a training time budget coupled ensembles are significantly better than single branch models. Preliminary results on ImageNet are also promising.", "pdf": "/pdf/6ee5f35f683eea109ab843fb98fd795f792fdd7b.pdf", "TL;DR": "We show that splitting a neural network into parallel branches improves performance for a parameter budget.", "paperhash": "dutt|coupled_ensembles_of_neural_networks", "_bibtex": "@misc{\ndutt2018coupled,\ntitle={Coupled Ensembles of Neural Networks},\nauthor={Anuvabh Dutt and Denis Pellerin and Georges Qu\u00e9not},\nyear={2018},\nurl={https://openreview.net/forum?id=Hk2MHt-3-},\n}", "keywords": ["neural networks", "ensemble learning"], "authors": ["Anuvabh Dutt", "Denis Pellerin", "Georges Qu\u00e9not"], "authorids": ["anuvabhdutt@gmail.com", "denis.pellerin@gipsa-lab.grenoble-inp.fr", "georges.quenot@imag.fr"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582991385, "id": "ICLR.cc/2018/Workshop/-/Paper198/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper198/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper198/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper198/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper198/AnonReviewer2"], "reply": {"forum": "rylRCUJDG", "replyto": "rylRCUJDG", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper198/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper198/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582991385}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521573573939, "tcdate": 1521573573939, "number": 136, "cdate": 1521573573596, "id": "B1R6CC0YG", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "rylRCUJDG", "replyto": "rylRCUJDG", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Accept", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "Congratulations, your paper was accepted to the ICLR workshop."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Coupled Ensembles of Neural Networks", "abstract": "We present coupled ensembles of neural networks, which is a reconfiguration of existing neural network models into parallel branches. We empirically show that this modification leads to results on CIFAR and SVHN that are competitive to state of the art, with a greatly reduced parameter count. Additionally, for a fixed parameter, or a training time budget coupled ensembles are significantly better than single branch models. Preliminary results on ImageNet are also promising.", "pdf": "/pdf/6ee5f35f683eea109ab843fb98fd795f792fdd7b.pdf", "TL;DR": "We show that splitting a neural network into parallel branches improves performance for a parameter budget.", "paperhash": "dutt|coupled_ensembles_of_neural_networks", "_bibtex": "@misc{\ndutt2018coupled,\ntitle={Coupled Ensembles of Neural Networks},\nauthor={Anuvabh Dutt and Denis Pellerin and Georges Qu\u00e9not},\nyear={2018},\nurl={https://openreview.net/forum?id=Hk2MHt-3-},\n}", "keywords": ["neural networks", "ensemble learning"], "authors": ["Anuvabh Dutt", "Denis Pellerin", "Georges Qu\u00e9not"], "authorids": ["anuvabhdutt@gmail.com", "denis.pellerin@gipsa-lab.grenoble-inp.fr", "georges.quenot@imag.fr"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}], "count": 5}