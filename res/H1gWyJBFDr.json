{"notes": [{"id": "H1gWyJBFDr", "original": "BJxL2squPB", "number": 1458, "cdate": 1569439449372, "ddate": null, "tcdate": 1569439449372, "tmdate": 1577168269836, "tddate": null, "forum": "H1gWyJBFDr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["nassar.marcel@gmail.com", "caseus.viridis@gmail.com", "nervetumer@gmail.com"], "title": "Fully Convolutional Graph Neural Networks using Bipartite Graph Convolutions", "authors": ["Marcel Nassar", "Xin Wang", "Evren Tumer"], "pdf": "/pdf/f3cb71c3f0490504344df3989a674f66cec639cd.pdf", "abstract": "Graph neural networks have been adopted in numerous applications ranging from learning relational representations to modeling data on irregular domains such as point clouds, social graphs, and molecular structures. Though diverse in nature, graph neural network architectures remain limited by the graph convolution operator whose input and output graphs must have the same structure. With this restriction, representational hierarchy can only be built by graph convolution operations followed by non-parameterized pooling or expansion layers. This is very much like early convolutional network architectures, which later have been replaced by more effective parameterized strided and transpose convolution operations in combination with skip connections. In order to bring a similar change to graph convolutional networks, here we introduce the bipartite graph convolution operation, a parameterized transformation between different input and output graphs. Our framework is general enough to subsume conventional graph convolution and pooling as its special cases and supports multi-graph aggregation leading to a class of flexible and adaptable network architectures, termed BiGraphNet. By replacing the sequence of graph convolution and pooling in hierarchical architectures with a single parametric bipartite graph convolution, (i) we answer the question of whether graph pooling matters, and (ii) accelerate computations and lower memory requirements in hierarchical networks by eliminating pooling layers. Then, with concrete examples, we demonstrate that the general BiGraphNet formalism (iii) provides the modeling flexibility to build efficient architectures such as graph skip connections, and autoencoders.", "keywords": ["Graph Neural Networks", "Graph Convolutional Networks"], "paperhash": "nassar|fully_convolutional_graph_neural_networks_using_bipartite_graph_convolutions", "original_pdf": "/attachment/f3cb71c3f0490504344df3989a674f66cec639cd.pdf", "_bibtex": "@misc{\nnassar2020fully,\ntitle={Fully Convolutional Graph Neural Networks using Bipartite Graph Convolutions},\nauthor={Marcel Nassar and Xin Wang and Evren Tumer},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gWyJBFDr}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "NE3C5d_yK", "original": null, "number": 1, "cdate": 1576798723822, "ddate": null, "tcdate": 1576798723822, "tmdate": 1576800912703, "tddate": null, "forum": "H1gWyJBFDr", "replyto": "H1gWyJBFDr", "invitation": "ICLR.cc/2020/Conference/Paper1458/-/Decision", "content": {"decision": "Reject", "comment": "All three reviewers are consistently negative on this paper. Thus a reject is recommended.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["nassar.marcel@gmail.com", "caseus.viridis@gmail.com", "nervetumer@gmail.com"], "title": "Fully Convolutional Graph Neural Networks using Bipartite Graph Convolutions", "authors": ["Marcel Nassar", "Xin Wang", "Evren Tumer"], "pdf": "/pdf/f3cb71c3f0490504344df3989a674f66cec639cd.pdf", "abstract": "Graph neural networks have been adopted in numerous applications ranging from learning relational representations to modeling data on irregular domains such as point clouds, social graphs, and molecular structures. Though diverse in nature, graph neural network architectures remain limited by the graph convolution operator whose input and output graphs must have the same structure. With this restriction, representational hierarchy can only be built by graph convolution operations followed by non-parameterized pooling or expansion layers. This is very much like early convolutional network architectures, which later have been replaced by more effective parameterized strided and transpose convolution operations in combination with skip connections. In order to bring a similar change to graph convolutional networks, here we introduce the bipartite graph convolution operation, a parameterized transformation between different input and output graphs. Our framework is general enough to subsume conventional graph convolution and pooling as its special cases and supports multi-graph aggregation leading to a class of flexible and adaptable network architectures, termed BiGraphNet. By replacing the sequence of graph convolution and pooling in hierarchical architectures with a single parametric bipartite graph convolution, (i) we answer the question of whether graph pooling matters, and (ii) accelerate computations and lower memory requirements in hierarchical networks by eliminating pooling layers. Then, with concrete examples, we demonstrate that the general BiGraphNet formalism (iii) provides the modeling flexibility to build efficient architectures such as graph skip connections, and autoencoders.", "keywords": ["Graph Neural Networks", "Graph Convolutional Networks"], "paperhash": "nassar|fully_convolutional_graph_neural_networks_using_bipartite_graph_convolutions", "original_pdf": "/attachment/f3cb71c3f0490504344df3989a674f66cec639cd.pdf", "_bibtex": "@misc{\nnassar2020fully,\ntitle={Fully Convolutional Graph Neural Networks using Bipartite Graph Convolutions},\nauthor={Marcel Nassar and Xin Wang and Evren Tumer},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gWyJBFDr}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "H1gWyJBFDr", "replyto": "H1gWyJBFDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795726828, "tmdate": 1576800279023, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1458/-/Decision"}}}, {"id": "HklAVQc3iH", "original": null, "number": 4, "cdate": 1573851958338, "ddate": null, "tcdate": 1573851958338, "tmdate": 1573851958338, "tddate": null, "forum": "H1gWyJBFDr", "replyto": "H1exCVyg9H", "invitation": "ICLR.cc/2020/Conference/Paper1458/-/Official_Comment", "content": {"title": "Thank you Reviewer #1", "comment": "We thank the reviewer for their suggestions and comments. \n\n1. The reviewer is correct that bigraphnet layer still requires a separate clustering (or expansion) block as indicated by fig 1. This block can be precomputed (non-learnable, non-parameterized) such as voxel grid for point cloud data or data-driven (learnable, and parametric) such diffpool and gpool. In fact, bigraphnet supports any arbitrary input/output graph structures, a more general case than clustering input graph where each output vertex is assigned one of the mutually disjoint clusters of input vertices, like in DiffPool. Fig 1 used the dashed-line to denote the clusters of nodes (while using dashed line to denote non-parametric) suggesting that the clustering is non-parametric; this was not intentional and will be corrected (there is no restriction on the learnability of the clustering). \nThe main advantage of the bigraphnet part is the parametrization of the reduction part of the graph convolution operation as opposed to the node selection done in learnable pooling like diffpool and gpool. The bigraphnet architecture is complementary to the different pooling techniques mentioned above and can be made differentiable and dynamic using those techniques. Another way is that it can be used to speed up some of those techniques.\n \n2.  Graph NNs have been used on image data (for example in ECC): in this formulation each pixel is a node with its rgb value as its feature. From this view, a strided convolution (a parametric operation) computes new representation on a downsampled image which is a subset of the original image graph. We used the concept only as a high-level motivation, and will clarify this in the updated manuscript. \n \n3. While we agree with the reviewer about other potential interesting experiments to run, there are several reasons we believe our current set of experiments are convincing in demonstrating the promise of our fully convolutional approach. Our intention in this paper is not to achieve state-of-the-art performance, but rather to (1) propose a new graph formalism that allows tremendous flexibility in expression, and (2) demonstrate that by replacing the pooling mechanisms in an existing GNN with our fully convolutional approach, while keeping parameter count and other operations constant, can improve performance while significantly reducing memory consumption by 2x and inference times by ~25%. This experiment best isolates the contribution of our proposal, rather than chasing SOTA. In addition, by comparing to ECC which have an extensive array of graph application, we feel that we demonstrated the wider application domains of this formalism."}, "signatures": ["ICLR.cc/2020/Conference/Paper1458/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1458/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["nassar.marcel@gmail.com", "caseus.viridis@gmail.com", "nervetumer@gmail.com"], "title": "Fully Convolutional Graph Neural Networks using Bipartite Graph Convolutions", "authors": ["Marcel Nassar", "Xin Wang", "Evren Tumer"], "pdf": "/pdf/f3cb71c3f0490504344df3989a674f66cec639cd.pdf", "abstract": "Graph neural networks have been adopted in numerous applications ranging from learning relational representations to modeling data on irregular domains such as point clouds, social graphs, and molecular structures. Though diverse in nature, graph neural network architectures remain limited by the graph convolution operator whose input and output graphs must have the same structure. With this restriction, representational hierarchy can only be built by graph convolution operations followed by non-parameterized pooling or expansion layers. This is very much like early convolutional network architectures, which later have been replaced by more effective parameterized strided and transpose convolution operations in combination with skip connections. In order to bring a similar change to graph convolutional networks, here we introduce the bipartite graph convolution operation, a parameterized transformation between different input and output graphs. Our framework is general enough to subsume conventional graph convolution and pooling as its special cases and supports multi-graph aggregation leading to a class of flexible and adaptable network architectures, termed BiGraphNet. By replacing the sequence of graph convolution and pooling in hierarchical architectures with a single parametric bipartite graph convolution, (i) we answer the question of whether graph pooling matters, and (ii) accelerate computations and lower memory requirements in hierarchical networks by eliminating pooling layers. Then, with concrete examples, we demonstrate that the general BiGraphNet formalism (iii) provides the modeling flexibility to build efficient architectures such as graph skip connections, and autoencoders.", "keywords": ["Graph Neural Networks", "Graph Convolutional Networks"], "paperhash": "nassar|fully_convolutional_graph_neural_networks_using_bipartite_graph_convolutions", "original_pdf": "/attachment/f3cb71c3f0490504344df3989a674f66cec639cd.pdf", "_bibtex": "@misc{\nnassar2020fully,\ntitle={Fully Convolutional Graph Neural Networks using Bipartite Graph Convolutions},\nauthor={Marcel Nassar and Xin Wang and Evren Tumer},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gWyJBFDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1gWyJBFDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1458/Authors", "ICLR.cc/2020/Conference/Paper1458/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1458/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1458/Reviewers", "ICLR.cc/2020/Conference/Paper1458/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1458/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1458/Authors|ICLR.cc/2020/Conference/Paper1458/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504155715, "tmdate": 1576860539707, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1458/Authors", "ICLR.cc/2020/Conference/Paper1458/Reviewers", "ICLR.cc/2020/Conference/Paper1458/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1458/-/Official_Comment"}}}, {"id": "S1gZufcnjr", "original": null, "number": 3, "cdate": 1573851753327, "ddate": null, "tcdate": 1573851753327, "tmdate": 1573851753327, "tddate": null, "forum": "H1gWyJBFDr", "replyto": "SkeIGYybcH", "invitation": "ICLR.cc/2020/Conference/Paper1458/-/Official_Comment", "content": {"title": "Thank you Reviewer #3", "comment": "We appreciate the reviewer's comments:\n\n1.  We appreciate the criticism.  We indeed agree the analogy to strided CNNs is simply motivational used to only draw a parallel to this highly effective type of convolution used in modern CNNs (We believe the bipartite graph convolution will take a similar role for large GNNs).  However, none of the formulation depends on any type of analogy with strided convolutions and we demonstrated with a comprehensive set of experiments that our proposed BiGraphNet operation sufficed to eliminate the graph pooling operations altogether, just like explicit pooling layers are no longer used in recent CNN architectures.  We will update the manuscript to emphasize on the concrete results over the high-level motivation. \n\n2.  We have responded to similar comments from other reviewers, please see our other written responses. Our goal is not to set SOTA, but to use an existing GNN model with explicit graph pooling, and replace with BiGraphNet to show comparable performance at substantial memory (2x) savings and 25% faster compute. For this goal, the ECC model is an appropriate set of experiments across both graph and the variety datasets the focus on large graphs that need pooling to fit onto GPUs.\n\n3.  We apologize for the oversight of this relevant paper, and will include it in Related Work of the revision."}, "signatures": ["ICLR.cc/2020/Conference/Paper1458/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1458/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["nassar.marcel@gmail.com", "caseus.viridis@gmail.com", "nervetumer@gmail.com"], "title": "Fully Convolutional Graph Neural Networks using Bipartite Graph Convolutions", "authors": ["Marcel Nassar", "Xin Wang", "Evren Tumer"], "pdf": "/pdf/f3cb71c3f0490504344df3989a674f66cec639cd.pdf", "abstract": "Graph neural networks have been adopted in numerous applications ranging from learning relational representations to modeling data on irregular domains such as point clouds, social graphs, and molecular structures. Though diverse in nature, graph neural network architectures remain limited by the graph convolution operator whose input and output graphs must have the same structure. With this restriction, representational hierarchy can only be built by graph convolution operations followed by non-parameterized pooling or expansion layers. This is very much like early convolutional network architectures, which later have been replaced by more effective parameterized strided and transpose convolution operations in combination with skip connections. In order to bring a similar change to graph convolutional networks, here we introduce the bipartite graph convolution operation, a parameterized transformation between different input and output graphs. Our framework is general enough to subsume conventional graph convolution and pooling as its special cases and supports multi-graph aggregation leading to a class of flexible and adaptable network architectures, termed BiGraphNet. By replacing the sequence of graph convolution and pooling in hierarchical architectures with a single parametric bipartite graph convolution, (i) we answer the question of whether graph pooling matters, and (ii) accelerate computations and lower memory requirements in hierarchical networks by eliminating pooling layers. Then, with concrete examples, we demonstrate that the general BiGraphNet formalism (iii) provides the modeling flexibility to build efficient architectures such as graph skip connections, and autoencoders.", "keywords": ["Graph Neural Networks", "Graph Convolutional Networks"], "paperhash": "nassar|fully_convolutional_graph_neural_networks_using_bipartite_graph_convolutions", "original_pdf": "/attachment/f3cb71c3f0490504344df3989a674f66cec639cd.pdf", "_bibtex": "@misc{\nnassar2020fully,\ntitle={Fully Convolutional Graph Neural Networks using Bipartite Graph Convolutions},\nauthor={Marcel Nassar and Xin Wang and Evren Tumer},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gWyJBFDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1gWyJBFDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1458/Authors", "ICLR.cc/2020/Conference/Paper1458/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1458/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1458/Reviewers", "ICLR.cc/2020/Conference/Paper1458/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1458/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1458/Authors|ICLR.cc/2020/Conference/Paper1458/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504155715, "tmdate": 1576860539707, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1458/Authors", "ICLR.cc/2020/Conference/Paper1458/Reviewers", "ICLR.cc/2020/Conference/Paper1458/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1458/-/Official_Comment"}}}, {"id": "Byl2wZchjS", "original": null, "number": 2, "cdate": 1573851492284, "ddate": null, "tcdate": 1573851492284, "tmdate": 1573851492284, "tddate": null, "forum": "H1gWyJBFDr", "replyto": "SJlhsq6d9S", "invitation": "ICLR.cc/2020/Conference/Paper1458/-/Official_Comment", "content": {"title": "Thank you Reviewer #2", "comment": "Thanks you for your comments, and your appreciation of the novelty of our proposed graph formalism. We address your comments below: \n1.  We chose the ECC model because it reports results on extensive set of applications (3D vision, molecular graphs, images ...) instead of multiple datasets of the same domain such as the typically used citation networks and because it extensively uses architectures that use pooling which are necessary for 3D vision applications and some of the molecular datasets selected in our experiments (due to their large size). Our goal here is not to beat SOTA, but rather perform experiments that isolate our proposed layer; e.g. to show, for existing GNN architectures with explicit graph pooling, a drop-in replacement with BiGraphNet pooling is more efficient in computation (2x less memory usage and 25% faster) without performance degradation. \n\n2.  Due to the abovementioned scope of the current study, we did not intend to do GNN architectural search to advance state-of-the-art of performance.  This is why optimization of architectural hyperparameters such as network depth are not a relevant ablation study here.  Rather, we focus on taking a published GNN model with explicit pooling and replace the graph pooling with BiGraphNet modules while holding all other architectural hyperparameters exactly the same, in order to have a fair comparison. We believe our contributions in the novel graph formalism, and demonstrated gains in an isolated comparison, our sufficient to warrant inclusion rather than achieving SOTA.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1458/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1458/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["nassar.marcel@gmail.com", "caseus.viridis@gmail.com", "nervetumer@gmail.com"], "title": "Fully Convolutional Graph Neural Networks using Bipartite Graph Convolutions", "authors": ["Marcel Nassar", "Xin Wang", "Evren Tumer"], "pdf": "/pdf/f3cb71c3f0490504344df3989a674f66cec639cd.pdf", "abstract": "Graph neural networks have been adopted in numerous applications ranging from learning relational representations to modeling data on irregular domains such as point clouds, social graphs, and molecular structures. Though diverse in nature, graph neural network architectures remain limited by the graph convolution operator whose input and output graphs must have the same structure. With this restriction, representational hierarchy can only be built by graph convolution operations followed by non-parameterized pooling or expansion layers. This is very much like early convolutional network architectures, which later have been replaced by more effective parameterized strided and transpose convolution operations in combination with skip connections. In order to bring a similar change to graph convolutional networks, here we introduce the bipartite graph convolution operation, a parameterized transformation between different input and output graphs. Our framework is general enough to subsume conventional graph convolution and pooling as its special cases and supports multi-graph aggregation leading to a class of flexible and adaptable network architectures, termed BiGraphNet. By replacing the sequence of graph convolution and pooling in hierarchical architectures with a single parametric bipartite graph convolution, (i) we answer the question of whether graph pooling matters, and (ii) accelerate computations and lower memory requirements in hierarchical networks by eliminating pooling layers. Then, with concrete examples, we demonstrate that the general BiGraphNet formalism (iii) provides the modeling flexibility to build efficient architectures such as graph skip connections, and autoencoders.", "keywords": ["Graph Neural Networks", "Graph Convolutional Networks"], "paperhash": "nassar|fully_convolutional_graph_neural_networks_using_bipartite_graph_convolutions", "original_pdf": "/attachment/f3cb71c3f0490504344df3989a674f66cec639cd.pdf", "_bibtex": "@misc{\nnassar2020fully,\ntitle={Fully Convolutional Graph Neural Networks using Bipartite Graph Convolutions},\nauthor={Marcel Nassar and Xin Wang and Evren Tumer},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gWyJBFDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1gWyJBFDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1458/Authors", "ICLR.cc/2020/Conference/Paper1458/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1458/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1458/Reviewers", "ICLR.cc/2020/Conference/Paper1458/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1458/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1458/Authors|ICLR.cc/2020/Conference/Paper1458/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504155715, "tmdate": 1576860539707, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1458/Authors", "ICLR.cc/2020/Conference/Paper1458/Reviewers", "ICLR.cc/2020/Conference/Paper1458/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1458/-/Official_Comment"}}}, {"id": "H1exCVyg9H", "original": null, "number": 1, "cdate": 1571972295558, "ddate": null, "tcdate": 1571972295558, "tmdate": 1572972466303, "tddate": null, "forum": "H1gWyJBFDr", "replyto": "H1gWyJBFDr", "invitation": "ICLR.cc/2020/Conference/Paper1458/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper proposes BiGraphNet, which proposes to replace the graph convolution and pooling with a single bipartite graph convolution. Its motivation comes from using stride(>1) convolution to replace pooling in CNN. The authors claim that the computation and memory can be reduced with the proposed bipartite graph convolution, because the pooling layers are removed. The authors also conduct experiments about graph skip connection and graph encoder-decoder to show that their method's flexibility.\n\nCons:\n1. If I understand it correctly, the bipartite graph convolution still needs a cluster algorithm to determine the output graph, which is identical to cluster-based pooling methods like DiffPool. In addition, previous pooling methods like DiffPool, gPool are NOT non-parametric as suggested by Figure 1. Therefore, the advantage of the proposed method is vague.\n2. The idea of bipartite graph convolution seems different from that of stride convolution. The connection should be better explained.\n3. The experiments of this paper are not very convincing. Comparison with more baselines and ablation study are needed to demonstrate the effectiveness of this method. On graph classification tasks, many other methods (GCN with pooling) are worth comparing with, like DiffPool, SAGPool, gPool, etc. More datasets should be included. In addition, it will be more convincing to do ablation study, e.g. single layer replacement."}, "signatures": ["ICLR.cc/2020/Conference/Paper1458/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1458/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["nassar.marcel@gmail.com", "caseus.viridis@gmail.com", "nervetumer@gmail.com"], "title": "Fully Convolutional Graph Neural Networks using Bipartite Graph Convolutions", "authors": ["Marcel Nassar", "Xin Wang", "Evren Tumer"], "pdf": "/pdf/f3cb71c3f0490504344df3989a674f66cec639cd.pdf", "abstract": "Graph neural networks have been adopted in numerous applications ranging from learning relational representations to modeling data on irregular domains such as point clouds, social graphs, and molecular structures. Though diverse in nature, graph neural network architectures remain limited by the graph convolution operator whose input and output graphs must have the same structure. With this restriction, representational hierarchy can only be built by graph convolution operations followed by non-parameterized pooling or expansion layers. This is very much like early convolutional network architectures, which later have been replaced by more effective parameterized strided and transpose convolution operations in combination with skip connections. In order to bring a similar change to graph convolutional networks, here we introduce the bipartite graph convolution operation, a parameterized transformation between different input and output graphs. Our framework is general enough to subsume conventional graph convolution and pooling as its special cases and supports multi-graph aggregation leading to a class of flexible and adaptable network architectures, termed BiGraphNet. By replacing the sequence of graph convolution and pooling in hierarchical architectures with a single parametric bipartite graph convolution, (i) we answer the question of whether graph pooling matters, and (ii) accelerate computations and lower memory requirements in hierarchical networks by eliminating pooling layers. Then, with concrete examples, we demonstrate that the general BiGraphNet formalism (iii) provides the modeling flexibility to build efficient architectures such as graph skip connections, and autoencoders.", "keywords": ["Graph Neural Networks", "Graph Convolutional Networks"], "paperhash": "nassar|fully_convolutional_graph_neural_networks_using_bipartite_graph_convolutions", "original_pdf": "/attachment/f3cb71c3f0490504344df3989a674f66cec639cd.pdf", "_bibtex": "@misc{\nnassar2020fully,\ntitle={Fully Convolutional Graph Neural Networks using Bipartite Graph Convolutions},\nauthor={Marcel Nassar and Xin Wang and Evren Tumer},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gWyJBFDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "H1gWyJBFDr", "replyto": "H1gWyJBFDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1458/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1458/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575088925026, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1458/Reviewers"], "noninvitees": [], "tcdate": 1570237737099, "tmdate": 1575088925041, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1458/-/Official_Review"}}}, {"id": "SkeIGYybcH", "original": null, "number": 2, "cdate": 1572038926203, "ddate": null, "tcdate": 1572038926203, "tmdate": 1572972466262, "tddate": null, "forum": "H1gWyJBFDr", "replyto": "H1gWyJBFDr", "invitation": "ICLR.cc/2020/Conference/Paper1458/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a new graph neural network model named BiGraphNet, which introduces a parameterized bipartite graph convolution operation to perform transformation between input and output graphs. The proposed method is claimed to have advantages over existing deep hierarchical GCN architectures mainly in terms of being able to construct analogous building blocks employed by modern lattice CNN architectures and the reduced computational and memory cost. The main weaknesses of this paper are listed as follows:\n\n1) The motivation is relatively weak, which is to bring in the analogous building blocks in CNN architectures. Although GNN is closely related to CNN and RNN, the graph learning tasks may not have the same property as in computer vision or natural language processing. It would be better to convince the readers from the GNN itself and carefully argue the necessity of the proposed method.\n\n2) The experiments in this paper are rather weak and not convincing. First there is no performance comparison to state-of-the-art GNN models, such as DGCNN, DIFFPOOL and GIN, etc. At least on the D&D dataset, many existing models report graph classification accuracy over 78.0, but the baseline method used in this paper only achieves 72.5. Thus it is not fair to claim the proposed method can retain or improve the performance of existing GNN models.\n\n3) The related work comparison is not sufficient. For example, some existing works have already explored to apply skip connections to the graph neural networks, such as [1], which is not mentioned and compared in this paper.\n\nBased on the above arguments, I would like to recommend a reject for this paper.\n\n\n[1] Xu, Keyulu, et al. \"Representation learning on graphs with jumping knowledge networks.\" arXiv preprint arXiv:1806.03536 (2018)."}, "signatures": ["ICLR.cc/2020/Conference/Paper1458/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1458/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["nassar.marcel@gmail.com", "caseus.viridis@gmail.com", "nervetumer@gmail.com"], "title": "Fully Convolutional Graph Neural Networks using Bipartite Graph Convolutions", "authors": ["Marcel Nassar", "Xin Wang", "Evren Tumer"], "pdf": "/pdf/f3cb71c3f0490504344df3989a674f66cec639cd.pdf", "abstract": "Graph neural networks have been adopted in numerous applications ranging from learning relational representations to modeling data on irregular domains such as point clouds, social graphs, and molecular structures. Though diverse in nature, graph neural network architectures remain limited by the graph convolution operator whose input and output graphs must have the same structure. With this restriction, representational hierarchy can only be built by graph convolution operations followed by non-parameterized pooling or expansion layers. This is very much like early convolutional network architectures, which later have been replaced by more effective parameterized strided and transpose convolution operations in combination with skip connections. In order to bring a similar change to graph convolutional networks, here we introduce the bipartite graph convolution operation, a parameterized transformation between different input and output graphs. Our framework is general enough to subsume conventional graph convolution and pooling as its special cases and supports multi-graph aggregation leading to a class of flexible and adaptable network architectures, termed BiGraphNet. By replacing the sequence of graph convolution and pooling in hierarchical architectures with a single parametric bipartite graph convolution, (i) we answer the question of whether graph pooling matters, and (ii) accelerate computations and lower memory requirements in hierarchical networks by eliminating pooling layers. Then, with concrete examples, we demonstrate that the general BiGraphNet formalism (iii) provides the modeling flexibility to build efficient architectures such as graph skip connections, and autoencoders.", "keywords": ["Graph Neural Networks", "Graph Convolutional Networks"], "paperhash": "nassar|fully_convolutional_graph_neural_networks_using_bipartite_graph_convolutions", "original_pdf": "/attachment/f3cb71c3f0490504344df3989a674f66cec639cd.pdf", "_bibtex": "@misc{\nnassar2020fully,\ntitle={Fully Convolutional Graph Neural Networks using Bipartite Graph Convolutions},\nauthor={Marcel Nassar and Xin Wang and Evren Tumer},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gWyJBFDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "H1gWyJBFDr", "replyto": "H1gWyJBFDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1458/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1458/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575088925026, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1458/Reviewers"], "noninvitees": [], "tcdate": 1570237737099, "tmdate": 1575088925041, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1458/-/Official_Review"}}}, {"id": "SJlhsq6d9S", "original": null, "number": 3, "cdate": 1572555428252, "ddate": null, "tcdate": 1572555428252, "tmdate": 1572972466220, "tddate": null, "forum": "H1gWyJBFDr", "replyto": "H1gWyJBFDr", "invitation": "ICLR.cc/2020/Conference/Paper1458/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper introduced a novel parametrized graph operation called bipartite graph convolution (BGC). The proposed bipartite graph convolution layer functions as a regular graph convolution followed by a graph pooling layer, but it uses less memory. Also, the BGC layer can be used to aggregate multiple different graphs with various number of nodes. This paper further discussed the possibility of extending it to construct bipartite graph U-net structure with skip connections. Experimental evaluations have been focused on (1) comparing BGC against regular graph convolution layer followed by graph pooling layer in terms of classification accuracy and memory cost; and (2) comparing the regular graph-AE with the graph U-Net built on the proposed BGC layer with the unsupervised feature learning task.\n\nOverall, reviewer is very positive about the technical novelty of the paper. However, the experimental results seem not very strong. \n\n(1) The ECC model (Simonovsky and Komodakis, 2017) is no longer the state-of-the-art one on ModelNet. Please consider more recent papers such as the following one. Besides that, the performance delta seems very incremental.\n\n-- Dynamic Graph CNN for Learning on Point Clouds. Wang et al. In ACM Transactions on Graphics, 2019. \n\n(2) The current results are not very convincing as only one network structure is compared for each of the experiment. The ablation studies on graph structure (e.g., number of layers) are currently missing (Figure 4 and Table 1). \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1458/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1458/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["nassar.marcel@gmail.com", "caseus.viridis@gmail.com", "nervetumer@gmail.com"], "title": "Fully Convolutional Graph Neural Networks using Bipartite Graph Convolutions", "authors": ["Marcel Nassar", "Xin Wang", "Evren Tumer"], "pdf": "/pdf/f3cb71c3f0490504344df3989a674f66cec639cd.pdf", "abstract": "Graph neural networks have been adopted in numerous applications ranging from learning relational representations to modeling data on irregular domains such as point clouds, social graphs, and molecular structures. Though diverse in nature, graph neural network architectures remain limited by the graph convolution operator whose input and output graphs must have the same structure. With this restriction, representational hierarchy can only be built by graph convolution operations followed by non-parameterized pooling or expansion layers. This is very much like early convolutional network architectures, which later have been replaced by more effective parameterized strided and transpose convolution operations in combination with skip connections. In order to bring a similar change to graph convolutional networks, here we introduce the bipartite graph convolution operation, a parameterized transformation between different input and output graphs. Our framework is general enough to subsume conventional graph convolution and pooling as its special cases and supports multi-graph aggregation leading to a class of flexible and adaptable network architectures, termed BiGraphNet. By replacing the sequence of graph convolution and pooling in hierarchical architectures with a single parametric bipartite graph convolution, (i) we answer the question of whether graph pooling matters, and (ii) accelerate computations and lower memory requirements in hierarchical networks by eliminating pooling layers. Then, with concrete examples, we demonstrate that the general BiGraphNet formalism (iii) provides the modeling flexibility to build efficient architectures such as graph skip connections, and autoencoders.", "keywords": ["Graph Neural Networks", "Graph Convolutional Networks"], "paperhash": "nassar|fully_convolutional_graph_neural_networks_using_bipartite_graph_convolutions", "original_pdf": "/attachment/f3cb71c3f0490504344df3989a674f66cec639cd.pdf", "_bibtex": "@misc{\nnassar2020fully,\ntitle={Fully Convolutional Graph Neural Networks using Bipartite Graph Convolutions},\nauthor={Marcel Nassar and Xin Wang and Evren Tumer},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gWyJBFDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "H1gWyJBFDr", "replyto": "H1gWyJBFDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1458/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1458/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575088925026, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1458/Reviewers"], "noninvitees": [], "tcdate": 1570237737099, "tmdate": 1575088925041, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1458/-/Official_Review"}}}, {"id": "HkxFvEe6uH", "original": null, "number": 1, "cdate": 1570731104949, "ddate": null, "tcdate": 1570731104949, "tmdate": 1570731104949, "tddate": null, "forum": "H1gWyJBFDr", "replyto": "ryen8-Qx_B", "invitation": "ICLR.cc/2020/Conference/Paper1458/-/Official_Comment", "content": {"comment": "thank you for pointing this out. we will update the citation in the manuscript. ", "title": "updated citation"}, "signatures": ["ICLR.cc/2020/Conference/Paper1458/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1458/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["nassar.marcel@gmail.com", "caseus.viridis@gmail.com", "nervetumer@gmail.com"], "title": "Fully Convolutional Graph Neural Networks using Bipartite Graph Convolutions", "authors": ["Marcel Nassar", "Xin Wang", "Evren Tumer"], "pdf": "/pdf/f3cb71c3f0490504344df3989a674f66cec639cd.pdf", "abstract": "Graph neural networks have been adopted in numerous applications ranging from learning relational representations to modeling data on irregular domains such as point clouds, social graphs, and molecular structures. Though diverse in nature, graph neural network architectures remain limited by the graph convolution operator whose input and output graphs must have the same structure. With this restriction, representational hierarchy can only be built by graph convolution operations followed by non-parameterized pooling or expansion layers. This is very much like early convolutional network architectures, which later have been replaced by more effective parameterized strided and transpose convolution operations in combination with skip connections. In order to bring a similar change to graph convolutional networks, here we introduce the bipartite graph convolution operation, a parameterized transformation between different input and output graphs. Our framework is general enough to subsume conventional graph convolution and pooling as its special cases and supports multi-graph aggregation leading to a class of flexible and adaptable network architectures, termed BiGraphNet. By replacing the sequence of graph convolution and pooling in hierarchical architectures with a single parametric bipartite graph convolution, (i) we answer the question of whether graph pooling matters, and (ii) accelerate computations and lower memory requirements in hierarchical networks by eliminating pooling layers. Then, with concrete examples, we demonstrate that the general BiGraphNet formalism (iii) provides the modeling flexibility to build efficient architectures such as graph skip connections, and autoencoders.", "keywords": ["Graph Neural Networks", "Graph Convolutional Networks"], "paperhash": "nassar|fully_convolutional_graph_neural_networks_using_bipartite_graph_convolutions", "original_pdf": "/attachment/f3cb71c3f0490504344df3989a674f66cec639cd.pdf", "_bibtex": "@misc{\nnassar2020fully,\ntitle={Fully Convolutional Graph Neural Networks using Bipartite Graph Convolutions},\nauthor={Marcel Nassar and Xin Wang and Evren Tumer},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gWyJBFDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1gWyJBFDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1458/Authors", "ICLR.cc/2020/Conference/Paper1458/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1458/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1458/Reviewers", "ICLR.cc/2020/Conference/Paper1458/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1458/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1458/Authors|ICLR.cc/2020/Conference/Paper1458/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504155715, "tmdate": 1576860539707, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1458/Authors", "ICLR.cc/2020/Conference/Paper1458/Reviewers", "ICLR.cc/2020/Conference/Paper1458/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1458/-/Official_Comment"}}}, {"id": "ryen8-Qx_B", "original": null, "number": 1, "cdate": 1569890644159, "ddate": null, "tcdate": 1569890644159, "tmdate": 1569890644159, "tddate": null, "forum": "H1gWyJBFDr", "replyto": "H1gWyJBFDr", "invitation": "ICLR.cc/2020/Conference/Paper1458/-/Public_Comment", "content": {"comment": "Hi, thanks for your citation of our paper (https://arxiv.org/abs/1906.11994). We just released our second version of this paper, please revise the reference description. Thanks.", "title": "Update our citation"}, "signatures": ["~Chaoyang_He1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Chaoyang_He1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["nassar.marcel@gmail.com", "caseus.viridis@gmail.com", "nervetumer@gmail.com"], "title": "Fully Convolutional Graph Neural Networks using Bipartite Graph Convolutions", "authors": ["Marcel Nassar", "Xin Wang", "Evren Tumer"], "pdf": "/pdf/f3cb71c3f0490504344df3989a674f66cec639cd.pdf", "abstract": "Graph neural networks have been adopted in numerous applications ranging from learning relational representations to modeling data on irregular domains such as point clouds, social graphs, and molecular structures. Though diverse in nature, graph neural network architectures remain limited by the graph convolution operator whose input and output graphs must have the same structure. With this restriction, representational hierarchy can only be built by graph convolution operations followed by non-parameterized pooling or expansion layers. This is very much like early convolutional network architectures, which later have been replaced by more effective parameterized strided and transpose convolution operations in combination with skip connections. In order to bring a similar change to graph convolutional networks, here we introduce the bipartite graph convolution operation, a parameterized transformation between different input and output graphs. Our framework is general enough to subsume conventional graph convolution and pooling as its special cases and supports multi-graph aggregation leading to a class of flexible and adaptable network architectures, termed BiGraphNet. By replacing the sequence of graph convolution and pooling in hierarchical architectures with a single parametric bipartite graph convolution, (i) we answer the question of whether graph pooling matters, and (ii) accelerate computations and lower memory requirements in hierarchical networks by eliminating pooling layers. Then, with concrete examples, we demonstrate that the general BiGraphNet formalism (iii) provides the modeling flexibility to build efficient architectures such as graph skip connections, and autoencoders.", "keywords": ["Graph Neural Networks", "Graph Convolutional Networks"], "paperhash": "nassar|fully_convolutional_graph_neural_networks_using_bipartite_graph_convolutions", "original_pdf": "/attachment/f3cb71c3f0490504344df3989a674f66cec639cd.pdf", "_bibtex": "@misc{\nnassar2020fully,\ntitle={Fully Convolutional Graph Neural Networks using Bipartite Graph Convolutions},\nauthor={Marcel Nassar and Xin Wang and Evren Tumer},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gWyJBFDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1gWyJBFDr", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504194466, "tmdate": 1576860573302, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper1458/Authors", "ICLR.cc/2020/Conference/Paper1458/Reviewers", "ICLR.cc/2020/Conference/Paper1458/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1458/-/Public_Comment"}}}], "count": 10}