{"notes": [{"id": "BJeGA6VtPS", "original": "B1gnBPbdvr", "number": 847, "cdate": 1569439177893, "ddate": null, "tcdate": 1569439177893, "tmdate": 1577168268715, "tddate": null, "forum": "BJeGA6VtPS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "TrojanNet: Exposing the Danger of Trojan Horse Attack on Neural Networks", "authors": ["Chuan Guo", "Ruihan Wu", "Kilian Q. Weinberger"], "authorids": ["cg563@cornell.edu", "rw565@cornell.edu", "kqw4@cornell.edu"], "keywords": ["machine learning security"], "TL;DR": "Parameters of a trained neural network can be permuted to produce a completely separate model for a different task, enabling the embedding of Trojan horse networks inside another network.", "abstract": "The complexity of large-scale neural networks can lead to poor understanding of their internal  details. We show that this opaqueness provides an opportunity for adversaries to embed unintended functionalities into the network in the form of Trojan horse attacks. Our novel framework hides the existence of a malicious network within a benign transport network. Our attack is flexible, easy to execute, and difficult to detect. We prove theoretically that the malicious network's detection is computationally infeasible and demonstrate empirically that the transport network does not compromise its disguise. Our attack exposes an important, previously unknown loophole that unveils a new direction in machine learning security.", "pdf": "/pdf/76714d2105a39c26f394e480061def2c45f8f3c9.pdf", "paperhash": "guo|trojannet_exposing_the_danger_of_trojan_horse_attack_on_neural_networks", "original_pdf": "/attachment/76714d2105a39c26f394e480061def2c45f8f3c9.pdf", "_bibtex": "@misc{\nguo2020trojannet,\ntitle={TrojanNet: Exposing the Danger of Trojan Horse Attack on Neural Networks},\nauthor={Chuan Guo and Ruihan Wu and Kilian Q. Weinberger},\nyear={2020},\nurl={https://openreview.net/forum?id=BJeGA6VtPS}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "D88GreM8KY", "original": null, "number": 1, "cdate": 1576798707666, "ddate": null, "tcdate": 1576798707666, "tmdate": 1576800928669, "tddate": null, "forum": "BJeGA6VtPS", "replyto": "BJeGA6VtPS", "invitation": "ICLR.cc/2020/Conference/Paper847/-/Decision", "content": {"decision": "Reject", "comment": "This paper presents a very creative threat model for neural networks.  The proposed attack requires systems-level intervention by the attacker, which prompts the reviewers to question how realistic the attack is, and whether it is well motivated by the authors.  After conversing with the reviewers on this topic, they have not changed their mind about these issues.  As an AC, I think the threat model is both interesting and potentially realistic in some scenarios, however I agree with the reviewers that the motivation for the threat model could be more powerful.  For example the authors could focus more on realistic types of malicious behaviors that a developer could embed into a neural network.  I also think there's lots of opportunities for a range of applications that exploit the type of \"two nets in one\" behavior that the authors study.  Despite the interesting ideas in this paper, the post-rebuttal scores are not strong enough to accept it.  I encourage the authors to address some of these presentation issues, and resubmit this interesting paper to another venue.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "TrojanNet: Exposing the Danger of Trojan Horse Attack on Neural Networks", "authors": ["Chuan Guo", "Ruihan Wu", "Kilian Q. Weinberger"], "authorids": ["cg563@cornell.edu", "rw565@cornell.edu", "kqw4@cornell.edu"], "keywords": ["machine learning security"], "TL;DR": "Parameters of a trained neural network can be permuted to produce a completely separate model for a different task, enabling the embedding of Trojan horse networks inside another network.", "abstract": "The complexity of large-scale neural networks can lead to poor understanding of their internal  details. We show that this opaqueness provides an opportunity for adversaries to embed unintended functionalities into the network in the form of Trojan horse attacks. Our novel framework hides the existence of a malicious network within a benign transport network. Our attack is flexible, easy to execute, and difficult to detect. We prove theoretically that the malicious network's detection is computationally infeasible and demonstrate empirically that the transport network does not compromise its disguise. Our attack exposes an important, previously unknown loophole that unveils a new direction in machine learning security.", "pdf": "/pdf/76714d2105a39c26f394e480061def2c45f8f3c9.pdf", "paperhash": "guo|trojannet_exposing_the_danger_of_trojan_horse_attack_on_neural_networks", "original_pdf": "/attachment/76714d2105a39c26f394e480061def2c45f8f3c9.pdf", "_bibtex": "@misc{\nguo2020trojannet,\ntitle={TrojanNet: Exposing the Danger of Trojan Horse Attack on Neural Networks},\nauthor={Chuan Guo and Ruihan Wu and Kilian Q. Weinberger},\nyear={2020},\nurl={https://openreview.net/forum?id=BJeGA6VtPS}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "BJeGA6VtPS", "replyto": "BJeGA6VtPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795728349, "tmdate": 1576800280746, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper847/-/Decision"}}}, {"id": "HylSZxH2FS", "original": null, "number": 2, "cdate": 1571733500567, "ddate": null, "tcdate": 1571733500567, "tmdate": 1574307382259, "tddate": null, "forum": "BJeGA6VtPS", "replyto": "BJeGA6VtPS", "invitation": "ICLR.cc/2020/Conference/Paper847/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #2", "review": "This paper studies an attack scenario, where the adversary trains a classifier in a way so that the learned model performs well on a main task, while after a certain permutation of the parameters specified by the adversary, the permuted model is also able to perform another secret task. They evaluate on several image classification benchmarks, and show that their trained model achieves a comparable performance on both the main task and the secret task to the models trained on a single task.\n\nI think this paper reveals an interesting phenomenon, i.e., the same model architecture trained with different benchmarks may share similar parameters after a proper permutation; but I am not convinced by the threat model studied in this work. For the attack scenario studied in this paper, it should be ideal to enable the model to perform both the main and the secret tasks at the same time. However, the permutation process could be very time-consuming, especially when the number of model parameters goes large. The time overhead of the transition among different tasks would make the model more suspicious to the user. It would be great if the authors can motivate their threat model better.\n\nOn the other hand, considering the purpose of training a single model to perform the prediction tasks on several benchmarks, I would like to see how general their conclusion holds. For example, what happens if the main task is on a benchmark with a large label set? I would like to know if two models trained on different datasets with a large label set could also share the same set of model parameters under a certain permutation; or if the secret task has a much smaller label set than the main task, how well the performance could be?\n\n---------\nPost-rebuttal comments\n\nI thank the authors for the explanation of the threat model. However, I think my concerns are not addressed, and thus I keep my original assessment.\n---------", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper847/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper847/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "TrojanNet: Exposing the Danger of Trojan Horse Attack on Neural Networks", "authors": ["Chuan Guo", "Ruihan Wu", "Kilian Q. Weinberger"], "authorids": ["cg563@cornell.edu", "rw565@cornell.edu", "kqw4@cornell.edu"], "keywords": ["machine learning security"], "TL;DR": "Parameters of a trained neural network can be permuted to produce a completely separate model for a different task, enabling the embedding of Trojan horse networks inside another network.", "abstract": "The complexity of large-scale neural networks can lead to poor understanding of their internal  details. We show that this opaqueness provides an opportunity for adversaries to embed unintended functionalities into the network in the form of Trojan horse attacks. Our novel framework hides the existence of a malicious network within a benign transport network. Our attack is flexible, easy to execute, and difficult to detect. We prove theoretically that the malicious network's detection is computationally infeasible and demonstrate empirically that the transport network does not compromise its disguise. Our attack exposes an important, previously unknown loophole that unveils a new direction in machine learning security.", "pdf": "/pdf/76714d2105a39c26f394e480061def2c45f8f3c9.pdf", "paperhash": "guo|trojannet_exposing_the_danger_of_trojan_horse_attack_on_neural_networks", "original_pdf": "/attachment/76714d2105a39c26f394e480061def2c45f8f3c9.pdf", "_bibtex": "@misc{\nguo2020trojannet,\ntitle={TrojanNet: Exposing the Danger of Trojan Horse Attack on Neural Networks},\nauthor={Chuan Guo and Ruihan Wu and Kilian Q. Weinberger},\nyear={2020},\nurl={https://openreview.net/forum?id=BJeGA6VtPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BJeGA6VtPS", "replyto": "BJeGA6VtPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper847/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper847/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575405499817, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper847/Reviewers"], "noninvitees": [], "tcdate": 1570237746125, "tmdate": 1575405499832, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper847/-/Official_Review"}}}, {"id": "SJey8T3AFr", "original": null, "number": 3, "cdate": 1571896646821, "ddate": null, "tcdate": 1571896646821, "tmdate": 1574200297682, "tddate": null, "forum": "BJeGA6VtPS", "replyto": "BJeGA6VtPS", "invitation": "ICLR.cc/2020/Conference/Paper847/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #1", "review": "This paper proposed a novel an very interesting attacking scenario (the authors called it the Trojan horse attack) that aims to embed a secret model for solving a secret task into a public model for solving a different public task, through the use of weight permutations, where the permutations can be considered as a key in the crypto setting. The authors prove the computational complexity (NP-completeness) of detecting such as a secret model. Experimental results show that it is possible to secretly embed multiple and different secret models into one publish model via joint training with permutation, while the performance of each model is similar to the individually trained models.\n\nOverall, the trojan horse attacking scenario considered in this paper is novel and provides new insights to the research in adversarial machine learning. The finding that permutation along is able to embed multiple models is highly non-trivial. While I agree with the authors' explanations on the difference between trojan horse attack versus multi-task learning (shared data or not), my main concern is the lack of comparison and discussion to another secrecy-based attack scheme, the \"Adversarial Reprogramming of Neural Networks\" published in 2019. In their adversarial reprogramming attack, the model weights also remain unchanged (and un-permuted). To train the secret \"model\", Elsayed et al. used a trainable input perturbation to learn how to solve the secret task. Although Elsayed et al. did not consider the case of reprogramming for multiple secret tasks, I believe the proposed method and adversarial reprogramming share common goals and their attacks are both stealthy in the sense final model weights are unchanged. I would like to know the authors' thoughts on the proposed attack v.s. adversarial reprogramming to better motivate the importance of the considered attack scenario. In my perspective, they have the same threat model but adversarial reprogramming seems to be even stealthier as it does not use the secret data to jointly train the final model. Some discussion and numerical comparisons will be very useful for clarifying the advantage of the proposed method over adversarial reprogramming in terms of \"attacks\". I am happy to increase my rating if the authors address this main concern.\n\n*** Post-rebuttal comments\nI thank the authors for the clarification. I actually quite like the idea and believe the threat model is valid if addressed fin a clearer manner. I look forward to a future version of this work.\n***", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper847/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper847/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "TrojanNet: Exposing the Danger of Trojan Horse Attack on Neural Networks", "authors": ["Chuan Guo", "Ruihan Wu", "Kilian Q. Weinberger"], "authorids": ["cg563@cornell.edu", "rw565@cornell.edu", "kqw4@cornell.edu"], "keywords": ["machine learning security"], "TL;DR": "Parameters of a trained neural network can be permuted to produce a completely separate model for a different task, enabling the embedding of Trojan horse networks inside another network.", "abstract": "The complexity of large-scale neural networks can lead to poor understanding of their internal  details. We show that this opaqueness provides an opportunity for adversaries to embed unintended functionalities into the network in the form of Trojan horse attacks. Our novel framework hides the existence of a malicious network within a benign transport network. Our attack is flexible, easy to execute, and difficult to detect. We prove theoretically that the malicious network's detection is computationally infeasible and demonstrate empirically that the transport network does not compromise its disguise. Our attack exposes an important, previously unknown loophole that unveils a new direction in machine learning security.", "pdf": "/pdf/76714d2105a39c26f394e480061def2c45f8f3c9.pdf", "paperhash": "guo|trojannet_exposing_the_danger_of_trojan_horse_attack_on_neural_networks", "original_pdf": "/attachment/76714d2105a39c26f394e480061def2c45f8f3c9.pdf", "_bibtex": "@misc{\nguo2020trojannet,\ntitle={TrojanNet: Exposing the Danger of Trojan Horse Attack on Neural Networks},\nauthor={Chuan Guo and Ruihan Wu and Kilian Q. Weinberger},\nyear={2020},\nurl={https://openreview.net/forum?id=BJeGA6VtPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BJeGA6VtPS", "replyto": "BJeGA6VtPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper847/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper847/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575405499817, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper847/Reviewers"], "noninvitees": [], "tcdate": 1570237746125, "tmdate": 1575405499832, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper847/-/Official_Review"}}}, {"id": "BJxnmlMYsB", "original": null, "number": 1, "cdate": 1573621795792, "ddate": null, "tcdate": 1573621795792, "tmdate": 1573621795792, "tddate": null, "forum": "BJeGA6VtPS", "replyto": "BJeGA6VtPS", "invitation": "ICLR.cc/2020/Conference/Paper847/-/Official_Comment", "content": {"title": "Explanation of the threat model", "comment": "We thank all reviewers for very insightful comments and relevant references.\n\nWe would like to address the common criticism amongst all three reviewers regarding the threat model. Indeed, our Trojan horse attack differs from most existing attacks in the literature in that it requires the adversary\u2019s capability to permute the model weights at test time. While we agree that under certain scenarios, active manipulation of the model parameters is an unrealistic requirement for the attacker, there are also many scenarios (e.g. malicious employees) where it would be extremely easy to hide such Trojan code. In particular, the permutation could be implemented within a few lines of code, as it only requires a simple hash function (and no look-up table, or different model weights). Our paper is less focused on the Trojan threat model (which is well established in the security community) but instead is meant to expose the possibility of an entirely new threat in neural network models. The ability to covertly embed a network for an arbitrary task within a benign network is very powerful and is in general an undesirable outcome. Moreover, differing from prior attacks that embed Trojan marks in the input to alter the model\u2019s behavior, the capability of the unintended malicious behavior cannot be detected or thwarted even if one is suspicious. For example, cropping and rescaling the input image may completely invalidate the added perturbation for the adversarial reprogramming attack.\n\nIn our view, the Trojan attack framework we proposed constitutes a rare but deeply unsettling possibility that the community should be made aware of. Since the affected model behaves identically to a benign model in all aspects, there is no prevention for this attack when without knowledge of this possibility. The goal of this paper is to inform the research community of this hazard.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper847/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper847/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "TrojanNet: Exposing the Danger of Trojan Horse Attack on Neural Networks", "authors": ["Chuan Guo", "Ruihan Wu", "Kilian Q. Weinberger"], "authorids": ["cg563@cornell.edu", "rw565@cornell.edu", "kqw4@cornell.edu"], "keywords": ["machine learning security"], "TL;DR": "Parameters of a trained neural network can be permuted to produce a completely separate model for a different task, enabling the embedding of Trojan horse networks inside another network.", "abstract": "The complexity of large-scale neural networks can lead to poor understanding of their internal  details. We show that this opaqueness provides an opportunity for adversaries to embed unintended functionalities into the network in the form of Trojan horse attacks. Our novel framework hides the existence of a malicious network within a benign transport network. Our attack is flexible, easy to execute, and difficult to detect. We prove theoretically that the malicious network's detection is computationally infeasible and demonstrate empirically that the transport network does not compromise its disguise. Our attack exposes an important, previously unknown loophole that unveils a new direction in machine learning security.", "pdf": "/pdf/76714d2105a39c26f394e480061def2c45f8f3c9.pdf", "paperhash": "guo|trojannet_exposing_the_danger_of_trojan_horse_attack_on_neural_networks", "original_pdf": "/attachment/76714d2105a39c26f394e480061def2c45f8f3c9.pdf", "_bibtex": "@misc{\nguo2020trojannet,\ntitle={TrojanNet: Exposing the Danger of Trojan Horse Attack on Neural Networks},\nauthor={Chuan Guo and Ruihan Wu and Kilian Q. Weinberger},\nyear={2020},\nurl={https://openreview.net/forum?id=BJeGA6VtPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJeGA6VtPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper847/Authors", "ICLR.cc/2020/Conference/Paper847/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper847/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper847/Reviewers", "ICLR.cc/2020/Conference/Paper847/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper847/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper847/Authors|ICLR.cc/2020/Conference/Paper847/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504165294, "tmdate": 1576860539903, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper847/Authors", "ICLR.cc/2020/Conference/Paper847/Reviewers", "ICLR.cc/2020/Conference/Paper847/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper847/-/Official_Comment"}}}, {"id": "rkxjIK2oFH", "original": null, "number": 1, "cdate": 1571699027510, "ddate": null, "tcdate": 1571699027510, "tmdate": 1572972544691, "tddate": null, "forum": "BJeGA6VtPS", "replyto": "BJeGA6VtPS", "invitation": "ICLR.cc/2020/Conference/Paper847/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes TrojanNet, a new threat model and corresponding attack in ML security. The paper demonstrates that it is possible for an adversary to train a network that performs well on some benign \"base task,\" while also being able to perform a (potentially malicious) secret task when the weights are permuted in a specific manner. Leveraging tools from traditional security and cryptography, the paper demonstrates that combined with a small piece of software that can apply permutations to the weights, an attacker can then leverage the network to perform the secret task once it is deployed.\n\nThe paper presents the method well, and appropriately lays out the threat model, approach, and results in a concrete way. My main concern is with the validity of the threat model, as it seems to assume the ability to get arbitrary software (in particular, the program that applies the permutations) onto the victim's server, at which point permuting the weights of a deployed neural network is just one of endless malicious things an adversary can do. That said, the idea of training a network to be able to perform a secret task on command is very interesting, and the results do show compelling evidence that it is possible. For now, my recommendation is to (weakly) reject the paper, primarily due to the unconvincing threat model. There are also a few more minor comments below:\n- It would be good to ensure that the technique still works with different normalization techniques to ensure the network doesn't have to store two sets of normalization statistics.\n- Some spelling grammar mistakes littered throughout, particularly noticeable in the section titles (e.g. section 2 title \"Network\" -> \"Networks\", page 1 \"undermine trustworthiness\" -> \"undermine the trustworthiness\", etc.)\n\nIt would be interesting to explore whether the trojan nn attack can be executed in a scenario when the adversary does not have the ability to inject malicious code into the victim's server, just a standard model. E.g., perhaps scrambling could be done in image space directly, or the scrambling process could be \"embedded\" into the weights network somehow? (Note that these are just ideas and not requests for revisions.)"}, "signatures": ["ICLR.cc/2020/Conference/Paper847/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper847/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "TrojanNet: Exposing the Danger of Trojan Horse Attack on Neural Networks", "authors": ["Chuan Guo", "Ruihan Wu", "Kilian Q. Weinberger"], "authorids": ["cg563@cornell.edu", "rw565@cornell.edu", "kqw4@cornell.edu"], "keywords": ["machine learning security"], "TL;DR": "Parameters of a trained neural network can be permuted to produce a completely separate model for a different task, enabling the embedding of Trojan horse networks inside another network.", "abstract": "The complexity of large-scale neural networks can lead to poor understanding of their internal  details. We show that this opaqueness provides an opportunity for adversaries to embed unintended functionalities into the network in the form of Trojan horse attacks. Our novel framework hides the existence of a malicious network within a benign transport network. Our attack is flexible, easy to execute, and difficult to detect. We prove theoretically that the malicious network's detection is computationally infeasible and demonstrate empirically that the transport network does not compromise its disguise. Our attack exposes an important, previously unknown loophole that unveils a new direction in machine learning security.", "pdf": "/pdf/76714d2105a39c26f394e480061def2c45f8f3c9.pdf", "paperhash": "guo|trojannet_exposing_the_danger_of_trojan_horse_attack_on_neural_networks", "original_pdf": "/attachment/76714d2105a39c26f394e480061def2c45f8f3c9.pdf", "_bibtex": "@misc{\nguo2020trojannet,\ntitle={TrojanNet: Exposing the Danger of Trojan Horse Attack on Neural Networks},\nauthor={Chuan Guo and Ruihan Wu and Kilian Q. Weinberger},\nyear={2020},\nurl={https://openreview.net/forum?id=BJeGA6VtPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BJeGA6VtPS", "replyto": "BJeGA6VtPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper847/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper847/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575405499817, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper847/Reviewers"], "noninvitees": [], "tcdate": 1570237746125, "tmdate": 1575405499832, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper847/-/Official_Review"}}}], "count": 6}