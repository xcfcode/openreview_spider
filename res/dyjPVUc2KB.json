{"notes": [{"id": "dyjPVUc2KB", "original": "AkUq-d7osEG", "number": 1570, "cdate": 1601308174154, "ddate": null, "tcdate": 1601308174154, "tmdate": 1615532042774, "tddate": null, "forum": "dyjPVUc2KB", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Adapting to Reward Progressivity via Spectral Reinforcement Learning", "authorids": ["~Michael_Dann1", "john.thangarajah@rmit.edu.au"], "authors": ["Michael Dann", "John Thangarajah"], "keywords": ["Reinforcement Learning", "Deep Reinforcement Learning"], "abstract": "In this paper we consider reinforcement learning tasks with progressive rewards; that is, tasks where the rewards tend to increase in magnitude over time. We hypothesise that this property may be problematic for value-based deep reinforcement learning agents, particularly if the agent must first succeed in relatively unrewarding regions of the task in order to reach more rewarding regions. To address this issue, we propose Spectral DQN, which decomposes the reward into frequencies such that the high frequencies only activate when large rewards are found. This allows the training loss to be balanced so that it gives more even weighting across small and large reward regions. In two domains with extreme reward progressivity, where standard value-based methods struggle significantly, Spectral DQN is able to make much farther progress. Moreover, when evaluated on a set of six standard Atari games that do not overtly favour the approach, Spectral DQN remains more than competitive: While it underperforms one of the benchmarks in a single game, it comfortably surpasses the benchmarks in three games. These results demonstrate that the approach is not overfit to its target problem, and suggest that Spectral DQN may have advantages beyond addressing reward progressivity.", "one-sentence_summary": "In this paper, we identify a problem with value-based deep reinforcement learning that has not previously been investigated -- namely, reward progressivity -- and propose an approach that addresses it via magnitudinal decomposition of the reward.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "dann|adapting_to_reward_progressivity_via_spectral_reinforcement_learning", "supplementary_material": "", "pdf": "/pdf/a3879d188d633661f6593af48f3c561aefbfb6ff.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ndann2021adapting,\ntitle={Adapting to Reward Progressivity via Spectral Reinforcement Learning},\nauthor={Michael Dann and John Thangarajah},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=dyjPVUc2KB}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 16, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "3GPrTOZZPhe", "original": null, "number": 1, "cdate": 1615531731638, "ddate": null, "tcdate": 1615531731638, "tmdate": 1615531731638, "tddate": null, "forum": "dyjPVUc2KB", "replyto": "dyjPVUc2KB", "invitation": "ICLR.cc/2021/Conference/Paper1570/-/Comment", "content": {"title": "Source code", "comment": "Source code for our agent is available at:\nhttps://github.com/mchldann/SpectralDQN"}, "signatures": ["ICLR.cc/2021/Conference/Paper1570/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1570/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adapting to Reward Progressivity via Spectral Reinforcement Learning", "authorids": ["~Michael_Dann1", "john.thangarajah@rmit.edu.au"], "authors": ["Michael Dann", "John Thangarajah"], "keywords": ["Reinforcement Learning", "Deep Reinforcement Learning"], "abstract": "In this paper we consider reinforcement learning tasks with progressive rewards; that is, tasks where the rewards tend to increase in magnitude over time. We hypothesise that this property may be problematic for value-based deep reinforcement learning agents, particularly if the agent must first succeed in relatively unrewarding regions of the task in order to reach more rewarding regions. To address this issue, we propose Spectral DQN, which decomposes the reward into frequencies such that the high frequencies only activate when large rewards are found. This allows the training loss to be balanced so that it gives more even weighting across small and large reward regions. In two domains with extreme reward progressivity, where standard value-based methods struggle significantly, Spectral DQN is able to make much farther progress. Moreover, when evaluated on a set of six standard Atari games that do not overtly favour the approach, Spectral DQN remains more than competitive: While it underperforms one of the benchmarks in a single game, it comfortably surpasses the benchmarks in three games. These results demonstrate that the approach is not overfit to its target problem, and suggest that Spectral DQN may have advantages beyond addressing reward progressivity.", "one-sentence_summary": "In this paper, we identify a problem with value-based deep reinforcement learning that has not previously been investigated -- namely, reward progressivity -- and propose an approach that addresses it via magnitudinal decomposition of the reward.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "dann|adapting_to_reward_progressivity_via_spectral_reinforcement_learning", "supplementary_material": "", "pdf": "/pdf/a3879d188d633661f6593af48f3c561aefbfb6ff.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ndann2021adapting,\ntitle={Adapting to Reward Progressivity via Spectral Reinforcement Learning},\nauthor={Michael Dann and John Thangarajah},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=dyjPVUc2KB}\n}"}, "tags": [], "invitation": {"reply": {"forum": "dyjPVUc2KB", "readers": {"values": ["everyone"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2021/Conference/Paper1570/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1570/Authors|ICLR.cc/2021/Conference/Paper1570/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs"}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}}, "multiReply": true, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["everyone"], "tcdate": 1610649443857, "tmdate": 1610649443857, "id": "ICLR.cc/2021/Conference/Paper1570/-/Comment"}}}, {"id": "VJODuYF7H3P", "original": null, "number": 1, "cdate": 1610040364275, "ddate": null, "tcdate": 1610040364275, "tmdate": 1610473954668, "tddate": null, "forum": "dyjPVUc2KB", "replyto": "dyjPVUc2KB", "invitation": "ICLR.cc/2021/Conference/Paper1570/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Poster)", "comment": "This paper presents a deep RL algorithm to handle tasks where rewards can differ greatly in magnitude.  The proposed solution decomposes the reward into a set of exponentially sized bins with a thermometer encoding, and computes a weighted sum of the value functions learned for each bin.  The approach addresses the common tactic of reward clipping and value rescaling in deep RL algorithms.  The experiments demonstrate the potential utility of this approach on artificially constructed Atari games, and the experiments also show the approach remains competitive on six standard Atari games.  \n\nThe reviewers found both strengths and weaknesses in the paper.  The overall approach was viewed as a clear and sensible (R1, R2, R4) approach to handling widely varying reward scales in a domain.  It may be a useful contribution in a manner similar to other methods that make deep RL algorithms more robust to scaling issues encountered in practice (R4).  The main concerns were whether this was solving a real problem or not (R2, R3), and the lack of a theoretical development for the multiple heuristics (R2,R3).  \n\nThe author response then simplified the algorithm, which also served to clarify which aspects of the algorithm were relevant to the performance improvements. The response removed some of the heuristics (mixing in Monte Carlo returns) and changed other choices to be more principled ($1/\\sigma^2$). The author response also described how the proposed algorithm addressed different scaling concerns from those handled by earlier methods. The author response also provided clarifications to many minor questions raised by the reviewers.  In the ensuing discussion, the reviewers were happy with the revised paper.  Though some minor theoretical reservations remained, the reviewers agreed this paper was a useful contribution.\n\nThe reviewers indicate to accept the paper as a useful contribution in deep RL to address certain reward scaling issues.  The paper is therefore accepted. "}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adapting to Reward Progressivity via Spectral Reinforcement Learning", "authorids": ["~Michael_Dann1", "john.thangarajah@rmit.edu.au"], "authors": ["Michael Dann", "John Thangarajah"], "keywords": ["Reinforcement Learning", "Deep Reinforcement Learning"], "abstract": "In this paper we consider reinforcement learning tasks with progressive rewards; that is, tasks where the rewards tend to increase in magnitude over time. We hypothesise that this property may be problematic for value-based deep reinforcement learning agents, particularly if the agent must first succeed in relatively unrewarding regions of the task in order to reach more rewarding regions. To address this issue, we propose Spectral DQN, which decomposes the reward into frequencies such that the high frequencies only activate when large rewards are found. This allows the training loss to be balanced so that it gives more even weighting across small and large reward regions. In two domains with extreme reward progressivity, where standard value-based methods struggle significantly, Spectral DQN is able to make much farther progress. Moreover, when evaluated on a set of six standard Atari games that do not overtly favour the approach, Spectral DQN remains more than competitive: While it underperforms one of the benchmarks in a single game, it comfortably surpasses the benchmarks in three games. These results demonstrate that the approach is not overfit to its target problem, and suggest that Spectral DQN may have advantages beyond addressing reward progressivity.", "one-sentence_summary": "In this paper, we identify a problem with value-based deep reinforcement learning that has not previously been investigated -- namely, reward progressivity -- and propose an approach that addresses it via magnitudinal decomposition of the reward.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "dann|adapting_to_reward_progressivity_via_spectral_reinforcement_learning", "supplementary_material": "", "pdf": "/pdf/a3879d188d633661f6593af48f3c561aefbfb6ff.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ndann2021adapting,\ntitle={Adapting to Reward Progressivity via Spectral Reinforcement Learning},\nauthor={Michael Dann and John Thangarajah},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=dyjPVUc2KB}\n}"}, "tags": [], "invitation": {"reply": {"forum": "dyjPVUc2KB", "replyto": "dyjPVUc2KB", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040364261, "tmdate": 1610473954648, "id": "ICLR.cc/2021/Conference/Paper1570/-/Decision"}}}, {"id": "P_2rs-Ix7u", "original": null, "number": 3, "cdate": 1603415305742, "ddate": null, "tcdate": 1603415305742, "tmdate": 1606407079857, "tddate": null, "forum": "dyjPVUc2KB", "replyto": "dyjPVUc2KB", "invitation": "ICLR.cc/2021/Conference/Paper1570/-/Official_Review", "content": {"title": "An intriguing idea, well-developed, somewhat lacking a use case", "review": "This paper proposes an extension to DQN, more generally applicable to value-based deep RL systems, that encodes the return using a thermometer encoding with exponentially-sized bins. This enables returns of vastly differing magnitudes to be learned without hurting performance. The authors propose an algorithm for learning these encode returns, including the use of a variance scaling term to speed up learning.\n\nOverall, I enjoyed reading the paper and appreciated the clear exposition, which is sensible throughout. My main concern is whether this is solving a real problem, or a hypothetical one. The experiments don't support the former case, while I would argue that solving the problem hypothetically would require a more thorough development. For example, if progressivity is an issue, what form do we expect it to take? The two synthetic examples, Ponglantis and Exponential Pong, make for a fun case but not necessarily a realistic one. Training on all Atari games with a single network, for example, might be a better case. \n\nI have a few technical questions for the authors:\n\n4.1: Why use a thermometer encoding versus a binary encoding? I.e., write the reward in binary, and encode its bits. \n\n4.2: Why not use base gamma instead of base 2? That seems like a more natural way to encode returns.\n\n4.3: \"impossible to train over full, infinite spectrum\" -- given that returns are bounded for gamma < 1, what do you mean? It seems like you need log(VMax) bins at most.\n\n5: \"We do not simply set ...\" I would have expected some ablation studies here, regarding the role of the parameter. The choice w_i = 1/sigma_i seems particularly ad-hoc. In particular, I'm disappointed that you did not include the experiment w_i = 1. It seems natural to me that exponential weights should fail, whereas it's not clear that w_i = 1 should, and the use of sigma complicates things (and adds a moving part).\n\nThe results of Fig. 4 are not terribly exciting. Sometimes spectral DQN works better, sometimes it does not. Can you comment on this? Should we be concerned? Also, you should include published DQN results (and if possible, other value-based methods) as I would expect these to be worse for games that have rewards of different magnitudes. \n\n8: \"the phenomenon of reward progressivity\". Is the issue really progressivity? Or is it that value-based learners struggle to deal with rewards of varying magnitude (the progression doesn't matter)?\n\nDistributional RL algorithms replace the L2 loss with probabilistic losses. For example, the C51 algorithm uses the KL divergence. That could help deal with reward scale also, by creating a loss that is insensitive to scale. In particular, I can imagine the binary cross-entropy loss making sense since most of your outputs are binary. \n\n\nMinor points\n\n- Condition on gamma < 1/L_h L_h-1 -- doesn't that mean that Lh must be almost 1? Not much slack. \n- Intro, 2nd paragraph. A concrete example would help the reader here.\n- Table 1 might want to be left justified, as it's a little jarring to read.\n\n\n==== Updated review\n\nIn light of the authors' revisions, I'm happy to raise my score to 6. I think the paper is better, although I still wish the presentation was more compelling -- as given, this seems more like an exercise than a contribution with a demonstrated impact. On the other hand, this level of contribution seems relatively on par with e.g. other deep RL papers.\n\nRegarding the revisions, I would encourage the authors to integrate them with the main text. For example, Figure 3 really wants the weight=1 result (maybe as two separate panels -- comparison to other algorithms, a); ablation on weights, b)).", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1570/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1570/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adapting to Reward Progressivity via Spectral Reinforcement Learning", "authorids": ["~Michael_Dann1", "john.thangarajah@rmit.edu.au"], "authors": ["Michael Dann", "John Thangarajah"], "keywords": ["Reinforcement Learning", "Deep Reinforcement Learning"], "abstract": "In this paper we consider reinforcement learning tasks with progressive rewards; that is, tasks where the rewards tend to increase in magnitude over time. We hypothesise that this property may be problematic for value-based deep reinforcement learning agents, particularly if the agent must first succeed in relatively unrewarding regions of the task in order to reach more rewarding regions. To address this issue, we propose Spectral DQN, which decomposes the reward into frequencies such that the high frequencies only activate when large rewards are found. This allows the training loss to be balanced so that it gives more even weighting across small and large reward regions. In two domains with extreme reward progressivity, where standard value-based methods struggle significantly, Spectral DQN is able to make much farther progress. Moreover, when evaluated on a set of six standard Atari games that do not overtly favour the approach, Spectral DQN remains more than competitive: While it underperforms one of the benchmarks in a single game, it comfortably surpasses the benchmarks in three games. These results demonstrate that the approach is not overfit to its target problem, and suggest that Spectral DQN may have advantages beyond addressing reward progressivity.", "one-sentence_summary": "In this paper, we identify a problem with value-based deep reinforcement learning that has not previously been investigated -- namely, reward progressivity -- and propose an approach that addresses it via magnitudinal decomposition of the reward.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "dann|adapting_to_reward_progressivity_via_spectral_reinforcement_learning", "supplementary_material": "", "pdf": "/pdf/a3879d188d633661f6593af48f3c561aefbfb6ff.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ndann2021adapting,\ntitle={Adapting to Reward Progressivity via Spectral Reinforcement Learning},\nauthor={Michael Dann and John Thangarajah},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=dyjPVUc2KB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "dyjPVUc2KB", "replyto": "dyjPVUc2KB", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1570/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538115748, "tmdate": 1606915762428, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1570/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1570/-/Official_Review"}}}, {"id": "20jv0MjuRf0", "original": null, "number": 1, "cdate": 1603243294410, "ddate": null, "tcdate": 1603243294410, "tmdate": 1606343659224, "tddate": null, "forum": "dyjPVUc2KB", "replyto": "dyjPVUc2KB", "invitation": "ICLR.cc/2021/Conference/Paper1570/-/Official_Review", "content": {"title": "Simple, effective, and intuitive -- but lacking analysis", "review": "This work describes and addresses the issue of _reward progressivity_ in reinforcement learning, where as the task progresses the scale of the reward changes. The authors argue that reward progressivity harms Q-learning when training signals arising from large rewards interfere with those arising from smaller rewards. They propose a form of reward decomposition with an analogous modification to the Q-network output, which together help to ensure that training losses from small and large rewards are similarly scaled. The authors present a handful of experimental results demonstrating that their proposed method outperforms two other reward re-scaling baselines when reward progressivity is an issue and maintains good performance in more standard tasks.\n  \n  \n### Clarity\nThis paper is exceptionally clear. There are a couple areas where techincal details are somewhat missing: namely, their implementation of the mixed Monte Carlo update. Otherwise, the ideas are very clearly presented and the authors are careful to spell out caveats and details, which will improve reproducibility and downstream adaptation.\n\n### Originality and Quality\n_Spectral DQN_ is, to my knowledge, original. However, comparison to other forms of reward decomposition is lacking, both experimentally and in the discussion. The authors mention several examples of works that enable non-scalar representation of rewards for handling variability but there is not much attention given to how  these prior works are different and/or why they are not included as baselines.\n  \n_Reward progressivity_ as a motivating issue is also, to my knowledge, original. Indeed, the authors point out the 2 main baselines do not consider _intra-task_ reward variability. This brings up one of my main issues with the paper: there is very little demonstration that reward progressivity is the disruptive force that the authors describe it as. The authors show that baselines suffer/fail on tasks that are engineered towards this problem, but this doesn't offer enough to convince a sceptical reader to believe the paper's underlying intuitions. My understanding of these intuitions is that reward progressivity causes RL to stagnate because it biases the Q-network to be accurate around large rewards at the expense of being accurate around low rewards -- the issue being that this latter inaccuracy leads to poor policies in important but low-reward regions of the task.\n   \nIf this is indeed the intuition, then it would be very useful to actually demonstrate some sort of accuracy trade-off induced by reward progressivity. This would also provide a useful lens through which to validate Spectral DQN, by perhaps being able to characterize how it balances Q-network accuracy for different reward scales compared to other baselines.\n  \nIt would also be valuable to discuss whether reward progressivity is ultimately an issue of discounting. The authors seem to touch on this when describing the _Mrs. Pacman_ results, but a bit more exposition would be useful.\n \nThere are numerous other examples where the authors add interpretation around a particular result to help characterize where Spectral DQN does/doesn't succeed and what a practitioner might want to look out for.These interpretations improve the paper's quality somewhat, but the level of analysis is ultimately lacking. For instance, the authors offer speculation over the results for _Centipede_ and _Bowling_ but don't back up this speculation with any meaningful analysis. I would not want to punish the authors for offering these insights (that commentary is valuable and honest) but I do think the paper would have been better served by spending more time on analysis to back up a couple key insights.\n  \n### Significance\nIf we take the authors' claims at face value (which I am still inclined to do even though the paper lacks more detailed validation), this paper presents a simple technique to handle reward progressivity. This contribution is significant from a practical standpoint but perhaps only when solving tasks with this particular reward structure. For this to have broader signficance, it would be necessary for the authors to provide a more detailed characterization of the potential pitfalls associated with the technique's heuristics. Again, these are discussed (which is appreciated) but only speculatively.\n\n\n**Pros**\n- Adds a simple technique for balancing losses associated with rewards of different\n scales.\n- Follows a straightforward intuition.\n- Overcomes extreme examples of the problem it is designed to address.\n- No obvious, systematic drawbacks of the proposed modification.\n- Exceptionally clear and well-written.\n\n**Cons**\n- Analysis is limited to a handful of training curves with only one minor ablation (in Figure 3)\n- The paper tends to emphasize speculation over more concrete demonstration.\n- Unclear whether the motivating problem presents a significant obstacle.\n\n### Suggestions for improvement\nI would be happy to increase my score if the authors are able to address my criticisms. In particular, I think an analysis demonstrating the how learning dynamics are negatively impacted by reward progressivity (via its measurable effects on the Q-network) would go a long way. In addition, a more thorough ablation study and/or sensitivity analysis would add very useful clarity around the impact of, for example, the mixed Monte Carlo update strategy.\n\n\n### Post-discussion feedback\nThank you for the valuable discussion and revisions. I believe the paper has improved and I've updated my score to reflect that.\n\nThe new analysis in Figure 4 and its surrounding discussion offer some useful insights. I think the authors could still improve this analysis a bit in a final version of the paper (if only just to make the plots a bit easier to parse visually), but the TD-error instability of the baselines seems reasonably clear and I'm inclined to agree with how the authors frame this as an example of the problem they solve. In addition, the added discussion around discounting is also valuable; and the simplification of the algorithm and added ablation experiments improve the quality of the contributions.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1570/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1570/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adapting to Reward Progressivity via Spectral Reinforcement Learning", "authorids": ["~Michael_Dann1", "john.thangarajah@rmit.edu.au"], "authors": ["Michael Dann", "John Thangarajah"], "keywords": ["Reinforcement Learning", "Deep Reinforcement Learning"], "abstract": "In this paper we consider reinforcement learning tasks with progressive rewards; that is, tasks where the rewards tend to increase in magnitude over time. We hypothesise that this property may be problematic for value-based deep reinforcement learning agents, particularly if the agent must first succeed in relatively unrewarding regions of the task in order to reach more rewarding regions. To address this issue, we propose Spectral DQN, which decomposes the reward into frequencies such that the high frequencies only activate when large rewards are found. This allows the training loss to be balanced so that it gives more even weighting across small and large reward regions. In two domains with extreme reward progressivity, where standard value-based methods struggle significantly, Spectral DQN is able to make much farther progress. Moreover, when evaluated on a set of six standard Atari games that do not overtly favour the approach, Spectral DQN remains more than competitive: While it underperforms one of the benchmarks in a single game, it comfortably surpasses the benchmarks in three games. These results demonstrate that the approach is not overfit to its target problem, and suggest that Spectral DQN may have advantages beyond addressing reward progressivity.", "one-sentence_summary": "In this paper, we identify a problem with value-based deep reinforcement learning that has not previously been investigated -- namely, reward progressivity -- and propose an approach that addresses it via magnitudinal decomposition of the reward.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "dann|adapting_to_reward_progressivity_via_spectral_reinforcement_learning", "supplementary_material": "", "pdf": "/pdf/a3879d188d633661f6593af48f3c561aefbfb6ff.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ndann2021adapting,\ntitle={Adapting to Reward Progressivity via Spectral Reinforcement Learning},\nauthor={Michael Dann and John Thangarajah},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=dyjPVUc2KB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "dyjPVUc2KB", "replyto": "dyjPVUc2KB", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1570/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538115748, "tmdate": 1606915762428, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1570/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1570/-/Official_Review"}}}, {"id": "XH6c0L9qq_", "original": null, "number": 12, "cdate": 1606174746917, "ddate": null, "tcdate": 1606174746917, "tmdate": 1606273084976, "tddate": null, "forum": "dyjPVUc2KB", "replyto": "iyRufRrh2iL", "invitation": "ICLR.cc/2021/Conference/Paper1570/-/Official_Comment", "content": {"title": "Increasing gamma to one can work in select tasks, but doesn't generalise.", "comment": "Hi, thanks for the clarifying questions.\n\nIn ongoing tasks, it is not possible to set gamma = 1, because the returns may become unbounded. While the returns are bounded in episodic tasks, with gamma = 1 the Bellman operator is no longer a contraction, so it may take a long time for action-values with inaccurately large magnitudes to compress. The only reliable \"source of truth\" to contract the estimates is episode termination. This can be problematic under deep function approximation, as is perhaps best illustrated via an example:\n\nSuppose that the task is a simple video game, where an enemy ship enters from the top of the screen that the player must shoot. Upon shooting the ship, the player receives 100 points, and a new ship enters the screen. Suppose that the screen looks more or less the same each time the new ship enters, so the sequences of states looks something like s_0, s_1, s_2, ... s_0. In other words, there is a loop from s_0 back to itself, with a positive reward in between. If gamma = 1, then the net effect of learning from this loop will be to push the action-values up. Now suppose that there is a very small, constant chance of the ship's gun backfiring and killing the player, such that the task is now episodic. There will now be a single, very rare TD error that pushes the action-values down. However, if we use gradient clipping, or a Huber loss, or some optimizer that prevents huge parameter updates via an adaptive denominator term (e.g. Adam), then the impact of this rare TD error may not be enough to offset the inflation in the other direction. In other words, it's not so much a theoretical issue as a practical one, which is probably why there isn't a definitive reference on this kind of problem, at least as far as we know of. For gamma < 1, the problem is mitigated, since once the action-values reach a great enough magnitude, the discounting will offset the positive rewards. (Side note: The loop from s_0 to s_0 need not be *exact*; the start and end states just have to be close enough that the agent estimates very similar action-values.)\n\nRegarding the papers you mentioned, the paper \"Separating value functions across time-scales\" uses an overall discount of 0.99 in their Atari experiments. (This is actually less than the discount of 0.99^(1/3) that we use.) They only use larger discounts in their tabular experiments, where the practical issues mentioned above don't arise. Regarding Agent-57, what we should have said is that deep RL agents don't learn *consistently* with discounts very close to 1. In particular, note their comment on page 8: \"running R2D2 with a high discount factor, gamma = 0.9999, surpasses the human baseline in the game of Skiing. However, using that hyperparameter across the full set of games, renders the algorithm very unstable and damages its end performance\". If a task's rewards are very sparse (e.g. a long task with a single reward of 1 upon successful completion), then practical issues like the above are less likely to arise. However, this is not a general solution, and Agent-57 relies on a meta-controller to select smaller discounts on the (majority) of games where gamma = 0.9999 is unstable. Thus the issue of reward progressivity is still relevant on the whole.\n\nRegarding both positive and negative rewards, you are correct. (This is why we gave the examples of Exponential Pong and Ponglantis, where optimal policies only receive positive rewards. We were being a bit inexact to simplify the argument, though we should have been clearer.) All we really mean is that under gamma = 1, the actions at the start of the episode are much more *consequential*, since they affect all future rewards. Thus the issue of \"indifference\" that we described in our previous response ought to be mitigated."}, "signatures": ["ICLR.cc/2021/Conference/Paper1570/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1570/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adapting to Reward Progressivity via Spectral Reinforcement Learning", "authorids": ["~Michael_Dann1", "john.thangarajah@rmit.edu.au"], "authors": ["Michael Dann", "John Thangarajah"], "keywords": ["Reinforcement Learning", "Deep Reinforcement Learning"], "abstract": "In this paper we consider reinforcement learning tasks with progressive rewards; that is, tasks where the rewards tend to increase in magnitude over time. We hypothesise that this property may be problematic for value-based deep reinforcement learning agents, particularly if the agent must first succeed in relatively unrewarding regions of the task in order to reach more rewarding regions. To address this issue, we propose Spectral DQN, which decomposes the reward into frequencies such that the high frequencies only activate when large rewards are found. This allows the training loss to be balanced so that it gives more even weighting across small and large reward regions. In two domains with extreme reward progressivity, where standard value-based methods struggle significantly, Spectral DQN is able to make much farther progress. Moreover, when evaluated on a set of six standard Atari games that do not overtly favour the approach, Spectral DQN remains more than competitive: While it underperforms one of the benchmarks in a single game, it comfortably surpasses the benchmarks in three games. These results demonstrate that the approach is not overfit to its target problem, and suggest that Spectral DQN may have advantages beyond addressing reward progressivity.", "one-sentence_summary": "In this paper, we identify a problem with value-based deep reinforcement learning that has not previously been investigated -- namely, reward progressivity -- and propose an approach that addresses it via magnitudinal decomposition of the reward.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "dann|adapting_to_reward_progressivity_via_spectral_reinforcement_learning", "supplementary_material": "", "pdf": "/pdf/a3879d188d633661f6593af48f3c561aefbfb6ff.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ndann2021adapting,\ntitle={Adapting to Reward Progressivity via Spectral Reinforcement Learning},\nauthor={Michael Dann and John Thangarajah},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=dyjPVUc2KB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "dyjPVUc2KB", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1570/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1570/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1570/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1570/Authors|ICLR.cc/2021/Conference/Paper1570/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1570/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923858251, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1570/-/Official_Comment"}}}, {"id": "HYWBr6NZM24", "original": null, "number": 13, "cdate": 1606229739675, "ddate": null, "tcdate": 1606229739675, "tmdate": 1606229739675, "tddate": null, "forum": "dyjPVUc2KB", "replyto": "dyjPVUc2KB", "invitation": "ICLR.cc/2021/Conference/Paper1570/-/Official_Comment", "content": {"title": "Revision Summary", "comment": "Hi all,\n\nJust letting you know that we've uploaded a revised version of the paper with new results. We're still making a few edits, but we wanted to leave a small window in case anyone has questions about the new experiments.\n\nSummary of the main changes:\n- We've included some additional analysis of what is happening behind-the-scenes as the agents train in Exponential Pong (page 7).\n- We've included an ablation of the error weighting adjustment across all six Atari games, showing what happens when w_i = 1. (See Appendix B.1. TLDR: The weight adjustment is a crucial component.)\n- We've included some additional experiments in variants of Ponglantis to find where \"cutoff point\" for Pop-Art and DQN+TC is, i.e. how big the jump in reward magnitude has to be for these agents to start breaking down. (See Appendix B.2.) We've also included a \"Reverse Ponglantis\" experiment, per our feedback to Reviewer 2. (See Appendix B.3.)\n- We've greatly simplified the implementation, removing both the adaptive mixed Monte Carlo update and the target compression on individual frequencies. The agent now performs noticeably better in Chopper Command and Video Pinball, and comes much closer to learning an optimal policy for Exponential Pong. (The key to this simplification was realising post submission that the weight adjustment ought to be w_i = 1/sigma_i^2, rather than w_i = 1/sigma_i. This is not an ad hoc choice; it has a proper theoretical underpinning, as now explained on page 6. Target compression was previously masking the issue, because it meant that the various sigma_i's were closer in value. After making this change, we found that the mixed Monte Carlo updates and target compression were no longer necessary.)\n- We have expanded Section 8 to provide a more in-depth discussion of the relationship to discounting. We also explain why various recent methods still do not offer a general, principled approach to dealing with reward progressivity.\n\nThanks again for your all feedback and discussion, we feel that it has significantly improved the paper."}, "signatures": ["ICLR.cc/2021/Conference/Paper1570/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1570/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adapting to Reward Progressivity via Spectral Reinforcement Learning", "authorids": ["~Michael_Dann1", "john.thangarajah@rmit.edu.au"], "authors": ["Michael Dann", "John Thangarajah"], "keywords": ["Reinforcement Learning", "Deep Reinforcement Learning"], "abstract": "In this paper we consider reinforcement learning tasks with progressive rewards; that is, tasks where the rewards tend to increase in magnitude over time. We hypothesise that this property may be problematic for value-based deep reinforcement learning agents, particularly if the agent must first succeed in relatively unrewarding regions of the task in order to reach more rewarding regions. To address this issue, we propose Spectral DQN, which decomposes the reward into frequencies such that the high frequencies only activate when large rewards are found. This allows the training loss to be balanced so that it gives more even weighting across small and large reward regions. In two domains with extreme reward progressivity, where standard value-based methods struggle significantly, Spectral DQN is able to make much farther progress. Moreover, when evaluated on a set of six standard Atari games that do not overtly favour the approach, Spectral DQN remains more than competitive: While it underperforms one of the benchmarks in a single game, it comfortably surpasses the benchmarks in three games. These results demonstrate that the approach is not overfit to its target problem, and suggest that Spectral DQN may have advantages beyond addressing reward progressivity.", "one-sentence_summary": "In this paper, we identify a problem with value-based deep reinforcement learning that has not previously been investigated -- namely, reward progressivity -- and propose an approach that addresses it via magnitudinal decomposition of the reward.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "dann|adapting_to_reward_progressivity_via_spectral_reinforcement_learning", "supplementary_material": "", "pdf": "/pdf/a3879d188d633661f6593af48f3c561aefbfb6ff.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ndann2021adapting,\ntitle={Adapting to Reward Progressivity via Spectral Reinforcement Learning},\nauthor={Michael Dann and John Thangarajah},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=dyjPVUc2KB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "dyjPVUc2KB", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1570/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1570/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1570/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1570/Authors|ICLR.cc/2021/Conference/Paper1570/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1570/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923858251, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1570/-/Official_Comment"}}}, {"id": "iyRufRrh2iL", "original": null, "number": 11, "cdate": 1606144538285, "ddate": null, "tcdate": 1606144538285, "tmdate": 1606144538285, "tddate": null, "forum": "dyjPVUc2KB", "replyto": "iCiiWhOMs-", "invitation": "ICLR.cc/2021/Conference/Paper1570/-/Official_Comment", "content": {"title": "Further questions on the relevance of discounting", "comment": "The reviewer question and the author response indicate that the interaction between discounting and reward structure is important for this problem. The authors also state that it was difficult to train a DQN-like system with larger discounts. Some previous papers have demonstrated techniques for training DQN-like systems with larger discounts, such as the Agent-57 paper (https://arxiv.org/pdf/2003.13350.pdf) and \"Separating value functions across time-scales\" (https://arxiv.org/pdf/1902.01883.pdf).  \n\nCan the authors expand on why increasing gamma to one is not viable? Does the problem identified in this paper also arise when gamma is one? The author response says the expected returns should be largest at the start of an episode, but this is not true when there are both negative and positive rewards. Is the proposed method intended primarily for deep RL architectures that have been developed for small discount factors?"}, "signatures": ["ICLR.cc/2021/Conference/Paper1570/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1570/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adapting to Reward Progressivity via Spectral Reinforcement Learning", "authorids": ["~Michael_Dann1", "john.thangarajah@rmit.edu.au"], "authors": ["Michael Dann", "John Thangarajah"], "keywords": ["Reinforcement Learning", "Deep Reinforcement Learning"], "abstract": "In this paper we consider reinforcement learning tasks with progressive rewards; that is, tasks where the rewards tend to increase in magnitude over time. We hypothesise that this property may be problematic for value-based deep reinforcement learning agents, particularly if the agent must first succeed in relatively unrewarding regions of the task in order to reach more rewarding regions. To address this issue, we propose Spectral DQN, which decomposes the reward into frequencies such that the high frequencies only activate when large rewards are found. This allows the training loss to be balanced so that it gives more even weighting across small and large reward regions. In two domains with extreme reward progressivity, where standard value-based methods struggle significantly, Spectral DQN is able to make much farther progress. Moreover, when evaluated on a set of six standard Atari games that do not overtly favour the approach, Spectral DQN remains more than competitive: While it underperforms one of the benchmarks in a single game, it comfortably surpasses the benchmarks in three games. These results demonstrate that the approach is not overfit to its target problem, and suggest that Spectral DQN may have advantages beyond addressing reward progressivity.", "one-sentence_summary": "In this paper, we identify a problem with value-based deep reinforcement learning that has not previously been investigated -- namely, reward progressivity -- and propose an approach that addresses it via magnitudinal decomposition of the reward.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "dann|adapting_to_reward_progressivity_via_spectral_reinforcement_learning", "supplementary_material": "", "pdf": "/pdf/a3879d188d633661f6593af48f3c561aefbfb6ff.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ndann2021adapting,\ntitle={Adapting to Reward Progressivity via Spectral Reinforcement Learning},\nauthor={Michael Dann and John Thangarajah},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=dyjPVUc2KB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "dyjPVUc2KB", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1570/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1570/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1570/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1570/Authors|ICLR.cc/2021/Conference/Paper1570/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1570/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923858251, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1570/-/Official_Comment"}}}, {"id": "5Eh-E86qR_", "original": null, "number": 2, "cdate": 1605851676965, "ddate": null, "tcdate": 1605851676965, "tmdate": 1606021940644, "tddate": null, "forum": "dyjPVUc2KB", "replyto": "dyjPVUc2KB", "invitation": "ICLR.cc/2021/Conference/Paper1570/-/Official_Comment", "content": {"title": "General comments (1/2)", "comment": "We thank the reviewers for the time and effort they have invested in reviewing our paper. Since there was some overlap in the questions and concerns raised by the reviewers, we thought it would help to address those in a general post. Other queries will be addressed below the individual reviews.\n\n**(A) Is Spectral DQN solving a real problem, or a hypothetical one?**\n\nSpectral DQN is addressing a real problem. However, a reason why reward progressivity might appear to be more of a hypothetical issue is because RL research has, to date, focussed mostly on single task learning. Since rewards are often relatively steady intra-task, reward progressivity is often not such a big problem. However, if we ultimately wish to train agents in multi-task settings then reward progressivity will need to be addressed. Progressive rewards are not the only difficulty here; for example, to train a single DQN-style agent on all Atari games, the replay memory would need to be expanded substantially, and the runtime would increase beyond our computational means. While don't have an answer to all of these issues in this work, we believe that reward progressivity is still an important piece of the puzzle.\n\nWhile there have been some large-scale attempts to train a single network to play all games in the Atari-57 suite, e.g. IMPALA (https://arxiv.org/abs/1802.01561), to the best of our knowledge they all clip rewards to [-1, 1]. As the Ponglantis experiments show, unclipping the reward is disastrous for Pop-Art and DQN+TC when even two of the games from this suite are combined.\n\nWe agree with Reviewer 2 that our standard Atari experiments, in and of themselves, do not strongly establish the \u201crealness\u201d of the problem. However, to clarify, that was not exactly what these experiments were intended to show. We do not expect the issue of reward progressivity to arise in all domains, but it does in some. As such, we thought it was important to establish that Spectral DQN is at least non-detrimental in less handpicked tasks, and the standard Atari experiments do support this. Moreover, while the results were less clear-cut than in the extreme domains, Spectral DQN did notably perform well in games with mild reward progressivity (Chopper Command, Ms Pacman and Video Pinball).\n\n**(B) Too many moving parts, lack of ablations.**\n\nIn Section 5, we mention three measures that we used to stabilise Spectral DQN:\n\n(1) Weighting the ith frequency\u2019s error by w_i.\n(2) Using target compression on the individual frequencies.\n(3) Adaptive Mixed Monte Carlo updates.\n\nThe overall feeling we got from the reviews is that this constitutes too many moving parts, and that extra ablations would help. In short, we agree, and we have been working on simplifying the approach. While our results are not ready to share yet, we have devised a cleaner approach that does away with measures (2) and (3). Moreover, we are running a full ablation to show the importance of (1). We will update the paper and provide more commentary on this when the results are in, but for now we just wanted to provide the reviewers with this update."}, "signatures": ["ICLR.cc/2021/Conference/Paper1570/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1570/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adapting to Reward Progressivity via Spectral Reinforcement Learning", "authorids": ["~Michael_Dann1", "john.thangarajah@rmit.edu.au"], "authors": ["Michael Dann", "John Thangarajah"], "keywords": ["Reinforcement Learning", "Deep Reinforcement Learning"], "abstract": "In this paper we consider reinforcement learning tasks with progressive rewards; that is, tasks where the rewards tend to increase in magnitude over time. We hypothesise that this property may be problematic for value-based deep reinforcement learning agents, particularly if the agent must first succeed in relatively unrewarding regions of the task in order to reach more rewarding regions. To address this issue, we propose Spectral DQN, which decomposes the reward into frequencies such that the high frequencies only activate when large rewards are found. This allows the training loss to be balanced so that it gives more even weighting across small and large reward regions. In two domains with extreme reward progressivity, where standard value-based methods struggle significantly, Spectral DQN is able to make much farther progress. Moreover, when evaluated on a set of six standard Atari games that do not overtly favour the approach, Spectral DQN remains more than competitive: While it underperforms one of the benchmarks in a single game, it comfortably surpasses the benchmarks in three games. These results demonstrate that the approach is not overfit to its target problem, and suggest that Spectral DQN may have advantages beyond addressing reward progressivity.", "one-sentence_summary": "In this paper, we identify a problem with value-based deep reinforcement learning that has not previously been investigated -- namely, reward progressivity -- and propose an approach that addresses it via magnitudinal decomposition of the reward.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "dann|adapting_to_reward_progressivity_via_spectral_reinforcement_learning", "supplementary_material": "", "pdf": "/pdf/a3879d188d633661f6593af48f3c561aefbfb6ff.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ndann2021adapting,\ntitle={Adapting to Reward Progressivity via Spectral Reinforcement Learning},\nauthor={Michael Dann and John Thangarajah},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=dyjPVUc2KB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "dyjPVUc2KB", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1570/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1570/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1570/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1570/Authors|ICLR.cc/2021/Conference/Paper1570/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1570/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923858251, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1570/-/Official_Comment"}}}, {"id": "iCiiWhOMs-", "original": null, "number": 8, "cdate": 1605853680504, "ddate": null, "tcdate": 1605853680504, "tmdate": 1605853948977, "tddate": null, "forum": "dyjPVUc2KB", "replyto": "20jv0MjuRf0", "invitation": "ICLR.cc/2021/Conference/Paper1570/-/Official_Comment", "content": {"title": "Author response", "comment": "Thank you for your considered review. Since some of your concerns were also raised by other reviewers, please see our general comments to all reviewers before reading the below.\n\n**\u201cThe authors show that baselines suffer/fail on tasks that are engineered towards this problem, but this doesn't offer enough to convince a sceptical reader to believe the paper's underlying intuitions.\u201d**\n\nThank you for explaining this criticism in such detail, it is a fair point. We are currently running some additional experiments of the type you asked for, plus some extra ablations, and we will have more to say about these once the results are in.\n\n**\u201cIt would also be valuable to discuss whether reward progressivity is ultimately an issue of discounting.\u201d**\n\nWe only touched on this in the current version of the paper due to space constraints, but we will make use of the extra page allowed for the next version to discuss this in more detail.\n\nIn our view, yes, reward progressivity is ultimately an issue of discounting. If it were somehow possible to train without a discount, i.e. gamma = 1, then the expected returns (and hence the temporal difference errors) ought to be largest at the *beginning* of Exponential Pong and Ponglantis. Put another way, the problem with discounting in progressive reward tasks is that it can cause \u201cindifference\u201d; if the large rewards late in the task are discounted too heavily, the agent will not care sufficiently about performing well in the early stages of the task in order to reach them.\n\nUnfortunately, increasing gamma towards 1 is not a viable solution, because algorithms like DQN start to destabilise with very mild discounts. In our experiments, we already pushed gamma to 0.99^(1/3), which triples the agent\u2019s temporal horizon compared to vanilla DQN. (As noted in the Appendix, we use n-step learning with n = 3 to maintain the contractiveness of the backup operator.) However, in our experience, increasing the discount beyond this point starts to cause instability.\n\n**\u201ccomparison to other forms of reward decomposition is lacking, both experimentally and in the discussion.\u201d**\n\nWe have responded to this in part (C) of our general comments, since Reviewer 2 raised a related point about distributional RL.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1570/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1570/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adapting to Reward Progressivity via Spectral Reinforcement Learning", "authorids": ["~Michael_Dann1", "john.thangarajah@rmit.edu.au"], "authors": ["Michael Dann", "John Thangarajah"], "keywords": ["Reinforcement Learning", "Deep Reinforcement Learning"], "abstract": "In this paper we consider reinforcement learning tasks with progressive rewards; that is, tasks where the rewards tend to increase in magnitude over time. We hypothesise that this property may be problematic for value-based deep reinforcement learning agents, particularly if the agent must first succeed in relatively unrewarding regions of the task in order to reach more rewarding regions. To address this issue, we propose Spectral DQN, which decomposes the reward into frequencies such that the high frequencies only activate when large rewards are found. This allows the training loss to be balanced so that it gives more even weighting across small and large reward regions. In two domains with extreme reward progressivity, where standard value-based methods struggle significantly, Spectral DQN is able to make much farther progress. Moreover, when evaluated on a set of six standard Atari games that do not overtly favour the approach, Spectral DQN remains more than competitive: While it underperforms one of the benchmarks in a single game, it comfortably surpasses the benchmarks in three games. These results demonstrate that the approach is not overfit to its target problem, and suggest that Spectral DQN may have advantages beyond addressing reward progressivity.", "one-sentence_summary": "In this paper, we identify a problem with value-based deep reinforcement learning that has not previously been investigated -- namely, reward progressivity -- and propose an approach that addresses it via magnitudinal decomposition of the reward.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "dann|adapting_to_reward_progressivity_via_spectral_reinforcement_learning", "supplementary_material": "", "pdf": "/pdf/a3879d188d633661f6593af48f3c561aefbfb6ff.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ndann2021adapting,\ntitle={Adapting to Reward Progressivity via Spectral Reinforcement Learning},\nauthor={Michael Dann and John Thangarajah},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=dyjPVUc2KB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "dyjPVUc2KB", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1570/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1570/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1570/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1570/Authors|ICLR.cc/2021/Conference/Paper1570/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1570/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923858251, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1570/-/Official_Comment"}}}, {"id": "mLslZvxaoXI", "original": null, "number": 7, "cdate": 1605852986021, "ddate": null, "tcdate": 1605852986021, "tmdate": 1605853399766, "tddate": null, "forum": "dyjPVUc2KB", "replyto": "es7eCmturDX", "invitation": "ICLR.cc/2021/Conference/Paper1570/-/Official_Comment", "content": {"title": "Author response", "comment": "Thank you for your considered review. Since some of your concerns were also raised by other reviewers, please see our general comments to all reviewers before reading the below.\n\n**Is there a \u201ccommonly used benchmark domain or one which was not specifically engineered for progressive rewards where value based deep reinforcement learning would fail or not perform well unless it was using Spectral DQN\u201d**\n\nSince Reviewer 2 raised a similar point, we have addressed it in part (A) of our general comments.\n\n**\u201cIt is unclear how much benefit the spectral decomposition is having\u201d / \u201cAnother useful ablation could be to remove the monte carlo mixing and show how unstable the updates get.\u201d**\n\nWe appreciate these points, and we are running additional experiments to address them. We will have more to say soon when the results are in.\n\n**How does the van Seijen et al. paper on logarithmic mappings (Using a Logarithmic Mapping to Enable Lower Discount Factors in Reinforcement Learning) compare to the related work?**\n\nThank you for making us aware of this work, it is indeed relevant and we will update the paper to cite it. Like us, it seems that the authors were strongly influenced by Pohlen et al. (2018). Interestingly though, their approach of using a logarithmic mapping is contradictory to Pohlen et al.\u2019s requirement that the mapping function have a Lipschitz continuous inverse in order to ensure convergence. (See our discussion at the end of page 3, where we explain why we didn\u2019t use a logarithmic mapping.) Practically, one way to get around poor convergence is to use a shaper discount factor, and this is indeed what van Seijen et al. do. Note that they use a discount of 0.96 in their main experiments, but that the performance of their method degrades significantly with a discount of 0.99, consistent with convergence issues (see Appendix C.2 in their work). The paper frames the enablement of a sharp discount as an advantage, but in reality a sharp discount comes with a big drawback; namely, it greatly constrains the agent\u2019s temporal horizon. It is easy to provide examples where this will lead to poor performance. For example, in the game Bowling, there is a delay of about 100 frames between the ball being released and the subsequent reward. However, if the player bowls a spare, the reward is delayed by a further 100 frames. Thus, under a discount of 0.96, it is better to consistently bowl 9s than to bowl spares. The reason why their Bowling results remain on par with DQN is because both methods use reward clipping, and all clipped reward methods perform terribly in this game. (See their Figure 14, and note that a score of 30 implies that the agent is bowling 3s on average.)\n\nGiven its violation of the Lipschitz constraint, we were initially surprised to find a convergence proof in van Seijen et al.\u2019s work. However, the proof should be taken with a grain of salt, since one of its requirements is for the step size to decay to zero. (Specifically beta_2 must go to zero, but beta_1 * beta_2 is analogous to the standard step size -- see the bottom of page 8 in their paper.) It is much easier to ensure convergence under this condition, but it is a significant practical limitation and van Seijen et al. do not actually decay the step size to zero in their experiments.\n\n**Figure 2 has a typo (Fiilled instead of filled).**\n\nThanks for picking this up!\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1570/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1570/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adapting to Reward Progressivity via Spectral Reinforcement Learning", "authorids": ["~Michael_Dann1", "john.thangarajah@rmit.edu.au"], "authors": ["Michael Dann", "John Thangarajah"], "keywords": ["Reinforcement Learning", "Deep Reinforcement Learning"], "abstract": "In this paper we consider reinforcement learning tasks with progressive rewards; that is, tasks where the rewards tend to increase in magnitude over time. We hypothesise that this property may be problematic for value-based deep reinforcement learning agents, particularly if the agent must first succeed in relatively unrewarding regions of the task in order to reach more rewarding regions. To address this issue, we propose Spectral DQN, which decomposes the reward into frequencies such that the high frequencies only activate when large rewards are found. This allows the training loss to be balanced so that it gives more even weighting across small and large reward regions. In two domains with extreme reward progressivity, where standard value-based methods struggle significantly, Spectral DQN is able to make much farther progress. Moreover, when evaluated on a set of six standard Atari games that do not overtly favour the approach, Spectral DQN remains more than competitive: While it underperforms one of the benchmarks in a single game, it comfortably surpasses the benchmarks in three games. These results demonstrate that the approach is not overfit to its target problem, and suggest that Spectral DQN may have advantages beyond addressing reward progressivity.", "one-sentence_summary": "In this paper, we identify a problem with value-based deep reinforcement learning that has not previously been investigated -- namely, reward progressivity -- and propose an approach that addresses it via magnitudinal decomposition of the reward.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "dann|adapting_to_reward_progressivity_via_spectral_reinforcement_learning", "supplementary_material": "", "pdf": "/pdf/a3879d188d633661f6593af48f3c561aefbfb6ff.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ndann2021adapting,\ntitle={Adapting to Reward Progressivity via Spectral Reinforcement Learning},\nauthor={Michael Dann and John Thangarajah},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=dyjPVUc2KB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "dyjPVUc2KB", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1570/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1570/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1570/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1570/Authors|ICLR.cc/2021/Conference/Paper1570/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1570/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923858251, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1570/-/Official_Comment"}}}, {"id": "9mPHMly1hE", "original": null, "number": 6, "cdate": 1605852635434, "ddate": null, "tcdate": 1605852635434, "tmdate": 1605852635434, "tddate": null, "forum": "dyjPVUc2KB", "replyto": "XxJCLLLTmL1", "invitation": "ICLR.cc/2021/Conference/Paper1570/-/Official_Comment", "content": {"title": "Author response (2/2)", "comment": "**\u201cDistributional RL algorithms replace the L2 loss with probabilistic losses. For example, the C51 algorithm uses the KL divergence. That could help deal with reward scale also, by creating a loss that is insensitive to scale.\u201d**\n\nSince Reviewer 3 raised a similar point, please see part (C) of our general comments.\n\n**\u201cCondition on gamma < 1/L_h L_h-1 -- doesn't that mean that Lh must be almost 1?\u201d**\n\nIt\u2019s possible that the compression function could just be a linear rescaling, in which case L_h = 1 / L_h-1. So you could have, for example, Lh = 5, L_h-1 = 0.2. However, you are correct that for non-trivial scaling functions, this is quite a strict requirement. It may be possible to derive a more relaxed bound, but we are not aware of any work to this end.\n\n**Other minor points**\n\nThank you for bringing these to our attention, we will address them in the next draft.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1570/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1570/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adapting to Reward Progressivity via Spectral Reinforcement Learning", "authorids": ["~Michael_Dann1", "john.thangarajah@rmit.edu.au"], "authors": ["Michael Dann", "John Thangarajah"], "keywords": ["Reinforcement Learning", "Deep Reinforcement Learning"], "abstract": "In this paper we consider reinforcement learning tasks with progressive rewards; that is, tasks where the rewards tend to increase in magnitude over time. We hypothesise that this property may be problematic for value-based deep reinforcement learning agents, particularly if the agent must first succeed in relatively unrewarding regions of the task in order to reach more rewarding regions. To address this issue, we propose Spectral DQN, which decomposes the reward into frequencies such that the high frequencies only activate when large rewards are found. This allows the training loss to be balanced so that it gives more even weighting across small and large reward regions. In two domains with extreme reward progressivity, where standard value-based methods struggle significantly, Spectral DQN is able to make much farther progress. Moreover, when evaluated on a set of six standard Atari games that do not overtly favour the approach, Spectral DQN remains more than competitive: While it underperforms one of the benchmarks in a single game, it comfortably surpasses the benchmarks in three games. These results demonstrate that the approach is not overfit to its target problem, and suggest that Spectral DQN may have advantages beyond addressing reward progressivity.", "one-sentence_summary": "In this paper, we identify a problem with value-based deep reinforcement learning that has not previously been investigated -- namely, reward progressivity -- and propose an approach that addresses it via magnitudinal decomposition of the reward.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "dann|adapting_to_reward_progressivity_via_spectral_reinforcement_learning", "supplementary_material": "", "pdf": "/pdf/a3879d188d633661f6593af48f3c561aefbfb6ff.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ndann2021adapting,\ntitle={Adapting to Reward Progressivity via Spectral Reinforcement Learning},\nauthor={Michael Dann and John Thangarajah},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=dyjPVUc2KB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "dyjPVUc2KB", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1570/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1570/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1570/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1570/Authors|ICLR.cc/2021/Conference/Paper1570/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1570/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923858251, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1570/-/Official_Comment"}}}, {"id": "XxJCLLLTmL1", "original": null, "number": 5, "cdate": 1605852494556, "ddate": null, "tcdate": 1605852494556, "tmdate": 1605852494556, "tddate": null, "forum": "dyjPVUc2KB", "replyto": "P_2rs-Ix7u", "invitation": "ICLR.cc/2021/Conference/Paper1570/-/Official_Comment", "content": {"title": "Author response (1/2)", "comment": "Thank you for your considered review. Since some of your concerns were also raised by other reviewers, please see our general comments to all reviewers before reading the below.\n\n**\u201cMy main concern is whether this is solving a real problem, or a hypothetical one.\u201d Also: \u201cThe results of Fig. 4 are not terribly exciting. Sometimes spectral DQN works better, sometimes it does not. Can you comment on this?\u201d**\n\nSince Reviewer 1 also raised this point, we have addressed it in part (A) of our general comments.\n\n**\u201cWhy use a thermometer encoding versus a binary encoding? I.e., write the reward in binary, and encode its bits.\u201d**\n\nWe actually touch on this in the Appendix (see the last paragraph of page 11), but since it is an important point we will make it more prominent. Essentially, it would be weird for the agent to treat rewards of very similar magnitude in a completely different way. For example, it should not matter much to a rational agent if a particular reward\u2019s magnitude is 63 or 64. However, under a binary encoding, 63 is 0111111, while 64 is 1000000. A reward of 63 would thus trigger a much larger TD error at the start of training than a reward of 64 (since 63 triggers many more frequencies). The thermometer encoding addresses this problem.\n\n**\u201cWhy not use base gamma instead of base 2? That seems like a more natural way to encode returns.\u201d**\n\nBase gamma might be a natural way to encode returns if our central problem was estimating how far away the rewards are, in terms of time steps. However, to deal with very large rewards, our approach requires a base greater than one, to ensure that the buckets have increasing width. (Without this, it would not be possible to represent rewards of arbitrary size with the thermometer encoding.) Another option would be to use base 1/gamma, but for bases very close to 1 we would require a huge number of frequencies to fully capture all rewards in Exponential Pong.\n\n**\u201cgiven that returns are bounded for gamma < 1, what do you mean? It seems like you need log(VMax) bins at most.\u201d**\n\nOur only point here is that you don\u2019t always know VMax in advance. That being said, we show that this isn\u2019t a big limitation, since with exponentially sized buckets it\u2019s possible to represent very large rewards with reasonably small N. Side note: We\u2019re not sure whether it is clear, but the number of heads required is actually determined by the maximum *reward* magnitude, not the maximum return. Please let us know if we can clarify this.\n\n**\u201cThe choice w_i = 1/sigma_i seems particularly ad-hoc. In particular, I'm disappointed that you did not include the experiment w_i = 1.\u201d**\n\nThis is a good point. We actually did run some initial experiments with w_i = 1, but found that the agent\u2019s performance was adversely affected. We will elaborate on the reasons for this in the revised paper. Moreover, we agree that this should be established properly via experiments, and we are now running a full ablation.\n\n**\u201cyou should include published DQN results (and if possible, other value-based methods) as I would expect these to be worse for games that have rewards of different magnitudes.\u201d**\n\nInterestingly, as discussed in the Pop-Art paper, this is not always the case. It turns out that the reward clipping mechanism is a very good heuristic for Atari, and disabling it sometimes leads to worse performance. (There is a nice discussion about why this happens on page 8 of the Pop-Art paper. In particular, see their discussion of the game Time Pilot.) Exponential Pong also provides a good example -- if we were to apply standard DQN to this game then it ought to do very well, because standard DQN sees no difference between Exponential Pong and standard Pong. However, the way it achieves this (reward clipping) is unprincipled, which is why we don\u2019t consider DQN as a benchmark in this paper.\n\n**\"the phenomenon of reward progressivity\". Is the issue really progressivity? Or is it that value-based learners struggle to deal with rewards of varying magnitude (the progression doesn't matter)?**\n\nA good illustration of this point is Ponglantis. If the Altantis phase happened to come before the Pong phase, then Pop-Art and DQN+TC would both get decent scores. (Both methods perform well on standard Atlantis, so they will do well on the first phase of the task. It doesn\u2019t matter if they subsequently fail on Pong, because the bulk of the points come from the Atlantis phase.) From this example, it\u2019s clear that the problem only comes to the fore if the large reward phase comes *after* the small reward phase, i.e. the rewards are progressive. We will include an experiment with reverse Ponglantis to properly establish this.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1570/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1570/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adapting to Reward Progressivity via Spectral Reinforcement Learning", "authorids": ["~Michael_Dann1", "john.thangarajah@rmit.edu.au"], "authors": ["Michael Dann", "John Thangarajah"], "keywords": ["Reinforcement Learning", "Deep Reinforcement Learning"], "abstract": "In this paper we consider reinforcement learning tasks with progressive rewards; that is, tasks where the rewards tend to increase in magnitude over time. We hypothesise that this property may be problematic for value-based deep reinforcement learning agents, particularly if the agent must first succeed in relatively unrewarding regions of the task in order to reach more rewarding regions. To address this issue, we propose Spectral DQN, which decomposes the reward into frequencies such that the high frequencies only activate when large rewards are found. This allows the training loss to be balanced so that it gives more even weighting across small and large reward regions. In two domains with extreme reward progressivity, where standard value-based methods struggle significantly, Spectral DQN is able to make much farther progress. Moreover, when evaluated on a set of six standard Atari games that do not overtly favour the approach, Spectral DQN remains more than competitive: While it underperforms one of the benchmarks in a single game, it comfortably surpasses the benchmarks in three games. These results demonstrate that the approach is not overfit to its target problem, and suggest that Spectral DQN may have advantages beyond addressing reward progressivity.", "one-sentence_summary": "In this paper, we identify a problem with value-based deep reinforcement learning that has not previously been investigated -- namely, reward progressivity -- and propose an approach that addresses it via magnitudinal decomposition of the reward.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "dann|adapting_to_reward_progressivity_via_spectral_reinforcement_learning", "supplementary_material": "", "pdf": "/pdf/a3879d188d633661f6593af48f3c561aefbfb6ff.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ndann2021adapting,\ntitle={Adapting to Reward Progressivity via Spectral Reinforcement Learning},\nauthor={Michael Dann and John Thangarajah},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=dyjPVUc2KB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "dyjPVUc2KB", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1570/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1570/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1570/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1570/Authors|ICLR.cc/2021/Conference/Paper1570/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1570/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923858251, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1570/-/Official_Comment"}}}, {"id": "YnEvTZPKC6Y", "original": null, "number": 4, "cdate": 1605852161746, "ddate": null, "tcdate": 1605852161746, "tmdate": 1605852161746, "tddate": null, "forum": "dyjPVUc2KB", "replyto": "E46BITZp1PY", "invitation": "ICLR.cc/2021/Conference/Paper1570/-/Official_Comment", "content": {"title": "Author response", "comment": "Thank you for your considered review. Since some of your concerns were also raised by other reviewers, please see our general comments to all reviewers before reading the below.\n\n**\u201cI see no reason that the constructed domain ExponentialPong shouldn't join the benchmark for any subsequent general-purpose agent\u201d**\n\nThank you for your encouragement, we agree!\n\n**\u201cWhile I believe the selected experiments are sufficient to demonstrate the claims in the paper, they do not fully explore the capabilities of the method and the intuitions that motivate it.\u201d**\n\nThis is a fair criticism, and we are currently running a number of additional experiments in order to provide a better exploration of the method. We will have more to say about this once the results are in.\n\n**\u201cIt would seem that this approach would work for a parametrizable class of reward functions - why not test that? Perhaps ExponentialPong with reward b^(alpha x N) evaluated on a grid for b and alpha?\u201d**\n\nEvaluating on a grid for b and alpha would require a lot of compute, but we agree that it would be helpful to include some additional experiments to find where the \u201ccutoff point\u201d is, i.e. how big the jump in reward magnitude has to be for Pop-Art and DQN+TC to start breaking down. To address this, we are running some additional experiments in Ponglantis, where we have scaled down the reward in the second, Atlantis phase of the task.\n\n**\u201cFrequencies needn't be geometric, how might other choices have performed.**\n\nThe issue here is tractability. With quadratically scaling frequencies (b_i = i^2), the approach would require almost 150 frequencies to capture rewards up to 1 million, as in Exponential Pong. If we could somehow know in advance that the rewards do not scale exponentially, then a sub-exponential scale would be appropriate. However, to make Spectral DQN as general as possible, we didn\u2019t want to rely on this kind of knowledge.\n\n**\u201cI'd like to have seen a slightly richer discussion/motivation of the mixed Monte Carlo update. Clearly something is necessary here, but why this? What else was tried?\u201d**\n\nWe too felt that the mixed Monte Carlo update was slightly unsatisfactory, and have since devised a cleaner method where we just initialise the weights and biases of the final layer to zero. We are running fresh experiments now, and will provide more commentary once the results are in.\n\n**Was an adaptive approach for b tried?**\n\nThis is indeed an interesting idea, as it would allow us to cut down on \u201cwasted\u201d frequencies in tasks where the rewards scale sub-exponentially. However, this would require the neural network\u2019s outputs to be adjusted when b is changed, so as to preserve the old predictions (similar to the way the Pop-Art algorithm works). Unlike Pop-Art, something much more sophisticated than a linear scale and shift would be required though, which is an interesting problem for future work.\n\n**Typos**\n\nThank you for picking these up, we will address them in the next draft of the paper."}, "signatures": ["ICLR.cc/2021/Conference/Paper1570/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1570/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adapting to Reward Progressivity via Spectral Reinforcement Learning", "authorids": ["~Michael_Dann1", "john.thangarajah@rmit.edu.au"], "authors": ["Michael Dann", "John Thangarajah"], "keywords": ["Reinforcement Learning", "Deep Reinforcement Learning"], "abstract": "In this paper we consider reinforcement learning tasks with progressive rewards; that is, tasks where the rewards tend to increase in magnitude over time. We hypothesise that this property may be problematic for value-based deep reinforcement learning agents, particularly if the agent must first succeed in relatively unrewarding regions of the task in order to reach more rewarding regions. To address this issue, we propose Spectral DQN, which decomposes the reward into frequencies such that the high frequencies only activate when large rewards are found. This allows the training loss to be balanced so that it gives more even weighting across small and large reward regions. In two domains with extreme reward progressivity, where standard value-based methods struggle significantly, Spectral DQN is able to make much farther progress. Moreover, when evaluated on a set of six standard Atari games that do not overtly favour the approach, Spectral DQN remains more than competitive: While it underperforms one of the benchmarks in a single game, it comfortably surpasses the benchmarks in three games. These results demonstrate that the approach is not overfit to its target problem, and suggest that Spectral DQN may have advantages beyond addressing reward progressivity.", "one-sentence_summary": "In this paper, we identify a problem with value-based deep reinforcement learning that has not previously been investigated -- namely, reward progressivity -- and propose an approach that addresses it via magnitudinal decomposition of the reward.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "dann|adapting_to_reward_progressivity_via_spectral_reinforcement_learning", "supplementary_material": "", "pdf": "/pdf/a3879d188d633661f6593af48f3c561aefbfb6ff.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ndann2021adapting,\ntitle={Adapting to Reward Progressivity via Spectral Reinforcement Learning},\nauthor={Michael Dann and John Thangarajah},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=dyjPVUc2KB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "dyjPVUc2KB", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1570/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1570/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1570/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1570/Authors|ICLR.cc/2021/Conference/Paper1570/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1570/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923858251, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1570/-/Official_Comment"}}}, {"id": "KHYM9_ZqOyo", "original": null, "number": 3, "cdate": 1605851862318, "ddate": null, "tcdate": 1605851862318, "tmdate": 1605851862318, "tddate": null, "forum": "dyjPVUc2KB", "replyto": "5Eh-E86qR_", "invitation": "ICLR.cc/2021/Conference/Paper1570/-/Official_Comment", "content": {"title": "General comments (2/2)", "comment": "**(C) \u201cThe authors mention several examples of works that enable non-scalar representation of rewards for handling variability but there is not much attention given to how these prior works are different and/or why they are not included as baselines\u201d**\n\nThis is an important point, so we will expand on it in the next revision. The reason we have cited these works is because they bear many surface-level similarities to our method. However, on a deeper level, they are not actually addressing the same problem. Some of them are not related to reward variability at all; for example, Romoff et al.\u2019s (2019) method for decomposing the expected return with respect to the time scale is unrelated to reward variability.\n\nReviewer 2 notes that the probabilistic loss used by the distributional C51 algorithm might help address reward variability. However, it is important to note that the C51 algorithm learns from the clipped reward, and it assumes that the expected return lies in [-10, 10]. While reward clipping is one way to deal with reward variability, it is unprincipled. To handle unclipped rewards and deal with games where the expected return can reach tens of thousands, one would either need to increase the number of return buckets substantially, so as to cover a wider range, or increase the size of the buckets. The first of these options is computationally infeasible. The second option would result in a loss of precision, and it was shown in the C51 paper that the performance of the algorithm degrades significantly when the bucket size is increased by even a factor of 10 (see Figure 3 in the C51 paper).\n\nThe other main type of distributional algorithm -- namely, implicit quantile methods -- still ultimately learns expected returns rather than probabilities. As such, we cannot see any reason why it would be especially well-suited to progressive reward tasks. The reason we benchmarked against DQN+TC, on the other hand, was because its square root compression *could* foreseeably help with the problem.\n\nVan Seijen et al. (2017) propose an approach where a human expert provides a decomposed reward function and the agent learns different value functions for the different rewards. This could potentially help with the problem (e.g. in Ponglantis, we could train one value function for Pong, and one for Atlantis), but the need for a human expert is a big drawback."}, "signatures": ["ICLR.cc/2021/Conference/Paper1570/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1570/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adapting to Reward Progressivity via Spectral Reinforcement Learning", "authorids": ["~Michael_Dann1", "john.thangarajah@rmit.edu.au"], "authors": ["Michael Dann", "John Thangarajah"], "keywords": ["Reinforcement Learning", "Deep Reinforcement Learning"], "abstract": "In this paper we consider reinforcement learning tasks with progressive rewards; that is, tasks where the rewards tend to increase in magnitude over time. We hypothesise that this property may be problematic for value-based deep reinforcement learning agents, particularly if the agent must first succeed in relatively unrewarding regions of the task in order to reach more rewarding regions. To address this issue, we propose Spectral DQN, which decomposes the reward into frequencies such that the high frequencies only activate when large rewards are found. This allows the training loss to be balanced so that it gives more even weighting across small and large reward regions. In two domains with extreme reward progressivity, where standard value-based methods struggle significantly, Spectral DQN is able to make much farther progress. Moreover, when evaluated on a set of six standard Atari games that do not overtly favour the approach, Spectral DQN remains more than competitive: While it underperforms one of the benchmarks in a single game, it comfortably surpasses the benchmarks in three games. These results demonstrate that the approach is not overfit to its target problem, and suggest that Spectral DQN may have advantages beyond addressing reward progressivity.", "one-sentence_summary": "In this paper, we identify a problem with value-based deep reinforcement learning that has not previously been investigated -- namely, reward progressivity -- and propose an approach that addresses it via magnitudinal decomposition of the reward.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "dann|adapting_to_reward_progressivity_via_spectral_reinforcement_learning", "supplementary_material": "", "pdf": "/pdf/a3879d188d633661f6593af48f3c561aefbfb6ff.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ndann2021adapting,\ntitle={Adapting to Reward Progressivity via Spectral Reinforcement Learning},\nauthor={Michael Dann and John Thangarajah},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=dyjPVUc2KB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "dyjPVUc2KB", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1570/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1570/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1570/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1570/Authors|ICLR.cc/2021/Conference/Paper1570/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1570/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923858251, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1570/-/Official_Comment"}}}, {"id": "es7eCmturDX", "original": null, "number": 2, "cdate": 1603325874912, "ddate": null, "tcdate": 1603325874912, "tmdate": 1605024412702, "tddate": null, "forum": "dyjPVUc2KB", "replyto": "dyjPVUc2KB", "invitation": "ICLR.cc/2021/Conference/Paper1570/-/Official_Review", "content": {"title": "Review for Spectral RL", "review": "## Summary\n\nThis paper details the problems that might arise in value-based reinforcement learning methods in domains where reward progressivity is present. To show that current methods do not handle reward progressivity, the authors introduce two domains, _Exponential Pong_ and _Ponglantis_.\n\nAfter showing that current methods do not work well in these domains, the paper then goes on to propose a solution, spectral decomposition of rewards. The paper shows that returns learned separately on decomposed rewards can be composed to get the original return. The paper then presents spectral Q-learning and spectral DQN, with experiments on the domains presented earlier as well as experiments on 6 Atari games.\n\nOn the two domains where progressive rewards were shown to be problematic, Spectral DQN is shown to work better than current approaches. On Atari games, spectral does as well as current approaches, doing better in 3 out of 6 domains.\n\n## Positives\n+ The setting of progressive rewards is interesting.\n+ The domains proposed for testing these rewards are clear.\n+ The spectral reward decomposition is a simple idea and is explained well.\n+ The effectiveness of spectral DQN on the two domains introduced in the paper is clear.\n+ Experimental details are clear and additional steps taken to stabilize learning are included.\n\n## Negatives/ Questions\n- One question that does not seem to be satisfactorily addressed is whether there is a commonly used benchmark domain or one which was not specifically engineered for progressive rewards where value based deep reinforcement learning would fail or not perform well unless it was using Spectral DQN.\n- The spectral DQN objective (eqn. 8) includes target compression. It is unclear how much benefit the spectral decomposition is having in the presence of target compression. While it is fine to require target compression for the full benefit of spectral DQN, it would be good to see an ablation of how well spectral DQN does without target compression.\n- Another useful ablation could be to remove the monte carlo mixing and show how unstable the updates get.\n- How does the van Seijen et al. paper on logarithmic mappings (Using a Logarithmic Mapping to Enable Lower Discount Factors in Reinforcement Learning) compare to the related work? It is motivated by a different problem, but since a possible logarithmic mapping might mitigate the problems that come up with progressive rewards their method might be a possible solution to be compared against.\n\n## Other Comments\n* Figure 2 has a typo (Fiilled instead of filled).\n\n## Summary\nOverall, I find the idea in this paper clear, simple, and effective. There are some additional questions and comments that if addressed would make the paper a more well-rounded submission.\n\n", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1570/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1570/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adapting to Reward Progressivity via Spectral Reinforcement Learning", "authorids": ["~Michael_Dann1", "john.thangarajah@rmit.edu.au"], "authors": ["Michael Dann", "John Thangarajah"], "keywords": ["Reinforcement Learning", "Deep Reinforcement Learning"], "abstract": "In this paper we consider reinforcement learning tasks with progressive rewards; that is, tasks where the rewards tend to increase in magnitude over time. We hypothesise that this property may be problematic for value-based deep reinforcement learning agents, particularly if the agent must first succeed in relatively unrewarding regions of the task in order to reach more rewarding regions. To address this issue, we propose Spectral DQN, which decomposes the reward into frequencies such that the high frequencies only activate when large rewards are found. This allows the training loss to be balanced so that it gives more even weighting across small and large reward regions. In two domains with extreme reward progressivity, where standard value-based methods struggle significantly, Spectral DQN is able to make much farther progress. Moreover, when evaluated on a set of six standard Atari games that do not overtly favour the approach, Spectral DQN remains more than competitive: While it underperforms one of the benchmarks in a single game, it comfortably surpasses the benchmarks in three games. These results demonstrate that the approach is not overfit to its target problem, and suggest that Spectral DQN may have advantages beyond addressing reward progressivity.", "one-sentence_summary": "In this paper, we identify a problem with value-based deep reinforcement learning that has not previously been investigated -- namely, reward progressivity -- and propose an approach that addresses it via magnitudinal decomposition of the reward.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "dann|adapting_to_reward_progressivity_via_spectral_reinforcement_learning", "supplementary_material": "", "pdf": "/pdf/a3879d188d633661f6593af48f3c561aefbfb6ff.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ndann2021adapting,\ntitle={Adapting to Reward Progressivity via Spectral Reinforcement Learning},\nauthor={Michael Dann and John Thangarajah},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=dyjPVUc2KB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "dyjPVUc2KB", "replyto": "dyjPVUc2KB", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1570/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538115748, "tmdate": 1606915762428, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1570/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1570/-/Official_Review"}}}, {"id": "E46BITZp1PY", "original": null, "number": 4, "cdate": 1603862818928, "ddate": null, "tcdate": 1603862818928, "tmdate": 1605024412576, "tddate": null, "forum": "dyjPVUc2KB", "replyto": "dyjPVUc2KB", "invitation": "ICLR.cc/2021/Conference/Paper1570/-/Official_Review", "content": {"title": "Review of 'Adapting to reward progressivity via spectral RL'", "review": "#######################################################################\n\nSummary:\n\nIn this paper the authors propose a new RL method, spectral DQN, in which rewards are decomposed into different frequencies. This decomposition allow for the training loss to better balanced on certain tasks - in particular those with progressive rewards. The new method is shown to perform well on specially constructed tasks with extreme reward progressively, as well as on a selection of standard Atari tasks.\n\n#######################################################################\n\nReasons for score:\n\nI think a weak accept is appropriate here. I think the authors correctly identify a class of worth while tasks - namely those with progressive rewards, and reasonably establish that standard approaches struggle here. The new method is a strong implementation of a simple idea which is demonstrated to work, and does not require significant fine-tuning.\n\n#######################################################################Pros:\n\n1. The paper is well written and clearly presented. I'd commend the authors on their exposition and balanced motivation throughout \n\n2. I think this is an interesting direction of work more generally - exploring the performance of different methods against different reward distributions, and having agents predict quantities more flexible than the mean return. This would seem to be a contribution to that body of work.\n\n3. I see no reason that the constructed domain ExponentialPong shouldn't join the benchmark for any subsequent general-purpose agent \ud83d\ude42\n\n#######################################################################\n\nCons:\n\n1. While I believe the selected experiments are sufficient to demonstrate the claims in the paper, they do not fully explore the capabilities of the method and the intuitions that motivate it. It would have been good to see some more thinking here (even if it means experiments outside of Atari)\n\n(1) It would seem that this approach would work for a parametrizable class of reward functions - why not test that? Perhaps ExponentialPong with reward $b^(\\alpha N)$ evaluated on a grid for $b$ and $\\alpha$?\n\n(2) Similarly for failure modes\n\n(3) Frequencies needn't be geometric, how might other choices have performed\n\n2. I'd like to have seen a slightly richer discussion/motivation of the mixed Monte Carlo update. Clearly something is necessary here, but why this? What else was tried?\n\n#######################################################################\n\nQuestions during rebuttal period:\n\nQ1: you note the obvious limitation in the selection of the number of reward frequencies, but as you note this is reasonably easily ameliorated in practice. A more interesting question perhaps is whether an adaptive approach for b might work - was this tried?\n\n#######################################################################\n\nSome typos:\n\n(1) 'Expoential Pong' \u2192 'Exponential Pong', just above 4.1'\n\n(2) Fig2, last row, \"Fiilled proportion\" \u2192 'Filed proportion'", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1570/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1570/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adapting to Reward Progressivity via Spectral Reinforcement Learning", "authorids": ["~Michael_Dann1", "john.thangarajah@rmit.edu.au"], "authors": ["Michael Dann", "John Thangarajah"], "keywords": ["Reinforcement Learning", "Deep Reinforcement Learning"], "abstract": "In this paper we consider reinforcement learning tasks with progressive rewards; that is, tasks where the rewards tend to increase in magnitude over time. We hypothesise that this property may be problematic for value-based deep reinforcement learning agents, particularly if the agent must first succeed in relatively unrewarding regions of the task in order to reach more rewarding regions. To address this issue, we propose Spectral DQN, which decomposes the reward into frequencies such that the high frequencies only activate when large rewards are found. This allows the training loss to be balanced so that it gives more even weighting across small and large reward regions. In two domains with extreme reward progressivity, where standard value-based methods struggle significantly, Spectral DQN is able to make much farther progress. Moreover, when evaluated on a set of six standard Atari games that do not overtly favour the approach, Spectral DQN remains more than competitive: While it underperforms one of the benchmarks in a single game, it comfortably surpasses the benchmarks in three games. These results demonstrate that the approach is not overfit to its target problem, and suggest that Spectral DQN may have advantages beyond addressing reward progressivity.", "one-sentence_summary": "In this paper, we identify a problem with value-based deep reinforcement learning that has not previously been investigated -- namely, reward progressivity -- and propose an approach that addresses it via magnitudinal decomposition of the reward.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "dann|adapting_to_reward_progressivity_via_spectral_reinforcement_learning", "supplementary_material": "", "pdf": "/pdf/a3879d188d633661f6593af48f3c561aefbfb6ff.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ndann2021adapting,\ntitle={Adapting to Reward Progressivity via Spectral Reinforcement Learning},\nauthor={Michael Dann and John Thangarajah},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=dyjPVUc2KB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "dyjPVUc2KB", "replyto": "dyjPVUc2KB", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1570/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538115748, "tmdate": 1606915762428, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1570/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1570/-/Official_Review"}}}], "count": 17}