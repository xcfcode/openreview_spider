{"notes": [{"ddate": null, "legacy_migration": true, "tmdate": 1392869340000, "tcdate": 1392869340000, "number": 3, "id": "mL6rEPWYehLYQ", "invitation": "ICLR.cc/2014/-/submission/workshop/review", "forum": "Hy_7-edzrEHx9", "replyto": "Hy_7-edzrEHx9", "signatures": ["Sida Wang"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "My reviewer response. \r\n\r\n\r\nWe thank the reviewers for the comments and questions.\r\n\r\nIssues on using rrr to estimating the partition function: we would agree with both reviewers that rrr is not necessarily good at estimating partition functions, unless the partition function is dominated by the MAP states. In the bipartite RBM case, this requirement is less restrictive since the partition function only needs to be dominated by a MAP visible state summing over hidden, or a MAP hidden state summing over visible units. We thought it is appealing to try rrr for this task since rrr gives us a distribution as well. However, we would only recommend its use for partition function estimation in these specific cases. This should be clarified further in a revision.\r\n\r\n***Reviewer 1 questions***\r\n> Figure 1... Is there any particular reason for this? Also, what would the result be if the Gibbs sampling were performed at different temperatures?\r\n\r\nOne hypothesis is that the RBM with learned weights has negative weights in expectation. So any random deviation from the optimum tend to have worse likelihood, causing the more spread distribution. In the random case, the mean weight is 0, and random derivations only cause variance. However, we do not understand the rounding distribution very well. It could be a hard problem since complexity theory rule out strong generic lower bounds on the variance of the rounding distribution.\r\nAt lower temperatures, Gibbs tend to sample more near the MAP, provided that it still mixes. This is why we would compare to annealed Gibbs in our MAP finding exercise.\r\n\r\n\r\n> Since the optimization problem itself is non-convex, it would seem that the results would also depend on the quality of the solution to the optimization problem. Is there much variance between optimization runs? What about the effect of the rank of X?\r\nWe'd like to note that the full SDP is convex, and can be solved for problems of this size (and we've tried that as well, with no performance difference with local low rank solution). There is much literature with theoretical and empirical evidence supporting that low rank solutions here are suitable and stable. Empirically there is very little variance between runs and using higher rank quickly lead to diminished returns. \r\n\r\n> Compare to Gurobi on small instances, graph-cut cases\r\nrrr usually solve small instances exactly as well. This comparison is a helpful one which we neglected here. Comparing to exact methods in the sub-modular case can also be very helpful, which we also neglected. Thanks for the suggestion.\r\n\r\n\r\n***Reviewer 2 questions***\r\n\r\n>It would be very interesting to try and estimate the partition function of an RBM with many more modes, and to compare it with other methods (such as AIS).\r\nIn some later work, we tried DBM trained on MNIST. Can the reviewer point us to the example with many more modes? We compared to AIS ourselves, the results is that with enough time budget, AIS does better and there exists time budgets under which rrr does better. However, rrr fundamentally does not give an unbiased estimate of the log partition and this comparison was omitted.\r\n\r\n\r\n> The approximation of (14) can be quite bad if p_X is very different from the RBM's distribution:\r\nindeed, and unlike actual, asymptotically unbiased, methods of estimating the partition function. rrr is fundamentally a MAP finding method that can only heuristically estimate the partition function. As figure 1 shows, samples from p_X can indeed be very different.\r\n\r\n> other energy based models, future works?\r\nIn current work, we tried rrr for MRF inference, among them DBNs and we gave some theoretical analysis. Thanks for the contrastive backprop suggestion."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Relaxations for inference in restricted Boltzmann machines", "decision": "submitted, no decision", "abstract": "We propose a relaxation-based approximate inference algorithm that samples near-MAP configurations of a binary pairwise Markov random field. We experiment on MAP inference tasks in several restricted Boltzmann machines. We also use our underlying sampler to estimate the log-partition function of restricted Boltzmann machines and compare against other sampling-based methods.", "pdf": "https://arxiv.org/abs/1312.6205", "paperhash": "wang|relaxations_for_inference_in_restricted_boltzmann_machines", "keywords": [], "conflicts": [], "authors": ["Sida I. Wang", "Roy Frostig", "Percy Liang", "Christopher D. Manning"], "authorids": ["sidaw@cs.stanford.edu", "rf@cs.stanford.edu", "pliang@cs.stanford.edu", "manning@cs.stanford.edu"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391904120000, "tcdate": 1391904120000, "number": 2, "id": "JM28f-ItaBfNF", "invitation": "ICLR.cc/2014/-/submission/workshop/review", "forum": "Hy_7-edzrEHx9", "replyto": "Hy_7-edzrEHx9", "signatures": ["anonymous reviewer e306"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Relaxations for inference in restricted Boltzmann machines", "review": "The paper introduces a gradient procedure for map estimation in MRFs and RBMs, which can also be used to draw approximate samples and therefore estimate partition functions.  \r\n\r\nPros:  the method is very novel in the context of RBMs, and it seems to work quite well, beating Gibbs-sampling almost every time.  It is also useful for estimating partition functions.\r\n\r\nCons:  the method was able to correctly estimate the partition function of an MNIST RBM well.  But MNIST has few modes.  It would be very interesting to try and estimate the partition function of an RBM with many more modes, and to compare it with other methods (such as AIS).\r\n\r\nThe approximation of (14) can be quite bad if p_X is very different from the RBM's distribution. \r\n\r\nI wonder if this method can be applied to general energy-based models, like the ones used in contrastive backpropagation."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Relaxations for inference in restricted Boltzmann machines", "decision": "submitted, no decision", "abstract": "We propose a relaxation-based approximate inference algorithm that samples near-MAP configurations of a binary pairwise Markov random field. We experiment on MAP inference tasks in several restricted Boltzmann machines. We also use our underlying sampler to estimate the log-partition function of restricted Boltzmann machines and compare against other sampling-based methods.", "pdf": "https://arxiv.org/abs/1312.6205", "paperhash": "wang|relaxations_for_inference_in_restricted_boltzmann_machines", "keywords": [], "conflicts": [], "authors": ["Sida I. Wang", "Roy Frostig", "Percy Liang", "Christopher D. Manning"], "authorids": ["sidaw@cs.stanford.edu", "rf@cs.stanford.edu", "pliang@cs.stanford.edu", "manning@cs.stanford.edu"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391848560000, "tcdate": 1391848560000, "number": 1, "id": "eeNW4HiDfE4DT", "invitation": "ICLR.cc/2014/-/submission/workshop/review", "forum": "Hy_7-edzrEHx9", "replyto": "Hy_7-edzrEHx9", "signatures": ["anonymous reviewer caba"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Relaxations for inference in restricted Boltzmann machines", "review": "This paper introduces an approach to finding near-MAP solutions in binary Markov random fields. The proposed technique is based on an SDP relaxation that is re-parameterized and solved using constrained gradient-based methods. The final step involves projecting the solution using a random unit-length vector and then rounding the resulting entries to the vertices of a hypercube. This stochastic process defines a sampler that empirically produces lower-energy configurations than Gibbs sampling. The method is simple and seems to perform well for approximate MAP estimation, although it is not clear whether this approach will be useful for estimating the partition function.\r\n\r\nI liked the result in Figure 1, although the entropy of the rrr-MAP method is much higher in the learned RBM than the one with random weights. Is there any particular reason for this? Also, what would the result be if the Gibbs sampling were performed at different temperatures?\r\n\r\nSince the optimization problem itself is non-convex, it would seem that the results would also depend on the quality of the solution to the optimization problem. Is there much variance between optimization runs? What about the effect of the rank of X?\r\n\r\nI think that the results should also be compared on a small RBM where the exact MAP solution can be found in a reasonable amount of time by Gurobi.\r\n\r\nPerhaps a good test would be on RBMs (or general binary MRFs) with non-negative edge weights. These are submodular and can therefore be globally optimized efficiently. This would serve as a good basis for comparison to other local-search methods."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Relaxations for inference in restricted Boltzmann machines", "decision": "submitted, no decision", "abstract": "We propose a relaxation-based approximate inference algorithm that samples near-MAP configurations of a binary pairwise Markov random field. We experiment on MAP inference tasks in several restricted Boltzmann machines. We also use our underlying sampler to estimate the log-partition function of restricted Boltzmann machines and compare against other sampling-based methods.", "pdf": "https://arxiv.org/abs/1312.6205", "paperhash": "wang|relaxations_for_inference_in_restricted_boltzmann_machines", "keywords": [], "conflicts": [], "authors": ["Sida I. Wang", "Roy Frostig", "Percy Liang", "Christopher D. Manning"], "authorids": ["sidaw@cs.stanford.edu", "rf@cs.stanford.edu", "pliang@cs.stanford.edu", "manning@cs.stanford.edu"]}, "tags": [], "invitation": {}}}, {"replyto": null, "ddate": null, "legacy_migration": true, "tmdate": 1387877280000, "tcdate": 1387877280000, "number": 14, "id": "Hy_7-edzrEHx9", "invitation": "ICLR.cc/2014/workshop/-/submission", "forum": "Hy_7-edzrEHx9", "signatures": ["sidaw@cs.stanford.edu"], "readers": ["everyone"], "content": {"title": "Relaxations for inference in restricted Boltzmann machines", "decision": "submitted, no decision", "abstract": "We propose a relaxation-based approximate inference algorithm that samples near-MAP configurations of a binary pairwise Markov random field. We experiment on MAP inference tasks in several restricted Boltzmann machines. We also use our underlying sampler to estimate the log-partition function of restricted Boltzmann machines and compare against other sampling-based methods.", "pdf": "https://arxiv.org/abs/1312.6205", "paperhash": "wang|relaxations_for_inference_in_restricted_boltzmann_machines", "keywords": [], "conflicts": [], "authors": ["Sida I. Wang", "Roy Frostig", "Percy Liang", "Christopher D. Manning"], "authorids": ["sidaw@cs.stanford.edu", "rf@cs.stanford.edu", "pliang@cs.stanford.edu", "manning@cs.stanford.edu"]}, "writers": [], "details": {"replyCount": 3, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1369422751717, "tmdate": 1496674357014, "id": "ICLR.cc/2014/workshop/-/submission", "writers": ["ICLR.cc/2014"], "signatures": ["OpenReview.net"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": []}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1377198751717, "cdate": 1496674357014}}}], "count": 4}