{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396569380, "tcdate": 1486396569380, "number": 1, "id": "ry-shzUde", "invitation": "ICLR.cc/2017/conference/-/paper415/acceptance", "forum": "Hk4_qw5xe", "replyto": "Hk4_qw5xe", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "The paper provides a detailed analysis of the instability issues surrounding the training of GANs. They demonstrate how perturbations can help with improving stability. Given the popularity of GANs, this paper is expected to have a significant impact.", "decision": "Accept (Oral)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards Principled Methods for Training Generative Adversarial Networks", "abstract": "The goal of this paper is not to introduce a single algorithm or method, but to make theoretical steps towards fully understanding the training dynamics of gen- erative adversarial networks. In order to substantiate our theoretical analysis, we perform targeted experiments to verify our assumptions, illustrate our claims, and quantify the phenomena. This paper is divided into three sections. The first sec- tion introduces the problem at hand. The second section is dedicated to studying and proving rigorously the problems including instability and saturation that arize when training generative adversarial networks. The third section examines a prac- tical and theoretically grounded direction towards solving these problems, while introducing new tools to study them.", "pdf": "/pdf/a39d6b9d0e4f4746a02732368a9fa08d458f0a45.pdf", "TL;DR": "We introduce a theory about generative adversarial networks and their issues.", "paperhash": "arjovsky|towards_principled_methods_for_training_generative_adversarial_networks", "keywords": [], "conflicts": ["nyu.edu", "fb.com", "umontreal.edu"], "authors": ["Martin Arjovsky", "Leon Bottou"], "authorids": ["martinarjovsky@gmail.com", "leonb@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396569963, "id": "ICLR.cc/2017/conference/-/paper415/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "Hk4_qw5xe", "replyto": "Hk4_qw5xe", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396569963}}}, {"tddate": null, "tmdate": 1486088014737, "tcdate": 1486088014737, "number": 20, "id": "B1wIvPZue", "invitation": "ICLR.cc/2017/conference/-/paper415/public/comment", "forum": "Hk4_qw5xe", "replyto": "Sk73OCuDx", "signatures": ["~Martin_Arjovsky1"], "readers": ["everyone"], "writers": ["~Martin_Arjovsky1"], "content": {"title": "Comment", "comment": "Thanks for the comments! I think the two questions you raise are the most important questions in this line of research, and they are not easy. Ideally one would start with a high noise, and decrease it, trying to make the discriminator's loss or accuracy fairly constant. However in my findings it's pretty hard to make D converge to chance, even that's the optimal discriminator. If one at such points \"oh, the disc error is high, then let's decrease the noise\" there appears to be a phase-transition behaviour when sometimes the discriminator suddenly has 1 accuracy and becomes perfect quickly. At the core of this problem, there's the issue of testing when the discriminator has converged (this is an important problem, that also appears in our newer paper).\n\nA few ideas on this line:\n - Train the disc until it's loss stops decreasing. This is tricky if we only train the disc for a few iterations for every generator iteration, since the loss is noisy. This actually works reasonably well with running averages.\n - Train until the gradient of the discriminator is small on a large batch (this is hard and slow, and might even require learning rate decay), or until the running average of the gradient norms is roughly constant (this number should converge to the variance of the gradients). This is reasonable, but in ~ 5 iterations it's pretty hard to say if convergence has appeared.\n - Never stop training the disc if it's error is much higher than in the last iteration (this can happen because of momentum, and while looking at curves it's easy to detect, it can be trickier to detect automatically).\n\nThe problem of testing convergence is an old one, and it's likely we'll be able to crack it soon as a community, but we aren't there just yet in my opinion. It's a matter of time, of people trying a few ideas that make sense, and we'll likely get there :)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards Principled Methods for Training Generative Adversarial Networks", "abstract": "The goal of this paper is not to introduce a single algorithm or method, but to make theoretical steps towards fully understanding the training dynamics of gen- erative adversarial networks. In order to substantiate our theoretical analysis, we perform targeted experiments to verify our assumptions, illustrate our claims, and quantify the phenomena. This paper is divided into three sections. The first sec- tion introduces the problem at hand. The second section is dedicated to studying and proving rigorously the problems including instability and saturation that arize when training generative adversarial networks. The third section examines a prac- tical and theoretically grounded direction towards solving these problems, while introducing new tools to study them.", "pdf": "/pdf/a39d6b9d0e4f4746a02732368a9fa08d458f0a45.pdf", "TL;DR": "We introduce a theory about generative adversarial networks and their issues.", "paperhash": "arjovsky|towards_principled_methods_for_training_generative_adversarial_networks", "keywords": [], "conflicts": ["nyu.edu", "fb.com", "umontreal.edu"], "authors": ["Martin Arjovsky", "Leon Bottou"], "authorids": ["martinarjovsky@gmail.com", "leonb@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287587462, "id": "ICLR.cc/2017/conference/-/paper415/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Hk4_qw5xe", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper415/reviewers", "ICLR.cc/2017/conference/paper415/areachairs"], "cdate": 1485287587462}}}, {"tddate": null, "tmdate": 1486010525118, "tcdate": 1486010525118, "number": 19, "id": "S1Hid4eOx", "invitation": "ICLR.cc/2017/conference/-/paper415/public/comment", "forum": "Hk4_qw5xe", "replyto": "BJHLvNgOx", "signatures": ["~Martin_Arjovsky1"], "readers": ["everyone"], "writers": ["~Martin_Arjovsky1"], "content": {"title": "Theorem 2.1", "comment": "Hi! You could do that. We decided to thicken M and P and use Urysohn on M^\\hat and P^\\hat so that D* would be locally constant on M and P (otherwise this wouldn't necessarily be true in the boundary). Thanks for the typo spot!"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards Principled Methods for Training Generative Adversarial Networks", "abstract": "The goal of this paper is not to introduce a single algorithm or method, but to make theoretical steps towards fully understanding the training dynamics of gen- erative adversarial networks. In order to substantiate our theoretical analysis, we perform targeted experiments to verify our assumptions, illustrate our claims, and quantify the phenomena. This paper is divided into three sections. The first sec- tion introduces the problem at hand. The second section is dedicated to studying and proving rigorously the problems including instability and saturation that arize when training generative adversarial networks. The third section examines a prac- tical and theoretically grounded direction towards solving these problems, while introducing new tools to study them.", "pdf": "/pdf/a39d6b9d0e4f4746a02732368a9fa08d458f0a45.pdf", "TL;DR": "We introduce a theory about generative adversarial networks and their issues.", "paperhash": "arjovsky|towards_principled_methods_for_training_generative_adversarial_networks", "keywords": [], "conflicts": ["nyu.edu", "fb.com", "umontreal.edu"], "authors": ["Martin Arjovsky", "Leon Bottou"], "authorids": ["martinarjovsky@gmail.com", "leonb@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287587462, "id": "ICLR.cc/2017/conference/-/paper415/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Hk4_qw5xe", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper415/reviewers", "ICLR.cc/2017/conference/paper415/areachairs"], "cdate": 1485287587462}}}, {"tddate": null, "tmdate": 1486010189083, "tcdate": 1486010189083, "number": 18, "id": "BJHLvNgOx", "invitation": "ICLR.cc/2017/conference/-/paper415/public/comment", "forum": "Hk4_qw5xe", "replyto": "Hk4_qw5xe", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "Theorem 2.1", "comment": "I am a bit confused with your proof of Theorem 2.1: shouldn't it be that the Urysohn's lemma imply that D* is 1 on M and 0 on P (not on M^{hat} and P^{hat})?\nAlso small typo in the formulation: discrimator -> discriminator. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards Principled Methods for Training Generative Adversarial Networks", "abstract": "The goal of this paper is not to introduce a single algorithm or method, but to make theoretical steps towards fully understanding the training dynamics of gen- erative adversarial networks. In order to substantiate our theoretical analysis, we perform targeted experiments to verify our assumptions, illustrate our claims, and quantify the phenomena. This paper is divided into three sections. The first sec- tion introduces the problem at hand. The second section is dedicated to studying and proving rigorously the problems including instability and saturation that arize when training generative adversarial networks. The third section examines a prac- tical and theoretically grounded direction towards solving these problems, while introducing new tools to study them.", "pdf": "/pdf/a39d6b9d0e4f4746a02732368a9fa08d458f0a45.pdf", "TL;DR": "We introduce a theory about generative adversarial networks and their issues.", "paperhash": "arjovsky|towards_principled_methods_for_training_generative_adversarial_networks", "keywords": [], "conflicts": ["nyu.edu", "fb.com", "umontreal.edu"], "authors": ["Martin Arjovsky", "Leon Bottou"], "authorids": ["martinarjovsky@gmail.com", "leonb@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287587462, "id": "ICLR.cc/2017/conference/-/paper415/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Hk4_qw5xe", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper415/reviewers", "ICLR.cc/2017/conference/paper415/areachairs"], "cdate": 1485287587462}}}, {"tddate": null, "tmdate": 1485527210869, "tcdate": 1485527210869, "number": 17, "id": "Sk73OCuDx", "invitation": "ICLR.cc/2017/conference/-/paper415/public/comment", "forum": "Hk4_qw5xe", "replyto": "Hk4_qw5xe", "signatures": ["~Arnaud_Sors1"], "readers": ["everyone"], "writers": ["~Arnaud_Sors1"], "content": {"title": "high quality paper", "comment": "Hi!\nI think this is an outstanding paper. \nIt has greatly helped me towards understanding where failure modes of GANs come from, and I think it will also have great practical implications on how to train GANs. \n\nFollowing your ideas, I have played with added noise on GANs. I am able to get the generator to yield 'good looking' samples in much less G iterations than in the standard setting. Also, it appears that 'successful' training is a LOT less sensitive to the setting of hyperparameters. For example I am able to use 10 times higher learning rates, forget about gradient clipping, etc... so this is great! I think this paper paves the way to further research on the following points:\n- how to choose noise variance and schedule its decrease over training ?\n- what does 'training D to convergence' mean ? In practice it seems that training D 'more than G' but only a few steps is already sufficient. Can we get a theoretical understanding of this...\n\nAlso, in my (quick and dirty) first experiments, using instance noise helped training in less G iterations, but the mode dropping problem remained. In my understanding, your theoretical analysis demonstrates that the addition of noise helps D provide gradients to G so that at anytime, G is able to escape its current modes. However, and due to the fact that D outputs are calculated from single examples there is no 'coverage guarantee', for example G could keep switching between different modes. Is this right ? Minibatch discrimination appears to be a good first way to answer this, although not 100% satisfactory because the metric is combinatorial... What is your view on these practical considerations ?\n\nMany thanks !\n\nArnaud Sors"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards Principled Methods for Training Generative Adversarial Networks", "abstract": "The goal of this paper is not to introduce a single algorithm or method, but to make theoretical steps towards fully understanding the training dynamics of gen- erative adversarial networks. In order to substantiate our theoretical analysis, we perform targeted experiments to verify our assumptions, illustrate our claims, and quantify the phenomena. This paper is divided into three sections. The first sec- tion introduces the problem at hand. The second section is dedicated to studying and proving rigorously the problems including instability and saturation that arize when training generative adversarial networks. The third section examines a prac- tical and theoretically grounded direction towards solving these problems, while introducing new tools to study them.", "pdf": "/pdf/a39d6b9d0e4f4746a02732368a9fa08d458f0a45.pdf", "TL;DR": "We introduce a theory about generative adversarial networks and their issues.", "paperhash": "arjovsky|towards_principled_methods_for_training_generative_adversarial_networks", "keywords": [], "conflicts": ["nyu.edu", "fb.com", "umontreal.edu"], "authors": ["Martin Arjovsky", "Leon Bottou"], "authorids": ["martinarjovsky@gmail.com", "leonb@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287587462, "id": "ICLR.cc/2017/conference/-/paper415/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Hk4_qw5xe", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper415/reviewers", "ICLR.cc/2017/conference/paper415/areachairs"], "cdate": 1485287587462}}}, {"tddate": null, "tmdate": 1484929090701, "tcdate": 1484929090701, "number": 1, "id": "HksH_3yPl", "invitation": "ICLR.cc/2017/conference/-/paper415/official/comment", "forum": "Hk4_qw5xe", "replyto": "Hk4_qw5xe", "signatures": ["ICLR.cc/2017/conference/paper415/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper415/AnonReviewer2"], "content": {"title": "final evaluation - strong submission", "comment": "Very good paper, I hope it will be accepted. I keep my original evaluation."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards Principled Methods for Training Generative Adversarial Networks", "abstract": "The goal of this paper is not to introduce a single algorithm or method, but to make theoretical steps towards fully understanding the training dynamics of gen- erative adversarial networks. In order to substantiate our theoretical analysis, we perform targeted experiments to verify our assumptions, illustrate our claims, and quantify the phenomena. This paper is divided into three sections. The first sec- tion introduces the problem at hand. The second section is dedicated to studying and proving rigorously the problems including instability and saturation that arize when training generative adversarial networks. The third section examines a prac- tical and theoretically grounded direction towards solving these problems, while introducing new tools to study them.", "pdf": "/pdf/a39d6b9d0e4f4746a02732368a9fa08d458f0a45.pdf", "TL;DR": "We introduce a theory about generative adversarial networks and their issues.", "paperhash": "arjovsky|towards_principled_methods_for_training_generative_adversarial_networks", "keywords": [], "conflicts": ["nyu.edu", "fb.com", "umontreal.edu"], "authors": ["Martin Arjovsky", "Leon Bottou"], "authorids": ["martinarjovsky@gmail.com", "leonb@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287587336, "id": "ICLR.cc/2017/conference/-/paper415/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "Hk4_qw5xe", "writers": {"values-regex": "ICLR.cc/2017/conference/paper415/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper415/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper415/reviewers", "ICLR.cc/2017/conference/paper415/areachairs"], "cdate": 1485287587336}}}, {"tddate": null, "tmdate": 1484245733580, "tcdate": 1484245733580, "number": 16, "id": "Sy0kjBBLx", "invitation": "ICLR.cc/2017/conference/-/paper415/public/comment", "forum": "Hk4_qw5xe", "replyto": "Sy9pZrWSx", "signatures": ["~Martin_Arjovsky1"], "readers": ["everyone"], "writers": ["~Martin_Arjovsky1"], "content": {"title": "Last paragraph", "comment": "Thanks for the comments!\n\nThe alternative training method definitely seems like a good direction, and we are already starting to see empirical evidence in other groups using it successfully (e.g: https://openreview.net/pdf?id=S1RP6GLle in image super resolution and http://mrdrozdov.com/papers/infogan.pdf in infogan variants). That being said, the variance of the noise can be important, since too little noise will likely not be enough to ameliorate all issues, and too much noise will likely lead to high variance of the gradients. The structure of the noise can be quite important as well, since the covariance matrix of images is quite complex (even if it is known), so N(0, sigma * I) noise is probably not the best we can do. We'd like to get a more complete understanding of how different types of noise behave, and in the end hopefully finish the story by properly stabilizing GANs, but due to time constraints and the paper already being quite long we chose to keep this for further research. This way we also give the possibility to other researchers to start experimenting on this issues (with some understanding) right away, which we in the end considered more valuable.\n\nWe hope to have new positive results very soon (earlier than the ICML deadline).\n\nIf you have any more questions or comments please let us know!\nBest,\nMartin"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards Principled Methods for Training Generative Adversarial Networks", "abstract": "The goal of this paper is not to introduce a single algorithm or method, but to make theoretical steps towards fully understanding the training dynamics of gen- erative adversarial networks. In order to substantiate our theoretical analysis, we perform targeted experiments to verify our assumptions, illustrate our claims, and quantify the phenomena. This paper is divided into three sections. The first sec- tion introduces the problem at hand. The second section is dedicated to studying and proving rigorously the problems including instability and saturation that arize when training generative adversarial networks. The third section examines a prac- tical and theoretically grounded direction towards solving these problems, while introducing new tools to study them.", "pdf": "/pdf/a39d6b9d0e4f4746a02732368a9fa08d458f0a45.pdf", "TL;DR": "We introduce a theory about generative adversarial networks and their issues.", "paperhash": "arjovsky|towards_principled_methods_for_training_generative_adversarial_networks", "keywords": [], "conflicts": ["nyu.edu", "fb.com", "umontreal.edu"], "authors": ["Martin Arjovsky", "Leon Bottou"], "authorids": ["martinarjovsky@gmail.com", "leonb@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287587462, "id": "ICLR.cc/2017/conference/-/paper415/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Hk4_qw5xe", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper415/reviewers", "ICLR.cc/2017/conference/paper415/areachairs"], "cdate": 1485287587462}}}, {"tddate": null, "tmdate": 1484245495258, "tcdate": 1484245495258, "number": 15, "id": "H1y-9rrUg", "invitation": "ICLR.cc/2017/conference/-/paper415/public/comment", "forum": "Hk4_qw5xe", "replyto": "SycA8eLNg", "signatures": ["~Martin_Arjovsky1"], "readers": ["everyone"], "writers": ["~Martin_Arjovsky1"], "content": {"title": "Review", "comment": "Thanks for the review! The perturbation trick definitely seems like a good direction, and we are already starting to see empirical evidence in other groups using it successfully (e.g: https://openreview.net/pdf?id=S1RP6GLle in image super resolution and http://mrdrozdov.com/papers/infogan.pdf in infogan variants). That being said, the variance of the noise can be important, since too little noise will likely not be enough to ameliorate all issues, and too much noise will likely lead to high variance of the gradients. The structure of the noise can be quite important as well, since the covariance matrix of images is quite complex (even if it is known), so N(0, sigma * I) noise is probably not the best we can do. We'd like to get a more complete understanding of how different types of noise behave, and in the end finish the story by properly stabilizing GANs, but due to time constraints and the paper already being quite long we chose to keep this for further research. This way we also give the possibility to other researchers to start experimenting on this issues (with some understanding) right away, which we in the end considered more valuable.\n\nThe connection with dropout seems quite interesting! In fact, adding noise at higher layers (similar to dropout) could be quite a good idea, since as the factors of variations get disentangled, a more simple noise will have the right metric structure on hidden unit space. We'll be glad to continue exploring this direction in future work :)\n\nIf you have more questions or comments please let us know!\nBest,\nMartin"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards Principled Methods for Training Generative Adversarial Networks", "abstract": "The goal of this paper is not to introduce a single algorithm or method, but to make theoretical steps towards fully understanding the training dynamics of gen- erative adversarial networks. In order to substantiate our theoretical analysis, we perform targeted experiments to verify our assumptions, illustrate our claims, and quantify the phenomena. This paper is divided into three sections. The first sec- tion introduces the problem at hand. The second section is dedicated to studying and proving rigorously the problems including instability and saturation that arize when training generative adversarial networks. The third section examines a prac- tical and theoretically grounded direction towards solving these problems, while introducing new tools to study them.", "pdf": "/pdf/a39d6b9d0e4f4746a02732368a9fa08d458f0a45.pdf", "TL;DR": "We introduce a theory about generative adversarial networks and their issues.", "paperhash": "arjovsky|towards_principled_methods_for_training_generative_adversarial_networks", "keywords": [], "conflicts": ["nyu.edu", "fb.com", "umontreal.edu"], "authors": ["Martin Arjovsky", "Leon Bottou"], "authorids": ["martinarjovsky@gmail.com", "leonb@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287587462, "id": "ICLR.cc/2017/conference/-/paper415/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Hk4_qw5xe", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper415/reviewers", "ICLR.cc/2017/conference/paper415/areachairs"], "cdate": 1485287587462}}}, {"tddate": null, "tmdate": 1484244482091, "tcdate": 1484244482091, "number": 14, "id": "rk9-8HS8x", "invitation": "ICLR.cc/2017/conference/-/paper415/public/comment", "forum": "Hk4_qw5xe", "replyto": "S19DFCSNe", "signatures": ["~Martin_Arjovsky1"], "readers": ["everyone"], "writers": ["~Martin_Arjovsky1"], "content": {"title": "Next steps", "comment": "Here's a few questions and problems that we think are important to be studied (and hopefully, tackled!)\n\n- We have yet to characterize what the distances between distributions with a suboptimal training of the discriminator behave like in a precise mathematical sense. It is clear that GANs do something if the discriminator is far from optimal (indeed, our paper shows that we cannot learn from an optimal one in the current setting, which is what causes most of the problems we have now). Intuitively this behaves as a smoothened very imperfect version of the JSD or the reverse KL, depending on which update is used, but how this smoothening process works and what properties it imposes to the learned generator is far from being understood.\n\n- How these divergences behave in comparison to other distances such as Wasserstein ones, which are properly defined and well behaved even on the manifold case.\n\n- Wasserstein is symmetric so we cannot in principle tune it to care more about mode dropping or generating plausible samples (as we can do with the KLs), so figuring out weak analogs to the f-divergences would be massively interesting.\n\n- The terms of generating plausible samples and mode dropping still need to be precisely understood in a mathematical setting that covers the manifold case. I think the comments of section 5.6 of https://arxiv.org/abs/1610.04490 is a very important starting point towards asking the right questions.\n\n- Finding out which is the optimal variance or form for the added noise. It is likely that too little noise won\u2019t be enough to fully ameliorate these issues, and too high noise will make the variance of the gradients too high, and give too little signal. Trying to define the problem for example of finding the optimal variance according to some quantitative estimate (such as making a bound like Theorem 3.3 the tightest possible, or minimizing the variance of the gradients) would be quite interesting.\n\n- I'd like to get a general statement of the form of Lemma 1, ideally with necessary and sufficient conditions.\n\nWe are considering adding these items to an appendix C as possible further directions of research, do you think this would be valuable to the readers?\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards Principled Methods for Training Generative Adversarial Networks", "abstract": "The goal of this paper is not to introduce a single algorithm or method, but to make theoretical steps towards fully understanding the training dynamics of gen- erative adversarial networks. In order to substantiate our theoretical analysis, we perform targeted experiments to verify our assumptions, illustrate our claims, and quantify the phenomena. This paper is divided into three sections. The first sec- tion introduces the problem at hand. The second section is dedicated to studying and proving rigorously the problems including instability and saturation that arize when training generative adversarial networks. The third section examines a prac- tical and theoretically grounded direction towards solving these problems, while introducing new tools to study them.", "pdf": "/pdf/a39d6b9d0e4f4746a02732368a9fa08d458f0a45.pdf", "TL;DR": "We introduce a theory about generative adversarial networks and their issues.", "paperhash": "arjovsky|towards_principled_methods_for_training_generative_adversarial_networks", "keywords": [], "conflicts": ["nyu.edu", "fb.com", "umontreal.edu"], "authors": ["Martin Arjovsky", "Leon Bottou"], "authorids": ["martinarjovsky@gmail.com", "leonb@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287587462, "id": "ICLR.cc/2017/conference/-/paper415/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Hk4_qw5xe", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper415/reviewers", "ICLR.cc/2017/conference/paper415/areachairs"], "cdate": 1485287587462}}}, {"tddate": null, "tmdate": 1484244242623, "tcdate": 1484244242623, "number": 13, "id": "r1iGHBHUe", "invitation": "ICLR.cc/2017/conference/-/paper415/public/comment", "forum": "Hk4_qw5xe", "replyto": "SyexY0HNe", "signatures": ["~Martin_Arjovsky1"], "readers": ["everyone"], "writers": ["~Martin_Arjovsky1"], "content": {"title": "Review", "comment": "Thank you for the amazing review!!! If you have any questions please send them our way and we'll be glad to answer them :)\n\nBest!\nMartin"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards Principled Methods for Training Generative Adversarial Networks", "abstract": "The goal of this paper is not to introduce a single algorithm or method, but to make theoretical steps towards fully understanding the training dynamics of gen- erative adversarial networks. In order to substantiate our theoretical analysis, we perform targeted experiments to verify our assumptions, illustrate our claims, and quantify the phenomena. This paper is divided into three sections. The first sec- tion introduces the problem at hand. The second section is dedicated to studying and proving rigorously the problems including instability and saturation that arize when training generative adversarial networks. The third section examines a prac- tical and theoretically grounded direction towards solving these problems, while introducing new tools to study them.", "pdf": "/pdf/a39d6b9d0e4f4746a02732368a9fa08d458f0a45.pdf", "TL;DR": "We introduce a theory about generative adversarial networks and their issues.", "paperhash": "arjovsky|towards_principled_methods_for_training_generative_adversarial_networks", "keywords": [], "conflicts": ["nyu.edu", "fb.com", "umontreal.edu"], "authors": ["Martin Arjovsky", "Leon Bottou"], "authorids": ["martinarjovsky@gmail.com", "leonb@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287587462, "id": "ICLR.cc/2017/conference/-/paper415/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Hk4_qw5xe", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper415/reviewers", "ICLR.cc/2017/conference/paper415/areachairs"], "cdate": 1485287587462}}}, {"tddate": null, "tmdate": 1484244200308, "tcdate": 1484244200308, "number": 12, "id": "B1gerSrLl", "invitation": "ICLR.cc/2017/conference/-/paper415/public/comment", "forum": "Hk4_qw5xe", "replyto": "HkfifhHNx", "signatures": ["~Martin_Arjovsky1"], "readers": ["everyone"], "writers": ["~Martin_Arjovsky1"], "content": {"title": "Review", "comment": "Thanks for the review! We agree with everything said. We did however choose to leave some of the proofs in the main text. We think it may be helpful for people that don't have such a strong analytical background to get acquainted with the ideas so that it doesn't appear as a black box result.\n\n- We extended the definitions in section 2 and the statements and proofs of Lemmas 2 and 3 to cover manifolds with and without boundary.\n\n- We would love to get a more general version of Lemma 1, but we couldn't come up with one. If you have any pointers on directions or a reference to look into that would be super helpful!\n\n- We fixed all the typos and the numbering of the theorems :)\n\nIf you have any more suggestions or questions please let us know!\n\nBest,\nMartin"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards Principled Methods for Training Generative Adversarial Networks", "abstract": "The goal of this paper is not to introduce a single algorithm or method, but to make theoretical steps towards fully understanding the training dynamics of gen- erative adversarial networks. In order to substantiate our theoretical analysis, we perform targeted experiments to verify our assumptions, illustrate our claims, and quantify the phenomena. This paper is divided into three sections. The first sec- tion introduces the problem at hand. The second section is dedicated to studying and proving rigorously the problems including instability and saturation that arize when training generative adversarial networks. The third section examines a prac- tical and theoretically grounded direction towards solving these problems, while introducing new tools to study them.", "pdf": "/pdf/a39d6b9d0e4f4746a02732368a9fa08d458f0a45.pdf", "TL;DR": "We introduce a theory about generative adversarial networks and their issues.", "paperhash": "arjovsky|towards_principled_methods_for_training_generative_adversarial_networks", "keywords": [], "conflicts": ["nyu.edu", "fb.com", "umontreal.edu"], "authors": ["Martin Arjovsky", "Leon Bottou"], "authorids": ["martinarjovsky@gmail.com", "leonb@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287587462, "id": "ICLR.cc/2017/conference/-/paper415/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Hk4_qw5xe", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper415/reviewers", "ICLR.cc/2017/conference/paper415/areachairs"], "cdate": 1485287587462}}}, {"tddate": null, "tmdate": 1484243655799, "tcdate": 1484243655799, "number": 11, "id": "HJxRzBS8g", "invitation": "ICLR.cc/2017/conference/-/paper415/public/comment", "forum": "Hk4_qw5xe", "replyto": "HJk0HO5He", "signatures": ["~Martin_Arjovsky1"], "readers": ["everyone"], "writers": ["~Martin_Arjovsky1"], "content": {"title": "Comments", "comment": "Thanks for the suggestion! We ended up adding a comment in the appendix, hopefully it will help other readers :)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards Principled Methods for Training Generative Adversarial Networks", "abstract": "The goal of this paper is not to introduce a single algorithm or method, but to make theoretical steps towards fully understanding the training dynamics of gen- erative adversarial networks. In order to substantiate our theoretical analysis, we perform targeted experiments to verify our assumptions, illustrate our claims, and quantify the phenomena. This paper is divided into three sections. The first sec- tion introduces the problem at hand. The second section is dedicated to studying and proving rigorously the problems including instability and saturation that arize when training generative adversarial networks. The third section examines a prac- tical and theoretically grounded direction towards solving these problems, while introducing new tools to study them.", "pdf": "/pdf/a39d6b9d0e4f4746a02732368a9fa08d458f0a45.pdf", "TL;DR": "We introduce a theory about generative adversarial networks and their issues.", "paperhash": "arjovsky|towards_principled_methods_for_training_generative_adversarial_networks", "keywords": [], "conflicts": ["nyu.edu", "fb.com", "umontreal.edu"], "authors": ["Martin Arjovsky", "Leon Bottou"], "authorids": ["martinarjovsky@gmail.com", "leonb@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287587462, "id": "ICLR.cc/2017/conference/-/paper415/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Hk4_qw5xe", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper415/reviewers", "ICLR.cc/2017/conference/paper415/areachairs"], "cdate": 1485287587462}}}, {"tddate": null, "tmdate": 1484243568256, "tcdate": 1484243568256, "number": 10, "id": "B1udfHrLe", "invitation": "ICLR.cc/2017/conference/-/paper415/public/comment", "forum": "Hk4_qw5xe", "replyto": "Hk4_qw5xe", "signatures": ["~Martin_Arjovsky1"], "readers": ["everyone"], "writers": ["~Martin_Arjovsky1"], "content": {"title": "New version", "comment": "Hi! We would like to comment that we added a revision with the following changes at its core:\n\n- We extended the proofs and definitions in section 2 to work with manifolds with boundary.\n\n- We included a one page Appendix B with further clarifications for the things that were suggested in the comments, such as a small comment on continuity vs absolute continuity of random variables, and how it is relevant to our paper.\n\n- We did some minor rewriting to take into account the suggestions from the reviewers and commenters.\n\n- We fixed all the typos :)\n\nBest!\nMartin"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards Principled Methods for Training Generative Adversarial Networks", "abstract": "The goal of this paper is not to introduce a single algorithm or method, but to make theoretical steps towards fully understanding the training dynamics of gen- erative adversarial networks. In order to substantiate our theoretical analysis, we perform targeted experiments to verify our assumptions, illustrate our claims, and quantify the phenomena. This paper is divided into three sections. The first sec- tion introduces the problem at hand. The second section is dedicated to studying and proving rigorously the problems including instability and saturation that arize when training generative adversarial networks. The third section examines a prac- tical and theoretically grounded direction towards solving these problems, while introducing new tools to study them.", "pdf": "/pdf/a39d6b9d0e4f4746a02732368a9fa08d458f0a45.pdf", "TL;DR": "We introduce a theory about generative adversarial networks and their issues.", "paperhash": "arjovsky|towards_principled_methods_for_training_generative_adversarial_networks", "keywords": [], "conflicts": ["nyu.edu", "fb.com", "umontreal.edu"], "authors": ["Martin Arjovsky", "Leon Bottou"], "authorids": ["martinarjovsky@gmail.com", "leonb@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287587462, "id": "ICLR.cc/2017/conference/-/paper415/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Hk4_qw5xe", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper415/reviewers", "ICLR.cc/2017/conference/paper415/areachairs"], "cdate": 1485287587462}}}, {"tddate": null, "replyto": null, "ddate": null, "cdate": null, "tmdate": 1484243230115, "tcdate": 1478290027892, "number": 415, "id": "Hk4_qw5xe", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "Hk4_qw5xe", "signatures": ["~Martin_Arjovsky1"], "readers": ["everyone"], "content": {"title": "Towards Principled Methods for Training Generative Adversarial Networks", "abstract": "The goal of this paper is not to introduce a single algorithm or method, but to make theoretical steps towards fully understanding the training dynamics of gen- erative adversarial networks. In order to substantiate our theoretical analysis, we perform targeted experiments to verify our assumptions, illustrate our claims, and quantify the phenomena. This paper is divided into three sections. The first sec- tion introduces the problem at hand. The second section is dedicated to studying and proving rigorously the problems including instability and saturation that arize when training generative adversarial networks. The third section examines a prac- tical and theoretically grounded direction towards solving these problems, while introducing new tools to study them.", "pdf": "/pdf/a39d6b9d0e4f4746a02732368a9fa08d458f0a45.pdf", "TL;DR": "We introduce a theory about generative adversarial networks and their issues.", "paperhash": "arjovsky|towards_principled_methods_for_training_generative_adversarial_networks", "keywords": [], "conflicts": ["nyu.edu", "fb.com", "umontreal.edu"], "authors": ["Martin Arjovsky", "Leon Bottou"], "authorids": ["martinarjovsky@gmail.com", "leonb@fb.com"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 28, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "tmdate": 1483535814640, "tcdate": 1483535814640, "number": 9, "id": "HJk0HO5He", "invitation": "ICLR.cc/2017/conference/-/paper415/public/comment", "forum": "Hk4_qw5xe", "replyto": "H19j1Etrx", "signatures": ["~Michael_Mathieu1"], "readers": ["everyone"], "writers": ["~Michael_Mathieu1"], "content": {"title": "Answer to comments", "comment": "Thank you for your answer. It makes sense to me. If I may, I would suggest you add a short reminder (in the appendix?) about absolute continuity of random variables, since as you wrote, the \"absolutely\" is often (incorrectly) omitted in the literature, and the distinction is indeed important in this context."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards Principled Methods for Training Generative Adversarial Networks", "abstract": "The goal of this paper is not to introduce a single algorithm or method, but to make theoretical steps towards fully understanding the training dynamics of gen- erative adversarial networks. In order to substantiate our theoretical analysis, we perform targeted experiments to verify our assumptions, illustrate our claims, and quantify the phenomena. This paper is divided into three sections. The first sec- tion introduces the problem at hand. The second section is dedicated to studying and proving rigorously the problems including instability and saturation that arize when training generative adversarial networks. The third section examines a prac- tical and theoretically grounded direction towards solving these problems, while introducing new tools to study them.", "pdf": "/pdf/a39d6b9d0e4f4746a02732368a9fa08d458f0a45.pdf", "TL;DR": "We introduce a theory about generative adversarial networks and their issues.", "paperhash": "arjovsky|towards_principled_methods_for_training_generative_adversarial_networks", "keywords": [], "conflicts": ["nyu.edu", "fb.com", "umontreal.edu"], "authors": ["Martin Arjovsky", "Leon Bottou"], "authorids": ["martinarjovsky@gmail.com", "leonb@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287587462, "id": "ICLR.cc/2017/conference/-/paper415/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Hk4_qw5xe", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper415/reviewers", "ICLR.cc/2017/conference/paper415/areachairs"], "cdate": 1485287587462}}}, {"tddate": null, "tmdate": 1483452321940, "tcdate": 1483452321940, "number": 8, "id": "H19j1Etrx", "invitation": "ICLR.cc/2017/conference/-/paper415/public/comment", "forum": "Hk4_qw5xe", "replyto": "SyXzDmYSe", "signatures": ["~Martin_Arjovsky1"], "readers": ["everyone"], "writers": ["~Martin_Arjovsky1"], "content": {"title": "Comments", "comment": "Hi! Thanks a lot for the feedback, it really helps a lot towards making a better paper. We will address the comments in the revision but in the meantime let me answer the questions in here :)\n\n- The tangent space of a point around the whole space for all practical purposes can be identified with the whole space itself (and each member of the tangent space is identified to a point of the space). You can consider points in the tangent space around a point x as vectors pointing from x, thus you can think the whole tangent space TxF as being F but shifted to have x in its center. Considering this, TxF is isomorphic to F itself when F = R^d, so your intuition about TxM + TxP = F in such a case is correct. The sum is a sum of vector spaces, but it is not necessarily a direct sum (since TxM and TxP can intersect) and in such a case we use the + symbol instead of the (+).\n\n- There are two different but very related properties a random variable can have. X is said to be continuous if P(X = x) = 0 for all single points x. Note that a random variable concentrated on a low dimensional manifold such as a plane can have this property. However, an absolutely continuous random variable has the following property: if a set A has Lebesgue measure 0, then P(X \\in A) = 0. Since points have measure 0 with the Lebesgue measure, absolute continuity implies continuity. A random variable that's supported on a low dimensional manifold therefore will NOT be absolutely continuous: let M a low dim manifold be the support of X. Since a low dim manifold has 0 Lebesgue measure, this would imply P(X \\in M) = 0, which is an absurd since M was the support of X. The property of X being absolutely continuous can be shown to be equivalent to X having a density: the existence of a function f such that P(X \\in A) = \\int_A f(x) dx (this is a consequence of the Radon-Nikodym theorem). \nThe annoying bit is that in everyday paper writing when we talk about continuous random variables, we omit the \"absolutely\" word to keep the text concise and actually talk about absolutely continuous random variables (ones that have a density), this is done through almost all sciences and throughout mathematics as well, annoying as it is. However we made the clarification in here since it's pretty relevant to our paper not to mistake the two terms.\n\nAgain thanks for the comments, and if you have any followups let us know!\n\nBest,\nMartin"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards Principled Methods for Training Generative Adversarial Networks", "abstract": "The goal of this paper is not to introduce a single algorithm or method, but to make theoretical steps towards fully understanding the training dynamics of gen- erative adversarial networks. In order to substantiate our theoretical analysis, we perform targeted experiments to verify our assumptions, illustrate our claims, and quantify the phenomena. This paper is divided into three sections. The first sec- tion introduces the problem at hand. The second section is dedicated to studying and proving rigorously the problems including instability and saturation that arize when training generative adversarial networks. The third section examines a prac- tical and theoretically grounded direction towards solving these problems, while introducing new tools to study them.", "pdf": "/pdf/a39d6b9d0e4f4746a02732368a9fa08d458f0a45.pdf", "TL;DR": "We introduce a theory about generative adversarial networks and their issues.", "paperhash": "arjovsky|towards_principled_methods_for_training_generative_adversarial_networks", "keywords": [], "conflicts": ["nyu.edu", "fb.com", "umontreal.edu"], "authors": ["Martin Arjovsky", "Leon Bottou"], "authorids": ["martinarjovsky@gmail.com", "leonb@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287587462, "id": "ICLR.cc/2017/conference/-/paper415/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Hk4_qw5xe", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper415/reviewers", "ICLR.cc/2017/conference/paper415/areachairs"], "cdate": 1485287587462}}}, {"tddate": null, "tmdate": 1483450123334, "tcdate": 1483450123334, "number": 7, "id": "SyXzDmYSe", "invitation": "ICLR.cc/2017/conference/-/paper415/public/comment", "forum": "Hk4_qw5xe", "replyto": "Hk4_qw5xe", "signatures": ["~Michael_Mathieu1"], "readers": ["everyone"], "writers": ["~Michael_Mathieu1"], "content": {"title": "Very good paper", "comment": "I think this paper is a very good step towards interpreting and understanding GANs. While some experiments have been (often successfully) done with adding noise, there was no theoretical reason, mainly intuition and hand waving.\nThis paper clearly states the reason why these methods work, by pointing a fundamental pitfall in GANs that was overlooked so far.\nI believe that this is a crucial step towards making useful, large scale GANs.\n\nSome notes and typos (sorry if it was already addressed in previous comments):\n- First paragraph of section 2, the cost part is quite unclear (it\u2019s the negative of what was described earlier (after equation 2), and the words \"maximizing\" and \"minimizing\" are mixed).\n- Definition 2.1: what is the tangent space of the whole space around a point? Isn\u2019t it just the point? I think you mean TxM + TxP = F . Also, I thin the + should be a (+) (vector space sum)\n- Bottom of page 4, before Lemma 2: its -> their (intersection)\n- Maybe I'm not familiar enough with the vocabulary, but I did not quite understand what you meant by \"non continuous distribution\", despite the footnote. As a consequence, it took me until much later in the paper to fully understand the link between non-continuous and low-dimensional manifold.\n- There is a numbering problem. In particular, theorem 2.6 refers to theorems 1.1 and 1.2, which don\u2019t exist.\n- Theorem 2.6: I think the definition and role of epsilon could be clarified.\n- Corollary 3.2: missing a tilde, g -> g_\\theta\n- Paragraph above lemma 4: missing \u2018)\u2019 after JSD(..."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards Principled Methods for Training Generative Adversarial Networks", "abstract": "The goal of this paper is not to introduce a single algorithm or method, but to make theoretical steps towards fully understanding the training dynamics of gen- erative adversarial networks. In order to substantiate our theoretical analysis, we perform targeted experiments to verify our assumptions, illustrate our claims, and quantify the phenomena. This paper is divided into three sections. The first sec- tion introduces the problem at hand. The second section is dedicated to studying and proving rigorously the problems including instability and saturation that arize when training generative adversarial networks. The third section examines a prac- tical and theoretically grounded direction towards solving these problems, while introducing new tools to study them.", "pdf": "/pdf/a39d6b9d0e4f4746a02732368a9fa08d458f0a45.pdf", "TL;DR": "We introduce a theory about generative adversarial networks and their issues.", "paperhash": "arjovsky|towards_principled_methods_for_training_generative_adversarial_networks", "keywords": [], "conflicts": ["nyu.edu", "fb.com", "umontreal.edu"], "authors": ["Martin Arjovsky", "Leon Bottou"], "authorids": ["martinarjovsky@gmail.com", "leonb@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287587462, "id": "ICLR.cc/2017/conference/-/paper415/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Hk4_qw5xe", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper415/reviewers", "ICLR.cc/2017/conference/paper415/areachairs"], "cdate": 1485287587462}}}, {"tddate": null, "tmdate": 1482932673694, "tcdate": 1482932673694, "number": 6, "id": "Sy9pZrWSx", "invitation": "ICLR.cc/2017/conference/-/paper415/public/comment", "forum": "Hk4_qw5xe", "replyto": "Hk4_qw5xe", "signatures": ["~Sander_Dieleman1"], "readers": ["everyone"], "writers": ["~Sander_Dieleman1"], "content": {"title": "last paragraph", "comment": "I've always considered myself something of a GAN sceptic, so I'm very excited to see so many papers popping up that try to analyse and address their shortcomings. There are many ICLR submissions on this topic but I found this one a particularly enlightening read, nice work!\n\nThe last paragraph of the paper is perhaps the most interesting, as it seems to propose an alternative training method that would solve most of the problems: the gradients no longer vanish when the discriminator is trained to convergence, which eliminates the careful balancing act that GAN training currently requires.\n\nBut unfortunately the paper ends there, with no discussion of how this would be implemented in practice and no experiments to demonstrate the proposed solution. Is this work that is currently underway? It seems like an excellent idea so it's strange that only one paragraph of the paper is dedicated to it (although admittedly the paper is already quite long)."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards Principled Methods for Training Generative Adversarial Networks", "abstract": "The goal of this paper is not to introduce a single algorithm or method, but to make theoretical steps towards fully understanding the training dynamics of gen- erative adversarial networks. In order to substantiate our theoretical analysis, we perform targeted experiments to verify our assumptions, illustrate our claims, and quantify the phenomena. This paper is divided into three sections. The first sec- tion introduces the problem at hand. The second section is dedicated to studying and proving rigorously the problems including instability and saturation that arize when training generative adversarial networks. The third section examines a prac- tical and theoretically grounded direction towards solving these problems, while introducing new tools to study them.", "pdf": "/pdf/a39d6b9d0e4f4746a02732368a9fa08d458f0a45.pdf", "TL;DR": "We introduce a theory about generative adversarial networks and their issues.", "paperhash": "arjovsky|towards_principled_methods_for_training_generative_adversarial_networks", "keywords": [], "conflicts": ["nyu.edu", "fb.com", "umontreal.edu"], "authors": ["Martin Arjovsky", "Leon Bottou"], "authorids": ["martinarjovsky@gmail.com", "leonb@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287587462, "id": "ICLR.cc/2017/conference/-/paper415/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Hk4_qw5xe", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper415/reviewers", "ICLR.cc/2017/conference/paper415/areachairs"], "cdate": 1485287587462}}}, {"tddate": null, "tmdate": 1482594676193, "tcdate": 1482594676193, "number": 5, "id": "Hy3dKzhEe", "invitation": "ICLR.cc/2017/conference/-/paper415/public/comment", "forum": "Hk4_qw5xe", "replyto": "Hk4_qw5xe", "signatures": ["~Martin_Arjovsky1"], "readers": ["everyone"], "writers": ["~Martin_Arjovsky1"], "content": {"title": "Revision", "comment": "Hi! We would first like to thank the reviewers for your comments. We will aim to make all the suggestions fit into a revision. We will update the paper shortly and provide individual responses to the reviews :)\n\nBest!\nMartin"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards Principled Methods for Training Generative Adversarial Networks", "abstract": "The goal of this paper is not to introduce a single algorithm or method, but to make theoretical steps towards fully understanding the training dynamics of gen- erative adversarial networks. In order to substantiate our theoretical analysis, we perform targeted experiments to verify our assumptions, illustrate our claims, and quantify the phenomena. This paper is divided into three sections. The first sec- tion introduces the problem at hand. The second section is dedicated to studying and proving rigorously the problems including instability and saturation that arize when training generative adversarial networks. The third section examines a prac- tical and theoretically grounded direction towards solving these problems, while introducing new tools to study them.", "pdf": "/pdf/a39d6b9d0e4f4746a02732368a9fa08d458f0a45.pdf", "TL;DR": "We introduce a theory about generative adversarial networks and their issues.", "paperhash": "arjovsky|towards_principled_methods_for_training_generative_adversarial_networks", "keywords": [], "conflicts": ["nyu.edu", "fb.com", "umontreal.edu"], "authors": ["Martin Arjovsky", "Leon Bottou"], "authorids": ["martinarjovsky@gmail.com", "leonb@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287587462, "id": "ICLR.cc/2017/conference/-/paper415/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Hk4_qw5xe", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper415/reviewers", "ICLR.cc/2017/conference/paper415/areachairs"], "cdate": 1485287587462}}}, {"tddate": null, "tmdate": 1482192594176, "tcdate": 1482192594176, "number": 3, "id": "SycA8eLNg", "invitation": "ICLR.cc/2017/conference/-/paper415/official/review", "forum": "Hk4_qw5xe", "replyto": "Hk4_qw5xe", "signatures": ["ICLR.cc/2017/conference/paper415/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper415/AnonReviewer3"], "content": {"title": "good submission", "rating": "8: Top 50% of accepted papers, clear accept", "review": "This paper makes a valuable contribution to provide a more clear understanding of generative adversarial network (GAN) training procedure. \n\nWith the new insight of the training dynamics of GAN, as well as its variant, the authors reveal the reason that why the gradient is either vanishing in original GAN or unstable in its variant. More importantly, they also provide a way to avoid such difficulties by introducing perturbation. I believe this paper will inspire more principled research in this direction. \n\nI am very interested in the perturbation trick to avoid the gradient instability and vanishment. In fact, this is quite related to dropout trick in where the perturbation can be viewed as Bernoulli distribution. It will be great if the connection can be discussed.  Besides the theoretical analysis, is there any empirical study to justify this trick? Could you please add some experiments like Fig 2 and 3 for the perturbated GAN for comparison? ", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards Principled Methods for Training Generative Adversarial Networks", "abstract": "The goal of this paper is not to introduce a single algorithm or method, but to make theoretical steps towards fully understanding the training dynamics of gen- erative adversarial networks. In order to substantiate our theoretical analysis, we perform targeted experiments to verify our assumptions, illustrate our claims, and quantify the phenomena. This paper is divided into three sections. The first sec- tion introduces the problem at hand. The second section is dedicated to studying and proving rigorously the problems including instability and saturation that arize when training generative adversarial networks. The third section examines a prac- tical and theoretically grounded direction towards solving these problems, while introducing new tools to study them.", "pdf": "/pdf/a39d6b9d0e4f4746a02732368a9fa08d458f0a45.pdf", "TL;DR": "We introduce a theory about generative adversarial networks and their issues.", "paperhash": "arjovsky|towards_principled_methods_for_training_generative_adversarial_networks", "keywords": [], "conflicts": ["nyu.edu", "fb.com", "umontreal.edu"], "authors": ["Martin Arjovsky", "Leon Bottou"], "authorids": ["martinarjovsky@gmail.com", "leonb@fb.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512594834, "id": "ICLR.cc/2017/conference/-/paper415/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper415/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper415/AnonReviewer1", "ICLR.cc/2017/conference/paper415/AnonReviewer2", "ICLR.cc/2017/conference/paper415/AnonReviewer3"], "reply": {"forum": "Hk4_qw5xe", "replyto": "Hk4_qw5xe", "writers": {"values-regex": "ICLR.cc/2017/conference/paper415/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper415/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512594834}}}, {"tddate": null, "tmdate": 1482185057904, "tcdate": 1482185057904, "number": 2, "id": "S19DFCSNe", "invitation": "ICLR.cc/2017/conference/-/paper415/pre-review/question", "forum": "Hk4_qw5xe", "replyto": "Hk4_qw5xe", "signatures": ["ICLR.cc/2017/conference/paper415/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper415/AnonReviewer2"], "content": {"title": "next steps", "question": "I'd like to ask a question what you think is the next research direction that should be taken regarding theoretical understanding of GANs."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards Principled Methods for Training Generative Adversarial Networks", "abstract": "The goal of this paper is not to introduce a single algorithm or method, but to make theoretical steps towards fully understanding the training dynamics of gen- erative adversarial networks. In order to substantiate our theoretical analysis, we perform targeted experiments to verify our assumptions, illustrate our claims, and quantify the phenomena. This paper is divided into three sections. The first sec- tion introduces the problem at hand. The second section is dedicated to studying and proving rigorously the problems including instability and saturation that arize when training generative adversarial networks. The third section examines a prac- tical and theoretically grounded direction towards solving these problems, while introducing new tools to study them.", "pdf": "/pdf/a39d6b9d0e4f4746a02732368a9fa08d458f0a45.pdf", "TL;DR": "We introduce a theory about generative adversarial networks and their issues.", "paperhash": "arjovsky|towards_principled_methods_for_training_generative_adversarial_networks", "keywords": [], "conflicts": ["nyu.edu", "fb.com", "umontreal.edu"], "authors": ["Martin Arjovsky", "Leon Bottou"], "authorids": ["martinarjovsky@gmail.com", "leonb@fb.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1482185058706, "id": "ICLR.cc/2017/conference/-/paper415/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper415/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper415/AnonReviewer1", "ICLR.cc/2017/conference/paper415/AnonReviewer2"], "reply": {"forum": "Hk4_qw5xe", "replyto": "Hk4_qw5xe", "writers": {"values-regex": "ICLR.cc/2017/conference/paper415/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper415/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1482185058706}}}, {"tddate": null, "tmdate": 1482184936345, "tcdate": 1482184936345, "number": 2, "id": "SyexY0HNe", "invitation": "ICLR.cc/2017/conference/-/paper415/official/review", "forum": "Hk4_qw5xe", "replyto": "Hk4_qw5xe", "signatures": ["ICLR.cc/2017/conference/paper415/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper415/AnonReviewer2"], "content": {"title": "very interesting submission", "rating": "10: Top 5% of accepted papers, seminal paper", "review": "This is a strong submission regarding one of the most important and recently introduced methods in neural networks - generative adversarial networks. The authors analyze theoretically the convergence of GANs and discuss the stability of GANs. Both are very important. To the best of my knowledge, this is one of the first theoretical papers about GANs and the paper, contrary to most of the submissions in the field, actually provides deep theoretical insight into this architecture. The stability issues regarding GANs are extremely important since the first proposed versions of GANs architecture were very unstable and did not work well in practice. Theorems 2.4-2.6 are novel and introduces mathematical techniques are interesting. I have some technical questions regarding the proof of Theorem 2.5 but these are pretty minor.\n", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards Principled Methods for Training Generative Adversarial Networks", "abstract": "The goal of this paper is not to introduce a single algorithm or method, but to make theoretical steps towards fully understanding the training dynamics of gen- erative adversarial networks. In order to substantiate our theoretical analysis, we perform targeted experiments to verify our assumptions, illustrate our claims, and quantify the phenomena. This paper is divided into three sections. The first sec- tion introduces the problem at hand. The second section is dedicated to studying and proving rigorously the problems including instability and saturation that arize when training generative adversarial networks. The third section examines a prac- tical and theoretically grounded direction towards solving these problems, while introducing new tools to study them.", "pdf": "/pdf/a39d6b9d0e4f4746a02732368a9fa08d458f0a45.pdf", "TL;DR": "We introduce a theory about generative adversarial networks and their issues.", "paperhash": "arjovsky|towards_principled_methods_for_training_generative_adversarial_networks", "keywords": [], "conflicts": ["nyu.edu", "fb.com", "umontreal.edu"], "authors": ["Martin Arjovsky", "Leon Bottou"], "authorids": ["martinarjovsky@gmail.com", "leonb@fb.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512594834, "id": "ICLR.cc/2017/conference/-/paper415/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper415/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper415/AnonReviewer1", "ICLR.cc/2017/conference/paper415/AnonReviewer2", "ICLR.cc/2017/conference/paper415/AnonReviewer3"], "reply": {"forum": "Hk4_qw5xe", "replyto": "Hk4_qw5xe", "writers": {"values-regex": "ICLR.cc/2017/conference/paper415/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper415/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512594834}}}, {"tddate": null, "tmdate": 1482175130303, "tcdate": 1482175130303, "number": 1, "id": "HkfifhHNx", "invitation": "ICLR.cc/2017/conference/-/paper415/official/review", "forum": "Hk4_qw5xe", "replyto": "Hk4_qw5xe", "signatures": ["ICLR.cc/2017/conference/paper415/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper415/AnonReviewer1"], "content": {"title": "review of ``TOWARDS PRINCIPLED METHODS FOR TRAINING GENERATIVE ADVERSARIAL NETWORKS'' ", "rating": "7: Good paper, accept", "review": "SUMMARY \nThis paper addresses important questions about the difficulties in training generative adversarial networks. It discusses consequences of using an asymmetric divergence function and sources of instability in training GANs. Then it proposes an alternative using a smoothening approach. \n\nPROS \nTheory, good questions, nice answers. \nMakes an interesting use of concepts form analysis and differential topology. \nProposes avenues to avoid instability in GANs. \n\nCONS \nA bit too long, technical. Some parts and consequences still need to be further developed (which is perfectly fine for future work). \n\nMINOR COMMENTS\n\n- Section 2.1 Maybe shorten this section a bit. E.g., move all proofs to the appendix. \n\n- Section 3 provides a nice, intuitive, simple solution. \n\n- On page 2 second bullet. This also means that P_g is smaller than the data distribution in some other x, which in turn will make the KL divergence non zero. \n\n- On page 2, ``for not generating plausibly looking pictures'' should be ``for generating not plausibly looking pictures''.  \n\n- Lemma 1 would also hold in more generality. \n\n- Theorem 2.1 seems to be basic analysis. (In other words, a reference could spare the proof). \n\n- In Theorem 2.4, it would be good to remind the reader about p(z). \n\n- Lemma 2 seems to be basic analysis. (In other words, a reference could spare the proof). \nSpecify the domain of the random variables. \n\n- relly - > rely \n\n- Theorem 2.2 the closed manifolds have boundary or not? (already in the questions)\n\n- Corollary 2.1, ``assumptions of Theorem 1.3''. I could not find Theorem 1.3. \n\n- Theorem 2.5 ``Therefore'' -> `Then'? \n\n- Theorem 2.6 ``Is a... '' -> `is a' ? \n\n- The number of the theorems is confusing. \n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards Principled Methods for Training Generative Adversarial Networks", "abstract": "The goal of this paper is not to introduce a single algorithm or method, but to make theoretical steps towards fully understanding the training dynamics of gen- erative adversarial networks. In order to substantiate our theoretical analysis, we perform targeted experiments to verify our assumptions, illustrate our claims, and quantify the phenomena. This paper is divided into three sections. The first sec- tion introduces the problem at hand. The second section is dedicated to studying and proving rigorously the problems including instability and saturation that arize when training generative adversarial networks. The third section examines a prac- tical and theoretically grounded direction towards solving these problems, while introducing new tools to study them.", "pdf": "/pdf/a39d6b9d0e4f4746a02732368a9fa08d458f0a45.pdf", "TL;DR": "We introduce a theory about generative adversarial networks and their issues.", "paperhash": "arjovsky|towards_principled_methods_for_training_generative_adversarial_networks", "keywords": [], "conflicts": ["nyu.edu", "fb.com", "umontreal.edu"], "authors": ["Martin Arjovsky", "Leon Bottou"], "authorids": ["martinarjovsky@gmail.com", "leonb@fb.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512594834, "id": "ICLR.cc/2017/conference/-/paper415/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper415/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper415/AnonReviewer1", "ICLR.cc/2017/conference/paper415/AnonReviewer2", "ICLR.cc/2017/conference/paper415/AnonReviewer3"], "reply": {"forum": "Hk4_qw5xe", "replyto": "Hk4_qw5xe", "writers": {"values-regex": "ICLR.cc/2017/conference/paper415/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper415/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512594834}}}, {"tddate": null, "tmdate": 1482005865616, "tcdate": 1482005865616, "number": 4, "id": "rkWOTzQNg", "invitation": "ICLR.cc/2017/conference/-/paper415/public/comment", "forum": "Hk4_qw5xe", "replyto": "SkTrgzMEl", "signatures": ["~Martin_Arjovsky1"], "readers": ["everyone"], "writers": ["~Martin_Arjovsky1"], "content": {"title": "Thanks!", "comment": "Thank you for the amazing review!!! If you have any questions please send them our way and we'll be glad to answer them :)\n\nBest!\nMartin"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards Principled Methods for Training Generative Adversarial Networks", "abstract": "The goal of this paper is not to introduce a single algorithm or method, but to make theoretical steps towards fully understanding the training dynamics of gen- erative adversarial networks. In order to substantiate our theoretical analysis, we perform targeted experiments to verify our assumptions, illustrate our claims, and quantify the phenomena. This paper is divided into three sections. The first sec- tion introduces the problem at hand. The second section is dedicated to studying and proving rigorously the problems including instability and saturation that arize when training generative adversarial networks. The third section examines a prac- tical and theoretically grounded direction towards solving these problems, while introducing new tools to study them.", "pdf": "/pdf/a39d6b9d0e4f4746a02732368a9fa08d458f0a45.pdf", "TL;DR": "We introduce a theory about generative adversarial networks and their issues.", "paperhash": "arjovsky|towards_principled_methods_for_training_generative_adversarial_networks", "keywords": [], "conflicts": ["nyu.edu", "fb.com", "umontreal.edu"], "authors": ["Martin Arjovsky", "Leon Bottou"], "authorids": ["martinarjovsky@gmail.com", "leonb@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287587462, "id": "ICLR.cc/2017/conference/-/paper415/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Hk4_qw5xe", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper415/reviewers", "ICLR.cc/2017/conference/paper415/areachairs"], "cdate": 1485287587462}}}, {"tddate": null, "tmdate": 1481936964796, "tcdate": 1481936964796, "number": 1, "id": "SkTrgzMEl", "invitation": "ICLR.cc/2017/conference/-/paper415/public/review", "forum": "Hk4_qw5xe", "replyto": "Hk4_qw5xe", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "very solid contribution to the theoretical understanding of GANS", "rating": "10: Top 5% of accepted papers, seminal paper", "review": "This is a strong submission regarding one of the most important and recently introduced methods in neural networks - generative adversarial networks. The authors analyze theoretically the convergence of GANs and discuss the stability of GANs. Both are very important. To the best of my knowledge, this is one of the first theoretical papers about GANs and the paper, contrary to most of the submissions in the field, actually provides deep theoretical insight into this architecture. The stability issues regarding GANs are extremely important since the first proposed versions of GANs architecture were very unstable and did not work well in practice. Theorems 2.4-2.6 are novel and introduces mathematical techniques are interesting. I have some technical questions regarding the proof of Theorem 2.5 but these are pretty minor.", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards Principled Methods for Training Generative Adversarial Networks", "abstract": "The goal of this paper is not to introduce a single algorithm or method, but to make theoretical steps towards fully understanding the training dynamics of gen- erative adversarial networks. In order to substantiate our theoretical analysis, we perform targeted experiments to verify our assumptions, illustrate our claims, and quantify the phenomena. This paper is divided into three sections. The first sec- tion introduces the problem at hand. The second section is dedicated to studying and proving rigorously the problems including instability and saturation that arize when training generative adversarial networks. The third section examines a prac- tical and theoretically grounded direction towards solving these problems, while introducing new tools to study them.", "pdf": "/pdf/a39d6b9d0e4f4746a02732368a9fa08d458f0a45.pdf", "TL;DR": "We introduce a theory about generative adversarial networks and their issues.", "paperhash": "arjovsky|towards_principled_methods_for_training_generative_adversarial_networks", "keywords": [], "conflicts": ["nyu.edu", "fb.com", "umontreal.edu"], "authors": ["Martin Arjovsky", "Leon Bottou"], "authorids": ["martinarjovsky@gmail.com", "leonb@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1482512921305, "id": "ICLR.cc/2017/conference/-/paper415/public/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Hk4_qw5xe", "replyto": "Hk4_qw5xe", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "noninvitees": ["martinarjovsky@gmail.com", "leonb@fb.com", "ICLR.cc/2017/conference/paper415/reviewers", "ICLR.cc/2017/conference/paper415/areachairs", "(anonymous)"], "cdate": 1482512921305}}}, {"tddate": null, "tmdate": 1481898412076, "tcdate": 1481898346228, "number": 3, "id": "SyMdYd-Ee", "invitation": "ICLR.cc/2017/conference/-/paper415/public/comment", "forum": "Hk4_qw5xe", "replyto": "B1ajPwZ4g", "signatures": ["~Martin_Arjovsky1"], "readers": ["everyone"], "writers": ["~Martin_Arjovsky1"], "content": {"title": "Answers", "comment": "First of all, many thanks for the comments! All of your remarks will help make this a better paper. The things you mentioned will be included in the revision.\n\n1) Yes my bad, this is a typo. The second one should be n > d as you say.\n\n2) a&b) Note that not everything that's outside of the support of Pr has to be a generated image. Generated images are only things that lie in the support of Pg, and there are things that don't need to be in the support of either Pr or Pg (these could be places where 0 < D < 1 for example). This is because the discriminator is not trained to discriminate Pr from all things that are not Pr, but to distinguish Pr from Pg. Points that don't lie in the support of Pr or Pg are not important to the performance of the discriminator (as is easily evidenced in its cost). Why we define it like this is to avoid the identification of a single \"tight\" support, since this typically leads to problems (if I take a measure 0 set from any support it still is the support of the distribution). In the end, what we aim for is:\n - We want D(x) = 1 with probability 1  when x ~ Pr\n - We want D(x) = 0 with probability 1 when x ~ Pg\n - Whatever happens elsewhere is irrelevant (as it is also reflected by the cost of the discriminator)\n\n3) Following up from (2), the way we formalize these three statements is by saying P_r[D(x) = 1]=1 and P_g[D(x) = 0] = 1. More formally, P_r[D(x) = 1] = 1 would be P_r[{x : D(x) = 1}] = 1 (when thinking of P_r as a measure applied on a set). Sometimes this is noted P_{x ~ P_r} [D(x) = 1] = 1 or with other equivalent variants (e.g. P_r[D^{-1}(1)] = 1). It seems the notation is very much suboptimal right now so I'll try to make it much clearer in the revision.\n\n4)\na) I'm using the distance d(A, B) = \\inf{x \\in A, y \\in B} d(x, y) the induced distance to sets. We'll make it clearer.\n\nb) The delta/3 comes from the fact that if I took delta/2 then M^ and P^ could overlap and delta/3 is enough :)\n\n5)\na) P^c is the complement of P. I think the lowercaps c should be uppercaps, I will look at what notation people typically use for this.\n\nb & c) It's defined for all x in P^c (which includes M\\L). Note that this is an open set, so for all x we have e_x > 0. I should clarify this a bit further.\n\nd) Look at the answer of 2). By construction the points in M^ not in M constitute a measure 0 set for both Pr and Pg, so the values taken there are irrelevant for the purpose of D being an optimal discriminator. The reason we expand M\\L to M^ is so we can ensure that D is locally constant for all points in M/L\n\n9) Ooops, bad reference. This should say \"Under the same assumptions of Theorem 2.4\", thanks for spotting this.\n\n11) Yes indeed, will do it in the revision :)\n\n12) You are correct, I will make it more explicit.\n\n16) Oops, should be Pr and Pg\n\n17)\na) My bad, will change it to JSD(P_x \\| P_{x+e})\nb) Yeah, that's true. I'll try to make it more precise.\n\nAgain, thanks a lot for all the comments, it's super helpful. If you have any followups please feel free to comment.\nBest!\nMartin"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards Principled Methods for Training Generative Adversarial Networks", "abstract": "The goal of this paper is not to introduce a single algorithm or method, but to make theoretical steps towards fully understanding the training dynamics of gen- erative adversarial networks. In order to substantiate our theoretical analysis, we perform targeted experiments to verify our assumptions, illustrate our claims, and quantify the phenomena. This paper is divided into three sections. The first sec- tion introduces the problem at hand. The second section is dedicated to studying and proving rigorously the problems including instability and saturation that arize when training generative adversarial networks. The third section examines a prac- tical and theoretically grounded direction towards solving these problems, while introducing new tools to study them.", "pdf": "/pdf/a39d6b9d0e4f4746a02732368a9fa08d458f0a45.pdf", "TL;DR": "We introduce a theory about generative adversarial networks and their issues.", "paperhash": "arjovsky|towards_principled_methods_for_training_generative_adversarial_networks", "keywords": [], "conflicts": ["nyu.edu", "fb.com", "umontreal.edu"], "authors": ["Martin Arjovsky", "Leon Bottou"], "authorids": ["martinarjovsky@gmail.com", "leonb@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287587462, "id": "ICLR.cc/2017/conference/-/paper415/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Hk4_qw5xe", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper415/reviewers", "ICLR.cc/2017/conference/paper415/areachairs"], "cdate": 1485287587462}}}, {"tddate": null, "tmdate": 1481893797346, "tcdate": 1481893797346, "number": 2, "id": "B1ajPwZ4g", "invitation": "ICLR.cc/2017/conference/-/paper415/public/comment", "forum": "Hk4_qw5xe", "replyto": "Hk4_qw5xe", "signatures": ["~Antonia_Creswell1"], "readers": ["everyone"], "writers": ["~Antonia_Creswell1"], "content": {"title": "Questions and inconsistencies.", "comment": "This paper makes many valuable contributions and explains many of the issues related to training GANs; I feel it will be an essential platform for future work. However I have the following questions and found the following inconsistencies.\n\n1) \n\t\u201cIf n <= d, we are done since the image of \u03c0 is contained in all Rn, a manifold with at \n\tmost dimension d. We now turn to the case where d > n.\u201d\n\nIs the case  d>n  not contained in the first,  n<=d ? Should this be n>d?\n\n2) The perfect discriminator theorems: \n\n\t\u201cWe say that a discriminator D : X \u2192 [0, 1] has accuracy 1 if it takes the value 1 on a set that \n\tcontains the support of P_r  and value 0 on a set that contains the support of P_g .\u201d\n\nIf a set S_g or S_r contains the support of P_g or P_r, this suggests that it may contain the support but may also contain regions outside of the support.  Are you suggesting that for x in S_r that are outside the support of P_r, D(x)=1? However, surely regions outside the support of P_r should correspond to generated images, so D(x) should be 0.\n\nb) Following from a), if a set S_g (or S_r) contained regions outside the support (as well as the support) then S_g (or S_r)  would contain the support of P_g (or P_r) but may also contain regions in the support of P_r (or P_g).\n\n3) \n\t\tP_r[D(x) = 1] = 1 and P_g[D(x) = 0] = 1 \n\na) This notation is not clear, do you mean:\n\tP_r(x)=1 for all x in {x : D(x)=1} and P_g(x)=1 for all x in {x : D(x)=0}\n\n4) Theorem 2.1\n\na) It is not clear what distance measure you are using between sets\nb) Where does the delta/3 come from? Is this arbitrary?\n\n5) Theorem 2.2\na) What is P^c?\nb) Is the ball B(x,e_x) defined for all x in M\\L? This is not explicit.\n\nIf defined for all x in M\\L:\nc) For x on the boundary of M\\L and P\\L is e_x=0? Otherwise, is it possible that for x on the boundary of M\\L or P\\L , M_hat and P_hat intersect with P\\L or M\\L respectively? \n\nFinally:\nd) M_hat is a superset of M\\L, how can you say D*(x)=1 for all x in M_hat, when M_hat may include regions outside the support of M?\n\n9) Corollary 2.1; \n\t\"Under the same assumptions of Theorem 1.3\u2028\"\na) What is Theorem 1.3?\n\n11) Theorem 2.5 \na) Could this be explicitly linked to the collapsing generator problem talked about by [1] and [2].  Citations might make the link clearer. \n\n12) Theorem 2.6 \na) Why do you assume that the distribution of difference between D and D* is white noise? This suggests that most of the time D is optimal, from the rest of the paper this is believable, however if this is what you are assuming, this could be made more explicit. \nb) Similar question for difference in gradient, though since grad D* is zero most of the time, that would suggest that grad D is zero most of the time. Is this true?\nc) extra bracket in final line of proof\n\n16) \n       \u201cThis is in drastic contrast to the noiseless variants P_g  and P_g,\u201d\na) repeated P_g\n\n17) \n         \u201cagain that JSD(P_x,P_{x+e} is maxed out,\u201d\na) missing bracket and inconsistent notation, JSD(P_x||P_{x+e})\nb) \u201cmaxed out\u201c is a colloquialism\n\n[1] Salimans, Tim, et al. \"Improved techniques for training gans.\" arXiv preprint arXiv:1606.03498 (2016).\n[2]Odena, Augustus, Christopher Olah, and Jonathon Shlens. \"Conditional Image Synthesis With Auxiliary Classifier GANs.\" arXiv preprint arXiv:1610.09585 (2016)."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards Principled Methods for Training Generative Adversarial Networks", "abstract": "The goal of this paper is not to introduce a single algorithm or method, but to make theoretical steps towards fully understanding the training dynamics of gen- erative adversarial networks. In order to substantiate our theoretical analysis, we perform targeted experiments to verify our assumptions, illustrate our claims, and quantify the phenomena. This paper is divided into three sections. The first sec- tion introduces the problem at hand. The second section is dedicated to studying and proving rigorously the problems including instability and saturation that arize when training generative adversarial networks. The third section examines a prac- tical and theoretically grounded direction towards solving these problems, while introducing new tools to study them.", "pdf": "/pdf/a39d6b9d0e4f4746a02732368a9fa08d458f0a45.pdf", "TL;DR": "We introduce a theory about generative adversarial networks and their issues.", "paperhash": "arjovsky|towards_principled_methods_for_training_generative_adversarial_networks", "keywords": [], "conflicts": ["nyu.edu", "fb.com", "umontreal.edu"], "authors": ["Martin Arjovsky", "Leon Bottou"], "authorids": ["martinarjovsky@gmail.com", "leonb@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287587462, "id": "ICLR.cc/2017/conference/-/paper415/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Hk4_qw5xe", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper415/reviewers", "ICLR.cc/2017/conference/paper415/areachairs"], "cdate": 1485287587462}}}, {"tddate": null, "tmdate": 1481128765072, "tcdate": 1481128765065, "number": 1, "id": "Byrro3S7l", "invitation": "ICLR.cc/2017/conference/-/paper415/public/comment", "forum": "Hk4_qw5xe", "replyto": "ByYDwryXl", "signatures": ["~Martin_Arjovsky1"], "readers": ["everyone"], "writers": ["~Martin_Arjovsky1"], "content": {"title": "Definitions and boundaries.", "comment": "1) Thanks for the comments! By optimal discriminator we meant that it is optimal with respect to its objective E_x~Pr [ log D(x)] + E_x~Pg[ log(1 - D)]. Clearly we forgot to define this properly so we'll add it in the revision :).\n\n2) This is a very important question (which we forgot to properly address). As is, the theorem assumes a manifold without boundary. We will extend the theorem to manifolds with boundary in the revision.\n\nThis is quite simple, since the definition of perfect alignment can be extended by considering the (as now defined) perfect alignment between the interior and boundary of M with the interior and boundary of P, and we can easily prove Lemma 2 for this new definition.\n\nAfter that, the missing step is to prove that L = intersection(M, P) has measure 0 on both M and P (analogous to Lemma 3). But L is the union of intersection(interior(M), interior(P)), intersection(interior(M), boundary(P)), intesection(boundary(M), interior(P)), intersection(boundary(M), boundary(P)), which is the union of 4 intersections of not perfectly aligned manifolds without boundary (so union of strictly lower dimensional manifolds inside P and M), so it will have to have measure 0 in both M and P.\n\nThe rest of the proof will follow in the same way as now :)\n\nAgain, a million thanks for the important observations!"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards Principled Methods for Training Generative Adversarial Networks", "abstract": "The goal of this paper is not to introduce a single algorithm or method, but to make theoretical steps towards fully understanding the training dynamics of gen- erative adversarial networks. In order to substantiate our theoretical analysis, we perform targeted experiments to verify our assumptions, illustrate our claims, and quantify the phenomena. This paper is divided into three sections. The first sec- tion introduces the problem at hand. The second section is dedicated to studying and proving rigorously the problems including instability and saturation that arize when training generative adversarial networks. The third section examines a prac- tical and theoretically grounded direction towards solving these problems, while introducing new tools to study them.", "pdf": "/pdf/a39d6b9d0e4f4746a02732368a9fa08d458f0a45.pdf", "TL;DR": "We introduce a theory about generative adversarial networks and their issues.", "paperhash": "arjovsky|towards_principled_methods_for_training_generative_adversarial_networks", "keywords": [], "conflicts": ["nyu.edu", "fb.com", "umontreal.edu"], "authors": ["Martin Arjovsky", "Leon Bottou"], "authorids": ["martinarjovsky@gmail.com", "leonb@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287587462, "id": "ICLR.cc/2017/conference/-/paper415/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Hk4_qw5xe", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper415/reviewers", "ICLR.cc/2017/conference/paper415/areachairs"], "cdate": 1485287587462}}}, {"tddate": null, "tmdate": 1480705889299, "tcdate": 1480705889295, "number": 1, "id": "ByYDwryXl", "invitation": "ICLR.cc/2017/conference/-/paper415/pre-review/question", "forum": "Hk4_qw5xe", "replyto": "Hk4_qw5xe", "signatures": ["ICLR.cc/2017/conference/paper415/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper415/AnonReviewer1"], "content": {"title": "minor questions", "question": "In Theorem 2.2. what is the definition of an `optimal' discriminator? \nClosed manifolds are intended to mean manifolds without boundary or manifolds with boundary? "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards Principled Methods for Training Generative Adversarial Networks", "abstract": "The goal of this paper is not to introduce a single algorithm or method, but to make theoretical steps towards fully understanding the training dynamics of gen- erative adversarial networks. In order to substantiate our theoretical analysis, we perform targeted experiments to verify our assumptions, illustrate our claims, and quantify the phenomena. This paper is divided into three sections. The first sec- tion introduces the problem at hand. The second section is dedicated to studying and proving rigorously the problems including instability and saturation that arize when training generative adversarial networks. The third section examines a prac- tical and theoretically grounded direction towards solving these problems, while introducing new tools to study them.", "pdf": "/pdf/a39d6b9d0e4f4746a02732368a9fa08d458f0a45.pdf", "TL;DR": "We introduce a theory about generative adversarial networks and their issues.", "paperhash": "arjovsky|towards_principled_methods_for_training_generative_adversarial_networks", "keywords": [], "conflicts": ["nyu.edu", "fb.com", "umontreal.edu"], "authors": ["Martin Arjovsky", "Leon Bottou"], "authorids": ["martinarjovsky@gmail.com", "leonb@fb.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1482185058706, "id": "ICLR.cc/2017/conference/-/paper415/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper415/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper415/AnonReviewer1", "ICLR.cc/2017/conference/paper415/AnonReviewer2"], "reply": {"forum": "Hk4_qw5xe", "replyto": "Hk4_qw5xe", "writers": {"values-regex": "ICLR.cc/2017/conference/paper415/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper415/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1482185058706}}}], "count": 29}