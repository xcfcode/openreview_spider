{"notes": [{"id": "wn0-UGKSwl7", "original": "92zrmpRpGKU", "number": 8, "cdate": 1615310249378, "ddate": null, "tcdate": 1615310249378, "tmdate": 1615313018712, "tddate": null, "forum": "wn0-UGKSwl7", "replyto": null, "invitation": "ICLR.cc/2021/Workshop/SSL-RL/-/Blind_Submission", "content": {"title": "Representation Matters: Offline Pretraining for Sequential Decision Making", "authorids": ["ICLR.cc/2021/Workshop/SSL-RL/Paper8/Authors"], "authors": ["Anonymous"], "keywords": ["Offline RL", "Representation Learning"], "TL;DR": "Learning state representations from offline datasets using unsupervised objectives can drastically elevate performance on downstream offline RL, online RL, and imitation learning tasks.", "abstract": "The recent success of supervised learning methods on ever larger offline datasets has spurred interest in the reinforcement learning (RL) field to investigate whether the same paradigms can be translated to RL algorithms. This research area, known as offline RL, has largely focused on offline policy optimization, aiming to find a return-maximizing policy exclusively from offline data. In this paper, we consider a slightly different approach to incorporating offline data into sequential decision-making. We aim to answer the question, what unsupervised objectives applied to offline datasets are able to learn state representations which elevate performance on downstream tasks, whether those downstream tasks be online RL, imitation learning from expert demonstrations, or even offline policy optimization based on the same offline dataset? Through a variety of experiments utilizing standard offline RL datasets, we find that the use of pretraining with unsupervised learning objectives can dramatically improve the performance of policy learning algorithms that otherwise yield mediocre performance on their own. Extensive ablations further provide insights into what components of these unsupervised objectives -- e.g., reward prediction, continuous or discrete representations, pretraining or finetuning -- are most important and in which settings.", "pdf": "/pdf/efac3d1fb7e44fa944311d6c2da943b863f72f57.pdf", "paperhash": "anonymous|representation_matters_offline_pretraining_for_sequential_decision_making", "_bibtex": "@inproceedings{\nanonymous2021representation,\ntitle={Representation Matters: Offline Pretraining for Sequential Decision Making},\nauthor={Anonymous},\nbooktitle={Submitted to Self-Supervision for Reinforcement Learning Workshop - ICLR 2021},\nyear={2021},\nurl={https://openreview.net/forum?id=wn0-UGKSwl7},\nnote={under review}\n}"}, "signatures": ["ICLR.cc/2021/Workshop/SSL-RL"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Workshop/SSL-RL"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Workshop/SSL-RL"]}, "signatures": {"values": ["ICLR.cc/2021/Workshop/SSL-RL"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Workshop/SSL-RL"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Workshop/SSL-RL"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1615310247528, "tmdate": 1615313016556, "id": "ICLR.cc/2021/Workshop/SSL-RL/-/Blind_Submission"}}, "tauthor": "~Super_User1"}], "count": 1}