{"notes": [{"id": "HyxUIj09KX", "original": "S1xSNg5xFX", "number": 180, "cdate": 1538087758488, "ddate": null, "tcdate": 1538087758488, "tmdate": 1545355387194, "tddate": null, "forum": "HyxUIj09KX", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "S-System, Geometry, Learning, and Optimization: A Theory of Neural Networks", "abstract": "We present a formal measure-theoretical theory of neural networks (NN) built on {\\it probability coupling theory}. Particularly, we present an algorithm framework, Hierarchical Measure Group and Approximate System (HMGAS), nicknamed S-System, of which NNs are special cases. In addition to many other results, the framework enables us to prove that 1) NNs implement {\\it renormalization group (RG)} using information geometry, which points out that the large scale property to renormalize is dual Bregman divergence and completes the analog between NNs and RG; 2) and under a set of {\\it realistic} boundedness and diversity conditions, for {\\it large size nonlinear deep} NNs with a class of losses, including the hinge loss, all local minima are global minima with zero loss errors, using random matrix theory.", "keywords": ["neural network theory", "probability measure theory", "probability coupling theory", "S-System", "optimization", "random matrix", "renormalization group", "information geometry", "coarse graining", "hierarchy", "activation function", "symmetry"], "authorids": ["lishuai918@gmail.com", "kuijia@scut.edu.cn"], "authors": ["Shuai Li", "Kui Jia"], "TL;DR": "We present a formal measure-theoretical theory of neural networks (NN) that quantitatively shows NNs renormalize on semantic difference, and under practical conditions large size deep nonlinear NNs can optimize objective functions to zero losses.", "pdf": "/pdf/6fb1ea402d64528f6560a40defd9b9f1f5d709fc.pdf", "paperhash": "li|ssystem_geometry_learning_and_optimization_a_theory_of_neural_networks", "_bibtex": "@misc{\nli2019ssystem,\ntitle={S-System, Geometry, Learning, and Optimization: A Theory of Neural Networks},\nauthor={Shuai Li and Kui Jia},\nyear={2019},\nurl={https://openreview.net/forum?id=HyxUIj09KX},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 3, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "r1gCKEg-gN", "original": null, "number": 1, "cdate": 1544778885998, "ddate": null, "tcdate": 1544778885998, "tmdate": 1545354523194, "tddate": null, "forum": "HyxUIj09KX", "replyto": "HyxUIj09KX", "invitation": "ICLR.cc/2019/Conference/-/Paper180/Meta_Review", "content": {"metareview": "The paper is extremely difficult to read, even given that both reviewers have very strong math / theoretical background. Although it may potentially include interesting ideas, nothing in the work could not be understood by the ICLR audience. \n", "confidence": "5: The area chair is absolutely certain", "recommendation": "Reject", "title": "Paper unreadable "}, "signatures": ["ICLR.cc/2019/Conference/Paper180/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper180/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "S-System, Geometry, Learning, and Optimization: A Theory of Neural Networks", "abstract": "We present a formal measure-theoretical theory of neural networks (NN) built on {\\it probability coupling theory}. Particularly, we present an algorithm framework, Hierarchical Measure Group and Approximate System (HMGAS), nicknamed S-System, of which NNs are special cases. In addition to many other results, the framework enables us to prove that 1) NNs implement {\\it renormalization group (RG)} using information geometry, which points out that the large scale property to renormalize is dual Bregman divergence and completes the analog between NNs and RG; 2) and under a set of {\\it realistic} boundedness and diversity conditions, for {\\it large size nonlinear deep} NNs with a class of losses, including the hinge loss, all local minima are global minima with zero loss errors, using random matrix theory.", "keywords": ["neural network theory", "probability measure theory", "probability coupling theory", "S-System", "optimization", "random matrix", "renormalization group", "information geometry", "coarse graining", "hierarchy", "activation function", "symmetry"], "authorids": ["lishuai918@gmail.com", "kuijia@scut.edu.cn"], "authors": ["Shuai Li", "Kui Jia"], "TL;DR": "We present a formal measure-theoretical theory of neural networks (NN) that quantitatively shows NNs renormalize on semantic difference, and under practical conditions large size deep nonlinear NNs can optimize objective functions to zero losses.", "pdf": "/pdf/6fb1ea402d64528f6560a40defd9b9f1f5d709fc.pdf", "paperhash": "li|ssystem_geometry_learning_and_optimization_a_theory_of_neural_networks", "_bibtex": "@misc{\nli2019ssystem,\ntitle={S-System, Geometry, Learning, and Optimization: A Theory of Neural Networks},\nauthor={Shuai Li and Kui Jia},\nyear={2019},\nurl={https://openreview.net/forum?id=HyxUIj09KX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper180/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353308193, "tddate": null, "super": null, "final": null, "reply": {"forum": "HyxUIj09KX", "replyto": "HyxUIj09KX", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper180/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper180/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper180/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353308193}}}, {"id": "HJehf-1Lam", "original": null, "number": 2, "cdate": 1541955860264, "ddate": null, "tcdate": 1541955860264, "tmdate": 1541955860264, "tddate": null, "forum": "HyxUIj09KX", "replyto": "HyxUIj09KX", "invitation": "ICLR.cc/2019/Conference/-/Paper180/Official_Review", "content": {"title": "Interesting take on neural networks from a measure-theoretic viewpoint, however not easy to follow for a non-expert", "review": "The paper provides a new framework \"S-System\" as a generalization of hierarchal models including neural networks. The paper shows an alternative way to derive the activation functions commonly used in practice in a principled way. It further shows that the landscape of the optimization problem of neural networks has nice properties in the setting where the number of input/hidden units tending to infinity and the neurons satisfy certain diversity conditions.\n\nOverall, the paper presents super interesting ideas that can potentially lead to a deeper understanding of the fundamentals of deep learning. However, for a general reader it is a hard-to-follow paper. Without a full understanding of the various domains this paper presents ideas from, it is hard to verify and fully understand the claims. I believe the paper would be better appreciated by an audience of a mathematical journal. As an alternative, I would encourage the readers to split the paper and possibly simplify the content by using a running example (more concrete than the one of MLP used) to explain the implications as well as assumptions.\n\nA clearer, more accessible presentation is necessary so that a non-expert can understand the paper's results. Thus, I vote to reject. \n", "rating": "4: Ok but not good enough - rejection", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2019/Conference/Paper180/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "S-System, Geometry, Learning, and Optimization: A Theory of Neural Networks", "abstract": "We present a formal measure-theoretical theory of neural networks (NN) built on {\\it probability coupling theory}. Particularly, we present an algorithm framework, Hierarchical Measure Group and Approximate System (HMGAS), nicknamed S-System, of which NNs are special cases. In addition to many other results, the framework enables us to prove that 1) NNs implement {\\it renormalization group (RG)} using information geometry, which points out that the large scale property to renormalize is dual Bregman divergence and completes the analog between NNs and RG; 2) and under a set of {\\it realistic} boundedness and diversity conditions, for {\\it large size nonlinear deep} NNs with a class of losses, including the hinge loss, all local minima are global minima with zero loss errors, using random matrix theory.", "keywords": ["neural network theory", "probability measure theory", "probability coupling theory", "S-System", "optimization", "random matrix", "renormalization group", "information geometry", "coarse graining", "hierarchy", "activation function", "symmetry"], "authorids": ["lishuai918@gmail.com", "kuijia@scut.edu.cn"], "authors": ["Shuai Li", "Kui Jia"], "TL;DR": "We present a formal measure-theoretical theory of neural networks (NN) that quantitatively shows NNs renormalize on semantic difference, and under practical conditions large size deep nonlinear NNs can optimize objective functions to zero losses.", "pdf": "/pdf/6fb1ea402d64528f6560a40defd9b9f1f5d709fc.pdf", "paperhash": "li|ssystem_geometry_learning_and_optimization_a_theory_of_neural_networks", "_bibtex": "@misc{\nli2019ssystem,\ntitle={S-System, Geometry, Learning, and Optimization: A Theory of Neural Networks},\nauthor={Shuai Li and Kui Jia},\nyear={2019},\nurl={https://openreview.net/forum?id=HyxUIj09KX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper180/Official_Review", "cdate": 1542234520910, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HyxUIj09KX", "replyto": "HyxUIj09KX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper180/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335666118, "tmdate": 1552335666118, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper180/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SygFw7xWpQ", "original": null, "number": 1, "cdate": 1541632865347, "ddate": null, "tcdate": 1541632865347, "tmdate": 1541632865347, "tddate": null, "forum": "HyxUIj09KX", "replyto": "HyxUIj09KX", "invitation": "ICLR.cc/2019/Conference/-/Paper180/Official_Review", "content": {"title": "has grand ideas but poorly written, cannot check for correctness", "review": "The paper is extremely difficult to read. There are too many concepts introduced at once, casual comments mixed with semi-formal statements. The theorems sound interesting, the implications are grand and of interest to ICLR, but the proofs are impossible to follow. As such, I am not in a position to make a recommendation. \n\nI strongly recommend the authors to split the paper into multiple parts with clear-cut statements in each, with clear and detailed proofs, and submit to appropriate journals / conferences. \n", "rating": "4: Ok but not good enough - rejection", "confidence": "1: The reviewer's evaluation is an educated guess"}, "signatures": ["ICLR.cc/2019/Conference/Paper180/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "S-System, Geometry, Learning, and Optimization: A Theory of Neural Networks", "abstract": "We present a formal measure-theoretical theory of neural networks (NN) built on {\\it probability coupling theory}. Particularly, we present an algorithm framework, Hierarchical Measure Group and Approximate System (HMGAS), nicknamed S-System, of which NNs are special cases. In addition to many other results, the framework enables us to prove that 1) NNs implement {\\it renormalization group (RG)} using information geometry, which points out that the large scale property to renormalize is dual Bregman divergence and completes the analog between NNs and RG; 2) and under a set of {\\it realistic} boundedness and diversity conditions, for {\\it large size nonlinear deep} NNs with a class of losses, including the hinge loss, all local minima are global minima with zero loss errors, using random matrix theory.", "keywords": ["neural network theory", "probability measure theory", "probability coupling theory", "S-System", "optimization", "random matrix", "renormalization group", "information geometry", "coarse graining", "hierarchy", "activation function", "symmetry"], "authorids": ["lishuai918@gmail.com", "kuijia@scut.edu.cn"], "authors": ["Shuai Li", "Kui Jia"], "TL;DR": "We present a formal measure-theoretical theory of neural networks (NN) that quantitatively shows NNs renormalize on semantic difference, and under practical conditions large size deep nonlinear NNs can optimize objective functions to zero losses.", "pdf": "/pdf/6fb1ea402d64528f6560a40defd9b9f1f5d709fc.pdf", "paperhash": "li|ssystem_geometry_learning_and_optimization_a_theory_of_neural_networks", "_bibtex": "@misc{\nli2019ssystem,\ntitle={S-System, Geometry, Learning, and Optimization: A Theory of Neural Networks},\nauthor={Shuai Li and Kui Jia},\nyear={2019},\nurl={https://openreview.net/forum?id=HyxUIj09KX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper180/Official_Review", "cdate": 1542234520910, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HyxUIj09KX", "replyto": "HyxUIj09KX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper180/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335666118, "tmdate": 1552335666118, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper180/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 4}