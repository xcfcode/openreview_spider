{"notes": [{"tddate": null, "number": null, "replyto": null, "ddate": null, "cdate": null, "tmdate": 1456347473995, "tcdate": 1456347473995, "id": "ROVAEyEoPHvnM0J1IpPO", "invitation": "ICLR.cc/2016/workshop/-/submission", "forum": "ROVAEyEoPHvnM0J1IpPO", "signatures": ["~Ning_Zhang1"], "readers": ["everyone"], "writers": ["~Ning_Zhang1"], "content": {"CMT_id": "334", "title": "Fine-grained pose prediction, normalization, and recognition", "abstract": "Pose variation and subtle differences in appearance are key challenges to fine-\ngrained classification. While deep networks have markedly improved general\nrecognition, many approaches to fine-grained recognition rely on anchoring net-\nworks to parts for better accuracy. Identifying parts to find correspondence dis-\ncounts pose variation so that features can be tuned to appearance. To this end\nprevious methods have examined how to find parts and extract pose-normalized\nfeatures. These methods have generally separated fine-grained recognition into\nstages which first localize parts using hand-engineered and coarsely-localized pro-\nposal features, and then separately learn deep descriptors centered on inferred part\npositions. We unify these steps in an end-to-end trainable network supervised by\nkeypoint locations and class labels that localizes parts by a fully convolutional\nnetwork to focus the learning of feature representations for the fine-grained clas-\nsification task. Experiments on the popular CUB200 dataset show that our method\nis state-of-the-art and suggest a continuing role for strong supervision.", "pdf": "/pdf/ROVAEyEoPHvnM0J1IpPO.pdf", "paperhash": "zhang|finegrained_pose_prediction_normalization_and_recognition", "conflicts": ["eecs.berkeley.edu", "snapchat.com"], "authors": ["Ning Zhang", "Evan Shelhamer", "Yang Gao", "Trevor Darrell"], "authorids": ["ning.zhang@snapchat.com", "shelhamer@eecs.berkeley.edu", "yg@eecs.berkeley.edu", "trevor@eecs.berkeley.edu"]}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1454464564200, "ddate": null, "super": null, "final": null, "duedate": 1455833700000, "tcdate": 1454464564200, "id": "ICLR.cc/2016/workshop/-/submission", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "readers": ["everyone"], "reply": {"pdf": null, "forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"order": 4, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv.", "value-regex": "upload|http://arxiv.org/pdf/.+"}, "title": {"order": 3, "description": "Title of paper.", "value-regex": ".{0,500}"}, "abstract": {"order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"order": 1, "description": "Comma separated list of author names, as they appear in the paper.", "value-regex": "[^,\\n]+(,[^,\\n]+)*"}, "author_emails": {"order": 2, "description": "Comma separated list of author email addresses, in the same order as above.", "value-regex": "[^,\\n]+(,[^,\\n]+)*"}, "conflicts": {"order": 100, "description": "Semi-colon separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.).", "value-regex": "^([a-zA-Z0-9][a-zA-Z0-9-_]{0,61}[a-zA-Z0-9]{0,1}\\.([a-zA-Z]{1,6}|[a-zA-Z0-9-]{1,30}\\.[a-zA-Z]{2,3}))+(;[a-zA-Z0-9][a-zA-Z0-9-_]{0,61}[a-zA-Z0-9]{0,1}\\.([a-zA-Z]{1,6}|[a-zA-Z0-9-]{1,30}\\.[a-zA-Z]{2,3}))*$"}, "CMT_id": {"order": 5, "value-regex": ".*", "description": "If the paper is a resubmission from the ICLR 2016 Conference Track, enter its CMT ID; otherwise, leave blank."}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "expdate": 1463609700000}}}], "count": 1}